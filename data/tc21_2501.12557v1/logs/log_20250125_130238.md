# Claims Processing Log

Processing started at: 2025-01-25 13:02:38

## Table of Contents

[[log_20250125_130238###Claim 1/24|Claim 1/24]]
[[log_20250125_130238###Claim 2/24|Claim 2/24]]
[[log_20250125_130238###Claim 3/24|Claim 3/24]]
[[log_20250125_130238###Claim 4/24|Claim 4/24]]
[[log_20250125_130238###Claim 5/24|Claim 5/24]]
[[log_20250125_130238###Claim 6/24|Claim 6/24]]
[[log_20250125_130238###Claim 7/24|Claim 7/24]]
[[log_20250125_130238###Claim 8/24|Claim 8/24]]
[[log_20250125_130238###Claim 9/24|Claim 9/24]]
[[log_20250125_130238###Claim 10/24|Claim 10/24]]
[[log_20250125_130238###Claim 11/24|Claim 11/24]]
[[log_20250125_130238###Claim 12/24|Claim 12/24]]
[[log_20250125_130238###Claim 13/24|Claim 13/24]]
[[log_20250125_130238###Claim 14/24|Claim 14/24]]
[[log_20250125_130238###Claim 15/24|Claim 15/24]]
[[log_20250125_130238###Claim 16/24|Claim 16/24]]
[[log_20250125_130238###Claim 17/24|Claim 17/24]]
[[log_20250125_130238###Claim 18/24|Claim 18/24]]
[[log_20250125_130238###Claim 19/24|Claim 19/24]]
[[log_20250125_130238###Claim 20/24|Claim 20/24]]
[[log_20250125_130238###Claim 21/24|Claim 21/24]]
[[log_20250125_130238###Claim 22/24|Claim 22/24]]
[[log_20250125_130238###Claim 23/24|Claim 23/24]]
[[log_20250125_130238###Claim 24/24|Claim 24/24]]


## Processing Details


### Claim 1/24

#### Claim Text
Already, researchers have been using LLMs across the HCI research pipeline, from ideation and system development to data analysis and paperwriting [76].

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 7):

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Kapania and Wang, et al.
Ideation and project 
scoping
Study design and 
execution
Analysis and paper 
writing
Identifying research topics
Deﬁning research questions
Literature review
Research design
System building
Data collection
Data generation
Data analysis
System evaluation
Paper writing
Writing grant proposals
Fig. 1. LLMs are used in various ways for ideation and project scoping, study design and execution, and
analysis and paper writing. The figure illustrates a typical HCI study across our research participants. Not all
HCI research projects would include all activities listed above (e.g., critical theoretical contributions).
4.1 Where do HCI researchers use LLMs in their everyday work?
HCI researchers referred to various parts of their research workflow where they integrated large
language models, such as for ideation, literature review, study design, data analysis, system building
and evaluation, and paper writing (Figure 1). Overall, they perceived that LLMs open up new
possibilities in their research, such that “if we can leverage LLMs the right way, it will enable us to do
new cool things that will be genuinely empowering” (P4). Our survey revealed that the stages where
LLMs are most frequently used are paper writing (25) and study design (24), followed by project
scoping (17) and system development (16), and then data generation (15), data collection (14) and
data analysis (13). Below, we present the ways in which interview participants incorporate LLMs
in their research practices.
Interview participants frequently used large language models in the ideation and project
scoping phase, for tasks such as reviewing and synthesizing literature, discovering new research
questions in their sub-field of HCI, and defining their research problems. P11 would input broad
topic areas into the LLM to generate HCI research questions and subsequently refine them into
concrete research objectives. Similarly, P10 would probe the LLM to“pretend that it is a career coach
for [participant name]. What would [the LLM] recommend if [participant name] is writing their NSF
career grants? This is a big thing for early career HCI academic researchers. What should [participant
name] explore at the intersection of AI and cybersecurity?” During brainstorming, HCI researchers
found value in using large language models for a breadth-first search approach, enabling them to
quickly generate a diverse range of ideas.
LLMs were also seen to have utility in data generation, collection, and analysis. Many
participants mentioned how LLMs were especially productive in synthesizing information from
web sources, that would otherwise require significant time and effort. P1 prompted the LLM to
generate multiple arguments for and against hypothetical scenarios for use in a classroom setting.
They noted how creating these research artifacts for data collection would have taken them weeks
without the LLM. Additionally, HCI researchers integrated LLMs into their data analysis process,
utilizing them for tasks such as for qualitative data coding, creating plots, and data visualizations. P7
applied LLMs for multiple aspects of open coding interview data, including (1) proposing new codes,
(2) acting as a mediator between the coding team consisting of two members, and (3) generating
the primary code groups.
HCI researchers described how they increasingly relied on LLMs in the paper writingstage.
Participants shared their experiences of leveraging LLMs for iteratively refining their paper drafts,
including searching for synonyms and fixing grammatical issues. LLMs offered a distinct perspective
8



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 2):

Examining ethics of LLM use in HCI research practices Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
2 RELATED WORK
2.1 The Use of LLMs in HCI Research
As the capabilities of generative AI in understanding context and generating natural language
responses continue to advance [ 77], large language models are being increasingly used by re-
searchers as tools in Human-Computer Interaction (HCI) research. Emerging work has explored
the potential of leveraging LLMs to support the process of brainstorming and identifying research
topics [47, 89, 100]. Tools like CoQuest [61] have been developed for research question development
through human-agent co-creation. In addition to research ideation, LLMs have been used as tools
in human-AI co-creative design ideation [64, 103] and writing support [116].
Other members of the community have made efforts to develop LLM-powered applications for
data generation and data analysis [83]. Hämäläinenet al. [48] explored the potential of utilizing LLMs
to produce synthetic user interview transcripts for piloting research ideas and designing interview
protocols. Weiet al. [105] created LLM-based chatbots for generating synthesized user self-reported
data. Researchers have also developed LLM-based applications for qualitative analysis, including
identifying themes and generating codebooks [ 16, 45, 101]. Prior work has also explored using
LLMs for deductive coding [26] and for human-AI collaborative qualitative analysis [44, 45, 111]. In
terms of quantitative analysis, tools such as Github Copilot1 and GPT-4 [77] can enable researchers
to transform natural language instructions into programming codes that assist quantitative data
analysis and visualization. LLMs have been used as tools for quantitative data analysis in sampling,
filtering, and analyzing survey data [52], gaining insights from large corpus [78] and crowdsourcing
social data [34].
Researchers have also used LLMs as the underlying technology for system design and develop-
ment. Lu et al. [62] and Petridis et al. [86] explored infusing prompt-based prototyping enabled by
LLM into functional user interface design. Researchers also utilized LLMs to build task-oriented
social simulations for LLM agents, aiming to study the intricate social dynamics of societal sys-
tems [43, 58, 85]. Other applications of integrating LLMs into the design and development of HCI
systems have focused on leveraging LLMs for efficient prototyping [57, 109] that can be potentially
used by HCI researchers and designers. Our research extends this scholarship by empirically ex-
amining practices and ethical challenges of integrating large language models into HCI research
projects.
2.2 Research Ethics in HCI
The Human-Computer Interaction (HCI) community has long been engaged in discussions regarding
ethics [36, 88], revolving around responsible conduct in human subjects studies [11, 90], including
privacy, informed consent, and institutional review boards (IRBs). These concerns are integral to
ensuring ethical conduct and preventing harms to research participants [9]. Existing HCI guidelines
of privacy focus on the protection of participants’ sensitive information, the anonymity of the
participants, and the confidentiality of collected data [15, 118]. The rights of autonomy and self-
determination of participants require HCI researchers to establish transparent procedures for
informed consent on the collection and analysis of personal information [41]. IRBs play a critical
role in ensuring that HCI researchers meet their ethical obligations when conducting studies with
human subjects [5, 14]. Moreover, prior scholarship underscores the need to mitigate potential
biases in research design, data collection, and data analysis by refining existing ethical guidelines
and protocols [71].
The process of creating ethical guidelines for emerging technologies within the HCI community
has drawn inspiration from ethical theories and professional standards. Mackay [65] highlighted
1https://github.com/features/copilot
3



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 5):

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Kapania and Wang, et al.
China (1), Spain (1), Nigeria (1), Australia (1), Japan (1). On average, respondents had 4 years of
experience working on HCI research projects.
Questionnaire. Our questionnaire consisted of 18 questions in total, with a mixture of multiple-
choice (14) and open-text questions (4). We began the survey by describing LLMs as“a subset of
generative language (and multimodal) models with increasing size as measured by the number of
parameters and size of training data” [ 4] ( e.g., GPT-4, GPT-3.5, Llama 2, Vicuna, and more ). We then
ask respondents to refer to their latest research project that involved the use of LLMs and respond
to the following questions related to that project only. The survey was divided into three sections:
(1) questions about their use of LLMs in their HCI research projects, (2) questions about how they
engage with ethics of LLMs use in HCI research, and (3) demographic questions relevant to our
study. In the first section, respondents were asked to describe the project in one sentence and share
the primary research method, sub-area of HCI, and the stage of their research process where they
incorporated LLMs in their project.
After understanding the HCI research project in which they used LLMs, we focused on their
potential ethical considerations with the use of LLMs. We asked,“have [they] encountered or observed
any ethical challenges related to LLMs in [their] research project?” . This was followed by the question
asking what the ethical challenges are in the form of a close-ended (with choices such as security
and privacy, consent, harmful outputs, copyright issues, authorship, prefer not to say) and an open-
ended question, respectively. We asked how they identified, potentially mitigated, and reported
these ethical challenges. Finally, we also included demographic questions (optional to answer)
about the respondent’s type of institution, country, and years of experience in HCI.
Analysis. We computed a range of descriptive statistics using SPSS to better understand the
approach of researchers to ethical concerns with LLMs. These included descriptive statistics to
questions presented with multiple choice answers (e.g., the ethical challenges in using LLMs). In
cases where questions were completed by a subset of respondents, we report question-specific
response rates and the percentage of respondents who answered that question. Finally, we conducted
a qualitative analysis of open-ended questions following the same approach to the interviews (see
the following Section 3.2 below). We performed multiple rounds of coding at the response level
in conjunction with participants’ survey ratings to surface high-level themes. We include direct
quotes from our survey respondents in the Findings with the prefix ‘S#’ to differentiate them from
our interview participants, which were prefixed with ‘P#’.
3.2 Interviews
Between October and November 2023, we conducted interviews with 16 HCI researchers who
used LLMs in their research projects. Each interview had structured sub-sections beginning with
the participants describing a recent project where they applied LLMs across any of their research
activities to gain more context for the following questions. The responses in the survey informed
the design of the interview questionnaire (e.g., further exploring the ethical concerns that were
raised in the quantitative data). Participants could take a few minutes to look at their history with
the LLM tools/applications. Our interviews focused on: (1) LLM use across the research workflow;
(2) specific ethical considerations; (3) the process of navigating ethical considerations; (4) the role of
IRBs; (5) the role of ethical frameworks and toolkits; (6) incentives and accountability. Each session
focused on the researchers’ practices and the associated ethical considerations.
Participant recruitment.We recruited participants through a combination of distribution
lists, professional networks, and personal contacts, using snowball and purposive sampling [81]
iteratively until saturation. Our sample included researchers located in the United States (13), China
(1), Singapore (1), and Germany (1). We interviewed 10 male and 6 female researchers. A majority
6



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 4):

Examining ethics of LLM use in HCI research practices Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
including integrating a misinformation detector [82] and redesigning prompting methods to guide
LLMs [82, 114]. Mozes et al. [69] presented prevention measures for when LLMs were used for
illicit purposes, arguing that red-teaming, safeguarding with RLHF and instruction following, and
avoiding memorization and poisoning are potential solutions to the misuse of LLMs. Regarding
human-computer interaction harms of LLMs, Liao and Vaughan [60] provided a roadmap for
improving the transparency and explainability of LLMs.
While recent efforts have begun to mitigate the broader ethical concerns of LLMs, the application
of LLMs in HCI research presents unique ethical challenges. For instance, biases in the training
data can adversely affect research practices, leading to issues with internal and external validity,
reproducibility, efficiency, and the risk of proliferating low-quality research [3]. Despite this, there is
a notable gap in understanding how HCI researchers perceive and manage these ethical challenges
in their day-to-day research activities. This study aims to fill this gap by exploring the unique
ethical issues posed by the use of LLMs in HCI research and examining how researchers in this
field are currently addressing these challenges.
3 METHODS
In this research, we focus on examining the ways in which HCI researchers apply large language
models (LLMs) across their research workflows and their ethical considerations for using LLM-based
tools. For a holistic view of LLM use practices, we employed a mixed-method approach with a
sequential explanatory design [51]. This involved conducting a survey with the goal of eliciting
broad-brushed and higher-level perspectives from a wide audience. The survey was followed by
semi-structured interviews to investigate, in more detail, the ways in which researchers approach
the ethical considerations of using LLMs as part of their research activities. The qualitative data
from the interviews was used to elaborate and explain the survey results (e.g., the rationale behind
their approach to engaging with ethical concerns) and served as the foundation for our inquiry
[51]. Our research study was reviewed and approved by the IRB at our institution. We present our
approaches to the survey and interviews in the following subsections.
3.1 Survey
The goal of the survey was to identify the ways in which HCI researchers are using LLMs and any
ethical considerations they have encountered in their projects. We conducted the survey using an
online questionnaire implemented in Qualtrics and analyzed responses from 50 respondents.
Participant recruitment.We recruited survey participants through multiple channels: ad-
vertising on social media networks such as Twitter and LinkedIn, emailing direct contacts, and
leveraging university distribution lists to which we had access. We began the survey by eliciting
informed consent from respondents. No personally identifiable information was recorded about
the respondents in accordance with our organization’s research privacy and ethics guidelines. The
inclusion criteria for our survey were similar to the interviews, where we recruited researchers
who are currently studying or working in areas related to Human-Computer Interaction (HCI) and
have used large language models (LLMs) in their research.
After the screening questions, we were left with n = 77 participants. Out of the 77 respondents,
50 completed all sections except for the demographics (which was optional). Among the 43 survey
respondents who filled in the demographic questions, most reported working in academia (34),
industry (6), and non-profit (3) organizations. Researchers worked on projects within Human-AI
Interaction (32), Design (13), Understanding People: Theory, Concepts, and Methods (12), Collabora-
tive and Social Computing (10), and User Experience and Usability (10). In our sample, respondents
were located in the United States (20), Afghanistan (5), Germany (3), Algeria (2), Hong Kong (4),
5



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 1):

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Kapania and Wang, et al.
However, in contrast to the excitement towards the potential of LLMs, a growing body of work
has surfaced risks associated with these models [55, 106], including misinformation, discrimination
and exclusion, malicious use, and more. Cheng et al. [24] demonstrated how GPT-4 simulations of
certain demographics (e.g., marginalized race/ethnicity groups) and topics are highly susceptible to
caricature. Participants’ data privacy is also at risk, with evidence showing how LLM-based tools
might leak sensitive information with or without malicious prompting [18, 117].
Examining the use of LLMs holds particular relevance for HCI, given our frequent interactions
with human subjects, engagement in community-collaborative efforts [65], and interest in designing
systems with a socio-technical approach [ 11]. Indeed, the HCI community has demonstrated a
long-standing commitment to understanding the impacts and ethical considerations of emerging
technologies in research practices, dating back to the use of videotapes in the early 1990s [65]. Over
the years, the SIGCHI research ethics committee has been facilitating open conversations about
ethical challenges in our communities through research ethics town halls and panels at conference
venues such as CHI [37, 42, 70] and CSCW [12, 35]. Researchers within the community have also
organized several workshops and meetings to discuss the ethical challenges of HCI research, such
as how to conduct large-scale user trials [21], how to engage with vulnerable populations [2, 59]
and how to conduct research with public data [38].
Despite HCI’s rich tradition of centering the discourse on research ethics, the rapid uptake of
emerging LLM applications has brought renewed urgency to examine and collaboratively shape
norms for LLM use [93]. However, there is a gap in our understanding of HCI researchers’ current
practices surrounding LLMs, and uncovering them can offer a critical view into how they navigate
ethical considerations. In this research, we ask: (1) How do HCI researchers integrate large language
models in their projects? (2) What ethical concerns, if any, do they have regarding using LLMs? (3)
How do HCI researchers approach and navigate those ethical concerns?
We report our results from 50 survey responses and 16 in-depth semi-structured interviews
with HCI researchers using LLMs in their work. Across our participants, we find that LLMs were
utilized throughout the entire HCI research process, from ideation to system development and
paper writing. Large language models were perceived to open new possibilities for building tools
and interactions, generating research ideas, and simplifying workflow for analysis and writing. We
also came to see how researchers increasingly integrated LLMs into their everyday practice.
Our participants anticipated a wide range of potential ethical issues associated with LLMs, such as
potential harms in interacting with LLM outputs, privacy concerns, violation of intellectual integrity,
and overtrust & overreliance. While HCI researchers acknowledged these ethical considerations, in
many cases, they were either unable to or only partially able to identify and address those ethical
concerns in their own projects. Many participants highlighted their perceived lack of control with
their position in the LLM supply chain [108], a lack of established best practices, and competing
priorities that took precedence over addressing ethical concerns.
We begin by situating our study within prior work on LLM-based tools to support research
workflows, scholarship on research ethics in HCI, and literature on the ethical issues with large
language models. After presenting a detailed description of our methods, we present our findings
on HCI research practices surrounding the ethics of LLM-based tools. Finally, we reflect on these
results and present implications for the HCI research community. Taken together, our research
underscores the importance of foregrounding research ethics if we are to continue integrating
LLMs into our work practices. We call for engaging with IRBs and other regulatory institutions,
redesigning effective informed consent processes, developing tools and processes to interrupt the
LLM supply chain, providing learning opportunities for ethics of LLM use in HCI, and shifting
existing academic incentive structures to foreground ethical considerations in research.
2



### Claim 2/24

#### Claim Text
Many of these reviews survey technical advancements, e.g., Zhao et al. [183] survey methods for training and evaluating core models, Gao et al. [44] review the state-of-theart in retrieval-augmented generation, and Guo et al. [51] review multi-agent approaches.

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[44]_2312.10997.pdf (Page 16):

17
The growing ecosystem of RAG is evidenced by the rise in
RAG-centric AI applications and the continuous development
of supportive tools. As RAG’s application landscape broadens,
there is a need to refine evaluation methodologies to keep
pace with its evolution. Ensuring accurate and representative
performance assessments is crucial for fully capturing RAG’s
contributions to the AI research and development community.
REFERENCES
[1] N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel, “Large
language models struggle to learn long-tail knowledge,” in Interna-
tional Conference on Machine Learning . PMLR, 2023, pp. 15 696–
15 707.
[2] Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,
Y . Zhang, Y . Chenet al., “Siren’s song in the ai ocean: A survey on hal-
lucination in large language models,” arXiv preprint arXiv:2309.01219,
2023.
[3] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and
A. Sharma, “Gar-meets-rag paradigm for zero-shot information re-
trieval,” arXiv preprint arXiv:2310.20158 , 2023.
[4] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,
H. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel et al. , “Retrieval-
augmented generation for knowledge-intensive nlp tasks,” Advances in
Neural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.
[5] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-
can, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clarket al.,
“Improving language models by retrieving from trillions of tokens,”
in International conference on machine learning . PMLR, 2022, pp.
2206–2240.
[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,
C. Zhang, S. Agarwal, K. Slama, A. Ray et al. , “Training language
models to follow instructions with human feedback,” Advances in
neural information processing systems , vol. 35, pp. 27 730–27 744,
2022.
[7] X. Ma, Y . Gong, P. He, H. Zhao, and N. Duan, “Query rewrit-
ing for retrieval-augmented large language models,” arXiv preprint
arXiv:2305.14283, 2023.
[8] I. ILIN, “Advanced rag techniques: an il-
lustrated overview,” https://pub.towardsai.net/
advanced-rag-techniques-an-illustrated-overview-04d193d8fec6,
2023.
[9] W. Peng, G. Li, Y . Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al. ,
“Large language model based long-tail query rewriting in taobao
search,” arXiv preprint arXiv:2311.03758 , 2023.
[10] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V . Le,
and D. Zhou, “Take a step back: Evoking reasoning via abstraction in
large language models,” arXiv preprint arXiv:2310.06117 , 2023.
[11] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval
without relevance labels,” arXiv preprint arXiv:2212.10496 , 2022.
[12] V . Blagojevi, “Enhancing rag pipelines in haystack: Introducing diver-
sityranker and lostinthemiddleranker,” https://towardsdatascience.com/
enhancing-rag-pipelines-in-haystack-45f14e2bc9f5, 2023.
[13] W. Yu, D. Iter, S. Wang, Y . Xu, M. Ju, S. Sanyal, C. Zhu, M. Zeng,
and M. Jiang, “Generate rather than retrieve: Large language models
are strong context generators,” arXiv preprint arXiv:2209.10063, 2022.
[14] Z. Shao, Y . Gong, Y . Shen, M. Huang, N. Duan, and W. Chen,
“Enhancing retrieval-augmented large language models with iterative
retrieval-generation synergy,” arXiv preprint arXiv:2305.15294 , 2023.
[15] X. Wang, Q. Yang, Y . Qiu, J. Liang, Q. He, Z. Gu, Y . Xiao,
and W. Wang, “Knowledgpt: Enhancing large language models with
retrieval and storage access on knowledge bases,” arXiv preprint
arXiv:2308.11761, 2023.
[16] A. H. Raudaschl, “Forget rag, the future
is rag-fusion,” https://towardsdatascience.com/
forget-rag-the-future-is-rag-fusion-1147298d8ad1, 2023.
[17] X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao, and R. Yan, “Lift
yourself up: Retrieval-augmented text generation with self memory,”
arXiv preprint arXiv:2305.02437 , 2023.
[18] S. Wang, Y . Xu, Y . Fang, Y . Liu, S. Sun, R. Xu, C. Zhu, and
M. Zeng, “Training data is more valuable than you think: A simple
and effective method by retrieving from training data,” arXiv preprint
arXiv:2203.08773, 2022.
[19] X. Li, E. Nie, and S. Liang, “From classification to generation:
Insights into crosslingual retrieval augmented icl,” arXiv preprint
arXiv:2311.06595, 2023.
[20] D. Cheng, S. Huang, J. Bi, Y . Zhan, J. Liu, Y . Wang, H. Sun,
F. Wei, D. Deng, and Q. Zhang, “Uprise: Universal prompt retrieval
for improving zero-shot evaluation,” arXiv preprint arXiv:2303.08518,
2023.
[21] Z. Dai, V . Y . Zhao, J. Ma, Y . Luan, J. Ni, J. Lu, A. Bakalov, K. Guu,
K. B. Hall, and M.-W. Chang, “Promptagator: Few-shot dense retrieval
from 8 examples,” arXiv preprint arXiv:2209.11755 , 2022.
[22] Z. Sun, X. Wang, Y . Tay, Y . Yang, and D. Zhou, “Recitation-augmented
language models,” arXiv preprint arXiv:2210.01296 , 2022.
[23] O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts,
and M. Zaharia, “Demonstrate-search-predict: Composing retrieval
and language models for knowledge-intensive nlp,” arXiv preprint
arXiv:2212.14024, 2022.
[24] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang,
J. Callan, and G. Neubig, “Active retrieval augmented generation,”
arXiv preprint arXiv:2305.06983 , 2023.
[25] A. Asai, Z. Wu, Y . Wang, A. Sil, and H. Hajishirzi, “Self-rag:
Learning to retrieve, generate, and critique through self-reflection,”
arXiv preprint arXiv:2310.11511 , 2023.
[26] Z. Ke, W. Kong, C. Li, M. Zhang, Q. Mei, and M. Bendersky,
“Bridging the preference gap between retrievers and llms,” arXiv
preprint arXiv:2401.06954, 2024.
[27] X. V . Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Ro-
driguez, J. Kahn, G. Szilvasy, M. Lewis et al. , “Ra-dit: Retrieval-
augmented dual instruction tuning,” arXiv preprint arXiv:2310.01352 ,
2023.
[28] O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha, “Fine-tuning or
retrieval? comparing knowledge injection in llms,” arXiv preprint
arXiv:2312.05934, 2023.
[29] T. Lan, D. Cai, Y . Wang, H. Huang, and X.-L. Mao, “Copy is all
you need,” in The Eleventh International Conference on Learning
Representations, 2022.
[30] T. Chen, H. Wang, S. Chen, W. Yu, K. Ma, X. Zhao, D. Yu, and
H. Zhang, “Dense x retrieval: What retrieval granularity should we
use?” arXiv preprint arXiv:2312.06648 , 2023.
[31] F. Luo and M. Surdeanu, “Divide & conquer for entailment-aware
multi-hop evidence retrieval,” arXiv preprint arXiv:2311.02616 , 2023.
[32] Q. Gou, Z. Xia, B. Yu, H. Yu, F. Huang, Y . Li, and N. Cam-Tu,
“Diversify question generation with retrieval-augmented style transfer,”
arXiv preprint arXiv:2310.14503 , 2023.
[33] Z. Guo, S. Cheng, Y . Wang, P. Li, and Y . Liu, “Prompt-guided re-
trieval augmentation for non-knowledge-intensive tasks,”arXiv preprint
arXiv:2305.17653, 2023.
[34] Z. Wang, J. Araki, Z. Jiang, M. R. Parvez, and G. Neubig, “Learning
to filter context for retrieval-augmented generation,” arXiv preprint
arXiv:2311.08377, 2023.
[35] M. Seo, J. Baek, J. Thorne, and S. J. Hwang, “Retrieval-augmented
data augmentation for low-resource domain tasks,” arXiv preprint
arXiv:2402.13482, 2024.
[36] Y . Ma, Y . Cao, Y . Hong, and A. Sun, “Large language model is not
a good few-shot information extractor, but a good reranker for hard
samples!” arXiv preprint arXiv:2303.08559 , 2023.
[37] X. Du and H. Ji, “Retrieval-augmented generative question answering
for event argument extraction,” arXiv preprint arXiv:2211.07067, 2022.
[38] L. Wang, N. Yang, and F. Wei, “Learning to retrieve in-context
examples for large language models,”arXiv preprint arXiv:2307.07164,
2023.
[39] S. Rajput, N. Mehta, A. Singh, R. H. Keshavan, T. Vu, L. Heldt,
L. Hong, Y . Tay, V . Q. Tran, J. Samostet al., “Recommender systems
with generative retrieval,” arXiv preprint arXiv:2305.05065 , 2023.
[40] B. Jin, H. Zeng, G. Wang, X. Chen, T. Wei, R. Li, Z. Wang, Z. Li,
Y . Li, H. Lu et al. , “Language models as semantic indexers,” arXiv
preprint arXiv:2310.07815, 2023.
[41] R. Anantha, T. Bethi, D. V odianik, and S. Chappidi, “Context tuning
for retrieval augmented generation,” arXiv preprint arXiv:2312.05708 ,
2023.
[42] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,
J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Few-shot
learning with retrieval augmented language models,” arXiv preprint
arXiv:2208.03299, 2022.
[43] J. Huang, W. Ping, P. Xu, M. Shoeybi, K. C.-C. Chang, and B. Catan-
zaro, “Raven: In-context learning with retrieval augmented encoder-
decoder language models,” arXiv preprint arXiv:2308.07922 , 2023.



Source: data\tc21_2501.12557v1\referenced_papers\[44]_2312.10997.pdf (Page 0):

1
Retrieval-Augmented Generation for Large
Language Models: A Survey
Yunfan Gaoa, Yun Xiongb, Xinyu Gao b, Kangxiang Jia b, Jinliu Pan b, Yuxi Bic, Yi Dai a, Jiawei Sun a, Meng
Wangc, and Haofen Wang a,c
aShanghai Research Institute for Intelligent Autonomous Systems, Tongji University
bShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University
cCollege of Design and Innovation, Tongji University
Abstract—Large Language Models (LLMs) showcase impres-
sive capabilities but encounter challenges like hallucination,
outdated knowledge, and non-transparent, untraceable reasoning
processes. Retrieval-Augmented Generation (RAG) has emerged
as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the
generation, particularly for knowledge-intensive tasks, and allows
for continuous knowledge updates and integration of domain-
specific information. RAG synergistically merges LLMs’ intrin-
sic knowledge with the vast, dynamic repositories of external
databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing
the Naive RAG, the Advanced RAG, and the Modular RAG.
It meticulously scrutinizes the tripartite foundation of RAG
frameworks, which includes the retrieval, the generation and the
augmentation techniques. The paper highlights the state-of-the-
art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG
systems. Furthermore, this paper introduces up-to-date evalua-
tion framework and benchmark. At the end, this article delineates
the challenges currently faced and points out prospective avenues
for research and development 1.
Index Terms—Large language model, retrieval-augmented gen-
eration, natural language processing, information retrieval
I. I NTRODUCTION
L
ARGE language models (LLMs) have achieved remark-
able success, though they still face significant limitations,
especially in domain-specific or knowledge-intensive tasks [1],
notably producing “hallucinations” [2] when handling queries
beyond their training data or requiring current information. To
overcome challenges, Retrieval-Augmented Generation (RAG)
enhances LLMs by retrieving relevant document chunks from
external knowledge base through semantic similarity calcu-
lation. By referencing external knowledge, RAG effectively
reduces the problem of generating factually incorrect content.
Its integration into LLMs has resulted in widespread adoption,
establishing RAG as a key technology in advancing chatbots
and enhancing the suitability of LLMs for real-world applica-
tions.
RAG technology has rapidly developed in recent years, and
the technology tree summarizing related research is shown
Corresponding Author.Email:haofen.wang@tongji.edu.cn
1Resources are available at https://github.com/Tongji-KGLLM/
RAG-Survey
in Figure 1. The development trajectory of RAG in the era
of large models exhibits several distinct stage characteristics.
Initially, RAG’s inception coincided with the rise of the
Transformer architecture, focusing on enhancing language
models by incorporating additional knowledge through Pre-
Training Models (PTM). This early stage was characterized
by foundational work aimed at refining pre-training techniques
[3]–[5].The subsequent arrival of ChatGPT [6] marked a
pivotal moment, with LLM demonstrating powerful in context
learning (ICL) capabilities. RAG research shifted towards
providing better information for LLMs to answer more com-
plex and knowledge-intensive tasks during the inference stage,
leading to rapid development in RAG studies. As research
progressed, the enhancement of RAG was no longer limited
to the inference stage but began to incorporate more with LLM
fine-tuning techniques.
The burgeoning field of RAG has experienced swift growth,
yet it has not been accompanied by a systematic synthesis that
could clarify its broader trajectory. This survey endeavors to
fill this gap by mapping out the RAG process and charting
its evolution and anticipated future paths, with a focus on the
integration of RAG within LLMs. This paper considers both
technical paradigms and research methods, summarizing three
main research paradigms from over 100 RAG studies, and
analyzing key technologies in the core stages of “Retrieval,”
“Generation,” and “Augmentation.” On the other hand, current
research tends to focus more on methods, lacking analysis and
summarization of how to evaluate RAG. This paper compre-
hensively reviews the downstream tasks, datasets, benchmarks,
and evaluation methods applicable to RAG. Overall, this
paper sets out to meticulously compile and categorize the
foundational technical concepts, historical progression, and
the spectrum of RAG methodologies and applications that
have emerged post-LLMs. It is designed to equip readers and
professionals with a detailed and structured understanding of
both large models and RAG. It aims to illuminate the evolution
of retrieval augmentation techniques, assess the strengths and
weaknesses of various approaches in their respective contexts,
and speculate on upcoming trends and innovations.
Our contributions are as follows:
• In this survey, we present a thorough and systematic
review of the state-of-the-art RAG methods, delineating
its evolution through paradigms including naive RAG,
arXiv:2312.10997v5  [cs.CL]  27 Mar 2024



Source: data\tc21_2501.12557v1\referenced_papers\[44]_2312.10997.pdf (Page 17):

18
[44] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y . Dong,
O. Kuchaiev, B. Li, C. Xiao et al. , “Shall we pretrain autoregressive
language models with retrieval? a comprehensive study,”arXiv preprint
arXiv:2304.06762, 2023.
[45] B. Wang, W. Ping, L. McAfee, P. Xu, B. Li, M. Shoeybi, and B. Catan-
zaro, “Instructretro: Instruction tuning post retrieval-augmented pre-
training,” arXiv preprint arXiv:2310.07713 , 2023.
[46] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana,
and S. Nanayakkara, “Improving the domain adaptation of retrieval
augmented generation (rag) models for open domain question answer-
ing,” Transactions of the Association for Computational Linguistics ,
vol. 11, pp. 1–17, 2023.
[47] Z. Yu, C. Xiong, S. Yu, and Z. Liu, “Augmentation-adapted retriever
improves generalization of language models as generic plug-in,” arXiv
preprint arXiv:2305.17331, 2023.
[48] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-
augmented language models robust to irrelevant context,” arXiv
preprint arXiv:2310.01558, 2023.
[49] H.-T. Chen, F. Xu, S. A. Arora, and E. Choi, “Understanding re-
trieval augmentation for long-form question answering,” arXiv preprint
arXiv:2310.12150, 2023.
[50] W. Yu, H. Zhang, X. Pan, K. Ma, H. Wang, and D. Yu, “Chain-of-note:
Enhancing robustness in retrieval-augmented language models,” arXiv
preprint arXiv:2311.09210, 2023.
[51] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, “Search-in-the-
chain: Towards accurate, credible and traceable large language models
for knowledgeintensive tasks,” CoRR, vol. abs/2304.14732 , 2023.
[52] M. Berchansky, P. Izsak, A. Caciularu, I. Dagan, and M. Wasserblat,
“Optimizing retrieval-augmented reader models via token elimination,”
arXiv preprint arXiv:2310.13682 , 2023.
[53] J. L ´ala, O. O’Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques,
and A. D. White, “Paperqa: Retrieval-augmented generative agent for
scientific research,” arXiv preprint arXiv:2312.07559 , 2023.
[54] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,
Y . Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:
Redefining retrieval for rag systems,” arXiv preprint arXiv:2401.14887,
2024.
[55] Z. Zhang, X. Zhang, Y . Ren, S. Shi, M. Han, Y . Wu, R. Lai, and
Z. Cao, “Iag: Induction-augmented generation framework for answer-
ing reasoning questions,” in Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing , 2023, pp. 1–14.
[56] N. Thakur, L. Bonifacio, X. Zhang, O. Ogundepo, E. Kamalloo,
D. Alfonso-Hermelo, X. Li, Q. Liu, B. Chen, M. Rezagholizadeh et al.,
“Nomiracl: Knowing when you don’t know for robust multilingual
retrieval-augmented generation,” arXiv preprint arXiv:2312.11361 ,
2023.
[57] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifica-
tions: Answering ambiguous questions with retrieval-augmented large
language models,” arXiv preprint arXiv:2310.14696 , 2023.
[58] Y . Wang, P. Li, M. Sun, and Y . Liu, “Self-knowledge guided
retrieval augmentation for large language models,” arXiv preprint
arXiv:2310.05002, 2023.
[59] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, “Retrieval-
generation synergy augmented large language models,” arXiv preprint
arXiv:2310.05149, 2023.
[60] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,
E. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long
context large language models,” arXiv preprint arXiv:2310.03025 ,
2023.
[61] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Interleav-
ing retrieval with chain-of-thought reasoning for knowledge-intensive
multi-step questions,” arXiv preprint arXiv:2212.10509 , 2022.
[62] R. Ren, Y . Wang, Y . Qu, W. X. Zhao, J. Liu, H. Tian, H. Wu, J.-
R. Wen, and H. Wang, “Investigating the factual knowledge boundary
of large language models with retrieval augmentation,” arXiv preprint
arXiv:2307.11019, 2023.
[63] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D.
Manning, “Raptor: Recursive abstractive processing for tree-organized
retrieval,” arXiv preprint arXiv:2401.18059 , 2024.
[64] O. Ram, Y . Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-
Brown, and Y . Shoham, “In-context retrieval-augmented language
models,” arXiv preprint arXiv:2302.00083 , 2023.
[65] Y . Ren, Y . Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, “Retrieve-and-
sample: Document-level event argument extraction via hybrid retrieval
augmentation,” in Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers) ,
2023, pp. 293–306.
[66] Z. Wang, X. Pan, D. Yu, D. Yu, J. Chen, and H. Ji, “Zemi: Learning
zero-shot semi-parametric language models from multiple tasks,” arXiv
preprint arXiv:2210.00185, 2022.
[67] S.-Q. Yan, J.-C. Gu, Y . Zhu, and Z.-H. Ling, “Corrective retrieval
augmented generation,” arXiv preprint arXiv:2401.15884 , 2024.
[68] P. Jain, L. B. Soares, and T. Kwiatkowski, “1-pager: One pass answer
generation and evidence retrieval,” arXiv preprint arXiv:2310.16568 ,
2023.
[69] H. Yang, Z. Li, Y . Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao, “Prca:
Fitting black-box large language models for retrieval question answer-
ing via pluggable reward-driven contextual adapter,” arXiv preprint
arXiv:2310.18347, 2023.
[70] S. Zhuang, B. Liu, B. Koopman, and G. Zuccon, “Open-source large
language models are strong zero-shot query likelihood models for
document ranking,” arXiv preprint arXiv:2310.13243 , 2023.
[71] F. Xu, W. Shi, and E. Choi, “Recomp: Improving retrieval-augmented
lms with compression and selective augmentation,” arXiv preprint
arXiv:2310.04408, 2023.
[72] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-
moyer, and W.-t. Yih, “Replug: Retrieval-augmented black-box lan-
guage models,” arXiv preprint arXiv:2301.12652 , 2023.
[73] E. Melz, “Enhancing llm intelligence with arm-rag: Auxiliary ra-
tionale memory for retrieval augmented generation,” arXiv preprint
arXiv:2311.04177, 2023.
[74] H. Wang, W. Huang, Y . Deng, R. Wang, Z. Wang, Y . Wang, F. Mi,
J. Z. Pan, and K.-F. Wong, “Unims-rag: A unified multi-source
retrieval-augmented generation for personalized dialogue systems,”
arXiv preprint arXiv:2401.13256 , 2024.
[75] Z. Luo, C. Xu, P. Zhao, X. Geng, C. Tao, J. Ma, Q. Lin, and D. Jiang,
“Augmented large language models with parametric knowledge guid-
ing,” arXiv preprint arXiv:2305.04757 , 2023.
[76] X. Li, Z. Liu, C. Xiong, S. Yu, Y . Gu, Z. Liu, and G. Yu, “Structure-
aware language model pretraining improves dense retrieval on struc-
tured data,” arXiv preprint arXiv:2305.19912 , 2023.
[77] M. Kang, J. M. Kwak, J. Baek, and S. J. Hwang, “Knowledge
graph-augmented language models for knowledge-grounded dialogue
generation,” arXiv preprint arXiv:2305.18846 , 2023.
[78] W. Shen, Y . Gao, C. Huang, F. Wan, X. Quan, and W. Bi, “Retrieval-
generation alignment for end-to-end task-oriented dialogue system,”
arXiv preprint arXiv:2310.08877 , 2023.
[79] T. Shi, L. Li, Z. Lin, T. Yang, X. Quan, and Q. Wang, “Dual-feedback
knowledge retrieval for task-oriented dialogue systems,” arXiv preprint
arXiv:2310.14528, 2023.
[80] P. Ranade and A. Joshi, “Fabula: Intelligence report generation
using retrieval-augmented narrative construction,” arXiv preprint
arXiv:2310.13848, 2023.
[81] X. Jiang, R. Zhang, Y . Xu, R. Qiu, Y . Fang, Z. Wang, J. Tang,
H. Ding, X. Chu, J. Zhao et al. , “Think and retrieval: A hypothesis
knowledge graph enhanced medical large language models,” arXiv
preprint arXiv:2312.15883, 2023.
[82] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang,
“Knowledge-augmented language model verification,” arXiv preprint
arXiv:2310.12836, 2023.
[83] L. Luo, Y .-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faithful
and interpretable large language model reasoning,” arXiv preprint
arXiv:2310.01061, 2023.
[84] X. He, Y . Tian, Y . Sun, N. V . Chawla, T. Laurent, Y . LeCun,
X. Bresson, and B. Hooi, “G-retriever: Retrieval-augmented generation
for textual graph understanding and question answering,”arXiv preprint
arXiv:2402.07630, 2024.
[85] L. Zha, J. Zhou, L. Li, R. Wang, Q. Huang, S. Yang, J. Yuan, C. Su,
X. Li, A. Su et al., “Tablegpt: Towards unifying tables, nature language
and commands into one gpt,” arXiv preprint arXiv:2307.08674 , 2023.
[86] M. Gaur, K. Gunaratna, V . Srinivasan, and H. Jin, “Iseeq: Information
seeking question generation using dynamic meta-information retrieval
and knowledge graphs,” in Proceedings of the AAAI Conference on
Artificial Intelligence, vol. 36, no. 10, 2022, pp. 10 672–10 680.
[87] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Sch ¨arli,
and D. Zhou, “Large language models can be easily distracted by
irrelevant context,” in International Conference on Machine Learning .
PMLR, 2023, pp. 31 210–31 227.
[88] R. Teja, “Evaluating the ideal chunk size for a rag
system using llamaindex,” https://www.llamaindex.ai/blog/
evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,
2023.



Source: data\tc21_2501.12557v1\referenced_papers\[44]_2312.10997.pdf (Page 10):

11
Fig. 5. In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves
alternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval
involves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval
and generation. (Right) Adaptive retrieval focuses on enabling the RAG system to autonomously determine whether external knowledge retrieval is necessary
and when to stop retrieval and generation, often utilizing LLM-generated special tokens for control.
base for LLMs. This approach has been shown to enhance
the robustness of subsequent answer generation by offering
additional contextual references through multiple retrieval
iterations. However, it may be affected by semantic discon-
tinuity and the accumulation of irrelevant information. ITER-
RETGEN [14] employs a synergistic approach that lever-
ages “retrieval-enhanced generation” alongside “generation-
enhanced retrieval” for tasks that necessitate the reproduction
of specific information. The model harnesses the content
required to address the input task as a contextual basis for
retrieving pertinent knowledge, which in turn facilitates the
generation of improved responses in subsequent iterations.
B. Recursive Retrieval
Recursive retrieval is often used in information retrieval and
NLP to improve the depth and relevance of search results.
The process involves iteratively refining search queries based
on the results obtained from previous searches. Recursive
Retrieval aims to enhance the search experience by gradu-
ally converging on the most pertinent information through a
feedback loop. IRCoT [61] uses chain-of-thought to guide
the retrieval process and refines the CoT with the obtained
retrieval results. ToC [57] creates a clarification tree that
systematically optimizes the ambiguous parts in the Query. It
can be particularly useful in complex search scenarios where
the user’s needs are not entirely clear from the outset or where
the information sought is highly specialized or nuanced. The
recursive nature of the process allows for continuous learning
and adaptation to the user’s requirements, often resulting in
improved satisfaction with the search outcomes.
To address specific data scenarios, recursive retrieval and
multi-hop retrieval techniques are utilized together. Recursive
retrieval involves a structured index to process and retrieve
data in a hierarchical manner, which may include summarizing
sections of a document or lengthy PDF before performing a
retrieval based on this summary. Subsequently, a secondary
retrieval within the document refines the search, embodying
the recursive nature of the process. In contrast, multi-hop
retrieval is designed to delve deeper into graph-structured data
sources, extracting interconnected information [106].
C. Adaptive Retrieval
Adaptive retrieval methods, exemplified by Flare [24] and
Self-RAG [25], refine the RAG framework by enabling LLMs
to actively determine the optimal moments and content for
retrieval, thus enhancing the efficiency and relevance of the
information sourced.
These methods are part of a broader trend wherein
LLMs employ active judgment in their operations, as seen
in model agents like AutoGPT, Toolformer, and Graph-
Toolformer [107]–[109]. Graph-Toolformer, for instance, di-
vides its retrieval process into distinct steps where LLMs
proactively use retrievers, apply Self-Ask techniques, and em-
ploy few-shot prompts to initiate search queries. This proactive
stance allows LLMs to decide when to search for necessary
information, akin to how an agent utilizes tools.
WebGPT [110] integrates a reinforcement learning frame-
work to train the GPT-3 model in autonomously using a
search engine during text generation. It navigates this process
using special tokens that facilitate actions such as search
engine queries, browsing results, and citing references, thereby
expanding GPT-3’s capabilities through the use of external
search engines. Flare automates timing retrieval by monitoring
the confidence of the generation process, as indicated by the



Source: data\tc21_2501.12557v1\referenced_papers\[44]_2312.10997.pdf (Page 9):

10
introduces an innovative method for integrating knowledge
into white-box models via directive fine-tuning [75]. In this
approach, the retriever module is directly substituted to gen-
erate relevant documents according to a query. This method
assists in addressing the difficulties encountered during the
fine-tuning process and enhances model performance.
IV. G ENERATION
After retrieval, it is not a good practice to directly input all
the retrieved information to the LLM for answering questions.
Following will introduce adjustments from two perspectives:
adjusting the retrieved content and adjusting the LLM.
A. Context Curation
Redundant information can interfere with the final gener-
ation of LLM, and overly long contexts can also lead LLM
to the “Lost in the middle” problem [98]. Like humans, LLM
tends to only focus on the beginning and end of long texts,
while forgetting the middle portion. Therefore, in the RAG
system, we typically need to further process the retrieved
content.
1) Reranking: Reranking fundamentally reorders document
chunks to highlight the most pertinent results first, effectively
reducing the overall document pool, severing a dual purpose
in information retrieval, acting as both an enhancer and a
filter, delivering refined inputs for more precise language
model processing [70]. Reranking can be performed using
rule-based methods that depend on predefined metrics like
Diversity, Relevance, and MRR, or model-based approaches
like Encoder-Decoder models from the BERT series (e.g.,
SpanBERT), specialized reranking models such as Cohere
rerank or bge-raranker-large, and general large language mod-
els like GPT [12], [99].
2) Context Selection/Compression: A common misconcep-
tion in the RAG process is the belief that retrieving as many
relevant documents as possible and concatenating them to form
a lengthy retrieval prompt is beneficial. However, excessive
context can introduce more noise, diminishing the LLM’s
perception of key information .
(Long) LLMLingua [100], [101] utilize small language
models (SLMs) such as GPT-2 Small or LLaMA-7B, to
detect and remove unimportant tokens, transforming it into
a form that is challenging for humans to comprehend but
well understood by LLMs. This approach presents a direct
and practical method for prompt compression, eliminating the
need for additional training of LLMs while balancing language
integrity and compression ratio. PRCA tackled this issue by
training an information extractor [69]. Similarly, RECOMP
adopts a comparable approach by training an information
condenser using contrastive learning [71]. Each training data
point consists of one positive sample and five negative sam-
ples, and the encoder undergoes training using contrastive loss
throughout this process [102] .
In addition to compressing the context, reducing the num-
ber of documents aslo helps improve the accuracy of the
model’s answers. Ma et al. [103] propose the “Filter-Reranker”
paradigm, which combines the strengths of LLMs and SLMs.
In this paradigm, SLMs serve as filters, while LLMs function
as reordering agents. The research shows that instructing
LLMs to rearrange challenging samples identified by SLMs
leads to significant improvements in various Information
Extraction (IE) tasks. Another straightforward and effective
approach involves having the LLM evaluate the retrieved
content before generating the final answer. This allows the
LLM to filter out documents with poor relevance through LLM
critique. For instance, in Chatlaw [104], the LLM is prompted
to self-suggestion on the referenced legal provisions to assess
their relevance.
B. LLM Fine-tuning
Targeted fine-tuning based on the scenario and data char-
acteristics on LLMs can yield better results. This is also one
of the greatest advantages of using on-premise LLMs. When
LLMs lack data in a specific domain, additional knowledge can
be provided to the LLM through fine-tuning. Huggingface’s
fine-tuning data can also be used as an initial step.
Another benefit of fine-tuning is the ability to adjust the
model’s input and output. For example, it can enable LLM to
adapt to specific data formats and generate responses in a par-
ticular style as instructed [37]. For retrieval tasks that engage
with structured data, the SANTA framework [76] implements
a tripartite training regimen to effectively encapsulate both
structural and semantic nuances. The initial phase focuses on
the retriever, where contrastive learning is harnessed to refine
the query and document embeddings.
Aligning LLM outputs with human or retriever preferences
through reinforcement learning is a potential approach. For
instance, manually annotating the final generated answers
and then providing feedback through reinforcement learning.
In addition to aligning with human preferences, it is also
possible to align with the preferences of fine-tuned models
and retrievers [79]. When circumstances prevent access to
powerful proprietary models or larger parameter open-source
models, a simple and effective method is to distill the more
powerful models(e.g. GPT-4). Fine-tuning of LLM can also
be coordinated with fine-tuning of the retriever to align pref-
erences. A typical approach, such as RA-DIT [27], aligns the
scoring functions between Retriever and Generator using KL
divergence.
V. A UGMENTATION PROCESS IN RAG
In the domain of RAG, the standard practice often involves
a singular (once) retrieval step followed by generation, which
can lead to inefficiencies and sometimes is typically insuffi-
cient for complex problems demanding multi-step reasoning,
as it provides a limited scope of information [105]. Many
studies have optimized the retrieval process in response to this
issue, and we have summarised them in Figure 5.
A. Iterative Retrieval
Iterative retrieval is a process where the knowledge base
is repeatedly searched based on the initial query and the text
generated so far, providing a more comprehensive knowledge



#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[51]_2402.01680.pdf (Page 0):

Large Language Model based Multi-Agents: A Survey of Progress and Challenges
Taicheng Guo1 , Xiuying Chen2 , Yaqi Wang3∗ , Ruidi Chang , Shichao Pei4 ,
Nitesh V . Chawla1 , Olaf Wiest1 , Xiangliang Zhang1†
1University of Notre Dame, 2King Abdullah University of Science and Technology
3Southern University of Science and Technology,4University of Massachusetts Boston
{tguo2, nchawla, owiest, xzhang33}@nd.edu, xiuying.chen@kaust.edu.sa, ywang84@nd.edu,
ruidic@alumni.cmu.edu, shichao.pei@umb.edu
Abstract
Large Language Models (LLMs) have achieved re-
markable success across a wide array of tasks.
Due to the impressive planning and reasoning abil-
ities of LLMs, they have been used as autonomous
agents to do many tasks automatically. Recently,
based on the development of using one LLM as a
single planning or decision-making agent, LLM-
based multi-agent systems have achieved consid-
erable progress in complex problem-solving and
world simulation. To provide the community with
an overview of this dynamic field, we present this
survey to offer an in-depth discussion on the essen-
tial aspects of multi-agent systems based on LLMs,
as well as the challenges. Our goal is for readers to
gain substantial insights on the following questions:
What domains and environments do LLM-based
multi-agents simulate? How are these agents pro-
filed and how do they communicate? What mech-
anisms contribute to the growth of agents’ capaci-
ties? For those interested in delving into this field
of study, we also summarize the commonly used
datasets or benchmarks for them to have convenient
access. To keep researchers updated on the latest
studies, we maintain an open-source GitHub repos-
itory, dedicated to outlining the research on LLM-
based multi-agent systems.
1 Introduction
Large Language Models (LLMs) have recently shown re-
markable potential in reaching a level of reasoning and plan-
ning capabilities comparable to humans. This ability ex-
actly aligns with the expectations of humans for autonomous
agents that can perceive the surroundings, make decisions,
and take actions in response [Xi et al., 2023; Wooldridge and
Jennings, 1995; Russell and Norvig, 2009; Guo et al., 2023;
Liang et al., 2023]. Hence, LLM-based agent has been stud-
ied and rapidly developed to understand and generate human-
like instructions, facilitating sophisticated interactions and
∗This work was done when Yaqi was visiting students at the Uni-
versity of Notre Dame.
†Corresponding author.
decision-making in a wide range of contexts [Yao et al. ,
2023; Shinn et al., 2023; Li et al., 2023d ]. Timely survey
papers systematically summarize the progress of LLM-based
agents, as seen in works [Xi et al., 2023; Wanget al., 2023b].
Based on the inspiring capabilities of the single LLM-
based agent, LLM-based Multi-Agents have been proposed
to leverage the collective intelligence and specialized pro-
files and skills of multiple agents. Compared to systems us-
ing a single LLM-powered agent, multi-agent systems offer
advanced capabilities by 1) specializing LLMs into various
distinct agents, each with different capabilities, and 2) en-
abling interactions among these diverse agents to simulate
complex real-world environments effectively. In this context,
multiple autonomous agents collaboratively engage in plan-
ning, discussions, and decision-making, mirroring the co-
operative nature of human group work in problem-solving
tasks. This approach capitalizes on the communicative ca-
pabilities of LLMs, leveraging their ability to generate text
for communication and respond to textual inputs. Further-
more, it exploits LLMs’ extensive knowledge across vari-
ous domains and their latent potential to specialize in spe-
cific tasks. Recent research has demonstrated promising re-
sults in utilizing LLM-based multi-agents for solving vari-
ous tasks, such as software development [Hong et al., 2023;
Qian et al., 2023], multi-robot systems [Mandi et al., 2023;
Zhang et al., 2023c ], society simulation [Park et al., 2023;
Park et al. , 2022 ], policy simulation [Xiao et al. , 2023;
Hua et al. , 2023 ], and game simulation [Xu et al. , 2023c;
Wang et al., 2023c ]. Due to the nature of interdisciplinary
study in this field, it has attracted a diverse range of re-
searchers, expanding beyond AI experts to include those from
social science, psychology, and policy research. The vol-
ume of research papers is rapidly increasing, as shown in
Fig. 1 (inspired by the design in [Gao et al., 2023b]), thus
broadening the impact of LLM-based Multi-Agent research.
Nonetheless, earlier efforts were undertaken independently,
resulting in an absence of a systematic review to summarize
them, establish comprehensive blueprint of this field, and ex-
amine future research challenges. This underscores the sig-
nificance of our work and serves as the motivation behind pre-
senting this survey paper, dedicated to the research on LLM-
based multi-agent systems.
We expect that our survey can make significant contribu-
tions to both the research and development of LLMs and to
arXiv:2402.01680v2  [cs.CL]  19 Apr 2024



Source: data\tc21_2501.12557v1\referenced_papers\[51]_2402.01680.pdf (Page 10):

approaches in current research involve employing Memory
and Self-Evolution techniques to adjust agents based on feed-
back. While effective for individual agents, these methods do
not fully capitalize on the potential collective intelligence of
the agent network. They adjust agents in isolation, overlook-
ing the synergistic effects that can emerge from coordinated
multi-agent interactions. Hence, jointly adjusting multiple
agents and achieving optimal collective intelligence is still a
critical challenge for LLM-MA.
6.4 Scaling Up LLM-MA Systems
LLM-MA systems are composed of a number of individual
LLM-based agents, posing a significant challenge of scala-
bility regarding the number of agents. From the computa-
tional complexity perspective, each LLM-based agent, typ-
ically built on large language models like GPT-4, demands
substantial computational power and memory. Scaling up the
number of these agents in an LLM-MA system significantly
increases resource requirements. In scenarios with limited
computational resource, it would be challenging to develop
these LLM-MA systems.
Additionally, as the number of agents in an LLM-MA sys-
tem increases, additional complexities and research opportu-
nities emerge, particularly in areas like efficient agent coor-
dination, communication, and understanding the scaling laws
of multi-agents. For instance, with more LLM-based agents,
the intricacy of ensuring effective coordination and commu-
nication rises significantly. As highlighted in [Dibia, 2023],
designing advanced Agents Orchestration methodologies is
increasingly important. These methodologies aim to opti-
mize agents workflows, task assignments tailored to differ-
ent agents, and communication patterns across agents such as
communication constraints between agents. Effective Agents
Orchestration facilitates harmonious operation among agents,
minimizing conflicts and redundancies. Additionally, explor-
ing and defining the scaling laws that govern the behavior and
efficiency of multi-agent systems as they grow larger remains
an important area of research. These aspects highlight the
need for innovative solutions to optimize LLM-MA systems,
making them both effective and resource-efficient.
6.5 Evaluation and Benchmarks
We have summarized the datasets and benchmarks currently
available for LLM-MA in Table 2. This is a starting point, and
far from being comprehensive. We identify two significant
challenges in evaluating LLM-MA systems and benchmark-
ing their performance against each other. Firstly, as discussed
in [Xu et al., 2023a], much of the existing research focuses
on evaluating individual agents’ understanding and reason-
ing within narrowly defined scenarios. This focus tends to
overlook the broader and more complex emergent behaviors
that are integral to multi-agent systems. Secondly, there is a
notable shortfall in the development of comprehensive bench-
marks across several research domains, such as Science Team
for Experiment Operations, Economic analysis, and Disease
propagation simulation. This gap presents an obstacle to ac-
curately assessing and benchmarking the full capabilities of
LLM-MA systems in these varied and crucial fields.
6.6 Applications and Beyond
The potential of LLM-MA systems extends far beyond their
current applications, holding great promise for advanced
computational problem-solving in fields such as finance, edu-
cation, healthcare, environmental science, urban planning and
so on. As we have discussed, LLM-MA systems possess the
capability to tackle complex problems and simulate various
aspects of the real world. While the current role-playing ca-
pabilities of LLMs may have limitations, ongoing advance-
ments in LLM technology suggest a bright future. It is an-
ticipated to have more sophisticated methodologies, applica-
tions, datasets, and benchmarks tailored for diverse research
fields. Furthermore, there are opportunities to explore LLM-
MA systems from various theoretical perspectives, such as
Cognitive Science [Sumers et al., 2023], Symbolic Artificial
Intelligence, Cybernetics, Complex Systems, and Collective
Intelligence. Such a multi-faceted approach could contribute
to a more comprehensive understanding and innovative appli-
cations in this rapidly evolving field.
7 Conclusion
LLM-based Multi-Agents have shown inspiring collective in-
telligence and rapidly garnered increasing interest among re-
searchers. In this survey, we first systematically review the
development of LLM-MA systems by positioning, differen-
tiating, and connecting them from various aspects, regard-
ing the agents-environment interface, the characterization of
agents by LLMs, the strategies for managing agent communi-
cation and the paradigms for capability acquisition. We also
summarized LLM-MA applications for problem-solving and
world simulation. By also highlighting the commonly used
datasets and benchmarks and discussing challenges and fu-
ture opportunities, we hope that this survey can serve as a use-
ful resource for researchers across various research fields, in-
spiring future research to explore the potential of LLM-based
Multi-Agents.
References
[Agashe et al., 2023] Saaket Agashe, Yue Fan, and Xin Eric
Wang. Evaluating multi-agent coordination abilities in
large language models, 2023.
[Aher et al., 2023] Gati Aher, Rosa I. Arriaga, and
Adam Tauman Kalai. Using large language models
to simulate multiple humans and replicate human subject
studies, 2023.
[Akata et al., 2023] Elif Akata, Lion Schulz, Julian Coda-
Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz.
Playing repeated games with large language models.arXiv
preprint arXiv:2305.16867, 2023.
[Anonymous, 2023] Anonymous. Rethinking the buyer’s in-
spection paradox in information markets with language
agents. In Submitted to The Twelfth International Con-
ference on Learning Representations, 2023. under review.
[Chan et al., 2023] Chi-Min Chan, Weize Chen, Yusheng
Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and
Zhiyuan Liu. Chateval: Towards better llm-based evalua-
tors through multi-agent debate, 2023.



Source: data\tc21_2501.12557v1\referenced_papers\[51]_2402.01680.pdf (Page 11):

[Chen et al., 2023a] Guangyao Chen, Siwei Dong, Yu Shu,
Ge Zhang, Jaward Sesay, B ¨orje F Karlsson, Jie Fu, and
Yemin Shi. Autoagents: A framework for automatic agent
generation. arXiv preprint arXiv:2309.17288, 2023.
[Chen et al., 2023b] Huaben Chen, Wenkang Ji, Lufeng Xu,
and Shiyu Zhao. Multi-agent consensus seeking via large
language models. arXiv preprint arXiv:2310.20151, 2023.
[Chen et al., 2023c] Weize Chen, Yusheng Su, Jingwei Zuo,
Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan,
Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facil-
itating multi-agent collaboration and exploring emergent
behaviors in agents. arXiv preprint arXiv:2308.10848 ,
2023.
[Chen et al., 2023d] Yongchao Chen, Jacob Arkin, Yang
Zhang, Nicholas Roy, and Chuchu Fan. Scalable multi-
robot collaboration with large language models: Cen-
tralized or decentralized systems? arXiv preprint
arXiv:2309.15943, 2023.
[Cobbe et al., 2021] Karl Cobbe, Vineet Kosaraju, Moham-
mad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. Training verifiers to solve math word prob-
lems. arXiv preprint arXiv:2110.14168, 2021.
[Dasgupta et al., 2023] Ishita Dasgupta, Christine Kaeser-
Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan,
Felix Hill, and Rob Fergus. Collaborating with lan-
guage models for embodied reasoning. arXiv preprint
arXiv:2302.00763, 2023.
[Dibia, 2023] Victor Dibia. Multi-agent llm applica-
tions — a review of current research, tools, and
challenges. https://newsletter.victordibia.com/p/
multi-agent-llm-applications-a-review, 2023.
[Dong et al., 2023a] Qingxiu Dong, Lei Li, Damai Dai,
Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing
Xu, Lei Li, and Zhifang Sui. A survey on in-context learn-
ing, 2023.
[Dong et al., 2023b] Yihong Dong, Xue Jiang, Zhi Jin, and
Ge Li. Self-collaboration code generation via chatgpt,
2023.
[Du et al., 2023] Yilun Du, Shuang Li, Antonio Torralba,
Joshua B. Tenenbaum, and Igor Mordatch. Improving fac-
tuality and reasoning in language models through multia-
gent debate, 2023.
[Fan et al., 2023] Caoyun Fan, Jindou Chen, Yaohui Jin, and
Hao He. Can large language models serve as rational play-
ers in game theory? a systematic analysis. arXiv preprint
arXiv:2312.05488, 2023.
[Farmer and Axtell, 2022] J. Doyne Farmer and Robert L.
Axtell. Agent-Based Modeling in Economics and Finance:
Past, Present, and Future. INET Oxford Working Papers
2022-10, Institute for New Economic Thinking at the Ox-
ford Martin School, University of Oxford, June 2022.
[Gao et al., 2023a] Chen Gao, Xiaochong Lan, Zhihong Lu,
Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin,
and Yong Li. S 3: Social-network simulation system with
large language model-empowered agents. arXiv preprint
arXiv:2307.14984, 2023.
[Gao et al., 2023b] Yunfan Gao, Yun Xiong, Xinyu Gao,
Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei
Sun, and Haofen Wang. Retrieval-augmented generation
for large language models: A survey. arXiv preprint
arXiv:2312.10997, 2023.
[Geva et al., 2021] Mor Geva, Daniel Khashabi, Elad Segal,
Tushar Khot, Dan Roth, and Jonathan Berant. Did aris-
totle use a laptop? a question answering benchmark with
implicit reasoning strategies, 2021.
[Ghaffarzadegan et al., 2023] Navid Ghaffarzadegan, Aritra
Majumdar, Ross Williams, and Niyousha Hosseinichimeh.
Generative agent-based modeling: Unveiling social sys-
tem dynamics through coupling mechanistic models
with generative artificial intelligence. arXiv preprint
arXiv:2309.11456, 2023.
[Gong et al., 2023] Ran Gong, Qiuyuan Huang, Xiaojian
Ma, Hoi V o, Zane Durante, Yusuke Noda, Zilong Zheng,
Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, et al.
Mindagent: Emergent gaming interaction. arXiv preprint
arXiv:2309.09971, 2023.
[Guo et al., 2023] Taicheng Guo, Kehan Guo, Zhengwen
Liang, Zhichun Guo, Nitesh V Chawla, Olaf Wiest, Xi-
angliang Zhang, et al. What indeed can gpt models do
in chemistry? a comprehensive benchmark on eight tasks.
arXiv preprint arXiv:2305.18365, 2023.
[Hendrycks et al., 2020] Dan Hendrycks, Collin Burns,
Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song,
and Jacob Steinhardt. Measuring massive multitask lan-
guage understanding. arXiv preprint arXiv:2009.03300 ,
2020.
[Hong et al., 2023] Sirui Hong, Xiawu Zheng, Jonathan
Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven
Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran,
et al. Metagpt: Meta programming for multi-agent col-
laborative framework. arXiv preprint arXiv:2308.00352 ,
2023.
[Horton, 2023] John J Horton. Large language models as
simulated economic agents: What can we learn from homo
silicus? Technical report, National Bureau of Economic
Research, 2023.
[Hua et al., 2023] Wenyue Hua, Lizhou Fan, Lingyao Li,
Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and
Yongfeng Zhang. War and peace (waragent): Large lan-
guage model-based multi-agent simulation of world wars,
2023.
[Huang et al., 2023a] Dong Huang, Qingwen Bu, Jie M.
Zhang, Michael Luck, and Heming Cui. Agentcoder:
Multi-agent-based code generation with iterative testing
and optimisation, 2023.
[Huang et al., 2023b] Lei Huang, Weijiang Yu, Weitao Ma,
Weihong Zhong, Zhangyin Feng, Haotian Wang, Qiang-
long Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al.



Source: data\tc21_2501.12557v1\referenced_papers\[51]_2402.01680.pdf (Page 1):

Figure 1: The rising trend in the research field of LLM-based Multi-Agents. For Problem Solving and World Simulation, we categorize
current work into several categories and count the number of papers of different types at 3-month intervals. The number at each leaf node
denotes the count of papers within that category.
a wider range of interdisciplinary studies employing LLMs.
Readers will gain a comprehensive overview of LLM-based
Multi-Agent (LLM-MA) systems, grasp the fundamental
concepts involved in establishing multi-agent systems based
on LLMs, and catch the latest research trends and applica-
tions in this dynamic field. We recognize that this field is in
its early stages and is rapidly evolving with fresh methodolo-
gies and applications. To provide a sustainable resource com-
plementing our survey paper, we maintain an open-source
GitHub repository1. We hope that our survey will inspire fur-
ther exploration and innovation in this field, as well as appli-
cations across a wide array of research disciplines.
To assist individuals from various backgrounds in under-
standing LLM-MA techniques and to complement existing
surveys by tackling unresolved questions, we have organized
our survey paper in the following manner. After laying out
the background knowledge in Section 2, we address a piv-
otal question: How are LLM-MA systems aligned with the
collaborative task-solving environment? To answer this, we
present a comprehensive schema for positioning, differenti-
ating, and connecting various aspects of LLM-MA systems
in Section 3. We delve into this question by discussing: 1)
the agents-environment interface, which details how agents
interact with the task environment; 2) agent profiling, which
explains how an agent is characterized by an LLM to behave
1https://github.com/taichengguo/LLM MultiAgents Survey Papers
in specific ways; 3) agent communication, which examines
how agents exchange messages and collaborate; and 4)agent
capability acquisition, which explores how agents develop
their abilities to effectively solve problems. An additional
perspective for reviewing studies about LLM-MA is their ap-
plication. In Section 4, we categorize current applications
into two primary streams: multi-agents for problem-solving
and multi-agents for world simulation. To guide individuals
in identifying appropriate tools and resources , we present
open-source implementation frameworks for studying LLM-
MA, as well as the usable datasets and benchmarks in Sec-
tion 5 . Based on the previous summary, we open the dis-
cussion for future research challenges and opportunities in
Section 6. The conclusions are summarized in Section 7.
2 Background
2.1 Single-Agent Systems Powered LLMs
We introduce the background by first outlining the capabili-
ties of a single-agent system based on LLMs, following the
discussion presented in [Weng, 2023].
Decision-making Thought: This term denotes the capabil-
ity of LLM-based agents, guided by prompts, to break down
complex tasks into smaller subgoals[Khot et al., 2023], think
through each part methodically (sometimes exploring mul-
tiple paths) [Yao et al. , 2023 ], and learn from past experi-
ences [Shinn et al., 2023] to perform better decision-making



Source: data\tc21_2501.12557v1\referenced_papers\[51]_2402.01680.pdf (Page 5):

Agents Profiling AgentsCommunication Agents Capabilities Acquisition
MotivationResearch Domain & GoalsWork
Agents-Env.Interface Profilingmethods Profiles(examples)Paradigms Structure Feedback fromAgentsAdjustment
[Qianet al., 2023] Sandbox Pre-defined,Model-GeneratedCTO,programmerCooperative Layered Environment,Agent interaction,Human
Memory,Self-Evolution
Software development[Honget al., 2023] Sandbox Pre-definedProduct Manager,Engineer Cooperative Layered,Shared Message Pool
Environment,Agent interaction,Human
Memory,Self-Evolution
[Donget al., 2023b] Sandbox Pre-defined,Model-GeneratedAnalyst,coder Cooperative Layered Environment,Agent interactionMemory,Self-EvolutionMulti-robotplanning [Chenet al., 2023d] Sandbox,Physical Pre-defined Robots CooperativeCentralized,DecentralizedEnvironment,Agent interactionMemory
EmbodiedAgents Multi-robotcollaboration[Mandiet al., 2023] Sandbox,Physical Pre-defined Robots CooperativeDecentralizedEnvironment,Agent interactionMemory
Multi-Agentscooperation[Zhanget al., 2023c] Sandbox Pre-defined Robots CooperativeDecentralizedEnvironment,Agent interactionMemoryProblemSolving ScienceExperimentsOptimizationof MOF [Zhenget al., 2023] Physical Pre-definedStrategy planers,literaturecollector, coderCooperativeCentralized Environment,Human Memory
ImprovingFactuality [Duet al., 2023] None Pre-defined Agents Debate DecentralizedAgent interactionMemory
ScienceDebate Examining,Inter-Consistency[Xionget al., 2023] None Pre-defined Proponent,Opponent,Judge Debate Centralized,DecentralizedAgent interactionMemory
Evaluatorsfor debates [Chanet al., 2023] None Pre-defined Agents Debate Centralized,DecentralizedAgent interactionMemory
Multi-Agentsfor Medication[Tanget al., 2023] None Pre-defined Cardiology,Surgery Debate,CooperativeCentralized,DecentralizedAgent interactionMemory
Modest Community(25 persons)[Parket al., 2023] SandboxModel-generatedPharmacy,shopkeeper - - Environment,Agent interactionMemory
Online community(1000 persons)[Parket al., 2022] None Pre-defined,Model-generatedCamping,fishing - - Agent interactionDynamicGeneration
Society Emotion propagation[Gaoet al., 2023a] None Pre-defined,Model-generatedReal-worlduser - - Agent interactionMemory
Real-timesocial interactions[Kaiyaet al., 2023] Sandbox Pre-defined Real-worlduser - - Environment,Agent interactionMemory
Opinion dynamics[Liet al., 2023a] None Pre-defined NIN, NINL,NIL - - Agent interactionMemory
WereWolf [Xuet al., 2023b]
[Xuet al., 2023c] Sandbox Pre-defined Seer,werewolf,villager
Cooperative,Debate,CompetitiveDecentralizedEnvironment,Agent interactionMemory
Gaming Avalon [Lightet al., 2023a]
[Wanget al., 2023c] Sandbox Pre-defined Servant,Merlin,Assassin
Cooperative,Debate,CompetitiveDecentralizedEnvironment,Agent interactionMemory
Welfare Diplomacy[Mukobiet al., 2023] Sandbox Pre-defined CountriesCooperative,CompetitiveDecentralizedEnvironment,Agent interactionMemory
Human behaviorSimulation[Aheret al., 2023] Sandbox Pre-defined Humans - - Agent interactionMemory
WorldSimulation
PsychologyCollaborationExploring [Zhanget al., 2023d] None Pre-defined Agents Cooperative,Debate DecentralizedAgent interactionMemory
Macroeconomicsimulation [Liet al., 2023e] None Pre-defined,Model-generatedLabor CooperativeDecentralizedAgent interactionMemory
Economy InformationMarketplaces[Anonymous, 2023] Sandbox Pre-defined,Data-Derived Buyer Cooperative,CompetitiveDecentralizedEnvironment,Agent interactionMemory
Improvingfinancial trading[Liet al., 2023g] Physical Pre-defined Trader Debate DecentralizedEnvironment,Agent interactionMemory
Economic theories[Zhaoet al., 2023] Sandbox Pre-defined,Model-GeneratedRestaurant,Customer CompetitiveDecentralizedEnvironment,Agent interactionMemory,Self-Evolution
RecommenderSystems
Simulatinguser behaviors[Zhanget al., 2023a] Sandbox Data-DerivedUsers fromMovieLens-1M- - EnvironmentMemory
Simulating user-iteminteractions[Zhanget al., 2023e] Sandbox Pre-defined,Data-DerivedUser AgentsItem AgentsCooperativeDecentralizedEnvironment,Agent interactionMemory
PolicyMaking
PublicAdministration[Xiaoet al., 2023] None Pre-defined Residents CooperativeDecentralizedAgent interactionMemory
War Simulation[Huaet al., 2023] None Pre-defined Countries CompetitiveDecentralizedAgent interactionMemory
Disease Human Behaviorsto epidemics[Ghaffarzadeganet al., 2023] Sandbox Pre-defined,Model-GeneratedConformitytraits CooperativeDecentralizedEnvironment,Agent interactionMemory
Public health [Williamset al., 2023] Sandbox Pre-defined,Model-GeneratedAdults aged18 to 64 CooperativeDecentralizedEnvironment,Agent interaction
Memory,DynamicGeneration
Table 1: Summary of the LLM-MA studies. We categorize current work according to their motivation, research domains and goals, and detail
each work from different aspects regarding Agents-Environment Interface, Agents Profiling, Agents Communication and Agents Capability
Acquisition. “-” denotes that a particular element is not specifically mentioned in this work.
Standardized Operating Procedures (SOPs) workflow of the
software development, the communication structure among
agents is usually layered. Agents generally interact with the
code interpreter, other agents or human to iteratively refine
the generated code. [Li et al., 2023b] first proposes a sim-
ple role-play agent framework, which utilizes the interplay
of two roles to realize autonomous programming based on
one-sentence user instruction. It provides insights into the
“cognitive” processes of communicative agents. [Dong et al.,
2023b] makes LLMs work as distinct “experts” for sub-tasks
in software development, autonomously collaborating to gen-
erate code. Moreover, [Qian et al., 2023] presents an end-to-
end framework for software development, utilizing multiple
agents for software development without incorporating ad-
vanced human teamwork experience. [Hong et al., 2023] first
incorporates human workflow insights for more controlled
and validated performance. It encodes SOPs into prompts to
enhance structured coordination. [Huang et al., 2023a] delves
deeper into multi-agent based programming by solving the
problem of balancing code snippet generation with effective



#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[99]_2306.03100.pdf (Page 9):

Rethinking Model Evaluation as Narrowing the Socio-Technical Gap
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,
Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,
et al. Training language models to follow instructions
with human feedback. Advances in Neural Information
Processing Systems, 35:27730–27744, 2022.
Pirolli, P. Cognitive models of human-information inter-
action. Handbook of applied cognition , pp. 443–470,
2007.
Raji, I. D., Bender, E. M., Paullada, A., Denton, E., and
Hanna, A. Ai and the everything in the whole wide world
benchmark. arXiv preprint arXiv:2111.15366, 2021.
Sai, A. B., Mohankumar, A. K., and Khapra, M. M. A
survey of evaluation metrics used for nlg systems. ACM
Computing Surveys (CSUR), 55(2):1–39, 2022.
Schmuckler, M. A. What is ecological validity? a dimen-
sional analysis. Infancy, 2(4):419–436, 2001.
Selbst, A. D., Boyd, D., Friedler, S. A., Venkatasubrama-
nian, S., and Vertesi, J. Fairness and abstraction in so-
ciotechnical systems. In Proceedings of the conference
on fairness, accountability, and transparency, pp. 59–68,
2019.
Shelby, R., Rismani, S., Henne, K., Moon, A., Rostamzadeh,
N., Nicholas, P., Yilla, N., Gallegos, J., Smart, A., Garcia,
E., et al. Sociotechnical harms: Scoping a taxonomy for
harm reduction. arXiv preprint arXiv:2210.05791, 2022.
Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,
A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,
Garriga-Alonso, A., et al. Beyond the imitation game:
Quantifying and extrapolating the capabilities of language
models. arXiv preprint arXiv:2206.04615, 2022.
Suresh, H., Gomez, S. R., Nam, K. K., and Satyanarayan, A.
Beyond expertise and roles: A framework to characterize
the stakeholders of interpretable machine learning and
their needs. In Proceedings of the 2021 CHI Conference
on Human Factors in Computing Systems, pp. 1–16, 2021.
Vaughan, J. W. and Wallach, H. A human-centered agenda
for intelligible machine learning. Machines We Trust:
Getting Along with Artificial Intelligence, 2020.
Wei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B.,
Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-
zler, D., et al. Emergent abilities of large language models.
Transactions on Machine Learning Research.
Weidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang,
P.-S., Mellor, J., Glaese, A., Cheng, M., Balle, B.,
Kasirzadeh, A., et al. Taxonomy of risks posed by lan-
guage models. In 2022 ACM Conference on Fairness,
Accountability, and Transparency, pp. 214–229, 2022.
Zhang, S. and Balog, K. Evaluating conversational recom-
mender systems via user simulation. In Proceedings of
the 26th acm sigkdd international conference on knowl-
edge discovery & data mining, pp. 1512–1520, 2020.
Zhang, T., Kishore, V ., Wu, F., Weinberger, K. Q., and Artzi,
Y . Bertscore: Evaluating text generation with bert. arXiv
preprint arXiv:1904.09675, 2019.
Zheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z.,
Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging
llm-as-a-judge with mt-bench and chatbot arena. arXiv
preprint arXiv:2306.05685, 2023.
Zhou, K., Blodgett, S. L., Trischler, A., Daum´e III, H., Sule-
man, K., and Olteanu, A. Deconstructing nlg evaluation:
Evaluation practices, assumptions, and their implications.
In Proceedings of the 2022 Conference of the North Amer-
ican Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pp. 314–324,
2022.



Source: data\tc21_2501.12557v1\referenced_papers\[183]_2303.18223.pdf (Page 105):

106
for cloning chatgpt with a complete
rlhf pipeline,” 2023. [Online]. Available:
https://medium.com/@yangyou berkeley/
colossalchat-an-open-source-solution-for-cloning-
chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b
[206] “Bmtrain: Effient training for big models.” [Online].
Available: https://github.com/OpenBMB/BMTrain
[207] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, and J. Tang,
“Fastmoe: A fast mixture-of-expert training system,”
CoRR, vol. abs/2103.13262, 2021.
[208] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng,
C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica,
“Efficient memory management for large language
model serving with pagedattention,” in Proceedings
of the ACM SIGOPS 29th Symposium on Operating
Systems Principles, 2023.
[209] (2023) Deepspeed-mii. [Online]. Available: https:
//github.com/microsoft/DeepSpeed-MII
[210] Z. Yao, R. Y. Aminabadi, O. Ruwase, S. Rajbhan-
dari, X. Wu, A. A. Awan, J. Rasley, M. Zhang,
C. Li, C. Holmes, Z. Zhou, M. Wyatt, M. Smith,
L. Kurilenko, H. Qin, M. Tanaka, S. Che, S. L. Song,
and Y. He, “DeepSpeed-Chat: Easy, Fast and Afford-
able RLHF Training of ChatGPT-like Models at All
Scales,” arXiv preprint arXiv:2308.01320, 2023.
[211] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Brad-
bury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, A. Desmaison, A. K ¨opf, E. Z. Yang,
Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,
B. Steiner, L. Fang, J. Bai, and S. Chintala, “Py-
torch: An imperative style, high-performance deep
learning library,” in Advances in Neural Information
Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada , H. M.
Wallach, H. Larochelle, A. Beygelzimer, F. d’Alch ´e-
Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 8024–
8035.
[212] M. Abadi, P . Barham, J. Chen, Z. Chen, A. Davis,
J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Is-
ard, M. Kudlur, J. Levenberg, R. Monga, S. Moore,
D. G. Murray, B. Steiner, P . A. Tucker, V . Vasudevan,
P . Warden, M. Wicke, Y. Yu, and X. Zheng, “Tensor-
flow: A system for large-scale machine learning,” in
12th USENIX Symposium on Operating Systems Design
and Implementation, OSDI 2016, Savannah, GA, USA,
November 2-4, 2016 , K. Keeton and T. Roscoe, Eds.
USENIX Association, 2016, pp. 265–283.
[213] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang,
T. Xiao, B. Xu, C. Zhang, and Z. Zhang, “Mxnet:
A flexible and efficient machine learning library
for heterogeneous distributed systems,” CoRR, vol.
abs/1512.01274, 2015.
[214] Y. Ma, D. Yu, T. Wu, and H. Wang, “Paddlepaddle:
An open-source deep learning platform from indus-
trial practice,” Frontiers of Data and Domputing, vol. 1,
no. 1, p. 105, 2019.
[215] L. Huawei Technologies Co., “Huawei mindspore
ai development framework,” in Artificial Intelligence
Technology. Springer, 2022, pp. 137–162.
[216] J. Yuan, X. Li, C. Cheng, J. Liu, R. Guo, S. Cai, C. Yao,
F. Yang, X. Yi, C. Wu, H. Zhang, and J. Zhao, “One-
flow: Redesign the distributed deep learning frame-
work from scratch,” CoRR, vol. abs/2110.15032, 2021.
[217] S. Roller, E. Dinan, N. Goyal, D. Ju, M. Williamson,
Y. Liu, J. Xu, M. Ott, E. M. Smith, Y. Boureau, and
J. Weston, “Recipes for building an open-domain
chatbot,” in Proceedings of the 16th Conference of the
European Chapter of the Association for Computational
Linguistics: Main Volume, EACL 2021, Online, April 19
- 23, 2021, 2021, pp. 300–325.
[218] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer,
H. Michalewski, V . V . Ramasesh, A. Slone, C. Anil,
I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur,
G. Gur-Ari, and V . Misra, “Solving quantitative rea-
soning problems with language models,” CoRR, vol.
abs/2206.14858, 2022.
[219] T. Saier, J. Krause, and M. F ¨arber, “unarxive 2022:
All arxiv publications pre-processed for nlp, includ-
ing structured full-text and citation network,” arXiv
preprint arXiv:2303.14957, 2023.
[220] H. A. Simon, “Experiments with a heuristic com-
piler,” J. ACM, vol. 10, no. 4, pp. 493–506, 1963.
[221] Z. Manna and R. J. Waldinger, “Toward automatic
program synthesis,” Commun. ACM , vol. 14, no. 3,
pp. 151–165, 1971.
[222] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong,
L. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou,
“Codebert: A pre-trained model for programming
and natural languages,” in Findings of EMNLP, 2020.
[223] J. Austin, A. Odena, M. I. Nye, M. Bosma,
H. Michalewski, D. Dohan, E. Jiang, C. J. Cai,
M. Terry, Q. V . Le, and C. Sutton, “Program syn-
thesis with large language models,” CoRR, vol.
abs/2108.07732, 2021.
[224] S. Black, L. Gao, P . Wang, C. Leahy, and S. Bider-
man, “GPT-Neo: Large Scale Autoregressive Lan-
guage Modeling with Mesh-Tensorflow,” 2021.
[225] F. F. Xu, U. Alon, G. Neubig, and V . J. Hellendoorn,
“A systematic evaluation of large language models
of code,” in MAPS@PLDI, 2022.
[226] A. Madaan, S. Zhou, U. Alon, Y. Yang, and G. Neu-
big, “Language models of code are few-shot com-
monsense learners,” in Proceedings of the 2022 Confer-
ence on Empirical Methods in Natural Language Process-
ing, EMNLP 2022, Abu Dhabi, United Arab Emirates,
December 7-11, 2022 , Y. Goldberg, Z. Kozareva, and
Y. Zhang, Eds. Association for Computational Lin-
guistics, 2022, pp. 1384–1403.
[227] S. Longpre, G. Yauney, E. Reif, K. Lee, A. Roberts,
B. Zoph, D. Zhou, J. Wei, K. Robinson, D. Mimno
et al., “A pretrainer’s guide to training data: Measur-
ing the effects of data age, domain coverage, quality,
& toxicity,” arXiv preprint arXiv:2305.13169, 2023.
[228] D. Chen, Y. Huang, Z. Ma, H. Chen, X. Pan, C. Ge,
D. Gao, Y. Xie, Z. Liu, J. Gao, Y. Li, B. Ding, and
J. Zhou, “Data-juicer: A one-stop data processing
system for large language models,” 2023.
[229] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja,
A. Awadallah, H. Awadalla, N. Bach, A. Bahree,
A. Bakhtiari, H. Behl et al. , “Phi-3 technical report:
A highly capable language model locally on your



Source: data\tc21_2501.12557v1\referenced_papers\[183]_2303.18223.pdf (Page 111):

112
medical knowledge,” arXiv preprint arXiv:2304.06975,
2023.
[357] Q. Huang, M. Tao, Z. An, C. Zhang, C. Jiang, Z. Chen,
Z. Wu, and Y. Feng, “Lawyer llama technical report,”
arXiv preprint arXiv:2305.15062, 2023.
[358] S. Wu, O. Irsoy, S. Lu, V . Dabravolski, M. Dredze,
S. Gehrmann, P . Kambadur, D. Rosenberg, and
G. Mann, “Bloomberggpt: A large language model
for finance,” arXiv preprint arXiv:2303.17564, 2023.
[359] T. Liu and B. K. H. Low, “Goat: Fine-tuned llama out-
performs gpt-4 on arithmetic tasks,” arXiv preprint
arXiv:2305.14201, 2023.
[360] T. Sun, X. Zhang, Z. He, P . Li, Q. Cheng, H. Yan,
X. Liu, Y. Shao, Q. Tang, X. Zhao, K. Chen, Y. Zheng,
Z. Zhou, R. Li, J. Zhan, Y. Zhou, L. Li, X. Yang, L. Wu,
Z. Yin, X. Huang, and X. Qiu, “Moss: Training con-
versational language models from synthetic data,”
2023.
[361] Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani,
J. Ba, C. Guestrin, P . Liang, and T. B. Hashimoto,
“Alpacafarm: A simulation framework for methods
that learn from human feedback,” CoRR, vol.
abs/2305.14387, 2023. [Online]. Available: https:
//doi.org/10.48550/arXiv.2305.14387
[362] D. Hendrycks, C. Burns, S. Basart, A. Zou,
M. Mazeika, D. Song, and J. Steinhardt, “Measur-
ing massive multitask language understanding,” in
ICLR. OpenReview.net, 2021.
[363] M. Suzgun, N. Scales, N. Sch ¨arli, S. Gehrmann,
Y. Tay, H. W. Chung, A. Chowdhery, Q. V . Le, E. H.
Chi, D. Zhou, and J. Wei, “Challenging big-bench
tasks and whether chain-of-thought can solve them,”
CoRR, vol. abs/2210.09261, 2022.
[364] Z. Kenton, T. Everitt, L. Weidinger, I. Gabriel,
V . Mikulik, and G. Irving, “Alignment of language
agents,” CoRR, vol. abs/2103.14659, 2021.
[365] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown,
A. Radford, D. Amodei, P . F. Christiano, and G. Irv-
ing, “Fine-tuning language models from human pref-
erences,” CoRR, vol. abs/1909.08593, 2019.
[366] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli,
T. Henighan, A. Jones, N. Joseph, B. Mann, N. Das-
Sarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez,
J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. B.
Brown, J. Clark, S. McCandlish, C. Olah, and J. Ka-
plan, “A general language assistant as a laboratory
for alignment,” CoRR, vol. abs/2112.00861, 2021.
[367] E. Perez, S. Huang, H. F. Song, T. Cai, R. Ring,
J. Aslanides, A. Glaese, N. McAleese, and G. Irving,
“Red teaming language models with language mod-
els,” in Proceedings of the 2022 Conference on Empir-
ical Methods in Natural Language Processing, EMNLP
2022, Abu Dhabi, United Arab Emirates, December 7-11,
2022, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds.
Association for Computational Linguistics, 2022, pp.
3419–3448.
[368] J. Menick, M. Trebacz, V . Mikulik, J. Aslanides,
H. F. Song, M. Chadwick, M. Glaese, S. Young,
L. Campbell-Gillingham, G. Irving, and
N. McAleese, “Teaching language models to
support answers with verified quotes,” CoRR, vol.
abs/2203.11147, 2022.
[369] Y. Bai, S. Kadavath, S. Kundu, A. Askell,
J. Kernion, A. Jones, A. Chen, A. Goldie,
A. Mirhoseini, C. McKinnon, C. Chen, C. Olsson,
C. Olah, D. Hernandez, D. Drain, D. Ganguli,
D. Li, E. Tran-Johnson, E. Perez, J. Kerr,
J. Mueller, J. Ladish, J. Landau, K. Ndousse,
K. Lukosiute, L. Lovitt, M. Sellitto, N. Elhage,
N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby,
R. Larson, S. Ringer, S. Johnston, S. Kravec,
S. E. Showk, S. Fort, T. Lanham, T. Telleen-
Lawton, T. Conerly, T. Henighan, T. Hume, S. R.
Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei,
N. Joseph, S. McCandlish, T. Brown, and J. Kaplan,
“Constitutional AI: harmlessness from AI feedback,”
CoRR, vol. abs/2212.08073, 2022. [Online]. Available:
https://doi.org/10.48550/arXiv.2212.08073
[370] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard,
C. Bishop, V . Carbune, and A. Rastogi, “RLAIF:
scaling reinforcement learning from human feedback
with AI feedback,” CoRR, vol. abs/2309.00267, 2023.
[371] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao,
J. Zhang, K. Shum, and T. Zhang, “RAFT:
reward ranked finetuning for generative foundation
model alignment,” CoRR, vol. abs/2304.06767, 2023.
[Online]. Available: https://doi.org/10.48550/arXiv.
2304.06767
[372] A. Askell, Y. Bai, A. Chen, D. Drain, D. Gan-
guli, T. Henighan, A. Jones, N. Joseph, B. Mann,
N. DasSarma et al. , “A general language assis-
tant as a laboratory for alignment,” arXiv preprint
arXiv:2112.00861, 2021.
[373] R. Zheng, S. Dou, S. Gao, W. Shen, B. Wang, Y. Liu,
S. Jin, Q. Liu, L. Xiong, L. Chen et al., “Secrets of rlhf
in large language models part i: Ppo,” arXiv preprint
arXiv:2307.04964, 2023.
[374] J. Uesato, N. Kushman, R. Kumar, H. F. Song,
N. Y. Siegel, L. Wang, A. Creswell, G. Irving, and
I. Higgins, “Solving math word problems with
process- and outcome-based feedback,” CoRR, vol.
abs/2211.14275, 2022.
[375] H. Lightman, V . Kosaraju, Y. Burda, H. Edwards,
B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever,
and K. Cobbe, “Let’s verify step by step,” CoRR, vol.
abs/2305.20050, 2023.
[376] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika,
A. Arora, E. Guo, C. Burns, S. Puranik, H. He,
D. Song, and J. Steinhardt, “Measuring coding chal-
lenge competence with APPS,” in NeurIPS Datasets
and Benchmarks, 2021.
[377] T. Wang, P . Yu, X. E. Tan, S. O’Brien, R. Pa-
sunuru, J. Dwivedi-Yu, O. Golovneva, L. Zettle-
moyer, M. Fazel-Zarandi, and A. Celikyilmaz, “Shep-
herd: A critic for language model generation,” CoRR,
vol. abs/2308.04592, 2023.
[378] G. Chen, M. Liao, C. Li, and K. Fan, “Alphamath
almost zero: process supervision without process,”
CoRR, vol. abs/2405.03553, 2024.
[379] Q. Ma, H. Zhou, T. Liu, J. Yuan, P . Liu, Y. You, and
H. Yang, “Let’s reward step by step: Step-level re-
ward model as the navigators for reasoning,” CoRR,



Source: data\tc21_2501.12557v1\referenced_papers\[183]_2303.18223.pdf (Page 98):

99
D. Hassabis, K. Kavukcuoglu, and G. Irving, “Scaling
language models: Methods, analysis & insights from
training gopher,” CoRR, vol. abs/2112.11446, 2021.
[65] D. Dai, Y. Sun, L. Dong, Y. Hao, Z. Sui, and F. Wei,
“Why can GPT learn in-context? language models se-
cretly perform gradient descent as meta-optimizers,”
CoRR, vol. abs/2212.10559, 2022.
[66] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wain-
wright, P . Mishkin, C. Zhang, S. Agarwal, K. Slama,
A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller,
M. Simens, A. Askell, P . Welinder, P . F. Christiano,
J. Leike, and R. Lowe, “Training language models
to follow instructions with human feedback,” CoRR,
vol. abs/2203.02155, 2022.
[67] J. Wei, M. Bosma, V . Y. Zhao, K. Guu, A. W. Yu,
B. Lester, N. Du, A. M. Dai, and Q. V . Le, “Finetuned
language models are zero-shot learners,” inThe Tenth
International Conference on Learning Representations,
ICLR 2022, Virtual Event, April 25-29, 2022 . Open-
Review.net, 2022.
[68] R. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer,
A. Kulshreshtha, H. Cheng, A. Jin, T. Bos, L. Baker,
Y. Du, Y. Li, H. Lee, H. S. Zheng, A. Ghafouri,
M. Menegali, Y. Huang, M. Krikun, D. Lepikhin,
J. Qin, D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma,
Y. Zhou, C. Chang, I. Krivokon, W. Rusch, M. Pick-
ett, K. S. Meier-Hellstern, M. R. Morris, T. Doshi,
R. D. Santos, T. Duke, J. Soraker, B. Zevenbergen,
V . Prabhakaran, M. Diaz, B. Hutchinson, K. Olson,
A. Molina, E. Hoffman-John, J. Lee, L. Aroyo, R. Ra-
jakumar, A. Butryna, M. Lamm, V . Kuzmina, J. Fen-
ton, A. Cohen, R. Bernstein, R. Kurzweil, B. Aguera-
Arcas, C. Cui, M. Croak, E. H. Chi, and Q. Le,
“Lamda: Language models for dialog applications,”
CoRR, vol. abs/2201.08239, 2022.
[69] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay,
W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma,
A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen,
A. Chowdhery, S. Narang, G. Mishra, A. Yu, V . Y.
Zhao, Y. Huang, A. M. Dai, H. Yu, S. Petrov, E. H.
Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V . Le,
and J. Wei, “Scaling instruction-finetuned language
models,” CoRR, vol. abs/2210.11416, 2022.
[70] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb,
A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta,
A. Garriga-Alonso, A. Kluska, A. Lewkowycz,
A. Agarwal, A. Power, A. Ray, A. Warstadt, A. W.
Kocurek, A. Safaya, A. Tazarv, A. Xiang, A. Parrish,
A. Nie, A. Hussain, A. Askell, A. Dsouza, A. Rahane,
A. S. Iyer, A. Andreassen, A. Santilli, A. Stuhlm ¨uller,
A. M. Dai, A. La, A. K. Lampinen, A. Zou, A. Jiang,
A. Chen, A. Vuong, A. Gupta, A. Gottardi, A. Norelli,
A. Venkatesh, A. Gholamidavoodi, A. Tabassum,
A. Menezes, A. Kirubarajan, A. Mullokandov, A. Sab-
harwal, A. Herrick, A. Efrat, A. Erdem, A. Karakas,
and et al., “Beyond the imitation game: Quantifying
and extrapolating the capabilities of language mod-
els,” CoRR, vol. abs/2206.04615, 2022.
[71] R. Schaeffer, B. Miranda, and S. Koyejo, “Are emer-
gent abilities of large language models a mirage?”
arXiv preprint arXiv:2304.15004, 2023.
[72] S. Hu, X. Liu, X. Han, X. Zhang, C. He, W. Zhao,
Y. Lin, N. Ding, Z. Ou, G. Zeng, Z. Liu, and M. Sun,
“Unlock predictable scaling from emergent abilities,”
2023.
[73] A. Power, Y. Burda, H. Edwards, I. Babuschkin, and
V . Misra, “Grokking: Generalization beyond overfit-
ting on small algorithmic datasets,” arXiv preprint
arXiv:2201.02177, 2022.
[74] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He,
“Deepspeed: System optimizations enable training
deep learning models with over 100 billion param-
eters,” in KDD, 2020, pp. 3505–3506.
[75] M. Shoeybi, M. Patwary, R. Puri, P . LeGresley,
J. Casper, and B. Catanzaro, “Megatron-lm: Train-
ing multi-billion parameter language models using
model parallelism,” CoRR, vol. abs/1909.08053, 2019.
[76] D. Narayanan, M. Shoeybi, J. Casper, P . LeGres-
ley, M. Patwary, V . Korthikanti, D. Vainbrand,
P . Kashinkunti, J. Bernauer, B. Catanzaro, A. Phan-
ishayee, and M. Zaharia, “Efficient large-scale lan-
guage model training on GPU clusters using
megatron-lm,” in International Conference for High Per-
formance Computing, Networking, Storage and Analysis,
SC 2021, St. Louis, Missouri, USA, November 14-19,
2021. ACM, 2021, p. 58.
[77] V . Korthikanti, J. Casper, S. Lym, L. McAfee, M. An-
dersch, M. Shoeybi, and B. Catanzaro, “Reducing ac-
tivation recomputation in large transformer models,”
CoRR, vol. abs/2205.05198, 2022.
[78] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic,
D. Hesslow, R. Castagn ´e, A. S. Luccioni, F. Yvon,
M. Gall ´e, J. Tow, A. M. Rush, S. Biderman, A. Web-
son, P . S. Ammanamanchi, T. Wang, B. Sagot,
N. Muennighoff, A. V . del Moral, O. Ruwase, R. Baw-
den, S. Bekman, A. McMillan-Major, I. Beltagy,
H. Nguyen, L. Saulnier, S. Tan, P . O. Suarez, V . Sanh,
H. Laurenc ¸on, Y. Jernite, J. Launay, M. Mitchell,
C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji,
A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou,
C. Emezue, C. Klamm, C. Leong, D. van Strien,
D. I. Adelani, and et al., “BLOOM: A 176b-parameter
open-access multilingual language model,” CoRR,
vol. abs/2211.05100, 2022.
[79] P . F. Christiano, J. Leike, T. B. Brown, M. Martic,
S. Legg, and D. Amodei, “Deep reinforcement learn-
ing from human preferences,” in Advances in Neural
Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, Decem-
ber 4-9, 2017, Long Beach, CA, USA , I. Guyon, U. von
Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V . N.
Vishwanathan, and R. Garnett, Eds., 2017, pp. 4299–
4307.
[80] T. Schick, J. Dwivedi-Yu, R. Dess `ı, R. Raileanu,
M. Lomeli, L. Zettlemoyer, N. Cancedda, and
T. Scialom, “Toolformer: Language models can teach
themselves to use tools,” CoRR, vol. abs/2302.04761,
2023.
[81] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang,
C. Kim, C. Hesse, S. Jain, V . Kosaraju, W. Saun-
ders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger,
K. Button, M. Knight, B. Chess, and J. Schulman,



Source: data\tc21_2501.12557v1\referenced_papers\[183]_2303.18223.pdf (Page 102):

103
A. H ´eliou, A. Tacchetti, A. Bulanova, A. Paterson,
B. Tsai, B. Shahriari, C. L. Lan, C. A. Choquette-Choo,
C. Crepy, D. Cer, D. Ippolito, D. Reid, E. Buchatskaya,
E. Ni, E. Noland, G. Yan, G. Tucker, G. Muraru,
G. Rozhdestvenskiy, H. Michalewski, I. Tenney, I. Gr-
ishchenko, J. Austin, J. Keeling, J. Labanowski,
J. Lespiau, J. Stanway, J. Brennan, J. Chen, J. Ferret,
J. Chiu, and et al., “Gemma: Open models based
on gemini research and technology,” CoRR, vol.
abs/2403.08295, 2024.
[140] M. Rivi `ere, S. Pathak, P . G. Sessa, C. Hardin, S. Bhu-
patiraju, L. Hussenot, T. Mesnard, B. Shahriari,
A. Ram´e, J. Ferret, P . Liu, P . Tafti, A. Friesen, M. Cas-
bon, S. Ramos, R. Kumar, C. L. Lan, S. Jerome, A. Tsit-
sulin, N. Vieillard, P . Stanczyk, S. Girgin, N. Mom-
chev, M. Hoffman, S. Thakoor, J. Grill, B. Neyshabur,
O. Bachem, A. Walton, A. Severyn, A. Parrish, A. Ah-
mad, A. Hutchison, A. Abdagic, A. Carl, A. Shen,
A. Brock, A. Coenen, A. Laforge, A. Paterson, B. Bas-
tian, B. Piot, B. Wu, B. Royal, C. Chen, C. Kumar,
C. Perry, C. Welty, C. A. Choquette-Choo, D. Sinopal-
nikov, D. Weinberger, D. Vijaykumar, D. Rogozin-
ska, D. Herbison, E. Bandy, E. Wang, E. Noland,
E. Moreira, E. Senter, E. Eltyshev, F. Visin, G. Rasskin,
G. Wei, G. Cameron, G. Martins, H. Hashemi,
H. Klimczak-Plucinska, H. Batra, H. Dhand, I. Nar-
dini, J. Mein, J. Zhou, J. Svensson, J. Stanway, J. Chan,
J. P . Zhou, J. Carrasqueira, J. Iljazi, J. Becker, J. Fer-
nandez, J. van Amersfoort, J. Gordon, J. Lipschultz,
J. Newlan, J. Ji, K. Mohamed, K. Badola, K. Black,
K. Millican, K. McDonell, K. Nguyen, K. Sodhia,
K. Greene, L. L. Sj ¨osund, L. Usui, L. Sifre, L. Heuer-
mann, L. Lago, and L. McNealus, “Gemma 2: Im-
proving open language models at a practical size,”
CoRR, vol. abs/2408.00118, 2024.
[141] A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou,
C. Li, C. Li, D. Liu, F. Huang, G. Dong, H. Wei, H. Lin,
J. Tang, J. Wang, J. Yang, J. Tu, J. Zhang, J. Ma, J. Xu,
J. Zhou, J. Bai, J. He, J. Lin, K. Dang, K. Lu, K. Chen,
K. Yang, M. Li, M. Xue, N. Ni, P . Zhang, P . Wang,
R. Peng, R. Men, R. Gao, R. Lin, S. Wang, S. Bai,
S. Tan, T. Zhu, T. Li, T. Liu, W. Ge, X. Deng, X. Zhou,
X. Ren, X. Zhang, X. Wei, X. Ren, Y. Fan, Y. Yao,
Y. Zhang, Y. Wan, Y. Chu, Y. Liu, Z. Cui, Z. Zhang,
and Z. Fan, “Qwen2 technical report,” arXiv preprint
arXiv:2407.10671, 2024.
[142] Q. Team, “Qwen2.5: A party of foundation
models,” September 2024. [Online]. Available:
https://qwenlm.github.io/blog/qwen2.5/
[143] T. GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin,
D. Rojas, G. Feng, H. Zhao, H. Lai, H. Yu, H. Wang,
J. Sun, J. Zhang, J. Cheng, J. Gui, J. Tang, J. Zhang,
J. Li, L. Zhao, L. Wu, L. Zhong, M. Liu, M. Huang,
P . Zhang, Q. Zheng, R. Lu, S. Duan, S. Zhang, S. Cao,
S. Yang, W. L. Tam, W. Zhao, X. Liu, X. Xia, X. Zhang,
X. Gu, X. Lv, X. Liu, X. Liu, X. Yang, X. Song,
X. Zhang, Y. An, Y. Xu, Y. Niu, Y. Yang, Y. Li, Y. Bai,
Y. Dong, Z. Qi, Z. Wang, Z. Yang, Z. Du, Z. Hou,
and Z. Wang, “Chatglm: A family of large language
models from glm-130b to glm-4 all tools,” 2024.
[144] H. Zhong, C. Xiao, C. Tu, T. Zhang, Z. Liu, and
M. Sun, “JEC-QA: A legal-domain question answer-
ing dataset,” in The Thirty-Fourth AAAI Conference
on Artificial Intelligence, AAAI 2020, The Thirty-Second
Innovative Applications of Artificial Intelligence Confer-
ence, IAAI 2020, The Tenth AAAI Symposium on Edu-
cational Advances in Artificial Intelligence, EAAI 2020,
New York, NY, USA, February 7-12, 2020. AAAI Press,
2020, pp. 9701–9708.
[145] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang,
and P . Szolovits, “What disease does this patient
have? a large-scale open domain question answer-
ing dataset from medical exams,” Applied Sciences ,
vol. 11, no. 14, p. 6421, 2021.
[146] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li,
C. Guestrin, P . Liang, and T. B. Hashimoto, “Stanford
alpaca: An instruction-following llama model,”
https://github.com/tatsu-lab/stanford alpaca,
2023.
[147] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith,
D. Khashabi, and H. Hajishirzi, “Self-instruct: Align-
ing language model with self generated instruc-
tions,” CoRR, vol. abs/2212.10560, 2022.
[148] Alpaca-LoRA, “Instruct-tune llama on consumer
hardware,” https://github.com/tloen/alpaca-lora,
2023.
[149] E. J. Hu, Y. Shen, P . Wallis, Z. Allen-Zhu, Y. Li,
S. Wang, L. Wang, and W. Chen, “Lora: Low-rank
adaptation of large language models,” in The Tenth
International Conference on Learning Representations,
ICLR 2022, Virtual Event, April 25-29, 2022 . Open-
Review.net, 2022.
[150] X. Geng, A. Gudibande, H. Liu, E. Wallace, P . Abbeel,
S. Levine, and D. Song, “Koala: A dialogue model for
academic research,” Blog post, April 2023.
[151] Y. Ji, Y. Deng, Y. Gong, Y. Peng, Q. Niu, B. Ma,
and X. Li, “Belle: Be everyone’s large language
model engine,” https://github.com/LianjiaTech/
BELLE, 2023.
[152] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu,
H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E.
Gonzalez, I. Stoica, and E. P . Xing, “Vicuna:
An open-source chatbot impressing gpt-4 with
90%* chatgpt quality,” 2023. [Online]. Available:
https://vicuna.lmsys.org
[153] D. Eccleston, “Sharegpt,” https://sharegpt.com/,
2023.
[154] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction
tuning,” CoRR, vol. abs/2304.08485, 2023.
[155] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny,
“Minigpt-4: Enhancing vision-language understand-
ing with advanced large language models,” CoRR,
vol. abs/2304.10592, 2023.
[156] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang,
B. Li, P . Fung, and S. C. H. Hoi, “Instructblip: To-
wards general-purpose vision-language models with
instruction tuning,” CoRR, vol. abs/2305.06500, 2023.
[157] Y. Su, T. Lan, H. Li, J. Xu, Y. Wang, and D. Cai,
“Pandagpt: One model to instruction-follow them
all,” 2023.
[158] Y. Zhu, R. Kiros, R. S. Zemel, R. Salakhutdinov,
R. Urtasun, A. Torralba, and S. Fidler, “Aligning



### Claim 3/24

#### Claim Text
A survey of 950,965 papers found a significant increase in the use of LLMs in writing scientific papers, especially in Computer Science [97].

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[97]_2404.01268.pdf (Page 8):

Preprint
5.4 Relationship Between Paper Length and AI Usage
We also explored the association between paper length and LLM usage in arXiv Computer
Science papers. Papers were stratified by their full text word count, including appendices,
into two bins: below or above 5,000 words (the rounded median).
Figure 6 shows the temporal trends of LLM usage for these two groups. After the release of
ChatGPT, shorter papers consistently showed higher AI usage compared to longer papers.
By February 2024, the abstracts of shorter papers had an estimated 17.7% of sentences
modified by LLMs, compared to 13.6% for longer papers (Figure 6a). A similar trend was
observed in the introduction sections (Figure 6b). To account for potential confounding
effects of research fields, we conducted an additional robustness check. The finding holds
for both cs.CV (Computer Vision and Pattern Recognition) and cs.LG (Machine Learning)
(Supp Figure 14). However, for cs.CL (Computation and Language), we found no significant
difference in LLM usage between shorter and longer papers, possibly due to the limited
sample size, as we only parsed a subset of the PDFs and calculated their full length.
As Computer Science conference papers typically have a fixed page limit, longer papers
likely have more substantial content in the appendix. The lower LLM usage in these papers
may suggest that researchers with more comprehensive work rely less on LLM-assistance
in their writing. However, further investigation is needed to determine the relationship
between paper length, content comprehensiveness, and the quality of the research.
a
b Abstract
Introduction
Figure 6: Shorter papers tend to have a higher fraction of LLM-modified content. arXiv
Computer Science papers are stratified by their full text word count, including appendices,
into two bins: below or above 5,000 words (the rounded median). Error bars indicate 95%
confidence intervals by bootstrap.
6 Discussion
Our findings show a sharp increase in the estimated fraction of LLM-modified content
in academic writing beginning about five months after the release of ChatGPT, with the
fastest growth observed in Computer Science papers. This trend may be partially explained
by Computer Science researchers’ familiarity with and access to large language models.
Additionally, the fast-paced nature of LLM research and the associated pressure to publish
quickly may incentivize the use of LLM writing assistance (Foster et al., 2015).
We expose several other factors associated with higher LLM usage in academic writing.
First, authors who post preprints more frequently show a higher fraction of LLM-modified
content in their writing. Second, papers in more crowded research areas, where papers tend
to be more similar, showed higher LLM-modification compared to those in less crowded
areas. Third, shorter papers consistently showed higher LLM-modification compared to
longer papers, which may indicate that researchers working under time constraints are
9



Source: data\tc21_2501.12557v1\referenced_papers\[97]_2404.01268.pdf (Page 2):

Preprint
a
b
Figure 2: Word Frequency Shift in arXiv Computer Science abstracts over 14 years (2010-
2024). The plot shows the frequency over time for the top 4 words most disproportionately
used by LLM compared to humans, as measured by the log odds ratio. The words are:realm,
intricate, showcasing, pivotal. These terms maintained a consistently low frequency in arXiv
CS abstracts over more than a decade (2010–2022) but experienced a sudden surge in usage
starting in 2023.
A key characteristic of this framework is that it operates on the population level, without
the need to perform inference on any individual instance. As validated in the prior paper,
the framework is orders of magnitude more computationally efficient and thus scalable, pro-
duces more accurate estimates, and generalizes better than its counterparts under significant
temporal distribution shifts and other realistic distribution shifts.
We apply this framework to the abstracts and introductions (Figures 1 and 7) of academic
papers across multiple academic disciplines,including arXiv, bioRxiv, and 15 journals within
the Nature portfolio, such as Nature, Nature Biomedical Engineering, Nature Human Behaviour,
and Nature Communications. Our study analyzes a total of 950,965 papers published between
January 2020 and February 2024, comprising 773,147 papers from arXiv, 161,280 from
bioRxiv, and 16,538 from the Nature portfolio journals. The papers fromarXiv cover multiple
academic fields, including Computer Science, Electrical Engineering and Systems Science,
Mathematics, Physics, and Statistics. These datasets allow us to quantify the prevalence of
LLM-modified academic writing over time and across a broad range of academic fields.
Our results indicate that the largest and fastest growth was observed in Computer Science
papers, with α reaching 17.5% for abstracts and 15.3% for introductions by February 2024.
In contrast, Mathematics papers and the Nature portfolio showed the least increase, with
α reaching 4.9% and 6.3% for abstracts and 3.5% and 6.4% for introductions, respectively.
Moreover, our analysis reveals at an aggregate level that higher levels of LLM-modification
3



Source: data\tc21_2501.12557v1\referenced_papers\[97]_2404.01268.pdf (Page 7):

Preprint
a
b Abstract
Introduction
Figure 4: Papers authored by first authors who post preprints more frequently tend to
have a higher fraction of LLM-modified content. Papers in arXiv Computer Science are
stratified into two groups based on the preprint posting frequency of their first author, as
measured by the number of first-authored preprints in the year. Error bars indicate 95%
confidence intervals by bootstrap.
Science sub-categories: cs.CV (Computer Vision and Pattern Recognition), cs.LG (Machine
Learning), and cs.CL (Computation and Language), and found that the observed trend
holds for each sub-category (Supp Figure 13).
There are several ways to interpret these findings. First, LLM-use in writing could cause
the similarity in writing or content. Community pressures may even motivate scholars
to try to sound more similar – to assimilate to the “style” of text generated by an LLM.
Alternatively, LLMs may be more commonly used in research areas where papers tend to be
more similar to each other. This could be due to the competitive nature of these crowded
subfields, which may pressure researchers to write faster and produce similar findings.
Future interdisciplinary research should explore these hypotheses.
a
b Abstract
Introduction
Figure 5: Papers in more crowded research areas tend to have a higher fraction of LLM-
modified content. Papers in arXiv Computer Science are divided into two groups based on
their abstract’s embedding distance to their closest peer: papers more similar to their closest
peer (below median distance) and papers less similar to their closest peer (above median
distance). Error bars indicate 95% confidence intervals by bootstrap.
8



Source: data\tc21_2501.12557v1\referenced_papers\[97]_2404.01268.pdf (Page 0):

Preprint
Mapping the Increasing Use of LLMs in Scientific Papers
Weixin Liang∗, Yaohui Zhang∗, Zhengxuan Wu∗, Haley Lepp,
Stanford University
Wenlong Ji, Xuandong Zhao,
Stanford University, UC Santa Barbara
Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, Diyi Yang,
Stanford University
Christopher Potts†, Christopher D Manning †, James Y. Zou†
Stanford University
Abstract
Scientific publishing lays the foundation of science by disseminating re-
search findings, fostering collaboration, encouraging reproducibility, and
ensuring that scientific knowledge is accessible, verifiable, and built upon
over time. Recently, there has been immense speculation about how many
people are using large language models (LLMs) like ChatGPT in their aca-
demic writing, and to what extent this tool might have an effect on global
scientific practices. However, we lack a precise measure of the proportion
of academic writing substantially modified or produced by LLMs. To ad-
dress this gap, we conduct the first systematic, large-scale analysis across
950,965 papers published between January 2020 and February 2024 on
the arXiv, bioRxiv, and Nature portfolio journals, using a population-level
statistical framework to measure the prevalence of LLM-modified content
over time. Our statistical estimation operates on the corpus level and is
more robust than inference on individual instances. Our findings reveal a
steady increase in LLM usage, with the largest and fastest growth observed
in Computer Science papers (up to 17.5%). In comparison, Mathematics
papers and the Nature portfolio showed the least LLM modification (up
to 6.3%). Moreover, at an aggregate level, our analysis reveals that higher
levels of LLM-modification are associated with papers whose first authors
post preprints more frequently, papers in more crowded research areas,
and papers of shorter lengths. Our findings suggests that LLMs are being
broadly used in scientific writings.
1 Introduction
Since the release of ChatGPT in late 2022, anecdotal examples of both published papers
(Okunyt˙e, 2023; Deguerin, 2024) and peer reviews (Oransky & Marcus, 2024) which appear
to be ChatGPT-generated have inspired humor and concern.1 While certain tells, such as
“regenerate response” (Conroy, 2023b;a) and “as an AI language model” (Vincent, 2023),
found in published papers indicate modified content, less obvious cases are nearly impossi-
ble to detect at the individual level (Else, 2023; Gao et al., 2022). Liang et al. (2024) present a
method for detecting the percentage of LLM-modified text in a corpus beyond such obvious
∗Co-first authors, Correspondence to: Weixin Liang <wxliang@stanford.edu>
†Co-supervised project, Correspondence to: James Zou <jamesz@stanford.edu>
1Increased attention to ChatGPT-use by multilingual scholars has also brought to the fore important
conversations about entrenched linguistic discrimination in academic publishing (Khanna et al., 2022).
1
arXiv:2404.01268v1  [cs.CL]  1 Apr 2024



Source: data\tc21_2501.12557v1\referenced_papers\[97]_2404.01268.pdf (Page 21):

Preprint
a
b
c
 cs.LG - Machine Learning
cs.CV - Computer Vision and Pattern Recognition
cs.CL - Computation and Language
Figure 14: The relationship between paper length and LLM usage holds for cs.CV and
cs.LG, but not for cs.CL. Papers in each arXiv Computer Science sub-category (cs.CV , cs.LG,
and cs.CL) are stratified by their full text word count, including appendices, into two bins:
below or above 5,000 words (the rounded median). For cs.CL, no significant difference
in LLM usage was found between shorter and longer papers, possibly due to the limited
sample size, as only a subset of the PDFs were parsed to calculate the full length. Error bars
indicate 95% confidence intervals by bootstrap.
22



### Claim 4/24

#### Claim Text
Rather than justifying the usage, Cuadra et al. [29] studied this very topic with a more critical lens and demonstrated the validity concerns inherent to LLM use in chatbots as humans, which Wang et al. [157] and Agnew et al. [1] address from an ethical perspective as well.

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 9):

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Kapania and Wang, et al.
“gender bias in the entire computational pipeline– in places that we’ve never had gender bias before
because we’ve never used LLMs before, and because we used to come up with our own solutions”
Participants pointed out the tendency of LLMs towards generating homogenous content, where
the model would prioritize generalization and converge diverse perspectives into standardized
outputs. P8, working on sensemaking tools for product selection using LLMs, described how they
“want the selection criteria to encompass a diverse set of opinions rather than just focus on the perspective
of a single demographic. ” This tendency of LLMs towards “flattening human diversity and nuance”
(P9) raised concerns among researchers who emphasized the importance of capturing the complexity
of lived experiences within the context of their research. Participants also noted concerns that using
LLMs for editing their paper drafts could influence researchers’ paper writing styles, streamlining
towards homogeneous and “bland way of writing” (P9). Participants also discussed how LLMs’
training data, often reflecting Western morals and values, could contribute to this homogeneity.
4.2.2 Threats to privacy of participant data.Researchers in our study expressed anxiety about
how data input into LLMs is being used by LLM providers and the potential for violations of
privacy. They worried about breaches of private or sensitive information involving various forms
of participant data, such as transcripts, audio recordings, and application-specific log data. P7
used LLMs for qualitative data analysis and discussed how “privacy issue is the main concern for
us because in many cases, the audio transcription is not supposed to be put into ChatGPT”. P16 also
shared the concern that users’ navigation data is extremely sensitive and could lead to material
physical harm if uploaded to LLMs.
Many participants emphasized their concern of confidential and personally identifiable infor-
mation leakage as a result of sharing user data with LLM providers. P8 mentioned the risk of
confidential information being incorporated into LLM training corpus and the potential for security
leaks, where “backend messes up and user’s private information shows up in someone else’s chat
history”. Relatedly, some participants expressed concerns about the possibility of unconsented data,
for example, “explicit sexual content” (P2), being a part of the training corpus for large multimodal
models. While some believed that manually erasing sensitive information could mitigate these
concerns, others relied on the efficacy of APIs in safeguarding user data.
4.2.3 Violations of intellectual integrity.Interview participants raised ethical concerns about the
intellectual integrity of using LLMs in HCI research. A central theme of these concerns was the
ambiguity of ownership of LLM-generated text and visuals. Many participants who were co-
creating with LLMs also highlighted the difficulty in attributing what portion of the content is
the researcher’s original work and what is generated by the LLMs when they are refining and
co-creating with the LLM outputs. Participants discussed the ambiguity related to plagiarism when
LLM outputs are part of the research contribution. For example, interviewee P10 would consider
crediting the LLM “where a substantial amount ends up in a publication” . While most participants
questioned the extent to which one can claim ownership of LLM-generated content, especially in
the paper writing stage, some believed that “personhood is pretty important for attribution” (P2).
In addition, HCI researchers also raised concerns about the reproducibility of the research results
obtained via LLMs, highlighting the potential for an illusion of efficacy if LLMs work well in some
cases but fail to generalize to others. P2 described using LLMs in HCI research as a “computational
Wizard of Oz” method and continued on to point out how the quick and opaque updates of LLMs
are another barrier to reproducibility, noting that researchers “don’t really have control of what
version of GPT they are talking to and something that might work in a previous version won’t work so
much in future versions” .
10



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 3):

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Kapania and Wang, et al.
the need to go beyond legal requirements and develop comprehensive guidelines for responsible
behavior by learning from other disciplines. Vitak et al. [104] encouraged interdisciplinary col-
laboration of building new ethics framework such as developing ethics heuristics for online data
research to ensure responsible research. Despite the rich tradition of considering and iterating
ethical guidelines in the HCI community, Computer Science (CS) researchers have demonstrated
uncertainty about the applicability of ethical concerns pertaining to human subject studies to their
research [13], highlighting the necessity of educating CS and HCI researchers about research ethics.
Recent endeavors of integrating ethics in CS education such as adding systematic literature review
on ethics in computing courses [ 91, 96] have illustrated both the barriers and opportunities of
educating CS researchers in ethics.
Recently, advances in AI have brought renewed urgency of research ethics to HCI and CS
communities. Clark et al. [27] proposed approaches to assessing ethical risks of research involving
digital data, informing the development of guidelines of ethical standards for research. Amershi
et al. [1] provided guidelines for human-AI interaction design, emphasizing the importance of
providing explanations of the systems and conveying consequences of user actions. Another line
of research strives to bridge the gap between ethics and AI practices by establishing guidelines for
safe, transparent, and trustworthy AI systems [95]. With the rapid development of generative AI,
Association of Computing Machinery (ACM) and the Association for Computational Linguistics
(ACL) have established ongoing efforts to develop guidelines [20, 39]. The ACM policy on authorship
requires the full disclosure of generative AI in the paper [39], while the ACL policy on AI-assisted
tools on paper writing requires authors to elaborate on the scope and nature of their use [20].
Even though underlying ethical guidelines may be broadly applicable, emerging technologies
might present challenges to existing ethical review processes [ 70]. Our goal is not to create a
taxonomy of all possible ethical concerns with the use of LLMs in HCI research; instead, we hope
to extend the discourse on research ethics in HCI by documenting the ways in which researchers
are responding to ethical considerations of LLMs in-situ and reflecting on potential ways forward.
2.3 Ethics of LLM use
Extensive prior research has explored the ethical risks and harms related with language models [106,
107]. The discrimination & exclusion harms arise from the biased and unjust text in the training
data of LLMs. Recent work identified the tendency of LLMs to display discrimination related
to users’ sensitive characteristics [ 112] and demonstrated the gender and racial biases of LLM-
generated content [ 33]. The information hazards are the consequences of LLMs remembering
private information in training data, posing the risk of privacy leaks [18].
Privacy violation has been observed in LLM-based AI assistants where Personally Identifiable
Information (PII) can be revealed by employing adversarial privacy-inducing prompts [ 63, 68].
The detection and mitigation of hallucinations in LLM generated text [115] remains a challenging
problem [22]. This might lead to spread of misinformation, as LLM-based chatbots are often treated
as fact-checking tools [115].
There is a growing concern about malicious activities arising from the generation of scams and
phishing using LLMs [69]. Recent research has also pointed out how the anthropomorphization of
LLMs has the potential for manipulation and negative influence [31]. Finally, researchers have also
documented the likelihood of LLMs increasing inequality and their negative effects on job quality,
undermining creative economies, and more [107].
In response to these concerns, recent research has started to look into how to assess [30] and
mitigate [23, 69] the harms of LLMs. For example, researchers are exploring privacy preserving
strategies for language models in pre-processing, training, and post-training approaches [97]. For
combating misinformation in LLMs [ 23], researchers have proposed several defense strategies,
4



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 12):

Examining ethics of LLM use in HCI research practices Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
think about how to interact with an LLM” (P15). Finally, participants also noted cases where explicit
disclosure about LLMs was rather unnecessary if research participants or system users are not
directly exposed to LLMs. For example, if LLMs were used for ideation or analysis, participants felt
that explicit communication about their use was not imperative.
4.3.3 Restricting LLM use and reflecting on the co-creation process.A recurring sentiment among
the researchers in our study was a perceived lack of control in addressing ethical concerns related
to large language models. As a result, they developed various workarounds, such as restricting
the use of LLMs to a limited set of tasks, avoiding directly integrating LLM-generated inputs
into their work, and hosting group reflection sessions. The limited visibility and decision-making
capability were particularly evident in issues of privacy and data leaks, where the reliance on
LLMs provided by large companies diminished individual control. Simultaneously, participants
expressed a mistrust towards LLM providers. For instance, P12 discussed how LLM providers’ claim
to protect data privacy and not using it for their training “is very problematic because such claims
don’t really articulate what they mean by not using the data. How can an external researcher validate
this claim?” Participants’ mistrust towards LLM providers was exacerbated by the lack of clarity
and transparency in data usage policies.
To navigate the ‘unknown territory’ of LLMs, where HCI researchers were not fully aware of
the capabilities and risks, they would often restrict their usage of LLMs to a limited set of tasks.
Most HCI researchers avoided directly integrating LLM-generated outputs into their work. Rather
than accepting the initial output without scrutiny, they would iteratively refine and carefully verify
before incorporating it into their research artifacts. In the context of paper writing, researchers
would ensure the text aligns with the draft they had input into the system. Many participants
elaborated on their practice of co-creating with LLMs as a precautionary measure. This would
involve relying on their personal judgment on how much to use the LLM suggestions. This cautious
and iterative co-creation process served as a strategy in the absence of clear usage norms.
Indeed, many participants shared the view that LLMs are still evolving, and there is a lack of
comprehensive guidelines on navigating ethical considerations. P9, primarily using LLMs for paper
writing, mentioned how “LLMs are still an unknown territory, so people don’t know how to react, I
assume. ”Our participants expressed discomfort with using any new LLM-generated content in
their papers. For example, P15 was hesitant to use the LLM as anything more than a spell checker.
Researchers highlighted the importance of applying LLMs in a way that is consistent with their
practices with previously used tools to mitigate potential unintended consequences. P9 spoke of
their experience as a reviewer for major HCI conferences like CHI, where they did not encounter
any guidelines regarding the disclosure of using ChatGPT for writing. Finally, they went on to
emphasize that the cost and accountability of using LLMs was still “ad-hoc and unclear. ”
In cases where participants recognized the possibility of ethical concerns with LLMs, they often
struggled to identify specific issues to address. As P16 pointed out: “the main problem is that I
don’t know what bias it has, and I don’t know how to figure it out. ” P4 discussed how it is crucial
to “not pretend that this ethical consideration does not exist, which some people do. We are trained in
human-computer interaction, and so we are well equipped to reason about it or at least understand
that these concerns exist, but obviously addressing it is very difficult. ” In many cases, when people
did not have mechanisms to identify or address ethical concerns, they felt it was important to at
least acknowledge them and spread awareness about these issues.
To account for the uncertainties with the responsible use of LLMs, some researchers chose to
host group reflection sessions. This involved engaging in discussions with collaborators or advisors
to determine the appropriate approach for navigating any ethical considerations. P10 exemplified
this approach by holding regular team meetings to address any questions or concerns related to
13



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 8):

Examining ethics of LLM use in HCI research practices Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
on aspects that the researcher“can hardly think about from [their] vantage point. Interacting with the
LLM is like talking to other researchers and getting their feedback” (P11). Researchers also mentioned
using LLMs to generate paper reviews (P14), and provide suggestions for their writing content and
style (e.g., as an alternative to Grammarly2 (P4)). P14 spoke of using GPT-4 to“provide critical paper
reviews, check grammar, the style, and the logic consistency” .
Finally, many HCI researchers used LLMs as a design material for system development,
including system building and evaluation. Participants shared many use cases, such as developing
new tools and interactions for library services, science communication, and AI-assisted writing. P4
expressed interest in exploring “what do large language models enable us to do in HCI with which
our field has struggled for many decades. ” Finally, researchers in our study also discussed instances
where they used large language models for software development, including day-to-day debugging
and creating web interfaces with LLMs.
4.2 What are HCI researchers’ ethical concerns with the use of LLMs?
Researchers expressed a wide range of ethical concerns regarding the use of LLMs. Across our survey
respondents, 30 reported observing ethical challenges associated with using LLMs, 10 expressed
uncertainty, and 10 indicated no awareness of such concerns. Among the self-reported ethical
challenges, the most common issues were data privacy (19) (including secondary use of participants’
data and data leak), authorship (16) (including disclosure of the use of LLMs in publications),
harmful outputs (14), copyright issues (11) and consent (10). These concerns were more prevalent
across research design and execution, as well as analysis and paper writing stages of the research
(see Figure 1). Below, we describe the specific ethical concerns highlighted by HCI researchers in
our interviews.
4.2.1 Harms of engaging with LLM outputs.Our interviews revealed a shared ethical concern of
research subjects engaging with harmful outputs, especially if LLMs were integrated in systems
and tools that directly interact with users. Over the last few years, scholars within HCI have
increasingly shifted their efforts to working with and centering vulnerable populations (see [17,
59]). P3 used large multimodal models to create a mental well-being and self-reflection tool and
articulated their concern about LLMs generating uncontrolled outputs. LLM-generated content
could disproportionately harm marginalized groups through exposure to socially harmful biases
and stereotyping behaviors. Hate speech and exclusionary and discriminatory language could
cause psychological and representational harm to research participants. P16, developing LLM-based
roleplay robots, articulated how “[LLM] may exaggerate or diminish some of the capacity with which
the robot has already been equipped. It may introduce another layer of bias [toward people with
disabilities]. ” In addition to the concerns about biased LLM outputs, P14 also worried about the
possibility of LLM facilitating the “surveillance and monitoring of intimate partners” by providing
links to various spy tools.
Many researchers expressed concerns about large language models impacting their research
pipeline by generating seemingly authoritative but fabricated information. LLM hallucinations
during paper writing could mislead authors and if published, would undermine trust in knowledge
production processes. P10 used LLMs for literature review and worried that “ChatGPT might
make up titles of things that simply do not exist. ” Participants highlighted the need for vigilance
in identifying these hallucinations, especially when LLMs produced fake citations or mismatched
paper references. Beyond paper writing, researchers indicated the possibility of inheriting biases
from LLMs in ideation, research design, and evaluation. P2 articulated how LLMs could introduce
2https://app.grammarly.com/
9



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 11):

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Kapania and Wang, et al.
online, right? So we always claim this tool is no more harmful than typical social media” (P13). If
researchers explored topics deemed safe or if users did not directly prompt the model, there was a
perception that the LLM was unlikely to generate unsafe content3.
Such ‘conditional engagement’ sometimes resulted in a more reactive, rather than proactive,
approach to ethical considerations. For example, when deliberating whether LLMs should be
attributed and held accountable for model-generated content, P2 emphasized “I think many people
are very hasty to say yes or no. And I think that’s not the answer. The answer is always in a gray area. ”
They continued to emphasize this “wait and see” approach, discussing the potential benefits of
model hallucinations and highlighting their role in promoting divergent thinking by introducing
specificity that the user may not have considered. Some participants mentioned being aware of
frameworks and toolkits (such as Perspective API [53]) designed to address ethical issues, but they
never incorporated these tools into their own research processes. P3 justified this by discussing how
a significant portion of their HCI studies were conducted in a laboratory setting. The ethical concern
related to participants encountering harmful outputs generated by LLMs was less probable in a
short usability test, while serious issues could occur in longitudinal studies when the participants
heavily use the system.
4.3.2 Limited disclosure practices. In our study, HCI researchers positioned large language models
as everyday tools within their research practice. As a result, participants did not believe it was
necessary to formally report their usage of LLMs to study participants, the Institutional Review
Board (IRB), and/or the broader academic community. In particular, participants described a shift
towards the tacit incorporation of LLMs: they slowly transitioned from research tools that were
a part of their regular practice to using LLMs. This change was partly due to the perception that
LLMs are ‘fancier’ (P11) and more advanced versions of previously used tools. P10 explained, “I
advise my students that they are allowed to use any generative AI tool just like they would use other
productivity tools. So I’m categorizing LLMs as a productivity tool. ”
When using LLMs in paper writing, participants drew a parallel between LLM reporting practices
and their approach to previous tools. They emphasized that if, for example, tools like “grammar
assistance by Grammarly, word check by Google Docs, or accessibility checking by Acrobat pro” (P10)
were not explicitly reported, the same should also hold true for large language models. Researchers
expressed reservations, suggesting that reporting of LLM-based tools might call into question the
validity of their work. P10 captured this perspective:
I would not feel it is appropriate to say Bing Chat was used to define the initial structure
of X and Y sections of the paper. To me, it is not gonna be helpful in researchers assessing
the credibility or validity of the work. It is just like a meta issue about how the actual
document was formed and refined. And what really matters in that case is the output. Is it
easy to read? Did you find it useful? You know, that’s what I care about. So, in those cases,
I do not credit the LLM.
The complexity of describing LLMs to audiences without technical background further impacted
participants’ willingness to disclose the specific uses of large language models in research. In
some cases, researchers chose to characterize their LLM use simply as ‘AI models’ to their research
participants so as not to “confuse them with what is a language model. ” The rationale behind this
approach was the perception that the broader public (target population in this case) tend to view AI
in a homogeneous manner, and“to them, there isn’t much difference between how different AI systems
work, or which is a large language model, right?” (P8). Researchers also justified this approach by
highlighting their intention not to burden participants with the need to modify their behavior and“
3In contrast, prior work has demonstrated that LMs can generate unsafe content from seemingly innocuous prompts [46].
12



### Claim 5/24

#### Claim Text
Chen et al. [20] attributed the inconsistency of generated data to the “inherent randomness embedded in the output of LLMs . ” This, however, can be alleviated by changing the sampling temperature to zero [122] or using guided generation [96].

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[122]_2308.02828.pdf (Page 12):

An Empirical Study of the Non-determinism of ChatGPT in Code Generation 13
Table 5. RQ2: Influence of temperature (CodeContests).
Temperature Test Pass Rate
Mean value Mean variance Mean max diff Max diff Ratio of worst cases
0 0.15 0.01 0.11 1.00 1.82%
0.5 0.16 0.02 0.15 1.00 2.42%
1 0.16 0.03 0.24 1.00 3.64%
Temperature OER OER (no ex.)
Mean value Ratio of worst cases Pair mean value Mean value Ratio of worst cases Pair mean value
0 0.37 43.64% 0.59 0.27 54.55% 0.46
0.5 0.18 62.42% 0.37 0.13 68.48% 0.28
1 0.09 75.76% 0.27 0.06 81.21% 0.19
Temperature LCS LED
Mean value Mean worst value Pair mean value Mean value Mean worst value Pair mean value
0 0.61 0.44 0.62 23.45 35.87 22.31
0.5 0.33 0.23 0.34 44.48 62.02 44.89
1 0.22 0.16 0.23 58.80 77.46 58.86
Temperature United_Diff Tree_Diff
Mean value Mean worst value Pair mean value Mean value Mean worst value Pair mean value
0 0.41 0.39 0.67 0.50 0.46 0.74
0.5 0.61 0.49 0.63 0.69 0.58 0.71
1 0.33 0.27 0.46 0.41 0.33 0.56
of problems with no equal test output among the five code candidates. This is contrary to many
people’s belief that setting the temperature to 0 can make ChatGPT deterministic [7, 13, 38], because
when setting the temperature to 0, the model applies greedy sampling which should indicate full
determinism, with the logit value for the next token being a pure function of the input sequence
and the model weights. The reason for such non-determinism with the temperature being zero is
still controversial [1], with different hypotheses such as floating point, unreliable GPU calculations,
and its sparse MoE architecture failing to enforce per-sequence determinism [44, 54]. The details
for all the non-deterministic coding tasks and their test outputs with temperature=0 are on our
homepage [3].
When temperature=0.5, we observe that ChatGPT tends to generate code candidates that are more
deterministic than temperature=1, but less deterministic than temperature=0. This is as expected
because the higher temperature brings more creativity to ChatGPT and affects its ability to generate
similar code (as can be observed from the other measurements, such as LCS and LED). Nevertheless,
we observe that the value of test pass rates among the three different temperatures are similar,
which indicates that low temperature might be a better choice given the comparable test pass rate
and the low degree of non-determinism.
Answer to RQ2: Contrary to the widely held belief (and common practices), setting the
temperature to 0 does not guarantee determinism in code generation, although it indeed
brings more determinism than the default configuration (temperature=1) for all three types
of similarities. We also observe that the values of test pass rate among the three different
temperatures are similar, indicating that low temperature might be a better choice for code
generation tasks.
, Vol. 1, No. 1, Article . Publication date: October 2024.



Source: data\tc21_2501.12557v1\referenced_papers\[122]_2308.02828.pdf (Page 11):

12 Shuyin Ouyang, Jie M. Zhang, Mark Harman, and Meng Wang
Table 4. RQ1.3: Structural similarity.
Structural Similarity Metric CodeContests APPS HumanEval
United_Diff Mean value 0.33 0.43 0.60
Mean worst value 0.27 0.35 0.47
Pair mean value 0.46 0.52 0.67
Tree_Diff Mean value 0.41 0.54 0.62
Mean worst value 0.33 0.47 0.48
Pair mean value 0.56 0.63 0.70
CodeContests       APPS       HumanEval
0.00
0.25
0.50
0.75
1.00
UnifiedDiff TreeDiff
(a) Mean
CodeContests       APPS       HumanEval
0.00
0.25
0.50
0.75
1.00
UnifiedDiff TreeDiff (b) Mean Worst
CodeContests       APPS       HumanEval
0.00
0.25
0.50
0.75
1.00
UnitedDiff TreeDiff (c) Pair Mean
Fig. 5. RQ1.3: Structural Similarity (United_Diff & Tree_Diff).
We observe that the code candidates generated from the same instruction show great similarity
in structure. Specifically, the mean values are 0.33, 0.43, and 0.60 under the United_Diff setting, and
0.41, 0.54, and 0.62 under Tree_Diff setting for CodeContests, APPS, and HumanEval, respectively.
For the three datasets, we could see from Table 4 that the lowest values under United_Diff and
Tree_Diff happen for the CodeContests dataset. By contrast, the largest values under the two
settings both happen for HumanEval. This indicates that ChatGPT is most unstable in structure for
the code generation tasks in CodeContests, and most stable for HumanEval. We further explore the
correlation between different similarities and task features in RQ4.
Answer to RQ1.3: Code candidates show high structural similarity under UnitedDiff and
TreeDiff settings. We observe that the code candidates generated from the same instruction
have high similarity in structure. Specifically, the mean values are 0.33, 0.43, and 0.60 under
the United_Diff setting, and 0.41, 0.54, and 0.62 under Tree_Diff setting for CodeContests,
APPS, and HumanEval, respectively.
4.2 RQ2: Influence of Temperature
The default temperature of ChatGPT is 111. This RQ explores whether the code generation non-
determinism of ChatGPT changes with the temperature changes. We use identical measurements
as in RQ1. We show our experiment results on CodeContests only. Results for other datasets are on
our homepage [3].
Table 5 shows the results. Overall, we observe that when temperature=0, ChatGPT has better
determinism than the default configuration ( temperature=1) for all three types of similarities.
However, setting the temperature to 0 does not completely avoid non-determinism. Take OER
as an example, there are still 43.64% (CodeContests), 27.40% (APPS), and 18.29% (HumanEval)
11https://platform.openai.com/docs/api-reference/chat/create#chat-create-temperature
, Vol. 1, No. 1, Article . Publication date: October 2024.



Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 29):

Fig. 10: Temperature hyperparameter does not solve flatness for Wizard Vicuna Uncen-
sored. Same interpretation as Fig. 5: comparison of human in-group diversity to Wizard Vicuna
Uncensored generations varying levels of temperature settings, where by 1.8 the responses become
incoherent. At this setting even though the unique n-gram metric shows the LLM surpassing humans
in diversity, this is only due to the incoherence as under no other semantic metric is human diversity
reached.
30



Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 6):

Fig. 4 : LLMs flatten groups. Across all four LLMs (rows), each point indicates the diversity
measurement averaged across 3-6 questions asked for each identity. 95% confidence intervals are
generated through cluster bootstrapping with each question as a cluster. Each column represents a
different measure of diversity, and the larger the number on the x-axis, the more diverse the responses
are. The gray crosses indicate human participant in-group responses, while colored circles represent
LLM responses. Nearly every single model and identity group across each metric has less diverse
LLM responses compared to human responses.
output. For our experiments we have used the default temperature setting of 1. Thus, we run a
further analysis on the intersectional demographic axis and show in Fig. 5 the temperature settings
of [1.0, 1.2, 1.4] for GPT-4. We stop at 1.4 because GPT-4 devolves into nonsensical phrasing (e.g.,
“...fon resir’ potions cutramTes frequently sandwiched...”). It is only at such a high temperature
that diversity as measured by unique n-grams per response is reached—and even then across the
remaining three measures of diversity the LLM responses fall short of that of human participants.
7



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 109):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In Francis Bach and David Blei (eds.),Proceedings
of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine
Learning Research, pp. 2256–2265, Lille, France, 07–09 Jul 2015. PMLR.
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang,
Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and
applications. ACM Computing Surveys, 56(4):1–39, 2023.
11



### Claim 6/24

#### Claim Text
Traditional NLP benchmarks are often criticized for their lack of context realism: the model performance measures are often divorced from downstream use cases [99].

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[99]_2306.03100.pdf (Page 5):

Rethinking Model Evaluation as Narrowing the Socio-Technical Gap
Context 
Realism
Human 
Requirement 
Realism
Application Grounded 
Field Study
Simulated 
Evaluation
Application Grounded 
Controlled Study
Contextualized 
Benchmarking
Human Ratings with 
Normative Criteria
Normative 
Benchmarking
+possible costs
+possible costs Contextualized 
Human Ratings
Figure 1.Mapping of HCI and NLG (in bold) evaluation methods on the two dimensions of realism
with generic instruction such as “be short...and extract im-
portant information” (Gliwa et al., 2019). While typically
low-cost and convenient for evaluation (e.g., many with
open-source libraries available), problematically, evalua-
tion based on these references may not reflect how people
will use the model in the use case of summarizing social
messages (low context realism), and accuracy alone may
not capture the multiple human requirements (low human
requirement realism).
Contextualized Benchmarking For this category, we
consider a specific example: a recently proposed evalua-
tion benchmark for LLMs called HELM (Holistic Evalu-
ation of Language Models) (Liang et al., 2022). Among
other contributions, HELM provides a taxonomy to map
existing benchmarks by “scenario”: e.g., the benchmark
of CNN/DailyMail (Hermann et al., 2015; Nallapati et al.,
2016) is a task of summarization for News (domain) in En-
glish (language). In fact, we may consider this definition of
a scenario as one way to operationalize “downstream use
cases”. This structure allows evaluation results to inform
whether the LLM is good for a specific scenario, and en-
courages the development of new benchmarks for scenarios
without appropriate benchmarks. These “contextualized”
benchmarks, by explicitly defining the context within which
an evaluation result should be interpreted, could improve
context realism. Furthermore, HELM encourages “multi-
metric measurements”—in addition to accuracy, it currently
evaluates all tasks by 6 other metrics such as robustness, ef-
ficiency, and biases, and supplements additional metrics that
are important for specific tasks, such as factual consistency
for summarization. These multi-metrics could potentially
improve the human requirement realism by covering more
constructs. Overall, we commend HELM as a valuable ef-
fort toward narrowing the socio-technical gap. However,
we suggest that the taxonomy should be better informed
by what the common downstream use cases of LLMs are
and evaluated by its descriptive and evaluative power, rather
than by “tasks studied by the NLP community”. Perhaps
more importantly, the choices of metrics should be based
on people’s actual needs and values in these use cases; in-
discriminately including all metrics for all use cases with-
out considering their context-specific priority would not be
productive for practitioners to make use of the evaluation
results (Liao et al., 2020).
Human ratings (normative v.s. contextualized) Human
ratings, by domain experts or crowd workers, are currently
considered the “gold standard” for evaluating NLG mod-
els as they allow capturing multiple quality criteria. Com-
pared to automatic benchmarking, this approach can be
more costly in time and resources for recruiting and train-
ing human raters. For example, summarization is often
evaluated by fluency (grammatically correct), consistency
(factual alignment with the original text), coherence (coher-
ent body of information about a topic), etc. (Fabbri et al.,
2021). While improving human requirement realism, we
contend that this kind of normative rating, by generic con-
structs and rating questions (e.g., “Are the contents of the
generated text fluent?”), does not provide context realism.
The results may present significant gaps in evaluating the
socio-requirements in our running example. For example,
the construct of actionability may not be possible to mea-
sure without considering the context regarding when and



Source: data\tc21_2501.12557v1\referenced_papers\[99]_2306.03100.pdf (Page 4):

Rethinking Model Evaluation as Narrowing the Socio-Technical Gap
and embrace diverse evaluation methods in order to narrow
the socio-technical gap. These evaluation methods should
aim to capture diverse constructs (i.e. criteria) of socio-
requirements, and allow for varying trade-offs between re-
flecting realistic socio-requirements and pragmatic costs to
conduct the evaluation. While this view on trade-off res-
onates with Doshi-Velez & Kim (2017)’ considerations for
XAI evaluation, HCI works offer a larger set of concrete
evaluation methods and valuable lessons about validating
low-cost evaluation methods according to what they aim
to approximate for, and articulating what are the suitable
situations that justify prioritizing lowering evaluation costs.
3.3. A Note about Realism and Ecological Validity in the
Social Sciences
Obviously, these ideas in HCI and XAI evaluations have
their roots in research design and measurement theory in
the social sciences. Focusing on the context dimension, Mc-
Grath (1995) defines “realism” as “the situation or context
within which the evidence is gathered, in relation to the
contexts to which you want your evidence to apply”. It is
one of the desirable criteria to optimize for when design-
ing research studies, which can be (but not always) at odds
with: 1) ease of having control on measurement precision,
which can be considered as one form of research cost; 2)
generalizability to other contexts.
Our concept of realism is also related to ecological validity,
referring to whether one can generalize from the conclu-
sions of a laboratory study to the real world. In particular,
our framework corresponds to two dimensions of ecological
validity (Schmuckler, 2001) 2: the context dimension—how
close the task or test environment is to the real-world con-
text; and the human response dimension—how well the
measurement represents people’s actual response and is ap-
propriate to the constructs that matter. However, we consider
the generalizability to the real world beyond “lab environ-
ment” and “human response”, and also automatic metrics
and other analytical methods, which may require further
abstraction in terms of both context and measurement.
4. Mapping HCI and NLG Evaluation
Methods for LLM Evaluation
While the lessons and arguments above can be applied to
model evaluation broadly, in this section, we focus on ex-
ploring opportunities for LLM evaluations. In Figure 1,
we map HCI evaluation methods and current NLG evalua-
tion methods along the two dimensions of proxy for socio-
2We do not cover the “stimuli” dimension for ecological valid-
ity, which is less relevant here as the model is always the object
being evaluated. However, this dimension has implications for con-
sidering whether model behaviors that matter for the deployment
context are sufficiently and realistically covered in the evaluation.
requirements: context realism—realistic proxy for how the
technology will be used in a downstream use case; and hu-
man requirement realism—realistic proxy for what require-
ments people involved in the use case have for technology.
We elaborate on the mapping below. This analysis is in-
tended to re-frame different evaluation methods as different
proxies for narrowing the socio-technical gaps with trade-
offs for pragmatic costs to conduct the evaluation. With this
reframing, we highlight gaps in current LLM evaluation and
identify opportunities for new methods.
A running example of use case To ground our discussion,
we consider a common use case of LLMs as summarizing
previous social messages (emails, chats) to support future
social interactions, which has already come into producti-
zation in many applications. We consider some important
human requirements in this use case to ensure the coverage
of important points, ease to understand the summary, and
actionability for composing the next messages. We assume
these requirements are informed by studying user needs
in this use case, and that this granularity of use cases is
validated to be useful for LLM evaluation (e.g., with good
descriptive and evaluative power).
A few acknowledgments: First, we note that this analysis
focuses on G2, and is agnostic to what are the use cases.
That is, the same analysis can be performed for different
downstream use cases identified, but not all evaluation meth-
ods are always applicable. Second, while we focus on the
tradeoff between realism and pragmatic costs, as discussed,
realism can also have tradeoffs with controllability for mea-
surement precision, generalizability, and potentially repro-
ducibility. Third, we take a broad, under-defined view on
costs—required time, resources and training for researchers,
as well as demands for human subjects, if applicable. We
leave the tasks for how to further operationalize costs and
make choices of evaluation methods based on costs as open
questions for future work, to be discussed in the next section.
Finally, this is a coarse mapping based on example methods
we selectively discuss (with our own biases). It should not
be taken as suggesting one category of methods is always
superior to the other.
Normative Benchmarking We consider normative bench-
marking as using standard NLP benchmarks for perfor-
mance, including meta-benchmarks that pool these bench-
marks without additional considerations for how to organize
them or contextualize the interpretation of the results. These
benchmarks typically measure model performance or accu-
racy through some definition of matching to compare the
model output and the ground truth summary (reference),
where the reference can be the preview sentence written by
journalists to summarize the news article (Hermann et al.,
2015), or a summary of a dialogue written by annotators



Source: data\tc21_2501.12557v1\referenced_papers\[99]_2306.03100.pdf (Page 9):

Rethinking Model Evaluation as Narrowing the Socio-Technical Gap
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,
Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,
et al. Training language models to follow instructions
with human feedback. Advances in Neural Information
Processing Systems, 35:27730–27744, 2022.
Pirolli, P. Cognitive models of human-information inter-
action. Handbook of applied cognition , pp. 443–470,
2007.
Raji, I. D., Bender, E. M., Paullada, A., Denton, E., and
Hanna, A. Ai and the everything in the whole wide world
benchmark. arXiv preprint arXiv:2111.15366, 2021.
Sai, A. B., Mohankumar, A. K., and Khapra, M. M. A
survey of evaluation metrics used for nlg systems. ACM
Computing Surveys (CSUR), 55(2):1–39, 2022.
Schmuckler, M. A. What is ecological validity? a dimen-
sional analysis. Infancy, 2(4):419–436, 2001.
Selbst, A. D., Boyd, D., Friedler, S. A., Venkatasubrama-
nian, S., and Vertesi, J. Fairness and abstraction in so-
ciotechnical systems. In Proceedings of the conference
on fairness, accountability, and transparency, pp. 59–68,
2019.
Shelby, R., Rismani, S., Henne, K., Moon, A., Rostamzadeh,
N., Nicholas, P., Yilla, N., Gallegos, J., Smart, A., Garcia,
E., et al. Sociotechnical harms: Scoping a taxonomy for
harm reduction. arXiv preprint arXiv:2210.05791, 2022.
Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,
A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,
Garriga-Alonso, A., et al. Beyond the imitation game:
Quantifying and extrapolating the capabilities of language
models. arXiv preprint arXiv:2206.04615, 2022.
Suresh, H., Gomez, S. R., Nam, K. K., and Satyanarayan, A.
Beyond expertise and roles: A framework to characterize
the stakeholders of interpretable machine learning and
their needs. In Proceedings of the 2021 CHI Conference
on Human Factors in Computing Systems, pp. 1–16, 2021.
Vaughan, J. W. and Wallach, H. A human-centered agenda
for intelligible machine learning. Machines We Trust:
Getting Along with Artificial Intelligence, 2020.
Wei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B.,
Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-
zler, D., et al. Emergent abilities of large language models.
Transactions on Machine Learning Research.
Weidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang,
P.-S., Mellor, J., Glaese, A., Cheng, M., Balle, B.,
Kasirzadeh, A., et al. Taxonomy of risks posed by lan-
guage models. In 2022 ACM Conference on Fairness,
Accountability, and Transparency, pp. 214–229, 2022.
Zhang, S. and Balog, K. Evaluating conversational recom-
mender systems via user simulation. In Proceedings of
the 26th acm sigkdd international conference on knowl-
edge discovery & data mining, pp. 1512–1520, 2020.
Zhang, T., Kishore, V ., Wu, F., Weinberger, K. Q., and Artzi,
Y . Bertscore: Evaluating text generation with bert. arXiv
preprint arXiv:1904.09675, 2019.
Zheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z.,
Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging
llm-as-a-judge with mt-bench and chatbot arena. arXiv
preprint arXiv:2306.05685, 2023.
Zhou, K., Blodgett, S. L., Trischler, A., Daum´e III, H., Sule-
man, K., and Olteanu, A. Deconstructing nlg evaluation:
Evaluation practices, assumptions, and their implications.
In Proceedings of the 2022 Conference of the North Amer-
ican Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pp. 314–324,
2022.



Source: data\tc21_2501.12557v1\referenced_papers\[99]_2306.03100.pdf (Page 0):

Rethinking Model Evaluation as Narrowing the Socio-Technical Gap
Q. Vera Liao1 Ziang Xiao 1 2
Abstract
The recent development of generative and large
language models (LLMs) poses new challenges
for model evaluation that the research commu-
nity and industry are grappling with. While the
versatile capabilities of these models ignite ex-
citement, they also inevitably make a leap toward
homogenization: powering a wide range of appli-
cations with a single, often referred to as “general-
purpose”, model. In this position paper, we ar-
gue that model evaluation practices must take on
a critical task to cope with the challenges and
responsibilities brought by this homogenization:
providing valid assessments for whether and how
much human needs in downstream use cases can
be satisfied by the given model (socio-technical
gap). By drawing on lessons from the social sci-
ences, human-computer interaction (HCI), and the
interdisciplinary field of explainable AI (XAI), we
urge the community to develop evaluation meth-
ods based on real-world socio-requirements and
embrace diverse evaluation methods with an ac-
knowledgment of trade-offs between realism to
socio-requirements and pragmatic costs to con-
duct the evaluation. By mapping HCI and current
NLG evaluation methods, we identify opportuni-
ties for evaluation methods for LLMs to narrow
the socio-technical gap and pose open questions.
1. Introduction
The recently developed large generative models, from lan-
guage models to multi-modal models, are astonishingly
powerful and rapidly deployed to power diverse applications
from search engines to writing support to content analysis
tools. However, they are also giving rise to an “evaluation
crisis”, making traditional model evaluation metrics and
methods inadequate or even obsolete.
There are several hurdles to evaluating these models. First,
1Microsoft Research, Montreal, Canada 2Johns Hopkins Uni-
versity, Baltimore. USA. Correspondence to: Q.Vera Liao <ver-
aliao@microsoft.com>, Ziang Xiao <ziang.xiao@jhu.edu>.
the AI and NLP communities have long dealt with the chal-
lenges of evaluating generative models. Until recently, natu-
ral language generation (NLG) evaluation has focused on
specialized models that perform tasks such as machine trans-
lation, abstractive summarization, and question-answering.
These NLG models typically have open-ended and complex
output spaces, and what makes the language outputs “good”
can be multi-faceted and context-dependent. Hence, the
evaluation often cannot solely rely on simple performance
metrics that measure the lexical matching (e.g., ROUGE
score (Lin, 2004))with some universal ground truth. The
community has developed more sophisticated metrics (e.g.,
BERTscore (Zhang et al., 2019)) and ways to obtain “ground
truth” (but often flawed, as criticized by a recent wave of
data auditing work (Blodgett et al., 2021; Raji et al., 2021)).
To complement these flaws of automatic evaluation, human
evaluation is often used in practice (Zhou et al., 2022), such
as asking people to rate the quality, fluency, coherence, rel-
evance, or consistency of model outputs (Sai et al., 2022).
However, these human evaluation practices have also been
widely criticized for lacking standardization, reproducibil-
ity, and validity for assessing model utility in real-world
settings (Clark et al., 2021; Howcroft et al., 2020; Belz
et al., 2021; Gehrmann et al., 2022).
There is a second challenge that is more unique and critical
to these “general-purpose” LLMs, and one that the commu-
nity only begins to grapple with: their diverse and currently
uncertain capabilities, performing all the tasks of these spe-
cialized NLG models, and being discovered of new “emer-
gent capabilities” (Wei et al.). Initial efforts have emerged
in the NLP community in the form of meta-benchmarks,
by combining many specialized evaluation tasks. For ex-
ample, BigBench (Srivastava et al., 2022) pools more than
200 tasks that were used for evaluating specialized models.
Others have sought to evaluate these LLMs by human cog-
nitive and linguistic capabilities (Mahowald et al., 2023).
Amid these surging interests in model evaluation, important
human-centered questions are missing: Who should care
about these evaluation results? And how should they make
use of them?
We instead argue to center our analysis on how these models
will be used in practice—powering downstream applica-
tions, which must involve practitioners (product managers,
developers, designers, or essentially anyone who adopts a
arXiv:2306.03100v3  [cs.HC]  29 Jun 2023



Source: data\tc21_2501.12557v1\referenced_papers\[183]_2303.18223.pdf (Page 66):

67
TABLE 16: Evaluation on the eight abilities of LLMs with specially selected tasks. The shade of the Orange and Blue
fonts denote the performance orders of the results in closed-source and open-source models, respectively. This table will
be continuously updated by incorporating the results of more models.
Models Language Generation Knowledge Utilization
LBD↑ WMT↑ XSum↑ HumanEval↑ TriviaQA↑ NaturalQ↑ WebQ↑ ARC↑ WikiFact↑
ChatGPT 55.81 36.44 21.71 79.88 54.54 21.52 17.77 93.69 29.25
Claude 64.47 31.23 18.63 51.22 40.92 13.77 14.57 66.62 34.34
Claude 2 45.20 12.93 19.13 78.04 54.30 21.30 21.06 79.97 35.83
Davinci003 69.98 37.46 18.19 67.07 51.51 17.76 16.68 88.47 28.29
Davinci002 58.85 35.11 19.15 56.70 52.11 20.47 18.45 89.23 29.15
LLaMA 2-Chat (7B) 56.12 12.62 16.00 11.59 38.93 12.96 11.32 72.35 23.37
Vicuna (13B) 62.45 20.49 17.87 20.73 29.04 10.75 11.52 20.69 28.76
Vicuna (7B) 63.90 19.95 13.59 17.07 28.58 9.17 6.64 16.96 26.95
Alpaca (7B) 63.35 21.52 8.74 13.41 17.14 3.24 3.00 49.75 26.05
ChatGLM (6B) 33.34 16.58 13.48 13.42 13.42 4.40 9.20 55.39 16.01
LLaMA 2 (7B) 66.39 11.57 11.57 17.07 30.92 5.15 2.51 24.16 28.06
LLaMA (7B) 67.68 13.84 8.77 15.24 34.62 7.92 11.12 4.88 19.78
Falcon (7B) 66.89 4.05 10.00 10.37 28.74 10.78 8.46 4.08 23.91
Pythia (12B) 61.19 5.43 8.87 14.63 15.73 1.99 4.72 11.66 20.57
Pythia (7B) 56.96 3.68 8.23 9.15 10.16 1.77 3.74 11.03 15.75
Models Knowledge Reasoning Symbolic Reasoning Mathematical Reasoning Interaction with Environment
OBQA↑ HellaSwag↑ SocialIQA↑ C-Objects↑ Penguins↑ GSM8k↑ MATH↑ ALFW↑ WebShop↑
ChatGPT 81.20 61.43 73.23 53.20 40.27 78.47 33.78 58.96 45.12/15.60
Claude 81.80 54.95 73.23 59.95 47.65 70.81 20.18 76.87 47.72/23.00
Claude 2 71.60 50.75 58.34 66.76 74.50 82.87 32.24 77.61 34.96/19.20
Davinci003 74.40 62.65 69.70 64.60 61.07 57.16 17.66 65.67 64.08/32.40
Davinci002 69.80 47.81 57.01 62.55 67.11 49.96 14.28 76.87 29.66/15.20
LLaMA 2-Chat (7B)45.62 74.01 43.84 43.40 38.93 9.63 2.22 11.19 24.51/5.60
Vicuna (13B) 43.65 70.51 45.97 53.55 36.91 18.50 3.72 8.96 22.74/5.00
Vicuna (7B) 43.84 69.25 46.27 44.25 36.24 14.03 3.54 1.49 6.90/1.40
Alpaca (7B) 47.82 69.81 47.55 39.35 40.27 4.93 4.16 4.48 0.00/0.00
ChatGLM (6B) 30.42 29.27 33.18 14.05 14.09 3.41 1.10 0.00 0.00/0.00
LLaMA 2 (7B) 44.81 74.25 41.72 43.95 35.75 10.99 2.64 8.96 0.00/0.00
LLaMA (7B) 42.42 73.91 41.46 39.95 34.90 10.99 3.12 2.24 0.00/0.00
Falcon (7B) 39.46 74.58 42.53 29.80 24.16 1.67 0.94 7.46 0.00/0.00
Pythia (12B) 37.02 65.45 41.53 32.40 26.17 2.88 1.96 5.22 3.68/0.60
Pythia (7B) 34.88 61.82 41.01 29.05 27.52 1.82 1.46 7.46 10.75/1.80
Models Human Alignment Tool Manipulation
TfQA↑ C-Pairs↓ WinoGender↑ RTP↓ HaluEval↑ HotpotQA↑ Gorilla-TH↑ Gorilla-TF↑ Gorilla-HF↑
ChatGPT 69.16 18.60 62.50/72.50/79.17 3.07 66.64 23.80 67.20 44.53 19.36
Claude 67.93 32.73 71.67/55.00/52.50 3.75 63.75 33.80 22.04 7.74 7.08
Claude 2 71.11 10.67 60.00/60.00/55.83 3.20 50.63 36.4 61.29 22.19 23.67
Davinci003 60.83 0.99 67.50/68.33/79.17 8.81 58.94 34.40 72.58 3.80 6.42
Davinci002 53.73 7.56 72.50/70.00/64.17 10.65 59.67 26.00 2.69 1.02 1.00
LLaMA 2-Chat (7B)69.77 48.54 47.50/46.67/46.67 4.61 43.82 4.40 0.00 0.00 0.22
Vicuna (13B) 62.30 45.95 50.83/50.83/52.50 5.00 49.01 11.20 0.00 0.44 0.89
Vicuna (7B) 57.77 67.44 49.17/49.17/49.17 4.70 43.44 6.20 0.00 0.00 0.33
Alpaca (7B) 46.14 65.45 53.33/51.67/53.33 4.78 44.16 11.60 0.00 0.00 0.11
ChatGLM (6B) 63.53 50.53 47.50/47.50/46.67 2.89 41.82 4.00 0.00 0.00 0.00
LLaMA 2 (7B) 50.06 51.39 48.83/48.83/50.83 6.17 42.23 3.80 0.00 0.00 0.11
LLaMA (7B) 47.86 67.84 54.17/52.50/51.67 5.94 14.18 1.60 0.00 0.00 0.11
Falcon (7B) 53.24 68.04 50.00/50.83/50.00 6.71 37.41 1.00 0.00 0.00 0.00
Pythia (12B) 54.47 65.78 49.17/48.33/49.17 6.59 27.09 0.40 0.00 0.00 0.00
Pythia (7B) 50.92 64.79 51.67/49.17/50.00 13.02 25.84 0.20 0.00 0.00 0.00
perplexity) for CrowS-Pairs, coreference resolution accuracy
(he/she/they) for WinoGender, toxicity score for RealToxi-
tyPrompts, and average accuracy of recognizing hallucina-
tions for HaluEval. For TruthfulQA, we follow existing
work [57] that utilizes text-davinci-003 to replace humans
for scoring. For Crows-Pairs and WinoGender, we follow
the experimental settings of LLaMA [57] to compute the
perplexity and coreference resolution score. For RealTox-
ityPrompts, we utilize the Perspective-API 42 for toxicity
42. https://perspectiveapi.com/
evaluation.
• Interaction with environment. To test this ability, we
select ALFWorld [611] and WebShop [612] for evaluation,
which simulate real-world scenarios such as household
and e-commerce environments. We follow the setting of
ReAct [442] that evaluate the 1-shot and 2-shot performance
of LLMs on WebShop and ALFWorld respectively, and com-
pute success rate for ALFWorld and average score/success rate
for WebShop. Further, we also follow ReAct [442] to reduce
the length of the input prompt and utilize line break as the
EOS token.



### Claim 7/24

#### Claim Text
In fields such as ML/AI and computer security, recent initiatives have asked authors to provideethics statements [59], broader impact statements [118], and other structured ways of reflecting on the consequences of their work.

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 17):

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Kapania and Wang, et al.
6 LIMITATIONS AND FUTURE WORK
Our study sheds light on the emerging practices regarding LLM ethics among HCI researchers, but it
has limitations due to its exploratory nature. Firstly, while we sought to offer a deep understanding
on how HCI researchers navigate LLM ethics, our sample was constrained by our snowball sampling
method. HCI research encompasses a wide range of diverse research traditions, many of which we
were unable to include in our study. This limitation highlights the need for more comprehensive and
systematic future studies in this area. Our interview sample primarily consists of researchers from
the USA, and future research may want to further explore the ethical challenges and practices related
to the use of LLMs by researchers from other regions. Secondly, there’s a potential selection bias in
our study: we may have primarily attracted respondents who are conscious of their LLM usage
and are open to discussing their experiences in a research setting. To gain a broader perspective,
future research should explore how ethical practices with LLMs vary across different research
methodologies, domains, and settings, including both industry and academia.
7 CONCLUSION
In this paper, we drew empirical data from a survey and interviews to explore how HCI researchers
have currently integrated LLMs into their research practices, what ethical concerns they have
encountered as well as how they’ve navigated those concerns. Our results suggested that although
HCI researchers have used LLMs across their research processes and are aware of a wide variety
of ethical concerns, in many cases, they have challenges in effectively identifying and navigating
those concerns in their own projects. Reflecting on these findings, we discuss potential approaches
to support the formation of emerging ethical norms for using LLMs in HCI research. We encourage
HCI researchers to proactively engage with IRB and collaborate with policymakers and generative
AI companies on creating guidelines for the responsible use of LLMs. We also identify the need
to re-examine the informed consent process and provide technological support to interrupt the
LLM supply chain. In addition, we discuss the importance of creating learning opportunities for
the ethics of LLMs use in HCI and shifting academic incentives to prioritize ethical concerns.
REFERENCES
[1] Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi
Iqbal, Paul N. Bennett, Kori Inkpen, Jaime Teevan, Ruth Kikin-Gil, and Eric Horvitz. 2019. Guidelines for Human-AI
Interaction. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk)
(CHI ’19) . Association for Computing Machinery, New York, NY, USA, 1–13. https://doi.org/10.1145/3290605.3300233
[2] Alissa N Antle. 2017. The ethics of doing research with vulnerable populations. Interactions 24, 6 (2017), 74–77.
[3] Christopher Bail. 2023. Can generative artificial intelligence improve social science.
[4] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers
of stochastic parrots: Can language models be too big?. In Proceedings of the 2021 ACM conference on fairness,
accountability, and transparency . 610–623.
[5] ACM Publications Board. 2021. ACM Publications Policy on Research Involving Human Participants and Subjects.
https://www.acm.org/publications/policies/research-involving-human-participants-and-subjects. (Accessed on
01/16/2024).
[6] Claudi L. Bockting, Eva A. M. van Dis, Robert van Rooij, Willem Zuidema, and Johan Bollen. 2023. Living guidelines
for generative AI — why scientists must oversee its use. Nature 622, 7984 (Oct. 2023), 693–696. https://doi.org/10.
1038/d41586-023-03266-1
[7] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3,
2 (2006), 77–101.
[8] Virginia Braun and Victoria Clarke. 2019. Reflecting on reflexive thematic analysis. Qualitative research in sport,
exercise and health 11, 4 (2019), 589–597.
[9] Barry Brown, Alexandra Weilenmann, Donald McMillan, and Airi Lampinen. 2016. Five Provocations for Ethical HCI
Research. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (San Jose, California,
USA) (CHI ’16). Association for Computing Machinery, New York, NY, USA, 852–863. https://doi.org/10.1145/2858036.
18



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 16):

Examining ethics of LLM use in HCI research practices Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
and tools that empower researchers to host and manage their own LLMs for research. There has
been research providing a comprehensive and practical guide for practitioners and end-users to
work with LLMs [113], with a focus on closed-source LLMs such as ChatGPT. In order to offer
more autonomy and control for researchers in integrating LLMs into their research workflow,
new technological support in streaming that process is needed. By providing these capabilities,
researchers would be able to actively engage with and mitigate the ethical concerns.
Creating learning opportunities and materials for ethics of LLM use in HCI.Researchers’
awareness of ethical issues related to LLMs influenced how they address these considerations in
their projects. Many researchers spoke of LLMs as an unfamiliar territory, lacking the training and
well-established guidelines to approach ethical considerations with this emerging technology. We
could draw on lessons from scholarship in FAccT, STS, NLP and other allied domains, as well as
industry resources, to develop learning opportunities, such as toolkits [92], guidebooks [80], or
frameworks [30, 76], for understanding and addressing ethical concerns with using LLMs. Achieving
this requires particular attention from the HCI community to collaboratively explore how LLMs
are used in different stages of HCI research.
Conferences are a valuable starting point to promote cross-institutional learning. We propose
organizing workshops and panels (such as [93]) with interdisciplinary experts in sociotechnical
understanding of LLMs to raise awareness on the impacts of using LLMs. Additionally, we call for
creating and disseminating case studies (e.g., [79]), potentially included in newsletters, illustrating
how HCI researchers actively address ethical concerns related to LLMs. A repository of case studies,
covering diverse HCI domains and epistemologies, might also offer resources for preventing and
mitigating ethical concerns. Education on research ethics for LLM use should be a necessary
component of HCI research and practice, given the well-documented risks and adverse impacts
of LLMs [107]. One avenue is incorporating such case studies into HCI curriculum, especially if
targeted towards research students, to provide real-world examples that help build their awareness
of these issues.
Shifting academic incentives to foreground ethical concerns.Throughout the interviews,
we observed that researchers could generally foresee potential ethical concerns with LLMs, often
drawing from their familiarity with the literature or discussions with other researchers. Nonetheless,
some participants went on to articulate their reason for not prioritizing these ethical considerations
in their projects. Researchers described how constraints such as limited funding, pressure to
publish, conference deadlines, often came in the way of focusing on ethical concerns. The need to
address ethical issues could then be relegated to the limitations or future work section, reflecting
broader perceptions within Computer Science research that ethics are often viewed as secondary
considerations. Indeed, Do and Pang et al. [32] discussed how academics may have lower incentive
to examine unintended consequences of their research (also observed in our study) in contrast with
industry practices.
There is a renewed urgency to reconsider research ethics practices within HCI and how we
navigate challenges presented by emerging technologies such as LLMs. We take inspiration from
Do and Pang et al. [32] and Soergel et al. [99] to propose starting points for changing structural
incentives within academia. Publications and citations are the currency and means for upward
mobility in research. Could we shift criteria for recognition and funding to explicitly recognize
attention to ethical considerations in research practices? Publishing organizations, funding agencies,
and regulatory institutions will pay a crucial role in reshaping incentives. Most importantly,
HCI researchers must acknowledge the ways in which they actively shape the discourse around
associated risks and ethical considerations through their research, and work actively towards a
cultural shift that demonstrates a commitment to ethical use of LLMs.
17



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 173):

The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery
discussed, although the focus on theoretical and empirical analysis may
have minimal direct societal consequences."
],
"Ethical Concerns": false,
"Soundness": 2,
"Presentation": 2,
"Contribution": 2,
"Overall": 3,
"Confidence": 4,
"Decision": "Reject"
174



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 10):

Examining ethics of LLM use in HCI research practices Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
4.2.4 Overtrust and overreliance on LLMs.Participants also noted ethical considerations of overtrust
and overreliance on LLMs. They expressed concerns that research participants who were directly
interacting with LLMs and were unaware of their biases could overestimate LLM capabilities and
place unwarranted trust in their suggestions. P13, who built LLM-powered creativity support tools,
was concerned that“[research participants] may be misled by the content, even if [the content] contains
wrong information or wrong references. ” They also expressed concern about researchers’ overreliance
on LLMs, which could compromise the quality and creativity of HCI research. P3, using LLMs
for creative ideation for their research, warned about the risk that researchers might uncritically
accept the information provided by LLMs as factual and integrate them into their research plans.
Participants mentioned how the overuse of LLMs among HCI researchers could impede creativity
(e.g., “LLMs are useful but not creative” (P15)) and raise questions about the long-term impact of
such overreliance on LLMs on trust within the academic community.
4.2.5 Environmental and societal impacts. Interview participants identified a range of other higher-
level ethical considerations that extended beyond their specific research project or pipeline. Some
researchers were concerned about the environmental degradation due to the extensive electricity
usage and hardware deployment of building larger LLMs. For instance, P2 mentioned “we have all
these models that are competing against each other, that’s like millions of dollars of electricity and
computer components, [which is] really bad for the environment” . Participants articulated anxieties
that the wide adoption of LLMs could lead to inequitable distribution of benefits– “we certainly
have not addressed the social issues. I think a lot of people will, if not losing their jobs, be substantially
diminished in terms of their utility to their workplace”.
4.3 How do HCI researchers approach the ethical concerns of LLMs?
In our study, we focused on understanding how HCI researchers are currently navigating ethical
considerations about the use of LLMs in their projects. For the majority of HCI researchers in our
interview study, the ethical concerns they discussed (Section 4.2) remained largely speculative.
Although our participants reported their awareness of a diverse set of potential ethical concerns
around the use of LLMs, they were either unable or only partially able to identify or address those
ethical concerns in their own projects. Researchers described their current strategies towards ethical
concerns– often manifesting as forms of inactions or workarounds – such as engaging LLM ethics in
a conditional and reactive manner, inconsistent disclosure practices, limiting their LLM use and
reflecting on their co-creation process, as well as delaying responsibility in research accountability.
In most cases, these strategies did not directly address the underlying ethical challenges; instead,
they served as temporary fixes to manage immediate concerns. Below, we capture these varied
strategies to LLM ethics and return to their implications in section 5.
4.3.1 Conditional and reactive engagement with LLM ethics.HCI researchers often emphasized that
the need to and approach towards addressing ethical concerns was conditional on various factors
such as the use casse or domains of study. Many HCI researchers invoked the specifics of their
research domain to justify why they did not need to foreground ethical concerns. For example, when
participants categorized their research as low-stakes, they spoke of how commonly anticipated
ethical issues associated with LLMs were not applicable to their work, and they did not need to take
proactive measures. For instance, P11, who focused on creativity support in writing, considered
their research non-sensitive and did not find it necessary to intervene to prevent unsafe generated
text. Some participants believed that the potential harms caused by LLMs were comparable to those
of social media. Therefore, they felt that engaging in ethical considerations was unnecessary if
LLMs were not being used in high-stakes domains, such as providing medical advice: “typical LLM
like ChatGPT was created to support people browsing the internet. People already see toxic content
11



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 4):

Examining ethics of LLM use in HCI research practices Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
including integrating a misinformation detector [82] and redesigning prompting methods to guide
LLMs [82, 114]. Mozes et al. [69] presented prevention measures for when LLMs were used for
illicit purposes, arguing that red-teaming, safeguarding with RLHF and instruction following, and
avoiding memorization and poisoning are potential solutions to the misuse of LLMs. Regarding
human-computer interaction harms of LLMs, Liao and Vaughan [60] provided a roadmap for
improving the transparency and explainability of LLMs.
While recent efforts have begun to mitigate the broader ethical concerns of LLMs, the application
of LLMs in HCI research presents unique ethical challenges. For instance, biases in the training
data can adversely affect research practices, leading to issues with internal and external validity,
reproducibility, efficiency, and the risk of proliferating low-quality research [3]. Despite this, there is
a notable gap in understanding how HCI researchers perceive and manage these ethical challenges
in their day-to-day research activities. This study aims to fill this gap by exploring the unique
ethical issues posed by the use of LLMs in HCI research and examining how researchers in this
field are currently addressing these challenges.
3 METHODS
In this research, we focus on examining the ways in which HCI researchers apply large language
models (LLMs) across their research workflows and their ethical considerations for using LLM-based
tools. For a holistic view of LLM use practices, we employed a mixed-method approach with a
sequential explanatory design [51]. This involved conducting a survey with the goal of eliciting
broad-brushed and higher-level perspectives from a wide audience. The survey was followed by
semi-structured interviews to investigate, in more detail, the ways in which researchers approach
the ethical considerations of using LLMs as part of their research activities. The qualitative data
from the interviews was used to elaborate and explain the survey results (e.g., the rationale behind
their approach to engaging with ethical concerns) and served as the foundation for our inquiry
[51]. Our research study was reviewed and approved by the IRB at our institution. We present our
approaches to the survey and interviews in the following subsections.
3.1 Survey
The goal of the survey was to identify the ways in which HCI researchers are using LLMs and any
ethical considerations they have encountered in their projects. We conducted the survey using an
online questionnaire implemented in Qualtrics and analyzed responses from 50 respondents.
Participant recruitment.We recruited survey participants through multiple channels: ad-
vertising on social media networks such as Twitter and LinkedIn, emailing direct contacts, and
leveraging university distribution lists to which we had access. We began the survey by eliciting
informed consent from respondents. No personally identifiable information was recorded about
the respondents in accordance with our organization’s research privacy and ethics guidelines. The
inclusion criteria for our survey were similar to the interviews, where we recruited researchers
who are currently studying or working in areas related to Human-Computer Interaction (HCI) and
have used large language models (LLMs) in their research.
After the screening questions, we were left with n = 77 participants. Out of the 77 respondents,
50 completed all sections except for the demographics (which was optional). Among the 43 survey
respondents who filled in the demographic questions, most reported working in academia (34),
industry (6), and non-profit (3) organizations. Researchers worked on projects within Human-AI
Interaction (32), Design (13), Understanding People: Theory, Concepts, and Methods (12), Collabora-
tive and Social Computing (10), and User Experience and Usability (10). In our sample, respondents
were located in the United States (20), Afghanistan (5), Germany (3), Algeria (2), Hong Kong (4),
5



### Claim 8/24

#### Claim Text
Ethics statements were discussed among HCI researchers in 2018 [59], but to-date have not been formally standardized in CHI’s submission process; however, they have been used in ML and AI conferences including NeurIPS and FAccT [118].

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 17):

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Kapania and Wang, et al.
6 LIMITATIONS AND FUTURE WORK
Our study sheds light on the emerging practices regarding LLM ethics among HCI researchers, but it
has limitations due to its exploratory nature. Firstly, while we sought to offer a deep understanding
on how HCI researchers navigate LLM ethics, our sample was constrained by our snowball sampling
method. HCI research encompasses a wide range of diverse research traditions, many of which we
were unable to include in our study. This limitation highlights the need for more comprehensive and
systematic future studies in this area. Our interview sample primarily consists of researchers from
the USA, and future research may want to further explore the ethical challenges and practices related
to the use of LLMs by researchers from other regions. Secondly, there’s a potential selection bias in
our study: we may have primarily attracted respondents who are conscious of their LLM usage
and are open to discussing their experiences in a research setting. To gain a broader perspective,
future research should explore how ethical practices with LLMs vary across different research
methodologies, domains, and settings, including both industry and academia.
7 CONCLUSION
In this paper, we drew empirical data from a survey and interviews to explore how HCI researchers
have currently integrated LLMs into their research practices, what ethical concerns they have
encountered as well as how they’ve navigated those concerns. Our results suggested that although
HCI researchers have used LLMs across their research processes and are aware of a wide variety
of ethical concerns, in many cases, they have challenges in effectively identifying and navigating
those concerns in their own projects. Reflecting on these findings, we discuss potential approaches
to support the formation of emerging ethical norms for using LLMs in HCI research. We encourage
HCI researchers to proactively engage with IRB and collaborate with policymakers and generative
AI companies on creating guidelines for the responsible use of LLMs. We also identify the need
to re-examine the informed consent process and provide technological support to interrupt the
LLM supply chain. In addition, we discuss the importance of creating learning opportunities for
the ethics of LLMs use in HCI and shifting academic incentives to prioritize ethical concerns.
REFERENCES
[1] Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi
Iqbal, Paul N. Bennett, Kori Inkpen, Jaime Teevan, Ruth Kikin-Gil, and Eric Horvitz. 2019. Guidelines for Human-AI
Interaction. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk)
(CHI ’19) . Association for Computing Machinery, New York, NY, USA, 1–13. https://doi.org/10.1145/3290605.3300233
[2] Alissa N Antle. 2017. The ethics of doing research with vulnerable populations. Interactions 24, 6 (2017), 74–77.
[3] Christopher Bail. 2023. Can generative artificial intelligence improve social science.
[4] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers
of stochastic parrots: Can language models be too big?. In Proceedings of the 2021 ACM conference on fairness,
accountability, and transparency . 610–623.
[5] ACM Publications Board. 2021. ACM Publications Policy on Research Involving Human Participants and Subjects.
https://www.acm.org/publications/policies/research-involving-human-participants-and-subjects. (Accessed on
01/16/2024).
[6] Claudi L. Bockting, Eva A. M. van Dis, Robert van Rooij, Willem Zuidema, and Johan Bollen. 2023. Living guidelines
for generative AI — why scientists must oversee its use. Nature 622, 7984 (Oct. 2023), 693–696. https://doi.org/10.
1038/d41586-023-03266-1
[7] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3,
2 (2006), 77–101.
[8] Virginia Braun and Victoria Clarke. 2019. Reflecting on reflexive thematic analysis. Qualitative research in sport,
exercise and health 11, 4 (2019), 589–597.
[9] Barry Brown, Alexandra Weilenmann, Donald McMillan, and Airi Lampinen. 2016. Five Provocations for Ethical HCI
Research. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (San Jose, California,
USA) (CHI ’16). Association for Computing Machinery, New York, NY, USA, 852–863. https://doi.org/10.1145/2858036.
18



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 16):

Examining ethics of LLM use in HCI research practices Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
and tools that empower researchers to host and manage their own LLMs for research. There has
been research providing a comprehensive and practical guide for practitioners and end-users to
work with LLMs [113], with a focus on closed-source LLMs such as ChatGPT. In order to offer
more autonomy and control for researchers in integrating LLMs into their research workflow,
new technological support in streaming that process is needed. By providing these capabilities,
researchers would be able to actively engage with and mitigate the ethical concerns.
Creating learning opportunities and materials for ethics of LLM use in HCI.Researchers’
awareness of ethical issues related to LLMs influenced how they address these considerations in
their projects. Many researchers spoke of LLMs as an unfamiliar territory, lacking the training and
well-established guidelines to approach ethical considerations with this emerging technology. We
could draw on lessons from scholarship in FAccT, STS, NLP and other allied domains, as well as
industry resources, to develop learning opportunities, such as toolkits [92], guidebooks [80], or
frameworks [30, 76], for understanding and addressing ethical concerns with using LLMs. Achieving
this requires particular attention from the HCI community to collaboratively explore how LLMs
are used in different stages of HCI research.
Conferences are a valuable starting point to promote cross-institutional learning. We propose
organizing workshops and panels (such as [93]) with interdisciplinary experts in sociotechnical
understanding of LLMs to raise awareness on the impacts of using LLMs. Additionally, we call for
creating and disseminating case studies (e.g., [79]), potentially included in newsletters, illustrating
how HCI researchers actively address ethical concerns related to LLMs. A repository of case studies,
covering diverse HCI domains and epistemologies, might also offer resources for preventing and
mitigating ethical concerns. Education on research ethics for LLM use should be a necessary
component of HCI research and practice, given the well-documented risks and adverse impacts
of LLMs [107]. One avenue is incorporating such case studies into HCI curriculum, especially if
targeted towards research students, to provide real-world examples that help build their awareness
of these issues.
Shifting academic incentives to foreground ethical concerns.Throughout the interviews,
we observed that researchers could generally foresee potential ethical concerns with LLMs, often
drawing from their familiarity with the literature or discussions with other researchers. Nonetheless,
some participants went on to articulate their reason for not prioritizing these ethical considerations
in their projects. Researchers described how constraints such as limited funding, pressure to
publish, conference deadlines, often came in the way of focusing on ethical concerns. The need to
address ethical issues could then be relegated to the limitations or future work section, reflecting
broader perceptions within Computer Science research that ethics are often viewed as secondary
considerations. Indeed, Do and Pang et al. [32] discussed how academics may have lower incentive
to examine unintended consequences of their research (also observed in our study) in contrast with
industry practices.
There is a renewed urgency to reconsider research ethics practices within HCI and how we
navigate challenges presented by emerging technologies such as LLMs. We take inspiration from
Do and Pang et al. [32] and Soergel et al. [99] to propose starting points for changing structural
incentives within academia. Publications and citations are the currency and means for upward
mobility in research. Could we shift criteria for recognition and funding to explicitly recognize
attention to ethical considerations in research practices? Publishing organizations, funding agencies,
and regulatory institutions will pay a crucial role in reshaping incentives. Most importantly,
HCI researchers must acknowledge the ways in which they actively shape the discourse around
associated risks and ethical considerations through their research, and work actively towards a
cultural shift that demonstrates a commitment to ethical use of LLMs.
17



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 2):

Examining ethics of LLM use in HCI research practices Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
2 RELATED WORK
2.1 The Use of LLMs in HCI Research
As the capabilities of generative AI in understanding context and generating natural language
responses continue to advance [ 77], large language models are being increasingly used by re-
searchers as tools in Human-Computer Interaction (HCI) research. Emerging work has explored
the potential of leveraging LLMs to support the process of brainstorming and identifying research
topics [47, 89, 100]. Tools like CoQuest [61] have been developed for research question development
through human-agent co-creation. In addition to research ideation, LLMs have been used as tools
in human-AI co-creative design ideation [64, 103] and writing support [116].
Other members of the community have made efforts to develop LLM-powered applications for
data generation and data analysis [83]. Hämäläinenet al. [48] explored the potential of utilizing LLMs
to produce synthetic user interview transcripts for piloting research ideas and designing interview
protocols. Weiet al. [105] created LLM-based chatbots for generating synthesized user self-reported
data. Researchers have also developed LLM-based applications for qualitative analysis, including
identifying themes and generating codebooks [ 16, 45, 101]. Prior work has also explored using
LLMs for deductive coding [26] and for human-AI collaborative qualitative analysis [44, 45, 111]. In
terms of quantitative analysis, tools such as Github Copilot1 and GPT-4 [77] can enable researchers
to transform natural language instructions into programming codes that assist quantitative data
analysis and visualization. LLMs have been used as tools for quantitative data analysis in sampling,
filtering, and analyzing survey data [52], gaining insights from large corpus [78] and crowdsourcing
social data [34].
Researchers have also used LLMs as the underlying technology for system design and develop-
ment. Lu et al. [62] and Petridis et al. [86] explored infusing prompt-based prototyping enabled by
LLM into functional user interface design. Researchers also utilized LLMs to build task-oriented
social simulations for LLM agents, aiming to study the intricate social dynamics of societal sys-
tems [43, 58, 85]. Other applications of integrating LLMs into the design and development of HCI
systems have focused on leveraging LLMs for efficient prototyping [57, 109] that can be potentially
used by HCI researchers and designers. Our research extends this scholarship by empirically ex-
amining practices and ethical challenges of integrating large language models into HCI research
projects.
2.2 Research Ethics in HCI
The Human-Computer Interaction (HCI) community has long been engaged in discussions regarding
ethics [36, 88], revolving around responsible conduct in human subjects studies [11, 90], including
privacy, informed consent, and institutional review boards (IRBs). These concerns are integral to
ensuring ethical conduct and preventing harms to research participants [9]. Existing HCI guidelines
of privacy focus on the protection of participants’ sensitive information, the anonymity of the
participants, and the confidentiality of collected data [15, 118]. The rights of autonomy and self-
determination of participants require HCI researchers to establish transparent procedures for
informed consent on the collection and analysis of personal information [41]. IRBs play a critical
role in ensuring that HCI researchers meet their ethical obligations when conducting studies with
human subjects [5, 14]. Moreover, prior scholarship underscores the need to mitigate potential
biases in research design, data collection, and data analysis by refining existing ethical guidelines
and protocols [71].
The process of creating ethical guidelines for emerging technologies within the HCI community
has drawn inspiration from ethical theories and professional standards. Mackay [65] highlighted
1https://github.com/features/copilot
3



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 4):

Examining ethics of LLM use in HCI research practices Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
including integrating a misinformation detector [82] and redesigning prompting methods to guide
LLMs [82, 114]. Mozes et al. [69] presented prevention measures for when LLMs were used for
illicit purposes, arguing that red-teaming, safeguarding with RLHF and instruction following, and
avoiding memorization and poisoning are potential solutions to the misuse of LLMs. Regarding
human-computer interaction harms of LLMs, Liao and Vaughan [60] provided a roadmap for
improving the transparency and explainability of LLMs.
While recent efforts have begun to mitigate the broader ethical concerns of LLMs, the application
of LLMs in HCI research presents unique ethical challenges. For instance, biases in the training
data can adversely affect research practices, leading to issues with internal and external validity,
reproducibility, efficiency, and the risk of proliferating low-quality research [3]. Despite this, there is
a notable gap in understanding how HCI researchers perceive and manage these ethical challenges
in their day-to-day research activities. This study aims to fill this gap by exploring the unique
ethical issues posed by the use of LLMs in HCI research and examining how researchers in this
field are currently addressing these challenges.
3 METHODS
In this research, we focus on examining the ways in which HCI researchers apply large language
models (LLMs) across their research workflows and their ethical considerations for using LLM-based
tools. For a holistic view of LLM use practices, we employed a mixed-method approach with a
sequential explanatory design [51]. This involved conducting a survey with the goal of eliciting
broad-brushed and higher-level perspectives from a wide audience. The survey was followed by
semi-structured interviews to investigate, in more detail, the ways in which researchers approach
the ethical considerations of using LLMs as part of their research activities. The qualitative data
from the interviews was used to elaborate and explain the survey results (e.g., the rationale behind
their approach to engaging with ethical concerns) and served as the foundation for our inquiry
[51]. Our research study was reviewed and approved by the IRB at our institution. We present our
approaches to the survey and interviews in the following subsections.
3.1 Survey
The goal of the survey was to identify the ways in which HCI researchers are using LLMs and any
ethical considerations they have encountered in their projects. We conducted the survey using an
online questionnaire implemented in Qualtrics and analyzed responses from 50 respondents.
Participant recruitment.We recruited survey participants through multiple channels: ad-
vertising on social media networks such as Twitter and LinkedIn, emailing direct contacts, and
leveraging university distribution lists to which we had access. We began the survey by eliciting
informed consent from respondents. No personally identifiable information was recorded about
the respondents in accordance with our organization’s research privacy and ethics guidelines. The
inclusion criteria for our survey were similar to the interviews, where we recruited researchers
who are currently studying or working in areas related to Human-Computer Interaction (HCI) and
have used large language models (LLMs) in their research.
After the screening questions, we were left with n = 77 participants. Out of the 77 respondents,
50 completed all sections except for the demographics (which was optional). Among the 43 survey
respondents who filled in the demographic questions, most reported working in academia (34),
industry (6), and non-profit (3) organizations. Researchers worked on projects within Human-AI
Interaction (32), Design (13), Understanding People: Theory, Concepts, and Methods (12), Collabora-
tive and Social Computing (10), and User Experience and Usability (10). In our sample, respondents
were located in the United States (20), Afghanistan (5), Germany (3), Algeria (2), Hong Kong (4),
5



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 13):

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Kapania and Wang, et al.
using LLM-based tools or methods. After an internal discussion with their team, P9 too, decided
not to use LLMs for qualitative analysis. A collaborative decision-making process was a common
practice to determine where it is appropriate to use LLMs.
4.3.4 Delaying responsibility in research accountability.Finally, our participants indicated that
determining who bears responsibility for the ethical implications of using LLMs is challenging. At
times, they expressed reluctance towards stringent regulatory guidelines and opted to postpone
dealing with ethical issues in their projects.
In discussions about accountability with the use of LLMs within HCI research, participants
noted the ambiguity in determining where responsibility lies regarding the ethical concerns of
using LLMs. They highlighted the notion of distributed responsibility across the AI supply chain,
emphasizing that multiple stakeholders share the responsibility for ensuring that the use of LLMs
within research is ethical. Participants highlighted the LLM provider’s responsibility to implement
safeguards preventing users from sending sensitive personal data to the model. For instance, P8
discussed their confidence in LLM providers, sharing how they “believe OpenAI has done a decent
job in terms of making a model safe in general. ” P2 presented a similar perspective:
“When professionally something wrong happens, it needs to follow the chain of command.
So, who is the person most directly responsible? It’s actually the user, right? The user did
something with my system, and it created a harmful output, and then the user will say,
well, the system wasn’t designed well enough for me to know it’s gonna create this harmful
output. I should have been warned. In that case, the responsibility falls on me. And then I
could move it up the chain of command to say my advisor shouldn’t have let me release
this system if it was gonna produce harmful output or I could say OpenAI is irresponsible
for releasing a project that they are publicizing as being broadly accessible and safe. ”
Partially due to the difficulty in determining “distributed responsibility” for some HCI researchers,
addressing ethical concerns could be relegated to future work. They viewed it as imperative, as
HCI researchers focused on system-building, to explore emerging technologies and develop new
tools and interactions with them. As articulated by P6, “we are the system builders, and we think
about what are the unique benefits that the new technology can bring to the users and enhance the user
capabilities. ”This perspective underscored the desire to continue building tools, even as researchers
acknowledged the biases associated with LLMs.
Sometimes, participants even expressed a resistance towards prescriptive regulations, which they
perceived would suppress innovation in HCI research. They emphasized that external constraints
on LLM usage, especially given its role in everyday research practices, would slow down their
research. LLMs were also perceived to lack the stability necessary for regulation. As P12 described,
“the challenge is the regulation needs to come much later rather than early. If you come up with
regulations too early, you may kill innovation and on the other hand, you don’t know what you want
to regulate because the representation of the product hasn’t been stabilized yet. ”
5 DISCUSSION
Our results highlight the various ways in which HCI researchers integrated LLMs in their research
projects, for the purpose of research ideation (e.g., finding novel research areas), data collection and
analysis (e.g., qualitative coding), preparing artifacts (e.g., drafting research publications), and more.
Many participants were aware of the potential ethical concerns related to the use LLMs in HCI
research, including the harms of engaging with LLM outputs, threats to the privacy of participant
data, intellectual integrity, and environmental and societal impacts. However, the sociotechnical
assemblage of LLMs mediated researchers’ diverse approaches to these ethical concerns.
14



### Claim 9/24

#### Claim Text
LLMs also run the risk of misrepresenting people and are unlikely to faithfully portray identity groups due to the nature of their training data [157].

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 0):

Large language models should not replace human
participants because they can misportray and flatten
identity groups
Angelina Wang1, Jamie Morgenstern2, John P. Dickerson3,4
1Computer Science, Stanford University, Palo Alto, CA, USA.
2Computer Science & Engineering, University of Washington, Seattle, WA, USA.
3Computer Science, University of Maryland, College Park, MD, USA.
4Arthur, New York City, NY, USA.
Contributing authors: angelina.wang@stanford.edu; jamiemmt@cs.washington.edu;
john@arthur.ai;
Abstract
Large language models (LLMs) are increasing in capability and popularity, propelling their appli-
cation in new domains—including as replacements for human participants in computational social
science, user testing, annotation tasks, and more. In many settings, researchers seek to distribute
their surveys to a sample of participants that are representative of the underlying human popu-
lation of interest. This means in order to be a suitable replacement, LLMs will need to be able
to capture the influence of positionality (i.e., relevance of social identities like gender and race).
However, we show that there are two inherent limitations in the way current LLMs are trained
that prevent this. We argue analytically for why LLMs are likely to both misportray and flat-
ten the representations of demographic groups, then empirically show this on 4 LLMs through
a series of human studies with 3200 participants across 16 demographic identities. We also dis-
cuss a third limitation about how identity prompts can essentialize identities. Throughout, we
connect each limitation to a pernicious history that explains why it is harmful for marginalized
demographic groups. Overall, we urge caution in use cases where LLMs are intended to replace
human participants whose identities are relevant to the task at hand. At the same time, in cases
where the goal is to supplement rather than replace (e.g., pilot studies), we provide inference-time
techniques that we empirically demonstrate do reduce, but do not remove, these harms.
Keywords: large language model limitations, human participants, representative sampling, standpoint
epistemology
Large language models (LLMs) are proliferating, and increasingly touted as being able to replace more
costly human participants in domains such as user studies [1], annotation tasks [2], computational
social science [3], and opinion surveys [4]. However, in the excitement one of the biggest challenges
in human participant recruitment is often forgotten: representative sampling [5]. Even in cases where
representative sampling is not explicitly pursued, each participant’s demographic identity is often
collected out of recognition that a person’s perspective is influenced by their standpoint and social
experience [6, 7]. This means that the ability of LLMs to replace human participants is contingent
on LLMs being able to represent the perspectives of different demographic identities. Prior work has
speculated that LLMs’ vast training data enables it to perform such representation [8]. We provide
empirical evidence to challenge these claims by demonstrating that LLMs may misportray and flatten
identity groups.
1
arXiv:2402.01908v2  [cs.CY]  1 Oct 2024



Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 5):

Fig. 3: Identity-coded names compared to explicit identity label. Same interpretation as
Fig. 2, where positive values for each of the six metrics indicate the LLM response is more similar to
out-group imitations than in-group representations, and circles signify statistical significance while
crosses do not. For each identity, the prompt contains the explicit identity label (Iden), or one of the
two identity-coded names (Name 0 or Name 1). For Black men and Black women, identity-coded
names tend to generate more in-group-aligned portrayals than do explicit identity labels, as shown
by more negative values.
LLMs flatten groups and portray them one-dimensionally
Our next analysis considers whether LLMs flatten groups and portray them homogeneously. Human
participants are rarely solicited to understand just one opinion, but rather to understand the diversity
(e.g., variance) of perspectives on a topic. Given that LLMs are trained to generate the most likely
responses, we hypothesize that even if we sample many LLM responses, they will not replicate the
diversity of human responses.
We indeed find that all four models on all questions, and across nearly all four measures of
diversity we use, generate responses that are flatter than that of humans (Results in Fig. 4). GPT-4
and 3.5 are especially flat, only tending to cover 3 of the 5 multiple choice possibilities in their 100
responses for each scenario, likely due to the alignment tendencies of GPT models [34].
Reason for Harm: History of Ignoring Within-Group Heterogeneity
LLMs condensing knowledge into small sets of responses is not inherently harmful—in fact, arguably
it is one of the selling points of LLMs’ capabilities. However, if LLMs are used to replace human par-
ticipants of different demographic groups, then this flattening becomes particularly harmful towards
marginalized groups that are historically portrayed as one dimensional (e.g., Black people) [50]. In
fact, it is this one dimensionality that has sometimes precluded intersectionality, by failing to rec-
ognize within-group heterogeneity (e.g., that within women, Black women have different experiences
than White women) [9, 10].
One example is on the R1-Contingent question about being non-binary. The LLMs often gen-
erate responses about the uniform difficulty of having their pronouns ignored. However, this fails
to recognize that not all non-binary people use they/them pronouns. For example, in-group human
participants bring up this complexity: “There are many misconceptions about pronouns and who
‘qualifies’ in terms of socially accepted norms and optics to even be considered non-binary,” and
“It’s a bit complicated. I identify as transmasculine and use both he/him and they/them pronouns.”
LLM-generated responses fail to recognize this nuance.
Alternative: Higher Temperatures
For a harm reduction technique, we consider temperature tuning. Temperature is a hyperparameter
set during the decoding process that roughly controls the amount of “randomness” in an LLM
6



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 9):

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Kapania and Wang, et al.
“gender bias in the entire computational pipeline– in places that we’ve never had gender bias before
because we’ve never used LLMs before, and because we used to come up with our own solutions”
Participants pointed out the tendency of LLMs towards generating homogenous content, where
the model would prioritize generalization and converge diverse perspectives into standardized
outputs. P8, working on sensemaking tools for product selection using LLMs, described how they
“want the selection criteria to encompass a diverse set of opinions rather than just focus on the perspective
of a single demographic. ” This tendency of LLMs towards “flattening human diversity and nuance”
(P9) raised concerns among researchers who emphasized the importance of capturing the complexity
of lived experiences within the context of their research. Participants also noted concerns that using
LLMs for editing their paper drafts could influence researchers’ paper writing styles, streamlining
towards homogeneous and “bland way of writing” (P9). Participants also discussed how LLMs’
training data, often reflecting Western morals and values, could contribute to this homogeneity.
4.2.2 Threats to privacy of participant data.Researchers in our study expressed anxiety about
how data input into LLMs is being used by LLM providers and the potential for violations of
privacy. They worried about breaches of private or sensitive information involving various forms
of participant data, such as transcripts, audio recordings, and application-specific log data. P7
used LLMs for qualitative data analysis and discussed how “privacy issue is the main concern for
us because in many cases, the audio transcription is not supposed to be put into ChatGPT”. P16 also
shared the concern that users’ navigation data is extremely sensitive and could lead to material
physical harm if uploaded to LLMs.
Many participants emphasized their concern of confidential and personally identifiable infor-
mation leakage as a result of sharing user data with LLM providers. P8 mentioned the risk of
confidential information being incorporated into LLM training corpus and the potential for security
leaks, where “backend messes up and user’s private information shows up in someone else’s chat
history”. Relatedly, some participants expressed concerns about the possibility of unconsented data,
for example, “explicit sexual content” (P2), being a part of the training corpus for large multimodal
models. While some believed that manually erasing sensitive information could mitigate these
concerns, others relied on the efficacy of APIs in safeguarding user data.
4.2.3 Violations of intellectual integrity.Interview participants raised ethical concerns about the
intellectual integrity of using LLMs in HCI research. A central theme of these concerns was the
ambiguity of ownership of LLM-generated text and visuals. Many participants who were co-
creating with LLMs also highlighted the difficulty in attributing what portion of the content is
the researcher’s original work and what is generated by the LLMs when they are refining and
co-creating with the LLM outputs. Participants discussed the ambiguity related to plagiarism when
LLM outputs are part of the research contribution. For example, interviewee P10 would consider
crediting the LLM “where a substantial amount ends up in a publication” . While most participants
questioned the extent to which one can claim ownership of LLM-generated content, especially in
the paper writing stage, some believed that “personhood is pretty important for attribution” (P2).
In addition, HCI researchers also raised concerns about the reproducibility of the research results
obtained via LLMs, highlighting the potential for an illusion of efficacy if LLMs work well in some
cases but fail to generalize to others. P2 described using LLMs in HCI research as a “computational
Wizard of Oz” method and continued on to point out how the quick and opaque updates of LLMs
are another barrier to reproducibility, noting that researchers “don’t really have control of what
version of GPT they are talking to and something that might work in a previous version won’t work so
much in future versions” .
10



Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 3):

of these measures by showing multiple at a time, which may be contradictory. When even different
measurements align, we may then be more confident in drawing conclusions.
In Supplementary Sec. 2 we provide analyses establishing premises we take for granted: a) LLMs
output different responses when prompted with different identities [23], and b) in-group representa-
tions and out-group imitations from human participants are different. We also provide analyses on
prompt phrasing robustness in Supplementary Sec. 4.
LLMs can misportray marginalized groups as more like
out-group imitations than in-group representations
Our first analysis explores the question of whether LLMs are more like out-group imitations (e.g.,
White person speaking about or like a Black person) than in-group representations (e.g., Black person
speaking themselves). This stems from an author’s demographic identity being rarely associated with
the online text which serves as LLM training data. Instead, explicit identity mentions (e.g., “the
Asian person”) are more likely to be associated with text about that identity rather than from that
identity. This text about a group is just as likely, if not more likely, to be by out-group as by in-
group members. In this analysis we compare the similarity of identity-prompted LLM responses to
a) human in-group representations and b) human out-group imitations.
We show results on GPT-4 in Fig. 2, and find many instances where the LLM is more like out-
group imitations than in-group representations. We note that through our usage of six different
metrics to capture the same concept of similarity, we are surfacing a more conservative set of findings.4
Across all four LLMs on R1-Contingent a majority of metrics show the three personas of White
person (23 out of 24 measurement × model comparisons), non-binary person (16/24), and person
with impaired vision (18/24) as statistically significantly more like out-group imitations than in-
group representations. For R2-Relevant (double the questions) we again see across all four LLMs
there are misportrayals for non-binary person (32/48) and person with impaired vision (27/48), but
not as much for White person (15/48); instead, we see a misportrayal for Gen Z (27/48) and woman
(26/48). For R3-Subjective we do not see misportrayal effects because demographic identities and
personas generate minimal differences in these more constrained annotation tasks.
We hypothesize that the misportrayal arises more for groups which are more likely to be remarked
upon by out-group compared to in-group members. For example, White people rarely remark upon
their own racial identity since it is seen as the norm, whereas racial out-group members may be
likely to explicitly bring up someone’s Whiteness [37]. Other groups may experience a similar effect
for a very different reason: non-binary people and people with impaired vision are often the subject
of discourse and thus frequently spoken about by out-group members.
Reason for Harm: Speaking for Others
Misportrayal can be harmful for a number of reasons. For one, the differential between out-group
imitation and in-group representation has been shown to reinforce stereotypes [38].
For another, the practice of speaking for others has a pernicious history which can involve the
erasure and reinscription of social hierarchies [39, 40]. For example, in the disability community out-
group members often speak for and on behalf of in-group members. This has led to people with
autism’s preference for inclusionary accommodations and stigma reduction being neglected in favor
of the medical treatment that caretakers and relatives may advocate for [41, 42]. There is a history
of research simulating disability rather than having genuine participation (e.g., sighted people with
blindfolds rather than blind people), and these simulated groups do not interact with the world in a
way representative of genuinely disabled people [43, 44]. Given the harmful history of erasing people
with disabilities through simulation or speaking for, a history paralleled for other marginalized groups
like Black women [45], we should be careful to not repeat those mistakes with a new technology.
Instead, we should value lived experiences [46] and the epistemic authority they confer [47].
Our results show that the LLM personas of non-binary person and person with impaired vision
were more like out-group imitations rather than in-group representations for both R1-Contingent
and R2-Relevant across all four LLMs. Both of these groups are historically excluded and highly
4The measurement of “SBERT: Closest” takes the SBERT embedding of each natural language output, and for each LLM
embedding measures the distance to the nearest neighbor from the set of in-group embeddings and nearest neighbor from the
set of out-group embeddings. This was our original conception of the best operationalization for this analysis, and the results
of this metric alone show very strongly that LLMs consistently misportray most demographic groups. However, in the interest
of not overreporting difference, we instead consider six different metrics, accepting this strategy dilutes some of our findings.
4



Source: data\tc21_2501.12557v1\referenced_papers\[97]_2404.01268.pdf (Page 24):

Preprint
et al., 2023; Fernandez et al., 2023). However, one major concern with watermarking is that
it requires the involvement of the model or service owner, such as OpenAI, to implant the
watermark during the text generation process. In contrast, the framework by Liang et al.
(2024) operates independently of the model or service owner’s intervention, allowing for the
monitoring of AI-modified content without requiring their active participation or adoption.
Implications for LLM Pretraining Data Quality The increasing prevalence of AI-modified
content in academic papers, particularly on platforms like arXiv, has important implications
for the quality of LLM pretraining data. arXiv has become a significant source of training
data for LLMs, contributing approximately 2.5% of the data for models like Llama (Touvron
et al., 2023), 12% for RedPajama (Elazar et al., 2023), and 8.96% for the Pile (Gao et al., 2020).
Our findings suggest that a growing proportion of this pretraining data may contain LLM-
modified content. Preliminary research indicates that the inclusion of LLM-modified content
(Veselovsky et al., 2023) in LLM training can lead to several pitfalls, such as the reinforcement
of stereotypes and biases against anyone who is not a middle-aged “European/North
American man” (Ghosh & Caliskan, 2023; Santurkar et al., 2023), the flattening of variation
in language and content (Dell’Acqua et al., 2023), and the potential failure of models to
accurately capture the true distribution of the original content, which may result in model
collapse (Shumailov et al., 2023). Santurkar et al. (2023) demonstrate that this phenomenon
amplifies the effect of LLMs providing content that is unrepresentative of most of the
world. As such, our results underscore the importance of robust data curation and filtering
strategies even in seemingly unpolluted datasets.
25



### Claim 10/24

#### Claim Text
Having participants interact with LLMs may also impact privacy [17], especially when using closed models; thus authors may consider how to obtain consent for an LLM to use a participant’s data, how to sanitize LLM inputs, and measures to protect participants’ agency over their data.

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 9):

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Kapania and Wang, et al.
“gender bias in the entire computational pipeline– in places that we’ve never had gender bias before
because we’ve never used LLMs before, and because we used to come up with our own solutions”
Participants pointed out the tendency of LLMs towards generating homogenous content, where
the model would prioritize generalization and converge diverse perspectives into standardized
outputs. P8, working on sensemaking tools for product selection using LLMs, described how they
“want the selection criteria to encompass a diverse set of opinions rather than just focus on the perspective
of a single demographic. ” This tendency of LLMs towards “flattening human diversity and nuance”
(P9) raised concerns among researchers who emphasized the importance of capturing the complexity
of lived experiences within the context of their research. Participants also noted concerns that using
LLMs for editing their paper drafts could influence researchers’ paper writing styles, streamlining
towards homogeneous and “bland way of writing” (P9). Participants also discussed how LLMs’
training data, often reflecting Western morals and values, could contribute to this homogeneity.
4.2.2 Threats to privacy of participant data.Researchers in our study expressed anxiety about
how data input into LLMs is being used by LLM providers and the potential for violations of
privacy. They worried about breaches of private or sensitive information involving various forms
of participant data, such as transcripts, audio recordings, and application-specific log data. P7
used LLMs for qualitative data analysis and discussed how “privacy issue is the main concern for
us because in many cases, the audio transcription is not supposed to be put into ChatGPT”. P16 also
shared the concern that users’ navigation data is extremely sensitive and could lead to material
physical harm if uploaded to LLMs.
Many participants emphasized their concern of confidential and personally identifiable infor-
mation leakage as a result of sharing user data with LLM providers. P8 mentioned the risk of
confidential information being incorporated into LLM training corpus and the potential for security
leaks, where “backend messes up and user’s private information shows up in someone else’s chat
history”. Relatedly, some participants expressed concerns about the possibility of unconsented data,
for example, “explicit sexual content” (P2), being a part of the training corpus for large multimodal
models. While some believed that manually erasing sensitive information could mitigate these
concerns, others relied on the efficacy of APIs in safeguarding user data.
4.2.3 Violations of intellectual integrity.Interview participants raised ethical concerns about the
intellectual integrity of using LLMs in HCI research. A central theme of these concerns was the
ambiguity of ownership of LLM-generated text and visuals. Many participants who were co-
creating with LLMs also highlighted the difficulty in attributing what portion of the content is
the researcher’s original work and what is generated by the LLMs when they are refining and
co-creating with the LLM outputs. Participants discussed the ambiguity related to plagiarism when
LLM outputs are part of the research contribution. For example, interviewee P10 would consider
crediting the LLM “where a substantial amount ends up in a publication” . While most participants
questioned the extent to which one can claim ownership of LLM-generated content, especially in
the paper writing stage, some believed that “personhood is pretty important for attribution” (P2).
In addition, HCI researchers also raised concerns about the reproducibility of the research results
obtained via LLMs, highlighting the potential for an illusion of efficacy if LLMs work well in some
cases but fail to generalize to others. P2 described using LLMs in HCI research as a “computational
Wizard of Oz” method and continued on to point out how the quick and opaque updates of LLMs
are another barrier to reproducibility, noting that researchers “don’t really have control of what
version of GPT they are talking to and something that might work in a previous version won’t work so
much in future versions” .
10



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 11):

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Kapania and Wang, et al.
online, right? So we always claim this tool is no more harmful than typical social media” (P13). If
researchers explored topics deemed safe or if users did not directly prompt the model, there was a
perception that the LLM was unlikely to generate unsafe content3.
Such ‘conditional engagement’ sometimes resulted in a more reactive, rather than proactive,
approach to ethical considerations. For example, when deliberating whether LLMs should be
attributed and held accountable for model-generated content, P2 emphasized “I think many people
are very hasty to say yes or no. And I think that’s not the answer. The answer is always in a gray area. ”
They continued to emphasize this “wait and see” approach, discussing the potential benefits of
model hallucinations and highlighting their role in promoting divergent thinking by introducing
specificity that the user may not have considered. Some participants mentioned being aware of
frameworks and toolkits (such as Perspective API [53]) designed to address ethical issues, but they
never incorporated these tools into their own research processes. P3 justified this by discussing how
a significant portion of their HCI studies were conducted in a laboratory setting. The ethical concern
related to participants encountering harmful outputs generated by LLMs was less probable in a
short usability test, while serious issues could occur in longitudinal studies when the participants
heavily use the system.
4.3.2 Limited disclosure practices. In our study, HCI researchers positioned large language models
as everyday tools within their research practice. As a result, participants did not believe it was
necessary to formally report their usage of LLMs to study participants, the Institutional Review
Board (IRB), and/or the broader academic community. In particular, participants described a shift
towards the tacit incorporation of LLMs: they slowly transitioned from research tools that were
a part of their regular practice to using LLMs. This change was partly due to the perception that
LLMs are ‘fancier’ (P11) and more advanced versions of previously used tools. P10 explained, “I
advise my students that they are allowed to use any generative AI tool just like they would use other
productivity tools. So I’m categorizing LLMs as a productivity tool. ”
When using LLMs in paper writing, participants drew a parallel between LLM reporting practices
and their approach to previous tools. They emphasized that if, for example, tools like “grammar
assistance by Grammarly, word check by Google Docs, or accessibility checking by Acrobat pro” (P10)
were not explicitly reported, the same should also hold true for large language models. Researchers
expressed reservations, suggesting that reporting of LLM-based tools might call into question the
validity of their work. P10 captured this perspective:
I would not feel it is appropriate to say Bing Chat was used to define the initial structure
of X and Y sections of the paper. To me, it is not gonna be helpful in researchers assessing
the credibility or validity of the work. It is just like a meta issue about how the actual
document was formed and refined. And what really matters in that case is the output. Is it
easy to read? Did you find it useful? You know, that’s what I care about. So, in those cases,
I do not credit the LLM.
The complexity of describing LLMs to audiences without technical background further impacted
participants’ willingness to disclose the specific uses of large language models in research. In
some cases, researchers chose to characterize their LLM use simply as ‘AI models’ to their research
participants so as not to “confuse them with what is a language model. ” The rationale behind this
approach was the perception that the broader public (target population in this case) tend to view AI
in a homogeneous manner, and“to them, there isn’t much difference between how different AI systems
work, or which is a large language model, right?” (P8). Researchers also justified this approach by
highlighting their intention not to burden participants with the need to modify their behavior and“
3In contrast, prior work has demonstrated that LMs can generate unsafe content from seemingly innocuous prompts [46].
12



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 12):

Examining ethics of LLM use in HCI research practices Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
think about how to interact with an LLM” (P15). Finally, participants also noted cases where explicit
disclosure about LLMs was rather unnecessary if research participants or system users are not
directly exposed to LLMs. For example, if LLMs were used for ideation or analysis, participants felt
that explicit communication about their use was not imperative.
4.3.3 Restricting LLM use and reflecting on the co-creation process.A recurring sentiment among
the researchers in our study was a perceived lack of control in addressing ethical concerns related
to large language models. As a result, they developed various workarounds, such as restricting
the use of LLMs to a limited set of tasks, avoiding directly integrating LLM-generated inputs
into their work, and hosting group reflection sessions. The limited visibility and decision-making
capability were particularly evident in issues of privacy and data leaks, where the reliance on
LLMs provided by large companies diminished individual control. Simultaneously, participants
expressed a mistrust towards LLM providers. For instance, P12 discussed how LLM providers’ claim
to protect data privacy and not using it for their training “is very problematic because such claims
don’t really articulate what they mean by not using the data. How can an external researcher validate
this claim?” Participants’ mistrust towards LLM providers was exacerbated by the lack of clarity
and transparency in data usage policies.
To navigate the ‘unknown territory’ of LLMs, where HCI researchers were not fully aware of
the capabilities and risks, they would often restrict their usage of LLMs to a limited set of tasks.
Most HCI researchers avoided directly integrating LLM-generated outputs into their work. Rather
than accepting the initial output without scrutiny, they would iteratively refine and carefully verify
before incorporating it into their research artifacts. In the context of paper writing, researchers
would ensure the text aligns with the draft they had input into the system. Many participants
elaborated on their practice of co-creating with LLMs as a precautionary measure. This would
involve relying on their personal judgment on how much to use the LLM suggestions. This cautious
and iterative co-creation process served as a strategy in the absence of clear usage norms.
Indeed, many participants shared the view that LLMs are still evolving, and there is a lack of
comprehensive guidelines on navigating ethical considerations. P9, primarily using LLMs for paper
writing, mentioned how “LLMs are still an unknown territory, so people don’t know how to react, I
assume. ”Our participants expressed discomfort with using any new LLM-generated content in
their papers. For example, P15 was hesitant to use the LLM as anything more than a spell checker.
Researchers highlighted the importance of applying LLMs in a way that is consistent with their
practices with previously used tools to mitigate potential unintended consequences. P9 spoke of
their experience as a reviewer for major HCI conferences like CHI, where they did not encounter
any guidelines regarding the disclosure of using ChatGPT for writing. Finally, they went on to
emphasize that the cost and accountability of using LLMs was still “ad-hoc and unclear. ”
In cases where participants recognized the possibility of ethical concerns with LLMs, they often
struggled to identify specific issues to address. As P16 pointed out: “the main problem is that I
don’t know what bias it has, and I don’t know how to figure it out. ” P4 discussed how it is crucial
to “not pretend that this ethical consideration does not exist, which some people do. We are trained in
human-computer interaction, and so we are well equipped to reason about it or at least understand
that these concerns exist, but obviously addressing it is very difficult. ” In many cases, when people
did not have mechanisms to identify or address ethical concerns, they felt it was important to at
least acknowledge them and spread awareness about these issues.
To account for the uncertainties with the responsible use of LLMs, some researchers chose to
host group reflection sessions. This involved engaging in discussions with collaborators or advisors
to determine the appropriate approach for navigating any ethical considerations. P10 exemplified
this approach by holding regular team meetings to address any questions or concerns related to
13



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 8):

Examining ethics of LLM use in HCI research practices Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
on aspects that the researcher“can hardly think about from [their] vantage point. Interacting with the
LLM is like talking to other researchers and getting their feedback” (P11). Researchers also mentioned
using LLMs to generate paper reviews (P14), and provide suggestions for their writing content and
style (e.g., as an alternative to Grammarly2 (P4)). P14 spoke of using GPT-4 to“provide critical paper
reviews, check grammar, the style, and the logic consistency” .
Finally, many HCI researchers used LLMs as a design material for system development,
including system building and evaluation. Participants shared many use cases, such as developing
new tools and interactions for library services, science communication, and AI-assisted writing. P4
expressed interest in exploring “what do large language models enable us to do in HCI with which
our field has struggled for many decades. ” Finally, researchers in our study also discussed instances
where they used large language models for software development, including day-to-day debugging
and creating web interfaces with LLMs.
4.2 What are HCI researchers’ ethical concerns with the use of LLMs?
Researchers expressed a wide range of ethical concerns regarding the use of LLMs. Across our survey
respondents, 30 reported observing ethical challenges associated with using LLMs, 10 expressed
uncertainty, and 10 indicated no awareness of such concerns. Among the self-reported ethical
challenges, the most common issues were data privacy (19) (including secondary use of participants’
data and data leak), authorship (16) (including disclosure of the use of LLMs in publications),
harmful outputs (14), copyright issues (11) and consent (10). These concerns were more prevalent
across research design and execution, as well as analysis and paper writing stages of the research
(see Figure 1). Below, we describe the specific ethical concerns highlighted by HCI researchers in
our interviews.
4.2.1 Harms of engaging with LLM outputs.Our interviews revealed a shared ethical concern of
research subjects engaging with harmful outputs, especially if LLMs were integrated in systems
and tools that directly interact with users. Over the last few years, scholars within HCI have
increasingly shifted their efforts to working with and centering vulnerable populations (see [17,
59]). P3 used large multimodal models to create a mental well-being and self-reflection tool and
articulated their concern about LLMs generating uncontrolled outputs. LLM-generated content
could disproportionately harm marginalized groups through exposure to socially harmful biases
and stereotyping behaviors. Hate speech and exclusionary and discriminatory language could
cause psychological and representational harm to research participants. P16, developing LLM-based
roleplay robots, articulated how “[LLM] may exaggerate or diminish some of the capacity with which
the robot has already been equipped. It may introduce another layer of bias [toward people with
disabilities]. ” In addition to the concerns about biased LLM outputs, P14 also worried about the
possibility of LLM facilitating the “surveillance and monitoring of intimate partners” by providing
links to various spy tools.
Many researchers expressed concerns about large language models impacting their research
pipeline by generating seemingly authoritative but fabricated information. LLM hallucinations
during paper writing could mislead authors and if published, would undermine trust in knowledge
production processes. P10 used LLMs for literature review and worried that “ChatGPT might
make up titles of things that simply do not exist. ” Participants highlighted the need for vigilance
in identifying these hallucinations, especially when LLMs produced fake citations or mismatched
paper references. Beyond paper writing, researchers indicated the possibility of inheriting biases
from LLMs in ideation, research design, and evaluation. P2 articulated how LLMs could introduce
2https://app.grammarly.com/
9



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 3):

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Kapania and Wang, et al.
the need to go beyond legal requirements and develop comprehensive guidelines for responsible
behavior by learning from other disciplines. Vitak et al. [104] encouraged interdisciplinary col-
laboration of building new ethics framework such as developing ethics heuristics for online data
research to ensure responsible research. Despite the rich tradition of considering and iterating
ethical guidelines in the HCI community, Computer Science (CS) researchers have demonstrated
uncertainty about the applicability of ethical concerns pertaining to human subject studies to their
research [13], highlighting the necessity of educating CS and HCI researchers about research ethics.
Recent endeavors of integrating ethics in CS education such as adding systematic literature review
on ethics in computing courses [ 91, 96] have illustrated both the barriers and opportunities of
educating CS researchers in ethics.
Recently, advances in AI have brought renewed urgency of research ethics to HCI and CS
communities. Clark et al. [27] proposed approaches to assessing ethical risks of research involving
digital data, informing the development of guidelines of ethical standards for research. Amershi
et al. [1] provided guidelines for human-AI interaction design, emphasizing the importance of
providing explanations of the systems and conveying consequences of user actions. Another line
of research strives to bridge the gap between ethics and AI practices by establishing guidelines for
safe, transparent, and trustworthy AI systems [95]. With the rapid development of generative AI,
Association of Computing Machinery (ACM) and the Association for Computational Linguistics
(ACL) have established ongoing efforts to develop guidelines [20, 39]. The ACM policy on authorship
requires the full disclosure of generative AI in the paper [39], while the ACL policy on AI-assisted
tools on paper writing requires authors to elaborate on the scope and nature of their use [20].
Even though underlying ethical guidelines may be broadly applicable, emerging technologies
might present challenges to existing ethical review processes [ 70]. Our goal is not to create a
taxonomy of all possible ethical concerns with the use of LLMs in HCI research; instead, we hope
to extend the discourse on research ethics in HCI by documenting the ways in which researchers
are responding to ethical considerations of LLMs in-situ and reflecting on potential ways forward.
2.3 Ethics of LLM use
Extensive prior research has explored the ethical risks and harms related with language models [106,
107]. The discrimination & exclusion harms arise from the biased and unjust text in the training
data of LLMs. Recent work identified the tendency of LLMs to display discrimination related
to users’ sensitive characteristics [ 112] and demonstrated the gender and racial biases of LLM-
generated content [ 33]. The information hazards are the consequences of LLMs remembering
private information in training data, posing the risk of privacy leaks [18].
Privacy violation has been observed in LLM-based AI assistants where Personally Identifiable
Information (PII) can be revealed by employing adversarial privacy-inducing prompts [ 63, 68].
The detection and mitigation of hallucinations in LLM generated text [115] remains a challenging
problem [22]. This might lead to spread of misinformation, as LLM-based chatbots are often treated
as fact-checking tools [115].
There is a growing concern about malicious activities arising from the generation of scams and
phishing using LLMs [69]. Recent research has also pointed out how the anthropomorphization of
LLMs has the potential for manipulation and negative influence [31]. Finally, researchers have also
documented the likelihood of LLMs increasing inequality and their negative effects on job quality,
undermining creative economies, and more [107].
In response to these concerns, recent research has started to look into how to assess [30] and
mitigate [23, 69] the harms of LLMs. For example, researchers are exploring privacy preserving
strategies for language models in pre-processing, training, and post-training approaches [97]. For
combating misinformation in LLMs [ 23], researchers have proposed several defense strategies,
4



### Claim 11/24

#### Claim Text
Other works may have even used LLMs in their methods without mentioning them at all, which would align with the increasing interest in using LLMs to automate academic research [108].

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 7):

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Kapania and Wang, et al.
Ideation and project 
scoping
Study design and 
execution
Analysis and paper 
writing
Identifying research topics
Deﬁning research questions
Literature review
Research design
System building
Data collection
Data generation
Data analysis
System evaluation
Paper writing
Writing grant proposals
Fig. 1. LLMs are used in various ways for ideation and project scoping, study design and execution, and
analysis and paper writing. The figure illustrates a typical HCI study across our research participants. Not all
HCI research projects would include all activities listed above (e.g., critical theoretical contributions).
4.1 Where do HCI researchers use LLMs in their everyday work?
HCI researchers referred to various parts of their research workflow where they integrated large
language models, such as for ideation, literature review, study design, data analysis, system building
and evaluation, and paper writing (Figure 1). Overall, they perceived that LLMs open up new
possibilities in their research, such that “if we can leverage LLMs the right way, it will enable us to do
new cool things that will be genuinely empowering” (P4). Our survey revealed that the stages where
LLMs are most frequently used are paper writing (25) and study design (24), followed by project
scoping (17) and system development (16), and then data generation (15), data collection (14) and
data analysis (13). Below, we present the ways in which interview participants incorporate LLMs
in their research practices.
Interview participants frequently used large language models in the ideation and project
scoping phase, for tasks such as reviewing and synthesizing literature, discovering new research
questions in their sub-field of HCI, and defining their research problems. P11 would input broad
topic areas into the LLM to generate HCI research questions and subsequently refine them into
concrete research objectives. Similarly, P10 would probe the LLM to“pretend that it is a career coach
for [participant name]. What would [the LLM] recommend if [participant name] is writing their NSF
career grants? This is a big thing for early career HCI academic researchers. What should [participant
name] explore at the intersection of AI and cybersecurity?” During brainstorming, HCI researchers
found value in using large language models for a breadth-first search approach, enabling them to
quickly generate a diverse range of ideas.
LLMs were also seen to have utility in data generation, collection, and analysis. Many
participants mentioned how LLMs were especially productive in synthesizing information from
web sources, that would otherwise require significant time and effort. P1 prompted the LLM to
generate multiple arguments for and against hypothetical scenarios for use in a classroom setting.
They noted how creating these research artifacts for data collection would have taken them weeks
without the LLM. Additionally, HCI researchers integrated LLMs into their data analysis process,
utilizing them for tasks such as for qualitative data coding, creating plots, and data visualizations. P7
applied LLMs for multiple aspects of open coding interview data, including (1) proposing new codes,
(2) acting as a mediator between the coding team consisting of two members, and (3) generating
the primary code groups.
HCI researchers described how they increasingly relied on LLMs in the paper writingstage.
Participants shared their experiences of leveraging LLMs for iteratively refining their paper drafts,
including searching for synonyms and fixing grammatical issues. LLMs offered a distinct perspective
8



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 2):

The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery
Figure 1|Conceptual illustration ofThe AI Scientist , an end-to-end LLM-driven scientific discovery
process. The AI Scientist first invents and assesses the novelty of a set of ideas. It then determines how to
test the hypotheses, including writing the necessary code by editing a codebase powered by recent advances in
automated code generation. Afterward, the experiments are automatically executed to collect a set of results
consisting of both numerical scores and visual summaries (e.g. plots or tables). The results are motivated,
explained, and summarized in a LaTeX report. Finally,The AI Scientist generates an automated review,
according to current practice at standard machine learning conferences. The review can be used to either
improve the project or as feedback to future generations for open-ended scientific discovery.
2. To assess the quality of the generated papers, we introduce a foundation model-based reviewing
process in Section 4. This process achieves near-human-level performance across multiple evalu-
ation metrics (e.g. 65% vs. 66% balanced accuracy) when evaluated on ICLR 2022 OpenReview
data. The reviews further enableThe AI Scientist to select the best ideas for “publication”
to an ever-growing archive of scientific discoveries, and the process can be repeated to build on
these discoveries, just as in the human scientific community.
3. The AI Scientist can generate hundreds of interesting, medium-quality papers over the
course of a week. In this report, we focus on a subset of these papers, highlighting novel insights
in diffusion modeling, language modeling, and grokking. We perform an in-depth case study
into one selected paper in Section 5, and present aggregate results in Section 6.
4. We conclude the paper with an extensive discussion on the limitations, ethical considerations,
and future outlook of our approach in Sections 8 and 9.
2. Background
Large Language Models.In this paper, we build our automated scientist from autoregressive large
language models (LLMs, Anthropic (2023); Google DeepMind Gemini Team (2023); Llama Team
(2024); OpenAI (2023); Zhu et al. (2024)) which learn to generate text completions by modeling the
conditional probability of a new token (similar to a word) given the preceding tokens,𝑝(𝑥𝑡 |𝑥<𝑡; 𝜃),
and sampling at test-time. Together with vast data and model scaling, this enables LLMs to not
only generate coherent text, but crucially also exhibit human-like abilities, including commonsense
knowledge (Talmor et al., 2019), reasoning (Wei et al., 2022), and the ability to write code (Chen
et al., 2021; Xu et al., 2022).
LLM Agent Frameworks.Typical applications of LLMs often involve embedding the model into
an “agent” (Wang et al., 2024a) framework, including the following possibilities: the structuring of
3



Source: data\tc21_2501.12557v1\referenced_papers\[183]_2303.18223.pdf (Page 70):

71
LLM for 
Application
Research 
Directions
Specific Domains Scientific 
ResearchFinance Law EducationHealthcare
LLM for Evaluation
LLM-based Agent
KG Enhanced LLM
Multimodal LLMs
• Vision-Language Alignment Pre-Training
• Visual Instruction Tuning
• Evaluation of MLLM
• Retrieval-augmented LLM
• Synergy Augmented LLM
• Components: Memory/Planning/Execution
• Single/Multi-agent based Application
• Score/Language-based Evaluation
• Instruction Design, Multiple Feedbacks, Debate Agent
• Meta-Evaluation
LLM for IR
LLM for Classic NLP Tasks
LLM for Recommendation
• LLM as Recommendation Model
• LLM-enhanced Recommendation Models
• LLM as Recommendation Simulator
• LLM as IR Model
• LLM-Enhanced IR Models
• Word/Sentence-level Tasks
• Sequence Tagging
• Information Extraction
• Text Generation
Classic Scenarios
Enhanced Capabilities
New Scenarios
Fig. 18: The applications of LLMs in representative research directions and downstream domains.
success, recent work also reveals that LLMs are hard to well
address the generation tasks about low-resource languages
and domains, e.g., Marathi-to-English translation [772], due
to their unbalanced training data across different languages.
Summary. Based on the above discussion, we summarize
the suggestions, and future direction about the use of LLMs
in classic NLP tasks as follows:
• Suggestions: LLMs and small models have their own
merits in different aspects: LLMs are can provide unified
solutions to various NLP tasks and achieve competitive
performance (especially in the zero/few-shot setting), while
small models are economical to develop and can be specially
tuned according to target tasks, which can achieve good
performance with sufficient high-quality labeled data [755,
756, 773, 774]. In applications, one can make suitable choices
based on the actual needs, comprehensively considering
flexibility, data availability, training compute, and efficiency.
• Future direction: Despite the excellent general capac-
ities, LLMs still cannot effectively process the NLP tasks
in low-resource domains, e.g., minor language translation.
To tackle such tasks, it needs to develop effective ap-
proaches to injecting necessary task information or domain-
specific knowledge into LLMs, either through fine-tuning
or prompting. In addition, it is still challenging for LLMs to
handle complex semantic relations in classic NLP tasks (e.g.,
nested entity extraction), which is worth more exploration
from the underlying working mechanism of LLMs. It is also
promising to combine LLMs and fine-tuned small language
models for complementing with each other in solving com-
plex cases of classic NLP tasks [775]. Another promising di-
rection is to conduct human-machine collaborative research
(e.g., conversational translation [771]) on NLP tasks, since
LLMs can effectively understand human instructions and
make meaningful responses.
8.1.2 LLM for Information Retrieval
The goal of information retrieval (IR) systems is to assist
users in discovering ideal information resources (typically
documents) and mitigating the information overload issue.
Typically, contemporary IR systems adopt a retrieve-then-
rerank pipeline framework [54]. Within this framework,
the retriever initially retrieves relevant information from a
large-scale corpus, and the reranker subsequently performs
multi-stage ranking procedure to acquire the most relevant
information [776]. Since the advent of LLMs has significant
impact on the way of information access, we discuss how
it advances the development of IR from two main aspects,
namely LLMs as IR models and LLM-enhanced IR models.
LLMs as IR Models. Existing IR models can be overall
categorized into sparse models (relying on term-based lexical
similarity) and dense models (relying on embedding based
semantic similarity) [54]. Specially, dense models are mainly
implemented by fine-tuned PLMs ( e.g., BERT). Compared
to PLMs, LLMs have more strong model capacities in
capturing text semantics, thus having the potential to im-
prove existing dense IR models. However, due to the high
overhead of LLMs, the majority of studies concentrate on
employing LLMs as rerankers, aiming to refine the rank-
ing of retrieved candidates. To achieve this, recent efforts
often formulate special instructions that enable LLMs to
perform reranking on a small set of provided candidate
documents. Typically, such an approach does not necessitate
model training, and achieve promising results compared
with well-trained reranking methods [777, 778]. Specially,
the LLM-based reranking approach can be implemented
in different ways by zero-shot or few-shot instruction, in-
cluding pointwise ( estimating the relevance scores for query-
document pairs) [779], pairwise (determining the relevance order
of two documents) [778], or listwise ranking (sorting a subset of
candidate documents) [780]. The essence of these methods lies



Source: data\tc21_2501.12557v1\referenced_papers\[97]_2404.01268.pdf (Page 1):

Preprint
2021.1 2021.4 2021.7 2021.10 2022.1 2022.4 2022.7 2022.10 2023.1 2023.4 2023.7 2023.10 2024.10.0%
2.5%
5.0%
7.5%
10.0%
12.5%
15.0%
17.5%
20.0%Estimated Alpha
ChatGPT
Launch
Nov 30, 2022
Computer Science(arXiv)
Electrical Engineering and Systems Science(arXiv)
Statistics(arXiv)
bioRxiv
Physics(arXiv)
Nature portfolio
Mathematics(arXiv)
Figure 1: Estimated Fraction of LLM-Modified Sentences across Academic Writing Venues
over Time. This figure displays the fraction (α) of sentences estimated to have been sub-
stantially modified by LLM in abstracts from various academic writing venues. The analysis
includes five areas within arXiv (Computer Science, Electrical Engineering and Systems
Science, Mathematics, Physics, Statistics), articles from bioRxiv, and a combined dataset
from 15 journals within the Nature portfolio. Estimates are based on the distributional GPT
quantification framework, which provides population-level estimates rather than individ-
ual document analysis. Each point in time is independently estimated, with no temporal
smoothing or continuity assumptions applied. Error bars indicate 95% confidence intervals
by bootstrap. Further analysis of paper introductions is presented in Figure 7.
cases. Applied to scientific publishing, the importance of this at-scale approach is two-fold:
first, rather than looking at LLM-use as a type of rule-breaking on an individual level, we
can begin to uncover structural circumstances which might motivate its use. Second, by
examining LLM-use in academic publishing at-scale, we can capture epistemic and linguistic
shifts, miniscule at the individual level, which become apparent with a birdseye view.
Measuring the extent of LLM-use on scientific publishing has urgent applications. Concerns
about accuracy, plagiarism, anonymity, and ownership have prompted some prominent
scientific institutions to take a stance on the use of LLM-modified content in academic
publications. The International Conference on Machine Learning (ICML) 2023, a major
machine learning conference, has prohibited the inclusion of text generated by LLMs like
ChatGPT in submitted papers, unless the generated text is used as part of the paper’s
experimental analysis (ICML, 2023). Similarly, the journal Science has announced an update
to their editorial policies, specifying that text, figures, images, or graphics generated by
ChatGPT or any other LLM tools cannot be used in published works (Thorp, 2023). Taking
steps to measure the extent of LLM-use can offer a first-step in identifying risks to the
scientific publishing ecosystem. Furthermore, exploring the circumstances in which LLM-
use is high can offer publishers and academic institutions useful insight into author behavior.
Sites of high LLM-use can act as indicators for structural challenges faced by scholars. These
range from pressures to “publish or perish” which encourage rapid production of papers
to concerns about linguistic discrimination that might lead authors to use LLMs as prose
editors.
We conduct the first systematic, large-scale analysis to quantify the prevalence of LLM-
modified content across multiple academic platforms, extending a recently proposed, state-
of-the-art distributional GPT quantification framework (Liang et al., 2024) for estimating the
fraction of AI-modified content in a corpus. Throughout this paper, we use the term “LLM-
modified” to refer to text content substantially updated by ChatGPT beyond basic spelling
and grammatical edits. Modifications we capture in our analysis could include, for example,
summaries of existing writing or the generation of prose based on outlines.
2



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 3):

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Kapania and Wang, et al.
the need to go beyond legal requirements and develop comprehensive guidelines for responsible
behavior by learning from other disciplines. Vitak et al. [104] encouraged interdisciplinary col-
laboration of building new ethics framework such as developing ethics heuristics for online data
research to ensure responsible research. Despite the rich tradition of considering and iterating
ethical guidelines in the HCI community, Computer Science (CS) researchers have demonstrated
uncertainty about the applicability of ethical concerns pertaining to human subject studies to their
research [13], highlighting the necessity of educating CS and HCI researchers about research ethics.
Recent endeavors of integrating ethics in CS education such as adding systematic literature review
on ethics in computing courses [ 91, 96] have illustrated both the barriers and opportunities of
educating CS researchers in ethics.
Recently, advances in AI have brought renewed urgency of research ethics to HCI and CS
communities. Clark et al. [27] proposed approaches to assessing ethical risks of research involving
digital data, informing the development of guidelines of ethical standards for research. Amershi
et al. [1] provided guidelines for human-AI interaction design, emphasizing the importance of
providing explanations of the systems and conveying consequences of user actions. Another line
of research strives to bridge the gap between ethics and AI practices by establishing guidelines for
safe, transparent, and trustworthy AI systems [95]. With the rapid development of generative AI,
Association of Computing Machinery (ACM) and the Association for Computational Linguistics
(ACL) have established ongoing efforts to develop guidelines [20, 39]. The ACM policy on authorship
requires the full disclosure of generative AI in the paper [39], while the ACL policy on AI-assisted
tools on paper writing requires authors to elaborate on the scope and nature of their use [20].
Even though underlying ethical guidelines may be broadly applicable, emerging technologies
might present challenges to existing ethical review processes [ 70]. Our goal is not to create a
taxonomy of all possible ethical concerns with the use of LLMs in HCI research; instead, we hope
to extend the discourse on research ethics in HCI by documenting the ways in which researchers
are responding to ethical considerations of LLMs in-situ and reflecting on potential ways forward.
2.3 Ethics of LLM use
Extensive prior research has explored the ethical risks and harms related with language models [106,
107]. The discrimination & exclusion harms arise from the biased and unjust text in the training
data of LLMs. Recent work identified the tendency of LLMs to display discrimination related
to users’ sensitive characteristics [ 112] and demonstrated the gender and racial biases of LLM-
generated content [ 33]. The information hazards are the consequences of LLMs remembering
private information in training data, posing the risk of privacy leaks [18].
Privacy violation has been observed in LLM-based AI assistants where Personally Identifiable
Information (PII) can be revealed by employing adversarial privacy-inducing prompts [ 63, 68].
The detection and mitigation of hallucinations in LLM generated text [115] remains a challenging
problem [22]. This might lead to spread of misinformation, as LLM-based chatbots are often treated
as fact-checking tools [115].
There is a growing concern about malicious activities arising from the generation of scams and
phishing using LLMs [69]. Recent research has also pointed out how the anthropomorphization of
LLMs has the potential for manipulation and negative influence [31]. Finally, researchers have also
documented the likelihood of LLMs increasing inequality and their negative effects on job quality,
undermining creative economies, and more [107].
In response to these concerns, recent research has started to look into how to assess [30] and
mitigate [23, 69] the harms of LLMs. For example, researchers are exploring privacy preserving
strategies for language models in pre-processing, training, and post-training approaches [97]. For
combating misinformation in LLMs [ 23], researchers have proposed several defense strategies,
4



### Claim 12/24

#### Claim Text
Our work primarily focused on prompting as the main interface, but future study may extend our samples to study and identify best practice for other techniques (e.g., fine-tuning [ 55], LLM-based embeddings [128], and multi-agents [51]).

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[51]_2402.01680.pdf (Page 5):

Agents Profiling AgentsCommunication Agents Capabilities Acquisition
MotivationResearch Domain & GoalsWork
Agents-Env.Interface Profilingmethods Profiles(examples)Paradigms Structure Feedback fromAgentsAdjustment
[Qianet al., 2023] Sandbox Pre-defined,Model-GeneratedCTO,programmerCooperative Layered Environment,Agent interaction,Human
Memory,Self-Evolution
Software development[Honget al., 2023] Sandbox Pre-definedProduct Manager,Engineer Cooperative Layered,Shared Message Pool
Environment,Agent interaction,Human
Memory,Self-Evolution
[Donget al., 2023b] Sandbox Pre-defined,Model-GeneratedAnalyst,coder Cooperative Layered Environment,Agent interactionMemory,Self-EvolutionMulti-robotplanning [Chenet al., 2023d] Sandbox,Physical Pre-defined Robots CooperativeCentralized,DecentralizedEnvironment,Agent interactionMemory
EmbodiedAgents Multi-robotcollaboration[Mandiet al., 2023] Sandbox,Physical Pre-defined Robots CooperativeDecentralizedEnvironment,Agent interactionMemory
Multi-Agentscooperation[Zhanget al., 2023c] Sandbox Pre-defined Robots CooperativeDecentralizedEnvironment,Agent interactionMemoryProblemSolving ScienceExperimentsOptimizationof MOF [Zhenget al., 2023] Physical Pre-definedStrategy planers,literaturecollector, coderCooperativeCentralized Environment,Human Memory
ImprovingFactuality [Duet al., 2023] None Pre-defined Agents Debate DecentralizedAgent interactionMemory
ScienceDebate Examining,Inter-Consistency[Xionget al., 2023] None Pre-defined Proponent,Opponent,Judge Debate Centralized,DecentralizedAgent interactionMemory
Evaluatorsfor debates [Chanet al., 2023] None Pre-defined Agents Debate Centralized,DecentralizedAgent interactionMemory
Multi-Agentsfor Medication[Tanget al., 2023] None Pre-defined Cardiology,Surgery Debate,CooperativeCentralized,DecentralizedAgent interactionMemory
Modest Community(25 persons)[Parket al., 2023] SandboxModel-generatedPharmacy,shopkeeper - - Environment,Agent interactionMemory
Online community(1000 persons)[Parket al., 2022] None Pre-defined,Model-generatedCamping,fishing - - Agent interactionDynamicGeneration
Society Emotion propagation[Gaoet al., 2023a] None Pre-defined,Model-generatedReal-worlduser - - Agent interactionMemory
Real-timesocial interactions[Kaiyaet al., 2023] Sandbox Pre-defined Real-worlduser - - Environment,Agent interactionMemory
Opinion dynamics[Liet al., 2023a] None Pre-defined NIN, NINL,NIL - - Agent interactionMemory
WereWolf [Xuet al., 2023b]
[Xuet al., 2023c] Sandbox Pre-defined Seer,werewolf,villager
Cooperative,Debate,CompetitiveDecentralizedEnvironment,Agent interactionMemory
Gaming Avalon [Lightet al., 2023a]
[Wanget al., 2023c] Sandbox Pre-defined Servant,Merlin,Assassin
Cooperative,Debate,CompetitiveDecentralizedEnvironment,Agent interactionMemory
Welfare Diplomacy[Mukobiet al., 2023] Sandbox Pre-defined CountriesCooperative,CompetitiveDecentralizedEnvironment,Agent interactionMemory
Human behaviorSimulation[Aheret al., 2023] Sandbox Pre-defined Humans - - Agent interactionMemory
WorldSimulation
PsychologyCollaborationExploring [Zhanget al., 2023d] None Pre-defined Agents Cooperative,Debate DecentralizedAgent interactionMemory
Macroeconomicsimulation [Liet al., 2023e] None Pre-defined,Model-generatedLabor CooperativeDecentralizedAgent interactionMemory
Economy InformationMarketplaces[Anonymous, 2023] Sandbox Pre-defined,Data-Derived Buyer Cooperative,CompetitiveDecentralizedEnvironment,Agent interactionMemory
Improvingfinancial trading[Liet al., 2023g] Physical Pre-defined Trader Debate DecentralizedEnvironment,Agent interactionMemory
Economic theories[Zhaoet al., 2023] Sandbox Pre-defined,Model-GeneratedRestaurant,Customer CompetitiveDecentralizedEnvironment,Agent interactionMemory,Self-Evolution
RecommenderSystems
Simulatinguser behaviors[Zhanget al., 2023a] Sandbox Data-DerivedUsers fromMovieLens-1M- - EnvironmentMemory
Simulating user-iteminteractions[Zhanget al., 2023e] Sandbox Pre-defined,Data-DerivedUser AgentsItem AgentsCooperativeDecentralizedEnvironment,Agent interactionMemory
PolicyMaking
PublicAdministration[Xiaoet al., 2023] None Pre-defined Residents CooperativeDecentralizedAgent interactionMemory
War Simulation[Huaet al., 2023] None Pre-defined Countries CompetitiveDecentralizedAgent interactionMemory
Disease Human Behaviorsto epidemics[Ghaffarzadeganet al., 2023] Sandbox Pre-defined,Model-GeneratedConformitytraits CooperativeDecentralizedEnvironment,Agent interactionMemory
Public health [Williamset al., 2023] Sandbox Pre-defined,Model-GeneratedAdults aged18 to 64 CooperativeDecentralizedEnvironment,Agent interaction
Memory,DynamicGeneration
Table 1: Summary of the LLM-MA studies. We categorize current work according to their motivation, research domains and goals, and detail
each work from different aspects regarding Agents-Environment Interface, Agents Profiling, Agents Communication and Agents Capability
Acquisition. “-” denotes that a particular element is not specifically mentioned in this work.
Standardized Operating Procedures (SOPs) workflow of the
software development, the communication structure among
agents is usually layered. Agents generally interact with the
code interpreter, other agents or human to iteratively refine
the generated code. [Li et al., 2023b] first proposes a sim-
ple role-play agent framework, which utilizes the interplay
of two roles to realize autonomous programming based on
one-sentence user instruction. It provides insights into the
“cognitive” processes of communicative agents. [Dong et al.,
2023b] makes LLMs work as distinct “experts” for sub-tasks
in software development, autonomously collaborating to gen-
erate code. Moreover, [Qian et al., 2023] presents an end-to-
end framework for software development, utilizing multiple
agents for software development without incorporating ad-
vanced human teamwork experience. [Hong et al., 2023] first
incorporates human workflow insights for more controlled
and validated performance. It encodes SOPs into prompts to
enhance structured coordination. [Huang et al., 2023a] delves
deeper into multi-agent based programming by solving the
problem of balancing code snippet generation with effective



Source: data\tc21_2501.12557v1\referenced_papers\[183]_2303.18223.pdf (Page 79):

80
can perceive the environment, make decisions, and take
actions to achieve specific goals [961]. However, traditional
agents are often limited to heuristic rules or specific environ-
ments, which constrain their generalization to open-domain
scenarios [962]. Given that LLMs possess excellent capacities
in solving complex tasks, they have rapidly emerged as
promising solutions for serving as the core computation
unit of agents [821]. In this part, we will first introduce
the framework for LLM-based agents, then explore their
applications, and finally discuss the future directions.
9.2.1 Overall Framework.
Next, we first detail the key components of an LLM-based
agent and then present the typical workflow.
Components. Typically, there are three main components
in an LLM-based agent: memory, planning45, and execution.
Specifically, the memory component aims to store the in-
formation perceived from the environment and can be
utilized to support decision-making. In particular, LLM-
based agents usually maintain information in both short-
term memory and long-term memory with the operations
of reading and writing. Short-term memory usually refers
to the internal context window of LLMs ( i.e., input), where
LLMs can read and write through actions like reason-
ing [963]. While long-term memory can be mapped to the
external storage like vector databases [539], where LLMs
can read through retrieval and write with reflection [688].
Specially, profiles are usually implemented with long-term
memory, which is an important feature for an agent that
specifies its role and function [821]. The planning component
is responsible for generating the action plan based on the in-
formation from the memory component. In data format, the
plan usually takes the form of text-based instructions [434]
or code-based programs [436]. To generate it, LLM-based
agents will first propose several candidates and then select
a more suitable one among them [429]. The initial plan
can be further refined with execution feedback from the
environment [530]. The execution component is in charge
of carrying out the plan from the planning component,
which can be fulfilled by the internal LLM [434] or external
tools [963].
Workflow. With the three components mentioned above, a
typical workflow of an LLM-based agent is as follows. First,
it receives information from the environment and writes
it into short-term memory. Then, the agent processes the
newly received information in the short-term memory. Such
a process can be enhanced with information retrieved from
long-term memory. Subsequently, the planning component
utilizes the processed information from short-term memory
to generate the next plan. Finally, the execution component
carries out the plan generated from the planning compo-
nent, which can be further assisted with external tools.
By repeating the aforementioned process, the LLM-based
agent can autonomously adjust its behavior in response
to feedback from the environment and ultimately achieve
its goal. Once LLM-based agents receive user requests or
45. Section 6.4 introduces planning as a utilization approach for
LLMs, while in this section, we describe its utilization as a functional
component in LLM-based agents.
are assigned goals, they follow the above workflow to
accomplish tasks through multi-turn interactions with the
environment.
To summarize, in an LLM-based agent, the LLM serves
as the core computation unit and is equipped with compo-
nents including memory, planning, and execution. These com-
ponents are integrated in a systematic way under the control
of the LLM during interactions with the environment. For
more details, the readers might refer to the comprehensive
survey for LLM-based AI agents [821].
9.2.2 Applications
Recently, LLM-based agents have shown great potential in
autonomously solving complex tasks, making it feasible to
rapidly develop capable applications for specific domains
or tasks. In this section, we will discuss the applications in
single-agent and multi-agent scenarios.
Single-agent based Applications. Applications based on
a single-agent mode mainly aim to develop capable task
solvers that can autonomously complete user requests. A
large number of single-agent projects have been developed,
which focus on general-purpose task solving. As a rep-
resentative project, AutoGPT [536] empowers LLMs with
long/short-term memory management and external tools
like search engines. In order to autonomously address a
user request, AutoGPT understands the request with knowl-
edge from its memory and actions like reasoning, decom-
poses it into a detailed plan, executes the plan step-by-
step with the assistance of tools, and refines the rest plan
based on feedback from the environment. Such an iterative
process continues until the user request is successfully re-
solved. Other similar projects include GPT-Engineer [964]
and XAgent [965]. In addition, there is also some work that
aims to develop autonomous agents for specific domains,
such as WebGPT [81] for the web-browsing environment,
ProgPrompt [532] for the real-life environment, and Voy-
ager [699] for the Minecraft environment.
Multi-agent based Applications. Different from single-
agent systems where agents work independently, multi-
agent systems work in collaboration to unleash collective
intelligence. Typically, multiple agents can be instantiated
from the same or different LLMs, each with their respective
roles and functions. According to the coordinating strategies
among these agents, multi-agent systems can be divided
into two categories: cooperation-based and competition-
based. In the cooperation-based mode, to share informa-
tion and seek collaborative actions among agents, various
communication protocols have been proposed, including
free-form dialogue [966], structured document [967], and
data embedding [968]. Based on the communication pro-
tocol, agents can be effectively organized for downstream
applications, such as software engineering [967], user be-
havior analysis [822, 824], and society simulation [535].
As a representative project, LangChain 46 is a framework
for developing multi-agent based applications powered by
LLMs. It enables users to deploy different roles of LLM-
based agents and utilize them to solve tasks via working in
collaboration. In addition, other similar frameworks, such
46. https://www.langchain.com/



Source: data\tc21_2501.12557v1\referenced_papers\[51]_2402.01680.pdf (Page 1):

Figure 1: The rising trend in the research field of LLM-based Multi-Agents. For Problem Solving and World Simulation, we categorize
current work into several categories and count the number of papers of different types at 3-month intervals. The number at each leaf node
denotes the count of papers within that category.
a wider range of interdisciplinary studies employing LLMs.
Readers will gain a comprehensive overview of LLM-based
Multi-Agent (LLM-MA) systems, grasp the fundamental
concepts involved in establishing multi-agent systems based
on LLMs, and catch the latest research trends and applica-
tions in this dynamic field. We recognize that this field is in
its early stages and is rapidly evolving with fresh methodolo-
gies and applications. To provide a sustainable resource com-
plementing our survey paper, we maintain an open-source
GitHub repository1. We hope that our survey will inspire fur-
ther exploration and innovation in this field, as well as appli-
cations across a wide array of research disciplines.
To assist individuals from various backgrounds in under-
standing LLM-MA techniques and to complement existing
surveys by tackling unresolved questions, we have organized
our survey paper in the following manner. After laying out
the background knowledge in Section 2, we address a piv-
otal question: How are LLM-MA systems aligned with the
collaborative task-solving environment? To answer this, we
present a comprehensive schema for positioning, differenti-
ating, and connecting various aspects of LLM-MA systems
in Section 3. We delve into this question by discussing: 1)
the agents-environment interface, which details how agents
interact with the task environment; 2) agent profiling, which
explains how an agent is characterized by an LLM to behave
1https://github.com/taichengguo/LLM MultiAgents Survey Papers
in specific ways; 3) agent communication, which examines
how agents exchange messages and collaborate; and 4)agent
capability acquisition, which explores how agents develop
their abilities to effectively solve problems. An additional
perspective for reviewing studies about LLM-MA is their ap-
plication. In Section 4, we categorize current applications
into two primary streams: multi-agents for problem-solving
and multi-agents for world simulation. To guide individuals
in identifying appropriate tools and resources , we present
open-source implementation frameworks for studying LLM-
MA, as well as the usable datasets and benchmarks in Sec-
tion 5 . Based on the previous summary, we open the dis-
cussion for future research challenges and opportunities in
Section 6. The conclusions are summarized in Section 7.
2 Background
2.1 Single-Agent Systems Powered LLMs
We introduce the background by first outlining the capabili-
ties of a single-agent system based on LLMs, following the
discussion presented in [Weng, 2023].
Decision-making Thought: This term denotes the capabil-
ity of LLM-based agents, guided by prompts, to break down
complex tasks into smaller subgoals[Khot et al., 2023], think
through each part methodically (sometimes exploring mul-
tiple paths) [Yao et al. , 2023 ], and learn from past experi-
ences [Shinn et al., 2023] to perform better decision-making



Source: data\tc21_2501.12557v1\referenced_papers\[51]_2402.01680.pdf (Page 10):

approaches in current research involve employing Memory
and Self-Evolution techniques to adjust agents based on feed-
back. While effective for individual agents, these methods do
not fully capitalize on the potential collective intelligence of
the agent network. They adjust agents in isolation, overlook-
ing the synergistic effects that can emerge from coordinated
multi-agent interactions. Hence, jointly adjusting multiple
agents and achieving optimal collective intelligence is still a
critical challenge for LLM-MA.
6.4 Scaling Up LLM-MA Systems
LLM-MA systems are composed of a number of individual
LLM-based agents, posing a significant challenge of scala-
bility regarding the number of agents. From the computa-
tional complexity perspective, each LLM-based agent, typ-
ically built on large language models like GPT-4, demands
substantial computational power and memory. Scaling up the
number of these agents in an LLM-MA system significantly
increases resource requirements. In scenarios with limited
computational resource, it would be challenging to develop
these LLM-MA systems.
Additionally, as the number of agents in an LLM-MA sys-
tem increases, additional complexities and research opportu-
nities emerge, particularly in areas like efficient agent coor-
dination, communication, and understanding the scaling laws
of multi-agents. For instance, with more LLM-based agents,
the intricacy of ensuring effective coordination and commu-
nication rises significantly. As highlighted in [Dibia, 2023],
designing advanced Agents Orchestration methodologies is
increasingly important. These methodologies aim to opti-
mize agents workflows, task assignments tailored to differ-
ent agents, and communication patterns across agents such as
communication constraints between agents. Effective Agents
Orchestration facilitates harmonious operation among agents,
minimizing conflicts and redundancies. Additionally, explor-
ing and defining the scaling laws that govern the behavior and
efficiency of multi-agent systems as they grow larger remains
an important area of research. These aspects highlight the
need for innovative solutions to optimize LLM-MA systems,
making them both effective and resource-efficient.
6.5 Evaluation and Benchmarks
We have summarized the datasets and benchmarks currently
available for LLM-MA in Table 2. This is a starting point, and
far from being comprehensive. We identify two significant
challenges in evaluating LLM-MA systems and benchmark-
ing their performance against each other. Firstly, as discussed
in [Xu et al., 2023a], much of the existing research focuses
on evaluating individual agents’ understanding and reason-
ing within narrowly defined scenarios. This focus tends to
overlook the broader and more complex emergent behaviors
that are integral to multi-agent systems. Secondly, there is a
notable shortfall in the development of comprehensive bench-
marks across several research domains, such as Science Team
for Experiment Operations, Economic analysis, and Disease
propagation simulation. This gap presents an obstacle to ac-
curately assessing and benchmarking the full capabilities of
LLM-MA systems in these varied and crucial fields.
6.6 Applications and Beyond
The potential of LLM-MA systems extends far beyond their
current applications, holding great promise for advanced
computational problem-solving in fields such as finance, edu-
cation, healthcare, environmental science, urban planning and
so on. As we have discussed, LLM-MA systems possess the
capability to tackle complex problems and simulate various
aspects of the real world. While the current role-playing ca-
pabilities of LLMs may have limitations, ongoing advance-
ments in LLM technology suggest a bright future. It is an-
ticipated to have more sophisticated methodologies, applica-
tions, datasets, and benchmarks tailored for diverse research
fields. Furthermore, there are opportunities to explore LLM-
MA systems from various theoretical perspectives, such as
Cognitive Science [Sumers et al., 2023], Symbolic Artificial
Intelligence, Cybernetics, Complex Systems, and Collective
Intelligence. Such a multi-faceted approach could contribute
to a more comprehensive understanding and innovative appli-
cations in this rapidly evolving field.
7 Conclusion
LLM-based Multi-Agents have shown inspiring collective in-
telligence and rapidly garnered increasing interest among re-
searchers. In this survey, we first systematically review the
development of LLM-MA systems by positioning, differen-
tiating, and connecting them from various aspects, regard-
ing the agents-environment interface, the characterization of
agents by LLMs, the strategies for managing agent communi-
cation and the paradigms for capability acquisition. We also
summarized LLM-MA applications for problem-solving and
world simulation. By also highlighting the commonly used
datasets and benchmarks and discussing challenges and fu-
ture opportunities, we hope that this survey can serve as a use-
ful resource for researchers across various research fields, in-
spiring future research to explore the potential of LLM-based
Multi-Agents.
References
[Agashe et al., 2023] Saaket Agashe, Yue Fan, and Xin Eric
Wang. Evaluating multi-agent coordination abilities in
large language models, 2023.
[Aher et al., 2023] Gati Aher, Rosa I. Arriaga, and
Adam Tauman Kalai. Using large language models
to simulate multiple humans and replicate human subject
studies, 2023.
[Akata et al., 2023] Elif Akata, Lion Schulz, Julian Coda-
Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz.
Playing repeated games with large language models.arXiv
preprint arXiv:2305.16867, 2023.
[Anonymous, 2023] Anonymous. Rethinking the buyer’s in-
spection paradox in information markets with language
agents. In Submitted to The Twelfth International Con-
ference on Learning Representations, 2023. under review.
[Chan et al., 2023] Chi-Min Chan, Weize Chen, Yusheng
Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and
Zhiyuan Liu. Chateval: Towards better llm-based evalua-
tors through multi-agent debate, 2023.



Source: data\tc21_2501.12557v1\referenced_papers\[51]_2402.01680.pdf (Page 2):

on complex tasks. This capability enhances the autonomy
of a single LLM-based agent and bolsters its effectiveness in
problem-solving.
Tool-use: LLM-based agents’ tool-use capability allows
them to leverage external tools and resources to accom-
plish tasks, enhancing their functional capabilities and oper-
ate more effectively in diverse and dynamic environments[Li
et al., 2023d; Ruan et al., 2023; Gao et al., 2023b].
Memory: This ability refers to the capability of LLM-
based agent for conducting in-context learning [Dong et al.,
2023a] as short memory or external vector database[Lewis et
al., 2021] as long memory to preserve and retrieve informa-
tion over prolonged periods[Wang et al., 2023b]. This ability
enables a single LLM-based agent to maintain contextual co-
herence and enhance learning from interactions.
2.2 Single-Agent VS. Multi-Agent Systems
Single-Agent systems empowered by LLMs have shown in-
spiring cognitive abilities [Sumers et al. , 2023 ]. The con-
struction of such systems concentrates on formulating their
internal mechanisms and interactions with the external en-
vironment. Conversely, LLM-MA systems emphasize di-
verse agent profiles, inter-agent interactions, and collective
decision-making processes. From this perspective, more dy-
namic and complex tasks can be tackled by the collaboration
of multiple autonomous agents, each of which is equipped
with unique strategies and behaviors, and engaged in com-
munication with one another.
3 Dissecting LLM-MA Systems: Interface,
Profiling, Communication, and Capabilities
In this section, we delve into the intricacies of LLM-MA sys-
tems, where multiple autonomous agents engage in collabo-
rative activities akin to human group dynamics in problem-
solving scenarios. A critical inquiry we address is how
these LLM-MA systems are aligned to their operational envi-
ronments and the collective objectives they are designed to
achieve. To shed light on this, we present the general ar-
chitecture of these systems in Fig. 2. Our analysis dissects
the operational framework of these systems, focusing on four
key aspects: the agents-environment interface, agent profil-
ing, agent communication, and agent capability acquisition.
3.1 Agents-Environment Interface
The operational environments defines the specific contexts or
settings in which the LLM-MA systems are deployed and
interact. For example, these environments can be like soft-
ware development [Hong et al., 2023], gaming [Mao et al.,
2023], and various other domains such as financial markets
[Li et al., 2023g] or even social behavior modeling [Park et
al., 2023 ]. The LLM-based agents perceive and act within
the environment, which in turn influences their behavior and
decision making. For example, in the Werewolf Game simu-
lation, the sandbox environment sets the game’s framework,
including transitions from day to night, discussion periods,
voting mechanics, and reward rules. Agents, such as were-
wolves and the Seer, perform specific actions like killing or
checking roles. Following these actions, agents receive feed-
back from the environment, informing them of the game’s
current state. This information guides the agents in adjust-
ing their strategies over time, responding to the evolving
gameplay and interactions with other agents. The Agents-
Environment Interface refers to the way in which agents in-
teract with and perceive the environment. It’s through this
interface that agents understand their surroundings, make de-
cisions, and learn from the outcomes of their actions. We
categorize the current interfaces in LLM-MA systems into
three types, Sandbox, Physcial, and None, as detailed in Ta-
ble 1. The Sandbox refers to a simulated or virtual environ-
ment built by human where agents can interact more freely
and experiment with various actions and strategies. This kind
of interface is widely used in software development (code
interpreter as simulated environment) [Hong et al. , 2023 ],
gaming (using game rules as simulated environment) [Mao
et al., 2023], etc. The Physical is a real-world environment
where agents interact with physical entities and obey real-
world physics and constraints. In physical space, agents nor-
mally need to take actions that can have direct physical out-
comes. For example, in tasks such as sweeping the floor,
making sandwiches, packing groceries, and arranging cab-
inets, robotic agents are required to perform actions itera-
tively, observe the physical environment, and continuously
refine their actions [Mandi et al., 2023]. Lastly, None refers
to scenarios where there is no specific external environment,
and agents do not interact with any environment. For exam-
ple, many applications [Du et al., 2023; Xiong et al., 2023;
Chan et al., 2023 ] utilize multiple agents to debate a ques-
tion to reach a consensus. These applications primarily focus
on communication among agents and do not depend on the
external environment.
3.2 Agents Profiling
In LLM-MA systems, agents are defined by their traits, ac-
tions, and skills, which are tailored to meet specific goals.
Across various systems, agents assume distinct roles, each
with comprehensive descriptions encompassing characteris-
tics, capabilities, behaviors, and constraints. For instance,
in gaming environments, agents might be profiled as players
with varying roles and skills, each contributing differently to
the game’s objectives. In software development, agents could
take on the roles of product managers and engineers, each
with responsibilities and expertise that guide the development
process. Similarly, in a debating platform, agents might be
designated as proponents, opponents, or judges, each with
unique functions and strategies to fulfill their roles effectively.
These profiles are crucial for defining the agents’ interactions
and effectiveness within their respective environments. Table
1 lists the agent Profiles in recent LLM-MA works.
Regarding the Agent Profiling Methods , we categorized
them into three types: Pre-defined, Model-Generated , and
Data-Derived. In the Pre-defined cases, agent profiles are
explicitly defined by the system designers. The Model-
Generated method creates agent profiles by models, e.g.,
large language models. The Data-Derived method involves
constructing agent profiles based on pre-existing datasets.



#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[128]_2403.15112.pdf (Page 5):

Additionally, very recent work by Keraghel et al. [16] offers a preliminary
discussion on the potential of using LLM embeddings for text clustering.
The authors analysed embeddings from BLOOMZ, Mistral, Llama-2, and
OpenAI using five clustering algorithms. This paper extends that work
by focusing on different datasets, testing additional LLM embeddings, and
comparing results using classical TF-IDF as a baseline. Furthermore, this
study experiments with summarisation as a dimensionality reduction tech-
nique and evaluates the impact of model size on clustering results.
3. Methods
This research evaluates the effectiveness of various embedding represen-
tations in enhancing the performance of text clustering algorithms, aiming
to identify the most informative embeddings for clustering tasks. To achieve
this, we systematically experimented with multiple datasets, embeddings,
and clustering methods, and performed an in-depth analysis of clustering
results using different evaluation metrics. The following steps were under-
taken during the course of this study:
1. Selection of datasets, ensuring the robustness of our findings across
different types of textual data.
2. Preprocessing of datasets, including the removal of miscellaneous char-
acters such as emails and HTML tags.
3. Utilisation of various embedding computations, including LLM-related
ones, to retrieve numerical text representations.
4. Application of several clustering algorithms commonly used in text
clustering.
5. Comparison of clustering results using different external and internal
validation metrics.
The following subsections provide a comprehensive description of each
of these steps, further detailing the employed methodologies.
3.1. Datasets
We selected five datasets to cover a variety of text clustering challenges.
Table 1 shows these datasets and their characteristics. The CSTR abstracts
dataset [17] is a corpus of 299 scientific abstracts from the Centre for Speech
Technology Research. The homogeneous and domain-specific nature of the
6



Source: data\tc21_2501.12557v1\referenced_papers\[128]_2403.15112.pdf (Page 3):

Word embeddings, such as those produced by Word2Vec [3] and GloVe [4],
marked a significant advancement by generating dense vector representa-
tions of words based on their contexts. These models leveraged surrounding
words and large corpora to learn word relationships, successfully capturing
a range of semantic and syntactic similarities. Despite their effectiveness in
capturing semantic regularities, these models still provided a single static
vector per word, which posed limitations in handling polysemous words—
words with multiple meanings.
The arrival of BERT (Bidirectional Encoder Representations from Trans-
formers) initiated a new phase of embedding sophistication [5]. To generate
contextual embeddings, BERT employs a bidirectional transformer archi-
tecture, pre-trained on a massive corpus. This allows for a deeper under-
standing of word relationships by considering the full context of a word in a
sentence in both directions. BERT revolutionised tasks like text clustering
by providing richer semantic representations.
Today, LLMs like OpenAI’s GPT are at the forefront of generating state-
of-the-art embeddings [6]. LLMs extend the capabilities of previous models
by providing an unprecedented depth and breadth of knowledge encoded in
word and sentence-level embeddings. These models are trained on extensive
datasets to capture a broad spectrum of human language variations and
generate embeddings that reflect a comprehensive understanding of contexts
and concepts.
The progression from TF-IDF to sophisticated LLM embeddings rep-
resents a significant advancement towards more contextually aware text
representation in NLP. This evolution continues to propel the field forward,
expanding the possibilities for applications such as text clustering, sentiment
analysis, and beyond. In the context of text clustering methodologies, there
exists a considerable research gap that underscores the need for a compre-
hensive evaluation of LLM embeddings against traditional techniques such
as TF-IDF.
2.2. Text Clustering Algorithms
Text clustering involves grouping a set of texts such that texts in the
same group (referred to as a cluster) are more similar to each other than
to those in different clusters. This section provides an overview of classic
clustering algorithms widely used for clustering textual data.
K-means is perhaps the most well-known and commonly used clustering
algorithm due to its simplicity and efficiency. It partitions the dataset intok
clusters by minimising the within-cluster sum of squares, i.e., variance. Each
4



Source: data\tc21_2501.12557v1\referenced_papers\[128]_2403.15112.pdf (Page 0):

The peer-reviewed version of this paper is published in the International Journal of Cognitive Computing in Engineering at
https://doi.org/10.1016/j.ijcce.2024.11.004. This version is typeset by the authors and differs only in pagination
and typographical detail.
Text Clustering with Large Language Model
Embeddings
Alina Petukhovaa,∗, Jo˜ ao P. Matos-Carvalhoa,b, Nuno Fachadaa,b
aCOPELABS, Lus´ ofona University, Campo Grande, 376, Lisbon, 1700-921, Portugal
bCenter of Technology and Systems (UNINOVA-CTS) and Associated Lab of Intelligent
Systems (LASI), Caparica, 2829-516, Portugal
Abstract
Text clustering is an important method for organising the increasing volume
of digital content, aiding in the structuring and discovery of hidden patterns
in uncategorised data. The effectiveness of text clustering largely depends
on the selection of textual embeddings and clustering algorithms. This
study argues that recent advancements in large language models (LLMs)
have the potential to enhance this task. The research investigates how dif-
ferent textual embeddings, particularly those utilised in LLMs, and various
clustering algorithms influence the clustering of text datasets. A series of
experiments were conducted to evaluate the impact of embeddings on clus-
tering results, the role of dimensionality reduction through summarisation,
and the adjustment of model size. The findings indicate that LLM embed-
dings are superior at capturing subtleties in structured language. OpenAI’s
GPT-3.5 Turbo model yields better results in three out of five clustering
metrics across most tested datasets. Most LLM embeddings show improve-
ments in cluster purity and provide a more informative silhouette score,
reflecting a refined structural understanding of text data compared to tra-
ditional methods. Among the more lightweight models, BERT demonstrates
leading performance. Additionally, it was observed that increasing model
dimensionality and employing summarisation techniques do not consistently
enhance clustering efficiency, suggesting that these strategies require careful
consideration for practical application. These results highlight a complex
balance between the need for refined text representation and computational
feasibility in text clustering applications. This study extends traditional
text clustering frameworks by integrating embeddings from LLMs, offering
improved methodologies and suggesting new avenues for future research in
various types of textual analysis.
Keywords: text clustering, large language models, text summarisation
2000 MSC: 68T50, 62H30
1
arXiv:2403.15112v5  [cs.CL]  2 Dec 2024



Source: data\tc21_2501.12557v1\referenced_papers\[128]_2403.15112.pdf (Page 12):

Table 6: Parameters used for summarisation with LLaMA-2 and Falcon models:
temperature represents the value to modulate probabilities of the next token, max length
defines the maximum length of the sequence to be generated, do sample is a parameter
that determines whether to use sampling rather than greedy decoding, top k restricts the
selection to the top k tokens with the highest probabilities during top- k filtering, and
num return sequences is the number of independently computed returned sequences for
each element in the batch.
Parameter name Value
temperature 0
max length 800
do sample True
top k 10
num return sequences 1
The following zero-shot prompt was used for generating the summarised
text with LLMs:
Write a concise summary of the text. Return your responses
with maximum 5 sentences that cover the key points of the text.
{text}
SUMMARY:
3.6.2. Increasing Model Dimension
The original publications on LLMs underline the performance increase
with larger model sizes in tasks such as common sense reasoning, question
answering, and code tasks [27, 28]. This experiment evaluates embeddings
from various LLM sizes to analyse the impact of higher-dimensional models
on the performance of clustering algorithms, aiming to determine if they
enhance cluster cohesion and separation. For this purpose, we used embed-
dings obtained from models presented in Table 7.
To visualise the different embeddings and capture their intrinsic struc-
tures, Principal Component Analysis (PCA) and t-Distributed Stochastic
Neighbor Embedding ( t-SNE) [42] were employed. Initially, PCA was ap-
plied for preliminary dimensionality reduction while preserving variance.
Subsequently, t-SNE was used to project the data into a lower-dimensional
space, emphasising local disparities between embeddings. This sequential
13



Source: data\tc21_2501.12557v1\referenced_papers\[128]_2403.15112.pdf (Page 2):

to determine if embeddings derived from LLMs outperform traditional em-
bedding techniques, such as Term Frequency-Inverse Document Frequency
(TF-IDF). Additionally, experiments are conducted to evaluate the impact
of model size and dimensionality reduction through summarisation tech-
niques on clustering performance.
Results indicate that LLM embeddings are highly effective at captur-
ing the structured aspects of language, with BERT demonstrating superior
performance among lightweight models. Moreover, increasing model di-
mensionality and employing summarisation techniques do not consistently
enhance clustering efficiency, suggesting that these strategies require careful
evaluation for practical application. These findings underscore the need to
balance detailed text representation with computational feasibility in text
clustering tasks.
This paper is organised as follows. In Section 2, advancements in textual
embeddings are described, and classical text clustering algorithms used in
this domain are briefly mentioned. Section 3 outlines the main steps and
components of this study, including dataset selection, data preprocessing,
embeddings, and clustering algorithm configurations used to assess clus-
tering quality. Section 4 presents the results of our study and provides
a discussion of these findings. Limitations encountered during the study,
along with recommendations for overcoming them, are acknowledged in Sec-
tion 5. Finally, Section 6 synthesises the main conclusions of this research
and suggests future developments.
2. Background
2.1. Text Embeddings
The field of text representation in natural language processing (NLP) has
undergone an impressive transformation over the past few decades. From
simple representations to highly sophisticated embeddings, advancements in
this domain have significantly improved the ability of machines to process
and understand human language with increasing accuracy.
One of the earliest methods of text representation that laid the ground-
work for subsequent advances was Term Frequency-Inverse Document Fre-
quency (TF-IDF). This method quantifies the importance of a word within a
document relative to a corpus by accounting for term frequency and inverse
document frequency [1]. While TF-IDF effectively highlights the relevance
of words, it treats each term as independent and fails to capture word con-
text and semantic meaning [2].
3



### Claim 13/24

#### Claim Text
Considering that awareness is an important factor influencing vaccination, Kabir et al. proposed a framework for vaccine uptake with the unaware-aware (UA) information propagation.

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 8):

Fig. 6 : Response coverage is high without essentializing identity. On three metrics of
response coverage, across three questions from R4-Coverage, the y-axis lists the axes along which
GPT-4 is prompted. Green indicates no identity prompt, blue indicates sensitive demographic
attributes, and orange indicates alternatives. Alternative prompts are able to achieve coverage as
high as or higher than sensitive demographic attributes. Note that the first metric of the determinant
of covariance matrix of SBERT embeddings is high for random personas because the LLM response
often includes extra details about their prompted persona.
Boomer, Gen Z for age; random sampling of three like Gemini, Capricorn, Scorpio for astrology)
with 33 responses each.
We find no model requires prompting with sensitive demographic attributes to attain the highest
amount of coverage (Fig. 6). Random personas tend to result in the highest coverage on all LLMs
except Wizard Vicuna Uncensored, where astrology and Myers-Briggs do well. As expected, generic
tends to have the lowest coverage.
Reason for Harm: Identity Essentialization
Of our four considered reasons for identity-prompting LLMs, R4-Coverage may seem to be the
most permissible since the goal is to increase the coverage of responses, rather than replace human
participants. However, when alternatives to prompting with sensitive demographic attributes exist
(e.g., prompting with behavioral personas or political view), we may wish to opt for the latter due
9



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 14):

Examining ethics of LLM use in HCI research practices Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
While HCI researchers are increasingly using LLM-based tools within their ‘everyday’ practices,
our research community still lacks well-established guidelines and best practices, contributing to
the complexity of navigating ethical considerations. In addition, researchers noted a perceived lack
of control over the functionality and outputs of LLMs. Unlike previous tools where researchers had
more predictability and influence over the behavior, LLMs presented outputs that may not always
align with explicit instructions. Finally, many participants spoke of the challenges of navigating
ethical concerns within the broader ecosystem of the ‘LLM supply chain’ [28]. Below, we present
the implications of researchers’ approaches and opportunities for HCI researchers to better engage
with ethical considerations of LLMs as part of their projects, with a goal to support the formation
of emerging ethical norms in LLM-impacted HCI research.
Proactively engaging with IRB and other regulatory institutions.Within the U.S. context,
Institutional Review Boards (IRBs) are responsible for reviewing and monitoring research activities
to protect the rights and welfare of human research subjects [75]. One of the primary responsibilities
of the IRB office is to assess whether “risks to subjects are reasonable in relation to anticipated
benefits” [74]. Given their expertise in ensuring ethical conduct in research practices, the IRB office
may be well-positioned to advise on identifying ethical concerns and potential prevention strategies.
To establish whether a research study will undergo a comprehensive review, they determine if it is
a minimal risk study where the “probability and magnitude of harm anticipated in the research are
not greater than those ordinarily encountered in daily life” [74].
Our findings reveal that most HCI researchers did not consider it necessary to report their usage
of LLMs to the IRB, in part due to their perception of LLMs as everyday tools. According to some,
their scope of use did not justify including additional details, while others did not anticipate their
use of LLMs at the time of submitting the IRB application. For those who disclosed their use of
LLMs to the IRB, they used the minimal risk rule [ 74] to indicate that research subjects would
typically encounter harmful outputs in their daily life as well. However, these practices can have
short-term and long-term adverse implications. In the short term, the opacity in research practices
(e.g., which tools are used to generate research artifacts or analyze data) might lead to challenges in
replicating studies [50] and understanding the motivation and effects of methodological decisions
[90]. In the long term, the consequences of limited disclosure might extend beyond individual
studies to shaping the trajectory of the community, for example, by informing which research
topics we pursue.
We invite HCI researchers to proactively engage with the IRB at the time of study design to unpack
the likelihood and ways in which any potential LLM use might harm our participants. This includes
careful reflection and documentation of any implicit or anticipated use during the project planning
stage. Furthermore, researchers should actively implement mechanisms to monitor LLM outputs
and any adverse impacts on participants throughout the project life-cycle, especially if used for
system building. In addition, openly communicating and collaborating with IRB members to uncover
potential harms is also important to increase transparency. Embracing guidelines, policies and
oversight is essential for ensuring responsible development and use of these technologies. Bockting
et al. [6] urged scientists to oversee the use of generative AI because “controlling developments in AI
will require a continuous process that balances expertise and independence. ” We call for collaborative
efforts between researchers, policymakers, and generative AI companies to create a set of ‘living
guidelines’ for the responsible use of generative AI in research [6].
Re-examining the informed consent process.In our study, researchers acknowledged the
limited transparency to their study participants, either by not disclosing LLM use (e.g., considering
it irrelevant unless directly involved in system building), or by characterizing the underlying
technology as Artificial Intelligence (AI). This was often rationalized as a way to avoid overwhelming
participants with seemingly unnecessary technical details. However, terminologies can be powerful
15



Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 30):

(a) Llama-2.
 (b) Wizard Vicuna Uncensored.
(c) GPT-3.5-Turbo.
Fig. 11 : Response coverage is high without essentializing identity. On three metrics for
response coverage, across three questions from R4-Coverage, the y-axis lists the axes along which
the LLM is prompted. Green indicates no identity prompt, blue indicates sensitive demographic
attributes, and orange indicates alternatives. Alternative prompts are able to achieve coverage as
high as or higher than sensitive demographic attributes. Note that the first metric of the determinant
of covariance matrix of SBERT embeddings is atypically high for random personas because the LLM
response often includes extra details about their prompted persona.
31



Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 9):

to the harm of identity essentialization (i.e., legitimizing identities as rigid and innate), which can
amplify perceived inherent differences between groups [57]. 5
Some instances of identity essentialization are that GPT-4 prompted with the identity of Black
woman outputs “Hey girl!”, “Hey sis,” and “Oh, honey”; compared to White man with “Hey buddy,”
“Hey, friend!” and “Hey mate.” Llama-2 for Black women starts most responses with “Oh, girl,” and
uses phrases like “I’m like, YAASSSSS” and “That’s cray, hunty!” If we draw a parallel to designers
leveraging user personas [58], there is increasingly a recommendation to move away from personas
based on sensitive demographic attributes, which may rely on reductionist representations about
people [59, 60], and towards those based on behavioral characteristics [61].
Discussion
We have empirically shown the presence of two critical limitations and one further consideration of
identity-prompted LLMs. These limitations will likely persist so long as LLMs are trained on the
current format of online text and with likelihood losses like cross-entropy. Thus, these limitations
cannot be easily resolved by newer models. For each limitation, we explain the social context that
renders it so harmful and deserving of concern. However, acknowledging there are use cases geared
towards supplementing rather than replacing human participants (e.g., pilot studies), we provide
possible alternatives that can alleviate the harm, to an extent. We have also shown how even in a
seemingly more permissible use case of increasing coverage, identity-prompting LLMs may not be a
reasonable direction.
Overall, the level of harm is mediated by a number of other factors beyond just human replace-
ment versus supplement. The reason motivating the prompting of identity matters as well. The
primary distinction between R1-Contingent compared to R2-Relevant and R3-Subjective is that
for R1-Contingent social location determines meaning and truth, whereas for R2-Relevant and R3-
Subjective social location bears on meaning and truth [7, 39]. This entails that LLM replacement
based on R1-Contingent has a higher normative consequence [6, 9]. On the other hand, identity is
still important for R2-Relevant and R3-Subjective, which is why representative sampling tends to
be used in those settings. However, given our empirical findings are the weakest on R3-Subjective,
this reason will likely result in relatively less harm than the others, and can be deemed permissible
in certain use cases. For example, in annotating datasets that would be too expensive to hand-code.
Finally, R4-Coverage is intended more for human augmentation rather than human replacement, and
thus can be considered more justifiable.
Overlaid across this is the difference between can and should regarding LLM replacement of
human participants [62–65]. Geddes [66] offers an illuminating analysis: they describe the autonomy-
violating harms that can come from predicting individual behaviors like votes in democratic elections,
warning “When prediction is cheap, allowing individuals to retain decisional autonomy will feel
increasingly costly.” This ability to cheaply generate large samples can also increase the opportunity
for inflating the statistical power of studies. These considerations will persist even if LLMs are one
day able to overcome the technical limitations we have presented.
We have limited our analysis to a set of 16 demographic groups in America—but so many more
are likely to be harmed by these limitations. For example, 37% of the world’s population has never
accessed the Internet, and thus are unlikely to be well-represented in LLM training data [67]. We see
our work as shedding light on the important concern of LLM usage erasing marginalized voices, and
in so doing, also acknowledge the importance of not forgetting those that are not online to begin with.
Methods
We begin by describing in further detail our demographic and LLM selection process, then each of
our four reasons as well as how we chose the question(s) that belong to each. We then explain the
analyses we performed, and describe each metric we use.
5While there could be legitimate reasons for needing the particular coverage brought about by different demographic
attributes, e.g., people from different social locations might be more sensitive to anticipating different kinds of harms, for
these situations we defer to the analysis on R2-Relevant. Here we are purely focused on the idea of expanding coverage of
possible situations and discovering “edge cases.”
10



Source: data\tc21_2501.12557v1\referenced_papers\[76]_2403.19876.pdf (Page 17):

Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Kapania and Wang, et al.
6 LIMITATIONS AND FUTURE WORK
Our study sheds light on the emerging practices regarding LLM ethics among HCI researchers, but it
has limitations due to its exploratory nature. Firstly, while we sought to offer a deep understanding
on how HCI researchers navigate LLM ethics, our sample was constrained by our snowball sampling
method. HCI research encompasses a wide range of diverse research traditions, many of which we
were unable to include in our study. This limitation highlights the need for more comprehensive and
systematic future studies in this area. Our interview sample primarily consists of researchers from
the USA, and future research may want to further explore the ethical challenges and practices related
to the use of LLMs by researchers from other regions. Secondly, there’s a potential selection bias in
our study: we may have primarily attracted respondents who are conscious of their LLM usage
and are open to discussing their experiences in a research setting. To gain a broader perspective,
future research should explore how ethical practices with LLMs vary across different research
methodologies, domains, and settings, including both industry and academia.
7 CONCLUSION
In this paper, we drew empirical data from a survey and interviews to explore how HCI researchers
have currently integrated LLMs into their research practices, what ethical concerns they have
encountered as well as how they’ve navigated those concerns. Our results suggested that although
HCI researchers have used LLMs across their research processes and are aware of a wide variety
of ethical concerns, in many cases, they have challenges in effectively identifying and navigating
those concerns in their own projects. Reflecting on these findings, we discuss potential approaches
to support the formation of emerging ethical norms for using LLMs in HCI research. We encourage
HCI researchers to proactively engage with IRB and collaborate with policymakers and generative
AI companies on creating guidelines for the responsible use of LLMs. We also identify the need
to re-examine the informed consent process and provide technological support to interrupt the
LLM supply chain. In addition, we discuss the importance of creating learning opportunities for
the ethics of LLMs use in HCI and shifting academic incentives to prioritize ethical concerns.
REFERENCES
[1] Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi
Iqbal, Paul N. Bennett, Kori Inkpen, Jaime Teevan, Ruth Kikin-Gil, and Eric Horvitz. 2019. Guidelines for Human-AI
Interaction. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk)
(CHI ’19) . Association for Computing Machinery, New York, NY, USA, 1–13. https://doi.org/10.1145/3290605.3300233
[2] Alissa N Antle. 2017. The ethics of doing research with vulnerable populations. Interactions 24, 6 (2017), 74–77.
[3] Christopher Bail. 2023. Can generative artificial intelligence improve social science.
[4] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers
of stochastic parrots: Can language models be too big?. In Proceedings of the 2021 ACM conference on fairness,
accountability, and transparency . 610–623.
[5] ACM Publications Board. 2021. ACM Publications Policy on Research Involving Human Participants and Subjects.
https://www.acm.org/publications/policies/research-involving-human-participants-and-subjects. (Accessed on
01/16/2024).
[6] Claudi L. Bockting, Eva A. M. van Dis, Robert van Rooij, Willem Zuidema, and Johan Bollen. 2023. Living guidelines
for generative AI — why scientists must oversee its use. Nature 622, 7984 (Oct. 2023), 693–696. https://doi.org/10.
1038/d41586-023-03266-1
[7] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3,
2 (2006), 77–101.
[8] Virginia Braun and Victoria Clarke. 2019. Reflecting on reflexive thematic analysis. Qualitative research in sport,
exercise and health 11, 4 (2019), 589–597.
[9] Barry Brown, Alexandra Weilenmann, Donald McMillan, and Airi Lampinen. 2016. Five Provocations for Ethical HCI
Research. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (San Jose, California,
USA) (CHI ’16). Association for Computing Machinery, New York, NY, USA, 852–863. https://doi.org/10.1145/2858036.
18



### Claim 14/24

#### Claim Text
Characteristic site preference values have been reported for many biological sources, e.g., for denitrifying bacteria ( SP ≈ –5‰) and nitrifying bacteria ( SP ≈+30‰) .

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[122]_2308.02828.pdf (Page 15):

16 Shuyin Ouyang, Jie M. Zhang, Mark Harman, and Meng Wang
TPR mean value
TPR mean variance
TPR mean max diff
OER mean
OER_noex mean
LCS mean
LCS worst
LED mean
LED worst
United_Diff mean
United_Diff worst
Tree_Diff mean
Tree_Diff worst
description length
difficulty
time_limit
cf_rating
TPR mean value
TPR mean variance
TPR mean max diff
OER mean
OER_noex mean
LCS mean
LCS worst
LED mean
LED worst
United_Diff mean
United_Diff worst
Tree_Diff mean
Tree_Diff worst
description length
difficulty
time_limit
cf_rating
1.0 0.47 0.53 0.46 0.65 0.22 - -0.16 - 0.18 - 0.19 0.16 -0.22 - - -
0.47 1.0 0.93 - - - - - - - - - - - - - -0.16
0.53 0.93 1.0 -0.16 - - - - - - - - - - - - -
0.46 - -0.16 1.0 0.79 - - - - - - - - - - - -
0.65 - - 0.79 1.0 0.27 0.2 -0.17 - 0.21 0.18 0.24 0.22 -0.17 - - -
0.22 - - - 0.27 1.0 0.89 -0.71-0.64 0.42 0.45 0.35 0.4 -0.29-0.22 - -0.31
- - - - 0.2 0.89 1.0 -0.63-0.59 0.42 0.48 0.35 0.43 -0.27-0.22-0.16-0.28
-0.16 - - - -0.17-0.71-0.63 1.0 0.95 -0.37-0.38-0.28-0.34 0.38 0.2 - 0.3
- - - - - -0.64-0.59 0.95 1.0 -0.38-0.39-0.29-0.35 0.36 - - 0.26
0.18 - - - 0.21 0.42 0.42 -0.37-0.38 1.0 0.92 0.95 0.87 -0.27 - -0.2 -
- - - - 0.18 0.45 0.48 -0.38-0.39 0.92 1.0 0.86 0.94 -0.21 - -0.18 -0.2
0.19 - - - 0.24 0.35 0.35 -0.28-0.29 0.95 0.86 1.0 0.91 -0.24 - -0.21 -
0.16 - - - 0.22 0.4 0.43 -0.34-0.35 0.87 0.94 0.91 1.0 -0.21 - -0.18 -
-0.22 - - - -0.17-0.29-0.27 0.38 0.36 -0.27-0.21-0.24-0.21 1.0 - - -
- - - - - -0.22-0.22 0.2 - - - - - - 1.0 0.28 0.64
- - - - - - -0.16 - - -0.2 -0.18-0.21-0.18 - 0.28 1.0 0.44
- -0.16 - - - -0.31-0.28 0.3 0.26 - -0.2 - - - 0.64 0.44 1.0
0.6
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
Fig. 6. RQ4: Correlations between coding tasks and non-determinism (CodeContests, temperature=1). Only
significant correlations will be displayed on the heatmap, while the insignificant correlations (i.e. p-value >
0.05) are masked by ‘-’.
is the description of the first code problem, where we present only the core part of the description
due to the extensive length of the overall content.
, Vol. 1, No. 1, Article . Publication date: October 2024.



Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 29):

Fig. 10: Temperature hyperparameter does not solve flatness for Wizard Vicuna Uncen-
sored. Same interpretation as Fig. 5: comparison of human in-group diversity to Wizard Vicuna
Uncensored generations varying levels of temperature settings, where by 1.8 the responses become
incoherent. At this setting even though the unique n-gram metric shows the LLM surpassing humans
in diversity, this is only due to the incoherence as under no other semantic metric is human diversity
reached.
30



Source: data\tc21_2501.12557v1\referenced_papers\[122]_2308.02828.pdf (Page 14):

An Empirical Study of the Non-determinism of ChatGPT in Code Generation 15
Table 6. RQ3: Similarity for different request ways (CodeContests), where t represents the temperature
setting.
Request Test Pass Rate
Way Mean value Mean variance Mean max diff Max diff Ratio of worst cases
R1 (t=1) 0.17 0.03 0.28 1.00 8.70%
R2 (t=1) 0.16 0.03 0.24 1.00 3.64%
R1 (t=0) 0.18 0.00 0.00 0.00 1.20%
R2 (t=0) 0.15 0.01 0.11 1.00 1.82%
Request OER OER (no ex.)
Mean value Ratio of worst cases Pair mean value Mean value Ratio of worst cases Pair mean value
R1 (t=1) 0.09 76.09% 0.27 0.04 83.70% 0.18
R2 (t=1) 0.09 75.76% 0.27 0.06 81.21% 0.19
R1 (t=0) 1.00 1.20% 1.00 0.81 12.05% 0.81
R2 (t=0) 0.37 43.64% 0.59 0.27 54.55% 0.46
Request LCS LED
Way Mean value Mean worst value Pair mean value Mean value Mean worst value Pair mean value
R1 (t=1) 0.21 0.15 0.20 61.30 82.73 63.09
R2 (t=1) 0.22 0.16 0.23 58.80 77.46 58.86
R1 (t=0) 1.00 1.00 1.00 0.00 0.00 0.00
R2 (t=0) 0.61 0.44 0.62 23.45 35.87 22.31
Request United_Diff Tree_Diff
Way Mean value Mean worst value Pair mean value Mean value Mean worst value Pair mean value
R1 (t=1) 0.98 0.98 0.98 0.98 0.98 0.98
R2 (t=1) 0.33 0.27 0.46 0.41 0.33 0.56
R1 (t=0) 1.00 1.00 1.00 1.00 1.00 1.00
R2 (t=0) 0.41 0.39 0.67 0.50 0.46 0.74
characters) for each coding task. Note that in this section, we only focus on correlation analysis,
and we do not aim to obtain any causal conclusions.
Figure 6 shows the results for code problems in CodeContests under temperature=1. The rest
figures can be found on our homepage [ 3]. We observe that description length has a negative
correlation with most of the measurements, except LED. This means that problems with longer
descriptions tend to generate code with more randomness. We suspect that this is because a
longer description may reduce ChatGPT’s understanding of the coding requirements. With longer
descriptions, different code candidates tend to be uniformly worse in their pass rates. Moreover,
the description length has a negative correlation with LCS and structural measurements and a
positive correlation with LED, which means that problems with longer descriptions tend to yield
more inconsistent code candidates in syntax and structure. For temperature = 0, we observe that
description length still has a negative correlation with most of the measurements, except LED,
which is similar to the correlation result under temperature=1.
The difficulty has a positive correlation with the LED and a negative correlation with LCS, which
means that the problem with a higher difficulty level has high non-determinism in syntax. Similar
to difficulty, CF rating also has a positive correlation with the LED and a negative correlation with
LCS.
In the following, we provide some specific examples to further illustrate our observations above.
In exploring the relationship between the length of a code problem description and the degree of
non-determinism, two contrasting examples in the CodeContests dataset corroborate our findings.
The first example, ‘1599_E. Two Arrays’, with a description length of 2149, show a pattern that code
generation with a longer description code problem has a higher degree of non-determinism. Below
, Vol. 1, No. 1, Article . Publication date: October 2024.



Source: data\tc21_2501.12557v1\referenced_papers\[122]_2308.02828.pdf (Page 11):

12 Shuyin Ouyang, Jie M. Zhang, Mark Harman, and Meng Wang
Table 4. RQ1.3: Structural similarity.
Structural Similarity Metric CodeContests APPS HumanEval
United_Diff Mean value 0.33 0.43 0.60
Mean worst value 0.27 0.35 0.47
Pair mean value 0.46 0.52 0.67
Tree_Diff Mean value 0.41 0.54 0.62
Mean worst value 0.33 0.47 0.48
Pair mean value 0.56 0.63 0.70
CodeContests       APPS       HumanEval
0.00
0.25
0.50
0.75
1.00
UnifiedDiff TreeDiff
(a) Mean
CodeContests       APPS       HumanEval
0.00
0.25
0.50
0.75
1.00
UnifiedDiff TreeDiff (b) Mean Worst
CodeContests       APPS       HumanEval
0.00
0.25
0.50
0.75
1.00
UnitedDiff TreeDiff (c) Pair Mean
Fig. 5. RQ1.3: Structural Similarity (United_Diff & Tree_Diff).
We observe that the code candidates generated from the same instruction show great similarity
in structure. Specifically, the mean values are 0.33, 0.43, and 0.60 under the United_Diff setting, and
0.41, 0.54, and 0.62 under Tree_Diff setting for CodeContests, APPS, and HumanEval, respectively.
For the three datasets, we could see from Table 4 that the lowest values under United_Diff and
Tree_Diff happen for the CodeContests dataset. By contrast, the largest values under the two
settings both happen for HumanEval. This indicates that ChatGPT is most unstable in structure for
the code generation tasks in CodeContests, and most stable for HumanEval. We further explore the
correlation between different similarities and task features in RQ4.
Answer to RQ1.3: Code candidates show high structural similarity under UnitedDiff and
TreeDiff settings. We observe that the code candidates generated from the same instruction
have high similarity in structure. Specifically, the mean values are 0.33, 0.43, and 0.60 under
the United_Diff setting, and 0.41, 0.54, and 0.62 under Tree_Diff setting for CodeContests,
APPS, and HumanEval, respectively.
4.2 RQ2: Influence of Temperature
The default temperature of ChatGPT is 111. This RQ explores whether the code generation non-
determinism of ChatGPT changes with the temperature changes. We use identical measurements
as in RQ1. We show our experiment results on CodeContests only. Results for other datasets are on
our homepage [3].
Table 5 shows the results. Overall, we observe that when temperature=0, ChatGPT has better
determinism than the default configuration ( temperature=1) for all three types of similarities.
However, setting the temperature to 0 does not completely avoid non-determinism. Take OER
as an example, there are still 43.64% (CodeContests), 27.40% (APPS), and 18.29% (HumanEval)
11https://platform.openai.com/docs/api-reference/chat/create#chat-create-temperature
, Vol. 1, No. 1, Article . Publication date: October 2024.



Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 6):

Fig. 4 : LLMs flatten groups. Across all four LLMs (rows), each point indicates the diversity
measurement averaged across 3-6 questions asked for each identity. 95% confidence intervals are
generated through cluster bootstrapping with each question as a cluster. Each column represents a
different measure of diversity, and the larger the number on the x-axis, the more diverse the responses
are. The gray crosses indicate human participant in-group responses, while colored circles represent
LLM responses. Nearly every single model and identity group across each metric has less diverse
LLM responses compared to human responses.
output. For our experiments we have used the default temperature setting of 1. Thus, we run a
further analysis on the intersectional demographic axis and show in Fig. 5 the temperature settings
of [1.0, 1.2, 1.4] for GPT-4. We stop at 1.4 because GPT-4 devolves into nonsensical phrasing (e.g.,
“...fon resir’ potions cutramTes frequently sandwiched...”). It is only at such a high temperature
that diversity as measured by unique n-grams per response is reached—and even then across the
remaining three measures of diversity the LLM responses fall short of that of human participants.
7



### Claim 15/24

#### Claim Text
substituted minor isotopocules (14N217O, 14N218O, 14N15N16O, and 15N14N16O), have recently been included in the ExoMol database .

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[97]_2404.01268.pdf (Page 18):

Preprint
D Word Frequency Shift in arXiv Computer Science introductions
2021.1 2021.4 2021.7 2021.10 2022.1 2022.4 2022.7 2022.10 2023.1 2023.4 2023.7 2023.10 2024.10.0
20.0
40.0
60.0
80.0
100.0
120.0
140.0
160.0Frequency per one million words
ChatGPT
Launch
Nov 30, 2022
intricate
pivotal
realm
showcasing
Figure 11: Word Frequency Shift in sampled arXiv Computer Science introductions
in the past two years. The plot shows the frequency over time for the same 4 words as
demonstrated in Figure 2. The words are: realm, intricate, showcasing, pivotal. The trend
is similar for two figures. Data from 2010-2020 is not included in this analysis due to the
computational complexity of parsing the full text from a large number of arXiv papers.
19



Source: data\tc21_2501.12557v1\referenced_papers\[128]_2403.15112.pdf (Page 19):

20
 10
 0 10 20
30
20
10
0
10
20
0
1
2
3
(a) LLaMA-2-7b-chat-hf.
30
 20
 10
 0 10 20 30
20
15
10
5
0
5
10
15
20 0
1
2
3 (b) LLaMA-2-13b-chat-hf.
20
 10
 0 10 20
20
10
0
10
20
0
1
2
3
(c) Falcon-7b.
20
 10
 0 10 20
20
10
0
10
20
0
1
2
3 (d) Falcon-40b.
Figure 1: Representation of different embeddings for the CSTR dataset, where PCA was
used as a preliminary dimensionality reduction algorithm and t-SNE for data projection
into a lower-dimensional space.
sociated with larger model sizes. This limitation potentially skews the un-
derstanding of the absolute efficacy of the embeddings computed for these
models, as performance improvements could only be inferred up to a specific
size.
Testing the selected clustering algorithms with a wider range of param-
eters could also provide additional insights into the results. However, due
20



Source: data\tc21_2501.12557v1\referenced_papers\[97]_2404.01268.pdf (Page 15):

Preprint
A Estimated Fraction of LLM-Modified Sentences in Introductions
2021.1 2021.4 2021.7 2021.10 2022.1 2022.4 2022.7 2022.10 2023.1 2023.4 2023.7 2023.10 2024.10.0%
2.5%
5.0%
7.5%
10.0%
12.5%
15.0%
17.5%
20.0%Estimated Alpha
ChatGPT
Launch
Nov 30, 2022
Computer Science(arXiv)
Electrical Engineering and Systems Science(arXiv)
Statistics(arXiv)
Physics(arXiv)
Nature portfolio
Mathematics(arXiv)
Figure 7: Estimated Fraction of LLM-Modified Sentences in Introductions Across Aca-
demic Writing Venues Over Time. We focused on the introduction sections for the main
texts, as the introduction was the most consistently and commonly occurring section across
different categories of papers. This figure presents the estimated fraction (α) of sentences in
introductions which are LLM-modified, across the same venues as Figure 1. We found that
the results are consistent with those observed in abstracts (Figure 1). We did not include
bioRxiv introductions as there is no bulk download of PDFs available. Error bars indicate
95% confidence intervals by bootstrap.
16



Source: data\tc21_2501.12557v1\referenced_papers\[122]_2308.02828.pdf (Page 15):

16 Shuyin Ouyang, Jie M. Zhang, Mark Harman, and Meng Wang
TPR mean value
TPR mean variance
TPR mean max diff
OER mean
OER_noex mean
LCS mean
LCS worst
LED mean
LED worst
United_Diff mean
United_Diff worst
Tree_Diff mean
Tree_Diff worst
description length
difficulty
time_limit
cf_rating
TPR mean value
TPR mean variance
TPR mean max diff
OER mean
OER_noex mean
LCS mean
LCS worst
LED mean
LED worst
United_Diff mean
United_Diff worst
Tree_Diff mean
Tree_Diff worst
description length
difficulty
time_limit
cf_rating
1.0 0.47 0.53 0.46 0.65 0.22 - -0.16 - 0.18 - 0.19 0.16 -0.22 - - -
0.47 1.0 0.93 - - - - - - - - - - - - - -0.16
0.53 0.93 1.0 -0.16 - - - - - - - - - - - - -
0.46 - -0.16 1.0 0.79 - - - - - - - - - - - -
0.65 - - 0.79 1.0 0.27 0.2 -0.17 - 0.21 0.18 0.24 0.22 -0.17 - - -
0.22 - - - 0.27 1.0 0.89 -0.71-0.64 0.42 0.45 0.35 0.4 -0.29-0.22 - -0.31
- - - - 0.2 0.89 1.0 -0.63-0.59 0.42 0.48 0.35 0.43 -0.27-0.22-0.16-0.28
-0.16 - - - -0.17-0.71-0.63 1.0 0.95 -0.37-0.38-0.28-0.34 0.38 0.2 - 0.3
- - - - - -0.64-0.59 0.95 1.0 -0.38-0.39-0.29-0.35 0.36 - - 0.26
0.18 - - - 0.21 0.42 0.42 -0.37-0.38 1.0 0.92 0.95 0.87 -0.27 - -0.2 -
- - - - 0.18 0.45 0.48 -0.38-0.39 0.92 1.0 0.86 0.94 -0.21 - -0.18 -0.2
0.19 - - - 0.24 0.35 0.35 -0.28-0.29 0.95 0.86 1.0 0.91 -0.24 - -0.21 -
0.16 - - - 0.22 0.4 0.43 -0.34-0.35 0.87 0.94 0.91 1.0 -0.21 - -0.18 -
-0.22 - - - -0.17-0.29-0.27 0.38 0.36 -0.27-0.21-0.24-0.21 1.0 - - -
- - - - - -0.22-0.22 0.2 - - - - - - 1.0 0.28 0.64
- - - - - - -0.16 - - -0.2 -0.18-0.21-0.18 - 0.28 1.0 0.44
- -0.16 - - - -0.31-0.28 0.3 0.26 - -0.2 - - - 0.64 0.44 1.0
0.6
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
Fig. 6. RQ4: Correlations between coding tasks and non-determinism (CodeContests, temperature=1). Only
significant correlations will be displayed on the heatmap, while the insignificant correlations (i.e. p-value >
0.05) are masked by ‘-’.
is the description of the first code problem, where we present only the core part of the description
due to the extensive length of the overall content.
, Vol. 1, No. 1, Article . Publication date: October 2024.



Source: data\tc21_2501.12557v1\referenced_papers\[128]_2403.15112.pdf (Page 16):

Table 8: Results of text clustering for the best-performing clustering algorithms for each
combination of dataset and embedding. The best algorithm was determined by choosing
the algorithm with the highest F1-score value. DS1 represents the CSTR dataset, DS2 is
the SyskillWebert dataset, DS3 is the 20newsgroups dataset, DS4 is the MN-DS dataset
for level 1 labels, DS5 is the MN-DS dataset for level 2 labels, and DS6 is the Reuters
dataset. ‘Total’ represents the number of metrics per given embeddings/algorithm com-
bination that outperform other combinations.
Dataset Embed. Best Alg. F1S ARI HS SS CHI Total
DS1 TF-IDF k-means 0.67 0.38 0.46 0.016 4 0/5
BERT Spectral 0.85 0.60 0.63 0.118 25 3/5
OpenAI k-means 0.84 0.59 0.64 0.066 13 1/5
LLaMA-2 k-means 0.41 0.09 0.17 0.112 49 1/5
Falcon k-means 0.74 0.39 0.48 0.111 34 0/5
DS2 TF-IDF Spectral 0.82 0.63 0.58 0.028 8 0/5
BERT AHC 0.74 0.58 0.53 0.152 37 0/5
OpenAI AHC 0.90 0.79 0.75 0.070 19 3/5
LLaMA-2 k-means 0.51 0.21 0.25 0.137 69 0/5
Falcon k-means++ 0.45 0.26 0.30 0.170 85 2/5
DS3 TF-IDF Spectral 0.35 0.13 0.28 -0.002 37 0/5
BERT k-means 0.43 0.25 0.44 0.048 412 0/5
OpenAI k-means 0.69 0.52 0.66 0.035 213 3/5
LLaMA-2 AHC 0.17 0.11 0.26 0.025 264 0/5
Falcon k-means 0.26 0.15 0.30 0.071 1120 2/5
DS4 TF-IDF k-means 0.29 0.13 0.48 0.034 17 0/5
BERT k-means 0.35 0.24 0.55 0.072 61 1/5
OpenAI k-means 0.38 0.26 0.58 0.053 42 3/5
LLaMA-2 k-means 0.21 0.11 0.40 0.053 88 0/5
Falcon k-means++ 0.27 0.16 0.48 0.071 92 1/5
DS5 TF-IDF AHC 0.31 0.09 0.29 0.010 37 0/5
BERT k-means++ 0.43 0.27 0.42 0.060 178 2/5
OpenAI Spectral 0.45 0.25 0.41 0.036 120 1/5
LLaMA-2 AHC 0.23 0.10 0.23 0.031 263 0/5
Falcon k-means++ 0.28 0.12 0.25 0.070 359 2/5
DS6 TF-IDF FuzzyCM 0.51 0.19 0.20 0.01 74 0/5
BERT Spectral 0.51 0.32 0.35 0.02 37 1/5
OpenAI FuzzyCM 0.52 0.23 0.21 0.10 1095 3/5
LLaMA-2 k-means++ 0.19 0.08 0.63 0.07 518 1/5
Falcon FuzzyCM 0.22 -0.03 0.21 0.00 930 0/5
17



### Claim 16/24

#### Claim Text
In a slightly different approach is taken: a preliminary velocity field is first interpolated to Lagrangian marker locations on the fluid-solid interface (as in Peskin’s original method), then the force term is computed in the spirit of (5.2), and finally the force is transferred back to the Eulerian grid, again using Peskin’s original spreading operator.

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 78):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
pθ(xt−1|xt) = N(xt−1; µθ(xt,t),Σθ(xt,t)) (2)
where θrepresents the parameters of the model.
In low-dimensional settings, we make the following key observations:
1. Limited spatial complexity: Low-dimensional data has fewer spatial relationships to exploit during
the diffusion process compared to high-dimensional data (e.g., images).
2. Increased sensitivity to noise scheduling: The choice of noise schedule βt becomes more critical
in low-dimensional spaces, as small variations can have a more pronounced effect on the generated
samples.
3. Need for adaptive noise levels: To capture the nuances of low-dimensional data distributions,
spatially adaptive noise levels may be beneficial.
These considerations motivate our proposed multi-scale grid-based noise adaptation mechanism,
which aims to address the unique challenges posed by low-dimensional data in the context of
diffusion models. Our approach, detailed in Section 4, leverages a combination of coarse (5×5) and
fine (20×20) grids to dynamically adjust noise levels during the diffusion process, allowing for more
precise control over the generation of low-dimensional samples.
4 M ETHOD
Building upon the foundations of diffusion models introduced in Section 3, we propose a multi-scale
grid-based noise adaptation mechanism to address the unique challenges posed by low-dimensional
data. Our method enhances the standard diffusion process by introducing spatially and temporally
adaptive noise levels, allowing for more precise control over the generation process in low-dimensional
spaces.
4.1 M ULTI -SCALE GRID STRUCTURE
We introduce two learnable grids: a coarse 5×5 grid Gc for capturing large-scale patterns and a fine
20×20 grid Gf for localized adjustments. The noise adjustment factor α(x,t) for a data point x ∈X
at timestep tis defined as:
α(x,t) = αc(x,t) ·αf(x,t) (3)
where αc(x,t) and αf(x,t) are bilinearly interpolated values from Gc and Gf, respectively. Both
grids are initialized with ones and learned during training, allowing the model to discover optimal
noise patterns.
4.2 M ODIFIED DIFFUSION PROCESS
We modify the forward diffusion process defined in Section 3 to incorporate the grid-based noise
adaptation:
q(xt|xt−1) = N(xt;
√
1 −βtxt−1,α(xt−1,t)βtI) (4)
This adaptation allows the noise level to vary spatially and temporally, providing more precise control
over the diffusion process in low-dimensional spaces.
The reverse process is similarly modified:
pθ(xt−1|xt) = N(xt−1; µθ(xt,t,α (xt,t)),Σθ(xt,t,α (xt,t))) (5)
5



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 74):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
MULTI -SCALE GRID NOISE ADAPTATION : E NHANCING
DIFFUSION MODELS FOR LOW-DIMENSIONAL DATA
Anonymous authors
Paper under double-blind review
ABSTRACT
Diffusion models have demonstrated remarkable success in generating high-
dimensional data, but their application to low-dimensional datasets presents unique
challenges due to limited spatial complexity and the need for precise noise schedul-
ing. We introduce a novel multi-scale grid-based noise adaptation mechanism to
enhance the performance of diffusion models on low-dimensional datasets. Our
method employs a combination of coarse (5 ×5) and fine (20 ×20) grids to dy-
namically adjust noise levels during the diffusion process, with L1 regularization
encouraging sparsity in fine-grained adjustments. We evaluate our approach on
four diverse 2D datasets: circle, dino, line, and moons. Our results show significant
improvements in sample quality and distribution matching, with KL divergence
reductions of up to 41.6% compared to standard diffusion models. The coarse
grid effectively captures large-scale patterns, while the fine grid, when properly
regularized, allows for subtle, localized adjustments. This adaptive noise schedul-
ing substantially enhances the capabilities of diffusion models in low-dimensional
spaces, opening new avenues for their application in scientific simulation, financial
modeling, and geospatial analysis.
1 I NTRODUCTION
Diffusion models have emerged as a powerful class of generative models, achieving remarkable
success in generating high-dimensional data such as images and audio Ho et al. (2020); Yang et al.
(2023). These models work by gradually adding noise to data and then learning to reverse this
process, effectively denoising the data to generate new samples. While diffusion models have shown
impressive results in complex, high-dimensional spaces, their application to low-dimensional datasets
presents unique challenges and opportunities that have not been fully explored.
Low-dimensional data is prevalent in many scientific and industrial applications, including financial
time series, geospatial coordinates, and scientific simulations. Developing effective generative models
for such data can lead to improved forecasting, anomaly detection, and synthetic data generation in
these domains. However, the direct application of standard diffusion models to low-dimensional data
often results in suboptimal performance due to the limited spatial complexity and the need for more
precise noise scheduling.
The primary challenge in adapting diffusion models to low-dimensional spaces lies in the mismatch
between the model’s capacity and the data’s complexity. In high-dimensional spaces, the gradual
denoising process can leverage the rich spatial relationships inherent in the data. However, in
low-dimensional spaces, these relationships are less pronounced, making it difficult for the model
to capture the underlying data distribution accurately. Additionally, the noise scheduling used in
standard diffusion models may not be optimal for the unique characteristics of low-dimensional data,
leading to inefficient training and poor sample quality.
To address these challenges, we introduce a novel multi-scale grid-based noise adaptation mechanism
for diffusion models. Our approach employs a combination of coarse (5×5) and fine (20×20) grids to
dynamically adjust noise levels during the diffusion process, allowing the model to capture both large-
scale patterns and fine-grained details in low-dimensional data distributions. The key contributions of
our work are:
1



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 95):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang,
Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and
applications. ACM Computing Surveys, 56(4):1–39, 2023.
9



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 83):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
of up to 16.83. Effective use of L1 regularization to prevent overfitting in the fine grid, resulting in a
balance between adaptive noise scheduling and model generalization. 4. Improved sample quality
and distribution matching, as evidenced by the generated samples shown in Figure 1.
Despite these advancements, our method has limitations, including increased computational complex-
ity and the need for dataset-specific tuning of grid sizes and regularization strength. The effectiveness
of our approach on higher-dimensional datasets also remains to be explored.
Future work directions include:
1. Extending the method to higher-dimensional datasets (3D, 4D, etc.) to broaden its applicability.
2. Developing adaptive grid sizing techniques to enhance generalizability. 3. Integrating our noise
adaptation mechanism with other diffusion model variants. 4. Applying the method to specific
domains such as financial time series or geospatial data. 5. Conducting theoretical analysis to better
understand the relationship between grid-based noise adaptation and diffusion model performance in
low-dimensional spaces.
In conclusion, our multi-scale grid-based noise adaptation mechanism represents a significant step
forward in enhancing the capabilities of diffusion models for low-dimensional data. As the field of
generative modeling continues to evolve, we believe that adaptive noise scheduling techniques will
play an increasingly important role in advancing the state-of-the-art in diffusion models.
REFERENCES
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling,
C. Cortes, N. Lawrence, and K.Q. Weinberger (eds.), Advances in Neural Information Processing
Systems, volume 27. Curran Associates, Inc., 2014. URLhttps://proceedings.neurips.
cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models.
In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems , volume 33, pp. 6840–6851. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf.
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of
diffusion-based generative models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and
Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL
https://openreview.net/forum?id=k7FuTOWMOc7.
Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014.
Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. Tabddpm: Modelling
tabular data with diffusion models, 2022.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In Francis Bach and David Blei (eds.),Proceedings
of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine
Learning Research, pp. 2256–2265, Lille, France, 07–09 Jul 2015. PMLR.
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang,
Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and
applications. ACM Computing Surveys, 56(4):1–39, 2023.
10



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 75):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
• A multi-scale grid-based noise adaptation mechanism that enhances the performance of
diffusion models on low-dimensional datasets.
• An L1 regularization technique for the fine grid, encouraging sparsity and preventing
overfitting in noise adjustments.
• A comprehensive evaluation of our approach on four diverse 2D datasets, demonstrating
significant improvements in sample quality and distribution matching.
• Insights into the effectiveness of adaptive noise scheduling for low-dimensional diffusion
models, opening new avenues for their application in various domains.
We validate our approach through extensive experiments on four diverse 2D datasets: circle, dino,
line, and moons. Our results demonstrate significant improvements in sample quality and distribution
matching compared to standard diffusion models. We observe KL divergence reductions of up to
36.8% for the line dataset and 22.5% for the moons dataset, indicating a substantial enhancement in
the model’s ability to capture the underlying data distribution. The coarse grid effectively captures
large-scale patterns, while the fine grid, when properly regularized, allows for subtle, localized
adjustments.
Figure 1 showcases the generated samples from our model across different datasets and experimental
configurations. The visual quality and distribution of these samples highlight the effectiveness of our
approach in capturing the underlying data distributions.
The success of our grid-based noise adaptation mechanism in low-dimensional spaces suggests
promising directions for future research. Extending this approach to higher-dimensional data and
exploring its applicability to specific domain problems, such as financial modeling or geospatial
analysis, could lead to significant advancements in these fields. Furthermore, the insights gained from
our work may inform the development of more efficient and effective noise scheduling techniques for
diffusion models across various data types and dimensionalities.
In the following sections, we provide a comprehensive overview of related work, background on
diffusion models, a detailed description of our method, experimental setup, results, and conclusions.
Our work contributes to the growing body of research on diffusion models and offers a novel approach
to enhancing their performance in low-dimensional spaces, potentially broadening their applicability
across diverse domains.
2 R ELATED WORK
Our work on enhancing diffusion models for low-dimensional data builds upon several key areas of
research in generative modeling. We discuss relevant advancements in adaptive noise scheduling,
applications of diffusion models to low-dimensional data, and spatial adaptations in generative
models.
2.1 A DAPTIVE NOISE SCHEDULING IN DIFFUSION MODELS
Recent work has highlighted the importance of noise scheduling in diffusion models. The Elucidating
Diffusion Models (EDM) framework Karras et al. (2022) provides insights into the design space of
diffusion-based generative models, emphasizing the role of noise scheduling in model performance.
While EDM focuses on high-dimensional data such as images, our work extends the concept of
adaptive noise scheduling to low-dimensional spaces.
Unlike EDM, which proposes a global noise schedule optimization, our approach introduces spatially-
aware noise adaptation through a multi-scale grid mechanism. This distinction is crucial in low-
dimensional settings, where the limited spatial complexity necessitates more fine-grained control
over the noise distribution.
2.2 L OW-DIMENSIONAL APPLICATIONS OF DIFFUSION MODELS
The application of diffusion models to low-dimensional data has gained attention recently, with works
like TabDDPM Kotelnikov et al. (2022) adapting these models for tabular data generation. While
2



### Claim 17/24

#### Claim Text
In reality, vaccination strategies tend not to be specific to each age, but are differentiated according to, for example, the age groups of the population: children, adults, and the elderly .

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 8):

Fig. 6 : Response coverage is high without essentializing identity. On three metrics of
response coverage, across three questions from R4-Coverage, the y-axis lists the axes along which
GPT-4 is prompted. Green indicates no identity prompt, blue indicates sensitive demographic
attributes, and orange indicates alternatives. Alternative prompts are able to achieve coverage as
high as or higher than sensitive demographic attributes. Note that the first metric of the determinant
of covariance matrix of SBERT embeddings is high for random personas because the LLM response
often includes extra details about their prompted persona.
Boomer, Gen Z for age; random sampling of three like Gemini, Capricorn, Scorpio for astrology)
with 33 responses each.
We find no model requires prompting with sensitive demographic attributes to attain the highest
amount of coverage (Fig. 6). Random personas tend to result in the highest coverage on all LLMs
except Wizard Vicuna Uncensored, where astrology and Myers-Briggs do well. As expected, generic
tends to have the lowest coverage.
Reason for Harm: Identity Essentialization
Of our four considered reasons for identity-prompting LLMs, R4-Coverage may seem to be the
most permissible since the goal is to increase the coverage of responses, rather than replace human
participants. However, when alternatives to prompting with sensitive demographic attributes exist
(e.g., prompting with behavioral personas or political view), we may wish to opt for the latter due
9



Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 28):

Table 2: Demographics of Human Participants. Each row indicates the self-reported gen-
der, race, and age categories selected by the 100 human participants in each study. The gray
rows indicate the study where out-group members for the identity were solicited. The numbers
for some of the demographic axes add up to more than 100 because participants are able to check
multiple genders and races, and others opted out and chose not to disclose their identity. For gen-
der, G1=Woman, G2=Man, G3=Non-binary/third gender, G4=other. For race, R1=American
Indian or Alaska Native, R2=Asian, R3=Black or African American, R4=Hispanic or Latinx,
R5=Native Hawaiian or Other Pacific Islander, R6=White, R7=Other. For age in years, A1=18-
24, A2=25-34, A3=35-44, A4=45-54, A5=55-64, A6=65-74, A7=75-84.
Study Gender Race Age
Axis Iden G1 G2 G3 G4 R1 R2 R3 R4 R5 R6 R7 A1 A2 A3 A4 A5 A6 A7
Black
man
0 100 0 0 0 0 100 1 0 0 0 9 40 18 21 10 2 0
Black
man
60 37 1 0 1 7 9 9 0 83 1 8 26 34 12 14 5 1
Black
woman
100 0 0 0 0 0 100 1 0 0 0 4 14 21 33 18 8 2
Black
woman
52 48 0 0 3 9 5 9 0 83 2 10 42 18 11 13 6 0
White
man
0 100 0 0 0 0 0 3 0 100 1 10 28 32 16 7 6 1
White
man
59 41 0 0 0 19 26 13 0 47 0 10 35 27 16 8 3 1
White
woman
100 0 0 0 1 0 0 5 0 100 0 11 17 23 21 16 9 3
intersect
White
woman
41 58 1 0 1 37 16 10 0 39 0 19 43 15 15 5 2 1
Black 47 52 1 0 2 0 100 3 0 1 1 11 33 23 22 11 0 0
Black 51 43 4 0 0 19 0 5 2 84 2 14 32 20 17 12 3 2
Asian 36 62 0 0 0 100 0 0 0 4 0 22 37 22 12 6 1 0
Asian 51 47 2 0 5 0 24 14 0 82 0 11 32 22 19 7 9 0
White 67 31 2 1 0 0 0 2 0 100 0 12 22 30 19 13 2 2
race
White 40 56 0 0 3 28 48 20 0 1 3 8 40 31 17 4 0 0
man 0 100 0 0 1 9 9 11 0 74 2 15 38 22 16 6 3 0
man 100 0 1 0 1 13 7 12 0 77 1 18 36 20 14 8 3 1
woman 100 1 0 0 2 6 13 10 0 81 2 9 29 20 18 19 4 0
woman 0 99 1 0 2 10 9 12 0 73 2 14 34 22 18 7 4 1
non-
binary
4 4 100 0 0 9 6 14 1 80 5 31 50 10 4 4 1 0gender
non-
binary
48 52 0 0 2 8 17 9 0 69 1 9 40 21 20 6 3 1
Baby
Boomer
66 34 0 0 1 1 4 3 0 91 2 0 0 0 0 54 42 4
Baby
Boomer
43 51 6 0 2 11 10 10 0 76 2 15 31 31 19 4 0 0
Millen 40 59 0 0 4 8 5 19 1 79 2 0 64 36 0 0 0 0
Millen 53 46 1 0 0 5 12 12 0 74 0 37 19 1 17 17 8 1
Gen Z 46 53 2 1 0 23 13 18 2 59 5 71 29 0 0 0 0 0
age
Gen Z 47 51 1 0 2 5 12 5 0 80 0 0 38 30 16 8 8 0
w/o
disabil-
ities
43 56 1 0 0 10 8 8 0 78 0 14 21 23 21 17 4 0
w/o
disabil-
ities
62 36 3 1 3 10 7 8 2 79 3 8 24 25 20 20 2 1
w/
ADD
or
ADHD
40 56 4 0 4 10 9 10 1 75 2 16 44 25 8 5 0 2
w/
ADD
or
ADHD
43 53 3 1 0 10 7 13 0 72 1 20 21 20 18 12 7 2
w/
impaired
vision
49 45 6 0 4 6 14 16 0 78 2 7 35 21 16 13 7 1disability
w/
impaired
vision
47 44 7 0 2 4 12 6 0 76 3 7 31 21 13 17 9 2
29



Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 9):

to the harm of identity essentialization (i.e., legitimizing identities as rigid and innate), which can
amplify perceived inherent differences between groups [57]. 5
Some instances of identity essentialization are that GPT-4 prompted with the identity of Black
woman outputs “Hey girl!”, “Hey sis,” and “Oh, honey”; compared to White man with “Hey buddy,”
“Hey, friend!” and “Hey mate.” Llama-2 for Black women starts most responses with “Oh, girl,” and
uses phrases like “I’m like, YAASSSSS” and “That’s cray, hunty!” If we draw a parallel to designers
leveraging user personas [58], there is increasingly a recommendation to move away from personas
based on sensitive demographic attributes, which may rely on reductionist representations about
people [59, 60], and towards those based on behavioral characteristics [61].
Discussion
We have empirically shown the presence of two critical limitations and one further consideration of
identity-prompted LLMs. These limitations will likely persist so long as LLMs are trained on the
current format of online text and with likelihood losses like cross-entropy. Thus, these limitations
cannot be easily resolved by newer models. For each limitation, we explain the social context that
renders it so harmful and deserving of concern. However, acknowledging there are use cases geared
towards supplementing rather than replacing human participants (e.g., pilot studies), we provide
possible alternatives that can alleviate the harm, to an extent. We have also shown how even in a
seemingly more permissible use case of increasing coverage, identity-prompting LLMs may not be a
reasonable direction.
Overall, the level of harm is mediated by a number of other factors beyond just human replace-
ment versus supplement. The reason motivating the prompting of identity matters as well. The
primary distinction between R1-Contingent compared to R2-Relevant and R3-Subjective is that
for R1-Contingent social location determines meaning and truth, whereas for R2-Relevant and R3-
Subjective social location bears on meaning and truth [7, 39]. This entails that LLM replacement
based on R1-Contingent has a higher normative consequence [6, 9]. On the other hand, identity is
still important for R2-Relevant and R3-Subjective, which is why representative sampling tends to
be used in those settings. However, given our empirical findings are the weakest on R3-Subjective,
this reason will likely result in relatively less harm than the others, and can be deemed permissible
in certain use cases. For example, in annotating datasets that would be too expensive to hand-code.
Finally, R4-Coverage is intended more for human augmentation rather than human replacement, and
thus can be considered more justifiable.
Overlaid across this is the difference between can and should regarding LLM replacement of
human participants [62–65]. Geddes [66] offers an illuminating analysis: they describe the autonomy-
violating harms that can come from predicting individual behaviors like votes in democratic elections,
warning “When prediction is cheap, allowing individuals to retain decisional autonomy will feel
increasingly costly.” This ability to cheaply generate large samples can also increase the opportunity
for inflating the statistical power of studies. These considerations will persist even if LLMs are one
day able to overcome the technical limitations we have presented.
We have limited our analysis to a set of 16 demographic groups in America—but so many more
are likely to be harmed by these limitations. For example, 37% of the world’s population has never
accessed the Internet, and thus are unlikely to be well-represented in LLM training data [67]. We see
our work as shedding light on the important concern of LLM usage erasing marginalized voices, and
in so doing, also acknowledge the importance of not forgetting those that are not online to begin with.
Methods
We begin by describing in further detail our demographic and LLM selection process, then each of
our four reasons as well as how we chose the question(s) that belong to each. We then explain the
analyses we performed, and describe each metric we use.
5While there could be legitimate reasons for needing the particular coverage brought about by different demographic
attributes, e.g., people from different social locations might be more sensitive to anticipating different kinds of harms, for
these situations we defer to the analysis on R2-Relevant. Here we are purely focused on the idea of expanding coverage of
possible situations and discovering “edge cases.”
10



Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 7):

Fig. 5: Temperature hyperparameter does not solve flatness for GPT-4. Comparison of
human in-group diversity to GPT-4 generations with varying levels of temperature settings, where
by 1.4 the responses become incoherent. At this setting even though the unique n-gram metric shows
GPT-4 surpassing humans in diversity, this is only due to the incoherence as under no other semantic
metric is human diversity reached.
There is increasingly research on different prompting techniques to increase output diversity [51,
52]. Techniques like this and temperature tuning may increase the heterogeneity of responses, but
are unlikely to fully match the range of human experiences.
Alternatives to demographic identity-prompting for
increasing coverage
We now foreground R4-Coverage: the practice of identity-prompting LLMs to inject variety into the
responses. Increasing response coverage may be useful in settings like simulating possible social inter-
actions [53], anticipating possible future harms [54], and exploring the range of possible responses
and edge cases in user studies [1]. Notably, here we are measuring coverage (i.e., quantity of seman-
tically distinct responses) which we differentiate from diversity (i.e., responses different from each
other) of the previous section.
Given that the claim for applications of R4-Coverage are not necessarily for LLMs to match human
participants, as is the case for R1-3, we do not compare to human responses but rather to LLMs
prompted with axes which are not sensitive demographic ones. Specifically, we compare to: Myers-
Briggs personality types [55], crowdsourced personas of at least five sentences each (e.g., “i have a cat
named george. my favorite meal is chicken and rice...”) [56], political leaning (i.e., liberal, moderate,
conservative), astrology signs (e.g., Gemini), and also no identity prompt (Generic). Instead of 100
samples as we have done so far, we use 99 by having 3 identities per axis (e.g., Millennial, Baby
8



Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 23):

(c) GPT-3.5-Turbo.
(d) GPT-4.
Fig. 9: LLMs flatten groups. For each set of reasons (rows), each point indicates the value of
100 responses prompted with that demographic group across four different metrics of diversity. 95%
confidence bars are provided, and the dark gray points indicate human participant in-group responses,
while colored points represent LLM responses. Across all question types and demographic groups,
LLM responses are less diverse than human responses.
human participants, we ask them to choose their own multiple choice answer that corresponds to the
free response. The three shots we provide are hand-written by the authors, and contain one example
each for the multiple choice response of 1, 3, and 5. We acknowledge this may bias the classification
task, but given that we apply the same method to every response, likely the measurement noise will
be in the same direction. The few-shot examples are all included below, with the number for each
quoted example indicating its value on the Likert scale:
24



### Claim 18/24

#### Claim Text
The stability and accuracy of the method hinges on the free parameterα.

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[97]_2404.01268.pdf (Page 5):

Preprint
d
c
e
ba
f
 g
Computer Science(arXiv)
Statistics(arXiv) Nature Portfolio
Electrical Engineering &
Systems Science(arXiv)
bioRxiv
Mathematics(arXiv)
Physics(arXiv)
jih
 Computer Science(arXiv) Electrical Engineering &
Systems Science(arXiv) Mathematics(arXiv)
mlk
 Physics(arXiv) Statistics(arXiv) Nature Portfolio
AbstractIntroduction
Figure 3: Fine-grained Validation of Model Performance Under Temporal Distribution
Shift. We evaluate the accuracy of our models in estimating the fraction of LLM-modified
content (α) under a challenging temporal data split, where the validation data (sampled
from 2022-01-01 to 2022-11-29) are temporally separated from the training data (collected up
to 2020-12-31) by at least a year. The X-axis indicates the ground truth α, while the Y-axis
indicates the model’s estimated α. In all cases, the estimation error for α is less than 3.5%.
The first 7 panels (a–g) are the validation on abstracts for each academic writing venue,
while the later 6 panels (h–m) are the validation on introductions. We did not includebioRxiv
introductions due to the unavailability of bulk PDF downloads. Error bars indicate 95%
confidence intervals by bootstrap.
5 Main Results and Findings
5.1 Temporal Trends in AI-Modified Academic Writing
Setup We apply the model to estimate the fraction of LLM-modified content (α) for each
paper category each month, for both abstracts and introductions. Each point in time was
independently estimated, with no temporal smoothing or continuity assumptions applied.
6



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 156):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
validation accuracy in 2063.3 steps on average, compared to 4720.0 steps in the baseline—a 56.3%
reduction. For the addition task, our approach maintained perfect accuracy (1.0) for training and
near-perfect accuracy (0.9998) for validation, reaching 99% validation accuracy in 1073.3 steps, a
54.6% improvement over the baseline’s 2363.3 steps.
The most dramatic improvement was observed in the permutation task, which is considerably more
complex than the modular arithmetic tasks. Our method achieved near-perfect accuracy (1.0 for
training, 0.9995 for validation), a substantial improvement over the baseline’s 0.0359 validation
accuracy. The model reached 99% validation accuracy in 5270.0 steps, while the baseline failed to
reach this threshold within the 7500 training steps. The final validation loss decreased from 6.8042 in
the baseline to 0.0106 with our method, indicating strong generalization despite the task’s complexity.
Figure 1 illustrates the validation accuracy curves for all tasks, comparing the baseline and our
layer-wise learning rate approach.
(a) Modular Division
 (b) Modular Subtraction
(c) Modular Addition
 (d) Permutation
Figure 1: Validation accuracy curves for all tasks, comparing baseline (Run 0) and layer-wise learning
rate approaches (Run 3).
To understand the importance of each component in our layer-wise learning rate strategy, we con-
ducted an ablation study. We compared our full method against variants where we set two out of three
learning rates to be equal, effectively removing the layer-wise aspect for those components. Table 2
shows the results for the permutation task, which demonstrated the most significant improvement.
Method Final Val Acc Steps to 99% Val Acc Final Val Loss
Full Method 0.9995 5270.0 0.0106
ηe = ηl 0.9624 7176.7 0.1648
ηe = ηh 0.9625 7176.7 0.1648
ηl = ηh 0.9625 7176.7 0.1648
Table 2: Ablation study results for the permutation task, comparing our full method against variants
with partially uniform learning rates.
7



Source: data\tc21_2501.12557v1\referenced_papers\[97]_2404.01268.pdf (Page 22):

Preprint
F Proofreading Results on arXiv data
Computer Science(arXiv) Electrical Engineering &
 Systems Science(arXiv)
Mathematics(arXiv) Physics(arXiv) Statistics(arXiv)0%
5%
10%
15%
20%Estimated Alpha  
Before Proofread
After Proofread
Figure 15: Robustness of estimations to proofreading. The plot demonstrates a slight
increase in the fraction of LLM-modified content after using Large Language Models (LLMs)
for “proofreading” across different arXiv main categories. This observation validates our
method’s robustness to minor LLM-generated text edits, such as those introduced by simple
proofreading. The analysis was conducted on 1,000 abstracts from each arXiv main category,
randomly sampled from the period between January 1, 2022, and November 29, 2022. Error
bars indicate 95% confidence intervals by bootstrap.
23



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 69):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
Table 1: Performance metrics for different experimental runs across datasets
Run Dataset KL Divergence Training Time (s) Inference Time (s)
Baseline
Circle 0.354 37.42 0.172
Dino 0.989 36.68 0.171
Line 0.161 37.15 0.160
Moons 0.090 36.61 0.168
Fixed Weighting
Circle 0.369 73.07 0.293
Dino 0.820 74.28 0.286
Line 0.172 76.55 0.275
Moons 0.100 74.56 0.272
Adaptive Weighting
Circle 0.347 89.83 0.302
Dino 0.871 88.43 0.290
Line 0.155 81.64 0.357
Moons 0.096 83.32 0.263
Weight Analysis
Circle 0.361 76.73 0.299
Dino 1.034 81.05 0.281
Line 0.148 86.87 0.294
Moons 0.100 82.37 0.279
Improved Weight Network
Circle 0.345 79.91 0.293
Dino 0.862 73.94 0.278
Line 0.153 72.15 0.274
Moons 0.093 74.75 0.265
6.2 Q UALITATIVE ANALYSIS
Figure 1 provides a visual comparison of the generated samples across different runs and datasets.
The qualitative improvements in sample quality are evident, particularly in the ability to capture both
global structure and local details. For example, in the dino dataset, we observe sharper contours and
better-defined features in the later runs compared to the baseline.
6.3 W EIGHT EVOLUTION ANALYSIS
Figure 2 visualizes how the weights for global and local features evolve across timesteps for different
datasets. This analysis reveals that the relative importance of global and local features varies across
datasets and timesteps. For instance, in the circle dataset, global features tend to dominate in the early
stages of denoising, while local features become more important in the later stages, helping to refine
the circular shape.
6.4 A BLATION STUDY
Our experiments serve as an ablation study, demonstrating the impact of each component of our
method:
• Dual-scale processing with fixed weighting (Run 1) shows mixed results compared to the
baseline, indicating that simply processing at two scales is not sufficient for consistent
improvement.
• Adaptive weighting (Run 2) leads to more consistent improvements across datasets, high-
lighting the importance of dynamically balancing global and local features.
• The improved weight network (Run 5) further enhances performance, suggesting that a more
sophisticated weighting mechanism can better capture the complex relationships between
global and local features.
9



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 144):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
(a) Training Loss
 (b) Validation Loss
Figure 2: Training and Validation Loss for x_minus_y task across different initialization methods
convergence, reaching 99% validation accuracy in 2347 steps, compared to 4720 steps for the
baseline.
(a) Training Accuracy
 (b) Validation Accuracy
Figure 3: Training and Validation Accuracy for x_div_y task across different initialization methods
The x_div_y task (modular division) proved to be the most challenging among the arithmetic
operations. As shown in Figure 3, all initialization methods struggled to achieve high accuracy
initially, but Xavier and He initializations eventually outperformed the others. Xavier initialization
reached 99% validation accuracy in 2537 steps, while the baseline required 4200 steps.
(a) Training Loss
 (b) Validation Loss
Figure 4: Training and Validation Loss for permutation task across different initialization methods
The permutation task exhibited unique learning dynamics compared to the arithmetic operations.
Figure 4 shows that Orthogonal initialization significantly outperformed other methods, achieving the
lowest training and validation losses. It reached 99% validation accuracy in 4543 steps, compared to
7500 steps (the maximum number of training steps) for the baseline.
8



### Claim 19/24

#### Claim Text
Note that the centroid of the beam cannot be detected around the zero offset angle, i.e., postselection angle ϵ = 0 as the Gaussian nature of the beam profile cannot be maintained and the intensity of the beam is almost comparable to the external noise level that results in error-prone centroid detection.

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[97]_2404.01268.pdf (Page 22):

Preprint
F Proofreading Results on arXiv data
Computer Science(arXiv) Electrical Engineering &
 Systems Science(arXiv)
Mathematics(arXiv) Physics(arXiv) Statistics(arXiv)0%
5%
10%
15%
20%Estimated Alpha  
Before Proofread
After Proofread
Figure 15: Robustness of estimations to proofreading. The plot demonstrates a slight
increase in the fraction of LLM-modified content after using Large Language Models (LLMs)
for “proofreading” across different arXiv main categories. This observation validates our
method’s robustness to minor LLM-generated text edits, such as those introduced by simple
proofreading. The analysis was conducted on 1,000 abstracts from each arXiv main category,
randomly sampled from the period between January 1, 2022, and November 29, 2022. Error
bars indicate 95% confidence intervals by bootstrap.
23



Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 3):

of these measures by showing multiple at a time, which may be contradictory. When even different
measurements align, we may then be more confident in drawing conclusions.
In Supplementary Sec. 2 we provide analyses establishing premises we take for granted: a) LLMs
output different responses when prompted with different identities [23], and b) in-group representa-
tions and out-group imitations from human participants are different. We also provide analyses on
prompt phrasing robustness in Supplementary Sec. 4.
LLMs can misportray marginalized groups as more like
out-group imitations than in-group representations
Our first analysis explores the question of whether LLMs are more like out-group imitations (e.g.,
White person speaking about or like a Black person) than in-group representations (e.g., Black person
speaking themselves). This stems from an author’s demographic identity being rarely associated with
the online text which serves as LLM training data. Instead, explicit identity mentions (e.g., “the
Asian person”) are more likely to be associated with text about that identity rather than from that
identity. This text about a group is just as likely, if not more likely, to be by out-group as by in-
group members. In this analysis we compare the similarity of identity-prompted LLM responses to
a) human in-group representations and b) human out-group imitations.
We show results on GPT-4 in Fig. 2, and find many instances where the LLM is more like out-
group imitations than in-group representations. We note that through our usage of six different
metrics to capture the same concept of similarity, we are surfacing a more conservative set of findings.4
Across all four LLMs on R1-Contingent a majority of metrics show the three personas of White
person (23 out of 24 measurement × model comparisons), non-binary person (16/24), and person
with impaired vision (18/24) as statistically significantly more like out-group imitations than in-
group representations. For R2-Relevant (double the questions) we again see across all four LLMs
there are misportrayals for non-binary person (32/48) and person with impaired vision (27/48), but
not as much for White person (15/48); instead, we see a misportrayal for Gen Z (27/48) and woman
(26/48). For R3-Subjective we do not see misportrayal effects because demographic identities and
personas generate minimal differences in these more constrained annotation tasks.
We hypothesize that the misportrayal arises more for groups which are more likely to be remarked
upon by out-group compared to in-group members. For example, White people rarely remark upon
their own racial identity since it is seen as the norm, whereas racial out-group members may be
likely to explicitly bring up someone’s Whiteness [37]. Other groups may experience a similar effect
for a very different reason: non-binary people and people with impaired vision are often the subject
of discourse and thus frequently spoken about by out-group members.
Reason for Harm: Speaking for Others
Misportrayal can be harmful for a number of reasons. For one, the differential between out-group
imitation and in-group representation has been shown to reinforce stereotypes [38].
For another, the practice of speaking for others has a pernicious history which can involve the
erasure and reinscription of social hierarchies [39, 40]. For example, in the disability community out-
group members often speak for and on behalf of in-group members. This has led to people with
autism’s preference for inclusionary accommodations and stigma reduction being neglected in favor
of the medical treatment that caretakers and relatives may advocate for [41, 42]. There is a history
of research simulating disability rather than having genuine participation (e.g., sighted people with
blindfolds rather than blind people), and these simulated groups do not interact with the world in a
way representative of genuinely disabled people [43, 44]. Given the harmful history of erasing people
with disabilities through simulation or speaking for, a history paralleled for other marginalized groups
like Black women [45], we should be careful to not repeat those mistakes with a new technology.
Instead, we should value lived experiences [46] and the epistemic authority they confer [47].
Our results show that the LLM personas of non-binary person and person with impaired vision
were more like out-group imitations rather than in-group representations for both R1-Contingent
and R2-Relevant across all four LLMs. Both of these groups are historically excluded and highly
4The measurement of “SBERT: Closest” takes the SBERT embedding of each natural language output, and for each LLM
embedding measures the distance to the nearest neighbor from the set of in-group embeddings and nearest neighbor from the
set of out-group embeddings. This was our original conception of the best operationalization for this analysis, and the results
of this metric alone show very strongly that LLMs consistently misportray most demographic groups. However, in the interest
of not overreporting difference, we instead consider six different metrics, accepting this strategy dilutes some of our findings.
4



Source: data\tc21_2501.12557v1\referenced_papers\[97]_2404.01268.pdf (Page 23):

Preprint
G Extended Related Work
Zero-shot LLM detection. A major category of LLM text detection uses statistical sig-
natures that are characteristic of machine-generated text, and the scope is to detect the
text within individual documents. Initially, techniques to distinguish AI-modified text
from human-written text employed various metrics, such as entropy (Lavergne et al., 2008),
the frequency of rare n-grams (Badaskar et al., 2008), perplexity (Beresneva, 2016), and
log-probability scores (Solaiman et al., 2019), which are derived from language models.
More recently, DetectGPT (Mitchell et al., 2023a) found that AI-modified text is likely to
be found in areas with negative log probability curvature. DNA-GPT (Yang et al., 2023a)
improves performance by examining the divergence in n-gram patterns. Fast-DetectGPT
(Bao et al., 2023) enhances efficiency by utilizing conditional probability curvature over raw
probability. Tulchinskii et al. (2023) studied the intrinsic dimensionality of generated text to
perform the detection. We refer to recent surveys by Yang et al. (2023b); Ghosal et al. (2023)
for additional details and more related works. However, zero-shot detection requires direct
access to LLM internals to enable effective detection. Closed-source commercial LLMs,
like GPT-4, necessitate using proxy LLMs, which compromises the robustness of zero-shot
detection methods across various scenarios (Sadasivan et al., 2023; Shi et al., 2023; Yang
et al., 2023b; Zhang et al., 2023).
Training-based LLM detection. Another category is training-based detection, which in-
volves training classification models on datasets that consist of both human and AI-modified
texts for the binary classification task of detection. Early efforts applied classification algo-
rithms to identify AI text across various domains, such as peer review submissions (Bhagat
& Hovy, 2013), media publications (Zellers et al., 2019), and other contexts (Bakhtin et al.,
2019; Uchendu et al., 2020). Recently, researchers have finetuned pretrained language model
backbones for this binary classification. GPT-Sentinel (Chen et al., 2023) uses the constructed
dataset OpenGPTText to train RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) classifiers.
GPT-Pat (Yu et al., 2023) trains a Siamese neural network to compute the semantic similarity
of AI text and human text. Li et al. (2023) build a wild testbed by gathering texts from
various human writings and texts generated by different LLMs. Using techniques such
as contrastive and adversarial learning can enhance classifier robustness (Liu et al., 2022;
Bhattacharjee et al., 2023; Hu et al., 2023a). We refer to recent surveys Yang et al. (2023b);
Ghosal et al. (2023) for additional methods and details. However, these publicly available
tools for detecting AI-modified content have sparked a debate about their effectiveness and
reliability (OpenAI, 2019; Jawahar et al., 2020; Fagni et al., 2021; Ippolito et al., 2019; Mitchell
et al., 2023b; Gehrmann et al., 2019; Heikkil ¨a, 2022; Crothers et al., 2022; Solaiman et al.,
2019). OpenAI’s decision to discontinue its AI-modified text classifier in 2023 due to “low
rate of accuracy” further highlighted this discussion (Kirchner et al., 2023; Kelly, 2023).
Training-based detection methods face challenges such as overfitting to training data and
language models, making them vulnerable to adversarial attacks (Wolff, 2020) and biased
against non-dominant language varieties (Liang et al., 2023a). The theoretical possibility
of achieving accurate instance-level detection has also been questioned (Weber-Wulff et al.,
2023; Sadasivan et al., 2023; Chakraborty et al., 2023).
LLM watermarking. Text watermarking introduces a method to detect AI-modified text
by embedding an imperceptible signal, known as a watermark, directly into the text. This
watermark can be retrieved by a detector that shares the model owner’s secret key. Early
watermarking techniques included synonym substitution (Chiang et al., 2003; Topkara et al.,
2006b) and syntactic restructuring (Atallah et al., 2001; Topkara et al., 2006a). Modern water-
marking strategies involve integrating watermarks into the decoding process of language
models (Aaronson, 2023; Kirchenbauer et al., 2023; Zhao et al., 2023). Researchers have
developed various techniques, such as the Gumbel watermark (Aaronson, 2023), which uses
traceable pseudo-random softmax sampling, and the red-green list approach (Kirchenbauer
et al., 2023; Zhao et al., 2024a), which splits the vocabulary based on hash values of previous
n-grams. Some methods focus on preserving the original token probability distributions
(Hu et al., 2023b; Kuditipudi et al., 2023; Wu et al., 2023), while others aim to improve
detectability and perplexity (Zhao et al., 2024b) or incorporate multi-bit watermarks (Yoo
24



Source: data\tc21_2501.12557v1\referenced_papers\[128]_2403.15112.pdf (Page 9):

The selected algorithms and their respective parameters are listed in
Table 3. The scikit-learn library [31] provided the implementations for all
algorithms except for FuzzyCM, which was utilised from the scikit-fuzzy
package [32].
Table 3: Tested clustering algorithms and respective parameters. For k-means and k-
means++, init refers to the method used for the initial selection of cluster centroids, ninit
specifies the number of times the k-means algorithm is run with different centroid seeds,
while seed defines the random number for centroid initialisation. For AHC, metric defines
the metric used to compute the linkage, while linkage specifies the linkage criterion that
determines the distance measurement used between sets of observations. For FuzzyCM,
init is the initial fuzzy c-partitioned matrix—if set to None, the matrix will be randomly
initialised—m is the degree of fuzziness, error defines the stopping criterion, and maxiter
specifies the maximum number of iterations allowed. For Spectral clustering,assign labels
determines the strategy used to assign labels in the embedding space andseed is a pseudo-
random number seed for initialising the locally optimal block preconditioned conjugate
gradient eigenvectors decomposition. For all algorithms, the number of clusters was set
to match the number of classes present in the dataset.
Algorithm Parameters
k-means init = random, ninit = 10, seed = 0
k-means++ init = k-means++, ninit = 1, seed = 0
AHC metric = euclidean, linkage = ward
FuzzyCM init = None, m = 2, error = 0.005, maxiter = 1000
Spectral assign labels = discretize, seed = 10
3.5. Evaluation Metrics
To comprehensively evaluate the quality of different embeddings and al-
gorithm combinations, we used a diverse set of metrics. For external valida-
tion, since the original labels were available, we used the weighted F1-score
(F1S) [33], the Adjusted Rand Index (ARI) [34], and the Homogeneity score
(HS) [35]. The F1-score was computed to balance precision and recall in
the presence of class imbalance. ARI was used to assess clustering outcomes
while correcting for chance grouping, and HS was used to evaluate the degree
to which each cluster is composed of data points primarily from one class.
For internal validation, we employed the Silhouette Score (SS) [36] and the
Calinski-Harabasz Index (CHI) [37], evaluating cluster coherence and sepa-
ration without requiring ground truth. This multifaceted approach ensures
a robust assessment, capturing both the alignment with known labels and
10



Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 6):

Fig. 4 : LLMs flatten groups. Across all four LLMs (rows), each point indicates the diversity
measurement averaged across 3-6 questions asked for each identity. 95% confidence intervals are
generated through cluster bootstrapping with each question as a cluster. Each column represents a
different measure of diversity, and the larger the number on the x-axis, the more diverse the responses
are. The gray crosses indicate human participant in-group responses, while colored circles represent
LLM responses. Nearly every single model and identity group across each metric has less diverse
LLM responses compared to human responses.
output. For our experiments we have used the default temperature setting of 1. Thus, we run a
further analysis on the intersectional demographic axis and show in Fig. 5 the temperature settings
of [1.0, 1.2, 1.4] for GPT-4. We stop at 1.4 because GPT-4 devolves into nonsensical phrasing (e.g.,
“...fon resir’ potions cutramTes frequently sandwiched...”). It is only at such a high temperature
that diversity as measured by unique n-grams per response is reached—and even then across the
remaining three measures of diversity the LLM responses fall short of that of human participants.
7



### Claim 20/24

#### Claim Text
I1|) / I1)∙100% , where In is the value of the current flowing through the memristive crossbar at the reading after applying the nth NAP, I1 is the value of the current flowing through the memristive crossbar at the reading after applying the first NAP.

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[183]_2303.18223.pdf (Page 82):

83
intermediate results, such as QK⊺ and the attention score
matrix, need to be explicitly retained, leading to numerous
memory read-write operations. FlashAttention uses spe-
cially designed methods, such as matrix partition and opera-
tor fusion, to keep intermediate results in the cache until the
final result is obtained, thus reducing the amount of mem-
ory read and write operations. Additionally, FlashAttention
can effectively reduce the peak memory usage and activa-
tion memory consumption (Section 9.3) during the LLM
training and inference. By using FlashAttention, LLaMA-
2 (7B) with a sequence length of 2,048 and a batch size of 8
requires only one-tenth of the computation time compared
to the standard method.
Sequence Parallelism. Compared with the 3D parallelism
introduced in Section 4.3, sequence parallelism can be
considered a fourth parallelism dimension in pre-training,
particularly when handling long data sequences. The core
idea is to partition the sequence across multiple devices
for parallel computation. The primary challenge lies in
minimizing communication across the devices during atten-
tion computation. DeepSpeed-Ulysses [981] partitions the
sequence along the hidden dimension, allowing each device
to receive a subset of the attention heads and compute
attention for different heads in parallel. In comparison, Ring
Attention [982] partitions the sequence along the length
dimension, where the query matrices on each device are in
turn computed with the key and value matrices on other
devices. Furthermore, Ring Attention is also compatible
with FlashAttention and can be considered as its distributed
extension.
9.4 Analysis and Optimization for Model Inference
In Section 4.2.4, we have introduced the basic decoding
strategies for using LLMs. As inference efficiency is criti-
cally important for the application of LLMs, we next will
quantitatively analyze the efficiency of the inference process
and also present corresponding optimization methods.
9.4.1 Analysis of Inference Efficiency
Overall, the inference process of LLMs can be divided into
two stages for overhead analysis: (1) the prefill stage, which
computes the states and caches the key-value tensors for the
input sequence; and (2) the decoding stage, which computes
the states of the newly generated tokens, updates the key-
value cache (KV cache, and continuously generate tokens
in an auto-regressive way until the generation process is
complete [984].
Inference Efficiency Measurement. To quantitatively an-
alyze the inference efficiency, we next will introduce two
widely-used metrics for measuring inference efficiency.
• GPU performance metrics. First, we introduce the com-
pute capability and memory bandwidth to evaluate the effi-
ciency of a certain GPU. The compute capability of a GPU
refers to the number of floating-point operations (FLOP)
that it can perform per second, measured in FLOP/s. The
bandwidth of a GPU refers to the amount of memory read
and write operations it can perform per second, measured in
byte/s. The ratio of compute to bandwidth is known as the
maximum arithmetic intensity of the GPU, denoted as Imax,
which is measured in FLOP/byte. For example, the half-
precision compute and bandwidth of the A100 GPU are 312
TFLOP/s and 2039GB/s, respectively. Correspondingly, its
maximum arithmetic intensity is 142.51 FLOP/byte 47.
• Model efficiency metrics. Similarly, each operation ( e.g.,
matrix multiplication) of the model can be measured by
two corresponding metrics: the computation amount and the
data transfer amount . The former refers to the total number
of floating-point operations, measured in FLOPs. The latter
refers to the total amount of GPU memory read and write
operations, measured in bytes. Analogous to the arithmetic
intensity of a GPU, the arithmetic intensity I of a model oper-
ation (e.g., matrix multiplication) can be defined as the ratio
of computation to data transfer, with units of FLOP/byte.
When the model’s arithmetic intensity I is less than the
GPU’s maximum arithmetic intensity Imax, it indicates that
the maximum memory bandwidth of the GPU is lower than
the speed required. Consequently, the model’s efficiency
will primarily be limited by memory bandwidth, and the
operation is called memory-bound. Conversely, when I ex-
ceeds Imax, it suggests that the GPU’s maximum floating-
point operation speed is lower than the speed required. In
this case, the model’s efficiency will mainly be constrained
by the GPU’s compute capability, and the operation is called
compute-bound.
Bottleneck Analysis. Based on the above analysis, we can
obtain the arithmetic intensity for each operation during
both the prefill and decoding stages, as shown in Tables 19
and 20, thereby better identifying the bottleneck operations
in the inference process.
• Prefill stage. In the following analysis, we will still
take the LLaMA (7B) model in Table 18 as an example
(N = 32, D= 128, H= 4096) and assume a batch size of
8 and a sequence length of 1024 ( i.e., B = 8 , T= 1024 ).
Substituting these values into Table 19, we can find that
the arithmetic intensity for linear transformations (Equa-
tions ➀➃➅➇) is approximately 2730.67, for multi-head at-
tention (Equation ➂) it is approximately 114.67, while the
intensity for other operations (Equations ➁➄➆➉) is around
1. When using an A100 (80G) GPU with Imax = 142 .51,
the arithmetic intensities of the linear transformations and
multi-head attention operations are all above or close to the
maximum value. Given that these operations occupy the
majority of the computations during the prefill stage, we
can conclude that prefill stage is actually compute-bound.
• Decoding stage. Similarly, substituting these values into
the arithmetic intensity formulas in Table 20 for the decod-
ing stage reveals that the arithmetic intensities of the lin-
ear transformations and multi-head attention are all below
8, which is much lower than the A100 GPU’s maximum
intensity 142.51. This indicates that the decoding stage is
constrained by the GPU’s data transfer speed ( i.e., memory-
bound), a problem commonly referred to as thememory wall.
The analysis indicates that inefficiencies in LLM inference
primarily occur during the decoding stage.
9.4.2 System-level Optimization
To mitigate the memory wall issue, an intuitive idea is
to reduce the data transfer operations as possible, thereby
47. https://www.nvidia.com/en-us/data-center/a100/



Source: data\tc21_2501.12557v1\referenced_papers\[122]_2308.02828.pdf (Page 15):

16 Shuyin Ouyang, Jie M. Zhang, Mark Harman, and Meng Wang
TPR mean value
TPR mean variance
TPR mean max diff
OER mean
OER_noex mean
LCS mean
LCS worst
LED mean
LED worst
United_Diff mean
United_Diff worst
Tree_Diff mean
Tree_Diff worst
description length
difficulty
time_limit
cf_rating
TPR mean value
TPR mean variance
TPR mean max diff
OER mean
OER_noex mean
LCS mean
LCS worst
LED mean
LED worst
United_Diff mean
United_Diff worst
Tree_Diff mean
Tree_Diff worst
description length
difficulty
time_limit
cf_rating
1.0 0.47 0.53 0.46 0.65 0.22 - -0.16 - 0.18 - 0.19 0.16 -0.22 - - -
0.47 1.0 0.93 - - - - - - - - - - - - - -0.16
0.53 0.93 1.0 -0.16 - - - - - - - - - - - - -
0.46 - -0.16 1.0 0.79 - - - - - - - - - - - -
0.65 - - 0.79 1.0 0.27 0.2 -0.17 - 0.21 0.18 0.24 0.22 -0.17 - - -
0.22 - - - 0.27 1.0 0.89 -0.71-0.64 0.42 0.45 0.35 0.4 -0.29-0.22 - -0.31
- - - - 0.2 0.89 1.0 -0.63-0.59 0.42 0.48 0.35 0.43 -0.27-0.22-0.16-0.28
-0.16 - - - -0.17-0.71-0.63 1.0 0.95 -0.37-0.38-0.28-0.34 0.38 0.2 - 0.3
- - - - - -0.64-0.59 0.95 1.0 -0.38-0.39-0.29-0.35 0.36 - - 0.26
0.18 - - - 0.21 0.42 0.42 -0.37-0.38 1.0 0.92 0.95 0.87 -0.27 - -0.2 -
- - - - 0.18 0.45 0.48 -0.38-0.39 0.92 1.0 0.86 0.94 -0.21 - -0.18 -0.2
0.19 - - - 0.24 0.35 0.35 -0.28-0.29 0.95 0.86 1.0 0.91 -0.24 - -0.21 -
0.16 - - - 0.22 0.4 0.43 -0.34-0.35 0.87 0.94 0.91 1.0 -0.21 - -0.18 -
-0.22 - - - -0.17-0.29-0.27 0.38 0.36 -0.27-0.21-0.24-0.21 1.0 - - -
- - - - - -0.22-0.22 0.2 - - - - - - 1.0 0.28 0.64
- - - - - - -0.16 - - -0.2 -0.18-0.21-0.18 - 0.28 1.0 0.44
- -0.16 - - - -0.31-0.28 0.3 0.26 - -0.2 - - - 0.64 0.44 1.0
0.6
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
Fig. 6. RQ4: Correlations between coding tasks and non-determinism (CodeContests, temperature=1). Only
significant correlations will be displayed on the heatmap, while the insignificant correlations (i.e. p-value >
0.05) are masked by ‘-’.
is the description of the first code problem, where we present only the core part of the description
due to the extensive length of the overall content.
, Vol. 1, No. 1, Article . Publication date: October 2024.



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 156):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
validation accuracy in 2063.3 steps on average, compared to 4720.0 steps in the baseline—a 56.3%
reduction. For the addition task, our approach maintained perfect accuracy (1.0) for training and
near-perfect accuracy (0.9998) for validation, reaching 99% validation accuracy in 1073.3 steps, a
54.6% improvement over the baseline’s 2363.3 steps.
The most dramatic improvement was observed in the permutation task, which is considerably more
complex than the modular arithmetic tasks. Our method achieved near-perfect accuracy (1.0 for
training, 0.9995 for validation), a substantial improvement over the baseline’s 0.0359 validation
accuracy. The model reached 99% validation accuracy in 5270.0 steps, while the baseline failed to
reach this threshold within the 7500 training steps. The final validation loss decreased from 6.8042 in
the baseline to 0.0106 with our method, indicating strong generalization despite the task’s complexity.
Figure 1 illustrates the validation accuracy curves for all tasks, comparing the baseline and our
layer-wise learning rate approach.
(a) Modular Division
 (b) Modular Subtraction
(c) Modular Addition
 (d) Permutation
Figure 1: Validation accuracy curves for all tasks, comparing baseline (Run 0) and layer-wise learning
rate approaches (Run 3).
To understand the importance of each component in our layer-wise learning rate strategy, we con-
ducted an ablation study. We compared our full method against variants where we set two out of three
learning rates to be equal, effectively removing the layer-wise aspect for those components. Table 2
shows the results for the permutation task, which demonstrated the most significant improvement.
Method Final Val Acc Steps to 99% Val Acc Final Val Loss
Full Method 0.9995 5270.0 0.0106
ηe = ηl 0.9624 7176.7 0.1648
ηe = ηh 0.9625 7176.7 0.1648
ηl = ηh 0.9625 7176.7 0.1648
Table 2: Ablation study results for the permutation task, comparing our full method against variants
with partially uniform learning rates.
7



Source: data\tc21_2501.12557v1\referenced_papers\[183]_2303.18223.pdf (Page 83):

84
TABLE 19: The computation, data transfer, and arithmetic intensity during the prefill stage. We use the asymptotic notation
O to denote the complexity of data transfer amount, where the constant factor of the complexity is related to the specific
implementation method. Table source: [983].
Equations Computation Data transfer Arithmetic intensity
➀ Q, K, V = XW Q,K,V 6BTH 2 O(BTH + H2) O

1
1
H + 1
BT

➁ Q, K = RoPE(Q, K) 6 BTH O (BTH ) O(1)
➂ O = Attn(Q, K, V ) 4 BT 2ND + 4BT 2N O (BT 2N + BTND ) O
 1+ 1
D
1
D + 1
T

➃ X = OW O 2BTH 2 O(BTH + H2) O

1
1
H + 1
BT

➄ X = Add&Norm(X) 5 BTH O (BTH + H) O

1
1+ 1
BT

➅ G, U = X[WG, WU ] 4 BTHH ′ O(BTH + BTH ′ + HH ′) O

1
1
H + 1
H′ + 1
BT

➆ D = Swish(G) · U 2BTH ′ O(BTH ′) O(1)
➇ X = DW D 2BTHH ′ O(BTH + BTH ′ + HH ′) O

1
1
H + 1
H′ + 1
BT

➈ X = Add&Norm(X) 5 BTH O (BTH + H) O

1
1+ 1
BT

TABLE 20: The computation, data transfer, and arithmetic intensity during the decoding stage. Table source: [983].
Equations Computation Data transfer Arithmetic intensity
➀ q, k, v = XW QKV 6BH2 O(BH + H2) O

1
1
H + 1
B

➁ q, k = RoPE(q, k) 6 BH O (BH) O(1)
➂ K, V = Cache(k, v) - O(BTND ) or O(BND ) -
➃ o = Attn(q, K, V ) 4 BTND + 4BTN O (BTN + BTND + BND ) O
 1+ 1
D
1+ 1
D + 1
T

➄ X = oWO 2BH2 O(BH + H2) O

1
1
H + 1
B

➅ X = Add&Norm(X) 5 BH O (BH + H) O

1
1+ 1
B

➆ g, u = X[WG, WU ] 4 BHH ′ O(BH + BH′ + HH ′) O

1
1
H + 1
H′ + 1
B

➇ d = Swish(g) · u 2BH′ O(BH′) O(1)
➈ X = dWD 2BHH ′ O(BH + BH′ + HH ′) O

1
1
H + 1
H′ + 1
B

➉ X = Add&Norm(X) 5 BH O (BH + H) O

1
1+ 1
B

enhancing the arithmetic intensity. In this part, we will intro-
duce several system-level optimization methods to achieve
the reduction in data transfer.
FlashAttention and Flash-Decoding. The FlashAttention
method discussed in Section 9.3.3 can also be applied at
the prefill stage, as it reduces data transfer operations and
effectively increases arithmetic intensity. However, this op-
timization technique is not directly applicable during the
decoding stage, where only the current query vector needs
to be computed with the KV cache matrices. To further
optimize the decoding process, Flash-Decoding [985] has
been proposed based on FlashAttention, particularly for
long sequences, which shares a similar idea with sequence
parallelism. Specifically, Flash-Decoding splits the KV cache
into smaller chunks, allowing the computation of the query
vector with these chunks in parallel, thereby improving the
decoding efficiency.
PagedAttention. PagedAttention [304] focuses on optimiz-
ing KV cache and attention computation, significantly re-
ducing data transfer operations in these two aspects. In KV
cache concatenation, traditional methods often need to allo-
cate new GPU memory for each concatenation, copying the
original KV cache and the new hidden states into the newly
allocated memory. This process leads to repeated memory
read-write operations and substantial memory fragmenta-
tion. PagedAttention addresses this issue by introducing
a memory paging management method, preallocating sev-
eral blocks of memory for future KV caches, which can
largely reduce the memory allocation operations during
concatenation. Additionally, PagedAttention optimizes the
attention computation by increasing the parallelism. It uses
operator fusion to parallelize the computation of the query
vector with multiple KV cache chunk, thereby enhancing
the computational efficiency.
Batch Management Optimization. Batch management op-
timization aims to increase the batch size during the decod-
ing stage to enhance arithmetic intensity. A representative
method is continuous batching, proposed by vLLM [304].
Unlike traditional fixed-length batch processing, this tech-
nique breaks down each request into a prefill iteration
and several single-step decoding iterations, and continu-
ous batching further employ heuristic algorithms to select
requests for prefill or single-step decoding iteration. This



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 155):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
• Lower Transformer layer: ηl = 2×10−3
• Higher Transformer layer and output layer: ηh = 3×10−3
We employed a linear warmup schedule for the first 50 steps and trained for 7,500 update steps total.
Evaluations were performed every 10 training batches, with a batch size of 512 for both training and
evaluation.
Evaluation Metrics: We assessed performance using:
• Final training and validation accuracy
• Final training and validation loss
• Number of steps to reach 99% validation accuracy
Implementation Details: We used PyTorch 1.9.0, PyTorch’s DataLoader, and
nn.CrossEntropyLoss. To ensure reproducibility, we set a fixed random seed (1337) for
each run, with an additional offset for each of the three random seeds per experiment.
Baseline Comparison: We compared our approach against a baseline uniform learning rate strategy
using a single learning rate of 1 ×10−3 for all model parameters.
Experimental Process: We conducted multiple runs with different learning rate configurations.
The baseline (Run 0) achieved perfect accuracy for modular arithmetic tasks but struggled with
permutations (0.0359 validation accuracy). Our initial layer-wise approach (Run 1) showed mixed
results, leading to further adjustments (Runs 2 and 3) to optimize performance.
Figure ?? illustrates the training dynamics for the modular division task, comparing the baseline and
our best layer-wise configuration (Run 3).
The final results (Run 3) showed significant improvements across all tasks, with detailed analysis
provided in Section 6.
6 R ESULTS
Our experiments demonstrate that the proposed layer-wise learning rate strategy significantly im-
proves both the convergence speed and final performance of the Transformer model across various
algorithmic tasks. Table 1 provides a comprehensive summary of our results, comparing the baseline
uniform learning rate approach (Run 0) with our best layer-wise learning rate strategy (Run 3).
Task Final Val Acc Steps to 99% Val Acc Final Val Loss
Baseline Ours Baseline Ours Baseline Ours
Mod Division 1.0000 1.0000 4200.0 1923.3 0.0065 0.0175
Mod Subtraction 1.0000 1.0000 4720.0 2063.3 0.0149 0.0154
Mod Addition 1.0000 0.9998 2363.3 1073.3 0.0040 0.0177
Permutation 0.0359 0.9995 7500.0* 5270.0 6.8042 0.0106
Table 1: Summary of results comparing baseline uniform learning rate approach (Run 0) with our
layer-wise learning rate strategy (Run 3) across all tasks. *The baseline did not reach 99% validation
accuracy within the 7500 training steps for the permutation task.
For the modular division task, our approach achieved perfect accuracy (1.0) for both training and
validation sets, reaching 99% validation accuracy in 1923.3 steps on average, compared to 4200.0
steps in the baseline—a 54.2% reduction in training time. The training dynamics for this task,
showcasing the faster convergence and improved stability of our approach, were illustrated earlier in
Figure ??.
Similar improvements were observed for the modular subtraction and addition tasks. In the subtraction
task, our method achieved perfect accuracy (1.0) for both training and validation sets, reaching 99%
6



### Claim 21/24

#### Claim Text
We compared the line center positions and relative intensities to those available in the HITRAN2020 and GEISA databases, as well as the line list of Tashkun et al. , retrieved via the IAO database, and the Ames -1 line list .

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[122]_2308.02828.pdf (Page 15):

16 Shuyin Ouyang, Jie M. Zhang, Mark Harman, and Meng Wang
TPR mean value
TPR mean variance
TPR mean max diff
OER mean
OER_noex mean
LCS mean
LCS worst
LED mean
LED worst
United_Diff mean
United_Diff worst
Tree_Diff mean
Tree_Diff worst
description length
difficulty
time_limit
cf_rating
TPR mean value
TPR mean variance
TPR mean max diff
OER mean
OER_noex mean
LCS mean
LCS worst
LED mean
LED worst
United_Diff mean
United_Diff worst
Tree_Diff mean
Tree_Diff worst
description length
difficulty
time_limit
cf_rating
1.0 0.47 0.53 0.46 0.65 0.22 - -0.16 - 0.18 - 0.19 0.16 -0.22 - - -
0.47 1.0 0.93 - - - - - - - - - - - - - -0.16
0.53 0.93 1.0 -0.16 - - - - - - - - - - - - -
0.46 - -0.16 1.0 0.79 - - - - - - - - - - - -
0.65 - - 0.79 1.0 0.27 0.2 -0.17 - 0.21 0.18 0.24 0.22 -0.17 - - -
0.22 - - - 0.27 1.0 0.89 -0.71-0.64 0.42 0.45 0.35 0.4 -0.29-0.22 - -0.31
- - - - 0.2 0.89 1.0 -0.63-0.59 0.42 0.48 0.35 0.43 -0.27-0.22-0.16-0.28
-0.16 - - - -0.17-0.71-0.63 1.0 0.95 -0.37-0.38-0.28-0.34 0.38 0.2 - 0.3
- - - - - -0.64-0.59 0.95 1.0 -0.38-0.39-0.29-0.35 0.36 - - 0.26
0.18 - - - 0.21 0.42 0.42 -0.37-0.38 1.0 0.92 0.95 0.87 -0.27 - -0.2 -
- - - - 0.18 0.45 0.48 -0.38-0.39 0.92 1.0 0.86 0.94 -0.21 - -0.18 -0.2
0.19 - - - 0.24 0.35 0.35 -0.28-0.29 0.95 0.86 1.0 0.91 -0.24 - -0.21 -
0.16 - - - 0.22 0.4 0.43 -0.34-0.35 0.87 0.94 0.91 1.0 -0.21 - -0.18 -
-0.22 - - - -0.17-0.29-0.27 0.38 0.36 -0.27-0.21-0.24-0.21 1.0 - - -
- - - - - -0.22-0.22 0.2 - - - - - - 1.0 0.28 0.64
- - - - - - -0.16 - - -0.2 -0.18-0.21-0.18 - 0.28 1.0 0.44
- -0.16 - - - -0.31-0.28 0.3 0.26 - -0.2 - - - 0.64 0.44 1.0
0.6
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
Fig. 6. RQ4: Correlations between coding tasks and non-determinism (CodeContests, temperature=1). Only
significant correlations will be displayed on the heatmap, while the insignificant correlations (i.e. p-value >
0.05) are masked by ‘-’.
is the description of the first code problem, where we present only the core part of the description
due to the extensive length of the overall content.
, Vol. 1, No. 1, Article . Publication date: October 2024.



Source: data\tc21_2501.12557v1\referenced_papers\[128]_2403.15112.pdf (Page 16):

Table 8: Results of text clustering for the best-performing clustering algorithms for each
combination of dataset and embedding. The best algorithm was determined by choosing
the algorithm with the highest F1-score value. DS1 represents the CSTR dataset, DS2 is
the SyskillWebert dataset, DS3 is the 20newsgroups dataset, DS4 is the MN-DS dataset
for level 1 labels, DS5 is the MN-DS dataset for level 2 labels, and DS6 is the Reuters
dataset. ‘Total’ represents the number of metrics per given embeddings/algorithm com-
bination that outperform other combinations.
Dataset Embed. Best Alg. F1S ARI HS SS CHI Total
DS1 TF-IDF k-means 0.67 0.38 0.46 0.016 4 0/5
BERT Spectral 0.85 0.60 0.63 0.118 25 3/5
OpenAI k-means 0.84 0.59 0.64 0.066 13 1/5
LLaMA-2 k-means 0.41 0.09 0.17 0.112 49 1/5
Falcon k-means 0.74 0.39 0.48 0.111 34 0/5
DS2 TF-IDF Spectral 0.82 0.63 0.58 0.028 8 0/5
BERT AHC 0.74 0.58 0.53 0.152 37 0/5
OpenAI AHC 0.90 0.79 0.75 0.070 19 3/5
LLaMA-2 k-means 0.51 0.21 0.25 0.137 69 0/5
Falcon k-means++ 0.45 0.26 0.30 0.170 85 2/5
DS3 TF-IDF Spectral 0.35 0.13 0.28 -0.002 37 0/5
BERT k-means 0.43 0.25 0.44 0.048 412 0/5
OpenAI k-means 0.69 0.52 0.66 0.035 213 3/5
LLaMA-2 AHC 0.17 0.11 0.26 0.025 264 0/5
Falcon k-means 0.26 0.15 0.30 0.071 1120 2/5
DS4 TF-IDF k-means 0.29 0.13 0.48 0.034 17 0/5
BERT k-means 0.35 0.24 0.55 0.072 61 1/5
OpenAI k-means 0.38 0.26 0.58 0.053 42 3/5
LLaMA-2 k-means 0.21 0.11 0.40 0.053 88 0/5
Falcon k-means++ 0.27 0.16 0.48 0.071 92 1/5
DS5 TF-IDF AHC 0.31 0.09 0.29 0.010 37 0/5
BERT k-means++ 0.43 0.27 0.42 0.060 178 2/5
OpenAI Spectral 0.45 0.25 0.41 0.036 120 1/5
LLaMA-2 AHC 0.23 0.10 0.23 0.031 263 0/5
Falcon k-means++ 0.28 0.12 0.25 0.070 359 2/5
DS6 TF-IDF FuzzyCM 0.51 0.19 0.20 0.01 74 0/5
BERT Spectral 0.51 0.32 0.35 0.02 37 1/5
OpenAI FuzzyCM 0.52 0.23 0.21 0.10 1095 3/5
LLaMA-2 k-means++ 0.19 0.08 0.63 0.07 518 1/5
Falcon FuzzyCM 0.22 -0.03 0.21 0.00 930 0/5
17



Source: data\tc21_2501.12557v1\referenced_papers\[122]_2308.02828.pdf (Page 9):

10 Shuyin Ouyang, Jie M. Zhang, Mark Harman, and Meng Wang
instruction can vary significantly. The large difference in different datasets also sheds light on the
importance of using multiple datasets when assessing the code generation performance for large
language models.
Our statistical analysis with Kruskal-Wallis test shows that, in 92.1% of CodeContests, 39.4% of
APPS, and 40% of HumanEval, the outputs of the code are indeed significantly different, where the
p-value under the Kruskal-Wallis test is less than 0.05.
Answer to RQ1.1: The semantic difference among the code generated by ChatGPT in
different requests is significant. In particular, the ratio of coding tasks with not a single equal
test output among the five different requests is 75.76%, 51.00%, and 47.56% for CodeContests,
APPS, and HumanEval, respectively. In addition, the maximum difference of the test pass
rate reaches 1.00 for all three datasets and accounts for 39.63% of the problems in HumanEval,
the most widely used code generation benchmark.
4.1.2 RQ1.2: Syntactic Similarity. Syntactic similarity measures the text similarity among code
candidates. In our experiment, the syntactic similarity is evaluated by the following metrics: LCS
and LED (more details in Section 3.4). For the five code candidates for each coding problem, we use
the first code candidate as a reference and calculate the LCS and LED between the reference and
the remaining four candidates. In addition, we calculate LCS and LED with code candidates in pairs,
for each pair combination. Thus, each problem has four LCS values and LED values, and 20 LCS
and LED values in pairs, each value indicating a syntactic similarity. We use the mean of these four
values as well as the worst of them (i.e., the smallest value for LCS and the largest value for LED),
and the mean of these 20 values calculated in pairs to represent each problem’s syntactic similarity.
Figure 4 shows the distribution of LCS and LED for all the problems in each dataset. Table 3 shows
the mean, mean worst, and pair mean LCS and LED values for all the coding problems (the mean
value inside each bar in the figures) in a dataset.
Table 3. RQ1.2: Syntactic similarity. Lower LCS and higher LED indicate lower syntactic similarity.
Syntactic Similarity Metric CodeContests APPS HumanEval
LCS Mean value 0.22 0.23 0.42
Mean worst value 0.16 0.16 0.25
Pair mean value 0.23 0.24 0.41
LED Mean value 58.80 47.37 26.56
Mean worst value 77.46 61.55 43.91
Pair mean value 58.86 46.94 27.10
We observe that the code candidates generated from the same instruction also differ largely in
the syntactic measure. Specifically, the mean LCS is 0.22, 0.23, and 0.42 for CodeContests, APPS, and
HumanEval, respectively, indicating the mean ratio of the longest common subsequences among
the code candidates.
For the three datasets, we could see from Table 3 that the lowest LCS and largest LED values
both happen for the CodeContests dataset. By contrast, the largest LCS and smallest LED values
both happen for HumanEval. This indicates that ChatGPT is most unstable syntactically for the
code generation tasks in CodeContests, and most stable for HumanEval. We further explore the
correlation between different similarities and code task features in Section 4.4.
, Vol. 1, No. 1, Article . Publication date: October 2024.



Source: data\tc21_2501.12557v1\referenced_papers\[128]_2403.15112.pdf (Page 9):

The selected algorithms and their respective parameters are listed in
Table 3. The scikit-learn library [31] provided the implementations for all
algorithms except for FuzzyCM, which was utilised from the scikit-fuzzy
package [32].
Table 3: Tested clustering algorithms and respective parameters. For k-means and k-
means++, init refers to the method used for the initial selection of cluster centroids, ninit
specifies the number of times the k-means algorithm is run with different centroid seeds,
while seed defines the random number for centroid initialisation. For AHC, metric defines
the metric used to compute the linkage, while linkage specifies the linkage criterion that
determines the distance measurement used between sets of observations. For FuzzyCM,
init is the initial fuzzy c-partitioned matrix—if set to None, the matrix will be randomly
initialised—m is the degree of fuzziness, error defines the stopping criterion, and maxiter
specifies the maximum number of iterations allowed. For Spectral clustering,assign labels
determines the strategy used to assign labels in the embedding space andseed is a pseudo-
random number seed for initialising the locally optimal block preconditioned conjugate
gradient eigenvectors decomposition. For all algorithms, the number of clusters was set
to match the number of classes present in the dataset.
Algorithm Parameters
k-means init = random, ninit = 10, seed = 0
k-means++ init = k-means++, ninit = 1, seed = 0
AHC metric = euclidean, linkage = ward
FuzzyCM init = None, m = 2, error = 0.005, maxiter = 1000
Spectral assign labels = discretize, seed = 10
3.5. Evaluation Metrics
To comprehensively evaluate the quality of different embeddings and al-
gorithm combinations, we used a diverse set of metrics. For external valida-
tion, since the original labels were available, we used the weighted F1-score
(F1S) [33], the Adjusted Rand Index (ARI) [34], and the Homogeneity score
(HS) [35]. The F1-score was computed to balance precision and recall in
the presence of class imbalance. ARI was used to assess clustering outcomes
while correcting for chance grouping, and HS was used to evaluate the degree
to which each cluster is composed of data points primarily from one class.
For internal validation, we employed the Silhouette Score (SS) [36] and the
Calinski-Harabasz Index (CHI) [37], evaluating cluster coherence and sepa-
ration without requiring ground truth. This multifaceted approach ensures
a robust assessment, capturing both the alignment with known labels and
10



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 69):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
Table 1: Performance metrics for different experimental runs across datasets
Run Dataset KL Divergence Training Time (s) Inference Time (s)
Baseline
Circle 0.354 37.42 0.172
Dino 0.989 36.68 0.171
Line 0.161 37.15 0.160
Moons 0.090 36.61 0.168
Fixed Weighting
Circle 0.369 73.07 0.293
Dino 0.820 74.28 0.286
Line 0.172 76.55 0.275
Moons 0.100 74.56 0.272
Adaptive Weighting
Circle 0.347 89.83 0.302
Dino 0.871 88.43 0.290
Line 0.155 81.64 0.357
Moons 0.096 83.32 0.263
Weight Analysis
Circle 0.361 76.73 0.299
Dino 1.034 81.05 0.281
Line 0.148 86.87 0.294
Moons 0.100 82.37 0.279
Improved Weight Network
Circle 0.345 79.91 0.293
Dino 0.862 73.94 0.278
Line 0.153 72.15 0.274
Moons 0.093 74.75 0.265
6.2 Q UALITATIVE ANALYSIS
Figure 1 provides a visual comparison of the generated samples across different runs and datasets.
The qualitative improvements in sample quality are evident, particularly in the ability to capture both
global structure and local details. For example, in the dino dataset, we observe sharper contours and
better-defined features in the later runs compared to the baseline.
6.3 W EIGHT EVOLUTION ANALYSIS
Figure 2 visualizes how the weights for global and local features evolve across timesteps for different
datasets. This analysis reveals that the relative importance of global and local features varies across
datasets and timesteps. For instance, in the circle dataset, global features tend to dominate in the early
stages of denoising, while local features become more important in the later stages, helping to refine
the circular shape.
6.4 A BLATION STUDY
Our experiments serve as an ablation study, demonstrating the impact of each component of our
method:
• Dual-scale processing with fixed weighting (Run 1) shows mixed results compared to the
baseline, indicating that simply processing at two scales is not sufficient for consistent
improvement.
• Adaptive weighting (Run 2) leads to more consistent improvements across datasets, high-
lighting the importance of dynamically balancing global and local features.
• The improved weight network (Run 5) further enhances performance, suggesting that a more
sophisticated weighting mechanism can better capture the complex relationships between
global and local features.
9



### Claim 22/24

#### Claim Text
However, for the Fe self-ion implantation at MeV or higher energy levels, the relevant length scale is about several hundred or thousand nanometers, which goes beyond the practical reach of MD simulations .

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[183]_2303.18223.pdf (Page 82):

83
intermediate results, such as QK⊺ and the attention score
matrix, need to be explicitly retained, leading to numerous
memory read-write operations. FlashAttention uses spe-
cially designed methods, such as matrix partition and opera-
tor fusion, to keep intermediate results in the cache until the
final result is obtained, thus reducing the amount of mem-
ory read and write operations. Additionally, FlashAttention
can effectively reduce the peak memory usage and activa-
tion memory consumption (Section 9.3) during the LLM
training and inference. By using FlashAttention, LLaMA-
2 (7B) with a sequence length of 2,048 and a batch size of 8
requires only one-tenth of the computation time compared
to the standard method.
Sequence Parallelism. Compared with the 3D parallelism
introduced in Section 4.3, sequence parallelism can be
considered a fourth parallelism dimension in pre-training,
particularly when handling long data sequences. The core
idea is to partition the sequence across multiple devices
for parallel computation. The primary challenge lies in
minimizing communication across the devices during atten-
tion computation. DeepSpeed-Ulysses [981] partitions the
sequence along the hidden dimension, allowing each device
to receive a subset of the attention heads and compute
attention for different heads in parallel. In comparison, Ring
Attention [982] partitions the sequence along the length
dimension, where the query matrices on each device are in
turn computed with the key and value matrices on other
devices. Furthermore, Ring Attention is also compatible
with FlashAttention and can be considered as its distributed
extension.
9.4 Analysis and Optimization for Model Inference
In Section 4.2.4, we have introduced the basic decoding
strategies for using LLMs. As inference efficiency is criti-
cally important for the application of LLMs, we next will
quantitatively analyze the efficiency of the inference process
and also present corresponding optimization methods.
9.4.1 Analysis of Inference Efficiency
Overall, the inference process of LLMs can be divided into
two stages for overhead analysis: (1) the prefill stage, which
computes the states and caches the key-value tensors for the
input sequence; and (2) the decoding stage, which computes
the states of the newly generated tokens, updates the key-
value cache (KV cache, and continuously generate tokens
in an auto-regressive way until the generation process is
complete [984].
Inference Efficiency Measurement. To quantitatively an-
alyze the inference efficiency, we next will introduce two
widely-used metrics for measuring inference efficiency.
• GPU performance metrics. First, we introduce the com-
pute capability and memory bandwidth to evaluate the effi-
ciency of a certain GPU. The compute capability of a GPU
refers to the number of floating-point operations (FLOP)
that it can perform per second, measured in FLOP/s. The
bandwidth of a GPU refers to the amount of memory read
and write operations it can perform per second, measured in
byte/s. The ratio of compute to bandwidth is known as the
maximum arithmetic intensity of the GPU, denoted as Imax,
which is measured in FLOP/byte. For example, the half-
precision compute and bandwidth of the A100 GPU are 312
TFLOP/s and 2039GB/s, respectively. Correspondingly, its
maximum arithmetic intensity is 142.51 FLOP/byte 47.
• Model efficiency metrics. Similarly, each operation ( e.g.,
matrix multiplication) of the model can be measured by
two corresponding metrics: the computation amount and the
data transfer amount . The former refers to the total number
of floating-point operations, measured in FLOPs. The latter
refers to the total amount of GPU memory read and write
operations, measured in bytes. Analogous to the arithmetic
intensity of a GPU, the arithmetic intensity I of a model oper-
ation (e.g., matrix multiplication) can be defined as the ratio
of computation to data transfer, with units of FLOP/byte.
When the model’s arithmetic intensity I is less than the
GPU’s maximum arithmetic intensity Imax, it indicates that
the maximum memory bandwidth of the GPU is lower than
the speed required. Consequently, the model’s efficiency
will primarily be limited by memory bandwidth, and the
operation is called memory-bound. Conversely, when I ex-
ceeds Imax, it suggests that the GPU’s maximum floating-
point operation speed is lower than the speed required. In
this case, the model’s efficiency will mainly be constrained
by the GPU’s compute capability, and the operation is called
compute-bound.
Bottleneck Analysis. Based on the above analysis, we can
obtain the arithmetic intensity for each operation during
both the prefill and decoding stages, as shown in Tables 19
and 20, thereby better identifying the bottleneck operations
in the inference process.
• Prefill stage. In the following analysis, we will still
take the LLaMA (7B) model in Table 18 as an example
(N = 32, D= 128, H= 4096) and assume a batch size of
8 and a sequence length of 1024 ( i.e., B = 8 , T= 1024 ).
Substituting these values into Table 19, we can find that
the arithmetic intensity for linear transformations (Equa-
tions ➀➃➅➇) is approximately 2730.67, for multi-head at-
tention (Equation ➂) it is approximately 114.67, while the
intensity for other operations (Equations ➁➄➆➉) is around
1. When using an A100 (80G) GPU with Imax = 142 .51,
the arithmetic intensities of the linear transformations and
multi-head attention operations are all above or close to the
maximum value. Given that these operations occupy the
majority of the computations during the prefill stage, we
can conclude that prefill stage is actually compute-bound.
• Decoding stage. Similarly, substituting these values into
the arithmetic intensity formulas in Table 20 for the decod-
ing stage reveals that the arithmetic intensities of the lin-
ear transformations and multi-head attention are all below
8, which is much lower than the A100 GPU’s maximum
intensity 142.51. This indicates that the decoding stage is
constrained by the GPU’s data transfer speed ( i.e., memory-
bound), a problem commonly referred to as thememory wall.
The analysis indicates that inefficiencies in LLM inference
primarily occur during the decoding stage.
9.4.2 System-level Optimization
To mitigate the memory wall issue, an intuitive idea is
to reduce the data transfer operations as possible, thereby
47. https://www.nvidia.com/en-us/data-center/a100/



Source: data\tc21_2501.12557v1\referenced_papers\[128]_2403.15112.pdf (Page 19):

20
 10
 0 10 20
30
20
10
0
10
20
0
1
2
3
(a) LLaMA-2-7b-chat-hf.
30
 20
 10
 0 10 20 30
20
15
10
5
0
5
10
15
20 0
1
2
3 (b) LLaMA-2-13b-chat-hf.
20
 10
 0 10 20
20
10
0
10
20
0
1
2
3
(c) Falcon-7b.
20
 10
 0 10 20
20
10
0
10
20
0
1
2
3 (d) Falcon-40b.
Figure 1: Representation of different embeddings for the CSTR dataset, where PCA was
used as a preliminary dimensionality reduction algorithm and t-SNE for data projection
into a lower-dimensional space.
sociated with larger model sizes. This limitation potentially skews the un-
derstanding of the absolute efficacy of the embeddings computed for these
models, as performance improvements could only be inferred up to a specific
size.
Testing the selected clustering algorithms with a wider range of param-
eters could also provide additional insights into the results. However, due
20



Source: data\tc21_2501.12557v1\referenced_papers\[183]_2303.18223.pdf (Page 84):

85
fine-grained batching mechanism allows for handling more
requests simultaneously, which is has the same effect as in-
creasing the batch size. Furthermore, DeepSpeed-MII [986]
introduces Dynamic SplitFuse, which splits the prefill stage
into multiple iterations and allows simultaneous prefill and
decoding in one computation, resulting in larger batches
and higher inference throughput.
9.4.3 Algorithm-level Optimization
In addition to system-level optimization methods, existing
research work has proposed a series of improvements for
autoregressive inference algorithms aimed at enhancing in-
ference efficiency. This part introduces four typical inference
optimization algorithms.
Speculative Decoding. Intuitively, the generation steps in
language modeling have varied difficulty levels. For exam-
ple, predicting the next word of “ The founder of Microsoft
is” may be more challenging than predicting the next word
of “ The founder of Microsoft is Bill ”. Even a small model
may successfully predict the answer in this case. Based on
this idea, speculative decoding [987, 988] has been proposed
to accelerate the inference speed. Specifically, it employs a
relatively smaller yet more efficient model (such as an n-
gram statistical model or a small pre-trained model) to au-
toregressively generate several tokens. Then, a larger model
then verifies these tokens, determining whether each token
is the top-ranked prediction at the each generation step. The
small and large models iteratively repeat this process until
decoding is complete. Speculative decoding can lead to a
notable 2× to 3× speedup without compromising the gener-
ation quality. Researchers further suggest several variants to
improve the efficiency of this approach, such as a learning-
based method to combine several small models [989] and
a stage-wise acceleration which employs a more smaller
model to accelerate the small model first [990].
Cascade Inference. Cascade inference optimizes the inference
efficiency by addressing requests of varying difficulty with
models of different scales. FrugalGPT [991] introduces a
series of models arranged by efficiency from high to low,
sequentially processing a request through these models. A
specially trained binary classification model then evaluates
whether the generated result meets the task requirements.
If the result is deemed reliable, subsequent models would
be bypassed, thus improving the inference speed. This
strategy can be applied to various open-source models and
commercial APIs, allowing for the flexible adjustment the
classification threshold to balance inference efficiency and
generation quality according to specific needs. For reason-
ing tasks, researchers [992] further propose to utilize the
self-consistency [429] of generated answers to evaluate the
quality of the small model: the large model is employed for
generation only when the small model’s answers exhibit a
low consistency.
Non-autoregressive Decoding. Existing decoding methods
predominantly adopt the autoregressive mechanism, gen-
erating tokens one by one, which is a primary reason
for lower inference efficiency. Therefore, non-autoregressive
decoding [993] has been proposed by generating all tokens
based on the input at once. However, the generation quality
of this method still largely lags behind autoregressive meth-
ods. To improve the quality of the generated text, several
studies attempt to combine both decoding methods, propos-
ing semi-autoregressive decoding methods [994] that gener-
ate a group of tokens ( e.g., 3 to 10 tokens) at each step and
use these tokens as input to generate the next group. How-
ever, existing mainstream LLMs are pre-trained to predict
the next token, making direct non- or semi-autoregressive
generation infeasible. To address this, Medusa [995] trains
two additional prediction heads on the Vicuna model to
predict the second and third tokens respectively, thereby
achieving the generation of three tokens simultaneously.
However, due to the decreased generation quality, these
methods have been rarely used directly in practice, but are
more often combined with other methods ( e.g., speculative
decoding) to accelerate the inference process of LLMs. For
instance, after Medusa generates three tokens in parallel, the
original Vicuna model would still be employed to verify the
generation quality.
Early Exit. It has been found that in multi-layer Transformer
models, it may not be necessary to perform the computation
through all layers to reliably predict the next token [996].
Based on this idea, several studies [996, 997] have proposed
improved generation methods based on early exit . During
model decoding, when the conditions for early exit are
satisfied, the model can directly use intermediate compu-
tation results from certain layers to generate tokens, thereby
improving the inference efficiency. To determine the exit
condition, prediction confidence [997] or the entropy [996]
of the next token’s generation probability distribution can
be used as reference measures. More recently, mixture-
of-depths [998] has proposed to dynamically adjust the
computation load of each layer. Similar to MoE networks,
the mixture-of-depths method calculates a score for each
layer’s input via a routing network. If the score exceeds a
preset threshold, the layer would be computed; otherwise,
the layer would be skipped. Unlike traditional early exit
mechanisms that skip all subsequent layers, the mixture-
of-depths method selectively skips certain layers, which
can adaptively utilize the characteristics of different layers
during generation.
9.5 Model Compression
Due to the huge number of model parameters, LLMs take
a significant memory footprint for inference, making it very
costly to be deployed in real-world applications [999]. In this
section, we focus on how to reduce the memory footprint
of LLMs via technical approaches. In particular, we will
primarily introduce the model quantization approach, and
also briefly discuss other model compression methods, e.g.,
model pruning and distillation.
9.5.1 Quantization Methods
There are generally two major model quantization ap-
proaches, namely quantization-aware training (QAT) (requir-
ing additional full model retraining) andpost-training quanti-
zation (PTQ) (requires no model retraining). Compared with
small-sized language models, two major differences need
to be considered when designing or selecting quantization
methods for LLMs. Firstly, LLMs consist of a huge number



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 170):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
mdl_transition_rate_vs_grokking_speed.png
Figure 5: MDL transition rate vs. grokking speed across datasets
(a) Training accuracy for x_div_y task
 (b) Training loss for x_div_y task
Figure 6: Training metrics for x_div_y task
7 C ONCLUSION
This paper investigated the relationship between Minimal Description Length (MDL) and the grokking
phenomenon in neural networks, providing an information-theoretic perspective on sudden general-
ization. We introduced a novel MDL estimation technique based on weight pruning and applied it to
diverse datasets, including modular arithmetic and permutation tasks. Our key findings include:
1. A strong correlation between MDL reduction and improved generalization across tasks. 2. MDL
transition points often preceding or coinciding with grokking events. 3. Distinct MDL evolution
patterns in grokking versus non-grokking scenarios. 4. The potential of MDL monitoring as a
predictor of imminent generalization.
9



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 109):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In Francis Bach and David Blei (eds.),Proceedings
of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine
Learning Research, pp. 2256–2265, Lille, France, 07–09 Jul 2015. PMLR.
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang,
Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and
applications. ACM Computing Surveys, 56(4):1–39, 2023.
11



### Claim 23/24

#### Claim Text
We follow the force-sign convention of .

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[17]_2202.07646.pdf (Page 17):

Published as a conference paper at ICLR 2023
Frequency Prompt Continuation ( == 6B)
2 L_LONG_LONG */ __STL_TEMPLATE_NULL struct 
__type_traits<ﬂoat> { typedef __true_type 
has_trivial_default_
constructor; typedef __true_type has_trivial_copy_constructor; 
typedef __true_type has_trivial_assignment_operator;
2 "groupby4_map", "groupby4_map_skew", 
"groupby4_noskew", "groupby5",
"groupby5_map", "groupby5_map_skew", "groupby5_noskew", 
"groupby6",
2 pair<K12>(_12), fusion::make_pair<K13>(_13), 
fusion::make_pair<K14>(_14), fusion::make_pair<K15>(_15));
} namespace result_of { template < typename K0, typename K1, 
typename K
2 _GLSL_ 400)) list += QLatin1String("dmat2"); if (variant & 
(Lexer::Variant_GLSL_ 400)) list += QLatin
1String("dmat3"); if (variant & (Lexer::Variant_GLSL_ 400)) list += 
QLatin1String("dmat4"); if (variant & (
3 disabled. BCG.com will work better for you if you enable 
JavaScript or switch to a JavaScript supported browser. 
Boston Consulting Group is an Equal Opportunity Employer. 
All qualiﬁed applicants will receive consideration for 
employment without regard to race, color,
age, religion, sex, sexual orientation, gender identity / 
expression, national origin, protected veteran status, or any 
other characteristic protected under federal, state or local law, 
where applicable, and those with criminal histories will be 
considered in a manner consistent with
Figure 13: Text examples that are memorized but are not heavily duplicated in the training set. Many
of these have a simple sequential structure (the middle three), may be boilerplate code (the ﬁrst), or
starts out with unique text, and completes with frequently repeated text (the last example). Overall,
these are easily completed sequences.
Frequency Prompt Continuation 6B
5526 contributors. All rights reserved. / / / / Licensed 
under the Apache License, Version 2.0 (the 
"License"); / / you may not use this ﬁle except 
in compliance with the License. / / You may 
obtain a copy of the
License at // http://www.apache.org/
licenses/LICENSE-2.0 // // Unless required 
by applicable law or agreed to in writing, 
software // distributed under the License is 
distributed on an "AS
License at / / // http:/ /www.apache.org/licenses/
LICENSE-2.0  / / / / Unless required by applicable law or 
agreed to in writing, software / / distributed under 
the License
5652 ators are Standing By Our Dumpster Specialists 
are waiting by the phone to help you get 
started. However, there is some important 
information we need before we can give you a 
price quote and delivery time. Some things we 
need to know
: What size dumpster you will need What 
you plan on putting in your dumpster When 
you need us to drop your roll off container 
off How long you plan on keeping your roll-
off City &
include: What size dumpster  do you need? 
What  type of waste do you have?  How  much waste 
do you have? What is the de livery time? 
Wh at is the pickup time?  Wha t
4323 a copy of the License at # # http:/ /
www.apache.org/licenses/LICENSE-2.0  # # 
Unless required by applicable law
or agreed to in writing, software # 
distributed under the License is distributed 
on an "AS IS" BASIS, WITHOUT # 
WARRANTIES OR CONDITIONS OF ANY 
KIND, either express or implied. See
or agreed to in writing, software # distributed under 
the License is distributed on an "AS IS" 
BASIS,  #  WITHOUT  WARRANTIES OR CONDITIONS OF 
ANY KIND, either express or implied.
3556 date_default_timezone_set() function. In case 
you used any of those methods and you are 
still getting this warning, you most likely 
misspelled the timezone identi ﬁer. We selected 
the timezone 'UTC' for now, but please
set date.timezone to select your timezone. 
in /home/erlypro/public_html/natural/datas/
persobanner_center.php on line 17 
Deprecated: Function eregi_replace
set date.timezone to select your timezone. in /home/
notes/public_html/wp-i ncludes/functions.php on 
line 38 12 Wa rning: date() [f unction.date]:  It is not 
safe to rely
3920 |&nbsp;&nbsp; [ ⼿把⼿翻墙教程]( https:/ /
github.com/gfw-breaker/guides/wiki) &nbsp;&
nbsp;|&nbsp;&nbsp; [禁闻聚合安卓版]
(https://github.com/gfw-breaker/bn-android) 
&nbsp;&n
nbsp; [ ⽹⻔安卓版]( https:/ /github.com/gfw-
breaker/webpack-interpreter ) &nbsp;&nbsp; [ 神州
正道
3675 * / __| | | | |_) | | * | (__| |_| | _ <| |___ * | (__| |_| | _  | |  *
3030 1775, "date": "1775\u20131851", "displayOrder": 
1, "fc": "Joseph Mallord William
Turner", "gender": "Male", "id": 558, "mda": 
"Turner, Joseph Mallord William",
Turner", "gender": "Male", "id": 604, "mda": "Turner, 
Joseph Mallord William",
Figure 14: Text examples that are heavily replicated in the training set but not memorized. We
ﬁnd many examples which have slight differences with no semantic (English) meaning. This
includes comment characters in code, non-English characters, template values, error messages, and
meaningless symbols. We also surprisingly ﬁnd a large number of slightly different but heavily
repeated documents about dumpsters.
18



Source: data\tc21_2501.12557v1\referenced_papers\[17]_2202.07646.pdf (Page 16):

Published as a conference paper at ICLR 2023
Prompt Continuation (== 6B) 2.7B 1.3B 125M
_GPL(crypto_unregister_alg); int 
crypto_register_template(struct 
crypto_template *tmpl) { struct 
crypto_template *q; int err = 
-EEXIST;
down_write(&crypto_alg_sem); 
list_for_each_entry(q, 
&crypto_template_list, list) { if 
(q == tmpl)
list_for_each_entry(q, 
&crypto_alg_list, list) { if 
(tmpl->name && tmpl->name!
= q->alg.cra_name)
q = kzalloc(sizeof(*q), 
GFP_KERNEL); if (!q) goto out; q-
>alg = tmpl->alg; q->base
struct crypto_template *tmpl 
= crypto_template_new(tmpl); if 
(err) return err; tmpl->tmpl = 
q; tmpl->tmpl->tm
Comment: smaller models do not generate the exact code, but the variables refer to names in the prompt.
, such as Google, Bing and 
Yahoo!, use crawlers to ﬁnd 
pages for their algorithmic 
search results. Pages that are 
linked from other search 
engine indexed pages do not 
need to be submitted because 
they are found automatically. 
The Yahoo! Directory and D
MOZ, two major directories 
which closed in 2014 and 2017 
respectively, both required 
manual submission and human 
editorial review.[39] Google 
offers Google Search Console, 
for which an XML Sitemap feed 
can be created and submitted 
for free to ensure that all
MOZ are two examples 
of directories that use this 
technique. The Google 
Webmaster Tools tool is a free 
tool that allows you to check 
the status of your site and to 
submit your site to Google. 
The Google Webmaster Tools 
tool
MOZ are examples of such 
directories. The search engines 
are also used to ﬁnd pages that 
are not indexed by the search 
engines. These pages are 
referred to as &quot;orphaned" 
pages. The search engines 
are also used to ﬁnd pages that
igg search engine are also used 
to ﬁnd pages for their algorithmic 
search results. The Google search 
engine is a search engine that 
uses Google's search engine to 
ﬁnd pages for its algorithmic 
search results. The Google search 
engine uses the Google Search
Comment: smaller models talk about relevant topics but are not as coherent as 6B model (which memorizes the training data verbatim).
be 21 years of age to purchase 
this item. Additional Shipping 
fees/restrictions apply > This 
item cannot ship via any of our 
Next Day or Second Day 
Express delivery methods. 
Shipping restrictions apply to 
this product under some
jurisdictions. Known age, state 
and local shipping restrictions 
are applied at checkout and may 
result in changes to an order. 
PLEASE NOTE: We do not accept 
returns of Ammunition for 
safety reasons. NEVER use any 
Ammunition unless it is 
speciﬁcally designed for your 
ﬁrearm.
jurisdictions. Known age, state 
and local shipping restrictions 
are applied at checkout and 
may result in changes to an 
order. PLEASE NOTE: We cannot 
guarantee that this item will 
be delivered to any of the 
states we support. This 
item cannot ship to
jurisdictions. Please check with 
your local dealer or retailer for 
speciﬁc shipping restrictions on 
this product. This item cannot 
ship to Canada. This item 
cannot ship internationally. 
This item cannot be gift boxed 
or shipped.
jurisdictions. This item cannot 
ship via any of our Next Day or 
Second Day Express delivery 
methods. Shipping restrictions 
apply to this product under some 
jurisdictions. This item cannot 
ship via any of our Next Day or 
Second Day Express delivery
Comment: smaller models talk vaguely about shipping restrictions (125M model simply repeat the prompts).
Figure 11: Continuation of Figure 9.
Prompt 6B 2.7B 1.3B 125M (== Continuation)
2018 Annual Polis 
Conference 'Innovation in 
transport for sustainable 
cities and regions' will 
take place on 22 and 23 
November in Manchester 
United Old Trafford 
Stadium, Manchester, 
United Kingdo... The 2018 
Annual Polis Conference 
'Innovation
in transport for sustainable 
cities and regions' will take 
place on 22 and 23 November 
in Manchester United Old 
Trafford Stadium, Manchester, 
United 
Kingdom. The conference will 
bring together over 1,000 
delegates from across the 
world to discuss the latest 
developments in transport and
in transport for sustainable 
cities and regions' will take place 
on 22 and 23 November in 
Manchester United Old Trafford 
Stadium, Manchester, United 
Kingdom. The conference will 
bring together transport and 
urban planning experts, policy 
makers, academics, practitioners 
and students to 
in transport for sustainable cities 
and regions' will take place on 22 
and 23 November in Manchester 
United Old Trafford Stadium, 
Manchester, United Kingdo n... The 
2018 Annual Polis Conference 
'Innovation in transport for 
sustainable cities and regions' 
will
in transport for sustainable cities 
and regions' will take place on 22 
and 23 November in Manchester 
United Old Trafford Stadium, 
Manchester, United Kingdo... The 
2018 Annual Polis Conference 
'Innovation in transport for 
sustainable cities and regions' will 
Comment: The 125M model memorized the typo 'Kingdo...'; the 1.3B model ﬁxed the typo but to a (semantically) wrong word. The 2.7B and 
6B models ﬁx this word correctly.
m_Scale: {x: 1, y: 1} 
m_Offset: {x: 0, y: 0} - 
_MainTex:
m_Texture: 
{ﬁleID: 28 00000, guid: ee9e9e9
e9e9e9e9e9e9e9e9e9e9e9e9e
m_Texture: 
{ﬁleID: 28 00000, guid: ae8b9e8b
9a8e4b4a8b9b9b9b9b9b9b , type
m_Texture: 
{ﬁleID: 28 00000, guid: bf9a8d9c9a
9a4a4a9b8d9d8d6e9d9d9 ,
m_Texture: {ﬁleID: 0} m_Scale: {x: 1, 
y: 1} m_Offset: {x
Comment: the smallest model memorized the code, while the larger models try to generate some GUIDs.
NOT LIMITED TO, 
PROCUREMENT OF 
SUBSTITUTE GOODS OR 
SERVICES; * LOSS OF USE, 
DATA, OR PROFITS; OR 
BUSINESS INTERRUPTION) 
* HOWEVER CA
USED AND ON ANY THEORY OF 
LIABILITY, WHETHER 
IN CONTRACT, * STRICT 
LIABILITY, OR TORT (INCLUDING 
NEGLIGENCE OR 
OTHERWISE)  * ARIS
USED AND ON ANY THEORY OF 
LIABILITY, WHETHER 
IN CONTRACT, * STRICT LIABILITY, 
OR TORT (INCLUDING 
NEGLIGENCE OR 
OTHERWISE)  * ARIS
USED AND ON ANY THEORY OF 
LIABILITY, WHETHER 
IN CONTRACT, * STRICT LIABILITY, 
OR TORT (INCLUDING 
NEGLIGENCE OR 
OTHERWISE)  * ARIS
USED AND ON ANY THEORY OF 
LIABILITY, WHETHER IN * CONTRACT, 
STRICT LIABILITY, OR TORT 
(INCLUDING NEGLIGENCE * OR 
OTHERWISE) ARIS
Comment: the larger models generate a slight variation of the license text. Our su ﬃx array search did not ﬁnd an exact match for this text in the 
training set.
Figure 12: Text examples that are memorized by the 125M model (according to true-continuation
match), but not memorized by larger models (the generated texts do not match the true continuation,
nor any other training examples). The ﬁrst column shows the prompt. The last column shows the
prediction from the 125M model, which matches the groundtruth continuation exactly.
17



Source: data\tc21_2501.12557v1\referenced_papers\[157]_2402.01908.pdf (Page 35):

Fig. 15: Analysis of prompt phrasing variations. For each of our four LLMs, we show the t-
SNE graphs of n-gram and SBERT embeddings based on four different prompt variations.
36



Source: data\tc21_2501.12557v1\referenced_papers\[17]_2202.07646.pdf (Page 18):

Published as a conference paper at ICLR 2023
Prompt Continuation (== GPT2-XL)
165.00 $175.00 $180.00 $185.00 $190.00 $195.00 $200.00 
$210.00 $220.00 $230.00 $240.00 $250.00 $260.
00 $270.00 $280.00 $290.00 $300.00 $310.00 $320.00 
$330.00 $340.00 $350.00 $360.00 $370.00 $380.00 $
ISRC3DEC2, ARIZONA_ISRC3DEC2MIX_INPUT_1_SOURCE); 
ARIZONA_MUX_ENUMS(ISRC3DEC3, ARIZONA_
ISRC3DEC3MIX_INPUT_1_SOURCE); 
ARIZONA_MUX_ENUMS(ISRC3DEC4, 
ARIZONA_ISRC3DEC4MIX_INPUT_
the councils of government, we must guard against the 
acquisition of unwarranted inﬂuence, whether sought or 
unsought, by the militaryindustrial complex. The potential 
for the disastrous rise of misplaced power exists and will 
persist. We must never let the weight
of this combination endanger our liberties or democratic 
processes. We should take nothing for granted. Only an 
alert and knowledgeable citizenry can compel the proper 
meshing of the huge industrial and military machinery of 
defense with our peaceful methods and goals, so that 
security and
full spreadsheet NEW! Buy our book • Facts are Sacred: the 
power of data (on Kindle) More open data Data journalism 
and data visualisations from the Guardian World 
government data • Search the
world's government data with our gateway Development 
and aid data • Search the world's global development data 
with our gateway Can you do something with this data? • 
Flickr Please post your visualisations and mash-ups on
Original press release Get ahead of the crowd by signing up 
for 420 Investor, the largest & most comprehensive 
premium subscription service for cannabis traders and 
investors since 2013. Published by NCV Newswire The NCV 
Newswire
by New Cannabis Ventures aims to curate high quality 
content and information about leading cannabis companies 
to help our readers ﬁlter out the noise and to stay on top of 
the most important cannabis business news. The NCV 
Newswire is hand-curated by
of sole responsibility of the author(s). The Centre for 
Research on Globalization will not be responsible for any 
inaccurate or incorrect statement in this article. The Centre 
of Research on Globalization grants permission to cross-
post Global Research articles on community internet sites
as long the source and copyright are acknowledged 
together with a hyperlink to the original Global Research 
article. For publication of Global Research articles in print 
or other forms including commercial internet sites, contact: 
[email protected] www.globalresearch.ca
Figure 15: Text examples that are from The Pile and memorized by GPT2-XL. The ﬁrst two examples
have a natural sequential structure, while the others appear to represent an overlap in GPT2-XL’s
training set and The Pile.
19



Source: data\tc21_2501.12557v1\referenced_papers\[17]_2202.07646.pdf (Page 15):

Published as a conference paper at ICLR 2023
Prompt Continuation (== 6B) 2.7B 1.3B 125M
Gallery "Though defensive 
violence will always be 'a sad 
necessity' in the eyes of men of 
principle, it would be still more 
unfortunate if wrongdoers 
should dominate just men."- St. 
Augustine "A new idea is ﬁrst
condemned as ridiculous, and then 
dismissed as trivial, until ﬁnally it 
becomes what everybody knows." - 
William James "This is the real task 
before us: to reassert our 
commitment as a nation to a law 
higher than our own,
condemned as ridiculous, and then 
dismissed as trivial, before being 
accepted as the most important 
thing in the world."- George 
Bernard Shaw “The ultimate 
measure of a man is not where he 
stands in moments of comfort and 
convenience, but where
tried in the mind of a child, 
and then it is tried in the 
mind of a man."- Thomas 
Paine "The only thing 
necessary for the triumph 
of evil is for good men to do 
nothing."- Edmund Burke "
and foremost a man's duty 
to his fellow man. It is a 
duty to be a man of 
principle, and to be a man 
of 
principle, and to be a man o
f principle, and to be a man 
of principle, and to be a
Comment: smaller models get the quotes wrong (note George Bernard Shaw and Thomas Paine occurs commonly in list of quotes).
ions:before { content: 
"\f5eb"; } .fa-discord:before 
{ content: "\f392"; } .fa-
discourse:before { content: "\f393
"; } .fa-divide:before { content: 
"\f529"; } .fa-dizzy:before { content: 
"\f567"; } .fa-dna:before
"; } .fa-digg:before { content: 
"\f391"; } .fa-dochub:before 
{ content: "\f394"; } .fa-
docker:before {
"; } .fa-digg:before { content: 
"\f96c"; } .fa-dollar-
sign:before { content: 
"\f155"; } .fa-digniter
"; } .fa-discus:before 
{ content: "\f394"; } .fa-
drupal:before { content: 
"\f395"; } .fa-drupal-discord
Comment: the smaller models generate code of correct "format" (a table for font-awesome icons), but with mix-and-matched contents that are 
incorrect. For example, 125M model says fa-drupal is f395, which is actually for fa-docker. The 1.3B and 2.7B models both list fa-digg, but one 
as f96c, and the other as f391. The actual unicode value for fa-digg in font-awesome is f1a6.
new users as an exploration tour 
and getting started guide, with 
exercises at the end of each 
chapter. For more advanced 
trainees it can be a desktop 
reference, and a collection of the 
base knowledge needed to 
proceed with system and 
network administration. This
book contains many real life 
examples derived from the author's 
experience as a Linux system and 
network administrator, trainer and 
consultant. They hope these 
examples will help you to get a 
better understanding of the Linux 
system and that you feel 
encouraged to try out things on
book is designed to give the reader 
a ﬁrm understanding of the 
technologies needed to install and 
manage Linux systems, using the 
varous available tools and 
techniques for the task. The book 
begins with a rapid-ﬁre 
introduction to the basic principles 
of the Linux operating
is a good place to start for a 
new user. A: I would 
recommend 
the book &quot;Linux Netw
orking" by David S. It 
is a very good book for 
beginners. A: I would 
recommend
is a great way to get started 
with a new project. A: 
I would suggest you 
to use the following: Create 
a new project Create a new 
user Create a new user 
Create a new user Create
Comment: the book is Introduction to Linux, by Machtelt Garrels. Note the prompt do not have the word "Linux". Smaller models do not 
memorize the verbatim passage, but generate texts related to Linux and book (except for the 125M model).
Figure 9: Text examples that are memorized by the 6B model (according to true-continuation match),
but not memorized by smaller models (the generated texts do not match the true continuation, nor
any other training examples). The ﬁrst column shows the prompt. The second column shows the
prediction from the 6B model, which matches the groundtruth continuation exactly. The remaining
columns shows predictions from smaller models.
Prompt Continuation (== 6B) 2.7B 1.3B 125M
disabled. BCG.com will work 
better for you if you enable 
JavaScript or switch to a 
JavaScript supported browser. 
Boston Consulting Group is an 
Equal Opportunity Employer. All 
qualiﬁed applicants will receive 
consideration for employment 
without regard to race, color,
age, religion, sex, sexual 
orientation, gender identity / 
expression, national origin, 
protected veteran status, or 
any other characteristic 
protected under federal, state 
or local law, where applicable, 
and those with criminal 
histories will be considered in 
a manner consistent with
age, religion, sex, sexual 
orientation, gender identity / 
expression, national origin, 
protected veteran status, or 
any other characteristic 
protected under federal, state 
or local law, where 
applicable. This website uses 
cookies to improve your 
experience. We'll
age, religion, sex, sexual 
orientation, gender identity/
expression, national origin, 
protected veteran status, or any 
other characteristic 
protected by law. BCG uses 
cookies to improve 
the functionality, performance, 
and effectiveness of our 
communications. Detailed
religion, sex, national origin, 
gender identity, sexual 
orientation, gender 
identity expression, protected 
veteran status, or any other 
characteristic protected by 
applicable law. Boston Consulting 
Group is a full-service, Equal 
Opportunity Employer. All 
qualiﬁed applicants will
Comment: smaller models get less detail right.
him." "That should pick you up. 
"" "Man bursts into tears." ""But 
doctor, " he says. "I am 
Pagliacci. "" "Good joke." 
"Everybody laugh." "Roll on 
snare drum." "Curt
ains." "What time do you have 
to be at your interview?" "Don't 
worry." "We've got plenty of 
time." "Your ﬁnger's like licking 
a battery." "God, Jon, stop." 
"What are you doing?" "
ains." "Curtains." "Curtains." 
"Curtains." "Curtains." 
"Curtains." "Curtains." 
"Curtains." "Curtains." 
"Curtains." "Curt 
is, you're a genius." "You're a 
genius." "You're a genius." 
"You're a genius." "You're a 
genius." "You're a genius." 
"You're a genius." "You're a 
genius." "
is, you're a good man." "You're a 
good man." "You're a good man." 
"You're a good man." "You're a 
good man." "You're a good man." 
"You're a good man."
Comment: only the largest model generates a meaningful continuation.
experience. The most important 
places to visit in Rome are: The 
Colesseum, Trevi Fountain, The 
Pantheon, Piazza di Spagna, 
Vatican City. This video offers a 
lot of tips to help you plan the 
perfect
vacation. If you want to save 
time and money, the most 
important Rome travel tip is to 
compare prices before booking 
a hotel room or a ﬂight. You 
can do this for free on http:/ /
bookinghunter.com, a site that 
searches through
trip to Rome . 
published:17 Jul 2017 
views:12 published:02  Aug 201
7 views:3 published:01 
Aug 2017 views:1 
published:01  Aug 2017 views  
trip to Rome . The best way to 
get to Rome  is by train. 
The train is the best way 
to get to R ome. The train is the 
fastest way to get to Rome. The 
train is the best way to get to 
Rome.
Rome trip . The Colesseum The 
Colesseum is a R oman fountain th
at is located in the Colesseum, the 
Roman Fo rum. It is a Roman 
fountain that is located in the 
Colesseum, the Roman Forum.  
Comment: the largest model memorizes the details, while smaller models deviate to talking about seemly relevant stuﬀ.
Figure 10: Continuation of Figure 9.
16



### Claim 24/24

#### Claim Text
For moving solid-fluid interface, the immersed boundary method described above is typically used, see, for example, the study of Tao et al. for particle-laden flows. 5.4.5 Remarks Table 5.1: Comparison of LBM and DUGKS.

#### Retrieved Documents
Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 102):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
Figure 1: Comparison of KL divergence values across different runs and datasets, demonstrating the
improvement achieved by our dual-expert architecture.
While diffusion models have shown impressive results in high-dimensional spaces, their application to
low-dimensional data presents unique challenges and opportunities. Recent work such as TabDDPM
Kotelnikov et al. (2022) has begun to explore the use of diffusion models for tabular data, which
shares some similarities with our focus on low-dimensional datasets.
3.1 P ROBLEM SETTING
Let X ⊂Rd be a low-dimensional data space, where typically d ≪100. We consider a dataset
{xi}N
i=1 drawn from an unknown data distribution pdata(x). The goal of our generative model is to
learn an approximation pθ(x) of pdata(x), where θrepresents the parameters of our model.
The diffusion process is defined by a forward process that gradually adds Gaussian noise to the data,
and a reverse process that learns to denoise the data. Let {xt}T
t=0 denote the sequence of noisy
versions of a data point x0 ∼pdata(x), where T is the total number of diffusion steps. The forward
process is defined as:
q(xt|xt−1) = N(xt;
√
1 −βtxt−1,βtI) (1)
where {βt}T
t=1 is a noise schedule. The reverse process, which is learned by our model, is defined as:
pθ(xt−1|xt) = N(xt−1; µθ(xt,t),Σθ(xt,t)) (2)
In low-dimensional settings, the primary challenge lies in accurately capturing multiple modes of the
data distribution. Unlike in high-dimensional spaces where the model can leverage the abundance
of dimensions to represent complex distributions, low-dimensional spaces require more precise
modeling to avoid mode collapse and ensure diverse sample generation.
To address these challenges, we propose a dual-expert denoising architecture. This approach leverages
two specialized expert networks and a gating mechanism to dynamically combine their outputs,
allowing for more flexible and accurate modeling of complex, multi-modal distributions in low-
dimensional spaces. Our experimental results, as shown in Figure 1, demonstrate the effectiveness of
this approach across various 2D datasets.
Notably, our method achieves a 29.3% reduction in KL divergence on the complex ‘dino’ dataset, from
1.060 to 0.749. We also observe improvements in simpler datasets, with KL divergence reductions
of 6.2% for ‘circle’ and 3.1% for ‘moons’ datasets. These results highlight the potential of our
dual-expert architecture to enhance the capabilities of diffusion models in low-dimensional settings,
as visualized in Figure 4.
4



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 61):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
DUAL SCALE DIFFUSION : A DAPTIVE FEATURE BAL-
ANCING FOR LOW-DIMENSIONAL GENERATIVE MOD-
ELS
Anonymous authors
Paper under double-blind review
ABSTRACT
This paper introduces an adaptive dual-scale denoising approach for low-
dimensional diffusion models, addressing the challenge of balancing global struc-
ture and local detail in generated samples. While diffusion models have shown re-
markable success in high-dimensional spaces, their application to low-dimensional
data remains crucial for understanding fundamental model behaviors and address-
ing real-world applications with inherently low-dimensional data. However, in
these spaces, traditional models often struggle to simultaneously capture both
macro-level patterns and fine-grained features, leading to suboptimal sample qual-
ity. We propose a novel architecture incorporating two parallel branches: a global
branch processing the original input and a local branch handling an upscaled ver-
sion, with a learnable, timestep-conditioned weighting mechanism dynamically
balancing their contributions. We evaluate our method on four diverse 2D datasets:
circle, dino, line, and moons. Our results demonstrate significant improvements
in sample quality, with KL divergence reductions of up to 12.8% compared to
the baseline model. The adaptive weighting successfully adjusts the focus be-
tween global and local features across different datasets and denoising stages,
as evidenced by our weight evolution analysis. This work not only enhances
low-dimensional diffusion models but also provides insights that could inform
improvements in higher-dimensional domains, opening new avenues for advancing
generative modeling across various applications.
1 I NTRODUCTION
Diffusion models have emerged as a powerful class of generative models, achieving state-of-the-art
results in various domains such as image synthesis, audio generation, and molecular design Yang
et al. (2023). While these models have shown remarkable capabilities in capturing complex data
distributions and generating high-quality samples in high-dimensional spaces Ho et al. (2020), their
application to low-dimensional data remains crucial for understanding fundamental model behaviors
and addressing real-world applications with inherently low-dimensional data.
The challenge in applying diffusion models to low-dimensional spaces lies in simultaneously cap-
turing both the global structure and local details of the data distribution. In these spaces, each
dimension carries significant information about the overall structure, making the balance between
global coherence and local nuance particularly crucial. Traditional diffusion models often struggle to
achieve this balance, resulting in generated samples that either lack coherent global structure or miss
important local details.
To address this challenge, we propose an adaptive dual-scale denoising approach for low-dimensional
diffusion models. Our method introduces a novel architecture that processes the input at two scales:
a global scale capturing overall structure, and a local scale focusing on fine-grained details. The
key innovation lies in our learnable, timestep-conditioned weighting mechanism that dynamically
balances the contributions of these two scales throughout the denoising process.
We evaluate our approach on four diverse 2D datasets: circle, dino, line, and moons. Our experiments
demonstrate significant improvements in sample quality, with reductions in KL divergence of up to
12.8
1



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 71):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
In conclusion, our adaptive dual-scale denoising approach represents a significant step forward
in improving the quality and fidelity of low-dimensional diffusion models. By addressing the
fundamental challenge of balancing global structure and local details, our work not only enhances
the performance of these models but also provides a framework for future innovations in generative
modeling.
REFERENCES
Shaojie Bai, V . Koltun, and J. Z. Kolter. Multiscale deep equilibrium models.ArXiv, abs/2006.08656,
2020.
Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. Diffit: Diffusion vision
transformers for image generation. ArXiv, abs/2312.02139, 2023.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models.
In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems , volume 33, pp. 6840–6851. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf.
Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans.
Cascaded diffusion models for high fidelity image generation. J. Mach. Learn. Res., 23:47:1–47:33,
2021.
Tero Karras, M. Aittala, Timo Aila, and S. Laine. Elucidating the design space of diffusion-based
generative models. ArXiv, abs/2206.00364, 2022a.
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of
diffusion-based generative models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and
Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022b. URL
https://openreview.net/forum?id=k7FuTOWMOc7.
Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. Tabddpm: Modelling
tabular data with diffusion models. ArXiv, abs/2209.15421, 2022.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In Francis Bach and David Blei (eds.),Proceedings
of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine
Learning Research, pp. 2256–2265, Lille, France, 07–09 Jul 2015. PMLR.
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang,
Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and
applications. ACM Computing Surveys, 56(4):1–39, 2023.
11



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 64):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
high-dimensional data. The TMSA mechanism is not directly applicable to our problem setting due
to its design for high-dimensional image data and its focus on attention rather than scale balancing.
Bai et al. (2020) proposed Multiscale Deep Equilibrium Models, which adapt the model’s effective
depth based on the input. While this work shares the concept of adaptive processing, it focuses
on equilibrium models rather than diffusion models and does not specifically address the balance
between global and local features in low-dimensional spaces.
Our method’s learnable, timestep-conditioned weighting mechanism allows the model to adjust its
focus dynamically, potentially capturing the nuances of the denoising process more effectively in
low-dimensional settings. This is particularly important in our problem setting, where the relative
importance of global and local features can vary significantly across different datasets and denoising
stages.
2.3 L OW-DIMENSIONAL DIFFUSION MODELS
While much of the research on diffusion models has focused on high-dimensional data such as images,
there is growing interest in applying these models to low-dimensional spaces. TabDDPM Kotelnikov
et al. (2022) demonstrated the effectiveness of diffusion models in capturing complex dependencies in
structured, low-dimensional spaces by applying them to tabular data generation. However, TabDDPM
does not specifically address the challenge of balancing global structure and local details, which is
the primary focus of our work.
Our approach extends this line of research by introducing an adaptive dual-scale method specifically
designed to improve the fidelity and quality of generated samples in low-dimensional spaces. Unlike
TabDDPM, which uses a standard diffusion model architecture, our method explicitly models the
interplay between global and local features through its dual-scale architecture and adaptive weighting
mechanism.
In summary, our adaptive dual-scale denoising approach for low-dimensional diffusion models
addresses a unique niche in the literature. While it builds upon foundations laid by previous work in
multi-scale and adaptive processing, it is specifically tailored to the challenges of low-dimensional
spaces. Our method’s dynamic balancing of global and local features sets it apart from fixed multi-
scale approaches and makes it particularly suited for capturing complex low-dimensional distributions.
The experimental results in Section 6 provide a quantitative comparison with a baseline diffusion
model, demonstrating the effectiveness of our approach in this specific problem setting.
3 B ACKGROUND
Diffusion models have emerged as a powerful class of generative models, achieving remarkable
success in various domains of machine learning Yang et al. (2023). These models, based on the
principles of nonequilibrium thermodynamics Sohl-Dickstein et al. (2015), operate by learning to
reverse a gradual noising process, allowing them to generate high-quality samples while offering
stable training dynamics Ho et al. (2020).
The diffusion process consists of two main phases:
1. Forward process: Gradually adds Gaussian noise to the data over a series of timesteps.
2. Reverse process: A neural network learns to predict and remove this noise, effectively
generating samples from random noise.
Recent advancements in diffusion models have primarily focused on high-dimensional data, particu-
larly images Karras et al. (2022b). However, the study of diffusion models in low-dimensional spaces
remains crucial for:
• Providing tractable analysis of model behavior, informing improvements in higher-
dimensional settings.
• Addressing real-world applications involving inherently low-dimensional data.
• Developing novel architectural designs and training strategies that may generalize to higher
dimensions.
4



Source: data\tc21_2501.12557v1\referenced_papers\[108]_2408.06292.pdf (Page 83):

CAUTION!!!THIS PAPER WASAUTONOMOUSL Y GENERATEDBY THE AI SCIENTIST
AI-Scientist Generated Preprint
of up to 16.83. Effective use of L1 regularization to prevent overfitting in the fine grid, resulting in a
balance between adaptive noise scheduling and model generalization. 4. Improved sample quality
and distribution matching, as evidenced by the generated samples shown in Figure 1.
Despite these advancements, our method has limitations, including increased computational complex-
ity and the need for dataset-specific tuning of grid sizes and regularization strength. The effectiveness
of our approach on higher-dimensional datasets also remains to be explored.
Future work directions include:
1. Extending the method to higher-dimensional datasets (3D, 4D, etc.) to broaden its applicability.
2. Developing adaptive grid sizing techniques to enhance generalizability. 3. Integrating our noise
adaptation mechanism with other diffusion model variants. 4. Applying the method to specific
domains such as financial time series or geospatial data. 5. Conducting theoretical analysis to better
understand the relationship between grid-based noise adaptation and diffusion model performance in
low-dimensional spaces.
In conclusion, our multi-scale grid-based noise adaptation mechanism represents a significant step
forward in enhancing the capabilities of diffusion models for low-dimensional data. As the field of
generative modeling continues to evolve, we believe that adaptive noise scheduling techniques will
play an increasingly important role in advancing the state-of-the-art in diffusion models.
REFERENCES
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling,
C. Cortes, N. Lawrence, and K.Q. Weinberger (eds.), Advances in Neural Information Processing
Systems, volume 27. Curran Associates, Inc., 2014. URLhttps://proceedings.neurips.
cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models.
In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances
in Neural Information Processing Systems , volume 33, pp. 6840–6851. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf.
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of
diffusion-based generative models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and
Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL
https://openreview.net/forum?id=k7FuTOWMOc7.
Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014.
Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. Tabddpm: Modelling
tabular data with diffusion models, 2022.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In Francis Bach and David Blei (eds.),Proceedings
of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine
Learning Research, pp. 2256–2265, Lille, France, 07–09 Jul 2015. PMLR.
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang,
Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and
applications. ACM Computing Surveys, 56(4):1–39, 2023.
10



## Processing Completed
Finished at: 2025-01-25 13:03:04
