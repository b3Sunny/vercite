[
  {
    "original_claim": "We train a Reward Model (RM) to score test cases based on these quality metrics, then employ it to provide feedback for the Proximal Policy Optimization (PPO) [39] algorithm to enhance LLMs to generate test cases maximizing the expected reward (i.e., higher quality tests).",
    "context_before": [
      "However, instead of relying on expensive, unpredictable, and often biased human feedback, we instill well-known quality properties into an LLM by scoring programs using static analysis, amenable to automated computation for the generated test cases."
    ],
    "context_after": [
      "We begin by generating test cases using foundational LLMs and investigating their alignment with established best practices and their susceptibility to test smells."
    ],
    "references": [
      "39"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "39",
        "main_query": "Proximal Policy Optimization (PPO) algorithm to enhance LLMs to generate test cases maximizing the expected reward (i.e., higher quality tests)",
        "rewritten_queries": [
          "PPO algorithm for improving LLMs to create test cases that maximize expected rewards",
          "Using Proximal Policy Optimization to optimize LLMs for generating high-quality test cases",
          "Enhancing LLMs with PPO to produce test cases that achieve higher expected rewards"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "This version of Codex is a GPT-style model, pretrained on 54 million public repositories using the causal language modeling objective, and has a maximum context length of 2,048 tokens [14].",
    "context_before": [
      "We generate tests using the OpenAI Codex Cushman model, version code-cushman-."
    ],
    "context_after": [
      "We focused on a specific prompt design, the‚Äúsimulated‚Äù prompt, to harness optimal test generation capabilities from the base Codex model."
    ],
    "references": [
      "14"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "14",
        "main_query": "This version of Codex is a GPT-style model, pretrained on 54 million public repositories using the causal language modeling objective, and has a maximum context length of 2,048 tokens",
        "rewritten_queries": [
          "Codex is a GPT-style model trained on 54 million public repositories with a maximum context length of 2,048 tokens",
          "The Codex model is pretrained on 54 million repositories and utilizes a causal language modeling objective with a context length of 2,048 tokens",
          "This Codex version, a GPT-style model, has been pretrained on 54 million public repositories and supports a maximum context length of 2,048 tokens"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "We employ adaptive focal context truncation to ensure the inputs fit within the model‚Äôs context length, similar to the representation introduced in [48].",
    "context_before": [
      "As shown in Figure 1, we construct the prompt as follows (top to bottom): ‚óè path/to/FocalClass.cs: The path to the file containing the focal method in the actual project structure. ‚óè ‚å©ContextAndFocalMethod‚å™: The source code of the focal method and its context. ‚óè path/to/TestFocalClass.cs:‚§¶ public void Test‚å©FocalMethod‚å™: A prompt hint which specifies a hypothetical filepath for the test class and the beginning of the test method signature."
    ],
    "context_after": [
      "In detail, we adopt a sequential approach to extract focal contexts, where each next option is a more concise representation than its predecessor: (1) Entire text of the focal file, which can include imports, namespaces, and other classes. (2) Focal class code, converting methods other than the focal method into their respective signatures. (3) Focal class code, converting non-focal methods to signatures and eliminating fields/comments. (4) A stripped-down version comprising only the focal class definition and focal method, while discarding all supplementary code. (5) Prompts persistently exceeding the permitted length were removed."
    ],
    "references": [
      "48"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "48",
        "main_query": "adaptive focal context truncation to ensure the inputs fit within the model‚Äôs context length, similar to the representation introduced in",
        "rewritten_queries": [
          "using adaptive focal context truncation to make inputs compatible with the model's context length, akin to the representation presented in",
          "applying adaptive focal context truncation to align inputs with the model's context length, as demonstrated in",
          "utilizing adaptive focal context truncation to fit inputs within the model's context length, comparable to the representation outlined in"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "RLSQM utilizes the Proximal Policy Optimization (PPO) [39] algorithm to train the policy model (PM).",
    "context_before": [
      "Once the reward model is trained, it is used to predict the rewards during PPO fine-tuning. 2.5.2 PPO Fine-tuning."
    ],
    "context_after": [
      "PPO operates as an actor-critic algorithm, combining two components using networks with separate weights: the actor, determining the policy ùúã, and the critic, valuing a given state-action pair to improve the actor‚Äôs choices."
    ],
    "references": [
      "39"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "39",
        "main_query": "Proximal Policy Optimization (PPO) algorithm to train the policy model (PM)",
        "rewritten_queries": [
          "PPO algorithm for training the policy model",
          "Using Proximal Policy Optimization to train the policy model",
          "Training the policy model with the PPO algorithm"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Additionally, PPO utilizes a clipped surrogate objective, which constrains updates to the actor network weights in order to avoid excessively large policy updates [39].",
    "context_before": [
      "We initialized the policy using the SFT model and initialized the value function using the weights of the reward model (this done following Ouyang et al. )."
    ],
    "context_after": [
      "Following Ouyang et al. , we use a variant of PPO which utilizes the Kullback-Leibler (KL) divergence to penalize the model from generating tokens which are dramatically different from the base model, modulated by a coefficient ùõΩ."
    ],
    "references": [
      "39"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "39",
        "main_query": "PPO utilizes a clipped surrogate objective, which constrains updates to the actor network weights in order to avoid excessively large policy updates",
        "rewritten_queries": [
          "PPO employs a clipped surrogate objective to limit the size of updates to the actor network weights",
          "The clipped surrogate objective in PPO restricts large policy updates to the actor network weights",
          "To prevent excessively large updates, PPO uses a clipped surrogate objective for the actor network weights"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "It is worth noting that our approach is not tied to the PPO algorithm and implementations may employ a variety of other reinforcement learning algorithms, such as A2C [30] or NLPO [35]. 3 EMPIRICAL STUDY DESIGN We have designed an empirical study with the primary objective of enhancing the quality of test cases generated by Language Model-based approaches through Reinforcement Learning from Static Quality Metrics (RLSQM).",
    "context_before": [
      "Figure 2 3 provides the details on the RLSQM‚Äôs PPO finetuning stage."
    ],
    "context_after": [
      "The study aims to address the following research questions: ‚óè RQ1: What is the quality of the test cases generated by LLMs? ‚óè RQ2: Can RLSQM be used to improve individual quality metrics? ‚óè RQ3: Can RLSQM improve the LLM to generate high-quality tests following best practices? 3.1 RQ 1: Assessing Test Case Quality To address RQ1, we generated test cases for a diverse set of public methods under tests (focal methods)."
    ],
    "references": [
      "30",
      "35"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "30",
        "main_query": "A2C",
        "rewritten_queries": [
          "A2C reinforcement learning algorithm",
          "Advantage Actor-Critic method",
          "A2C as an alternative to PPO"
        ]
      },
      {
        "related_to_reference": "35",
        "main_query": "A2C or NLPO",
        "rewritten_queries": [
          "Alternative reinforcement learning algorithms like A2C and NLPO",
          "Reinforcement learning methods other than PPO, including A2C and NLPO",
          "Various RL algorithms such as A2C and NLPO that can be used in place of PPO"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Policy models tended to diverge substantially from the base model in order to maximize reward from the reward model, resulting in errant behavior such as (1) mode collapse [11] where the model learned to generate a narrow band of tests (such as empty tests like TestFocalMethod(){}), and (2) catastrophic forgetting, where tests disregarding all properties save for the one being optimized.",
    "context_before": [
      "For policy model training, we selected the checkpoint with the best overall quality of the generated tests on the validation dataset, measured by summing all positive properties and subtracting all negative properties."
    ],
    "context_after": [
      "Thus, validation was crucial in order to select a model which retained the abilities of the initial weights and avoided mode collapse."
    ],
    "references": [
      "11"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "11",
        "main_query": "mode collapse where the model learned to generate a narrow band of tests (such as empty tests like TestFocalMethod(){})",
        "rewritten_queries": [
          "mode collapse resulting in the generation of a limited range of tests, including empty tests like TestFocalMethod(){}",
          "the phenomenon of mode collapse causing the model to produce a narrow set of outputs, such as empty tests like TestFocalMethod(){}",
          "the occurrence of mode collapse, leading to the model creating a restricted variety of tests, including empty tests like TestFocalMethod(){}"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Since GPT-4 is trained to understand and generate human-like code [33], it‚Äôs not surprising that it produced documentation and avoid consecutive duplicated assertions.",
    "context_before": [
      "GPT-4 produced tests with more Descriptive Names (23%), more Comments (78%), and fewer Duplicate Assertions (0.55%) than the Codex-based models (Base)."
    ],
    "context_after": [
      "The SFT model improved upon the Base model in all properties except Descriptive Names and Comments, but it could not be optimized beyond a certain point."
    ],
    "references": [
      "33"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "33",
        "main_query": "GPT-4 is trained to understand and generate human-like code",
        "rewritten_queries": [
          "GPT-4 has been designed to comprehend and produce code that resembles human writing",
          "The training of GPT-4 enables it to generate code in a human-like manner",
          "GPT-4's training focuses on understanding and creating code similar to that of humans"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "The aim of these efforts is to enhance compilation rates, produce passing tests from the current code, generate failing tests to expose bugs, or boost code coverage. [41] demonstrated that ChatGPT and Codex are prone to produce test smells on Python and Java code, but do not suggest how to improve the models.",
    "context_before": [
      "RL from Sequential and Combined Rewards (optimizing ‚úì Focal ‚Üí‚úó Conditional/Exception) improved upon the Base model further than Individual Rewards by increasing best practices: up to ‚Üë23.1% Assertions and ‚Üë20.5% Focal calls, and reducing test smells: up to ‚Üì1.8% Duplicate Assertions and ‚Üì2.4% Conditionals/Exceptions. 5 RELATED WORK Test Generation: Previous research on unit test generation has employed evolutionary algorithms, leading to tools such as EvoSuite and machine learning models ."
    ],
    "context_after": [
      "In this study, we introduce RLSQM as a method to enhance language models based on static quality metrics."
    ],
    "references": [
      "41"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "41",
        "main_query": "ChatGPT and Codex are prone to produce test smells on Python and Java code, but do not suggest how to improve the models.",
        "rewritten_queries": [
          "ChatGPT and Codex generate test smells in Python and Java without providing improvement suggestions.",
          "The models ChatGPT and Codex are known to create test smells in Python and Java, lacking guidance for enhancements.",
          "Test smells are produced by ChatGPT and Codex in Python and Java, yet they fail to offer ways to improve the models."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "This alignment is primarily achieved through a methodology termed Reinforcement Learning from Human Feedback (RLHF) [16].",
    "context_before": [
      "18 Benjamin Steenhoek, Michele Tufano, Neel Sundaresan, and Alexey Svyatkovskiy RL for LLMs: Reinforcement Learning has emerged as a promising tool to better align LLMs with human intentions and preferences."
    ],
    "context_after": [
      "Foundational works in this direction, such as InstructGPT , RLAIF , LLAMA2, and Code LLama , have proven the effectiveness of RLHF in refining LLM performance."
    ],
    "references": [
      "16"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "16",
        "main_query": "Reinforcement Learning from Human Feedback (RLHF)",
        "rewritten_queries": [
          "Methodology of Reinforcement Learning from Human Feedback",
          "RLHF as a method for aligning LLMs with human preferences",
          "Using Reinforcement Learning from Human Feedback for alignment"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "However, due to variations in internal factors such as age and interaction patterns, notable differences exist in individual immunity, vaccine efficacy, and vaccine information acquisition , leading to heterogeneity in vaccination.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "For the laser power used in this research, O‚àí 2 is almost 100% detached within the laser beam, taking into account the photodetachment cross-section .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Except for the data available in the GEISA database and the Ames-1 line list , there exists a global modelling study of the measured line lists of 15N2O within the framework of a polyad model of the effective Hamiltonian 2024-12-10 3 by Tashkun et al. .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Supposing that a desired velocity value u(d) f is known at a grid point coinciding with the fluid-solid interface, an explicit expression for the forcing term fn+1/2 can be obtained from (5.1) by requiring that un+1 f = u(d) f , viz. fn+1/2 = u(d) f ‚àíun f ‚àÜt ‚àíRHSn+1/2 . (5.2) Since, however, the particle surface does in general not coincide with the Eulerian grid nodes, interpolation of some kind is necessary in order to define the desired velocity.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "When coupled with OKMC models, this method could offer great efficiency with a considerable level of calculation accuracy .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Healthcare workers, frontline personnel, and other essential workers were identified as the priority groups, followed by the elderly aged 70 and above .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "From research on the memory effect, it is determined that electron densities of 108 ‚àí 109 cm‚àí3, which are far below the breakdown threshold, are already sufficient to guide the plasma in a direction nearly perpendicular to the background electric field direction .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "T he lower state constants were fixed to values from rotation spectroscopy in our simulations .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Reference deals with a number of practicalities when applying the approach to multiple spherical particles suspended in fluid. 19 Modelling approaches and computational methods for particle-laden turbulent flows Figure 5.3: Left panel: Half-way bounce-back representation of a circular no-slip boundary on a square lattice.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Note that line positions of Toth are not purely experimental; they we re calculated from fitted spectroscopic constants of the measured bands with uncertainties of the calculated line positions ranging between 0.001 ‚Äì 0.01 cm‚àí1 (or 30 ‚Äì 300 MHz) and of the line intensities ranging from 5% up to 100% (depending on the band).",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  }
]