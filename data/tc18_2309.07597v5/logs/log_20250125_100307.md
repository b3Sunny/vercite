# Claims Processing Log

Processing started at: 2025-01-25 10:03:07

## Table of Contents

[[log_20250125_100307###Claim 1/38|Claim 1/38]]
[[log_20250125_100307###Claim 2/38|Claim 2/38]]
[[log_20250125_100307###Claim 3/38|Claim 3/38]]
[[log_20250125_100307###Claim 4/38|Claim 4/38]]
[[log_20250125_100307###Claim 5/38|Claim 5/38]]
[[log_20250125_100307###Claim 6/38|Claim 6/38]]
[[log_20250125_100307###Claim 7/38|Claim 7/38]]
[[log_20250125_100307###Claim 8/38|Claim 8/38]]
[[log_20250125_100307###Claim 9/38|Claim 9/38]]
[[log_20250125_100307###Claim 10/38|Claim 10/38]]
[[log_20250125_100307###Claim 11/38|Claim 11/38]]
[[log_20250125_100307###Claim 12/38|Claim 12/38]]
[[log_20250125_100307###Claim 13/38|Claim 13/38]]
[[log_20250125_100307###Claim 14/38|Claim 14/38]]
[[log_20250125_100307###Claim 15/38|Claim 15/38]]
[[log_20250125_100307###Claim 16/38|Claim 16/38]]
[[log_20250125_100307###Claim 17/38|Claim 17/38]]
[[log_20250125_100307###Claim 18/38|Claim 18/38]]
[[log_20250125_100307###Claim 19/38|Claim 19/38]]
[[log_20250125_100307###Claim 20/38|Claim 20/38]]
[[log_20250125_100307###Claim 21/38|Claim 21/38]]
[[log_20250125_100307###Claim 22/38|Claim 22/38]]
[[log_20250125_100307###Claim 23/38|Claim 23/38]]
[[log_20250125_100307###Claim 24/38|Claim 24/38]]
[[log_20250125_100307###Claim 25/38|Claim 25/38]]
[[log_20250125_100307###Claim 26/38|Claim 26/38]]
[[log_20250125_100307###Claim 27/38|Claim 27/38]]
[[log_20250125_100307###Claim 28/38|Claim 28/38]]
[[log_20250125_100307###Claim 29/38|Claim 29/38]]
[[log_20250125_100307###Claim 30/38|Claim 30/38]]
[[log_20250125_100307###Claim 31/38|Claim 31/38]]
[[log_20250125_100307###Claim 32/38|Claim 32/38]]
[[log_20250125_100307###Claim 33/38|Claim 33/38]]
[[log_20250125_100307###Claim 34/38|Claim 34/38]]
[[log_20250125_100307###Claim 35/38|Claim 35/38]]
[[log_20250125_100307###Claim 36/38|Claim 36/38]]
[[log_20250125_100307###Claim 37/38|Claim 37/38]]
[[log_20250125_100307###Claim 38/38|Claim 38/38]]


## Processing Details


### Claim 1/38

#### Claim Text
Thus, the collected data must be properly cleaned before being utilized for the training of embeddings [53]. •Training.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 2):

consistency-based filter
share
……
1.3B unfiltered
270M
Encoder Encoder
query: text 1 passage: text 2
average pool
Eq Ep
CCPairs
average pool
Figure 1: Overview of our data curation pipeline and model architecture.
Harvesting semi-structured data sources Large-scale high-quality datasets like C4 [ 48] and
CCMatrix [ 51] are vital for the success of language model pre-training and machine translation. For
learning text embeddings, existing works either utilize small-scale human-annotated data such as NLI
[22] and MS-MARCO [ 8] or adopt heuristics such as random cropping [ 28] to obtain large-scale
but very noisy supervision signals.
Instead, we curate a text pair dataset CCPairs (Colossal Clean text Pairs) by harvesting heterogeneous
semi-structured data sources. Let ( q, p) denote a text pair consisting of a query q and a passage p.
Here we use “passage” to denote word sequences of arbitrary length, which can be a short sentence, a
paragraph, or a long document. Our dataset includes (post, comment) pairs from Reddit 3, (question,
upvoted answer) pairs from Stackexchange 4, (entity name + section title, passage) pairs from English
Wikipedia, (title, abstract) and citation pairs from Scientific papers [ 36], and (title, passage) pairs
from Common Crawl 5 web pages and various News sources.
We only include data sources that can be automatically mined, and some subsets are directly reused
from existing datasets. Simple heuristic rules are applied to filter data from Reddit and Common
Crawl. For example, we remove Reddit comments that are either too long ( > 4096 characters) or
receive score less than 1, and remove passages from web pages with high perplexity [ 60]. After
preliminary filtering, we end up with ∼ 1.3 billion text pairs, most of which come from Reddit and
Common Crawl. For more details and examples, please refer to Appendix A.
Consistency-based filter To further improve data quality and make training costs manageable, we
propose a consistency-based data filtering technique: a model is first trained on the 1.3B noisy text
pairs, and then used to rank each pair against a pool of 1 million random passages. A text pair is kept
only if it falls in the top-k ranked lists. In other words, the model’s prediction should be consistent
with the training labels. Here we set k = 2based on manual inspection of data quality. After this
step, we end up with ∼ 270M text pairs for contrastive pre-training.
The intuition for this technique comes from the memorization behaviors of neural networks [ 19]:
when trained on noisy datasets, neural networks tend to memorize the clean labels first and then
gradually overfit the noisy labels. Similar techniques [42, 15, 23] have been widely used for removing
dataset noises. It is also possible to apply this filter iteratively, we will leave it for future work.
4 Method
Our embeddings can be trained with only unlabeled text pairs from CCPairs with contrastive pre-
training. A second-stage fine-tuning on small, high-quality labeled datasets can be performed to
further boost the quality of the resulted embeddings. See Figure 1 for an overview.
3https://files.pushshift.io/reddit/
4https://archive.org/details/stackexchange
5https://commoncrawl.org/
3



Source: data\tc18_2309.07597v5\referenced_papers\[28]_2308.03281.pdf (Page 3):

where s(q, d) estimates the similarity between two
pieces of text q and d via vector distance between
q = E(q) and d = E(d).
To acquire text embeddings of superior quality
that can be applied across a wide range of scenar-
ios, we compile an extensive text pair dataset from
multiple formats and domains. This dataset is then
trained using an improved contrastive loss method
in a multi-stage fashion.
3.2 Unsupervised Pre-training Data
Weakly supervised text relevance data is readily
available in publicly accessible web sources, such
as the inherent connection between queries and
answers on QA forums. These data can be exten-
sively collected without the need for manual anno-
tation, thereby efficiently aiding in training text rep-
resentation models. Inspired by previous work (Ni
et al., 2022a,b; Neelakantan et al., 2022; Wang
et al., 2022b), our model is initially pre-trained
on naturally occurring text pairs extracted from
diverse sources. To ensure the versatility of the
embedding model, we explore a range of resources
for text pair extraction, including web pages (e.g.,
CommonCrawl, ClueWeb), scientific papers (e.g.,
arXiv, SemanticScholar), community QA forums
(e.g., StackExchange), social media (e.g., Reddit),
knowledge bases (e.g., Wikipedia, DBPedia), and
code repositories ( e.g., StackOverflow, GitHub).
Additionally, we harness the presence of hyperlinks
in certain datasets to facilitate text pair extraction.
Table 2 demonstrates some examples of text pair
format from different sources. Further details re-
garding the data collection process can be found
in Appendix A. In total, we utilized ∼800M text
pairs text pairs for the unsupervised pre-training
stage. Simple statistics and data distributions are
illustrated in Table 1.
3.3 Supervised Fine-tuning Data
In the supervised fine-tuning stage, we use rela-
tively lower-sized datasets with human annotation
of the relevance between two pieces of text and op-
tional hard negatives mined by an extra retriever to
form text triples. To handle both symmetric tasks
(e.g., semantic textual similarity) and asymmet-
ric tasks (e.g., passage retrieval), we collect data
from a large variety of tasks and domains, includ-
ing web search (e.g., MS MARCO), open-domain
QA (e.g., NQ), NLI (e.g., SNLI), fact verification
(e.g., FEVER), paraphrases (e.g., Quora). We to-
tally used ∼3M pairs for fine-tuning, which is a
Source Datasets Prop. Size
Web Page 3 18.7% 147M
Academic Paper 5 5.7% 45M
Hyperlink 4 13.4% 106M
Social Media 2 41.5% 327M
Knowledge Base 2 4.8% 38M
Community QA 7 1.5% 12M
News 5 0.4% 3M
Code 2 2.5% 20M
Others 3 11.6% 91M
Total 33 100% 788M
Table 1: Statistics of pre-training data.
combination of training data used by previous re-
search (Gao et al., 2021; Gao and Callan, 2022;
Asai et al., 2023; Su et al., 2023; Li et al., 2023).
More details can be found in Appendix A.
3.4 Training Details
Data Sampling In the initial stage of unsuper-
vised pre-training, data sources often differ signifi-
cantly in terms of the number of training instances.
To address this imbalance, we employ a multino-
mial distribution to sample data batches from differ-
ent data sources, taking into account their respec-
tive sizes. Suppose the whole pre-training dataset
D consists of m different subsets {D1, . . . , Dm}
and denote the size of each subset as ni = |Di|, at
each training iteration, the probability of sampling
data from the i-th subset Di can be represented by:
pi = nα
iPm
j=1 nα
j
, (4)
where we set α = 0.5 in this work. Furthermore,
to prevent the model from solely learning task-
specific shortcuts for discrimination, we ensure
that all training instances within a batch originate
from the same task.
Improved Contrastive Loss When using the
contrastive objective, people usually reuse in-batch
documents as negative candidates to improve train-
ing efficiency (Karpukhin et al., 2020). This paper
uses an improved contrastive learning objective
which is bidirectional and enlarges the negative
samples with both in-batched queries and docu-
ments. This can be viewd as a combination of loss
variants proposed by Radford et al. (2021); Ren
et al. (2021); Moiseev et al. (2023).



Source: data\tc18_2309.07597v5\referenced_papers\[37]_2201.10005.pdf (Page 0):

Text and Code Embeddings by Contrastive Pre-Training
Arvind Neelakantan* 1 Tao Xu* 1 Raul Puri1 Alec Radford1 Jesse Michael Han1 Jerry Tworek1
Qiming Yuan1 Nikolas Tezak1 Jong Wook Kim1 Chris Hallacy1 Johannes Heidecke1 Pranav Shyam1
Boris Power1 Tyna Eloundou Nekoul1 Girish Sastry1 Gretchen Krueger1 David Schnurr1
Felipe Petroski Such1 Kenny Hsu1 Madeleine Thompson1 Tabarak Khan1 Toki Sherbakov1 Joanne Jang1
Peter Welinder1 Lilian Weng1
Abstract
Text embeddings are useful features in many
applications such as semantic search and com-
puting text similarity. Previous work typically
trains models customized for different use cases,
varying in dataset choice, training objective and
model architecture. In this work, we show that
contrastive pre-training on unsupervised data at
scale leads to high quality vector representations
of text and code. The same unsupervised text em-
beddings that achieve new state-of-the-art results
in linear-probe classiﬁcation also display impres-
sive semantic search capabilities and sometimes
even perform competitively with ﬁne-tuned mod-
els. On linear-probe classiﬁcation accuracy aver-
aging over 7 tasks, our best unsupervised model
achieves a relative improvement of 4% and 1.8%
over previous best unsupervised and supervised
text embedding models respectively. The same
text embeddings when evaluated on large-scale
semantic search attains a relative improvement
of 23.4%, 14.7%, and 10.6% over previous best
unsupervised methods on MSMARCO, Natural
Questions and TriviaQA benchmarks, respec-
tively. Similarly to text embeddings, we train
code embedding models on (text, code) pairs, ob-
taining a 20.8% relative improvement over prior
best work on code search.
1. Introduction
Deep unsupervised learning with generative and embed-
ding models has seen dramatic success in the past few
years. Generative models (Peters et al., 2018; Raffel et al.,
2019; van den Oord et al., 2016; Ramesh et al., 2021;
Brown et al., 2020; Chen et al., 2021) are trained to max-
*Equal contribution 1OpenAI. Correspondence to: Arvind
Neelakantan <arvind@openai.com>.
S-300M M-1.2B L-6B XL-175B
Model Size
60
62
64
66
68
70Performance
Average performance vs model size
Figure 1.Average performance of unsupervised cpt-text
models of different sizes across 22 tasks consisting of linear-probe
classiﬁcation, text search, and sentence similarity tasks.
imize the likelihood of observed data while embedding
models are trained to distinguish observed data from noise
(Sohn, 2016; van den Oord et al., 2018; Radford et al.,
2021; Jia et al., 2021; Gao et al., 2021; Izacard et al., 2021).
Generative models have been shown to produce realistic
content and beneﬁt many downstream applications, reduc-
ing the need for labeled training datasets. In generative
models, the information about the input is typically dis-
tributed over multiple hidden states of the model. While
some generative models (Kingma & Welling, 2014; Kiros
et al., 2015) can learn a single representation of the in-
put, most autoregressive Transformer (Vaswani et al., 2017)
models do not (Raffel et al., 2019; Brown et al., 2020; Chen
et al., 2021; Ramesh et al., 2021). However, learning such a
representation (or embedding) is necessary for many tasks.
Systems that search over millions or billions of items re-
quire each entry to be embedded as a dense representation
and build an index in advance to save computational costs
at query time. These embeddings are useful features for
classiﬁcation tasks and can also enable data visualization
applications via techniques such as clustering. Embedding
models are explicitly optimized to learn a low dimensional
representation that captures the semantic meaning of the
input (Radford et al., 2021; Jia et al., 2021; Giorgi et al.,
2020; Gao et al., 2021; Izacard et al., 2021).
arXiv:2201.10005v1  [cs.CL]  24 Jan 2022



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 3):

4.1 Contrastive Pre-training with Unlabeled Data
Contrastive pre-training aims to distinguish the relevant text pairs from other irrelevant or negative
pairs. Given a collection of text pairs {(qi, pi)}n
i=1, we assign a list of negative passages {p−
ij}m
j=1
for the i-th example. Then the InfoNCE contrastive loss [10] is as follows:
min Lcont = −1
n
X
i
log esθ(qi,pi)
esθ(qi,pi) + P
j esθ(qi,p−
ij ) (1)
where sθ(q, p) is a scoring function between query q and passage p parameterized by θ. Following
the popular biencoder architecture, we use a pre-trained Transformer encoder and average pooling
over the output layer to get fixed-size text embeddings Eq and Ep. The score is the cosine similarity
scaled by a temperature hyperparameter τ:
sθ(p, q) =cos(Eq, Ep) / τ (2)
Where τ is set to 0.01 in our experiments by default. We use a shared encoder for all input texts and
break the symmetry by adding two prefix identifiers “query:” and “passage:” to q and d respectively.
For some data sources such as citation pairs, it is not obvious which side should be the query, we
randomly choose one for simplicity. Such an asymmetric design turns out to be important for some
retrieval tasks where there exist paraphrases of the query in the target corpus.
Another critical issue for contrastive training is how to select the negative samples. Here we choose
to use the in-batch negatives [ 10], where the passages from other pairs in a batch serve as negative
samples. We find that this simple strategy enables more stable training and outperforms methods
such as MoCo [25] when the batch size is sufficiently large.
4.2 Fine-tuning with Labeled Data
While contrastive pre-training on the CCPairs provides a solid foundation for general-purpose
embeddings, further training on labeled data can inject human knowledge into the model to boost the
performance. Although these datasets are small, existing works [ 43, 44] have shown that supervised
fine-tuning leads to consistent performance gains. In this paper, we choose to further train with
a combination of 3 datasets: NLI 6 (Natural Language Inference), MS-MARCO passage ranking
dataset [ 8], and NQ (Natural Questions) dataset [ 30, 32]. Empirically, tasks like STS (Semantic
Textual Similarity) and linear probing benefit from NLI data, while MS-MARCO and NQ datasets
transfer well to retrieval tasks.
Building on the practices of training state-of-the-art dense retrievers [ 50, 58], we use mined hard
negatives and knowledge distillation from a cross-encoder (CE) teacher model for the MS-MARCO
and NQ datasets. For the NLI dataset, contradiction sentences are regarded as hard negatives. The
loss function is a linear interpolation between contrastive lossLcont for hard labels and KL divergence
DKL for distilling soft labels from the teacher model.
min DKL(pce, pstu) +αLcont (3)
Where pce and pstu are the probabilities from the cross-encoder teacher model and our student model.
α is a hyperparameter to balance the two loss functions. Lcont is the same as in Equation 1.
4.3 Applications to Text Embedding Tasks
After the above two steps, we obtain high-quality text embeddings transferring well to a wide range
of tasks without fine-tuning the model parameters. Combined with techniques like approximate
nearest neighbor search, embeddings provide a scalable and efficient solution for applications like
web search. Here we briefly illustrate several use cases of our text embeddings.
Zero-shot Retrieval First, the passage embeddings for the target corpus are computed and indexed
offline. Then for each query, we compute its query embedding and return the top-k ranked lists from
the corpus based on cosine similarity.
Few-shot Text Classification A linear classifier is trained on top of the frozen embeddings with a
few labeled examples. Different tasks only need to train and save the parameters of the classification
heads. It can be seen as a particular form of parameter-efficient learning [27].
6The version released by SimCSE.
4



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 1):

heterogeneous training signals. We construct the CCPairs dataset by combining various semi-
structured data sources such as CommunityQA, Common Crawl and Scientific papers, and perform
aggressive filtering with a consistency-based filter [ 15] to improve data quality. We choose a
simple contrastive learning recipe using in-batch negatives with a large batch-size to train our model.
Extensive experiments on both BEIR and MTEB benchmarks demonstrate the effectiveness of
the proposed method. On the BEIR zero-shot retrieval benchmark [ 53], E5 is the first model to
outperform the strong BM25 baseline without using any labeled data. When fine-tuned on labeled
datasets, the performance can be further improved. Results on56 datasets from the recently introduced
MTEB benchmark [ 40] show that our E5base is competitive against GTRxxl and Sentence-T5xxl, which
have 40× more parameters.
2 Related Work
There have been long-lasting interests in transforming texts into low-dimensional dense embeddings.
Early works include Latent Semantic Indexing (LSA) [ 16] and Latent Dirichlet Allocation (LDA)
[3]. LSA utilizes the decomposition of a word-document co-occurrence matrix to generate document
embeddings, while LDA adopts probabilistic graphical models to learn topic distributions. Arora
et al. show that a simple weighted average of word vectors [ 38] can be a strong baseline for sentence
embeddings.
With the development of pre-trained language models [ 17, 35, 48] and large-scale labeled datasets
such as SNLI [ 6] and MS-MARCO [ 8], methods like Sentence-BERT [49], SimCSE [ 22], Sentence-
T5 [ 44] and SGPT [ 39] directly fine-tune language models to output continuous embeddings. Most
research focuses on short texts and thus uses the term "sentence embeddings". For long documents, it
remains an open research question whether fixed-length embeddings can encode all the information.
Contrastive loss popularized by SimCLR [ 10] turns out to be more effective than classification-
based losses [ 49, 14] for embeddings. LaBSE [ 20], LASER [ 2] and CLIP [ 47] further extend to
multilingual and multi-modal scenarios using parallel sentences and image-text pairs.
Another direction is to design self-supervised pre-training tasks for text matching and retrieval. [ 9]
proposes the well-known inverse cloze task (ICT), where a random sentence within a passage is
chosen as a pseudo-query and the rest is treated as a positive sample. However, Contriever [28] shows
that random cropping with data augmentation is more effective than ICT on a range of zero-shot
information retrieval tasks. OpenAI text embeddings [ 41] use neighboring texts as positives and
scale up the model size to 175B. Oguz et al. [45] performs domain-matched pre-training to improve
in-domain results. SPAR [ 11] trains a dense retriever by treating BM25 as a teacher model. Although
the aforementioned approaches can easily obtain abundant supervision signals, such synthetic data
tend to be of low quality. Results on the BEIR benchmark [ 53] show they struggle to match the
performance of BM25 if not further fine-tuned on labeled datasets.
Evaluation and interpretation of text embeddings are also non-trivial. Most benchmarks measure the
embedding quality through downstream task performances. For example, SentEval [ 13] uses linear
probing and a collection of semantic textual similarity (STS) datasets, while the BEIR benchmark
[53] focuses on zero-shot information retrieval scenarios. The recently introduced MTEB benchmark
[40] combines 56 datasets spanning across 8 tasks and 112 languages. Experiments show no model
can achieve state-of-the-art results on all embedding tasks yet. In this paper, we do not use the
SentEval toolkit since its linear probing setup depends on the optimization hyperparameters.
Most closely related to our work is a series of community efforts by sentence-transformers 2 to train
embeddings with a collection of labeled and automatically collected datasets. In this paper, we show
that it is possible to train high-quality embeddings using self-supervised pre-training only. In terms of
benchmark results, our model can achieve superior performance when fine-tuned on less labeled data.
3 CCPairs: A Large Collection of Text Pair Dataset
The quality and diversity of the data is crucial for training general-purpose text embeddings. In this
work, we mine and assemble CCPairs, a large high-quality text pair dataset from web sources which
provide diverse training signals transferring well to a wide range of tasks.
2https://github.com/UKPLab/sentence-transformers
2



### Claim 2/38

#### Claim Text
BEIR [51] provides a collection of 18 to evaluate the embedding’s general performances on different retrieval tasks, e.g., question answering and fact-checking.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 1):

Fact Checking
Citation-Prediction
W iki 
FEVER
QUERY
DOCS
Natural Claim
Wikipedia Articles
W iki 
Climate-FEVER
QUERY
DOCS
Climate-based Claim
Wikipedia Articles
SciFact
QUERY
DOCS
Scientific claim
PubMed ArticlesScientific 
SCIDOCS
QUERY
DOCS
Article Title
PubMed ArticlesScientific 
Dup. Question Retrieval
Quora 
Quora
QUERY
DOCS
StackEx. 
CQADupStack
QUERY
DOCS
Argument Retrieval
Misc. 
QUERY
DOCS
Misc. 
ArguAna
QUERY
DOCS
Tóuche-2020
Query Title
Query Title + Body
Query Title
Quora Questions
Argument
Idebate Arguments
Args.me Arguments
News Retrieval
TREC-NEWS
QUERY
DOCS News ArticlesNews Tweet Retrieval
Signal-1M
QUERY
DOCS
News Headline
Twitter TweetsT witter 
Question-Answering
W iki 
NQ
QUERY
DOCS
W iki 
HotpotQA
QUERY
DOCS
FiQA-2018
QUERY
DOCSFinance 
Bio-Medical IR
QUERY
DOCS
Scientific 
BioASQ
QUERY
DOCS
NFCorpus
QUERY
DOCSScientific 
Entity Retrieval
DBPedia
QUERY
DOCS
Entity-based Query
DBPedia ArticlesW iki 
TREC-COVID
Scientific 
Wikipedia Articles
Wikipedia Articles
Natural Query
Multi-Hop Query
CORD-19 Articles
PubMed Articles
PubMed Articles
COVID-19 Query
Nutrition Facts
Bio-Medical Query
Financial Query
Investment Articles
Controversial Query
9 Tasks
18 Datasets
News Headline
Robust04
QUERY
DOCSNews 
 News Articles
News Query
Figure 1: An overview of the diverse tasks and datasets in BEIR benchmark.
So far, it is unclear how well existing trained neural models will perform for other text domains or
textual retrieval tasks. Even more important, it is unclear how well different approaches, like sparse
embeddings vs. dense embeddings, generalize to out-of-distribution data.
In this work, we present a novel robust and heterogeneous benchmark called BEIR (Benchmarking
IR), comprising of 18 retrieval datasets for comparison and evaluation of model generalization. Prior
retrieval benchmarks [19, 50] have issues of a comparatively narrow evaluation focusing either only
on a single task, like question-answering, or on a certain domain. In BEIR , we focus on Diversity, we
include nine different retrieval tasks: Fact checking, citation prediction, duplicate question retrieval,
argument retrieval, news retrieval, question answering, tweet retrieval, bio-medical IR, and entity
retrieval. Further, we include datasets from diverse text domains, datasets that cover broad topics (like
Wikipedia) and specialized topics (like COVID-19 publications), different text types (news articles vs.
Tweets), datasets of various sizes (3.6k - 15M documents), and datasets with different query lengths
(average query length between 3 and 192 words) and document lengths (average document length
between 11 and 635 words).
We use BEIR to evaluate ten diverse retrieval methods from ﬁve broad architectures: lexical, sparse,
dense, late interaction, and re-ranking. From our analysis, we ﬁnd that no single approach consistently
outperforms other approaches on all datasets. Further, we notice that the in-domain performance of a
model does not correlate well with its generalization capabilities: models ﬁne-tuned with identical
training data might generalize differently. In terms of efﬁciency, we ﬁnd a trade-off between the
performances and the computational cost: computationally expensive models, like re-ranking models
and late interaction model perform the best. More efﬁcient approaches e.g. based on dense or sparse
embeddings can substantially underperform traditional lexical models like BM25. Overall, BM25
remains a strong baseline for zero-shot text retrieval.
Finally, we notice that there can be a strong lexical bias present in datasets included within the
benchmark, likely as lexical models are pre-dominantly used during the annotation or creation of
datasets. This can give an unfair disadvantage to non-lexical approaches. We analyze this for the
TREC-COVID [65] dataset: We manually annotate the missing relevance judgements for the tested
systems and see a signiﬁcant performance improvement for non-lexical approaches. Hence, future
work requires better unbiased datasets that allow a fair comparison for all types of retrieval systems.
With BEIR , we take an important step towards a single and uniﬁed benchmark to evaluate the zero-shot
capabilities of retrieval systems. It allows to study when and why certain approaches perform well,
and hopefully steers innovation to more robust retrieval systems. We release BEIR and an integration
of diverse retrieval systems and datasets in a well-documented, easy to use and extensible open-source
package. BEIR is model-agnostic, welcomes methods of all kinds, and also allows easy integration of
new tasks and datasets. More details are available at https://github.com/UKPLab/beir.
2 Related Work and Background
To our knowledge, BEIR is the ﬁrst broad, zero-shot information retrieval benchmark. Existing works
[19, 50] do not evaluate retrieval in a zero-shot setting in depth, they either focus over a single task,
small corpora or on a certain domain. This setting hinders for investigation of model generalization
across diverse set of domains and task types. MultiReQA [19] consists of eight Question-Answering
(QA) datasets and evaluates sentence-level answer retrieval given a question. It only tests a single
task and ﬁve out of eight datasets are from Wikipedia. Further, MultiReQA evaluates retrieval over
rather small corpora: six out of eight tasks have less than 100k candidate sentences, which beneﬁts
dense retrieval over lexical as previously shown [54]. KILT [50] consists of ﬁve knowledge-intensive
2



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 3):

Split(→) Train Dev Test Avg. Word Lengths
Task (↓) Domain (↓) Dataset (↓) TitleRelevancy#Pairs#Query#Query #Corpus Avg. D / QQuery Document
Passage-RetrievalMisc. MS MARCO [45] Binary 532,761 —- 6,980 8,841,823 1.1 5.96 55.98
Bio-Medical Bio-MedicalTREC-COVID [65] 3-level —- —- 50 171,332 493.5 10.60 160.77Information Bio-MedicalNFCorpus [7]  3-level 110,575 324 323 3,633 38.2 3.30 232.26Retrieval (IR) Bio-MedicalBioASQ [61]  Binary 32,916 —- 500 14,914,602 4.7 8.05 202.61
Question WikipediaNQ [34]  Binary 132,803 —- 3,452 2,681,468 1.2 9.16 78.88Answering WikipediaHotpotQA [76]  Binary 170,0005,447 7,405 5,233,329 2.0 17.61 46.30(QA) Finance FiQA-2018 [44]  Binary 14,166 500 648 57,638 2.6 10.77 132.32
Tweet-RetrievalTwitter Signal-1M (RT) [59] 3-level —- —- 97 2,866,316 19.6 9.30 13.93
News News TREC-NEWS [58] 5-level —- —- 57 594,977 19.6 11.14 634.79Retrieval News Robust04 [64]  3-level —- —- 249 528,155 69.9 15.27 466.40
Argument Misc. ArguAna [67]  Binary —- —- 1,406 8,674 1.0 192.98 166.80Retrieval Misc. Touché-2020 [6] 3-level —- —- 49 382,545 19.0 6.55 292.37
Duplicate-QuestionStackEx. CQADupStack [25] Binary —- —- 13,145 457,199 1.4 8.59 129.09Retrieval Quora Quora  Binary —- 5,000 10,000 522,931 1.6 9.53 11.44
Entity-RetrievalWikipediaDBPedia [21]  3-level —- 67 400 4,635,922 38.2 5.39 49.68
Citation-PredictionScientiﬁc SCIDOCS [9]  Binary —- —- 1,000 25,657 4.9 9.38 176.19
WikipediaFEVER [60]  Binary 140,0856,666 6,666 5,416,568 1.2 8.13 84.76Fact CheckingWikipediaClimate-FEVER [14] Binary —- —- 1,535 5,416,593 3.0 20.13 84.76Scientiﬁc SciFact [68]  Binary 920 —- 300 5,183 1.1 12.37 213.63
Table 1: Statistics of datasets in BEIR benchmark. Few datasets contain documents without titles. Relevancy
indicates the query-document relation: binary (relevant, non-relevant) or graded into sub-levels. Avg. D/Q
indicates the average relevant documents per query.
datasets in depth. Examples for each dataset are listed in Table 8. We additionally provide dataset
licenses in Appendix E, and links to the datasets in Table 5.
Table 1 summarizes the statistics of the datasets provided in BEIR . A majority of datasets contain
binary relevancy judgements, i.e. relevant or non-relevant, and a few contain ﬁne-grained relevancy
judgements. Some datasets contain few relevant documents for a query (< 2), while other datasets
like TREC-COVID [65] can contain up to even 500 relevant documents for a query. Only 8 out of 19
datasets (including MS MARCO) have training data denoting the practical importance for zero-shot
retrieval benchmarking. All datasets except ArguAna [67] have short queries (either a single sentence
or 2-3 keywords). Figure 1 shows an overview of the tasks and datasets in the BEIR benchmark.
Information Retrieval (IR) is ubiquitous, there are lots of datasets available within each task and
further even more tasks with retrieval. However, it is not feasible to include all datasets within the
benchmark for evaluation. We tried to cover a balanced mixture of a wide range of tasks and datasets
and paid importance not to overweight a speciﬁc task like question-answering. Future datasets can
easily be integrated in BEIR , and existing models can be evaluated on any new dataset quickly. The
BEIR website will host an actively maintained leaderboard2 with all datasets and models.
3.1 Dataset and Diversity Analysis
The datasets present in BEIR are selected from diverse domains ranging from Wikipedia, scientiﬁc
publications, Twitter, news, to online user communities, and many more. To measure the diversity in
domains, we compute the domain overlap between the pairwise datasets using a pairwise weighted
Jaccard similarity [26] score on unigram word overlap between all dataset pairs. For more details
on the theoretical formulation of the similarity score, please refer to Appendix F. Figure 2 shows a
heatmap denoting the pairwise weighted jaccard scores and the clustered force-directed placement
diagram. Nodes (or datasets) close in this graph have a high word overlap, while nodes far away in
the graph have a low overlap. From Figure 2, we observe a rather low weighted Jaccard word overlap
across different domains, indicating that BEIR is a challenging benchmark where approaches must
generalize well to diverse out-of-distribution domains.
3.2 BEIR Software and Framework
The BEIR software3 provides an is an easy to use Python framework (pip install beir) for model
evaluation. It contains extensive wrappers to replicate experiments and evaluate models from well-
known repositories including Sentence-Transformers [53], Transformers [72], Anserini [74], DPR
[31], Elasticsearch, ColBERT [32], and Universal Sentence Encoder [75]. This makes the software
useful for both academia and industry. The software also provides you with all IR-based metrics
from Precision, Recall, MAP (Mean Average Precision), MRR (Mean Reciprocal Rate) to nDCG
2 BEIR Leaderboard: https://tinyurl.com/beir-leaderboard
3 BEIR Code & documentation: https://github.com/UKPLab/beir
4



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 2):

tasks including a total of eleven datasets. The tasks involve retrieval, but it is not the primary task.
Further, KILT retrieves documents only from Wikipedia.
2.1 Neural Retrieval
Information retrieval is the process of searching and returning relevant documents for a query from
a collection. In our paper, we focus on text retrieval and use document as a cover term for text of
any length in the given collection and query for the user input, which can be of any length as well.
Traditionally, lexical approaches like TF-IDF and BM25 [55] have dominated textual information
retrieval. Recently, there is a strong interest in using neural networks to improve or replace these
lexical approaches. In this section, we highlight a few neural-based approaches and we refer the
reader to Lin et al. [37] for a recent survey in neural retrieval.
Retriever-based Lexical approaches suffer from the lexical gap [ 5]. To overcome this, earlier
techniques proposed to improve lexical retrieval systems with neural networks. Sparse methods such
as docT5query [48] identiﬁed document expansion terms using a sequence-to-sequence model that
generated possible queries for which the given document would be relevant. DeepCT [ 11] on the
other hand used a BERT [13] model to learn relevant term weights in a document and generated a
pseudo-document representation. Both methods still rely on BM25 for the remaining parts. Similarly,
SPARTA [79] learned token-level contextualized representations with BERT and converted the
document into an efﬁcient inverse index. More recently, dense retrieval approaches were proposed.
They are capable of capturing semantic matches and try to overcome the (potential) lexical gap.
Dense retrievers map queries and documents in a shared, dense vector space [18]. This allowed the
document representation to be pre-computed and indexed. A bi-encoder neural architecture based on
pre-trained Transformers has shown strong performance for various open-domain question-answering
tasks [19, 31, 35, 43]. This dense approach was recently extended by hybrid lexical-dense approaches
which aims to combine the strengths of both approaches [ 17, 57, 42]. Another parallel line of
work proposed an unsupervised domain-adaption approach [ 35, 43] for training dense retrievers
by generating synthetic queries on a target domain. Lastly, ColBERT [ 32] (Contextualized late
interaction over BERT) computes multiple contextualized embeddings on a token level for queries
and documents and uses an maximum-similarity function for retrieving relevant documents.
Re-ranking-based Neural re-ranking approaches use the output of a ﬁrst-stage retrieval system,
often BM25, and re-ranks the documents to create a better comparison of the retrieved documents.
Signiﬁcant improvement in performance was achieved with the cross-attention mechanism of BERT
[46]. However, at a disadvantage of a high computational overhead [53].
3 The BEIR Benchmark
BEIR aims to provide a one-stop zero-shot evaluation benchmark for all diverse retrieval tasks. To
construct a comprehensive evaluation benchmark, the selection methodology is crucial to collect
tasks and datasets with desired properties. For BEIR , the methodology is motivated by the following
three factors: (i) Diverse tasks: Information retrieval is a versatile task and the lengths of queries and
indexed documents can differ between tasks. Sometimes, queries are short, like a keyword, while in
other cases, they can be long like a news article. Similarly, indexed documents can sometimes be long,
and for other tasks, short like a tweet. (ii) Diverse domains: Retrieval systems should be evaluated
in various types of domains. From broad ones like News or Wikipedia, to highly specialized ones
such as scientiﬁc publications in one particular ﬁeld. Hence, we include domains which provide a
representation of real-world problems and are diverse ranging from generic to specialized. (iii) Task
difﬁculties: Our benchmark is challenging and the difﬁculty of a task included has to be sufﬁcient.
If a task is easily solved by any algorithm, it will not be useful to compare various models used
for evaluation. We evaluated several tasks based on existing literature and selected popular tasks
which we believe are recently developed, challenging and are not yet fully solved with existing
approaches. (iv) Diverse annotation strategies: Creating retrieval datasets are inherently complex
and are subject to annotation biases (see Section 6 for details), which hinders a fair comparison of
approaches. To reduce the impact of such biases, we selected datasets which have been created in
many different ways: Some where annotated by crowd-workers, others by experts, and others are
based on the feedback from large online communities.
In total, we include 18 English zero-shot evaluation datasets from 9 heterogeneous retrieval tasks. As
the majority of the evaluated approaches are trained on the MS MARCO [45] dataset, we also report
performances on this dataset, but don’t include the outcome in our zero-shot comparison. We would
like to refer the reader to Appendix D where we motivate each one of the 9 retrieval tasks and 18
3



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 16):

A Complementing Information
We provide the following additional sections in detail and information that complement discussions
in the main paper:
• Limitations of the BEIR benchmark in Appendix B.
• Training and in-domain evaluation task details in Appendix C.
• Description of all zero-shot tasks and datasets used in BEIR in Appendix D.
• Details of dataset licenses in Appendix E.
• Overview of the weighted jaccard similarity metric in Appendix F.
• Overview of the capped recall at k metric in Appendix G.
• Length preference for dense retrieval system in Appendix H.
B Limitations of the BEIR Benchmark
Even though we cover a wide range of tasks and domains in BEIR , no benchmark is perfect and has
its limitations. Making those explicit is a critical point in understanding the results on the benchmark
and, for future work, to improve up-on the benchmark.
1. Multilingual Tasks: Although we aim for a diverse retrieval evaluation benchmark, due to the
limited availability of multilingual retrieval datasets, all datasets covered in the BEIR benchmark
are currently English. It is worthwhile to add more multilingual datasets [ 2, 77] (in consideration
of the selection criteria) as a next step for the benchmark. Future work could include multi- and
cross-lingual tasks and models.
2. Long Document Retrieval: Most of our tasks have average document lengths up-to a few hundred
words roughly equivalent to a few paragraphs. Including tasks that require the retrieval of longer
documents would be highly relevant. However, as transformer-based approaches often have a length
limit of 512 word pieces, a fundamental different setup would be required to compare approaches.
3. Multi-factor Search: Until now, we focused on pure textual search in BEIR . In many real-world
applications, further signals are used to estimate the relevancy of documents, such as PageRank
[49], recency [16], authority score [33] or user-interactions such as click-through rates [ 51]. The
integration of such signals in the tested approaches is often not straight-forward and is an interesting
direction for research.
4. Multi-ﬁeld Retrieval: Retrieval can often be performed over multiple ﬁelds. For example, for
scientiﬁc publication we have the title, the abstract, the document body, the authors list, and the
journal name. So far we focused only on datasets that have one or two ﬁelds.
5. Task-speciﬁc Models: In our benchmark, we focus on evaluating models that are able to generalize
well for a broad range of retrieval tasks. Naturally in real-world, for some few tasks or domains,
specialized models are available which can easily outperform generic models as they focus and
perform well on a single task, lets say on question-answering. Such task-speciﬁc models do not
necessarily need to generalize across all diverse tasks.
C Training and In-domain Evaluation
We use the MS MARCO Passage Ranking dataset [45], which contains 8.8M Passages and an ofﬁcial
training set of 532,761 query-passage pairs for ﬁne-tuning for a majority of retrievers. The dataset
contains queries from Bing search logs with one text passage from various web sources annotated as
relevant. We ﬁnd the dataset useful for training, in terms of covering a wide variety of topics and
providing the highest number of training pairs. It has been extensively explored and used for ﬁne-
tuning dense retrievers in recent works [46, 17, 15]. We use the ofﬁcial MS MARCO development
set for our in-domain evaluation which has been widely used in prior research [46, 17, 15]. It has
6,980 queries. Most of the queries have only 1 document judged relevant; the labels are binary.
D Zero-shot Evaluation Tasks
Following the selection criteria mentioned in Section 3, we include 18 evaluation datasets that span
across 9 heterogeneous tasks. Each dataset mentioned below contains a document corpus denoted
17



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 4):

TREC-COVID
BioASQ
NFCorpus
NQ
HotpotQA
Signal-1M (RT)
FiQA-2018
ArguAna
Touché-2020
CQADupStack
Quora
DBPedia
SCIDOCS
FEVER
SciFact
TREC-NEWS
Robust04
TREC-COVID
BioASQ
NFCorpus
NQ
HotpotQA
Signal-1M (RT)
FiQA-2018
ArguAna
Touché-2020
CQADupStack
Quora
DBPedia
SCIDOCS
FEVER
SciFact
TREC-NEWS
Robust04
0.51
0.39 0.44
0.24 0.22 0.19
0.16 0.16 0.13 0.46
0.14 0.13 0.11 0.34 0.25
0.18 0.17 0.15 0.28 0.17 0.26
0.21 0.19 0.17 0.34 0.22 0.26 0.33
0.2 0.19 0.17 0.36 0.21 0.29 0.41 0.44
0.18 0.18 0.14 0.24 0.16 0.2 0.3 0.22 0.29
0.18 0.17 0.15 0.33 0.22 0.3 0.36 0.29 0.37 0.29
0.16 0.15 0.13 0.44 0.89 0.24 0.17 0.21 0.2 0.15 0.22
0.32 0.29 0.23 0.26 0.16 0.16 0.22 0.24 0.23 0.26 0.22 0.16
0.18 0.18 0.15 0.52 0.8 0.27 0.2 0.24 0.24 0.18 0.25 0.78 0.19
0.44 0.53 0.41 0.19 0.13 0.11 0.15 0.18 0.17 0.16 0.15 0.13 0.26 0.15
0.18 0.16 0.15 0.42 0.29 0.4 0.32 0.34 0.37 0.23 0.32 0.28 0.2 0.32 0.14
0.23 0.21 0.18 0.46 0.31 0.34 0.35 0.38 0.35 0.23 0.31 0.3 0.24 0.35 0.18 0.45
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Figure 2: Domain overlap across each pairwise dataset in the BEIR benchmark. Heatmap (left) shows the
pairwise weighted jaccard similarity scores between BEIR datasets. 2D representation (right) using a force-
directed placement algorithm with NetworkX [20]. We color and mark datasets differently for different domains.
(Normalised Cumulative Discount Gain) for any top-k hits. One can use the BEIR benchmark for
evaluating existing models on new retrieval datasets and for evaluating new models on the included
datasets.
Datasets are often scattered online and are provided in various ﬁle-formats, making the evaluation of
models on various datasets difﬁcult. BEIR introduces a standard format (corpus, queries and qrels)
and converts existing datasets in this easy universal data format, allowing to evaluate faster on an
increasing number of datasets.
3.3 Evaluation Metric
Depending upon the nature and requirements of real-world applications, retrieval tasks can be either
be precision or recall focused. To obtain comparable results across models and datasets in BEIR , we
argue that it is important to leverage a single evaluation metric that can be computed comparably
across all tasks. Decision support metrics such as Precision and Recall which are both rank unaware
are not suitable. Binary rank-aware metrics such as MRR (Mean Reciprocal Rate) and MAP (Mean
Average Precision) fail to evaluate tasks with graded relevance judgements. We ﬁnd thatNormalised
Cumulative Discount Gain (nDCG@k) provides a good balance suitable for both tasks involving
binary and graded relevance judgements. We refer the reader to Wang et al. [71] for understanding
the theoretical advantages of the metric. For our experiments, we utilize the Python interface of the
ofﬁcial TREC evaluation tool [63] and compute nDCG@10 for all datasets.
4 Experimental Setup
We use BEIR to compare diverse, recent, state-of-the-art retrieval architectures with a focus on
transformer-based neural approaches. We evaluate on publicly available pre-trained checkpoints,
which we provide in Table 6. Due to the length limitations of transformer-based networks, we use
only the ﬁrst 512 word pieces within all documents in our experiments across all neural architectures.
We group the models based on their architecture: (i) lexical, (ii) sparse, (iii) dense, (iv) late-interaction,
and (v) re-ranking. Besides the included models, the BEIR benchmark is model agnostic and in future
different model conﬁgurations can be easily incorporated within the benchmark.
(i) Lexical Retrieval: (a) BM25 [55] is a commonly-used bag-of-words retrieval function based on
token-matching between two high-dimensional sparse vectors with TF-IDF token weights. We use
Anserini [36] with the default Lucene parameters (k=0.9 and b=0.4). We index the title (if available)
and passage as separate ﬁelds for documents. In our leaderboard, we also tested Elasticsearch BM25
and Anserini + RM3 expansion, but found Anserini BM25 to perform the best.
5



### Claim 3/38

#### Claim Text
Later, MTEB [35] proposes a more holistic evaluation of embeddings and extends BEIR.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 1):

inating different tasks. Our benchmarking sheds
light on the weaknesses and strengths of individual
models, such as SimCSE’s (Gao et al., 2021b) low
performance on clustering and retrieval despite its
strong performance on STS. We hope our work
makes selecting the right embedding model easier
and simpliﬁes future embedding research.
2 Related Work
2.1 Benchmarks
Benchmarks, such as (Super)GLUE (Wang et al.,
2018, 2019) or Big-BENCH (Srivastava et al.,
2022), and evaluation frameworks (Gao et al.,
2021a) play a key role in driving NLP progress.
Yearly released SemEval datasets (Agirre et al.,
2012, 2013, 2014, 2015, 2016) are commonly used
as the go-to benchmark for text embeddings. Se-
mEval datasets correspond to the task of semantic
textual similarity (STS) requiring models to embed
similar sentences with geometrically close embed-
dings. Due to the limited expressivity of a single Se-
mEval dataset, SentEval (Conneau and Kiela, 2018)
aggregates multiple STS datasets. SentEval focuses
on ﬁne-tuning classiﬁers on top of embeddings. It
lacks tasks like retrieval or clustering, where em-
beddings are directly compared without additional
classiﬁers. Further, the toolkit was proposed in
2018 and thus does not provide easy support for
recent trends like text embeddings from transform-
ers (Reimers and Gurevych, 2019). Due to the
insufﬁciency of STS benchmarking, USEB (Wang
et al., 2021) was introduced consisting mostly of
reranking tasks. Consequently, it does not cover
tasks like retrieval or classiﬁcation. Meanwhile, the
recently released BEIR Benchmark (Thakur et al.,
2021) has become the standard for the evaluation
of embeddings for zero-shot information retrieval.
MTEB uniﬁes datasets from different embed-
ding tasks into a common, accessible evaluation
framework. MTEB incorporates SemEval datasets
(STS11 - STS22) and BEIR alongside a variety of
other datasets from various tasks to provide a holis-
tic performance review of text embedding models.
2.2 Embedding Models
Text embedding models like Glove (Pennington
et al., 2014) lack context awareness and are thus
commonly labeled as Word Embedding Models.
They consist of a layer mapping each input word
to a vector often followed by an averaging layer to
provide a ﬁnal embedding invariant of input length.
Transformers (Vaswani et al., 2017) inject context
awareness into language models via self-attention
and form the foundation of most recent embed-
ding models. BERT (Devlin et al., 2018) uses the
transformer architecture and performs large-scale
self-supervised pre-training. The resulting model
can directly be used to produce text embeddings
via an averaging operation alike Glove. Build-
ing on InferSent (Conneau et al., 2017), SBERT
(Reimers and Gurevych, 2019) demonstrated it to
be beneﬁcial to perform additional ﬁne-tuning of
the transformer for competitive embedding perfor-
mance. Most recent ﬁne-tuned embedding models
use a contrastive loss objective to perform super-
vised ﬁne-tuning on positive and negative text pairs
(Gao et al., 2021b; Wang et al., 2021; Ni et al.,
2021b; Muennighoff, 2022). Due to the large va-
riety of available pre-trained transformers (Wolf
et al., 2020), there is an at least equally large va-
riety of potential text embedding models to be ex-
plored. This leads to confusion about which model
provides practitioners with the best performance
for their embedding use case.
We benchmark both word embedding and trans-
former models on MTEB quantifying gains pro-
vided by often much slower context aware models.
3 The MTEB Benchmark
3.1 Desiderata
MTEB is built on a set of desiderata: (a) Diversity:
MTEB aims to provide an understanding of the
usability of embedding models in various use cases.
The benchmark comprises 8 different tasks, with
up to 15 datasets each. Of the 58 total datasets in
MTEB, 10 are multilingual, covering 112 differ-
ent languages. Sentence-level and paragraph-level
datasets are included to contrast performance on
short and long texts. (b) Simplicity: MTEB pro-
vides a simple API for plugging in any model that
given a list of texts can produce a vector for each
list item with a consistent shape. This makes it
possible to benchmark a diverse set of models. (c)
Extensibility: New datasets for existing tasks can
be benchmarked in MTEB via a single ﬁle that
speciﬁes the task and a Hugging Face dataset name
where the data has been uploaded (Lhoest et al.,
2021). New tasks require implementing a task in-
terface for loading the data and an evaluator for
benchmarking. We welcome dataset, task or metric
contributions from the community via pull requests
to continue the development of MTEB. (d) Repro-



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 0):

MTEB: Massive Text Embedding Benchmark
Niklas Muennighoff1, Nouamane Tazi1, Loïc Magne1, Nils Reimers2*
1Hugging Face 2cohere.ai
1firstname@hf.co 2info@nils-reimers.de
Abstract
Text embeddings are commonly evaluated on
a small set of datasets from a single task not
covering their possible applications to other
tasks. It is unclear whether state-of-the-art em-
beddings on semantic textual similarity (STS)
can be equally well applied to other tasks like
clustering or reranking. This makes progress
in the ﬁeld difﬁcult to track, as various models
are constantly being proposed without proper
evaluation. To solve this problem, we intro-
duce the Massive Text Embedding Benchmark
(MTEB). MTEB spans 8 embedding tasks cov-
ering a total of 58 datasets and 112 languages.
Through the benchmarking of 33 models on
MTEB, we establish the most comprehensive
benchmark of text embeddings to date. We
ﬁnd that no particular text embedding method
dominates across all tasks. This suggests that
the ﬁeld has yet to converge on a universal text
embedding method and scale it up sufﬁciently
to provide state-of-the-art results on all embed-
ding tasks. MTEB comes with open-source
code and a public leaderboard at https:
//github.com/embeddings-benchm
ark/mteb.
1 Introduction
Natural language embeddings power a variety of
use cases from clustering and topic representa-
tion (Aggarwal and Zhai, 2012; Angelov, 2020)
to search systems and text mining (Huang et al.,
2020; Zhu et al., 2021; Nayak, 2019) to feature
representations for downstream models (Saharia
et al., 2022; Borgeaud et al., 2022). Using gener-
ative language models or cross-encoders for these
applications is often intractable, as they may re-
quire exponentially more computations (Reimers
and Gurevych, 2019).
However, the evaluation regime of current text
embedding models rarely covers the breadth of
*Most of the work done while at Hugging Face. Corre-
spondence to n.muennighoff@gmail.com.
their possible use cases. For example, Sim-
CSE (Gao et al., 2021b) or SBERT (Reimers and
Gurevych, 2019) solely evaluate on STS and clas-
siﬁcation tasks, leaving open questions about the
transferability of the embedding models to search
or clustering tasks. STS is known to poorly corre-
late with other real-world use cases (Neelakantan
et al., 2022; Wang et al., 2021). Further, evaluating
embedding methods on many tasks requires imple-
menting multiple evaluation pipelines. Implemen-
tation details like pre-processing or hyperparam-
eters may inﬂuence the results making it unclear
whether performance improvements simply come
from a favorable evaluation pipeline. This leads to
the “blind” application of these models to new use
cases in industry or requires incremental work to
reevaluate them on different tasks.
The Massive Text Embedding Benchmark
(MTEB) aims to provide clarity on how models
perform on a variety of embedding tasks and thus
serves as the gateway to ﬁnding universal text em-
beddings applicable to a variety of tasks. MTEB
consists of 58 datasets covering 112 languages
from 8 embedding tasks: Bitext mining, classi-
ﬁcation, clustering, pair classiﬁcation, reranking,
retrieval, STS and summarization. MTEB software
is available open-source 1 enabling evaluation of
any embedding model by adding less than 10 lines
of code. Datasets and the MTEB leaderboard are
available on the Hugging Face Hub2.
We evaluate over 30 models on MTEB with addi-
tional speed and memory benchmarking to provide
a holistic view of the state of text embedding mod-
els. We cover both models available open-source
as well as models accessible via APIs, such as the
OpenAI Embeddings endpoint. We ﬁnd there to be
no single best solution, with different models dom-
1https://github.com/embeddings-benchm
ark/mteb
2https://huggingface.co/spaces/mteb/l
eaderboard
arXiv:2210.07316v3  [cs.CL]  19 Mar 2023



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 8):

4.4 Multilinguality
MTEB comes with 10 multilingual datasets across
bitext mining, classiﬁcation and STS tasks. We in-
vestigate performance on these in Figure 5. Tabular
results can be found in Tables 12, 13 and 14.
Bitext Mining LaBSE (Feng et al., 2020) per-
forms strongly across a wide array of languages in
bitext mining. Meanwhile, LASER2 shows high
variance across different languages. While there
are additional language-speciﬁc LASER2 models
available for some of the languages we benchmark,
we use the default multilingual LASER2 model
for all languages. This is to provide a fair one-to-
one comparison of models. In practice, however,
the high variance of LASER2’s performance may
be resolved by mixing its model variants. MP-
Net, MiniLM and SGPT-BLOOM-7B1-msmarco
perform poorly on languages they have not been
pre-trained on, such as German for the latter.
Classiﬁcation & STS On multilingual classiﬁ-
cation and STS, the multilingual MPNet provides
the overall strongest performance. It outperforms
the slightly faster multilingual MiniLM on almost
all languages. Both models have been trained
on the same languages, thus bringing decision-
making down to performance vs speed. SGPT-
BLOOM-7B1-msmarco provides state-of-the-art
performance on languages like Hindi, Portuguese,
Chinese or French, which the model has seen ex-
tensively during pre-training. It also performs com-
petitively on languages like Russian or Japanese
that unintentionally leaked into its pre-training
data (Muennighoff et al., 2022). However, it is not
much ahead of the much cheaper MPNet. LASER2
performs consistently worse than other models.
5 Conclusion
In this work, we presented the Massive Text Em-
bedding Benchmark (MTEB). Consisting of 8 text
embedding tasks with up to 15 datasets each and
covering 112 languages, MTEB aims to provide re-
liable embedding performance estimates. By open-
sourcing MTEB alongside a leaderboard, we pro-
vide a foundation for further pushing the state-of-
the-art of available text embeddings.
To introduce MTEB, we have conducted the
most comprehensive benchmarking of text embed-
dings to date. Through the course of close to 5,000
experiments on over 30 different models, we have
set up solid baselines for future research to build
on. We found model performance on different tasks
to vary strongly with no model claiming state-of-
the-art on all tasks. Our studies on scaling behav-
ior, model efﬁciency and multilinguality revealed
various intricacies of models that should ease the
decision-making process for future research or in-
dustry applications of text embeddings.
We welcome task, dataset or metric contributions
to the MTEB codebase7 as well as additions to the
leaderboard via our automatic submission format8.
7https://github.com/embeddings-benchm
ark/mteb
8https://huggingface.co/spaces/mteb/l
eaderboard



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 16):

STSBenchmark are monolingual english bench-
marks. STS17 and STS22 contain crosslingual
pairs of sentences, where the goal is to assess the
similarity of two sentences in different languages.
STS17 has 11 language pairs (among Korean, Ara-
bic, English, French, German, Turkish, Spanish,
Italian and Dutch) and STS22 has 18 language pairs
(among Arabic, English, French, German, Turkish,
Spanish, Polish, Italian, Russian and Chinese).
BIOSSES21 Contains 100 sentence pairs from
the biomedical ﬁeld.
SICK-R (Agirre et al., 2014) Sentences Involv-
ing Compositional Knowledge (SICK) contains a
large number of sentence pairs (10 0000) that are
lexically, syntactically and semantically rich.
A.7 Summarization
SummEval (Fabbri et al., 2020) Summaries gen-
erated by recent summarization models trained on
CNN or DailyMail alongside human annotations.
A.8 Retrieval
We refer to the BEIR paper (Thakur et al., 2021),
which contains description of each dataset. For
MTEB, we include all publicly available datasets:
ArguAna, ClimateFEVER, CQADupstack, DB-
Pedia, FEVER, FiQA2018, HotpotQA, MS-
MARCO, NFCorpus, NQ, Quora, SCIDOCS,
SciFact, Touche2020, TRECCOVID.
B Limitations of MTEB
While MTEB aims to be a diverse benchmark to
provide holistic performance reviews, the bench-
mark has its limitations. We list them here:
1. Long document datasets MTEB covers mul-
tiple text lengths (S2S, P2P, S2P), but very long
documents are still missing. The longest datasets in
MTEB have a few hundred words, and longer text
sizes could be relevant for use cases like retrieval.
2. Task imbalance Tasks in MTEB have a differ-
ent amount of datasets with summarization consist-
ing of only a single dataset. This means MTEB av-
erage scores, which are computed over all datasets,
are biased towards tasks with many datasets, no-
tably retrieval, classiﬁcation and clustering. As
MTEB grows, we hope to add more datasets to cur-
rently underrepresented tasks like summarization
or pair classiﬁcation.
21https://tabilab.cmpe.boun.edu.tr/BIO
SSES/DataSet.html
3. Multinguality MTEB contains multilingual
classiﬁcation, STS and bitext mining datasets.
However, retrieval and clustering are English-only.
SGPT-BLOOM-7B1-msmarco is geared towards
multilingual retrieval datasets and due to the lack
thereof cannot be comprehensively benchmarked
in MTEB. Further, MTEB does not contain any
code datasets that could be used to benchmark code
models (Neelakantan et al., 2022; Allal et al., 2023).
It should be easy to extend MTEB with datasets,
such as CodeSearchNet (Husain et al., 2019), TyDI
QA (Clark et al., 2020), XOR QA (Asai et al., 2020)
or MIRACL (Zhang et al., 2022).
4. Additional modalities Text embeddings are
commonly used as input features for downstream
models, such as in our classiﬁcation task. This
can involve other modalities, notably image con-
tent (Carvalho et al., 2018; Tan and Bansal, 2019;
Muennighoff, 2020; Nichol et al., 2021; Saharia
et al., 2022; Weinbach et al., 2022). We have fo-
cused solely on natural language applications and
leave extensive benchmarking of text embeddings
as inputs for other modalities to future work.
C Examples
Tables 3-9 provide examples for each dataset for
each task. For retrieval datasets, we refer to the
BEIR paper (Thakur et al., 2021).
D Correlations
Figure 6 provides correlation heatmaps for model
performance and MTEB tasks.
E Models
Table 10 provides publicly available model check-
points used for MTEB evaluation.
F Additional results
Tables 11 until the end provide results on individ-
ual datasets of MTEB. The results are additionally
available in json format on the Hugging Face Hub22
and can be inspected on the leaderboard23.
22https://huggingface.co/datasets/mteb
/results
23https://huggingface.co/spaces/mteb/l
eaderboard



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 3):

AmazonCounterfactualClassification
AmazonPolarityClassification
AmazonReviewsClassification
Banking77Classification
EmotionClassification
ImdbClassification
MassiveIntentClassification
MassiveScenarioClassification
MTOPDomainClassification
MTOPIntentClassification
T oxicConversationsClassification
T weetSentimentExtractionClassification
ArxivClusteringP2P
ArxivClusteringS2S
BiorxivClusteringP2P
BiorxivClusteringS2S
MedrxivClusteringP2P
MedrxivClusteringS2S
RedditClustering
RedditClusteringP2P
StackExchangeClustering
StackExchangeClusteringP2P
T wentyNewsgroupsClustering
SprintDuplicateQuestions
T witterSemEval2015
T witterURLCorpus
AskUbuntuDupQuestions
MindSmallReranking
SciDocsRR
StackOverflowDupQuestions
ArguAna
ClimateFEVER
CQADupstackAndroidRetrieval
CQADupstackEnglishRetrieval
CQADupstackGamingRetrieval
CQADupstackGisRetrieval
CQADupstackMathematicaRetrieval
CQADupstackPhysicsRetrieval
CQADupstackProgrammersRetrieval
CQADupstackStatsRetrieval
CQADupstackT exRetrieval
CQADupstackUnixRetrieval
CQADupstackWebmastersRetrieval
CQADupstackWordpressRetrieval
DBPedia
FEVER
FiQA2018
HotpotQA
MSMARCO
NFCorpus
NQ
QuoraRetrieval
SCIDOCS
SciFact
T ouche2020
TRECCOVID
BIOSSES
SICK-R
STS12
STS13
STS14
STS15
STS16
STS17
STS22
STSBenchmark
SummEval
AmazonCounterfactualClassification
AmazonPolarityClassification
AmazonReviewsClassification
Banking77Classification
EmotionClassification
ImdbClassification
MassiveIntentClassification
MassiveScenarioClassification
MTOPDomainClassification
MTOPIntentClassification
T oxicConversationsClassification
T weetSentimentExtractionClassification
ArxivClusteringP2P
ArxivClusteringS2S
BiorxivClusteringP2P
BiorxivClusteringS2S
MedrxivClusteringP2P
MedrxivClusteringS2S
RedditClustering
RedditClusteringP2P
StackExchangeClustering
StackExchangeClusteringP2P
T wentyNewsgroupsClustering
SprintDuplicateQuestions
T witterSemEval2015
T witterURLCorpus
AskUbuntuDupQuestions
MindSmallReranking
SciDocsRR
StackOverflowDupQuestions
ArguAna
ClimateFEVER
CQADupstackAndroidRetrieval
CQADupstackEnglishRetrieval
CQADupstackGamingRetrieval
CQADupstackGisRetrieval
CQADupstackMathematicaRetrieval
CQADupstackPhysicsRetrieval
CQADupstackProgrammersRetrieval
CQADupstackStatsRetrieval
CQADupstackT exRetrieval
CQADupstackUnixRetrieval
CQADupstackWebmastersRetrieval
CQADupstackWordpressRetrieval
DBPedia
FEVER
FiQA2018
HotpotQA
MSMARCO
NFCorpus
NQ
QuoraRetrieval
SCIDOCS
SciFact
T ouche2020
TRECCOVID
BIOSSES
SICK-R
STS12
STS13
STS14
STS15
STS16
STS17
STS22
STSBenchmark
SummEval
97
85 84
90 89 83
90 89 84 87
91 94 81 85 85
92 92 89 91 89 88
92 92 89 91 89 88 100
91 92 87 92 88 88 98 98
91 92 87 92 88 88 98 98 100
93 93 87 90 89 90 96 96 95 95
94 94 89 91 92 90 97 97 96 96 98
91 91 83 87 83 86 90 90 89 89 89 89
92 93 87 90 87 89 97 97 95 95 96 96 93
88 88 81 85 82 85 87 87 87 87 87 87 95 90
91 91 85 87 85 88 93 93 92 92 92 92 95 96 94
87 87 81 84 81 84 87 87 87 87 87 87 92 89 96 93
89 89 83 85 83 85 90 90 89 89 89 90 93 93 93 97 96
94 94 88 92 90 89 95 95 95 95 95 96 90 95 88 93 88 91
94 95 86 93 92 91 95 95 95 95 96 97 92 95 90 93 89 91 96
92 92 89 91 88 88 95 95 94 94 94 94 92 96 89 94 89 92 95 95
87 87 79 86 82 83 90 90 89 89 89 88 89 91 86 90 85 88 89 91 92
93 93 88 91 87 88 96 96 95 95 95 96 92 98 89 95 90 93 95 95 96 91
74 74 69 78 72 69 77 77 79 79 74 75 73 75 71 75 71 74 77 76 77 74 77
88 89 83 85 85 85 91 91 90 90 92 92 85 91 83 88 83 85 91 92 89 84 90 71
92 92 84 88 87 89 92 92 92 92 93 93 89 92 87 91 87 89 93 93 92 86 93 74 88
88 87 85 89 84 84 92 92 91 91 89 91 89 91 86 90 85 88 90 90 92 88 92 77 85 87
84 86 80 81 80 84 89 89 88 88 88 88 85 88 82 88 82 86 88 88 87 82 89 67 84 88 83
91 92 86 89 85 88 95 95 93 93 93 93 94 97 91 97 91 95 94 94 95 92 96 76 89 92 91 89
88 88 84 87 83 83 92 92 91 91 89 90 90 92 86 92 86 90 90 90 93 92 92 75 85 87 92 84 93
92 91 84 87 85 89 90 90 90 90 91 90 92 91 91 91 91 90 91 93 92 87 92 72 86 91 87 86 92 88
87 88 83 86 83 84 91 91 90 90 90 91 88 91 85 90 86 88 91 90 90 85 92 72 85 88 86 84 90 86 87
88 87 80 89 82 84 90 90 90 90 88 88 87 89 85 88 85 86 89 90 91 92 90 79 84 86 90 81 90 90 87 85
91 91 83 89 86 87 92 92 92 92 92 92 90 93 88 91 88 89 93 93 96 91 93 74 86 90 87 83 92 89 91 88 91
91 90 82 90 85 87 93 93 92 92 91 91 89 93 87 91 87 89 93 94 94 95 93 75 87 89 90 84 92 91 90 88 94 94
86 85 79 86 80 81 87 87 87 87 86 86 88 89 85 89 86 88 87 88 91 93 89 74 81 84 88 80 90 91 86 84 91 90 92
88 87 80 87 82 83 89 89 89 89 87 87 91 91 88 91 87 89 89 90 93 94 90 77 82 86 89 81 92 92 88 85 92 92 93 94
88 88 80 87 82 83 89 89 88 88 89 88 93 92 88 92 87 89 90 91 93 92 91 73 83 87 87 82 92 88 90 86 91 94 93 90 93
88 88 81 87 82 85 90 90 89 89 88 88 90 91 88 91 87 89 90 91 94 95 92 75 83 87 88 81 93 92 91 85 92 95 94 93 94 94
87 87 80 86 81 82 88 88 88 88 87 87 92 91 89 92 90 91 89 90 92 93 91 74 83 86 87 81 93 90 89 85 90 93 92 93 96 94 95
87 87 80 86 80 82 88 88 88 88 86 87 90 90 87 90 86 88 89 89 93 92 90 75 82 85 88 81 90 91 87 84 90 92 91 92 96 91 93 93
88 87 81 88 82 84 90 90 89 89 88 88 89 90 87 90 86 88 90 90 93 93 91 76 83 86 93 80 91 91 88 85 94 93 94 93 95 93 95 94 94
88 88 80 88 82 84 90 90 89 89 89 88 89 91 87 90 87 89 90 91 93 93 91 74 83 87 89 83 92 91 89 85 93 93 94 93 93 92 96 93 93 94
87 87 80 87 82 83 89 89 88 88 88 88 89 89 86 90 86 88 89 90 92 92 90 75 83 87 89 81 91 92 88 84 92 92 93 92 93 91 93 92 93 94 96
90 90 86 87 84 86 93 93 92 92 92 93 90 95 89 93 88 91 93 92 93 87 94 73 88 89 88 85 93 89 89 91 86 91 89 86 87 88 87 88 87 87 88 87
87 88 83 86 83 84 91 91 90 90 90 91 88 91 85 90 86 88 91 90 90 85 92 72 85 88 86 84 90 86 87 100 85 88 88 84 85 86 85 85 84 85 85 84 91
92 92 87 90 87 88 95 95 93 93 95 94 91 95 88 92 88 90 94 94 95 90 95 75 89 92 91 87 94 91 92 89 89 92 92 88 90 90 91 90 90 90 91 90 91 89
90 90 86 89 85 88 95 95 94 94 94 94 90 96 88 93 88 92 94 93 94 88 95 74 90 91 90 88 94 90 89 93 88 91 90 88 88 88 88 88 87 88 89 87 96 93 93
90 91 87 91 86 87 94 94 95 95 93 94 89 94 87 91 88 90 94 93 93 87 94 77 88 90 89 85 92 89 89 91 89 91 90 87 88 88 88 88 87 88 89 87 93 91 93 93
89 89 83 85 84 87 89 89 88 88 90 89 92 91 94 94 95 94 90 91 92 87 92 72 85 89 86 84 92 87 93 87 86 90 89 85 88 89 89 90 86 87 88 87 90 87 90 89 89
91 92 86 89 86 88 95 95 94 94 94 95 91 96 88 94 88 92 95 94 95 89 97 74 90 92 90 89 95 91 91 93 88 92 91 87 89 90 89 88 88 89 90 89 95 93 93 97 92 90
92 92 88 92 88 89 97 97 96 96 98 97 91 97 88 93 88 91 96 96 97 91 96 76 90 93 91 88 94 91 91 91 91 94 94 89 90 92 92 91 90 91 92 91 93 91 95 95 95 91 95
89 90 85 86 82 85 91 91 89 89 89 89 95 93 92 97 92 96 90 91 93 90 93 74 85 89 88 86 97 91 91 88 87 90 89 90 92 90 92 93 90 90 90 89 91 88 91 91 90 92 92 91
89 90 83 85 83 86 89 89 89 89 89 89 93 92 95 96 95 95 90 91 92 88 92 73 85 89 86 83 94 88 92 87 87 90 90 87 90 91 90 92 88 89 89 88 91 87 90 89 89 97 90 91 95
92 93 85 88 88 90 93 93 92 92 94 93 91 93 89 92 89 90 94 95 93 88 93 74 89 93 88 87 92 89 96 88 88 92 91 86 87 89 90 88 88 88 89 88 90 88 93 91 91 91 93 94 90 90
88 89 84 84 83 86 90 90 89 89 89 90 93 92 94 96 96 97 90 91 91 86 92 72 85 89 88 86 94 88 91 88 86 89 88 86 88 88 88 90 87 88 88 87 91 88 90 91 89 96 91 90 95 96 90
86 86 80 82 80 83 85 85 85 85 85 85 91 88 93 93 92 92 86 87 88 84 88 71 82 85 83 80 90 85 90 83 84 87 86 83 86 87 86 88 85 85 85 84 87 83 86 85 85 93 86 86 91 97 87 92
83 84 81 82 81 84 89 89 88 88 89 89 81 88 78 84 78 82 88 88 86 81 88 68 89 85 83 83 86 83 82 84 81 82 83 77 79 81 80 79 78 80 80 79 84 84 85 88 86 82 88 89 81 80 86 81 76
92 92 89 90 88 88 97 97 95 95 95 96 90 96 87 93 87 91 95 94 95 89 96 76 91 93 91 87 94 91 91 91 89 92 92 87 88 89 89 88 88 89 89 88 93 91 94 95 94 90 95 96 90 90 93 90 86 90
92 91 89 90 88 88 96 96 94 94 94 95 91 95 89 93 89 91 95 94 95 90 95 75 90 92 91 87 94 91 92 90 89 94 92 88 89 91 91 89 89 90 90 89 92 90 93 93 92 91 94 95 91 91 93 91 87 86 97
94 94 90 92 91 90 97 97 96 96 97 98 92 96 90 94 90 92 96 97 96 90 97 76 91 94 92 89 95 92 94 92 90 94 94 89 90 91 91 90 89 91 91 90 93 92 95 95 94 92 95 97 92 92 95 92 88 89 98 98
93 92 89 90 89 89 96 96 95 95 95 96 91 95 89 93 88 90 95 95 95 89 95 74 90 92 91 88 94 91 92 90 89 92 92 87 88 90 89 88 87 89 89 88 92 90 94 94 93 90 94 96 90 90 94 90 87 89 96 96 98
93 93 89 91 91 89 95 95 94 94 95 95 90 93 88 91 88 89 95 95 95 90 94 75 88 93 91 85 92 90 92 88 90 93 92 87 89 90 91 89 89 90 91 90 91 88 95 92 93 91 92 96 90 90 93 89 86 85 95 94 97 95
91 90 86 88 87 88 95 95 95 95 95 96 87 94 85 91 85 88 94 94 92 87 94 73 92 91 89 88 92 89 89 89 87 89 90 85 86 87 87 85 84 86 87 86 90 89 91 94 92 88 93 95 88 87 92 88 83 93 95 93 95 95 92
89 89 91 88 85 85 94 94 92 92 93 93 89 93 87 91 87 90 92 93 94 87 93 75 88 90 90 86 92 90 89 89 87 89 89 86 87 87 87 87 87 87 88 87 91 89 97 92 92 88 93 94 90 88 90 89 85 85 93 92 94 92 92 90
93 93 87 90 89 91 96 96 95 95 96 97 89 95 87 92 87 90 96 96 94 89 95 75 92 93 90 88 93 90 92 90 89 92 92 86 88 89 90 88 87 89 89 89 91 90 94 94 93 90 94 96 89 89 94 89 85 94 96 95 97 96 96 97 91
93 92 85 89 87 90 94 94 93 93 94 94 91 94 90 92 90 91 94 95 93 88 94 73 92 93 88 89 92 89 94 91 88 91 92 86 88 88 89 88 87 87 89 88 91 91 92 93 92 92 93 94 90 91 93 91 88 89 94 94 95 94 92 95 91 95
70
75
80
85
90
95
100
Figure 2: Similarity of MTEB datasets. We use the best model on MTEB STS (ST5-XXL, see Table 1) to embed
100 samples for each dataset. Cosine similarities between the averaged embeddings are computed and visualized.
Retrieval Each dataset consists of a corpus,
queries and a mapping for each query to relevant
documents from the corpus. The aim is to ﬁnd these
relevant documents. The provided model is used
to embed all queries and all corpus documents and
similarity scores are computed using cosine simi-
larity. After ranking the corpus documents for each
query based on the scores, nDCG@k, MRR@k,
MAP@k, precision@k and recall@k are computed
for several values of k. nDCG@10 serves as the
main metric. MTEB reuses datasets and evaluation
from BEIR (Thakur et al., 2021).
Semantic Textual Similarity (STS) Given a
sentence pair the aim is to determine their simi-
larity. Labels are continuous scores with higher
numbers indicating more similar sentences. The
provided model is used to embed the sentences and
their similarity is computed using various distance
metrics. Distances are benchmarked with ground
truth similarities using Pearson and Spearman cor-
relations. Spearman correlation based on cosine
similarity serves as the main metric (Reimers et al.,
2016).
Summarization A set of human-written and
machine-generated summaries are provided. The
aim is to score the machine summaries. The pro-
vided model is ﬁrst used to embed all summaries.
For each machine summary embedding, distances
to all human summary embeddings are computed.
The closest score (e.g. highest cosine similarity)
is kept and used as the model’s score of a single
machine-generated summary. Pearson and Spear-
man correlations with ground truth human assess-
ments of the machine-generated summaries are
computed. Like for STS, Spearman correlation
based on cosine similarity serves as the main met-
ric (Reimers et al., 2016).
3.3 Datasets
To further the diversity of MTEB, datasets of vary-
ing text lengths are included. All datasets are
grouped into three categories:
Sentence to sentence (S2S) A sentence is com-
pared with another sentence. An example of S2S
are all current STS tasks in MTEB, where the simi-
larity between two sentences is assessed.



### Claim 4/38

#### Claim Text
In recent years, continual progresses have been achieved in this field, such as Contriever [22], E5 [53], GTR [40], and OpenAI Text Embedding [37].

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 0):

Text Embeddings by Weakly-Supervised
Contrastive Pre-training
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao
Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei
Microsoft Corporation
https://github.com/microsoft/unilm
Abstract
This paper presents E5 1, a family of state-of-the-art text embeddings that transfer
well to a wide range of tasks. The model is trained in a contrastive manner with
weak supervision signals from our curated large-scale text pair dataset (called
CCPairs). E5 can be readily used as a general-purpose embedding model for any
tasks requiring a single-vector representation of texts such as retrieval, clustering,
and classification, achieving strong performance in both zero-shot and fine-tuned
settings. We conduct extensive evaluations on 56 datasets from the BEIR and
MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms
the strong BM25 baseline on the BEIR retrieval benchmark without using any
labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark,
beating existing embedding models with 40× more parameters.
1 Introduction
Text embeddings are low-dimensional vector representations for arbitrary-length texts and play key
roles in many NLP tasks such as large-scale retrieval. Compared to the high-dimensional and sparse
representations like TF-IDF, text embeddings have the potential to overcome the lexical mismatch
issue and facilitate efficient retrieval and matching between texts. It also offers a versatile interface
easily consumable by downstream applications.
While pre-trained language models such as BERT [ 17] and GPT [ 7] can produce transferrable
text representations, they are not ideal for tasks such as retrieval and text matching where a single-
vector embedding of texts is more desired due to its efficiency and versatility. To obtain better
text embeddings, contrastive learning is often the go-to framework to enhance the sequence-level
representations from text pairs. Along this line of research, some works are geared towards learning
task-specific embeddings. For example, GTR [ 43] and Sentence-T5 [ 44] fine-tune pre-trained
models with supervised datasets to learn embeddings customized for passage retrieval and semantic
textual similarity, respectively. Other works learn unsupervised embeddings from automatically
constructed text pairs. Typical methods to construct text pairs include Inverse Close Task (ICT)
[9], random cropping [ 28] and neighboring text spans [ 41], etc. While such synthetic data are
of unlimited quantity, they are often poor in quality and the resulted embeddings fail to match the
performance of the classic BM25 baseline without further fine-tuning [40].
In this work, we learn a high-quality general-purpose text embedding termed E5, EmbEddings from
bidirEctional Encoder rEpresentations. E5 aims to provide strong off-the-shelf text embeddings
suitable for any tasks requiring single-vector representations in both zero-shot or fine-tuned settings.
To achieve this goal, instead of relying on limited labeled data or low-quality synthetic text pairs, we
contrastively train E5 embeddings from CCPairs, a curated web-scale text pair dataset containing
1E5: EmbEddings from bidirEctional Encoder rEpresentations
Work in progress.
arXiv:2212.03533v2  [cs.CL]  22 Feb 2024



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 7):

Table 5: Impacts of different batch sizes for contrastive pre-training.
batch size NFCorpus NQ FiQA Quora DBPedia Scifact Avg
32k 35.8 39.0 40.0 85.7 35.4 73.7 51.6
8k 33.3 38.5 37.6 85.7 34.0 71.8 50.2
1k 28.2 33.1 30.4 84.0 30.1 69.1 45.8
Table 5, increasing batch size from 1K to 32K leads to consistent gains across all 6 datasets. It is also
possible to train with smaller batch sizes by adding hard negatives [ 50]. However, the engineering
efforts of mining hard negatives for large datasets (>100M) are non-trivial.
Table 6: Fine-tuning with different combinations of labeled data.
Fine-tuned on Retrieval STS Classification Summ. MTEB Avg
No fine-tuning 42.9 69.5 67.9 31.1 55.6
MS-MARCO + NQ 50.3 78.3 68.3 30.6 59.0
NLI 38.3 81.1 72.6 31.6 57.3
All above 48.7 81.0 73.1 31.0 60.4
Fine-tuning Datasets GTR models are fine-tuned with “MS-MARCO + NQ”, while Sentence-T5
models use NLI instead. In Table 6, we can see that the “MS-MARCO + NQ” setting performs best
on retrieval tasks, and the NLI data is beneficial for STS and linear probing classification. Similar
observations are also made by Muennighoff et al. [40]. Combining all of them leads to the best
overall scores on the MTEB benchmark. This also illustrates the importance of dataset diversity for
learning text embeddings.
Table 7: Data filtering. For the top 2 rows, we train with 1M random text pairs.
# of pairs NFCorpus NQ FiQA Quora DBPedia Scifact Avg
1M w/o filter 23.0 15.1 18.5 83.1 18.2 51.4 34.9
w/ filter 26.8 22.7 24.5 85.0 27.5 57.5 40.7
All w/o filter 34.5 35.4 39.1 85.7 32.9 72.5 50.0
w/ filter 35.8 39.0 40.0 85.7 35.4 73.7 51.6
Data Filtering One crucial step in our dataset curation pipeline is filtering out low-quality text pairs.
In Table 7, when training with 1M pairs, using filtered data has a nearly 6 points advantage. When
all the text pairs are used, the “w/o filter” setting has about 4× more data but is still behind by 1.6
points. Though recent studies [ 29, 47] show that deep learning models are quite robust to dataset
noises, data filtering still has benefits in improving training efficiency and model quality.
Negative Sampling We explore two alternative methods to enlarge the number of negatives: Pre-
batch negatives [ 33] reuse embeddings from previous batches as additional negatives, while MoCo
[25] introduces a momentum encoder and uses a FIFO queue to store negatives. For both approaches,
the negative size can be easily scaled up without incurring much GPU memory overhead. The
downside is that most negatives are produced by an older version of model parameters. In Table 8,
in-batch negatives still perform favorably. Empirically, we find that MoCo is more sensitive to certain
hyperparameters such as temperature, better results are possible with more tuning.
BM25 vs Dense Retrieval With the rapid development of dense retrieval models, can we replace
the long-standing BM25 algorithm from now on? The answer is likely “not yet”. BM25 still holds
obvious advantages in terms of simplicity, efficiency, and interpretability. For long-tail domains such
as Trec-Covid [ 55] and retrieval tasks that involve long documents (Touche-2020) [4] or rely heavily
on exact lexical match (Fever) [ 54], further research efforts are still necessary to improve current
dense retrievers.
6 Conclusion
In this work, we train a general-purpose text embedding model E5 from weak supervision signals. We
adopt a simple contrastive training framework with in-batch negatives and learn from a large-scale text
pair dataset we harvest from heterogeneous data sources across the web. E5 offers strong off-the-shelf
8



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 16):

Table 13: Results for each dataset in the MTEB benchmark [ 40]. The numbers for the Retrieval
category are not included here since the datasets are the same as the BEIR benchmark.
unsupervised supervised
E5-PTsmall E5-PTbase E5-PTlarge E5small E5base E5large
AmazonCounterfactualClassification 71.7 73.6 70.4 76.2 79.7 77.7
AmazonPolarityClassification 76.1 77.0 83.2 87.5 88.0 90.1
AmazonReviewsClassification 35.0 35.8 37.4 42.6 42.7 43.0
Banking77Classification 82.1 82.9 83.5 81.9 83.3 84.1
EmotionClassification 42.2 44.2 43.5 46.9 49.4 48.1
ImdbClassification 67.9 67.3 77.7 75.6 76.0 82.1
MassiveIntentClassification 70.2 71.1 70.8 72.2 72.3 73.2
MassiveScenarioClassification 74.6 75.4 75.9 75.8 76.8 77.4
MTOPDomainClassification 91.3 92.3 93.2 92.1 93.2 93.9
MTOPIntentClassification 71.9 74.0 74.2 73.2 74.8 76.4
ToxicConversationsClassification 67.0 67.4 66.1 72.8 74.1 70.6
TweetSentimentExtractionClass. 54.4 53.3 52.5 63.3 61.4 61.2
ArxivClusteringP2P 47.9 49.3 49.4 44.1 44.6 46.2
ArxivClusteringS2S 39.9 42.8 43.6 37.1 40.5 41.4
BiorxivClusteringP2P 38.5 38.8 39.2 35.8 36.2 37.6
BiorxivClusteringS2S 35.4 36.5 36.7 31.9 32.7 35.1
MedrxivClusteringP2P 34.4 33.7 33.3 31.3 31.5 32.3
MedrxivClusteringS2S 32.0 32.1 32.2 28.2 28.3 29.7
RedditClustering 46.9 49.3 52.4 42.9 48.2 50.7
RedditClusteringP2P 60.2 64.4 64.6 56.4 62.2 61.4
StackExchangeClustering 57.7 60.2 63.3 59.1 63.9 65.0
StackExchangeClusteringP2P 32.0 34.0 34.7 30.3 32.6 33.6
TwentyNewsgroupsClustering 34.4 36.2 37.9 37.5 42.6 43.8
SprintDuplicateQuestions 91.6 90.8 92.0 95.3 94.9 95.4
TwitterSemEval2015 60.0 62.8 64.7 74.2 74.4 76.1
TwitterURLCorpus 83.2 84.0 84.1 85.8 86.0 86.3
AskUbuntuDupQuestions 57.8 57.6 58.3 59.4 59.7 60.1
MindSmallReranking 29.0 29.6 29.2 29.6 30.1 30.8
SciDocsRR 81.1 82.6 84.3 79.8 82.9 83.9
StackOverflowDupQuestions 44.4 44.2 45.8 49.1 50.1 51.3
BIOSSES 69.2 71.9 69.7 84.2 85.1 84.7
SICK-R 66.6 68.7 69.7 78.9 79.7 80.5
STS12 60.7 57.9 54.7 75.2 74.2 75.9
STS13 71.1 73.5 74.0 81.8 83.3 85.2
STS14 64.2 64.0 65.3 78.5 78.5 80.5
STS15 74.3 75.4 75.8 87.5 88.4 88.8
STS16 76.6 79.8 80.1 84.6 84.2 85.3
STS17 78.3 77.2 76.0 87.9 87.2 89.4
STS22 59.2 56.2 62.8 63.8 62.9 63.0
STSBenchmark 67.7 70.5 70.9 86.4 86.2 87.2
SummEval 32.7 31.1 32.6 31.4 31.0 31.0
17



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 4):

Zero-shot Text Classification The input and label texts are converted to sentences based on manually
written prompt templates. The predicted label is the one closest to the input text in the embedding
space. Take the sentiment classification of movie reviews as an example, with the original input “I
enjoy watching it”, the label text is “it is an example of terrible/great movie review” and the input text
becomes “movie review: I enjoy watching it”.
Semantic Textual Similarity Given two text embeddings, we use the cosine function to measure
their semantic similarity. Since the absolute similarity scores do not enable an easy interpretation, the
evaluation is usually based on rank correlation coefficients.
Text Clustering Standard clustering algorithms such as k-means can be applied straightforwardly.
Texts belonging to the same category are expected to be close in the embedding space.
For tasks other than zero-shot text classification and retrieval, we use the query embeddings by
default.
5 Experiments
5.1 Pre-training and Fine-tuning Configurations
Pre-training We pre-train on our proposed text pair dataset for three model sizes: E5small, E5base
and E5large initialized from MiniLM [ 59], bert-base-uncased, and bert-large-uncased-whole-word-
masking respectively. The batch size is set to a large value of 32, 768 to increase the number of
negatives. The learning rate is {3, 2, 1}×10−4 for the {small, base, large} models, with linear decay
and the first 1, 000 steps for warmup. We pre-train for 20k steps in total with AdamW optimizer,
which is approximately 2.5 epochs over the dataset. It takes {16, 32, 64} V100 GPUs and {1, 1, 2}
days for the {small, base, large} models. To improve training efficiency and reduce GPU memory
usage, we adopt mixed precision training and gradient checkpointing.
Fine-tuning is performed on the concatenation of 3 datasets: MS-MARCO passage ranking [ 8],
NQ [ 32, 30], and NLI [ 22] datasets. We reuse the mined hard negatives and re-ranker scores from
SimLM [ 58] for the first two datasets. Models are fine-tuned for 3 epochs with batch size 256 on 8
GPUs. Learning rate is {3, 2, 1}×10−5 for the {small, base, large} models with 400 steps warmup.
For each example, we use 7 hard negatives. Since the NLI dataset only has 1 hard negative for each
example, 6 sentences are randomly sampled from the entire corpus.
We use E5-PT to denote models with contrastive pre-training only. More implementation details can
be found in Appendix B.
5.2 Evaluation Datasets
BEIR Benchmark [ 53] is a collection of 19 information retrieval datasets, ranging across ad-hoc
web search, question answering, fact verification and duplicate question retrieval, etc. We evaluate
the 15 datasets that provide public downloads. The main metric is nDCG@10.
MTEB Benchmark [ 40] is recently proposed for benchmarking massive text embedding tasks.
Though MTEB is multilingual due to the inclusion of bitext mining datasets, most datasets are
still only available in English. In this paper, we evaluate the English subsets, which have 56
datasets spanning across 6 categories: Classification (Class.), Clustering (Clust.), Pair Classification
(PairClass.), Rerank, Retrieval (Retr.), STS, and Summarization (Summ.). The evaluation metrics are
accuracy, v-measure, average precision, MAP, nDCG@10, and Spearman coefficients, respectively.
Please refer to the MTEB paper for details.
5.3 Results on BEIR benchmark
Results with Unsupervised Methods In Table 1, we show model results that do not use any labeled
data. When averaged over all 15 datasets, E5-PTbase outperforms the classic BM25 algorithm by 1.2
points. To the best of our knowledge, this is the first reported result that an unsupervised model can
beat BM25 on the BEIR benchmark. When scaling up to E5-PT large, we see further benefits from
42.9 to 44.2.
5



Source: data\tc18_2309.07597v5\referenced_papers\[28]_2308.03281.pdf (Page 6):

Trec-COVIDNFCorpus
Tóuche-2020
DBPediaScidocs
Climate-fever
HotpotQA
FiQA
CQADupStackMSMARCO
NQ FeverScifactArguAna
Quora
Avg.0
50
100Recall@100
SimCSE Contriever BM25 GTE
Figure 2: Recall@100 of unsupervised text retrieval methods on BEIR benchmark (Thakur et al., 2021). We
compare our model GTEbase (based on BERTbase) without using any annotated data to SimCSE (Gao et al., 2021)
(based on RoBERTalarge), Contriever (Izacard et al., 2022a) (based on BERTbase) and BM25. Baseline results are
borrowed from the Contriever paper (Izacard et al., 2022a) with dot product being the similarity function.
Dataset BM25 SimCSE Contriever CPT-S E5 small E5base E5large GTEsmall GTEbase GTElarge
MS MARCO 22.8 9.4 20.6 19.9 25.4 26.0 26.2 31.3 31.8 31.7
Trec-Covid 65.6 26.2 27.4 52.9 52.0 61.0 61.8 61.8 64.0 64.8
NFCorpus 32.5 9.9 31.7 32.0 29.3 35.8 33.7 34.9 36.2 38.1
NQ 32.9 11.7 25.4 - 37.3 39.0 41.7 32.0 35.3 34.5
HotpotQA 60.3 19.8 48.1 51.5 46.0 52.4 52.2 49.3 50.8 49.2
FiQA 23.6 9.8 24.5 34.1 38.3 40.0 43.2 37.0 36.9 40.6
ArguAna 31.5 38.3 37.9 38.7 42.5 42.2 44.4 41.6 41.0 41.3
Touche-2020 36.7 8.9 19.3 21.0 19.9 16.9 19.8 17.7 18.2 18.5
CQADupStack 29.9 13.2 28.4 - 35.0 35.4 38.9 38.1 39.9 39.8
Quora 78.9 78.0 83.5 68.1 85.8 85.7 86.1 86.1 85.0 84.8
DBPedia 31.3 15.0 29.2 27.2 34.5 35.4 37.1 33.5 33.2 33.6
Scidocs 15.8 5.5 14.9 - 19.9 21.1 21.8 21.5 22.5 22.7
Fever 75.3 21.1 68.2 57.1 62.5 63.4 68.6 71.3 72.7 70.5
Climate-Fever 21.3 11.8 15.5 15.8 14.5 15.4 15.7 21.4 21.0 25.4
Scifact 66.5 25.7 64.9 65.4 68.5 73.7 72.3 72.7 74.1 74.1
Average 41.7 20.3 36.0 - 40.8 42.9 44.2 43.4 44.2 44.6
Table 5: nDCG@10 of different unsupervised methods on the BEIR benchmark (Thakur et al., 2021). SimCSE is
based on BERTbase backbone. CPT-S (Neelakantan et al., 2022) is of similar size to BERTlarge. Baseline results are
borrowed from E5 paper (Wang et al., 2022b). Note that Contriever uses dot product as the similarity metric while
other models uses cosine similarity.
on the tasks covered in the MTEB benchmark,
please refer to the Appendix B.
Two settings are considered for comparison: the
unsupervised setting and the supervised setting. In
the unsupervised setting, models are trained us-
ing unlabeled data, while supervised models are
fine-tuned using high-quality datasets with human
labels. The results of strong baseline models are
presented in Table 6.
In the unsupervised setting, our model outper-
forms the previous best model, E5, by a signifi-
cant margin across all considered tasks, without
the use of task-specific prompts. This improve-
ment can be attributed to the inclusion of more
training data formats and various sources of self-
supervision signals. Furthermore, it is worth noting
that our unsupervised pre-trained model narrows
the gap even further with larger supervised base-
lines, such as GTR and Sentence-T5. In the super-
vised setting, our model surpasses OpenAI results



#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[37]_2201.10005.pdf (Page 0):

Text and Code Embeddings by Contrastive Pre-Training
Arvind Neelakantan* 1 Tao Xu* 1 Raul Puri1 Alec Radford1 Jesse Michael Han1 Jerry Tworek1
Qiming Yuan1 Nikolas Tezak1 Jong Wook Kim1 Chris Hallacy1 Johannes Heidecke1 Pranav Shyam1
Boris Power1 Tyna Eloundou Nekoul1 Girish Sastry1 Gretchen Krueger1 David Schnurr1
Felipe Petroski Such1 Kenny Hsu1 Madeleine Thompson1 Tabarak Khan1 Toki Sherbakov1 Joanne Jang1
Peter Welinder1 Lilian Weng1
Abstract
Text embeddings are useful features in many
applications such as semantic search and com-
puting text similarity. Previous work typically
trains models customized for different use cases,
varying in dataset choice, training objective and
model architecture. In this work, we show that
contrastive pre-training on unsupervised data at
scale leads to high quality vector representations
of text and code. The same unsupervised text em-
beddings that achieve new state-of-the-art results
in linear-probe classiﬁcation also display impres-
sive semantic search capabilities and sometimes
even perform competitively with ﬁne-tuned mod-
els. On linear-probe classiﬁcation accuracy aver-
aging over 7 tasks, our best unsupervised model
achieves a relative improvement of 4% and 1.8%
over previous best unsupervised and supervised
text embedding models respectively. The same
text embeddings when evaluated on large-scale
semantic search attains a relative improvement
of 23.4%, 14.7%, and 10.6% over previous best
unsupervised methods on MSMARCO, Natural
Questions and TriviaQA benchmarks, respec-
tively. Similarly to text embeddings, we train
code embedding models on (text, code) pairs, ob-
taining a 20.8% relative improvement over prior
best work on code search.
1. Introduction
Deep unsupervised learning with generative and embed-
ding models has seen dramatic success in the past few
years. Generative models (Peters et al., 2018; Raffel et al.,
2019; van den Oord et al., 2016; Ramesh et al., 2021;
Brown et al., 2020; Chen et al., 2021) are trained to max-
*Equal contribution 1OpenAI. Correspondence to: Arvind
Neelakantan <arvind@openai.com>.
S-300M M-1.2B L-6B XL-175B
Model Size
60
62
64
66
68
70Performance
Average performance vs model size
Figure 1.Average performance of unsupervised cpt-text
models of different sizes across 22 tasks consisting of linear-probe
classiﬁcation, text search, and sentence similarity tasks.
imize the likelihood of observed data while embedding
models are trained to distinguish observed data from noise
(Sohn, 2016; van den Oord et al., 2018; Radford et al.,
2021; Jia et al., 2021; Gao et al., 2021; Izacard et al., 2021).
Generative models have been shown to produce realistic
content and beneﬁt many downstream applications, reduc-
ing the need for labeled training datasets. In generative
models, the information about the input is typically dis-
tributed over multiple hidden states of the model. While
some generative models (Kingma & Welling, 2014; Kiros
et al., 2015) can learn a single representation of the in-
put, most autoregressive Transformer (Vaswani et al., 2017)
models do not (Raffel et al., 2019; Brown et al., 2020; Chen
et al., 2021; Ramesh et al., 2021). However, learning such a
representation (or embedding) is necessary for many tasks.
Systems that search over millions or billions of items re-
quire each entry to be embedded as a dense representation
and build an index in advance to save computational costs
at query time. These embeddings are useful features for
classiﬁcation tasks and can also enable data visualization
applications via techniques such as clustering. Embedding
models are explicitly optimized to learn a low dimensional
representation that captures the semantic meaning of the
input (Radford et al., 2021; Jia et al., 2021; Giorgi et al.,
2020; Gao et al., 2021; Izacard et al., 2021).
arXiv:2201.10005v1  [cs.CL]  24 Jan 2022



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 8):

Table 8: Comparison of different negative sampling strategies.
# negatives NFCorpus NQ FiQA Quora DBPedia Scifact Avg
In batch 32k 35.8 39.0 40.0 85.7 35.4 73.7 51.6
+ pre-batch 64k 29.4 27.2 29.4 84.6 25.0 64.3 43.3
MoCo 130k 29.7 36.1 32.0 81.6 29.9 63.6 45.5
performance for a wide range of tasks requiring single-vector text representations such as retrieval,
semantic textual similarity, and text matching. When further customized for downstream tasks, E5
achieves superior fine-tuned performance compared to existing embedding models with 40× more
parameters on the large, 56-task MTEB benchmark datasets.
References
[1] Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence
embeddings. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net, 2017. URL
https://openreview.net/forum?id=SyK00v5xx.
[2] Mikel Artetxe and Holger Schwenk. Massively multilingual sentence embeddings for zero-
shot cross-lingual transfer and beyond. Transactions of the Association for Computational
Linguistics, 7:597–610, 2019. doi: 10.1162/tacl_a_00288. URL https://aclanthology.
org/Q19-1038.
[3] David M. Blei, Andrew Y . Ng, and Michael I. Jordan. Latent dirichlet allocation. In
Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani, editors, Advances in Neural
Information Processing Systems 14 [Neural Information Processing Systems: Natural and
Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada] , pages
601–608. MIT Press, 2001. URLhttps://proceedings.neurips.cc/paper/2001/hash/
296472c9542ad4d4788d543508116cbc-Abstract.html.
[4] Alexander Bondarenko, Maik Fröbe, Johannes Kiesel, Shahbaz Syed, Timon Gurcke, Meriem
Beloucif, Alexander Panchenko, Chris Biemann, Benno Stein, Henning Wachsmuth, et al.
Overview of touché 2022: argument retrieval. In International Conference of the Cross-
Language Evaluation Forum for European Languages, pages 311–336. Springer, 2022.
[5] Vera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. A full-text learning to rank
dataset for medical information retrieval. In European Conference on Information Retrieval,
pages 716–722. Springer, 2016.
[6] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. InProceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing , pages 632–642, Lisbon, Portugal,
2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https:
//aclanthology.org/D15-1075.
[7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learn-
ers. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-
12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.
[8] Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh
Tiwary, Rangan Majumder, Li Deng, and Bhaskar Mitra. Ms marco: A human generated
machine reading comprehension dataset. ArXiv, abs/1611.09268, 2016.
9



Source: data\tc18_2309.07597v5\referenced_papers\[28]_2308.03281.pdf (Page 0):

Towards General Text Embeddings with Multi-stage Contrastive Learning
Zehan Li1, Xin Zhang1, Yanzhao Zhang1, Dingkun Long1, Pengjun Xie1, Meishan Zhang
1Alibaba Group
{lizehan.lzh,linzhang.zx,zhangyanzhao.zyz,
dingkun.ldk,pengjun.xpj}@alibaba-inc.com
Abstract
We present GTE, a general-purpose text embed-
ding model trained with multi-stage contrastive
learning. In line with recent advancements in
unifying various NLP tasks into a single for-
mat, we train a unified text embedding model
by employing contrastive learning over a di-
verse mixture of datasets from multiple sources.
By significantly increasing the number of train-
ing data during both unsupervised pre-training
and supervised fine-tuning stages, we achieve
substantial performance gains over existing em-
bedding models. Notably, even with a relatively
modest parameter count of 110M, GTEbase out-
performs the black-box embedding API pro-
vided by OpenAI and even surpasses 10x larger
text embedding models on the massive text
embedding benchmark. Furthermore, without
additional fine-tuning on each programming
language individually, our model outperforms
previous best code retrievers of similar size by
treating code as text. In summary, our model
achieves impressive results by effectively har-
nessing multi-stage contrastive learning, offer-
ing a powerful and efficient text embedding
model with broad applicability across various
NLP and code-related tasks.1
1 Introduction
Text embeddings have became an indispensable
component in many natural language processing
tasks, such as text classification, text retrieval, ques-
tion answering and dialogue systems (Karpukhin
et al., 2020; Humeau et al., 2020; Choi et al., 2021;
Izacard et al., 2022a; Long et al., 2022a; Rajapakse,
2023). These embedding models represent texts us-
ing low-dimensional vectors and capture their sim-
ilarity through vector operations. The emergence
of recent large language models (LLMs) (Radford
et al., 2018; Touvron et al., 2023; OpenAI, 2023)
has generated considerable interest in retrieval-
1The GTE model is publicly available at https://
huggingface.co/thenlper/gte-large
UnsupervisedContrastive Pre-training on Massive Text Pairs mined from the WebSupervisedContrastive Fine-tuningon Annotated Text Triples from Multiple Tasks
… …
MSMARCO
NaturalQuestionsTriviaQAWebQuestionsHotpotQAMNLI
QuoraStackExchangeDupWebSearch
OpenQA NaturalLanguageInferenceSNLI
FactVerificationFEVER
Paraphrase
MEDIOthersBERRI
Figure 1: Illustration of the multi-stage contrastive learn-
ing pipeline used to train our text embedding model.
augmented systems based on text embedding mod-
els that integrate the reasoning and comprehension
capabilities of LLMs (Izacard et al., 2022b; Ram
et al., 2023; Shi et al., 2023). Consequently, there
has been a growing focus on general text represen-
tation in both industry and academia.
The pursuit of developing a unified model to ad-
dress a multitude of downstream tasks has been
long-standing due to the diverse formats, domains
and downstream applications of natural language.
The emergence of pre-trained language models has
further opened up possibilities for training such a
universal model. Nonetheless, within the realm
of text representation research, previous text em-
bedding models have primarily focused on specific
tasks, and their training strategies or models, tai-
lored to a single task, may not perform optimally
in other contexts. For example, the text represen-
tation model SimCSE (Gao et al., 2021), trained
on symmetric text pairs, demonstrates limitations
in text retrieval tasks. Similarly, certain text rep-
resentation models specifically designed for dense
retrieval tasks do not exhibit robust performance
in sentence textual similarity tasks. Recently, there
has been a shift in research focus towards develop-
ing more comprehensive models for text represen-
tation leveraging large quantities of unlabeled web
data through unsupervised contrastive pre-training,
coupled with task-specific data, prompts, or in-
structions to mitigate task conflicts during fine-
tuning (Ni et al., 2022a,b; Neelakantan et al., 2022;
arXiv:2308.03281v1  [cs.CL]  7 Aug 2023



Source: data\tc18_2309.07597v5\referenced_papers\[28]_2308.03281.pdf (Page 1):

Wang et al., 2022b; Su et al., 2023). Additionally,
the introduction of benchmarks, such as the Mas-
sive Text Embedding Benchmark (MTEB) (Muen-
nighoff et al., 2023), has established a robust basis
for assessing the universality of text representation
models. However, a significant limitation in ex-
isting research is the reliance on in-house data for
pre-training, creating a bottleneck in the utilization
of pre-trained model weights or APIs. Furthermore,
the formulation of prompts specifically tailored for
each task requires extra human effort during imple-
mentation (Su et al., 2023).
This work presents a straightforward approach
to construct a general text embedding (GTE) model
solely using contrastive learning on open-source
data, as illustrated in Figure 1. Specifically, we
first gather a large-scale dataset comprising un-
supervised text pairs extracted from various data
sources for contrastive pre-training. Surprisingly,
our model, pre-trained on this dataset, exhibits re-
markable performance, surpassing BM25 and E5
model (Wang et al., 2022b) in zero-shot text re-
trieval tasks and surpassing many supervised mod-
els in the MTEB benchmark. To further enhance
the quality of the learned text representations, we
obtain high-quality text pairs with human labels
from multiple sources for contrastive fine-tuning.
After supervised fine-tuning, our 110M BERT-
based (Devlin et al., 2019) model already outper-
forms the current commercial embedding API of
OpenAI and ranks highly in the MTEB benchmark.
Furthermore, since our model is trained using code
data as well, we evaluate its code search capabili-
ties on the CodeSearchNet benchmark, which en-
compasses six programming languages. Notably,
even without language-specific fine-tuning on each
subset, our model significantly outperforms state-
of-the-art code retrievers of similar size that have
been fine-tuned for each programming language.
In the rest of this paper, we provide a detailed
account of the data sources and training configu-
rations employed. Subsequently, we present the
evaluation results on widely recognized text em-
bedding benchmarks and compare them with the
performance of previous state-of-the-art baselines
that were specifically optimized for each individual
task. Our model consistently demonstrates supe-
rior performance or, at the very least, comparable
results to those achieved by larger models, owing
to its incorporation of a more diverse mixture of
training datasets. We aspire for our model to serve
as a robust baseline for the research community
investigating text and code embedding.
2 Related Work
Text embeddings serve as low-dimensional vector
representations for texts of varying lengths and are
essential in numerous natural language processing
(NLP) tasks. In contrast to high-dimensional and
sparse representations such as TF-IDF, dense text
embeddings possess the capacity to address the lex-
ical mismatch problem and enhance the efficiency
of text retrieval and matching.
Pre-trained language models, exemplified by
BERT (Devlin et al., 2019) and GPT (Radford
et al., 2018), have demonstrated remarkable suc-
cess across various NLP tasks. Nonetheless, ex-
tracting a high-quality sentence embedding from
pre-trained language models poses a significant
challenge due to the presence of anisotropic em-
bedding spaces resulting from the masked language
modeling objective. To address this issue, subse-
quent studies have proposed different approaches,
including supervised fine-tuning (Reimers and
Gurevych, 2019), normalizing flow (Li et al., 2020),
normalizing flow (Li et al., 2020), whitening (Su
et al., 2021), or unsupervised contrastive learn-
ing (Gao et al., 2021). These investigations pri-
marily concentrate on enhancing performance in
semantic textual similarity tasks, wherein two sen-
tences exhibit similar formats.
Another line of research focuses on the text re-
trieval problem, where the query and document
typically exhibit an asymmetric relationship. In
this context, the dual-encoder architecture necessi-
tates training with both positive and negative pairs.
Lee et al. (2019) propose the Inverse Close Task
(ICT) as a self-supervised pre-training approach for
generating a dense retriever. The ICT method in-
volves cropping a random sentence from a passage
to construct pseudo query-document pairs. Ad-
ditionally, Chang et al. (2020) leverage the link
structure within Wikipedia to introduce further su-
pervision signals in the pre-training data. In a sim-
ilar vein, REALM (Guu et al., 2020) proposes a
joint training approach, wherein a dense retriever
and a language model are trained concurrently. The
learning signal for the language model is derived
from masked language modeling, with backpropa-
gation incorporated through the retrieval step. Re-
cent advancements, such as Contriever (Izacard
et al., 2022a) and coCondenser (Gao and Callan,



Source: data\tc18_2309.07597v5\referenced_papers\[37]_2201.10005.pdf (Page 11):

Text and Code Embeddings by Contrastive Pre-Training
Rudinger, R., Naradowsky, J., Leonard, B., and Durme,
B. V . Gender bias in coreference resolution. arXiv
preprint arXiv:1804.09301, 2018.
Sachan, D. S., Patwary, M., Shoeybi, M., Kant, N., Ping,
W., Hamilton, W. L., and Catanzaro, B. End-to-end
training of neural retrievers for open-domain question
answering. In Zong, C., Xia, F., Li, W., and Navigli,
R. (eds.), Proceedings of ACL/IJCNLP, pp. 6648–6662.
ACL, 2021.
Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C.,
and Zaharia, M. Colbertv2: Effective and efﬁcient re-
trieval via lightweight late interaction. arXiv preprint
arXiv:2112.01488, 2021.
Schroff, F., Kalenichenko, D., and Philbin, J. Facenet: A
uniﬁed embedding for face recognition and clustering.
In Computer Vision and Pattern Recognition (CVPR) ,
2015.
Shen, D., Zheng, M., Shen, Y ., Qu, Y ., and Chen, W. A
simple but tough-to-beat data augmentation approach for
natural language understanding and generation. arXiv
preprint arXiv:2009.13818, 2020.
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning,
C. D., Ng, A., and Potts, C. Recursive deep models for
semantic compositionality over a sentiment treebank. In
Conference on Empirical Methods in Natural Language
Processing (EMNLP), 2013.
Sohn, K. Improved deep metric learning with multi-class
n-pair loss objective. In Advances in Neural Information
Processing Systems (NeuriPS), 2016.
Solaiman, I. and Dennison, C. Process for adapting lan-
guage models to society (PALMS) with values-targeted
datasets. arXiv preprint arXiv:2106.10328, 2021.
Su, J., Cao, J., Liu, W., and Ou, Y . Whitening sentence
representations for better semantics and faster retrieval.
arXiv preprint arXiv:2103.15316, 2021.
Sun, C., Myers, A., V ondrick, C., Murphy, K., and Schmid,
C. Videobert: A joint model for video and language
representation learning. In International Conference on
Computer Vision (ICCV), 2019.
Thakur, N., Reimers, N., R ¨uckl´e, A., Srivastava, A., and
Gurevych, I. BEIR: A heterogenous benchmark for
zero-shot evaluation of information retrieval models.
In Advances in Neural Information Processing Systems
(NeuriPS), 2021.
Tian, Y ., Krishnan, D., and Isola, P. Contrastive multi-
view coding. European Conference on Computer Vision
(ECCV), 2019.
van den Oord, A., Dieleman, S., Zen, H., Simonyan, K.,
Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A.,
and Kavukcuoglu, K. Wavenet: A generative model for
raw audio. arXiv preprint arXiv:1609.03499, 2016.
van den Oord, A., Li, Y ., and Vinyals, O. Representa-
tion learning with contrastive predictive coding. arXiv
preprint arXiv:1807.03748, 2018.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Atten-
tion is all you need. In Advances in Neural Information
Processing Systems (NeuriPS), 2017.
Vervaeke, J., Lillicrap, T. P., and Richards, B. A. Relevance
realization and the emerging framework in cognitive sci-
ence. Journal of logic and computation , 22(1):79–99,
2012.
Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., and Zhou,
M. Minilm: Deep self-attention distillation for task-
agnostic compression of pre-trained transformers. arXiv
preprint arXiv:2002.10957, 2020.
Wei, J. W. and Zou, K. EDA: easy data augmentation tech-
niques for boosting performance on text classiﬁcation
tasks. arXiv preprint arXiv:1901.11196, 2019.
Williams, A., Nangia, N., and Bowman, S. A broad-
coverage challenge corpus for sentence understanding
through inference. In Conference of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL). ACL, 2018.
Wu, Z., Xiong, Y ., Yu, S. X., and Lin, D. Unsupervised
feature learning via non-parametric instance-level dis-
crimination. In Computer Vision and Pattern Recogni-
tion (CVPR), 2018.
Xiong, L., Xiong, C., Li, Y ., Tang, K., Liu, J., Bennett,
P. N., Ahmed, J., and Overwijk, A. Approximate near-
est neighbor negative contrastive learning for dense text
retrieval. arXiv preprint arXiv:2007.00808, 2020.
Yih, W.-t., Toutanova, K., Platt, J. C., and Meek, C. Learn-
ing discriminative projections for text similarity mea-
sures. In Conference on Computational Natural Lan-
guage Learning (CoNLL). ACL, 2011.
Zbontar, J., Jing, L., Misra, I., LeCun, Y ., and Deny, S. Bar-
low twins: Self-supervised learning via redundancy re-
duction. In International Conference on Machine Learn-
ing (ICML), 2021.
Zhang, Y ., He, R., Liu, Z., Lim, K. H., and Bing, L.
An unsupervised sentence embedding method by mutual
information maximization. In Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP),
2020.



#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 7):

173 176 173 172
ArguAna
176 179
173
182
BioASQ
206
228
244
234
Climate-Fever
57
60
62 62
DBPedia-entity
88
97
104 105
Fever
136
156
151 154
FiQA-2018
53
56
61
63
HotpotQA
49
50 50 50
MS Marco
236
243 245 243
NFCorpus
75
78
80 81
NQ
8
9 9 9
Quora
410
453
525
490
Robust04
166
170 169 168
SCIDOCS
211
216
221 219
SciFact
10 10 10 10
Signal-1M
BaseLarge XL XXL
15
10
12
9
Trec-Covid
BaseLarge XL XXL
678
707 700 708
Trec-News
BaseLarge XL XXL
32 32
41
62
Touché-2020
BaseLarge XL XXL
111
116 117 116
CQADupStack
Figure 5: Median lengths (in words) of top-10 retrieved
documents for all queries.
Touche2020 are longer.
On the other hand, the only exception we ob-
serve is the Trec-Covid dataset, where GTR-XXL
model retrieves much shorter documents than those
retrieved by the smaller size counterparts. This may
explain the inferior performance of GTR-XXL on
Trec-Covid shown in table 3 and table 8. We leave
it as future work to explore the effects of using the
dot-product as similarity function for large dual
encoders.
7 Related Work
Neural information retrieval. Document re-
trieval is an important task in the NLP and informa-
tion retrieval (IR) communities. The goal is to ﬁnd
the relevant document from a large corpus given a
query. Traditionally, lexical based approaches try-
ing to match the query and document based on term
overlap, such as TF-IDF and BM25 (Robertson
and Zaragoza, 2009), have achieved great success
in this task. Recently, neural based approaches,
which go beyond the simple term matching, are be-
ing quickly adopted by the community and achieve
state-of-the-art performance on multiple retrieval
tasks, such as passage retrieval (Karpukhin et al.,
2020), question answering (Ahmad et al., 2019),
conversational question answering (Qu et al., 2020)
and bitext retrieval (Feng et al., 2020).
Dual encoders for neural retrieval. Dual en-
coders have demonstrated to be one type of neural
retrievers that can achieve great performance com-
pared to traditional sparse models such as BM25
for a wide range of retrieval tasks (Karpukhin et al.,
2020; Gillick et al., 2018). One key aspect to their
success is the adoption of pre-trained language
models, which enables the dual encoders to have
backbone contextual embeddings to initialize from.
Other techniques such as negative mining (Xiong
et al., 2020; Lu et al., 2021; Sachan et al., 2021)
and large training batch sizes (Qu et al., 2021) have
also shown great effectiveness. However, few of
the previous works have discussed the effect of the
backbone model’s capacity.
Zero-shot neural retrieval. Recent works have
shown great improvement under the zero-shot set-
ting for dual encoders by leveraging distillation
and synthetic data generation (Thakur et al., 2021;
Hofstätter et al., 2021; Ma et al., 2020). Both of
these techniques, and scaling up backbone mod-
els, are effective ways to close the gap between
dual encoders and the upper bound of the single-
product approaches with ﬁxed-dimension embed-
dings. On the other hand, multi-vector approaches
introduce more interactions between dense embed-
dings, which could also beneﬁt from scaling up the
backbone multi-vector encoders. We hope that our
observation about scaling up model sizes for single
dot-product based methods can be combined with
these techniques and further push the frontier of
neural retrieval models.
8 Inference latency
One caveat for scaling up model size is the incre-
ment in the latency overhead. We investigate the
inference speed in terms of microseconds (ms) for
all GTR models with batch size 1 and input length
128. We found the latency increases from 17 ms,
34 ms, 96 ms to 349 ms. The GTR-Base model
has close latency compared to TAS-B while the
largest GTR-XXL model has a similar latency to
the re-ranking models (Thakur et al., 2021). With
the recent work towards making large models efﬁ-
cient from angles such as sparsity, distillation and
prompt-tuning, we hope the inference time for large
dual encoders can be signiﬁcantly reduced in the
future.
9 Conclusion
This paper presents the Generalizable T5 Retriever
(GTR), a scaled-up dual encoder model with a
ﬁxed-size bottleneck layer. We show that scal-
ing up the model size brings signiﬁcant improve-
ment on retrieval performance across the board on
the BEIR zero-shot retrieval benchmark, especially



Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 6):

GTR-FT GTR-PT GTR
Fine-tuning   
NDCG@10 on MS Marco
Base 0.400 0.258 0.420
Large 0.415 0.262 0.430
XL 0.418 0.259 0.439
XXL 0.422 0.252 0.442
Zero-shot average NDCG@10 w/o MS Marco
Base 0.387 0.295 0.416
Large 0.412 0.315 0.445
XL 0.433 0.315 0.453
XXL 0.430 0.332 0.458
Table 5: Comparisons (NDCG@10) of the models
trained with and without pre-training and ﬁne-tuning.
Notably, the GTR-FT XL model already achieves an
average zero-shot NDCG@10 of 0.433, which outper-
forms the previous best dual encoder model TAS-B
(NDCG@10=0.415).
6.1 Effect of scaling up for different training
stages
The ﬁrst ablation study aims to investigate how
scaling up effects dual encoder pre-training and
ﬁne-tuning. Results are listed in table 5.
For ﬁne-tuning only models, scaling up beneﬁts
both in-domain and out-of-domain performance.
For pre-training only models, the improvement on
in-domain performance is not obvious; meanwhile
for out-of-domain tasks, scaling up also improves
the generalization. Finally with both pre-training
and ﬁne-tuning, GTR models consistently improve
over GTR-FT models of all sizes. This shows the
power of combining scaling up and a generic pre-
training stage.
6.2 Importance of the ﬁne-tuning dataset
In table 5, we compare GTR and GTR-PT on the
BEIR benchmark to understand the importance of
ﬁne-tuning on MS Marco. The table shows that
there is a clear gap between GTR models before
and after ﬁne-tuning. The result shows the neces-
sity of leveraging a high quality dataset (e.g. search
data) to ﬁne-tune the dual encoders.
In table 6, we compare ﬁne-tuning GTR on NQ
instead of MS Marco. Compared to MS Marco,
NQ only covers Wikipedia documents and is much
smaller in size, which allows us to investigate the
performance of GTR when ﬁne-tuned on a less
generalizable dataset. In addition, ﬁne-tuning on
NQ can give us a fair comparison with DPR.
As shown in table 6, the GTR-base model ﬁne-
Model Fine-tuning dataset Zero-shot aver-
age NDCG@10
DPR NQ 0.237
GTR-Base NQ 0.360
GTR-Large NQ 0.379
GTR-XL NQ 0.407
GTR-Large MS Marco 0.445
GTR-XL MS Marco 0.453
Table 6: Comparisons of GTR models ﬁne-tuned on
MS Marco and NQ. We report the zero-shot average
NDCG@10. Scaling up improves model performance
both on NQ and MS Marco.
tuned on NQ outperforms the original DPR model,
which uses a BERT-Base model as the encoder
backbone. This demonstrates the effectiveness of
our pre-training on the Web dataset as well as the
hard negatives introduced from Lu et al. (2021)
for NQ. Fine-tuning on NQ leads to inferior per-
formance compared to ﬁne-tuning on MS Marco,
which is consistent with prior work (Thakur et al.,
2021). However, importantly, scaling up GTR size
improves zero-shot performance on BEIR when
ﬁne-tuning on NQ. This shows that the beneﬁt of
scaling up holds for different ﬁne-tuning datasets.
Furthermore, when scaling from Large to XL, we
observe a larger gain when ﬁne-tuning with NQ
than with MS Marco, indicating that scaling up
helps more when using weaker ﬁne-tuning data.
6.3 Document length vs model capacity
Previously, BEIR has shown that models trained
with cosine similarity prefer short documents while
those trained with dot-product prefer long docu-
ments (Thakur et al., 2021). We investigate whether
scaling up affect this observation. Speciﬁcally, we
compute the median lengths (in words) of the top-
10 retrieved documents for all queries. Results are
shown in ﬁg. 5.
Though all GTR models are trained using co-
sine similarity, we found that scaling up the model
size has inﬂuence over the lengths of retrieved
documents. We observe an increasing trend of
document length for DB-Pedia, Fever, HotpotQA,
Signal-1M, Trec-News, and Web-Touche2020 with
scaling up. In particular, for Web-Touche2020, the
lengths of the retrieved documents grow drastically
as the models scale up: The largest GTR-XXL
retrieves documents that are on average twice as
long compared with the smallest GTR-Base. This
plays in our favor since Thakur et al. (2021) show
that the majority of relevant documents in Web-



Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 10):

Model NDCG@10 MRR@10 Recall@1000
ANCE 0.388 0.330 0.959
TAS-Balanced 0.408 0.340 0.975
ColBERT 0.401 0.360 0.968
RocketQA / 0.370 0.979
GTR-Base 0.420 0.366 0.983
GTR-Large 0.430 0.379 0.991
GTR-XL 0.439 0.385 0.989
GTR-XXL 0.442 0.388 0.990
Table 7: Comparisons of different models on MS
Marco. Scaling up can improve GTR models’ in-
domain performance.
A More results
A.1 Comparisons on MS Marco
Table 7 shows the comparisons of GTR models and
the baselines. Note that the best RocketQA model
used additional augmented data other than MS
Marco to improve the model performance while
all others do not. Our best GTR-XXL models out-
performs RocketQA on both MRR and recall.
A.2 Comparison of different dual encoder
pre-training strategies
In a concurrent work (Anonymous, 2022), re-
searchers proposed to conduct contrastive learning
(CL) pre-training for improving the generalizability
of neural retrievers. The paired data for contrastive
training is constructed from C4 and Wiki dataset
in an unsupervised way. In particular, they con-
struct pairs by randomly choosing two spans from
a single document and conduct word deletion or
replacement to each span. We compare the perfor-
mance of our GTR models to their models to gain
insights into different pretraining strategies for dual
encoders.
As shown in Figure 6, on over half of the
datasets, models with our pre-training approach
under-perform CL-Pretrain with the base size;
while as the model size increases, GTR-Large
and -XXL models show signiﬁcant gains over CL-
Pretrain. The best GTR-XXL model achieves
0.49 for NDCG@10 on average while CL-Pretrain
achieves 0.46. This demonstrates that scaling up
can mitigate the disadvantage of the potentially
inferior pre-training approach. Note that our pre-
training is additive to CL-Pretrain and we can lever-
age the pre-training on C4 and Wiki to further im-
prove the results. We leave this exploration as
future work.
0.66
0.54
0.56
0.58
0.5
Trec-Covid
0.32
0.31
0.33
0.340.34
NFCorpus
0.33
0.5
0.550.560.57
NQ
0.6
0.54
0.58
0.590.6
HotpotQA
0.24
0.35
0.420.44
0.47
FiQA-2018
0.32
0.510.520.530.54
ArguAna
0.37
0.2 0.220.23
0.26
Touché-2020
0.79
0.880.890.890.89
Quora
0.31
0.35
0.390.4
0.41
DBPedia-entity
0.16
0.15
0.160.160.16
SCIDOCS
CL
BaseLarge
XLXXL
0.75
0.66
0.710.72
0.74
Fever
CL
BaseLarge
XLXXL
0.21
0.24
0.26
0.270.27
Climate-Fever
CL
BaseLarge
XLXXL
0.66
0.6
0.640.64
0.66
SciFact
CL
BaseLarge
XLXXL
0.3
0.36
0.380.390.4
CQADupStack
CL
BaseLarge
XLXXL
0.410.42
0.440.450.46
Avg
Figure 6: Comparison with Anonymous (2022) on
NDCG@10. “CL” denotes Anonymous (2022) with
contrastive learning on C4 and Wiki while others de-
note our GTR models with different sizes. Note that
they only report results on 15 datasets of the BEIR
benchmark.
A.3 Recall on BEIR
Table 8 presents the Recall@100 of GTR mod-
els and the baselines. Similar to NDCG@10, we
observe that scaling up dual encoders lead to sig-
niﬁcant gains on the BEIR benchmark in terms of
recall.



Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 3):

The training process includes a pre-training stage
on a web-mined corpus and a ﬁne-tuning stage on
search datasets. The web-mined corpus provides a
large amount of semi-structured data pairs (such as
question-answer pairs and conversations), which
can provide rich semantic relevance information. It
is easy to collect but it is often not well annotated,
if at all. The search datasets are often annotated by
humans, and the queries and documents are also
authored by humans. These datasets are of high
quality but costly to collect.
In this work, for dual encoder pre-training, we
initialize the dual encoders from the T5 models
and train on question-answer pairs collected from
the Web. Recently, Sentence-T5 (Ni et al., 2021)
explored different ways to extract strong text em-
beddings and achieved remarkable performance
on SentEval and Sentence Textual Similarity tasks.
We follow that setting to encode queries and pas-
sages via mean pooling from the T5 encoders and
focus on the dense retrieval tasks.
For ﬁne-tuning, our aim is to adapt the model to
retrieval using a high quality search corpus so the
model can learn to better match generic queries to
documents. In this paper, we consider two datasets
for ﬁne-tuning: MS Marco (Nguyen et al., 2016)
and Natural Questions (Kwiatkowski et al., 2019).
4 Experimental setup
4.1 Training Data
Community QA. In order to leverage most of
the power from the large scale models, we col-
lect input-response pairs and question-answer pairs
from online forums and QA websites including
Reddit, Stack-Overﬂow, etc. This results in 2 bil-
lion question-answer pairs that we use to pre-train
the dual encoder.
MS Marco. We consider the MS Marco
dataset (Nguyen et al., 2016), which includes 532K
query and document pairs, as search data for ﬁne-
tuning. The dataset is sampled from Bing search
logs, which covers a broad range of domains
and concepts. Most of the neural models com-
pared in (Thakur et al., 2021) are trained on MS
Marco, including DeepCT (Dai and Callan, 2020),
DocT5Query (Nogueira, 2019), ANCE (Xiong
et al., 2020) and ColBERT (Khattab and Zaharia,
2020). Some of these models have shown great
generalization with comparable or even better per-
formance relative to BM25.
GTR Models Base Large XL XXL
# of params 110M 335M 1.24B 4.8B
Table 1: Number of parameters in the GTR models.
Models Dim. size
ColBERT 128
DPR, ANCE, TAS-B, GenQ, GTR 768
BM25, DocT5Query -
Table 2: Dimension size of different models. Most dual
encoder models set the embedding dimension size to
768.
Natural Questions. In the ﬁne-tuning stage,
we also consider the Natural Questions dataset
(Kwiatkowski et al., 2019) , which has been widely
used in the dense retrieval literature (Karpukhin
et al., 2020; Xiong et al., 2020). It consists of 130k
query and passage pairs which are also human-
annotated.
4.2 Conﬁgurations
We implement GTR models in JAX4 and train them
on Cloud TPU-V8. We consider different sizes of
the T5 transformer (Vaswani et al., 2017) architec-
ture including Base, Large, XL and XXL. Their
number of parameters are listed in table 1.
Note that we only use the encoder portion of the
T5 models and thus the number of parameters are
less than half of the full model size. We use the off-
the-shelf checkpoints as the initial parameters and
use the same sentencepiece vocabulary model.5
During pre-training and ﬁne-tuning, we set the
batch size to 2048 and use a softmax temperature
τ of 0.01. We use Adafactor optimizer (Shazeer
and Stern, 2018) and set the initial learning rate to
1e-3 with a linear decay. We train the model for
800K steps and 20K steps for the pre-training and
ﬁne-tuning stages, respectively.
For ﬁne-tuning, we use the hard negatives re-
leased by RocketQA (Qu et al., 2021) when ﬁne-
tuning with MS Marco data and the hard negatives
release by (Lu et al., 2021) for Natural Questions,
which were proven to lead to better retriever perfor-
mance. By default, we use the complete MS Marco
dataset and the NQ dataset for ﬁne-tuning.
When evaluating on the BEIR benchmark, we
use sequences of 64 tokens for the questions and
512 for the documents in all datasets except Trec-
4https://github.com/google/jax
5https://github.com/google-research/
text-to-text-transfer-transformer



Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 5):

0.420.430.440.44
0.23
MS MARCO
0.540.56
0.58
0.5
0.66
Trec-Covid
0.27
0.320.320.32
0.46
BioASQ
0.31
0.33
0.340.34
0.32
NFCorpus
0.5
0.550.560.57
0.33
NQ
0.54
0.580.590.6 0.6
HotpotQA
0.35
0.420.44
0.47
0.24
FiQA-2018
0.260.260.270.27
0.33
Signal-1M
0.340.340.350.35
0.4
Trec-News
0.44
0.470.48
0.51
0.41
Robust04
0.510.520.530.54
0.32
ArguAna
0.2 0.220.23
0.26
0.37
Touché-2020
0.880.890.890.89
0.79
Quora
0.35
0.390.4 0.41
0.31
DBPedia-entity
0.15
0.160.160.160.16
SCIDOCS
BaseLarge
XLXXLBM25
0.66
0.710.72
0.740.75
Fever
BaseLarge
XLXXLBM25
0.24
0.260.270.27
0.21
Climate-Fever
BaseLarge
XLXXLBM25
0.6
0.640.64
0.660.66
SciFact
BaseLarge
XLXXLBM25
0.36
0.380.390.4
0.3
CQADupStack
BaseLarge
XLXXLBM25
0.42
0.440.450.46
0.41
Avg
Figure 4: Comparison with BM25 on NDCG@10. The
GTR-Base model outperforms BM25 on 9 datasets and
the larger GTR models continue to improve on these 9
tasks. The GTR-XXL model catches up or surpasses
BM25 on the other 5 datasets and only under-performs
on 5 of the remaining tasks.
model already outperforms the previous best dense
retrieval model TAS-B as well as the best sparse
model DocT5Query. Scaling up to GTR-XXL
leads to another jump in retrieval performance.
Similar improvements are found on Recall@100
as shown in the Appendix’s table 8. On average,
the scaling up process demonstrates an encourag-
ing ascending trend that eventually outperforms all
baseline methods on all evaluation metrics. This
conﬁrms that scaling up is a valid path towards
generalizability.
Previously, dual encoders failed to match the
performance of BM25 for tasks that require better
lexical matching capabilities. Thus, we wanted to
investigate what kind of tasks can get improved
by scaling up the model size. Figure 4 presents a
detailed comparison of all sizes of GTR models
against the BM25 baseline.
For tasks like NQ where dual encoders have been
previously shown to be more effective than BM25,
increasing the model size continues to advance the
performance of dual encoders. This suggests scal-
ing up can further boost the head start of dense
models over sparse models on these datasets.
For tasks like BioASQ and NFCorpus, where
dual encoders previously struggled to match the
performance of BM25 for inherent reasons, we dis-
covered that scaling up consistently improves the
retrieval performance. In particular, for NFCor-
pus, our Base model under-performs BM25 but the
GTR-FT GTR
Ratio of data Large XL Large XL XXL
NDCG@10 on MS Marco
10% 0.402 0.397 0.428 0.426 -
100% 0.415 0.418 0.430 0.439 0.430
Zero-shot average NDCG@10 w/o MS Marco
10% 0.413 0.418 0.452 0.462 0.465
100% 0.412 0.433 0.445 0.453 0.458
Table 4: Comparisons of NDCG@10 for GTR models
trained with different amount of ﬁne-tuning data. With
only 10% of the MS Marco data, both GTR-FT and
GTR large and XL models achieve slightly worse in-
domain performance; meanwhile they obtain compara-
ble or even superior out-of-domain performance than
using the complete MS Marco data.
XL model outperforms BM25 by 5.5% (0.343 vs.
0.325). This exciting ﬁnding veriﬁes our assump-
tion that scaling up can further exploit the powerful
semantic matching capabilities of the dual encoder
models and enable them to ultimately outperform
BM25.
5.3 Data efﬁciency for large retrievers
To better understand the data efﬁciency for large
dual encoders, we trained models using different
portions of the MS Marco dataset during ﬁne-
tuning. In particular, we sampled a subset of the
training data by keeping only 10% of the training
queries as well as their relevant (positive) passages
and irrelevant (hard negative) passages.
As shown in table 4, using 10% of training data
reduces the in-domain performance of the GTR
models on MS Marco. For the GTR-FT (ﬁne-
tuning only) models, using 10% of the data leads
to a mixed result of out-of-domain performance.
On the other hand, for full GTR models, using
10% of the MS Marco dataset is sufﬁcient for ﬁne-
tuning. In particular, the GTR-Large, XL and XXL
models achieve comparable or even better OOD
performance than ﬁne-tuning on the complete MS
Marco dataset. This might suggest that GTR mod-
els have the beneﬁt of data efﬁciency and could use
less training data for domain adaptation.
6 Ablation Study and Analysis
In this section we present ablations and analysis to
further understand the effects of scaling up, the im-
pact of ﬁne-tuning and pre-training, and the trends
of the GTR model on different experimental condi-
tions.



#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[22]_2112.09118.pdf (Page 5):

Published in Transactions on Machine Learning Research (08/2022)
Negative pairs across batches.An alternative approach is to store representations from previous batches
in a queue and use them as negative examples in the loss (Wu et al., 2018). This allows for smaller batch
size but slightly changes the loss by making it asymmetric between “queries” (one of the view generated
from the elements of the current batch), and “keys” (the elements stored in the queue). Gradient is only
backpropagated through the “queries”, and the representation of the “keys” are considered as ﬁxed. In
practice, the features stored in the queue from previous batches comes form previous iterations of the network.
This leads to a drop of performance when the network rapidly changes during training. Instead, He et al.
(2020) proposed to generate representations of keys from a second network that is updated more slowly. This
approach, called MoCo, considers two networks: one for the keys, parametrized byθk, and one of the query,
parametrized byθq. The parameters of the query network are updated with backpropagation and stochastic
gradient descent, similarly to when using in-batch negatives, while the parameters of the key network, or
Momentum encoder, is updated from the parameters of the query network by using a exponential moving
average:
θk ← mθk + (1−m)θq, (2)
where m is the momentum parameter that takes its value in[0,1].
4 Experiments
In this section, we empirically evaluate our best retriever trained with contrastive learning, calledContriever
(contrastive retriever), which uses MoCo with random cropping. We use a contrastive learning procedure that
diﬀers from ICT (Lee et al., 2019) mainly in three aspects. First, positive pairs are sampled using random
cropping and tokens from each element of the pair are deleted with a probability of 10%. Second we use
MoCo where negatives consists of elements from previous batches stored in a queue. This allows to scale to a
large number of negatives. Third we use data from Wikipedia and CCNet (Wenzek et al., 2020) for training.
Ablation studies motivating these technical choices are performed in Section 6. More technical details about
our model are given in Appendix A.1.
4.1 Datasets
Contriever is trained with contrastive learning on documents sampled from a mix between Wikipedia data
and CCNet data (Wenzek et al., 2020), where half the batches are sampled from each source.
First, we evaluate our model on two question answering datasets: NaturalQuestions (Kwiatkowski et al.,
2019) and TriviaQA (Joshi et al., 2017). For both datasets, we use the open domain versions as introduced
by Lee et al. (2019), and the English Wikipedia dump from Dec. 20, 2018 as the collection of documents to
retrieve from. We report the top-k retrieval accuracy,i.e. the number of questions for which at least one of
the top-k passages contain the answer.
Second, we use the BEIR benchmark, introduced by Thakur et al. (2021), which contains 18 retrieval datasets,
corresponding to nine tasks, such as fact checking or citation prediction, and covering diﬀerent domains, such
as Wikipedia or scientiﬁc publications. Most datasets from BEIR do not contain a training set, and the focus
of the benchmark iszero-shot retrieval. However, most machine learning based retrievers are still trained
on supervised data, such as the large scale retrieval dataset MS MARCO (Bajaj et al., 2016). Following
standard practice, we report two metrics on this benchmark: nDCG@10 and Recall@100. The nDCG@10
focuses on the ranking of the top 10 retrieved documents, and is good at evaluating rankings returned to
humans, for example in a search engine. On the other hand, Recall@100 is relevant to evaluate retrievers that
are used in machine learning systems, such as question answering. Indeed, such models can process hundreds
of documents, and ignore their ranking (Izacard & Grave, 2020b). While nDCG@10 is the main metric of
BEIR, we are more interested in the Recall@100 to evaluate bi-encoders, as our goal is to develop retrievers
that can be used in ML systems. Moreover, in many settings, retrieved documents can be re-ranked with a
more powerful model such as a cross-encoder, thus improving the nDCG@10.
6



Source: data\tc18_2309.07597v5\referenced_papers\[22]_2112.09118.pdf (Page 6):

Published in Transactions on Machine Learning Research (08/2022)
Table 1:Unsupervised recall@kon the test sets of NaturalQuestions and TriviaQA. For Inverse Cloze
Task and Masked Salient Spans we report the results of Sachan et al. (2021). The Masked Salient Spans
model uses annotated named entity recognition data. For BM25 we report the results of Ma et al. (2021)
NaturalQuestions TriviaQA
R@5 R@20 R@100 R@5 R@20 R@100
Inverse Cloze Task (Sachan et al., 2021) 32.3 50.9 66.8 40.2 57.5 73.6
Masked salient spans (Sachan et al., 2021) 41.7 59.8 74.9 53.3 68.2 79.4
BM25 (Ma et al., 2021) - 62.9 78.3 - 76.4 83.2
Contriever 47.8 67.8 82.1 59.4 74.2 83.2
supervised model:DPR (Karpukhin et al., 2020) - 78.4 85.4 - 79.4 85.0
supervised model:FiD-KD (Izacard & Grave, 2020a) 73.8 84.3 89.3 77.0 83.6 87.7
4.2 Baselines
First, we compare Contriever to BM25, which does not require supervision. On QA datasets, we compare to
dense retrievers trained with ICT and the Masked Salient Spans from Sachan et al. (2021). On BEIR, we
consider the retriever from REALM (Guu et al., 2020), and RoBERTa large ﬁne-tuned with SimCSE (Gao
et al., 2021), as unsupervised dense retrievers. We also compare to ML-based retrievers trained on MS MARCO,
classiﬁed in three categories: sparse, dense and late-interaction. For sparse methods, we compare toSplade
v2 (Formal et al., 2021), which computes sparse representations of documents with BERT pre-trained
model. For dense methods, we useDPR (Karpukhin et al., 2020) andANCE (Xiong et al., 2020), which
are bi-encoders trained on supervised data such as NaturalQuestions or MS MARCO. We also compare
to TAS-B (Hofstätter et al., 2021), which performs distillation from a cross-encoder to a bi-encoder, and
GenQ, which creates synthetic query-document pairs with a generative model.1 For late-interaction, we use
ColBERT (Khattab et al., 2020), which computes pairwise scores between contextualized representations of
queries and documents, as well as a cross-encoder used to re-rank documents retrieved with BM25.
4.3 Results
First, we compare the performance of fully unsupervised models, i.e., without ﬁne-tuning on MS MARCO or
other annotated data. In Table 1, we report the retrieval performance on two question answering datasets:
NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). Here, our model is competitive
with a strong BM25 baseline (Ma et al., 2021), for example leading to 3 points improvement for the recall@100
on NaturalQuestions. It also outperforms previously proposed dense retrievers which were trained with ICT
or salient span masking. In Figure 1 we report the recall@100 performance of unsupervised models on the
BEIR benchmark. Interestingly, we observe that in this setting, Contriever is competitive compared to BM25
on all datasets, but TREC-COVID and Tóuche-2020. In particular, it obtains better performance than BM25
on 11 out of 15 datasets from the benchmark for the recall@100. Contriever also outperforms previously
proposed unsupervised dense retrievers, which obtains lower performance than BM25 in general. For the
nDCG@10, which puts more emphasis on the very ﬁrst retrieved documents, while Contriever largely closes
the gap between unsupervised retrievers and BM25, it is still outperformed by BM25 as reported in Table 11.
The diﬀerence is mainly due to the fact that BM25 largely outperforms Contriever on two datasets with
speciﬁc features: Trec-COVID and Tóuche-2020. Trec-COVID is an information retrieval dataset related to
COVID. However data used to train Contriever were collected before the COVID outbreak, thus they may
not be adapted. Tóuche-2020 contains long documents, which does not seem to be very well supported by
dense neural retrievers: even after supervised training, models are still lagging behind BM25. Overall, these
results show the potential of contrastive learning to train fully unsupervised dense retrievers.
1GenQ thus leads to one diﬀerent model for each dataset.
7



Source: data\tc18_2309.07597v5\referenced_papers\[22]_2112.09118.pdf (Page 16):

Published in Transactions on Machine Learning Research (08/2022)
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand
Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data.
In Proceedings of the 12th Language Resources and Evaluation Conference, 2020. 6, 9, 17
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric
instance discrimination. InProceedings of the IEEE conference on computer vision and pattern recognition,
pp. 3733–3742, 2018. 2, 4, 6
Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. Clear: Contrastive learning
for sentence representation.arXiv preprint arXiv:2012.15466, 2020. 3
Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold
Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval.arXiv
preprint arXiv:2007.00808, 2020. 3, 4, 7
Sohee Yang and Minjoon Seo. Is retriever merely an approximator of reader?arXiv preprint arXiv:2010.10999,
2020. 3
Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. Mr. tydi: A multi-lingual benchmark for dense
retrieval. CoRR, abs/2108.08787, 2021. URLhttps://arxiv.org/abs/2108.08787. 9, 10
A Technical details for Contriever
A.1 Contrastive pre-training
For the model with ﬁne-tuning on MS MARCO, we use the MoCo algorithm He et al. (2020) with a queue of
size 131,072, a momentum value of 0.9995 and a temperature of 0.05. We use the random cropping data
augmentation, with documents of 256 tokens and span sizes sampled between 5% and 50% of the document
length. Documents are simply random piece of text sampled from a mix between Wikipedia and CCNet
data (Wenzek et al., 2020), where half the batches are sampled from each source. We also apply token
deletion with a probability of 10%. We optimize the model with the AdamW (Loshchilov & Hutter, 2019)
optimizer, with learning rate of5 ·10−5, batch size of 2,048 and 500,000 steps. We initialize the network with
the publicly available BERT base uncased model.
A.2 Fine-tuning on MS MARCO
For the ﬁne-tuning on MS MARCO we do not use the MoCo algorithm and simply use in-batch negatives.
We use the ASAM optimizer (Kwon et al., 2021), with a learning rate of10−5 and a batch size of 1024 with a
temperature of0.05, also used during pre-training. We train an initial model with random negative examples
for 20000 steps, mine hard negatives with this ﬁrst model, and re-train a second model with those. Each
query is associated with a gold document and a negative document, which is a random document in the ﬁrst
phase and a hard negative 10% of the time in the second phase. For each query, all documents from the
current batch aside of the gold document are used as negatives.
A.3 Few-shot training
For the few-shot evaluation presented in Table 3, we train for 500 epochs on each dataset with a batch size of
256 with in-batch random negatives. We evaluate performance performance on the development set every 100
gradient updates and perform early stopping based on this metric. For SciFact, we hold out randomly 10% of
the training data and use them as development set, leading to a train set containing 729 samples.
17



Source: data\tc18_2309.07597v5\referenced_papers\[22]_2112.09118.pdf (Page 17):

Published in Transactions on Machine Learning Research (08/2022)
Table 10: BEIR Benchmark. We report the recall@100 on the test sets from the BEIR benchmark for
bi-encoder methods. We report the capped recall@100 on Trec-COVID following the original BEIR setup.
Note that using a cross-encoder to re-rank the top-100 documents do not change the recall@100, hence, we do
not include these methods in this table. We also report the average and number of datasets where a method
is the best (“Best on”) over the entire BEIR benchmark (excluding three datasets because of their licence).
Bold is the best overall. On Trec-COVID we report the capped Recall@100, see Thakur et al. (2021) for
more details. MS MARCO is excluded from the average.
BM25 DPR ANCE TAS-B Gen-Q ColBERT Splade v2 Ours
MS MARCO 65.8 55.2 85.2 88.4 88.4 86.5 - 89.1
Trec-COVID 49.8 21.2 45.7 38.7 45.6 46.4 12.3 40.7
NFCorpus 25.0 20.8 23.2 28.0 28.0 25.4 27.7 30.0
NQ 76.0 88.0 83.6 90.3 86.2 91.2 93.0 92.5
HotpotQA 74.0 59.1 57.8 72.8 67.3 74.8 82.0 77.7
FiQA 53.9 34.2 58.1 59.3 61.8 60.3 62.1 65.6
ArguAna 94.2 75.1 93.7 94.2 97.8 91.4 97.2 97.7
Touche-2020 53.8 30.1 45.8 43.1 45.1 43.9 35.4 29.4
CQADupStack 60.6 40.3 57.9 62.2 65.4 62.4 - 66.3
Quora 97.3 47.0 98.7 98.6 98.8 98.9 98.7 99.3
DBPedia 39.8 34.9 31.9 49.9 43.3 46.1 57.5 54.1
Scidocs 35.6 21.9 26.9 33.5 33.2 34.4 36.4 37.8
Fever 93.1 84.0 90.0 93.7 92.8 93.4 95.1 94.9
Climate-fever 43.6 39.0 44.5 53.4 45.0 44.4 52.4 57.4
Scifact 90.8 72.7 81.6 89.1 89.3 87.8 92.0 94.7
Avg. w/o CQA 63.6 48.3 60.1 65.0 64.2 64.5 64.8 67.1
Avg. 63.4 47.7 60.0 64.8 64.2 64.3 - 67.0
Best on 2 0 0 0 1 0 4 7
B Multilingual retrieval with mContriever
B.1 Hyperparameters for multilingual contrastive pre-training
The pre-trained mContriever model is pre-trained for 500,000 steps with a queue of size 32768, and temperature
of 0.05 and a momentum value of 0.999. We optimize the model with the AdamW (Loshchilov & Hutter,
2019) optimizer, with learning rate of5 ·10−5. The learning rate follows a linear warmup for 20,000 steps
followed by linear decay until the end of training. Languages used for pre-training are detailed in Table 12.
B.2 Hyperparameters for multilingual ﬁne-tuning
We ﬁne-tune mContriever using in-batch negatives, AdamW optimizer (Loshchilov & Hutter, 2019), a learning
rate of10−5, and a batch size of 1024 samples with a temperatureτof 0.05. On MS MARCO and Mr. TyDi
the model is trained for 20k gradient steps. We notice overﬁtting on NaturalQuestions, and thus reduced the
training to 1k gradient steps. We use a warmup of 1000 gradient steps with linear decay afterwards in all
cases. Hard negatives are mined on Mr. TyDi with the model trained on MS MARCO. We did not observe
signiﬁcant improvements using hard negatives on MS MARCO and NaturalQuestions.
For the ﬁne-tuning on MS MARCO of the models initiliazed from mBERT (resp. XLM-R) without contrastive
pre-training, we use a temperatureτof 1 (resp. 5). We tried temperatures in{10,5,2,1,0.1,0.05}and chose
the one leading to the best performance. We observed a decrease in performance for lower temperatures.
We ﬁne-tuned mContriever withτ= 0.05 following the temperature used during pre-training. We followed
the temperatureτ= 0.05 used for the training of Contriever, and did not test other temperatures for the
contrastive pre-training of the multilingual model, mContriever.
18



Source: data\tc18_2309.07597v5\referenced_papers\[22]_2112.09118.pdf (Page 2):

Published in Transactions on Machine Learning Research (08/2022)
(2019) introduced across-encoder model, based on the BERT model (Devlin et al., 2019), which jointly
encodes queries and documents. The application of a strong pre-trained model, as well as the cross-encoder
architecture, lead to important improvement on the MS MARCO benchmark (Bajaj et al., 2016).
The methods described in the previous paragraph were applied to re-rank documents, which were retrieved
with a traditional IR system such as BM25. Gillick et al. (2018) ﬁrst studied whether continuous retrievers,
based on bi-encoder neural models, could be viable alternative to re-ranking. In the context of question
answering, Karpukhin et al. (2020) introduced a dense passage retriever (DPR) based on the bi-encoder
architecture. This model is initialized with a BERT network, and trained discriminatively using pairs of
queries and relevant documents, with hard negatives from BM25. Xiong et al. (2020) further extended this
work by mining hard negatives with the model itself during optimization, and trained on the MS MARCO
dataset. Once a collection of documents, such as Wikipedia articles, is encoded, retrieval is performed
with a fast k-nearest neighbors library such as FAISS Johnson et al. (2019). To alleviate the limitations of
bi-encoders, Humeau et al. (2019) introduces the poly-encoder architecture, where documents are encoded
by multiple vectors. Similarly, Khattab et al. (2020) proposes the ColBERT model, which keeps a vector
representation for each term of the queries and documents. To make the retrieval tractable, the term-level
function is approximated to ﬁrst retrieve an initial set of candidates, which are then re-ranked with the true
score. In the context of question answering, knowledge distillation has been used to train retrievers, either
using the attention scores of the reader of the downstream task as synthetic labels (Izacard & Grave, 2020a),
or the relevance score from a cross encoder (Yang & Seo, 2020). Luan et al. (2020) compares, theoretically and
empirically, the performance of sparse and dense retrievers, including bi-, cross- and poly-encoders. Dense
retrievers, such as DPR, can lead to indices weighing nearly 100GB when encoding document collections such
as Wikipedia. Izacard et al. (2020) shows how to compress such indices, with limited impact on performance,
making them more practical to use.
Self-supervised learning for NLP. Following the success of word2vec (Mikolov et al., 2013), many
self-supervised techniques have been proposed to learn representation of text. Here, we brieﬂy review the ones
that are most related to our approach: sentence level models and contrastive techniques. Jernite et al. (2017)
introduced diﬀerent objective functions to learn sentence representations, including next sentence prediction
and sentence order prediction. These objectives were later used in pre-trained models based on transformers,
such as BERT (Devlin et al., 2019) and AlBERT (Lan et al., 2019). In the context of retrieval, Lee et al.
(2019) introduced the inverse cloze task (ICT), whose purpose is to predict the context surrounding a span of
text. Guu et al. (2020) integrated a bi-encoder retriever model in a BERT pre-training scheme. The retrieved
documents are used as additional context in the BERT task, and the whole system is trained end-to-end in
an unsupervised way. Similarly, Lewis et al. (2020) proposed to jointly learn a retriever and a generative
seq2seq model, using self-supervised training. Chang et al. (2020) compares diﬀerent pre-training tasks for
retrieval, including the inverse cloze task. In the context of natural language processing, Fang et al. (2020)
proposed to apply MoCo where positive pairs of sentences are obtained using back-translation. Diﬀerent
works augmented the masked language modeling objective with a contrastive loss (Giorgi et al., 2020; Wu
et al., 2020; Meng et al., 2021). SBERT (Reimers & Gurevych, 2019) uses a Siamese network similar to
contrastive learning to learn a BERT-like model that is adapted to matching sentence embeddings. Their
formulation is similar to our work but requires aligned pairs of sentences to form positive pairs while we
propose to use data augmentation to leverage large unaligned textual corpora. Concurrent to this work, Gao
& Callan (2021) have also shown the potential of contrastive learning for information retrieval; building on the
same observation that both tasks share a similar structure. Spider (Ram et al., 2021), a contemporary work,
uses spans appearing multiple times in a document to create pseudo examples for contrastive learning in order
to train unsupervised retrievers. Finally, Chen et al. (2021) train a dense retriever to imitate unsupervised
lexical-based methods. This improves performance on a range of tasks and achieves state-of-the-art results
when combining the resulting dense retriever with Contriever, our model pre-trained with contrastive learning.
3 Method
In this section, we describe how to train a dense retriever with no supervision. We review the model
architecture and then describe contrastive learning — a key component of its training.
3



### Claim 5/38

#### Claim Text
In recent years, there has been a continual effort in this field, where a series of well-known works are proposed, like Contriever [22], GTR [40], sentence-T5 [39], Sentence-Transformer [46], E5 [52], OpenAI text embedding [37], etc.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[39]_2108.08877.pdf (Page 0):

Sentence-T5: Scalable Sentence Encoders
from Pre-trained Text-to-Text Models
Jianmo Ni, Gustavo Hernández Ábrego, Noah Constant, Ji Ma,
Keith B. Hall, Daniel Cer, Yinfei Yang
Google Research
Mountain View, CA
Abstract
We provide the ﬁrst exploration of sen-
tence embeddings from text-to-text transform-
ers (T5). Sentence embeddings are broadly
useful for language processing tasks. While
T5 achieves impressive performance on lan-
guage tasks cast as sequence-to-sequence map-
ping problems, it is unclear how to produce
sentence embeddings from encoder-decoder
models. We investigate three methods for ex-
tracting T5 sentence embeddings: two utilize
only the T5 encoder and one uses the full T5
encoder-decoder model. To support our inves-
tigation, we establish a new sentence represen-
tation transfer benchmark, SentGLUE, which
extends the SentEval toolkit to nine tasks from
the GLUE benchmark (Wang et al., 2018). Our
encoder-only models outperforms Sentence-
BERT (Reimers and Gurevych, 2019) and
SimCSE (Gao et al., 2021) sentence embed-
dings on both SentEval and SentGLUE trans-
fer tasks, including semantic textual similarity
(STS). Scaling up T5 from millions to billions
of parameters is found to produce consistent
further improvements. Finally, our encoder-
decoder method achieves a new state-of-the-
art on STS when using sentence embeddings.1
1 Introduction
Sentence embeddings providing compact mean-
ing representations that are broadly useful for a
variety of language processing tasks include clas-
siﬁcation, question-answering, semantic retrieval,
bitext mining, and semantic similarity tasks. Sen-
tence embedding models have been trained using
a variety of methods including: supervised tasks
such as natural language inference (Conneau et al.,
2017; Gao et al., 2021) or with semi-structured data
such as question-answer pairs (Cer et al., 2018);
translation pairs (Yang et al., 2020a; Feng et al.,
2020); paraphrasing pairs (Wieting et al., 2016)
1Our models are released at https://tfhub.dev/
google/collections/sentence-t5/1.
Figure 1: Scaling up our ST5 model size improves per-
formance on SentEval (left) and STS (right).
Transfer STS
ST5-EncDec (11B params) 90.46 84.94
ST5-Enc (11B params) 91.63 84.96
SimCSE-RoBERTa (large) (Gao et al., 2021)90.232 83.76
SBERT (large) (Reimers and Gurevych, 2019)87.69 76.55
USE (Cer et al., 2018) 85.10 71.22
InferSent (Conneau et al., 2017) 85.59 65.01
Table 1: ST5 versus notable sentence embedding mod-
els on SentEval tasks. The reported numbers are the
average of transfer tasks and STS tasks
and adjacent sentence pairs (Kiros et al., 2015;
Logeswaran and Lee, 2018). Recent work has
shown that scaling up model parameters and lever-
aging pre-trained models (Devlin et al., 2019; Liu
et al., 2019) are two effective approaches to im-
prove performance (Reimers and Gurevych, 2019,
2020; Yang et al., 2020b; Gao et al., 2021).
We explore sentence embeddings from a new
family of pre-trained models: Text-to-Text Trans-
fer Transformer (T5) (Raffel et al., 2020). Unlike
encoder-only models, which use a transformer en-
coder to predict random masked tokens, T5 uses
an encoder-decoder architecture and a generative
span corruption pre-training task. T5 models can
be scaled up to hundreds of billions of parameters
(Fedus et al., 2021) and have achieved state-of-the-
2SimCSE-RoBERTa achieves the best performance on
transfer tasks by adding an additional masked language model
loss during training while ST5 and other models don’t.
arXiv:2108.08877v3  [cs.CL]  14 Dec 2021



Source: data\tc18_2309.07597v5\referenced_papers\[39]_2108.08877.pdf (Page 1):

x1 x2 x3 x4
Encoder Decoder y1 y2 y3 ・
(a) T5 Encoder-Decoder
x1 x2 x3 x4
Encoder (b) ST5 Encoder-only
(ST5-Enc) ﬁrst
x1 x2 x3 x4
Encoder
(c) ST5 Encoder-only
(ST5-Enc) mean
x1 x2 x3 x4
Encoder Decoder (d) ST5 Encoder-Decoder
(ST5-EncDec) ﬁrst
Figure 2: Architecture diagrams for T5 and three ST5 variants to extract sentence representations from T5.
art performance on a broad range of NLP tasks
including GLUE (Wang et al., 2018) and Super-
GLUE (Wang et al., 2019). However, it is difﬁcult
to efﬁciently apply T5 to some tasks such as re-
trieval or clustering. To score retrieval candidates,
T5 would need to perform full inference with cross-
attention on each query-candidate pair. In contrast,
sentence embeddings allow for efﬁcient retrieval
and clustering (Gillick et al., 2018; Reimers and
Gurevych, 2019; Yang et al., 2020a).
As shown in ﬁg. 2, we explore three ways of
turning a pre-trained T5 encoder-decoder model
into a sentence embedding model: (i) using the
ﬁrst token representation of the encoder; (ii) aver-
aging all token representations from the encoder;
(iii) using the ﬁrst token representation from the
decoder. We evaluate the quality of the resulting
sentence embeddings on sentence transfer tasks us-
ing SentEval (Conneau and Kiela, 2018) and on se-
mantic textual similarity (Agirre et al., 2012, 2013,
2014, 2015, 2016; Cer et al., 2017). We contrast
raw representations from pre-trained T5 models
with those learned through ﬁne-tuning on natural
language inference (NLI) and Retrieval Question-
Answering (ReQA) (Ahmad et al., 2019) using dual
encoders and contrastive learning (Conneau et al.,
2017; Cer et al., 2018; Yang et al., 2018; Gao et al.,
2021). We introduce a multi-stage contrastive learn-
ing recipe involving ﬁne-tuning ﬁrst on ReQA and
then on NLI. Finally, we investigate scaling our T5
sentence embedding model up to 11B parameters.
As shown in ﬁg. 1, transfer tasks and STS both
improve with increased model capacity.
To our knowledge, we are the ﬁrst to study us-
ing large-scale pre-trained text-to-text models for
sentence representation learning and to scale sen-
tence embedding models up to 11 billion parame-
ters. We summarize our contributions as follows:
(i) even without ﬁne-tuning, encoder-only ST5
models perform well on sentence transfer tasks,
outperforming state-of-the-art ﬁne-tuned models
such as SimBERT and SimRoBERTa (Gao et al.,
2021); (ii) encoder-decoder sentence embedding
models achieve strong performance on STS, es-
tablishing a new state-of-the-art on sentence em-
bedding based STS; (iii) contrastive learning is
effective for ﬁne-tuning sentence encoders from
T5-style pre-trained models, particularly using our
proposed two-stage contrastive learning approach;
(iv) training ST5 longer and with more data using a
contrastive loss leads to consistent improvement on
both sentence transfer and STS tasks; (v) creating
a new sentence representation transfer benchmark
‘SentGLUE’ which extends the sentence evaluation
toolkit (Conneau and Kiela, 2018) to nine tasks
from GLUE (Wang et al., 2018) benchmark and
evaluating ST5 and other state-of-the-art models on
SentGLUE to compare their transfer performance
on these challenging tasks. We name our model
Sentence T5 (ST5).
2 Text-to-Text Transfer
Transformers (T5)
Text-to-Text transfer transformers (T5) (Raffel
et al., 2020) are gaining popularity due to their
competitive performance and ease of use in solving
a variety of tasks as simple text-to-text mapping
problems. As shown in ﬁg. 2a, T5 consists of an
encoder-decoder transformer model (Vaswani et al.,
2017) pre-trained on an unsupervised span corrup-
tion task. Though T5 has been successfully applied
to numerous NLP tasks, how to extract high quality
text representations from T5 remains unexplored.



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 12):

Adrià Garriga-Alonso, et al. 2022. Beyond the
imitation game: Quantifying and extrapolating the
capabilities of language models. arXiv preprint
arXiv:2206.04615.
Hao Tan and Mohit Bansal. 2019. Lxmert: Learning
cross-modality encoder representations from trans-
formers. arXiv preprint arXiv:1908.07490.
Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-
hishek Srivastava, and Iryna Gurevych. 2021. Beir:
A heterogenous benchmark for zero-shot evaluation
of information retrieval models.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information process-
ing systems, 30.
Alex Wang, Yada Pruksachatkun, Nikita Nangia,
Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, and Samuel Bowman. 2019. Superglue: A
stickier benchmark for general-purpose language un-
derstanding systems. Advances in neural informa-
tion processing systems, 32.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. arXiv preprint
arXiv:1804.07461.
Ben Wang and Aran Komatsuzaki. 2021. GPT-J-
6B: A 6 Billion Parameter Autoregressive Language
Model. https://github.com/kingoflol
z/mesh-transformer-jax.
Kexin Wang, Nils Reimers, and Iryna Gurevych. 2021.
Tsdae: Using transformer-based sequential denois-
ing auto-encoder for unsupervised sentence embed-
ding learning. arXiv preprint arXiv:2104.06979.
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan
Yang, and Ming Zhou. 2020. Minilm: Deep self-
attention distillation for task-agnostic compression
of pre-trained transformers. Advances in Neural In-
formation Processing Systems, 33:5776–5788.
Samuel Weinbach, Marco Bellagente, Constantin
Eichenberg, Andrew Dai, Robert Baldock,
Souradeep Nanda, Björn Deiseroth, Koen Oost-
ermeijer, Hannah Teufel, and Andres Felipe
Cruz-Salinas. 2022. M-vader: A model for dif-
fusion with multimodal context. arXiv preprint
arXiv:2212.02936.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, et al. 2020. Transformers: State-of-the-art nat-
ural language processing. In Proceedings of the
2020 conference on empirical methods in natural
language processing: system demonstrations, pages
38–45.
Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan
Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie,
Jianfeng Gao, Winnie Wu, et al. 2020. Mind: A
large-scale dataset for news recommendation. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 3597–
3606.
Wei Xu, Chris Callison-Burch, and William B Dolan.
2015. Semeval-2015 task 1: Paraphrase and seman-
tic similarity in twitter (pit). In Proceedings of the
9th international workshop on semantic evaluation
(SemEval 2015), pages 1–11.
Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo,
Ehsan Kamalloo, David Alfonso-Hermelo, Xi-
aoguang Li, Qun Liu, Mehdi Rezagholizadeh, and
Jimmy Lin. 2022. Making a miracl: Multilingual in-
formation retrieval across a continuum of languages.
arXiv preprint arXiv:2210.09984.
Jeffrey Zhu, Mingqin Li, Jason Li, and Cassandra
Oduola. 2021. Bing delivers more contextualized
search using quantized transformer inference on
nvidia gpus in azure.
Pierre Zweigenbaum, Serge Sharoff, and Reinhard
Rapp. 2016. Towards preparation of the second bucc
shared task: Detecting parallel sentences in compa-
rable corpora. In Proceedings of the Ninth Workshop
on Building and Using Comparable Corpora. Euro-
pean Language Resources Association (ELRA), Por-
toroz, Slovenia, pages 38–43.
Pierre Zweigenbaum, Serge Sharoff, and Reinhard
Rapp. 2017. Overview of the second bucc shared
task: Spotting parallel sentences in comparable cor-
pora. In Proceedings of the 10th Workshop on Build-
ing and Using Comparable Corpora, pages 60–67.
Pierre Zweigenbaum, Serge Sharoff, and Reinhard
Rapp. 2018. Overview of the third bucc shared task:
Spotting parallel sentences in comparable corpora.
In Proceedings of 11th workshop on building and
using comparable corpora, pages 39–42.



Source: data\tc18_2309.07597v5\referenced_papers\[32]_2202.08904.pdf (Page 0):

SGPT: GPT Sentence Embeddings for Semantic
Search
Niklas Muennighoff
Peking University
muennighoff@stu.pku.edu.cn
Abstract
Decoder transformers have continued increasing in scale reaching hundreds of
billions of parameters. Due to their scale the same decoder sets state-of-the-art
results on various language tasks via prompting or ﬁne-tuning. Yet, these large
foundation models remain unusable for the related ﬁelds of semantic search and
sentence embeddings. This prevents possibly new state-of-the-art results and forces
organizations to train and maintain separate models. To this end, we propose SGPT
to use decoders for sentence embeddings and semantic search via prompting or ﬁne-
tuning. At 5.8 billion parameters SGPT improves on the previously best sentence
embeddings by a margin of 7% and outperforms a concurrent method with 175
billion parameters as measured on the BEIR search benchmark. Code, models and
result ﬁles are freely available at https://github.com/Muennighoff/sgpt.
1 Introduction
Semantic search consists of two parts: Search refers to ﬁnding the top kanswers from a document
corpus given a query. Semantic refers to understanding the documents and queries beyond keywords.
Transformers [45] are the dominant semantic architecture [ 8, 44] competing with non-semantic
models like BM25 [41]. Search applications like Google [26] or Bing [54] rely on transformers to
provide semantically relevant results. However, they have been limited to BERT-like encoder-only
transformers [26, 54, 10, 39, 12, 29].
Meanwhile, GPT-like decoder-only transformers [35] have been the focus of recent scaling efforts of
up to 540 billion parameters [9]. Increasing language model parameters has been repeatedly shown to
improve downstream zero-shot and ﬁne-tuning performance on a variety of language tasks [5, 37, 9].
For example, increasing scale has allowed decoder-only transformers to outperform all encoder-only
and catch-up with encoder-decoder transformers on the SuperGLUE benchmark [47, 9].
However, the related ﬁelds of semantic search and language embeddings have not been part of the
proliferation of decoders. They are dominated by comparatively small encoders [44], as it remains
unclear how to extract semantically meaningful embeddings from decoders and use them for semantic
search. Methods to do so are desirable for two reasons:
Performance Taking advantage of the available scale of decoders has the potential to produce new
state-of-the-art results in search. Available encoders are orders of magnitude smaller [ 10, 23, 38].
Google search, for example, processes an estimated 4 billion searches daily [19], thus better search
models could have wide-reaching impacts.
Compute Savings Large-scale pre-trained decoders have been reused for different tasks via prompt-
ing or ﬁne-tuning [5, 37, 9]. A well-performing method to extract embeddings from billion parameter
decoders may prevent the need to train and maintain separate encoder and decoder models. Training
just one large decoder and reusing it for search prevents additional cost to the environment [2].
Preprint. Under review.
arXiv:2202.08904v5  [cs.CL]  5 Aug 2022



Source: data\tc18_2309.07597v5\referenced_papers\[39]_2108.08877.pdf (Page 2):

3 Sentence T5
3.1 Model Architecture
In this work we explore three strategies to extract
sentence representations from T5, as shown in
ﬁgs. 2b to 2d:
• Encoder-only ﬁrst (ST5-Enc ﬁrst): The en-
coder output of the ﬁrst token is taken as the
sentence embedding.
• Encoder-only mean (ST5-Enc mean): The
sentence embedding is deﬁned as the average
of the encoder outputs across all input tokens.
• Encoder-Decoder ﬁrst (ST5-EncDec ﬁrst):
The ﬁrst decoder output is taken as the sen-
tence embedding. To obtain the decoder out-
put, the input text is fed into the encoder, and
the standard “start” symbol is fed as the ﬁrst
decoder input.
The ﬁrst two are pooling strategies widely used
in encoder-only pre-trained models such as BERT.
Unlike BERT models, T5 models do not have a
CLS token at the beginning of each sentence. For
T5 encoder-decoder models, we assume the de-
coder is aware of the semantics of the entire in-
put sentence when generating its ﬁrst token predic-
tion; and if so, the ﬁrst decoder output embeddings
(i.e. input to the softmax layer) might naturally
capture the sentence semantics.
For sentence encoder training, we adopt a dual
encoder architecture (Gillick et al., 2018; Cer et al.,
2018; Reimers and Gurevych, 2019). As shown
in ﬁg. 3, this architecture consists of two shared-
weight transformer modules that encode the inputs.
The transformer module can be either an encoder-
only or encoder-decoder architecture. In our ex-
periments, we initialize the transformer modules
from the pre-trained T5 models. After each module
computes a ﬁxed-length representation for its input
sentence, a projection layer and L2 normalization
are applied to the resulting embeddings. The pro-
jection layer transforms the output to a conﬁgurable
ﬁxed dimensionality (i.e. the sentence embedding
size). The embeddings from paired encoding tow-
ers can be scored for similarity tasks using a dot-
product3 or provide as input to additional layers
layers for pairwise classiﬁcation tasks (e.g., NLI).
3Since L2 normalization is applied to the output of each
tower, the dot-product between the embeddings will produce
their cosine similarity.
Transformer Encoder 
(optional Decoder)
Transformer Encoder 
(optional Decoder)
Sentence 1 Sentence 2
Embedding 1 Embedding 2
Loss
Projection & Norm Projection & Norm
Figure 3: Architecture of the dual encoder model.
3.2 Contrastive Learning
Applying contrastive learning to sentence embed-
dings improves the uniformity of the embeddings
space, leading to better performance on down-
stream tasks such as STS (Gao et al., 2021). We
apply contrastive learning to ﬁne-tune the T5 sen-
tence representations.4
3.2.1 Contrastive Loss
Using a contrastive loss to train a sentence encoder
requires paired examplesD= {(vi,v+
i )}as a train-
ing set, where vi is an input sentence and v+
i is a
related sentence (e.g., that is semantically close).
During training, v+
i is considered as a positive ex-
ample for vi and all other examples in the batch are
considered as negatives. The model should learn
to pull the positive example closer to the input
example while pushing away the negatives. We op-
erationalize our contrastive loss using an in-batch
sampled softmax (Henderson et al., 2017):
L= esim(vi,v+
i )/τ
∑
j∈Besim(vi,v+
j )/τ, (1)
The similarity scoring function is sim. Bis a mini-
batch of examples andτis the softmax temperature.
When additional negatives v−
j are provided for in-
put example v, the loss can be computed as:
L= esim(vi,v+
i )/τ
∑
j∈Besim(vi,v+
j )/τ + esim(vi,v−
j )/τ. (2)
4In preliminary experiments, we also explored ﬁne-tuning
with the classiﬁcation loss used in InferSent (Conneau et al.,
2017) and Sentence-BERT (Reimers and Gurevych, 2019).
However we found ﬁne-tuning for classiﬁcation on an NLI
dataset is inferior to contrastive learning as reported in (Gao
et al., 2021)



#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[22]_2112.09118.pdf (Page 5):

Published in Transactions on Machine Learning Research (08/2022)
Negative pairs across batches.An alternative approach is to store representations from previous batches
in a queue and use them as negative examples in the loss (Wu et al., 2018). This allows for smaller batch
size but slightly changes the loss by making it asymmetric between “queries” (one of the view generated
from the elements of the current batch), and “keys” (the elements stored in the queue). Gradient is only
backpropagated through the “queries”, and the representation of the “keys” are considered as ﬁxed. In
practice, the features stored in the queue from previous batches comes form previous iterations of the network.
This leads to a drop of performance when the network rapidly changes during training. Instead, He et al.
(2020) proposed to generate representations of keys from a second network that is updated more slowly. This
approach, called MoCo, considers two networks: one for the keys, parametrized byθk, and one of the query,
parametrized byθq. The parameters of the query network are updated with backpropagation and stochastic
gradient descent, similarly to when using in-batch negatives, while the parameters of the key network, or
Momentum encoder, is updated from the parameters of the query network by using a exponential moving
average:
θk ← mθk + (1−m)θq, (2)
where m is the momentum parameter that takes its value in[0,1].
4 Experiments
In this section, we empirically evaluate our best retriever trained with contrastive learning, calledContriever
(contrastive retriever), which uses MoCo with random cropping. We use a contrastive learning procedure that
diﬀers from ICT (Lee et al., 2019) mainly in three aspects. First, positive pairs are sampled using random
cropping and tokens from each element of the pair are deleted with a probability of 10%. Second we use
MoCo where negatives consists of elements from previous batches stored in a queue. This allows to scale to a
large number of negatives. Third we use data from Wikipedia and CCNet (Wenzek et al., 2020) for training.
Ablation studies motivating these technical choices are performed in Section 6. More technical details about
our model are given in Appendix A.1.
4.1 Datasets
Contriever is trained with contrastive learning on documents sampled from a mix between Wikipedia data
and CCNet data (Wenzek et al., 2020), where half the batches are sampled from each source.
First, we evaluate our model on two question answering datasets: NaturalQuestions (Kwiatkowski et al.,
2019) and TriviaQA (Joshi et al., 2017). For both datasets, we use the open domain versions as introduced
by Lee et al. (2019), and the English Wikipedia dump from Dec. 20, 2018 as the collection of documents to
retrieve from. We report the top-k retrieval accuracy,i.e. the number of questions for which at least one of
the top-k passages contain the answer.
Second, we use the BEIR benchmark, introduced by Thakur et al. (2021), which contains 18 retrieval datasets,
corresponding to nine tasks, such as fact checking or citation prediction, and covering diﬀerent domains, such
as Wikipedia or scientiﬁc publications. Most datasets from BEIR do not contain a training set, and the focus
of the benchmark iszero-shot retrieval. However, most machine learning based retrievers are still trained
on supervised data, such as the large scale retrieval dataset MS MARCO (Bajaj et al., 2016). Following
standard practice, we report two metrics on this benchmark: nDCG@10 and Recall@100. The nDCG@10
focuses on the ranking of the top 10 retrieved documents, and is good at evaluating rankings returned to
humans, for example in a search engine. On the other hand, Recall@100 is relevant to evaluate retrievers that
are used in machine learning systems, such as question answering. Indeed, such models can process hundreds
of documents, and ignore their ranking (Izacard & Grave, 2020b). While nDCG@10 is the main metric of
BEIR, we are more interested in the Recall@100 to evaluate bi-encoders, as our goal is to develop retrievers
that can be used in ML systems. Moreover, in many settings, retrieved documents can be re-ranked with a
more powerful model such as a cross-encoder, thus improving the nDCG@10.
6



Source: data\tc18_2309.07597v5\referenced_papers\[22]_2112.09118.pdf (Page 6):

Published in Transactions on Machine Learning Research (08/2022)
Table 1:Unsupervised recall@kon the test sets of NaturalQuestions and TriviaQA. For Inverse Cloze
Task and Masked Salient Spans we report the results of Sachan et al. (2021). The Masked Salient Spans
model uses annotated named entity recognition data. For BM25 we report the results of Ma et al. (2021)
NaturalQuestions TriviaQA
R@5 R@20 R@100 R@5 R@20 R@100
Inverse Cloze Task (Sachan et al., 2021) 32.3 50.9 66.8 40.2 57.5 73.6
Masked salient spans (Sachan et al., 2021) 41.7 59.8 74.9 53.3 68.2 79.4
BM25 (Ma et al., 2021) - 62.9 78.3 - 76.4 83.2
Contriever 47.8 67.8 82.1 59.4 74.2 83.2
supervised model:DPR (Karpukhin et al., 2020) - 78.4 85.4 - 79.4 85.0
supervised model:FiD-KD (Izacard & Grave, 2020a) 73.8 84.3 89.3 77.0 83.6 87.7
4.2 Baselines
First, we compare Contriever to BM25, which does not require supervision. On QA datasets, we compare to
dense retrievers trained with ICT and the Masked Salient Spans from Sachan et al. (2021). On BEIR, we
consider the retriever from REALM (Guu et al., 2020), and RoBERTa large ﬁne-tuned with SimCSE (Gao
et al., 2021), as unsupervised dense retrievers. We also compare to ML-based retrievers trained on MS MARCO,
classiﬁed in three categories: sparse, dense and late-interaction. For sparse methods, we compare toSplade
v2 (Formal et al., 2021), which computes sparse representations of documents with BERT pre-trained
model. For dense methods, we useDPR (Karpukhin et al., 2020) andANCE (Xiong et al., 2020), which
are bi-encoders trained on supervised data such as NaturalQuestions or MS MARCO. We also compare
to TAS-B (Hofstätter et al., 2021), which performs distillation from a cross-encoder to a bi-encoder, and
GenQ, which creates synthetic query-document pairs with a generative model.1 For late-interaction, we use
ColBERT (Khattab et al., 2020), which computes pairwise scores between contextualized representations of
queries and documents, as well as a cross-encoder used to re-rank documents retrieved with BM25.
4.3 Results
First, we compare the performance of fully unsupervised models, i.e., without ﬁne-tuning on MS MARCO or
other annotated data. In Table 1, we report the retrieval performance on two question answering datasets:
NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). Here, our model is competitive
with a strong BM25 baseline (Ma et al., 2021), for example leading to 3 points improvement for the recall@100
on NaturalQuestions. It also outperforms previously proposed dense retrievers which were trained with ICT
or salient span masking. In Figure 1 we report the recall@100 performance of unsupervised models on the
BEIR benchmark. Interestingly, we observe that in this setting, Contriever is competitive compared to BM25
on all datasets, but TREC-COVID and Tóuche-2020. In particular, it obtains better performance than BM25
on 11 out of 15 datasets from the benchmark for the recall@100. Contriever also outperforms previously
proposed unsupervised dense retrievers, which obtains lower performance than BM25 in general. For the
nDCG@10, which puts more emphasis on the very ﬁrst retrieved documents, while Contriever largely closes
the gap between unsupervised retrievers and BM25, it is still outperformed by BM25 as reported in Table 11.
The diﬀerence is mainly due to the fact that BM25 largely outperforms Contriever on two datasets with
speciﬁc features: Trec-COVID and Tóuche-2020. Trec-COVID is an information retrieval dataset related to
COVID. However data used to train Contriever were collected before the COVID outbreak, thus they may
not be adapted. Tóuche-2020 contains long documents, which does not seem to be very well supported by
dense neural retrievers: even after supervised training, models are still lagging behind BM25. Overall, these
results show the potential of contrastive learning to train fully unsupervised dense retrievers.
1GenQ thus leads to one diﬀerent model for each dataset.
7



Source: data\tc18_2309.07597v5\referenced_papers\[22]_2112.09118.pdf (Page 16):

Published in Transactions on Machine Learning Research (08/2022)
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand
Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data.
In Proceedings of the 12th Language Resources and Evaluation Conference, 2020. 6, 9, 17
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric
instance discrimination. InProceedings of the IEEE conference on computer vision and pattern recognition,
pp. 3733–3742, 2018. 2, 4, 6
Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. Clear: Contrastive learning
for sentence representation.arXiv preprint arXiv:2012.15466, 2020. 3
Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold
Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval.arXiv
preprint arXiv:2007.00808, 2020. 3, 4, 7
Sohee Yang and Minjoon Seo. Is retriever merely an approximator of reader?arXiv preprint arXiv:2010.10999,
2020. 3
Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. Mr. tydi: A multi-lingual benchmark for dense
retrieval. CoRR, abs/2108.08787, 2021. URLhttps://arxiv.org/abs/2108.08787. 9, 10
A Technical details for Contriever
A.1 Contrastive pre-training
For the model with ﬁne-tuning on MS MARCO, we use the MoCo algorithm He et al. (2020) with a queue of
size 131,072, a momentum value of 0.9995 and a temperature of 0.05. We use the random cropping data
augmentation, with documents of 256 tokens and span sizes sampled between 5% and 50% of the document
length. Documents are simply random piece of text sampled from a mix between Wikipedia and CCNet
data (Wenzek et al., 2020), where half the batches are sampled from each source. We also apply token
deletion with a probability of 10%. We optimize the model with the AdamW (Loshchilov & Hutter, 2019)
optimizer, with learning rate of5 ·10−5, batch size of 2,048 and 500,000 steps. We initialize the network with
the publicly available BERT base uncased model.
A.2 Fine-tuning on MS MARCO
For the ﬁne-tuning on MS MARCO we do not use the MoCo algorithm and simply use in-batch negatives.
We use the ASAM optimizer (Kwon et al., 2021), with a learning rate of10−5 and a batch size of 1024 with a
temperature of0.05, also used during pre-training. We train an initial model with random negative examples
for 20000 steps, mine hard negatives with this ﬁrst model, and re-train a second model with those. Each
query is associated with a gold document and a negative document, which is a random document in the ﬁrst
phase and a hard negative 10% of the time in the second phase. For each query, all documents from the
current batch aside of the gold document are used as negatives.
A.3 Few-shot training
For the few-shot evaluation presented in Table 3, we train for 500 epochs on each dataset with a batch size of
256 with in-batch random negatives. We evaluate performance performance on the development set every 100
gradient updates and perform early stopping based on this metric. For SciFact, we hold out randomly 10% of
the training data and use them as development set, leading to a train set containing 729 samples.
17



Source: data\tc18_2309.07597v5\referenced_papers\[22]_2112.09118.pdf (Page 17):

Published in Transactions on Machine Learning Research (08/2022)
Table 10: BEIR Benchmark. We report the recall@100 on the test sets from the BEIR benchmark for
bi-encoder methods. We report the capped recall@100 on Trec-COVID following the original BEIR setup.
Note that using a cross-encoder to re-rank the top-100 documents do not change the recall@100, hence, we do
not include these methods in this table. We also report the average and number of datasets where a method
is the best (“Best on”) over the entire BEIR benchmark (excluding three datasets because of their licence).
Bold is the best overall. On Trec-COVID we report the capped Recall@100, see Thakur et al. (2021) for
more details. MS MARCO is excluded from the average.
BM25 DPR ANCE TAS-B Gen-Q ColBERT Splade v2 Ours
MS MARCO 65.8 55.2 85.2 88.4 88.4 86.5 - 89.1
Trec-COVID 49.8 21.2 45.7 38.7 45.6 46.4 12.3 40.7
NFCorpus 25.0 20.8 23.2 28.0 28.0 25.4 27.7 30.0
NQ 76.0 88.0 83.6 90.3 86.2 91.2 93.0 92.5
HotpotQA 74.0 59.1 57.8 72.8 67.3 74.8 82.0 77.7
FiQA 53.9 34.2 58.1 59.3 61.8 60.3 62.1 65.6
ArguAna 94.2 75.1 93.7 94.2 97.8 91.4 97.2 97.7
Touche-2020 53.8 30.1 45.8 43.1 45.1 43.9 35.4 29.4
CQADupStack 60.6 40.3 57.9 62.2 65.4 62.4 - 66.3
Quora 97.3 47.0 98.7 98.6 98.8 98.9 98.7 99.3
DBPedia 39.8 34.9 31.9 49.9 43.3 46.1 57.5 54.1
Scidocs 35.6 21.9 26.9 33.5 33.2 34.4 36.4 37.8
Fever 93.1 84.0 90.0 93.7 92.8 93.4 95.1 94.9
Climate-fever 43.6 39.0 44.5 53.4 45.0 44.4 52.4 57.4
Scifact 90.8 72.7 81.6 89.1 89.3 87.8 92.0 94.7
Avg. w/o CQA 63.6 48.3 60.1 65.0 64.2 64.5 64.8 67.1
Avg. 63.4 47.7 60.0 64.8 64.2 64.3 - 67.0
Best on 2 0 0 0 1 0 4 7
B Multilingual retrieval with mContriever
B.1 Hyperparameters for multilingual contrastive pre-training
The pre-trained mContriever model is pre-trained for 500,000 steps with a queue of size 32768, and temperature
of 0.05 and a momentum value of 0.999. We optimize the model with the AdamW (Loshchilov & Hutter,
2019) optimizer, with learning rate of5 ·10−5. The learning rate follows a linear warmup for 20,000 steps
followed by linear decay until the end of training. Languages used for pre-training are detailed in Table 12.
B.2 Hyperparameters for multilingual ﬁne-tuning
We ﬁne-tune mContriever using in-batch negatives, AdamW optimizer (Loshchilov & Hutter, 2019), a learning
rate of10−5, and a batch size of 1024 samples with a temperatureτof 0.05. On MS MARCO and Mr. TyDi
the model is trained for 20k gradient steps. We notice overﬁtting on NaturalQuestions, and thus reduced the
training to 1k gradient steps. We use a warmup of 1000 gradient steps with linear decay afterwards in all
cases. Hard negatives are mined on Mr. TyDi with the model trained on MS MARCO. We did not observe
signiﬁcant improvements using hard negatives on MS MARCO and NaturalQuestions.
For the ﬁne-tuning on MS MARCO of the models initiliazed from mBERT (resp. XLM-R) without contrastive
pre-training, we use a temperatureτof 1 (resp. 5). We tried temperatures in{10,5,2,1,0.1,0.05}and chose
the one leading to the best performance. We observed a decrease in performance for lower temperatures.
We ﬁne-tuned mContriever withτ= 0.05 following the temperature used during pre-training. We followed
the temperatureτ= 0.05 used for the training of Contriever, and did not test other temperatures for the
contrastive pre-training of the multilingual model, mContriever.
18



Source: data\tc18_2309.07597v5\referenced_papers\[22]_2112.09118.pdf (Page 2):

Published in Transactions on Machine Learning Research (08/2022)
(2019) introduced across-encoder model, based on the BERT model (Devlin et al., 2019), which jointly
encodes queries and documents. The application of a strong pre-trained model, as well as the cross-encoder
architecture, lead to important improvement on the MS MARCO benchmark (Bajaj et al., 2016).
The methods described in the previous paragraph were applied to re-rank documents, which were retrieved
with a traditional IR system such as BM25. Gillick et al. (2018) ﬁrst studied whether continuous retrievers,
based on bi-encoder neural models, could be viable alternative to re-ranking. In the context of question
answering, Karpukhin et al. (2020) introduced a dense passage retriever (DPR) based on the bi-encoder
architecture. This model is initialized with a BERT network, and trained discriminatively using pairs of
queries and relevant documents, with hard negatives from BM25. Xiong et al. (2020) further extended this
work by mining hard negatives with the model itself during optimization, and trained on the MS MARCO
dataset. Once a collection of documents, such as Wikipedia articles, is encoded, retrieval is performed
with a fast k-nearest neighbors library such as FAISS Johnson et al. (2019). To alleviate the limitations of
bi-encoders, Humeau et al. (2019) introduces the poly-encoder architecture, where documents are encoded
by multiple vectors. Similarly, Khattab et al. (2020) proposes the ColBERT model, which keeps a vector
representation for each term of the queries and documents. To make the retrieval tractable, the term-level
function is approximated to ﬁrst retrieve an initial set of candidates, which are then re-ranked with the true
score. In the context of question answering, knowledge distillation has been used to train retrievers, either
using the attention scores of the reader of the downstream task as synthetic labels (Izacard & Grave, 2020a),
or the relevance score from a cross encoder (Yang & Seo, 2020). Luan et al. (2020) compares, theoretically and
empirically, the performance of sparse and dense retrievers, including bi-, cross- and poly-encoders. Dense
retrievers, such as DPR, can lead to indices weighing nearly 100GB when encoding document collections such
as Wikipedia. Izacard et al. (2020) shows how to compress such indices, with limited impact on performance,
making them more practical to use.
Self-supervised learning for NLP. Following the success of word2vec (Mikolov et al., 2013), many
self-supervised techniques have been proposed to learn representation of text. Here, we brieﬂy review the ones
that are most related to our approach: sentence level models and contrastive techniques. Jernite et al. (2017)
introduced diﬀerent objective functions to learn sentence representations, including next sentence prediction
and sentence order prediction. These objectives were later used in pre-trained models based on transformers,
such as BERT (Devlin et al., 2019) and AlBERT (Lan et al., 2019). In the context of retrieval, Lee et al.
(2019) introduced the inverse cloze task (ICT), whose purpose is to predict the context surrounding a span of
text. Guu et al. (2020) integrated a bi-encoder retriever model in a BERT pre-training scheme. The retrieved
documents are used as additional context in the BERT task, and the whole system is trained end-to-end in
an unsupervised way. Similarly, Lewis et al. (2020) proposed to jointly learn a retriever and a generative
seq2seq model, using self-supervised training. Chang et al. (2020) compares diﬀerent pre-training tasks for
retrieval, including the inverse cloze task. In the context of natural language processing, Fang et al. (2020)
proposed to apply MoCo where positive pairs of sentences are obtained using back-translation. Diﬀerent
works augmented the masked language modeling objective with a contrastive loss (Giorgi et al., 2020; Wu
et al., 2020; Meng et al., 2021). SBERT (Reimers & Gurevych, 2019) uses a Siamese network similar to
contrastive learning to learn a BERT-like model that is adapted to matching sentence embeddings. Their
formulation is similar to our work but requires aligned pairs of sentences to form positive pairs while we
propose to use data augmentation to leverage large unaligned textual corpora. Concurrent to this work, Gao
& Callan (2021) have also shown the potential of contrastive learning for information retrieval; building on the
same observation that both tasks share a similar structure. Spider (Ram et al., 2021), a contemporary work,
uses spans appearing multiple times in a document to create pseudo examples for contrastive learning in order
to train unsupervised retrievers. Finally, Chen et al. (2021) train a dense retriever to imitate unsupervised
lexical-based methods. This improves performance on a range of tasks and achieves state-of-the-art results
when combining the resulting dense retriever with Contriever, our model pre-trained with contrastive learning.
3 Method
In this section, we describe how to train a dense retriever with no supervision. We review the model
architecture and then describe contrastive learning — a key component of its training.
3



#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 0):

Text Embeddings by Weakly-Supervised
Contrastive Pre-training
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao
Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei
Microsoft Corporation
https://github.com/microsoft/unilm
Abstract
This paper presents E5 1, a family of state-of-the-art text embeddings that transfer
well to a wide range of tasks. The model is trained in a contrastive manner with
weak supervision signals from our curated large-scale text pair dataset (called
CCPairs). E5 can be readily used as a general-purpose embedding model for any
tasks requiring a single-vector representation of texts such as retrieval, clustering,
and classification, achieving strong performance in both zero-shot and fine-tuned
settings. We conduct extensive evaluations on 56 datasets from the BEIR and
MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms
the strong BM25 baseline on the BEIR retrieval benchmark without using any
labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark,
beating existing embedding models with 40× more parameters.
1 Introduction
Text embeddings are low-dimensional vector representations for arbitrary-length texts and play key
roles in many NLP tasks such as large-scale retrieval. Compared to the high-dimensional and sparse
representations like TF-IDF, text embeddings have the potential to overcome the lexical mismatch
issue and facilitate efficient retrieval and matching between texts. It also offers a versatile interface
easily consumable by downstream applications.
While pre-trained language models such as BERT [ 17] and GPT [ 7] can produce transferrable
text representations, they are not ideal for tasks such as retrieval and text matching where a single-
vector embedding of texts is more desired due to its efficiency and versatility. To obtain better
text embeddings, contrastive learning is often the go-to framework to enhance the sequence-level
representations from text pairs. Along this line of research, some works are geared towards learning
task-specific embeddings. For example, GTR [ 43] and Sentence-T5 [ 44] fine-tune pre-trained
models with supervised datasets to learn embeddings customized for passage retrieval and semantic
textual similarity, respectively. Other works learn unsupervised embeddings from automatically
constructed text pairs. Typical methods to construct text pairs include Inverse Close Task (ICT)
[9], random cropping [ 28] and neighboring text spans [ 41], etc. While such synthetic data are
of unlimited quantity, they are often poor in quality and the resulted embeddings fail to match the
performance of the classic BM25 baseline without further fine-tuning [40].
In this work, we learn a high-quality general-purpose text embedding termed E5, EmbEddings from
bidirEctional Encoder rEpresentations. E5 aims to provide strong off-the-shelf text embeddings
suitable for any tasks requiring single-vector representations in both zero-shot or fine-tuned settings.
To achieve this goal, instead of relying on limited labeled data or low-quality synthetic text pairs, we
contrastively train E5 embeddings from CCPairs, a curated web-scale text pair dataset containing
1E5: EmbEddings from bidirEctional Encoder rEpresentations
Work in progress.
arXiv:2212.03533v2  [cs.CL]  22 Feb 2024



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 7):

Table 5: Impacts of different batch sizes for contrastive pre-training.
batch size NFCorpus NQ FiQA Quora DBPedia Scifact Avg
32k 35.8 39.0 40.0 85.7 35.4 73.7 51.6
8k 33.3 38.5 37.6 85.7 34.0 71.8 50.2
1k 28.2 33.1 30.4 84.0 30.1 69.1 45.8
Table 5, increasing batch size from 1K to 32K leads to consistent gains across all 6 datasets. It is also
possible to train with smaller batch sizes by adding hard negatives [ 50]. However, the engineering
efforts of mining hard negatives for large datasets (>100M) are non-trivial.
Table 6: Fine-tuning with different combinations of labeled data.
Fine-tuned on Retrieval STS Classification Summ. MTEB Avg
No fine-tuning 42.9 69.5 67.9 31.1 55.6
MS-MARCO + NQ 50.3 78.3 68.3 30.6 59.0
NLI 38.3 81.1 72.6 31.6 57.3
All above 48.7 81.0 73.1 31.0 60.4
Fine-tuning Datasets GTR models are fine-tuned with “MS-MARCO + NQ”, while Sentence-T5
models use NLI instead. In Table 6, we can see that the “MS-MARCO + NQ” setting performs best
on retrieval tasks, and the NLI data is beneficial for STS and linear probing classification. Similar
observations are also made by Muennighoff et al. [40]. Combining all of them leads to the best
overall scores on the MTEB benchmark. This also illustrates the importance of dataset diversity for
learning text embeddings.
Table 7: Data filtering. For the top 2 rows, we train with 1M random text pairs.
# of pairs NFCorpus NQ FiQA Quora DBPedia Scifact Avg
1M w/o filter 23.0 15.1 18.5 83.1 18.2 51.4 34.9
w/ filter 26.8 22.7 24.5 85.0 27.5 57.5 40.7
All w/o filter 34.5 35.4 39.1 85.7 32.9 72.5 50.0
w/ filter 35.8 39.0 40.0 85.7 35.4 73.7 51.6
Data Filtering One crucial step in our dataset curation pipeline is filtering out low-quality text pairs.
In Table 7, when training with 1M pairs, using filtered data has a nearly 6 points advantage. When
all the text pairs are used, the “w/o filter” setting has about 4× more data but is still behind by 1.6
points. Though recent studies [ 29, 47] show that deep learning models are quite robust to dataset
noises, data filtering still has benefits in improving training efficiency and model quality.
Negative Sampling We explore two alternative methods to enlarge the number of negatives: Pre-
batch negatives [ 33] reuse embeddings from previous batches as additional negatives, while MoCo
[25] introduces a momentum encoder and uses a FIFO queue to store negatives. For both approaches,
the negative size can be easily scaled up without incurring much GPU memory overhead. The
downside is that most negatives are produced by an older version of model parameters. In Table 8,
in-batch negatives still perform favorably. Empirically, we find that MoCo is more sensitive to certain
hyperparameters such as temperature, better results are possible with more tuning.
BM25 vs Dense Retrieval With the rapid development of dense retrieval models, can we replace
the long-standing BM25 algorithm from now on? The answer is likely “not yet”. BM25 still holds
obvious advantages in terms of simplicity, efficiency, and interpretability. For long-tail domains such
as Trec-Covid [ 55] and retrieval tasks that involve long documents (Touche-2020) [4] or rely heavily
on exact lexical match (Fever) [ 54], further research efforts are still necessary to improve current
dense retrievers.
6 Conclusion
In this work, we train a general-purpose text embedding model E5 from weak supervision signals. We
adopt a simple contrastive training framework with in-batch negatives and learn from a large-scale text
pair dataset we harvest from heterogeneous data sources across the web. E5 offers strong off-the-shelf
8



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 16):

Table 13: Results for each dataset in the MTEB benchmark [ 40]. The numbers for the Retrieval
category are not included here since the datasets are the same as the BEIR benchmark.
unsupervised supervised
E5-PTsmall E5-PTbase E5-PTlarge E5small E5base E5large
AmazonCounterfactualClassification 71.7 73.6 70.4 76.2 79.7 77.7
AmazonPolarityClassification 76.1 77.0 83.2 87.5 88.0 90.1
AmazonReviewsClassification 35.0 35.8 37.4 42.6 42.7 43.0
Banking77Classification 82.1 82.9 83.5 81.9 83.3 84.1
EmotionClassification 42.2 44.2 43.5 46.9 49.4 48.1
ImdbClassification 67.9 67.3 77.7 75.6 76.0 82.1
MassiveIntentClassification 70.2 71.1 70.8 72.2 72.3 73.2
MassiveScenarioClassification 74.6 75.4 75.9 75.8 76.8 77.4
MTOPDomainClassification 91.3 92.3 93.2 92.1 93.2 93.9
MTOPIntentClassification 71.9 74.0 74.2 73.2 74.8 76.4
ToxicConversationsClassification 67.0 67.4 66.1 72.8 74.1 70.6
TweetSentimentExtractionClass. 54.4 53.3 52.5 63.3 61.4 61.2
ArxivClusteringP2P 47.9 49.3 49.4 44.1 44.6 46.2
ArxivClusteringS2S 39.9 42.8 43.6 37.1 40.5 41.4
BiorxivClusteringP2P 38.5 38.8 39.2 35.8 36.2 37.6
BiorxivClusteringS2S 35.4 36.5 36.7 31.9 32.7 35.1
MedrxivClusteringP2P 34.4 33.7 33.3 31.3 31.5 32.3
MedrxivClusteringS2S 32.0 32.1 32.2 28.2 28.3 29.7
RedditClustering 46.9 49.3 52.4 42.9 48.2 50.7
RedditClusteringP2P 60.2 64.4 64.6 56.4 62.2 61.4
StackExchangeClustering 57.7 60.2 63.3 59.1 63.9 65.0
StackExchangeClusteringP2P 32.0 34.0 34.7 30.3 32.6 33.6
TwentyNewsgroupsClustering 34.4 36.2 37.9 37.5 42.6 43.8
SprintDuplicateQuestions 91.6 90.8 92.0 95.3 94.9 95.4
TwitterSemEval2015 60.0 62.8 64.7 74.2 74.4 76.1
TwitterURLCorpus 83.2 84.0 84.1 85.8 86.0 86.3
AskUbuntuDupQuestions 57.8 57.6 58.3 59.4 59.7 60.1
MindSmallReranking 29.0 29.6 29.2 29.6 30.1 30.8
SciDocsRR 81.1 82.6 84.3 79.8 82.9 83.9
StackOverflowDupQuestions 44.4 44.2 45.8 49.1 50.1 51.3
BIOSSES 69.2 71.9 69.7 84.2 85.1 84.7
SICK-R 66.6 68.7 69.7 78.9 79.7 80.5
STS12 60.7 57.9 54.7 75.2 74.2 75.9
STS13 71.1 73.5 74.0 81.8 83.3 85.2
STS14 64.2 64.0 65.3 78.5 78.5 80.5
STS15 74.3 75.4 75.8 87.5 88.4 88.8
STS16 76.6 79.8 80.1 84.6 84.2 85.3
STS17 78.3 77.2 76.0 87.9 87.2 89.4
STS22 59.2 56.2 62.8 63.8 62.9 63.0
STSBenchmark 67.7 70.5 70.9 86.4 86.2 87.2
SummEval 32.7 31.1 32.6 31.4 31.0 31.0
17



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 4):

Zero-shot Text Classification The input and label texts are converted to sentences based on manually
written prompt templates. The predicted label is the one closest to the input text in the embedding
space. Take the sentiment classification of movie reviews as an example, with the original input “I
enjoy watching it”, the label text is “it is an example of terrible/great movie review” and the input text
becomes “movie review: I enjoy watching it”.
Semantic Textual Similarity Given two text embeddings, we use the cosine function to measure
their semantic similarity. Since the absolute similarity scores do not enable an easy interpretation, the
evaluation is usually based on rank correlation coefficients.
Text Clustering Standard clustering algorithms such as k-means can be applied straightforwardly.
Texts belonging to the same category are expected to be close in the embedding space.
For tasks other than zero-shot text classification and retrieval, we use the query embeddings by
default.
5 Experiments
5.1 Pre-training and Fine-tuning Configurations
Pre-training We pre-train on our proposed text pair dataset for three model sizes: E5small, E5base
and E5large initialized from MiniLM [ 59], bert-base-uncased, and bert-large-uncased-whole-word-
masking respectively. The batch size is set to a large value of 32, 768 to increase the number of
negatives. The learning rate is {3, 2, 1}×10−4 for the {small, base, large} models, with linear decay
and the first 1, 000 steps for warmup. We pre-train for 20k steps in total with AdamW optimizer,
which is approximately 2.5 epochs over the dataset. It takes {16, 32, 64} V100 GPUs and {1, 1, 2}
days for the {small, base, large} models. To improve training efficiency and reduce GPU memory
usage, we adopt mixed precision training and gradient checkpointing.
Fine-tuning is performed on the concatenation of 3 datasets: MS-MARCO passage ranking [ 8],
NQ [ 32, 30], and NLI [ 22] datasets. We reuse the mined hard negatives and re-ranker scores from
SimLM [ 58] for the first two datasets. Models are fine-tuned for 3 epochs with batch size 256 on 8
GPUs. Learning rate is {3, 2, 1}×10−5 for the {small, base, large} models with 400 steps warmup.
For each example, we use 7 hard negatives. Since the NLI dataset only has 1 hard negative for each
example, 6 sentences are randomly sampled from the entire corpus.
We use E5-PT to denote models with contrastive pre-training only. More implementation details can
be found in Appendix B.
5.2 Evaluation Datasets
BEIR Benchmark [ 53] is a collection of 19 information retrieval datasets, ranging across ad-hoc
web search, question answering, fact verification and duplicate question retrieval, etc. We evaluate
the 15 datasets that provide public downloads. The main metric is nDCG@10.
MTEB Benchmark [ 40] is recently proposed for benchmarking massive text embedding tasks.
Though MTEB is multilingual due to the inclusion of bitext mining datasets, most datasets are
still only available in English. In this paper, we evaluate the English subsets, which have 56
datasets spanning across 6 categories: Classification (Class.), Clustering (Clust.), Pair Classification
(PairClass.), Rerank, Retrieval (Retr.), STS, and Summarization (Summ.). The evaluation metrics are
accuracy, v-measure, average precision, MAP, nDCG@10, and Spearman coefficients, respectively.
Please refer to the MTEB paper for details.
5.3 Results on BEIR benchmark
Results with Unsupervised Methods In Table 1, we show model results that do not use any labeled
data. When averaged over all 15 datasets, E5-PTbase outperforms the classic BM25 algorithm by 1.2
points. To the best of our knowledge, this is the first reported result that an unsupervised model can
beat BM25 on the BEIR benchmark. When scaling up to E5-PT large, we see further benefits from
42.9 to 44.2.
5



Source: data\tc18_2309.07597v5\referenced_papers\[28]_2308.03281.pdf (Page 6):

Trec-COVIDNFCorpus
Tóuche-2020
DBPediaScidocs
Climate-fever
HotpotQA
FiQA
CQADupStackMSMARCO
NQ FeverScifactArguAna
Quora
Avg.0
50
100Recall@100
SimCSE Contriever BM25 GTE
Figure 2: Recall@100 of unsupervised text retrieval methods on BEIR benchmark (Thakur et al., 2021). We
compare our model GTEbase (based on BERTbase) without using any annotated data to SimCSE (Gao et al., 2021)
(based on RoBERTalarge), Contriever (Izacard et al., 2022a) (based on BERTbase) and BM25. Baseline results are
borrowed from the Contriever paper (Izacard et al., 2022a) with dot product being the similarity function.
Dataset BM25 SimCSE Contriever CPT-S E5 small E5base E5large GTEsmall GTEbase GTElarge
MS MARCO 22.8 9.4 20.6 19.9 25.4 26.0 26.2 31.3 31.8 31.7
Trec-Covid 65.6 26.2 27.4 52.9 52.0 61.0 61.8 61.8 64.0 64.8
NFCorpus 32.5 9.9 31.7 32.0 29.3 35.8 33.7 34.9 36.2 38.1
NQ 32.9 11.7 25.4 - 37.3 39.0 41.7 32.0 35.3 34.5
HotpotQA 60.3 19.8 48.1 51.5 46.0 52.4 52.2 49.3 50.8 49.2
FiQA 23.6 9.8 24.5 34.1 38.3 40.0 43.2 37.0 36.9 40.6
ArguAna 31.5 38.3 37.9 38.7 42.5 42.2 44.4 41.6 41.0 41.3
Touche-2020 36.7 8.9 19.3 21.0 19.9 16.9 19.8 17.7 18.2 18.5
CQADupStack 29.9 13.2 28.4 - 35.0 35.4 38.9 38.1 39.9 39.8
Quora 78.9 78.0 83.5 68.1 85.8 85.7 86.1 86.1 85.0 84.8
DBPedia 31.3 15.0 29.2 27.2 34.5 35.4 37.1 33.5 33.2 33.6
Scidocs 15.8 5.5 14.9 - 19.9 21.1 21.8 21.5 22.5 22.7
Fever 75.3 21.1 68.2 57.1 62.5 63.4 68.6 71.3 72.7 70.5
Climate-Fever 21.3 11.8 15.5 15.8 14.5 15.4 15.7 21.4 21.0 25.4
Scifact 66.5 25.7 64.9 65.4 68.5 73.7 72.3 72.7 74.1 74.1
Average 41.7 20.3 36.0 - 40.8 42.9 44.2 43.4 44.2 44.6
Table 5: nDCG@10 of different unsupervised methods on the BEIR benchmark (Thakur et al., 2021). SimCSE is
based on BERTbase backbone. CPT-S (Neelakantan et al., 2022) is of similar size to BERTlarge. Baseline results are
borrowed from E5 paper (Wang et al., 2022b). Note that Contriever uses dot product as the similarity metric while
other models uses cosine similarity.
on the tasks covered in the MTEB benchmark,
please refer to the Appendix B.
Two settings are considered for comparison: the
unsupervised setting and the supervised setting. In
the unsupervised setting, models are trained us-
ing unlabeled data, while supervised models are
fine-tuned using high-quality datasets with human
labels. The results of strong baseline models are
presented in Table 6.
In the unsupervised setting, our model outper-
forms the previous best model, E5, by a signifi-
cant margin across all considered tasks, without
the use of task-specific prompts. This improve-
ment can be attributed to the inclusion of more
training data formats and various sources of self-
supervision signals. Furthermore, it is worth noting
that our unsupervised pre-trained model narrows
the gap even further with larger supervised base-
lines, such as GTR and Sentence-T5. In the super-
vised setting, our model surpasses OpenAI results



#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 7):

173 176 173 172
ArguAna
176 179
173
182
BioASQ
206
228
244
234
Climate-Fever
57
60
62 62
DBPedia-entity
88
97
104 105
Fever
136
156
151 154
FiQA-2018
53
56
61
63
HotpotQA
49
50 50 50
MS Marco
236
243 245 243
NFCorpus
75
78
80 81
NQ
8
9 9 9
Quora
410
453
525
490
Robust04
166
170 169 168
SCIDOCS
211
216
221 219
SciFact
10 10 10 10
Signal-1M
BaseLarge XL XXL
15
10
12
9
Trec-Covid
BaseLarge XL XXL
678
707 700 708
Trec-News
BaseLarge XL XXL
32 32
41
62
Touché-2020
BaseLarge XL XXL
111
116 117 116
CQADupStack
Figure 5: Median lengths (in words) of top-10 retrieved
documents for all queries.
Touche2020 are longer.
On the other hand, the only exception we ob-
serve is the Trec-Covid dataset, where GTR-XXL
model retrieves much shorter documents than those
retrieved by the smaller size counterparts. This may
explain the inferior performance of GTR-XXL on
Trec-Covid shown in table 3 and table 8. We leave
it as future work to explore the effects of using the
dot-product as similarity function for large dual
encoders.
7 Related Work
Neural information retrieval. Document re-
trieval is an important task in the NLP and informa-
tion retrieval (IR) communities. The goal is to ﬁnd
the relevant document from a large corpus given a
query. Traditionally, lexical based approaches try-
ing to match the query and document based on term
overlap, such as TF-IDF and BM25 (Robertson
and Zaragoza, 2009), have achieved great success
in this task. Recently, neural based approaches,
which go beyond the simple term matching, are be-
ing quickly adopted by the community and achieve
state-of-the-art performance on multiple retrieval
tasks, such as passage retrieval (Karpukhin et al.,
2020), question answering (Ahmad et al., 2019),
conversational question answering (Qu et al., 2020)
and bitext retrieval (Feng et al., 2020).
Dual encoders for neural retrieval. Dual en-
coders have demonstrated to be one type of neural
retrievers that can achieve great performance com-
pared to traditional sparse models such as BM25
for a wide range of retrieval tasks (Karpukhin et al.,
2020; Gillick et al., 2018). One key aspect to their
success is the adoption of pre-trained language
models, which enables the dual encoders to have
backbone contextual embeddings to initialize from.
Other techniques such as negative mining (Xiong
et al., 2020; Lu et al., 2021; Sachan et al., 2021)
and large training batch sizes (Qu et al., 2021) have
also shown great effectiveness. However, few of
the previous works have discussed the effect of the
backbone model’s capacity.
Zero-shot neural retrieval. Recent works have
shown great improvement under the zero-shot set-
ting for dual encoders by leveraging distillation
and synthetic data generation (Thakur et al., 2021;
Hofstätter et al., 2021; Ma et al., 2020). Both of
these techniques, and scaling up backbone mod-
els, are effective ways to close the gap between
dual encoders and the upper bound of the single-
product approaches with ﬁxed-dimension embed-
dings. On the other hand, multi-vector approaches
introduce more interactions between dense embed-
dings, which could also beneﬁt from scaling up the
backbone multi-vector encoders. We hope that our
observation about scaling up model sizes for single
dot-product based methods can be combined with
these techniques and further push the frontier of
neural retrieval models.
8 Inference latency
One caveat for scaling up model size is the incre-
ment in the latency overhead. We investigate the
inference speed in terms of microseconds (ms) for
all GTR models with batch size 1 and input length
128. We found the latency increases from 17 ms,
34 ms, 96 ms to 349 ms. The GTR-Base model
has close latency compared to TAS-B while the
largest GTR-XXL model has a similar latency to
the re-ranking models (Thakur et al., 2021). With
the recent work towards making large models efﬁ-
cient from angles such as sparsity, distillation and
prompt-tuning, we hope the inference time for large
dual encoders can be signiﬁcantly reduced in the
future.
9 Conclusion
This paper presents the Generalizable T5 Retriever
(GTR), a scaled-up dual encoder model with a
ﬁxed-size bottleneck layer. We show that scal-
ing up the model size brings signiﬁcant improve-
ment on retrieval performance across the board on
the BEIR zero-shot retrieval benchmark, especially



Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 6):

GTR-FT GTR-PT GTR
Fine-tuning   
NDCG@10 on MS Marco
Base 0.400 0.258 0.420
Large 0.415 0.262 0.430
XL 0.418 0.259 0.439
XXL 0.422 0.252 0.442
Zero-shot average NDCG@10 w/o MS Marco
Base 0.387 0.295 0.416
Large 0.412 0.315 0.445
XL 0.433 0.315 0.453
XXL 0.430 0.332 0.458
Table 5: Comparisons (NDCG@10) of the models
trained with and without pre-training and ﬁne-tuning.
Notably, the GTR-FT XL model already achieves an
average zero-shot NDCG@10 of 0.433, which outper-
forms the previous best dual encoder model TAS-B
(NDCG@10=0.415).
6.1 Effect of scaling up for different training
stages
The ﬁrst ablation study aims to investigate how
scaling up effects dual encoder pre-training and
ﬁne-tuning. Results are listed in table 5.
For ﬁne-tuning only models, scaling up beneﬁts
both in-domain and out-of-domain performance.
For pre-training only models, the improvement on
in-domain performance is not obvious; meanwhile
for out-of-domain tasks, scaling up also improves
the generalization. Finally with both pre-training
and ﬁne-tuning, GTR models consistently improve
over GTR-FT models of all sizes. This shows the
power of combining scaling up and a generic pre-
training stage.
6.2 Importance of the ﬁne-tuning dataset
In table 5, we compare GTR and GTR-PT on the
BEIR benchmark to understand the importance of
ﬁne-tuning on MS Marco. The table shows that
there is a clear gap between GTR models before
and after ﬁne-tuning. The result shows the neces-
sity of leveraging a high quality dataset (e.g. search
data) to ﬁne-tune the dual encoders.
In table 6, we compare ﬁne-tuning GTR on NQ
instead of MS Marco. Compared to MS Marco,
NQ only covers Wikipedia documents and is much
smaller in size, which allows us to investigate the
performance of GTR when ﬁne-tuned on a less
generalizable dataset. In addition, ﬁne-tuning on
NQ can give us a fair comparison with DPR.
As shown in table 6, the GTR-base model ﬁne-
Model Fine-tuning dataset Zero-shot aver-
age NDCG@10
DPR NQ 0.237
GTR-Base NQ 0.360
GTR-Large NQ 0.379
GTR-XL NQ 0.407
GTR-Large MS Marco 0.445
GTR-XL MS Marco 0.453
Table 6: Comparisons of GTR models ﬁne-tuned on
MS Marco and NQ. We report the zero-shot average
NDCG@10. Scaling up improves model performance
both on NQ and MS Marco.
tuned on NQ outperforms the original DPR model,
which uses a BERT-Base model as the encoder
backbone. This demonstrates the effectiveness of
our pre-training on the Web dataset as well as the
hard negatives introduced from Lu et al. (2021)
for NQ. Fine-tuning on NQ leads to inferior per-
formance compared to ﬁne-tuning on MS Marco,
which is consistent with prior work (Thakur et al.,
2021). However, importantly, scaling up GTR size
improves zero-shot performance on BEIR when
ﬁne-tuning on NQ. This shows that the beneﬁt of
scaling up holds for different ﬁne-tuning datasets.
Furthermore, when scaling from Large to XL, we
observe a larger gain when ﬁne-tuning with NQ
than with MS Marco, indicating that scaling up
helps more when using weaker ﬁne-tuning data.
6.3 Document length vs model capacity
Previously, BEIR has shown that models trained
with cosine similarity prefer short documents while
those trained with dot-product prefer long docu-
ments (Thakur et al., 2021). We investigate whether
scaling up affect this observation. Speciﬁcally, we
compute the median lengths (in words) of the top-
10 retrieved documents for all queries. Results are
shown in ﬁg. 5.
Though all GTR models are trained using co-
sine similarity, we found that scaling up the model
size has inﬂuence over the lengths of retrieved
documents. We observe an increasing trend of
document length for DB-Pedia, Fever, HotpotQA,
Signal-1M, Trec-News, and Web-Touche2020 with
scaling up. In particular, for Web-Touche2020, the
lengths of the retrieved documents grow drastically
as the models scale up: The largest GTR-XXL
retrieves documents that are on average twice as
long compared with the smallest GTR-Base. This
plays in our favor since Thakur et al. (2021) show
that the majority of relevant documents in Web-



Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 10):

Model NDCG@10 MRR@10 Recall@1000
ANCE 0.388 0.330 0.959
TAS-Balanced 0.408 0.340 0.975
ColBERT 0.401 0.360 0.968
RocketQA / 0.370 0.979
GTR-Base 0.420 0.366 0.983
GTR-Large 0.430 0.379 0.991
GTR-XL 0.439 0.385 0.989
GTR-XXL 0.442 0.388 0.990
Table 7: Comparisons of different models on MS
Marco. Scaling up can improve GTR models’ in-
domain performance.
A More results
A.1 Comparisons on MS Marco
Table 7 shows the comparisons of GTR models and
the baselines. Note that the best RocketQA model
used additional augmented data other than MS
Marco to improve the model performance while
all others do not. Our best GTR-XXL models out-
performs RocketQA on both MRR and recall.
A.2 Comparison of different dual encoder
pre-training strategies
In a concurrent work (Anonymous, 2022), re-
searchers proposed to conduct contrastive learning
(CL) pre-training for improving the generalizability
of neural retrievers. The paired data for contrastive
training is constructed from C4 and Wiki dataset
in an unsupervised way. In particular, they con-
struct pairs by randomly choosing two spans from
a single document and conduct word deletion or
replacement to each span. We compare the perfor-
mance of our GTR models to their models to gain
insights into different pretraining strategies for dual
encoders.
As shown in Figure 6, on over half of the
datasets, models with our pre-training approach
under-perform CL-Pretrain with the base size;
while as the model size increases, GTR-Large
and -XXL models show signiﬁcant gains over CL-
Pretrain. The best GTR-XXL model achieves
0.49 for NDCG@10 on average while CL-Pretrain
achieves 0.46. This demonstrates that scaling up
can mitigate the disadvantage of the potentially
inferior pre-training approach. Note that our pre-
training is additive to CL-Pretrain and we can lever-
age the pre-training on C4 and Wiki to further im-
prove the results. We leave this exploration as
future work.
0.66
0.54
0.56
0.58
0.5
Trec-Covid
0.32
0.31
0.33
0.340.34
NFCorpus
0.33
0.5
0.550.560.57
NQ
0.6
0.54
0.58
0.590.6
HotpotQA
0.24
0.35
0.420.44
0.47
FiQA-2018
0.32
0.510.520.530.54
ArguAna
0.37
0.2 0.220.23
0.26
Touché-2020
0.79
0.880.890.890.89
Quora
0.31
0.35
0.390.4
0.41
DBPedia-entity
0.16
0.15
0.160.160.16
SCIDOCS
CL
BaseLarge
XLXXL
0.75
0.66
0.710.72
0.74
Fever
CL
BaseLarge
XLXXL
0.21
0.24
0.26
0.270.27
Climate-Fever
CL
BaseLarge
XLXXL
0.66
0.6
0.640.64
0.66
SciFact
CL
BaseLarge
XLXXL
0.3
0.36
0.380.390.4
CQADupStack
CL
BaseLarge
XLXXL
0.410.42
0.440.450.46
Avg
Figure 6: Comparison with Anonymous (2022) on
NDCG@10. “CL” denotes Anonymous (2022) with
contrastive learning on C4 and Wiki while others de-
note our GTR models with different sizes. Note that
they only report results on 15 datasets of the BEIR
benchmark.
A.3 Recall on BEIR
Table 8 presents the Recall@100 of GTR mod-
els and the baselines. Similar to NDCG@10, we
observe that scaling up dual encoders lead to sig-
niﬁcant gains on the BEIR benchmark in terms of
recall.



Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 3):

The training process includes a pre-training stage
on a web-mined corpus and a ﬁne-tuning stage on
search datasets. The web-mined corpus provides a
large amount of semi-structured data pairs (such as
question-answer pairs and conversations), which
can provide rich semantic relevance information. It
is easy to collect but it is often not well annotated,
if at all. The search datasets are often annotated by
humans, and the queries and documents are also
authored by humans. These datasets are of high
quality but costly to collect.
In this work, for dual encoder pre-training, we
initialize the dual encoders from the T5 models
and train on question-answer pairs collected from
the Web. Recently, Sentence-T5 (Ni et al., 2021)
explored different ways to extract strong text em-
beddings and achieved remarkable performance
on SentEval and Sentence Textual Similarity tasks.
We follow that setting to encode queries and pas-
sages via mean pooling from the T5 encoders and
focus on the dense retrieval tasks.
For ﬁne-tuning, our aim is to adapt the model to
retrieval using a high quality search corpus so the
model can learn to better match generic queries to
documents. In this paper, we consider two datasets
for ﬁne-tuning: MS Marco (Nguyen et al., 2016)
and Natural Questions (Kwiatkowski et al., 2019).
4 Experimental setup
4.1 Training Data
Community QA. In order to leverage most of
the power from the large scale models, we col-
lect input-response pairs and question-answer pairs
from online forums and QA websites including
Reddit, Stack-Overﬂow, etc. This results in 2 bil-
lion question-answer pairs that we use to pre-train
the dual encoder.
MS Marco. We consider the MS Marco
dataset (Nguyen et al., 2016), which includes 532K
query and document pairs, as search data for ﬁne-
tuning. The dataset is sampled from Bing search
logs, which covers a broad range of domains
and concepts. Most of the neural models com-
pared in (Thakur et al., 2021) are trained on MS
Marco, including DeepCT (Dai and Callan, 2020),
DocT5Query (Nogueira, 2019), ANCE (Xiong
et al., 2020) and ColBERT (Khattab and Zaharia,
2020). Some of these models have shown great
generalization with comparable or even better per-
formance relative to BM25.
GTR Models Base Large XL XXL
# of params 110M 335M 1.24B 4.8B
Table 1: Number of parameters in the GTR models.
Models Dim. size
ColBERT 128
DPR, ANCE, TAS-B, GenQ, GTR 768
BM25, DocT5Query -
Table 2: Dimension size of different models. Most dual
encoder models set the embedding dimension size to
768.
Natural Questions. In the ﬁne-tuning stage,
we also consider the Natural Questions dataset
(Kwiatkowski et al., 2019) , which has been widely
used in the dense retrieval literature (Karpukhin
et al., 2020; Xiong et al., 2020). It consists of 130k
query and passage pairs which are also human-
annotated.
4.2 Conﬁgurations
We implement GTR models in JAX4 and train them
on Cloud TPU-V8. We consider different sizes of
the T5 transformer (Vaswani et al., 2017) architec-
ture including Base, Large, XL and XXL. Their
number of parameters are listed in table 1.
Note that we only use the encoder portion of the
T5 models and thus the number of parameters are
less than half of the full model size. We use the off-
the-shelf checkpoints as the initial parameters and
use the same sentencepiece vocabulary model.5
During pre-training and ﬁne-tuning, we set the
batch size to 2048 and use a softmax temperature
τ of 0.01. We use Adafactor optimizer (Shazeer
and Stern, 2018) and set the initial learning rate to
1e-3 with a linear decay. We train the model for
800K steps and 20K steps for the pre-training and
ﬁne-tuning stages, respectively.
For ﬁne-tuning, we use the hard negatives re-
leased by RocketQA (Qu et al., 2021) when ﬁne-
tuning with MS Marco data and the hard negatives
release by (Lu et al., 2021) for Natural Questions,
which were proven to lead to better retriever perfor-
mance. By default, we use the complete MS Marco
dataset and the NQ dataset for ﬁne-tuning.
When evaluating on the BEIR benchmark, we
use sequences of 64 tokens for the questions and
512 for the documents in all datasets except Trec-
4https://github.com/google/jax
5https://github.com/google-research/
text-to-text-transfer-transformer



Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 5):

0.420.430.440.44
0.23
MS MARCO
0.540.56
0.58
0.5
0.66
Trec-Covid
0.27
0.320.320.32
0.46
BioASQ
0.31
0.33
0.340.34
0.32
NFCorpus
0.5
0.550.560.57
0.33
NQ
0.54
0.580.590.6 0.6
HotpotQA
0.35
0.420.44
0.47
0.24
FiQA-2018
0.260.260.270.27
0.33
Signal-1M
0.340.340.350.35
0.4
Trec-News
0.44
0.470.48
0.51
0.41
Robust04
0.510.520.530.54
0.32
ArguAna
0.2 0.220.23
0.26
0.37
Touché-2020
0.880.890.890.89
0.79
Quora
0.35
0.390.4 0.41
0.31
DBPedia-entity
0.15
0.160.160.160.16
SCIDOCS
BaseLarge
XLXXLBM25
0.66
0.710.72
0.740.75
Fever
BaseLarge
XLXXLBM25
0.24
0.260.270.27
0.21
Climate-Fever
BaseLarge
XLXXLBM25
0.6
0.640.64
0.660.66
SciFact
BaseLarge
XLXXLBM25
0.36
0.380.390.4
0.3
CQADupStack
BaseLarge
XLXXLBM25
0.42
0.440.450.46
0.41
Avg
Figure 4: Comparison with BM25 on NDCG@10. The
GTR-Base model outperforms BM25 on 9 datasets and
the larger GTR models continue to improve on these 9
tasks. The GTR-XXL model catches up or surpasses
BM25 on the other 5 datasets and only under-performs
on 5 of the remaining tasks.
model already outperforms the previous best dense
retrieval model TAS-B as well as the best sparse
model DocT5Query. Scaling up to GTR-XXL
leads to another jump in retrieval performance.
Similar improvements are found on Recall@100
as shown in the Appendix’s table 8. On average,
the scaling up process demonstrates an encourag-
ing ascending trend that eventually outperforms all
baseline methods on all evaluation metrics. This
conﬁrms that scaling up is a valid path towards
generalizability.
Previously, dual encoders failed to match the
performance of BM25 for tasks that require better
lexical matching capabilities. Thus, we wanted to
investigate what kind of tasks can get improved
by scaling up the model size. Figure 4 presents a
detailed comparison of all sizes of GTR models
against the BM25 baseline.
For tasks like NQ where dual encoders have been
previously shown to be more effective than BM25,
increasing the model size continues to advance the
performance of dual encoders. This suggests scal-
ing up can further boost the head start of dense
models over sparse models on these datasets.
For tasks like BioASQ and NFCorpus, where
dual encoders previously struggled to match the
performance of BM25 for inherent reasons, we dis-
covered that scaling up consistently improves the
retrieval performance. In particular, for NFCor-
pus, our Base model under-performs BM25 but the
GTR-FT GTR
Ratio of data Large XL Large XL XXL
NDCG@10 on MS Marco
10% 0.402 0.397 0.428 0.426 -
100% 0.415 0.418 0.430 0.439 0.430
Zero-shot average NDCG@10 w/o MS Marco
10% 0.413 0.418 0.452 0.462 0.465
100% 0.412 0.433 0.445 0.453 0.458
Table 4: Comparisons of NDCG@10 for GTR models
trained with different amount of ﬁne-tuning data. With
only 10% of the MS Marco data, both GTR-FT and
GTR large and XL models achieve slightly worse in-
domain performance; meanwhile they obtain compara-
ble or even superior out-of-domain performance than
using the complete MS Marco data.
XL model outperforms BM25 by 5.5% (0.343 vs.
0.325). This exciting ﬁnding veriﬁes our assump-
tion that scaling up can further exploit the powerful
semantic matching capabilities of the dual encoder
models and enable them to ultimately outperform
BM25.
5.3 Data efﬁciency for large retrievers
To better understand the data efﬁciency for large
dual encoders, we trained models using different
portions of the MS Marco dataset during ﬁne-
tuning. In particular, we sampled a subset of the
training data by keeping only 10% of the training
queries as well as their relevant (positive) passages
and irrelevant (hard negative) passages.
As shown in table 4, using 10% of training data
reduces the in-domain performance of the GTR
models on MS Marco. For the GTR-FT (ﬁne-
tuning only) models, using 10% of the data leads
to a mixed result of out-of-domain performance.
On the other hand, for full GTR models, using
10% of the MS Marco dataset is sufﬁcient for ﬁne-
tuning. In particular, the GTR-Large, XL and XXL
models achieve comparable or even better OOD
performance than ﬁne-tuning on the complete MS
Marco dataset. This might suggest that GTR mod-
els have the beneﬁt of data efﬁciency and could use
less training data for domain adaptation.
6 Ablation Study and Analysis
In this section we present ablations and analysis to
further understand the effects of scaling up, the im-
pact of ﬁne-tuning and pre-training, and the trends
of the GTR model on different experimental condi-
tions.



#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 8):

Table 8: Comparison of different negative sampling strategies.
# negatives NFCorpus NQ FiQA Quora DBPedia Scifact Avg
In batch 32k 35.8 39.0 40.0 85.7 35.4 73.7 51.6
+ pre-batch 64k 29.4 27.2 29.4 84.6 25.0 64.3 43.3
MoCo 130k 29.7 36.1 32.0 81.6 29.9 63.6 45.5
performance for a wide range of tasks requiring single-vector text representations such as retrieval,
semantic textual similarity, and text matching. When further customized for downstream tasks, E5
achieves superior fine-tuned performance compared to existing embedding models with 40× more
parameters on the large, 56-task MTEB benchmark datasets.
References
[1] Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence
embeddings. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net, 2017. URL
https://openreview.net/forum?id=SyK00v5xx.
[2] Mikel Artetxe and Holger Schwenk. Massively multilingual sentence embeddings for zero-
shot cross-lingual transfer and beyond. Transactions of the Association for Computational
Linguistics, 7:597–610, 2019. doi: 10.1162/tacl_a_00288. URL https://aclanthology.
org/Q19-1038.
[3] David M. Blei, Andrew Y . Ng, and Michael I. Jordan. Latent dirichlet allocation. In
Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani, editors, Advances in Neural
Information Processing Systems 14 [Neural Information Processing Systems: Natural and
Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada] , pages
601–608. MIT Press, 2001. URLhttps://proceedings.neurips.cc/paper/2001/hash/
296472c9542ad4d4788d543508116cbc-Abstract.html.
[4] Alexander Bondarenko, Maik Fröbe, Johannes Kiesel, Shahbaz Syed, Timon Gurcke, Meriem
Beloucif, Alexander Panchenko, Chris Biemann, Benno Stein, Henning Wachsmuth, et al.
Overview of touché 2022: argument retrieval. In International Conference of the Cross-
Language Evaluation Forum for European Languages, pages 311–336. Springer, 2022.
[5] Vera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. A full-text learning to rank
dataset for medical information retrieval. In European Conference on Information Retrieval,
pages 716–722. Springer, 2016.
[6] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. InProceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing , pages 632–642, Lisbon, Portugal,
2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https:
//aclanthology.org/D15-1075.
[7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learn-
ers. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-
12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.
[8] Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh
Tiwary, Rangan Majumder, Li Deng, and Bhaskar Mitra. Ms marco: A human generated
machine reading comprehension dataset. ArXiv, abs/1611.09268, 2016.
9



Source: data\tc18_2309.07597v5\referenced_papers\[37]_2201.10005.pdf (Page 0):

Text and Code Embeddings by Contrastive Pre-Training
Arvind Neelakantan* 1 Tao Xu* 1 Raul Puri1 Alec Radford1 Jesse Michael Han1 Jerry Tworek1
Qiming Yuan1 Nikolas Tezak1 Jong Wook Kim1 Chris Hallacy1 Johannes Heidecke1 Pranav Shyam1
Boris Power1 Tyna Eloundou Nekoul1 Girish Sastry1 Gretchen Krueger1 David Schnurr1
Felipe Petroski Such1 Kenny Hsu1 Madeleine Thompson1 Tabarak Khan1 Toki Sherbakov1 Joanne Jang1
Peter Welinder1 Lilian Weng1
Abstract
Text embeddings are useful features in many
applications such as semantic search and com-
puting text similarity. Previous work typically
trains models customized for different use cases,
varying in dataset choice, training objective and
model architecture. In this work, we show that
contrastive pre-training on unsupervised data at
scale leads to high quality vector representations
of text and code. The same unsupervised text em-
beddings that achieve new state-of-the-art results
in linear-probe classiﬁcation also display impres-
sive semantic search capabilities and sometimes
even perform competitively with ﬁne-tuned mod-
els. On linear-probe classiﬁcation accuracy aver-
aging over 7 tasks, our best unsupervised model
achieves a relative improvement of 4% and 1.8%
over previous best unsupervised and supervised
text embedding models respectively. The same
text embeddings when evaluated on large-scale
semantic search attains a relative improvement
of 23.4%, 14.7%, and 10.6% over previous best
unsupervised methods on MSMARCO, Natural
Questions and TriviaQA benchmarks, respec-
tively. Similarly to text embeddings, we train
code embedding models on (text, code) pairs, ob-
taining a 20.8% relative improvement over prior
best work on code search.
1. Introduction
Deep unsupervised learning with generative and embed-
ding models has seen dramatic success in the past few
years. Generative models (Peters et al., 2018; Raffel et al.,
2019; van den Oord et al., 2016; Ramesh et al., 2021;
Brown et al., 2020; Chen et al., 2021) are trained to max-
*Equal contribution 1OpenAI. Correspondence to: Arvind
Neelakantan <arvind@openai.com>.
S-300M M-1.2B L-6B XL-175B
Model Size
60
62
64
66
68
70Performance
Average performance vs model size
Figure 1.Average performance of unsupervised cpt-text
models of different sizes across 22 tasks consisting of linear-probe
classiﬁcation, text search, and sentence similarity tasks.
imize the likelihood of observed data while embedding
models are trained to distinguish observed data from noise
(Sohn, 2016; van den Oord et al., 2018; Radford et al.,
2021; Jia et al., 2021; Gao et al., 2021; Izacard et al., 2021).
Generative models have been shown to produce realistic
content and beneﬁt many downstream applications, reduc-
ing the need for labeled training datasets. In generative
models, the information about the input is typically dis-
tributed over multiple hidden states of the model. While
some generative models (Kingma & Welling, 2014; Kiros
et al., 2015) can learn a single representation of the in-
put, most autoregressive Transformer (Vaswani et al., 2017)
models do not (Raffel et al., 2019; Brown et al., 2020; Chen
et al., 2021; Ramesh et al., 2021). However, learning such a
representation (or embedding) is necessary for many tasks.
Systems that search over millions or billions of items re-
quire each entry to be embedded as a dense representation
and build an index in advance to save computational costs
at query time. These embeddings are useful features for
classiﬁcation tasks and can also enable data visualization
applications via techniques such as clustering. Embedding
models are explicitly optimized to learn a low dimensional
representation that captures the semantic meaning of the
input (Radford et al., 2021; Jia et al., 2021; Giorgi et al.,
2020; Gao et al., 2021; Izacard et al., 2021).
arXiv:2201.10005v1  [cs.CL]  24 Jan 2022



Source: data\tc18_2309.07597v5\referenced_papers\[28]_2308.03281.pdf (Page 0):

Towards General Text Embeddings with Multi-stage Contrastive Learning
Zehan Li1, Xin Zhang1, Yanzhao Zhang1, Dingkun Long1, Pengjun Xie1, Meishan Zhang
1Alibaba Group
{lizehan.lzh,linzhang.zx,zhangyanzhao.zyz,
dingkun.ldk,pengjun.xpj}@alibaba-inc.com
Abstract
We present GTE, a general-purpose text embed-
ding model trained with multi-stage contrastive
learning. In line with recent advancements in
unifying various NLP tasks into a single for-
mat, we train a unified text embedding model
by employing contrastive learning over a di-
verse mixture of datasets from multiple sources.
By significantly increasing the number of train-
ing data during both unsupervised pre-training
and supervised fine-tuning stages, we achieve
substantial performance gains over existing em-
bedding models. Notably, even with a relatively
modest parameter count of 110M, GTEbase out-
performs the black-box embedding API pro-
vided by OpenAI and even surpasses 10x larger
text embedding models on the massive text
embedding benchmark. Furthermore, without
additional fine-tuning on each programming
language individually, our model outperforms
previous best code retrievers of similar size by
treating code as text. In summary, our model
achieves impressive results by effectively har-
nessing multi-stage contrastive learning, offer-
ing a powerful and efficient text embedding
model with broad applicability across various
NLP and code-related tasks.1
1 Introduction
Text embeddings have became an indispensable
component in many natural language processing
tasks, such as text classification, text retrieval, ques-
tion answering and dialogue systems (Karpukhin
et al., 2020; Humeau et al., 2020; Choi et al., 2021;
Izacard et al., 2022a; Long et al., 2022a; Rajapakse,
2023). These embedding models represent texts us-
ing low-dimensional vectors and capture their sim-
ilarity through vector operations. The emergence
of recent large language models (LLMs) (Radford
et al., 2018; Touvron et al., 2023; OpenAI, 2023)
has generated considerable interest in retrieval-
1The GTE model is publicly available at https://
huggingface.co/thenlper/gte-large
UnsupervisedContrastive Pre-training on Massive Text Pairs mined from the WebSupervisedContrastive Fine-tuningon Annotated Text Triples from Multiple Tasks
… …
MSMARCO
NaturalQuestionsTriviaQAWebQuestionsHotpotQAMNLI
QuoraStackExchangeDupWebSearch
OpenQA NaturalLanguageInferenceSNLI
FactVerificationFEVER
Paraphrase
MEDIOthersBERRI
Figure 1: Illustration of the multi-stage contrastive learn-
ing pipeline used to train our text embedding model.
augmented systems based on text embedding mod-
els that integrate the reasoning and comprehension
capabilities of LLMs (Izacard et al., 2022b; Ram
et al., 2023; Shi et al., 2023). Consequently, there
has been a growing focus on general text represen-
tation in both industry and academia.
The pursuit of developing a unified model to ad-
dress a multitude of downstream tasks has been
long-standing due to the diverse formats, domains
and downstream applications of natural language.
The emergence of pre-trained language models has
further opened up possibilities for training such a
universal model. Nonetheless, within the realm
of text representation research, previous text em-
bedding models have primarily focused on specific
tasks, and their training strategies or models, tai-
lored to a single task, may not perform optimally
in other contexts. For example, the text represen-
tation model SimCSE (Gao et al., 2021), trained
on symmetric text pairs, demonstrates limitations
in text retrieval tasks. Similarly, certain text rep-
resentation models specifically designed for dense
retrieval tasks do not exhibit robust performance
in sentence textual similarity tasks. Recently, there
has been a shift in research focus towards develop-
ing more comprehensive models for text represen-
tation leveraging large quantities of unlabeled web
data through unsupervised contrastive pre-training,
coupled with task-specific data, prompts, or in-
structions to mitigate task conflicts during fine-
tuning (Ni et al., 2022a,b; Neelakantan et al., 2022;
arXiv:2308.03281v1  [cs.CL]  7 Aug 2023



Source: data\tc18_2309.07597v5\referenced_papers\[28]_2308.03281.pdf (Page 10):

α Retrieval STS MTEB
0 36.7 73.2 55.4
0.3 44.6 75.9 58.9
0.5 44.2 76.5 59.0
1 42.0 75.5 58.3
Table 10: Influence of ratio α used in pre-training data
sampling.
training task ( α = 0) nor directly combining all
data sources (α = 1) is the best choice. Setting α
to 0.5 can improve results on all tasks.
5.5 Ablation of the Contrastive Objective
This work uses an improved contrastive objective
which can efficiently enlarges the negative pool
under fixed batch size. We compare it against the
vanilla contrastive loss with only in-batch negatives
in both pre-training and fine-tuning stages.
Setting PT FT
Vanilla 57.3 61.8
Improved 57.8 62.4
Table 11: Comparison of the vanilla contrastive loss
with in-batch negatives, and the improved contrastive
loss with enlarged negative pool. For ablation we run the
pre-training (PT) for 30k steps to reduce computational
cost. We report average score on MTEB.
According to Table 11, using improved con-
trastive loss consistently improves model perfor-
mance in both pre-training and fine-tuning stages.
6 Discussion
Despite the strong performance on English tasks,
our current model can only handle text with a
length of less than 512, as it is initialized from
BERT and lacks multilingual capabilities. Conse-
quently, longer texts must be truncated or split for
encoding. However, with more data engineering
and compute resources, the described training ap-
proach could easily be extended to a multilingual
version and accommodate longer contexts.
Another issue is the problem of data contami-
nation resulting from large-scale pre-training on
Internet data. Currently, we only conduct dedupli-
cation based on exact matching of text pairs, which
is an overly strict filter. This issue has also been
highlighted by Brown et al. (2020) during the train-
ing of large-scale generative language models. We
suspect that this is a common problem that other
models also suffer from, but quantifying it without
details about the training data sources is even more
challenging (Neelakantan et al., 2022).
Furthermore, the models trained in this study
are based on a non-causal architecture with bidi-
rectional context attention. It would be intriguing
to explore similar pre-training methods for causal
or prefix language models, as these models could
optimize generation and retrieval jointly and unify
them within a single model.
7 Conclusion
This paper presents a multi-stage contrastive learn-
ing approach to develop text embedding model that
can be applied to various tasks. Our model benefits
from a diverse training data mixture, enabling it to
achieve good generalization performance for single
vector embedding. Through extensive evaluation
on multiple benchmarks, we demonstrate the ef-
fectiveness and versatility of our text embedding
model. Our future work will focus on scaling the
model to support longer context, extending it to
support multilingual and multi-modal applications,
as well as exploring the benefits of prompts and
instructions.
References
Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen,
Gautier Izacard, Sebastian Riedel, Hannaneh Ha-
jishirzi, and Wen-tau Yih. 2023. Task-aware retrieval
with instructions. In Findings of the Association for
Computational Linguistics: ACL 2023, pages 3650–
3675, Toronto, Canada. Association for Computa-
tional Linguistics.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yim-
ing Yang, and Sanjiv Kumar. 2020. Pre-training tasks
for embedding-based large-scale retrieval. In Inter-
national Conference on Learning Representations.



Source: data\tc18_2309.07597v5\referenced_papers\[28]_2308.03281.pdf (Page 1):

Wang et al., 2022b; Su et al., 2023). Additionally,
the introduction of benchmarks, such as the Mas-
sive Text Embedding Benchmark (MTEB) (Muen-
nighoff et al., 2023), has established a robust basis
for assessing the universality of text representation
models. However, a significant limitation in ex-
isting research is the reliance on in-house data for
pre-training, creating a bottleneck in the utilization
of pre-trained model weights or APIs. Furthermore,
the formulation of prompts specifically tailored for
each task requires extra human effort during imple-
mentation (Su et al., 2023).
This work presents a straightforward approach
to construct a general text embedding (GTE) model
solely using contrastive learning on open-source
data, as illustrated in Figure 1. Specifically, we
first gather a large-scale dataset comprising un-
supervised text pairs extracted from various data
sources for contrastive pre-training. Surprisingly,
our model, pre-trained on this dataset, exhibits re-
markable performance, surpassing BM25 and E5
model (Wang et al., 2022b) in zero-shot text re-
trieval tasks and surpassing many supervised mod-
els in the MTEB benchmark. To further enhance
the quality of the learned text representations, we
obtain high-quality text pairs with human labels
from multiple sources for contrastive fine-tuning.
After supervised fine-tuning, our 110M BERT-
based (Devlin et al., 2019) model already outper-
forms the current commercial embedding API of
OpenAI and ranks highly in the MTEB benchmark.
Furthermore, since our model is trained using code
data as well, we evaluate its code search capabili-
ties on the CodeSearchNet benchmark, which en-
compasses six programming languages. Notably,
even without language-specific fine-tuning on each
subset, our model significantly outperforms state-
of-the-art code retrievers of similar size that have
been fine-tuned for each programming language.
In the rest of this paper, we provide a detailed
account of the data sources and training configu-
rations employed. Subsequently, we present the
evaluation results on widely recognized text em-
bedding benchmarks and compare them with the
performance of previous state-of-the-art baselines
that were specifically optimized for each individual
task. Our model consistently demonstrates supe-
rior performance or, at the very least, comparable
results to those achieved by larger models, owing
to its incorporation of a more diverse mixture of
training datasets. We aspire for our model to serve
as a robust baseline for the research community
investigating text and code embedding.
2 Related Work
Text embeddings serve as low-dimensional vector
representations for texts of varying lengths and are
essential in numerous natural language processing
(NLP) tasks. In contrast to high-dimensional and
sparse representations such as TF-IDF, dense text
embeddings possess the capacity to address the lex-
ical mismatch problem and enhance the efficiency
of text retrieval and matching.
Pre-trained language models, exemplified by
BERT (Devlin et al., 2019) and GPT (Radford
et al., 2018), have demonstrated remarkable suc-
cess across various NLP tasks. Nonetheless, ex-
tracting a high-quality sentence embedding from
pre-trained language models poses a significant
challenge due to the presence of anisotropic em-
bedding spaces resulting from the masked language
modeling objective. To address this issue, subse-
quent studies have proposed different approaches,
including supervised fine-tuning (Reimers and
Gurevych, 2019), normalizing flow (Li et al., 2020),
normalizing flow (Li et al., 2020), whitening (Su
et al., 2021), or unsupervised contrastive learn-
ing (Gao et al., 2021). These investigations pri-
marily concentrate on enhancing performance in
semantic textual similarity tasks, wherein two sen-
tences exhibit similar formats.
Another line of research focuses on the text re-
trieval problem, where the query and document
typically exhibit an asymmetric relationship. In
this context, the dual-encoder architecture necessi-
tates training with both positive and negative pairs.
Lee et al. (2019) propose the Inverse Close Task
(ICT) as a self-supervised pre-training approach for
generating a dense retriever. The ICT method in-
volves cropping a random sentence from a passage
to construct pseudo query-document pairs. Ad-
ditionally, Chang et al. (2020) leverage the link
structure within Wikipedia to introduce further su-
pervision signals in the pre-training data. In a sim-
ilar vein, REALM (Guu et al., 2020) proposes a
joint training approach, wherein a dense retriever
and a language model are trained concurrently. The
learning signal for the language model is derived
from masked language modeling, with backpropa-
gation incorporated through the retrieval step. Re-
cent advancements, such as Contriever (Izacard
et al., 2022a) and coCondenser (Gao and Callan,



#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[39]_2108.08877.pdf (Page 0):

Sentence-T5: Scalable Sentence Encoders
from Pre-trained Text-to-Text Models
Jianmo Ni, Gustavo Hernández Ábrego, Noah Constant, Ji Ma,
Keith B. Hall, Daniel Cer, Yinfei Yang
Google Research
Mountain View, CA
Abstract
We provide the ﬁrst exploration of sen-
tence embeddings from text-to-text transform-
ers (T5). Sentence embeddings are broadly
useful for language processing tasks. While
T5 achieves impressive performance on lan-
guage tasks cast as sequence-to-sequence map-
ping problems, it is unclear how to produce
sentence embeddings from encoder-decoder
models. We investigate three methods for ex-
tracting T5 sentence embeddings: two utilize
only the T5 encoder and one uses the full T5
encoder-decoder model. To support our inves-
tigation, we establish a new sentence represen-
tation transfer benchmark, SentGLUE, which
extends the SentEval toolkit to nine tasks from
the GLUE benchmark (Wang et al., 2018). Our
encoder-only models outperforms Sentence-
BERT (Reimers and Gurevych, 2019) and
SimCSE (Gao et al., 2021) sentence embed-
dings on both SentEval and SentGLUE trans-
fer tasks, including semantic textual similarity
(STS). Scaling up T5 from millions to billions
of parameters is found to produce consistent
further improvements. Finally, our encoder-
decoder method achieves a new state-of-the-
art on STS when using sentence embeddings.1
1 Introduction
Sentence embeddings providing compact mean-
ing representations that are broadly useful for a
variety of language processing tasks include clas-
siﬁcation, question-answering, semantic retrieval,
bitext mining, and semantic similarity tasks. Sen-
tence embedding models have been trained using
a variety of methods including: supervised tasks
such as natural language inference (Conneau et al.,
2017; Gao et al., 2021) or with semi-structured data
such as question-answer pairs (Cer et al., 2018);
translation pairs (Yang et al., 2020a; Feng et al.,
2020); paraphrasing pairs (Wieting et al., 2016)
1Our models are released at https://tfhub.dev/
google/collections/sentence-t5/1.
Figure 1: Scaling up our ST5 model size improves per-
formance on SentEval (left) and STS (right).
Transfer STS
ST5-EncDec (11B params) 90.46 84.94
ST5-Enc (11B params) 91.63 84.96
SimCSE-RoBERTa (large) (Gao et al., 2021)90.232 83.76
SBERT (large) (Reimers and Gurevych, 2019)87.69 76.55
USE (Cer et al., 2018) 85.10 71.22
InferSent (Conneau et al., 2017) 85.59 65.01
Table 1: ST5 versus notable sentence embedding mod-
els on SentEval tasks. The reported numbers are the
average of transfer tasks and STS tasks
and adjacent sentence pairs (Kiros et al., 2015;
Logeswaran and Lee, 2018). Recent work has
shown that scaling up model parameters and lever-
aging pre-trained models (Devlin et al., 2019; Liu
et al., 2019) are two effective approaches to im-
prove performance (Reimers and Gurevych, 2019,
2020; Yang et al., 2020b; Gao et al., 2021).
We explore sentence embeddings from a new
family of pre-trained models: Text-to-Text Trans-
fer Transformer (T5) (Raffel et al., 2020). Unlike
encoder-only models, which use a transformer en-
coder to predict random masked tokens, T5 uses
an encoder-decoder architecture and a generative
span corruption pre-training task. T5 models can
be scaled up to hundreds of billions of parameters
(Fedus et al., 2021) and have achieved state-of-the-
2SimCSE-RoBERTa achieves the best performance on
transfer tasks by adding an additional masked language model
loss during training while ST5 and other models don’t.
arXiv:2108.08877v3  [cs.CL]  14 Dec 2021



Source: data\tc18_2309.07597v5\referenced_papers\[39]_2108.08877.pdf (Page 1):

x1 x2 x3 x4
Encoder Decoder y1 y2 y3 ・
(a) T5 Encoder-Decoder
x1 x2 x3 x4
Encoder (b) ST5 Encoder-only
(ST5-Enc) ﬁrst
x1 x2 x3 x4
Encoder
(c) ST5 Encoder-only
(ST5-Enc) mean
x1 x2 x3 x4
Encoder Decoder (d) ST5 Encoder-Decoder
(ST5-EncDec) ﬁrst
Figure 2: Architecture diagrams for T5 and three ST5 variants to extract sentence representations from T5.
art performance on a broad range of NLP tasks
including GLUE (Wang et al., 2018) and Super-
GLUE (Wang et al., 2019). However, it is difﬁcult
to efﬁciently apply T5 to some tasks such as re-
trieval or clustering. To score retrieval candidates,
T5 would need to perform full inference with cross-
attention on each query-candidate pair. In contrast,
sentence embeddings allow for efﬁcient retrieval
and clustering (Gillick et al., 2018; Reimers and
Gurevych, 2019; Yang et al., 2020a).
As shown in ﬁg. 2, we explore three ways of
turning a pre-trained T5 encoder-decoder model
into a sentence embedding model: (i) using the
ﬁrst token representation of the encoder; (ii) aver-
aging all token representations from the encoder;
(iii) using the ﬁrst token representation from the
decoder. We evaluate the quality of the resulting
sentence embeddings on sentence transfer tasks us-
ing SentEval (Conneau and Kiela, 2018) and on se-
mantic textual similarity (Agirre et al., 2012, 2013,
2014, 2015, 2016; Cer et al., 2017). We contrast
raw representations from pre-trained T5 models
with those learned through ﬁne-tuning on natural
language inference (NLI) and Retrieval Question-
Answering (ReQA) (Ahmad et al., 2019) using dual
encoders and contrastive learning (Conneau et al.,
2017; Cer et al., 2018; Yang et al., 2018; Gao et al.,
2021). We introduce a multi-stage contrastive learn-
ing recipe involving ﬁne-tuning ﬁrst on ReQA and
then on NLI. Finally, we investigate scaling our T5
sentence embedding model up to 11B parameters.
As shown in ﬁg. 1, transfer tasks and STS both
improve with increased model capacity.
To our knowledge, we are the ﬁrst to study us-
ing large-scale pre-trained text-to-text models for
sentence representation learning and to scale sen-
tence embedding models up to 11 billion parame-
ters. We summarize our contributions as follows:
(i) even without ﬁne-tuning, encoder-only ST5
models perform well on sentence transfer tasks,
outperforming state-of-the-art ﬁne-tuned models
such as SimBERT and SimRoBERTa (Gao et al.,
2021); (ii) encoder-decoder sentence embedding
models achieve strong performance on STS, es-
tablishing a new state-of-the-art on sentence em-
bedding based STS; (iii) contrastive learning is
effective for ﬁne-tuning sentence encoders from
T5-style pre-trained models, particularly using our
proposed two-stage contrastive learning approach;
(iv) training ST5 longer and with more data using a
contrastive loss leads to consistent improvement on
both sentence transfer and STS tasks; (v) creating
a new sentence representation transfer benchmark
‘SentGLUE’ which extends the sentence evaluation
toolkit (Conneau and Kiela, 2018) to nine tasks
from GLUE (Wang et al., 2018) benchmark and
evaluating ST5 and other state-of-the-art models on
SentGLUE to compare their transfer performance
on these challenging tasks. We name our model
Sentence T5 (ST5).
2 Text-to-Text Transfer
Transformers (T5)
Text-to-Text transfer transformers (T5) (Raffel
et al., 2020) are gaining popularity due to their
competitive performance and ease of use in solving
a variety of tasks as simple text-to-text mapping
problems. As shown in ﬁg. 2a, T5 consists of an
encoder-decoder transformer model (Vaswani et al.,
2017) pre-trained on an unsupervised span corrup-
tion task. Though T5 has been successfully applied
to numerous NLP tasks, how to extract high quality
text representations from T5 remains unexplored.



Source: data\tc18_2309.07597v5\referenced_papers\[39]_2108.08877.pdf (Page 2):

3 Sentence T5
3.1 Model Architecture
In this work we explore three strategies to extract
sentence representations from T5, as shown in
ﬁgs. 2b to 2d:
• Encoder-only ﬁrst (ST5-Enc ﬁrst): The en-
coder output of the ﬁrst token is taken as the
sentence embedding.
• Encoder-only mean (ST5-Enc mean): The
sentence embedding is deﬁned as the average
of the encoder outputs across all input tokens.
• Encoder-Decoder ﬁrst (ST5-EncDec ﬁrst):
The ﬁrst decoder output is taken as the sen-
tence embedding. To obtain the decoder out-
put, the input text is fed into the encoder, and
the standard “start” symbol is fed as the ﬁrst
decoder input.
The ﬁrst two are pooling strategies widely used
in encoder-only pre-trained models such as BERT.
Unlike BERT models, T5 models do not have a
CLS token at the beginning of each sentence. For
T5 encoder-decoder models, we assume the de-
coder is aware of the semantics of the entire in-
put sentence when generating its ﬁrst token predic-
tion; and if so, the ﬁrst decoder output embeddings
(i.e. input to the softmax layer) might naturally
capture the sentence semantics.
For sentence encoder training, we adopt a dual
encoder architecture (Gillick et al., 2018; Cer et al.,
2018; Reimers and Gurevych, 2019). As shown
in ﬁg. 3, this architecture consists of two shared-
weight transformer modules that encode the inputs.
The transformer module can be either an encoder-
only or encoder-decoder architecture. In our ex-
periments, we initialize the transformer modules
from the pre-trained T5 models. After each module
computes a ﬁxed-length representation for its input
sentence, a projection layer and L2 normalization
are applied to the resulting embeddings. The pro-
jection layer transforms the output to a conﬁgurable
ﬁxed dimensionality (i.e. the sentence embedding
size). The embeddings from paired encoding tow-
ers can be scored for similarity tasks using a dot-
product3 or provide as input to additional layers
layers for pairwise classiﬁcation tasks (e.g., NLI).
3Since L2 normalization is applied to the output of each
tower, the dot-product between the embeddings will produce
their cosine similarity.
Transformer Encoder 
(optional Decoder)
Transformer Encoder 
(optional Decoder)
Sentence 1 Sentence 2
Embedding 1 Embedding 2
Loss
Projection & Norm Projection & Norm
Figure 3: Architecture of the dual encoder model.
3.2 Contrastive Learning
Applying contrastive learning to sentence embed-
dings improves the uniformity of the embeddings
space, leading to better performance on down-
stream tasks such as STS (Gao et al., 2021). We
apply contrastive learning to ﬁne-tune the T5 sen-
tence representations.4
3.2.1 Contrastive Loss
Using a contrastive loss to train a sentence encoder
requires paired examplesD= {(vi,v+
i )}as a train-
ing set, where vi is an input sentence and v+
i is a
related sentence (e.g., that is semantically close).
During training, v+
i is considered as a positive ex-
ample for vi and all other examples in the batch are
considered as negatives. The model should learn
to pull the positive example closer to the input
example while pushing away the negatives. We op-
erationalize our contrastive loss using an in-batch
sampled softmax (Henderson et al., 2017):
L= esim(vi,v+
i )/τ
∑
j∈Besim(vi,v+
j )/τ, (1)
The similarity scoring function is sim. Bis a mini-
batch of examples andτis the softmax temperature.
When additional negatives v−
j are provided for in-
put example v, the loss can be computed as:
L= esim(vi,v+
i )/τ
∑
j∈Besim(vi,v+
j )/τ + esim(vi,v−
j )/τ. (2)
4In preliminary experiments, we also explored ﬁne-tuning
with the classiﬁcation loss used in InferSent (Conneau et al.,
2017) and Sentence-BERT (Reimers and Gurevych, 2019).
However we found ﬁne-tuning for classiﬁcation on an NLI
dataset is inferior to contrastive learning as reported in (Gao
et al., 2021)



Source: data\tc18_2309.07597v5\referenced_papers\[39]_2108.08877.pdf (Page 7):

Model Sent. Embed. Fine-tuningScore CoLA SST-2 MRPC STS-B QQP MNLI-m MNLI-mm QNLI RTE
InferSent (Wang et al., 2018) NLI 66.71 8.60 83.90 76.50 80.20 81.70 67.80 - 63.50 71.50
SBERT (RoBERTa Base)♣ NLI+MNLI 73.40 21.22 90.83 73.34 74.08 80.75 77.21 78.13 73.92 57.76
SBERT (RoBERTa Large)♣ NLI+MNLI 75.81 20.69 93.00 73.39 76.26 82.26 79.46 80.18 75.80 60.65
SimCSE (RoBERTa Base)♣ NLI 77.05 35.87 90.71 76.47 83.93 81.39 70.74 72.05 76.30 60.29
SimCSE (RoBERTa Large)♣ NLI 76.23 40.11 93.23 70.23 81.45 84.45 73.44 73.56 75.95 60.65
ST5 Enc (Base) CommQA+NLI 76.89 22.73 91.40 76.88 86.58 84.55 69.73 70.00 79.54 61.73
ST5 Enc (Large) CommQA+NLI 78.52 29.46 93.92 77.26 86.07 85.32 72.20 72.44 79.64 66.43
ST5 Enc (3B) CommQA+NLI 79.06 34.78 94.95 78.71 85.84 85.78 72.38 73.10 79.70 66.06
ST5 Enc (11B) CommQA+NLI 80.07 43.91 95.30 78.46 86.54 86.21 73.46 74.42 80.12 66.06
ST5 Enc 1.1 (Base) CommQA+NLI 76.63 21.59 90.60 76.66 86.34 84.53 70.40 70.76 77.92 61.01
ST5 Enc-Dec (Base) NLI 76.37 17.46 90.71 76.44 85.98 82.65 70.49 70.96 76.20 63.18
T5 (Base) (Raffel et al., 2020) - 83.40 53.84 92.68 88.92 88.02 91.56 84.24 84.57 90.48 76.28
Table 6: Performance on transfer tasks on the Dev set of the GLUE benchmark. ♣ denotes that the models are
released by HuggingFace. T5 (base) is a cross-attention model and other models are embedding based.
Enc-11B
EncDec-
11B
Enc-3B
Enc-Large Enc-Base
EncDec-
3B
EncDec-
Large EncDec-
BaseScaling up
Figure 4: Alignment and uniformity losses for different
model sizes. We consider the test split of the STS-B
dataset. Lalign is calculated considering all pairs with
score greater than 4. Luniform is computed using all sen-
tences. The colormap denotes the models’ Spearman’s
correlation score.
8 SentGLUE Evaluation
In this section we introduce a new sentence repre-
sentation transfer benchmark – SentGLUE – which
extends the sentence evaluation toolkit to nine
challenge tasks from GLUE benchmark includ-
ing: CoLA, SST-2, MRPC, STS-B, QQP, MNLI-m,
MNLI-mm, QNLI, RTE 8. The GLUE benchmark
has been widely adopted for measuring language
understanding models. GLUE tasks are either sin-
gle sentence or sentence pair classiﬁcation (e.g.
NLI) or similarity (STS) tasks. The best mod-
els on the GLUE leaderboard are ﬁne-tuned cross-
attention models like BERT or T5. Such models
change all the parameters in the underlying model
during ﬁne-tuning and for the pairwise tasks they
allow for early fusion of input features from both
sentences being compared. For SentGLUE, we
introduce the constraint that each input needs to
8We found the WNLI task from GLUE benchmark is too
challenge for existing sentence embedding models, thus we
exclude it in the current version.
be independently encoded into a ﬁxed embedding
space representation that can then be feed to ad-
ditional layers in order to make a prediction. We
believe this best adapts the spirit of the original
SentEval benchmark for sentence embeddings to
the GLUE benchmark tasks.
From table 6, ST5-Enc Base outperforms both
SBERT-RoBERTa Base and SimCSE-RoBERTa
Base on all SentGLUE tasks except CoLA and
MNLI.9 With its increased model capacity, ST5
Enc 11B’s sentence embeddings achieve the best
overall performance. Notably, as model size is
scaled up, aggregate performance using sentence
embeddings approaches that of T5 base. This is re-
markable given that T5 base makes use of full cross-
attention between sentence pairs and adjusts all of
the parameters in the model during ﬁne-tuning.
9 Conclusion
In this paper, we study effective methods to build
T5 sentence encoders (ST5) from pre-trained mod-
els. We propose three architectures and a two-stage
contrastive learning method to ﬁne-tune ST5. We
compare the difference between encoder-only and
encoder-decoder architectures as sentence encoders
and analyze their performance on downstream
tasks. Through extensive experiments on the Sent-
Eval benchmark, we show that encoder-only mod-
els have strong transfer performance while encoder-
decoder models perform better on textual similarity
tasks. We also demonstrate the effectiveness of
scaling up the model size, which greatly improves
sentence embedding quality. These ﬁndings sug-
gest that future improvements in scale and quality
of pre-trained text-to-text models may translate into
further gains for sentence encoder models.
9MNLI-m and MNLI-mm experiments for SBERT
RoBERTa Large and SimCSE RoBERTa Base are still running
at the time of submission.



Source: data\tc18_2309.07597v5\referenced_papers\[39]_2108.08877.pdf (Page 3):

3.3 Two-stage Training
To investigate the effect of additional training data,
we explore two-stage training: (i) ﬁrst training on
mined question-answering data from Community
QA sites; (ii) then, ﬁne-tune the model on sentence
pairs with human annotated NLI labels.
4 Experimental Setup
4.1 Training Corpus
For two-stage training, we use two datasets: one is
collected from web forums while the other is from
the Stanford Natural Language Inference (SNLI)
dataset (Bowman et al., 2015). For the ﬁrst stage
and similar to Cer et al. (2018), we collect 2 Bil-
lion question-answers pairs from community QA
websites. During training, the associated answer
is considered as the positive example for each in-
put question. For the second stage, we utilize the
contrastive version of the NLI dataset (Gao et al.,
2021) containing 275K examples, where the posi-
tives are the ‘entailment’ hypothesis and premise
pairs while the negatives are the ‘contradict’ pairs.
4.2 Evaluation
We evaluate using SentEval, which includes 7 trans-
fer and 7 STS tasks (Conneau and Kiela, 2018). For
the transfer tasks, sentence embeddings are eval-
uated by how well they perform as features for a
linear classiﬁcation model. For STS, embeddings
are evaluated by how well their cosine similarities
correlate with human annotated similiarity scores.5
4.3 Conﬁgurations
Our models are implemented using JAX 6 and
trained on Cloud TPU-v8. We initialize the dual
encoder modules from public T5 checkpoints 7.
During training, we use Adafactor (Shazeer and
Stern, 2018) as the optimizer and set the learning
rate to 0.001. Linear decay is applied after 10%
of the total number of training steps, reducing the
learning rate to 0 by the end of training. To ﬁne-
tune on NLI we use a batch size of 512, while for
the Community QA dataset the batch size is 2048.
We use a softmax temperature τ of 0.01.
5Following SimCSE (Gao et al., 2021), we report Spear-
man’s correlation for the ‘all’ setting for all STS tasks which
aggregates the data across different subsets.
6https://github.com/google/jax
7https://github.com/google-research/
text-to-text-transfer-transformer
5 Experimental Goals
Our experiments aim to answer the following:
• Q1: What is the best way to extract sentence
representations from T5?
• Q2: How well do raw T5 sentence embed-
dings perform on downstream tasks?
• Q3: How much do contrastive sentence em-
bedding tasks (e.g., NLI, QA) improve the T5
sentence embeddings.
• Q4: Can we beneﬁt from scaling up model
capacity for better sentence representations?
With these goals, we study transfer and STS
performance of T5 sentence embeddings using a
variety of model and training conﬁgurations, com-
paring ST5 to state-of-the-art methods including
SBERT/SRoBERTa (Reimers and Gurevych, 2019)
and SimCSE (Gao et al., 2021).
6 Results
Table 2 and 3 provide performance on transfer and
STS tasks, respectively. We compare ST5 mod-
els with two types of baselines: (ii) a model that
extracts sentence embeddings from a pre-trained
BERT model, listed in rows 1–2 of each table;
(ii) the current state-of-the-art sentence embedding
models ﬁne-tuned from BERT or RoBERTa, listed
in rows 6–8 of each table.
6.1 Results for Raw T5 Sentence Embeddings
We ﬁrst evaluate the T5 sentence embeddings with-
out ﬁne-tuning. We evaluate all three strategies
from section 3.1: (i) Encoder-only ﬁrst token,
(ii) Encoder-only mean, and (iii) Encoder-decoder
start token. For all experiments, we use the en-
coder or decoder outputs from the T5 transformer
directly, without doing any projection. This enables
us to fully leverage the embedding capacity from
the pre-trained models.
Transfer tasks Results for ST5 models using
raw embeddings on transfer tasks are shown in
rows 3–5 of table 2. Unlike BERT, T5’s ﬁrst token
(either for encoder or decoder) is not reserved for
a special placeholder (i.e. CLS) and there are no
speciﬁc pre-training tasks using the ﬁrst token’s
embeddings. Therefore, it is unlikely that with-
out additional ﬁne-tuning the ﬁrst token’s represen-
tation would capture the semantics of the whole



### Claim 6/38

#### Claim Text
Unlike previous task-specific evaluations, like MSMARCO [ 38], SentEval [14], it is needed to substantially augment the benchmarks so as to evaluate the embedding’s performance for a wide variety of tasks.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[14]_1803.05449.pdf (Page 0):

SentEval: An Evaluation Toolkit for Universal Sentence Representations
Alexis Conneau∗ and Douwe Kiela
Facebook Artiﬁcial Intelligence Research
{aconneau,dkiela}@fb.com
Abstract
We introduce SentEval, a toolkit for evaluating the quality of universal sentence representations. SentEval encompasses a variety of
tasks, including binary and multi-class classiﬁcation, natural language inference and sentence similarity. The set of tasks was selected
based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations.
The toolkit comes with scripts to download and preprocess datasets, and an easy interface to evaluate sentence encoders. The aim is to
provide a fairer, less cumbersome and more centralized way for evaluating sentence representations.
Keywords: representation learning, evaluation
1. Introduction
Following the recent word embedding upheaval, one of
NLP’s next challenges has become the hunt for univer-
sal general-purpose sentence representations. What distin-
guishes these representations, or embeddings, is that they
are not necessarily trained to perform well on one speciﬁc
task. Rather, their value lies in their transferability, i.e.,
their ability to capture information that can be of use in any
kind of system or pipeline, on a variety of tasks.
Word embeddings are particularly useful in cases where
there is limited training data, leading to sparsity and poor
vocabulary coverage, which in turn lead to poor generaliza-
tion capabilities. Similarly, sentence embeddings (which
are often built on top of word embeddings) can be used
to further increase generalization capabilities, composing
unseen combinations of words and encoding grammatical
constructions that are not present in the task-speciﬁc train-
ing data. Hence, high-quality universal sentence represen-
tations are highly desirable for a variety of downstream
NLP tasks.
The evaluation of general-purpose word and sentence em-
beddings has been problematic (Chiu et al., 2016; Faruqui
et al., 2016), leading to much discussion about the best way
to go about it 1. On the one hand, people have measured
performance on intrinsic evaluations, e.g. of human judg-
ments of word or sentence similarity ratings (Agirre et al.,
2012; Hill et al., 2016b) or of word associations (Vuli ´c et
al., 2017). On the other hand, it has been argued that the
focus should be on downstream tasks where these repre-
sentations would actually be applied (Ettinger et al., 2016;
Nayak et al., 2016). In the case of sentence representa-
tions, there is a wide variety of evaluations available, many
from before the “embedding era”, that can be used to as-
sess representational quality on that particular task. Over
the years, something of a consensus has been established,
mostly based on the evaluations in seminal papers such as
SkipThought (Kiros et al., 2015), concerning what evalu-
ations to use. Recent works in which various alternative
sentence encoders are compared use a similar set of tasks
∗LIUM, Universit´e Le Mans
1See also recent workshops on evaluating representations for
NLP, e.g. RepEval: https://repeval2017.github.io/
(Hill et al., 2016a; Conneau et al., 2017).
Implementing pipelines for this large set of evaluations,
each with its own peculiarities, is cumbersome and in-
duces unnecessary wheel reinventions. Another well-
known problem with the current status quo, where every-
one uses their own evaluation pipeline, is that different pre-
processing schemes, evaluation architectures and hyperpa-
rameters are used. The datasets are often small, meaning
that minor differences in evaluation setup may lead to very
different outcomes, which implies that results reported in
papers are not always fully comparable.
In order to overcome these issues, we introduce SentEval2:
a toolkit that makes it easy to evaluate universal sentence
representation encoders on a large set of evaluation tasks
that has been established by community consensus.
2. Aims
The aim of SentEval is to make research on universal sen-
tence representations fairer, less cumbersome and more
centralized. To achieve this goal, SentEval encompasses
the following:
• one central set of evaluations, based on what appears
to be community consensus;
• one common evaluation pipeline with ﬁxed standard
hyperparameters, apart from those tuned on validation
sets, in order to avoid discrepancies in reported results;
and
• easy access for anyone, meaning: a straightforward
interface in Python, and scripts necessary to download
and preprocess the relevant datasets.
In addition, we provide examples of models, such as a sim-
ple bag-of-words model. These could potentially also be
used to extrinsically evaluate the quality of word embed-
dings in NLP tasks.
3. Evaluations
Our aim is to obtain general-purpose sentence embeddings
that capture generic information, which should be useful
2https://github.com/facebookresearch/SentEval
arXiv:1803.05449v1  [cs.CL]  14 Mar 2018



Source: data\tc18_2309.07597v5\referenced_papers\[14]_1803.05449.pdf (Page 2):

approach as with SICK-E, except that our classiﬁer has only
2 classes, i.e., the aim is to predict whether the sentences are
paraphrases or not.
Caption-Image retrieval The caption-image retrieval
task evaluates joint image and language feature models (Lin
et al., 2014). The goal is either to rank a large collection
of images by their relevance with respect to a given query
caption (Image Retrieval), or ranking captions by their rel-
evance for a given query image (Caption Retrieval). The
COCO dataset provides a training set of 113k images with 5
captions each. The objective consists of learning a caption-
image compatibility score Lcir(x,y) from a set of aligned
image-caption pairs as training data. We use a pairwise
ranking-loss Lcir(x,y):
∑
y
∑
k
max(0,α −s(Vy,Ux ) +s(Vy,Ux k)) +
∑
x
∑
k′
max(0,α −s(Ux,Vy ) +s(Ux,Vy k′ )),
where (x,y) consists of an image y with one of its asso-
ciated captions x, (yk)k and (yk′ )k′ are negative examples
of the ranking loss, α is the margin and s corresponds to
the cosine similarity. U and V are learned linear trans-
formations that project the caption x and the image y to
the same embedding space. We measure Recall@K, with
K ∈{1,5,10}, i.e., the percentage of images/captions for
which the corresponding caption/image is one of the ﬁrst
K retrieved; and median rank. We use the same splits
as Karpathy and Fei-Fei (2015), i.e., we use 113k images
(each containing 5 captions) for training, 5k images for
validation and 5k images for test. For evaluation, we split
the 5k images in 5 random sets of 1k images on which we
compute the mean R@1, R@5, R@10 and median (Med r)
over the 5 splits. We include 2048-dimensional pretrained
ResNet-101 (He et al., 2016) features for all images.
4. Usage and Requirements
Our evaluations comprise two different types: ones where
we need to learn on top of the provided sentence represen-
tations (e.g. classiﬁcation/regression) and ones where we
simply take the cosine similarity between the two represen-
tations, as in the STS tasks. In the binary and multi-class
classiﬁcation tasks, we ﬁt either a Logistic Regression clas-
siﬁer or an MLP with one hidden layer on top of the sen-
tence representations. For the natural language inference
tasks, where we are given two sentences uand v, we pro-
vide the classiﬁer with the input ⟨u,v, |u−v|,u ∗v⟩. To ﬁt
the Pytorch models, we use Adam (Kingma and Ba, 2014),
with a batch size 64. We tune the L2 penalty of the classiﬁer
with grid-search on the validation set. When using Sent-
Eval, two functions should be implemented by the user:
• prepare(params, dataset): sees the whole
dataset and applies any necessary preprocessing, such
as constructing a lookup table of word embeddings
(this function is optional); and
• batcher(params, batch): given a batch of in-
put sentences, returns an array of the sentence embed-
dings for the respective inputs.
The main batcherfunction allows the user to encode text
sentences using any Python framework. For example, the
batcher function might be a wrapper around a model writ-
ten in Pytorch, TensorFlow, Theano, DyNet, or any other
framework5. To illustrate the use, here is an example of
what an evaluation script looks like, having deﬁned the pre-
pare and batcher functions:
import senteval
se = senteval.engine.SE(
params, batcher, prepare)
transfer_tasks = ['MR', 'CR']
results = se.eval(transfer_tasks)
Parameters Both functions make use of a params ob-
ject, which contains the settings of the network and the
evaluation. SentEval has several parameters that inﬂuence
the evaluation procedure. These include the following:
• task path(str, required): path to the data.
• seed(int): random seed for reproducibility.
• batch size (int): size of minibatch of text sen-
tences provided to batcher (sentences are sorted by
length).
• kfold(int): k in the kfold-validation (default: 10).
The default conﬁg is:
params = {'task_path': PATH_TO_DATA,
'usepytorch': True,
'kfold': 10}
We also give the user the ability to customize the classiﬁer
used for the classiﬁcation tasks.
Classiﬁer To be comparable to the results published in
the literature, users should use the following parameters for
Logistic Regression:
params['classifier'] =
{'nhid': 0, 'optim': 'adam',
'batch_size': 64, 'tenacity': 5,
'epoch_size': 4}
The parameters of the classiﬁer include:
• nhid (int): number of hidden units of the MLP; if
nhid> 0, a Multi-Layer Perceptron with one hidden
layer and a Sigmoid nonlinearity is used.
• optim(str): classiﬁer optimizer (default: adam).
• batch size(int): batch size for training the classi-
ﬁer (default: 64).
• tenacity(int): stopping criterion; maximum num-
ber of times the validation error does not decrease.
• epoch size (int): number of passes through the
training set for one epoch.
5Or any other programming language, as long as the vectors
can be passed to, or loaded from, code written in Python.



Source: data\tc18_2309.07597v5\referenced_papers\[14]_1803.05449.pdf (Page 3):

Model MR CR SUBJ MPQA SST-2 SST-5 TREC MRPC SICK-E
Representation learning (transfer)
GloVe LogReg 77.4 78.7 91.2 87.7 80.3 44.7 83.0 72.7/81.0 78.5
GloVe MLP 77.7 79.9 92.2 88.7 82.3 45.4 85.2 73.0/80.9 79.0
fastText LogReg 78.2 80.2 91.8 88.0 82.3 45.1 83.4 74.4/82.4 78.9
fastText MLP 78.0 81.4 92.9 88.5 84.0 45.1 85.6 74.4/82.3 80.2
SkipThought 79.4 83.1 93.7 89.3 82.9 - 88.4 72.4/81.6 79.5
InferSent 81.1 86.3 92.4 90.2 84.6 46.3 88.2 76.2/83.1 86.3
Supervised methods directly trained for each task (no transfer)
SOTA 83.1 1 86.3 1 95.5 1 93.3 1 89.5 2 52.4 2 96.1 2 80.4/85.9 3 84.5 4
Table 3: Transfer test results for various baseline methods. We include supervised results trained directly on each task (no
transfer). Results 1 correspond to AdaSent (Zhao et al., 2015), 2 to BLSTM-2DCNN (Zhou et al., 2016), 3 to TF-KLD (Ji
and Eisenstein, 2013) and 4 to Illinois-LH system (Lai and Hockenmaier, 2014).
• dropout(ﬂoat): dropout rate in the case of MLP.
For use cases where there are multiple calls to SentEval,
e.g when evaluating the sentence encoder at every epoch of
training, we propose the following prototyping set of pa-
rameters, which will lead to slightly worse results but will
make the evaluation signiﬁcantly faster:
params['classifier'] =
{'nhid': 0, 'optim': 'rmsprop',
'batch_size': 128, 'tenacity': 3,
'epoch_size': 2}
You may also pass additional parameters to the params
object in order which will further be accessible from the
prepare and batcher functions (e.g a pretrained model).
Datasets In order to obtain the data and preprocess
it so that it can be fed into SentEval, we provide the
get transfer data.bashscript in the data directory.
The script fetches the different datasets from their known
locations, unpacks them and preprocesses them. We to-
kenize each of the datasets with the MOSES tokenizer
(Koehn et al., 2007) and convert all ﬁles to UTF-8 encod-
ing. Once this script has been executed, the task path pa-
rameter can be set to indicate the path of the data directory.
Requirements SentEval is written in Python. In order
to run the evaluations, the user will need to install numpy,
scipy and recent versions of pytorch and scikit-learn. In
order to facilitate research where no GPUs are available,
we offer for the evaluations to be run on CPU (using scikit-
learn) where possible. For the bigger datasets, where more
complicated models are often required, for instance STS
Benchmark, SNLI, SICK-R and the image-caption retrieval
tasks, we recommend pytorch models on a single GPU.
5. Baselines
Several baseline models are evaluated in Table 3:
• Continuous bag-of-words embeddings (average of
word vectors). We consider the most commonly used
pretrained word vectors available, namely the fastText
(Mikolov et al., 2017) and the GloVe (Pennington et
al., 2014) vectors trained on CommonCrawl.
• SkipThought vectors (Ba et al., 2016)
• InferSent vectors (Conneau et al., 2017)
In addition to these methods, we include the results of cur-
rent state-of-the-art methods for which both the encoder
and the classiﬁer are trained on each task (no transfer). For
GloVe and fastText bag-of-words representations, we re-
port the results for Logistic Regression and Multi-Layer
Perceptron (MLP). For the MLP classiﬁer, we tune the
dropout rate and the number of hidden units in addition
to the L2 regularization. We do not observe any improve-
ment over Logistic Regression for methods that already
have a large embedding size (4096 for Infersent and 4800
for SkipThought). On most transfer tasks, supervised meth-
ods that are trained directly on each task still outperform
transfer methods. Our hope is that SentEval will help the
community build sentence representations with better gen-
eralization power that can outperform both the transfer and
the supervised methods.
6. Conclusion
Universal sentence representations are a hot topic in NLP
research. Making use of a generic sentence encoder allows
models to generalize and transfer better, even when trained
on relatively small datasets, which makes them highly de-
sirable for downstream NLP tasks.
We introduced SentEval as a fair, straightforward and cen-
tralized toolkit for evaluating sentence representations. We
have aimed to make evaluation as easy as possible: sen-
tence encoders can be evaluated by implementing a simple
Python interface, and we provide a script to download the
necessary evaluation datasets. In future work, we plan to
enrich SentEval with additional tasks as the consensus on
the best evaluation for sentence embeddings evolves. In
particular, tasks that probe for speciﬁc linguistic properties
of the sentence embeddings (Shi et al., 2016; Adi et al.,
2017) are interesting directions towards understanding how
the encoder understands language. We hope that our toolkit
will be used by the community in order to ensure that fully
comparable results are published in research papers.



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 16):

Table 13: Results for each dataset in the MTEB benchmark [ 40]. The numbers for the Retrieval
category are not included here since the datasets are the same as the BEIR benchmark.
unsupervised supervised
E5-PTsmall E5-PTbase E5-PTlarge E5small E5base E5large
AmazonCounterfactualClassification 71.7 73.6 70.4 76.2 79.7 77.7
AmazonPolarityClassification 76.1 77.0 83.2 87.5 88.0 90.1
AmazonReviewsClassification 35.0 35.8 37.4 42.6 42.7 43.0
Banking77Classification 82.1 82.9 83.5 81.9 83.3 84.1
EmotionClassification 42.2 44.2 43.5 46.9 49.4 48.1
ImdbClassification 67.9 67.3 77.7 75.6 76.0 82.1
MassiveIntentClassification 70.2 71.1 70.8 72.2 72.3 73.2
MassiveScenarioClassification 74.6 75.4 75.9 75.8 76.8 77.4
MTOPDomainClassification 91.3 92.3 93.2 92.1 93.2 93.9
MTOPIntentClassification 71.9 74.0 74.2 73.2 74.8 76.4
ToxicConversationsClassification 67.0 67.4 66.1 72.8 74.1 70.6
TweetSentimentExtractionClass. 54.4 53.3 52.5 63.3 61.4 61.2
ArxivClusteringP2P 47.9 49.3 49.4 44.1 44.6 46.2
ArxivClusteringS2S 39.9 42.8 43.6 37.1 40.5 41.4
BiorxivClusteringP2P 38.5 38.8 39.2 35.8 36.2 37.6
BiorxivClusteringS2S 35.4 36.5 36.7 31.9 32.7 35.1
MedrxivClusteringP2P 34.4 33.7 33.3 31.3 31.5 32.3
MedrxivClusteringS2S 32.0 32.1 32.2 28.2 28.3 29.7
RedditClustering 46.9 49.3 52.4 42.9 48.2 50.7
RedditClusteringP2P 60.2 64.4 64.6 56.4 62.2 61.4
StackExchangeClustering 57.7 60.2 63.3 59.1 63.9 65.0
StackExchangeClusteringP2P 32.0 34.0 34.7 30.3 32.6 33.6
TwentyNewsgroupsClustering 34.4 36.2 37.9 37.5 42.6 43.8
SprintDuplicateQuestions 91.6 90.8 92.0 95.3 94.9 95.4
TwitterSemEval2015 60.0 62.8 64.7 74.2 74.4 76.1
TwitterURLCorpus 83.2 84.0 84.1 85.8 86.0 86.3
AskUbuntuDupQuestions 57.8 57.6 58.3 59.4 59.7 60.1
MindSmallReranking 29.0 29.6 29.2 29.6 30.1 30.8
SciDocsRR 81.1 82.6 84.3 79.8 82.9 83.9
StackOverflowDupQuestions 44.4 44.2 45.8 49.1 50.1 51.3
BIOSSES 69.2 71.9 69.7 84.2 85.1 84.7
SICK-R 66.6 68.7 69.7 78.9 79.7 80.5
STS12 60.7 57.9 54.7 75.2 74.2 75.9
STS13 71.1 73.5 74.0 81.8 83.3 85.2
STS14 64.2 64.0 65.3 78.5 78.5 80.5
STS15 74.3 75.4 75.8 87.5 88.4 88.8
STS16 76.6 79.8 80.1 84.6 84.2 85.3
STS17 78.3 77.2 76.0 87.9 87.2 89.4
STS22 59.2 56.2 62.8 63.8 62.9 63.0
STSBenchmark 67.7 70.5 70.9 86.4 86.2 87.2
SummEval 32.7 31.1 32.6 31.4 31.0 31.0
17



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 14):

Name Type Categ. #Lang. Train Dev Test Train avg. Dev avg. Test avg.
Samples Samples Samples chars chars chars
BUCC BitextMining s2s 4 0 0 641684 0 0 101.3
Tatoeba BitextMining s2s 112 0 0 2000 0 0 39.4
AmazonCounterfactualClassiﬁcation Classiﬁcation s2s 4 4018 335 670 107.3 109.2 106.1
AmazonPolarityClassiﬁcation Classiﬁcation p2p 1 3600000 0 400000 431.6 0 431.4
AmazonReviewsClassiﬁcation Classiﬁcation s2s 6 1200000 30000 30000 160.5 159.2 160.4
Banking77Classiﬁcation Classiﬁcation s2s 1 10003 0 3080 59.5 0 54.2
EmotionClassiﬁcation Classiﬁcation s2s 1 16000 2000 2000 96.8 95.3 96.6
ImdbClassiﬁcation Classiﬁcation p2p 1 25000 0 25000 1325.1 0 1293.8
MassiveIntentClassiﬁcation Classiﬁcation s2s 51 11514 2033 2974 35.0 34.8 34.6
MassiveScenarioClassiﬁcation Classiﬁcation s2s 51 11514 2033 2974 35.0 34.8 34.6
MTOPDomainClassiﬁcation Classiﬁcation s2s 6 15667 2235 4386 36.6 36.5 36.8
MTOPIntentClassiﬁcation Classiﬁcation s2s 6 15667 2235 4386 36.6 36.5 36.8
ToxicConversationsClassiﬁcation Classiﬁcation s2s 1 50000 0 50000 298.8 0 296.6
TweetSentimentExtractionClassiﬁcation Classiﬁcation s2s 1 27481 0 3534 68.3 0 67.8
ArxivClusteringP2P Clustering p2p 1 0 0 732723 0 0 1009.9
ArxivClusteringS2S Clustering s2s 1 0 0 732723 0 0 74.0
BiorxivClusteringP2P Clustering p2p 1 0 0 75000 0 0 1666.2
BiorxivClusteringS2S Clustering s2s 1 0 0 75000 0 0 101.6
MedrxivClusteringP2P Clustering p2p 1 0 0 37500 0 0 1981.2
MedrxivClusteringS2S Clustering s2s 1 0 0 37500 0 0 114.7
RedditClustering Clustering s2s 1 0 420464 420464 0 64.7 64.7
RedditClusteringP2P Clustering p2p 1 0 0 459399 0 0 727.7
StackExchangeClustering Clustering s2s 1 0 417060 373850 0 56.8 57.0
StackExchangeClusteringP2P Clustering p2p 1 0 0 75000 0 0 1090.7
TwentyNewsgroupsClustering Clustering s2s 1 0 0 59545 0 0 32.0
SprintDuplicateQuestions PairClassiﬁcation s2s 1 0 101000 101000 0 65.2 67.9
TwitterSemEval2015 PairClassiﬁcation s2s 1 0 0 16777 0 0 38.3
TwitterURLCorpus PairClassiﬁcation s2s 1 0 0 51534 0 0 79.5
AskUbuntuDupQuestions Reranking s2s 1 0 0 2255 0 0 52.5
MindSmallReranking Reranking s2s 1 231530 0 107968 69.0 0 70.9
SciDocsRR Reranking s2s 1 0 19594 19599 0 69.4 69.0
StackOverﬂowDupQuestions Reranking s2s 1 23018 3467 3467 49.6 49.8 49.8
ArguAna Retrieval p2p 1 0 0 10080 0 0 1052.9
ClimateFEVER Retrieval s2p 1 0 0 5418128 0 0 539.1
CQADupstackAndroidRetrieval Retrieval s2p 1 0 0 23697 0 0 578.7
CQADupstackEnglishRetrieval Retrieval s2p 1 0 0 41791 0 0 467.1
CQADupstackGamingRetrieval Retrieval s2p 1 0 0 46896 0 0 474.7
CQADupstackGisRetrieval Retrieval s2p 1 0 0 38522 0 0 991.1
CQADupstackMathematicaRetrieval Retrieval s2p 1 0 0 17509 0 0 1103.7
CQADupstackPhysicsRetrieval Retrieval s2p 1 0 0 39355 0 0 799.4
CQADupstackProgrammersRetrieval Retrieval s2p 1 0 0 33052 0 0 1030.2
CQADupstackStatsRetrieval Retrieval s2p 1 0 0 42921 0 0 1041.0
CQADupstackTexRetrieval Retrieval s2p 1 0 0 71090 0 0 1246.9
CQADupstackUnixRetrieval Retrieval s2p 1 0 0 48454 0 0 984.7
CQADupstackWebmastersRetrieval Retrieval s2p 1 0 0 17911 0 0 689.8
CQADupstackWordpressRetrieval Retrieval s2p 1 0 0 49146 0 0 1111.9
DBPedia Retrieval s2p 1 0 4635989 4636322 0 310.2 310.1
FEVER Retrieval s2p 1 0 0 5423234 0 0 538.6
FiQA2018 Retrieval s2p 1 0 0 58286 0 0 760.4
HotpotQA Retrieval s2p 1 0 0 5240734 0 0 288.6
MSMARCO Retrieval s2p 1 0 8848803 8841866 0 336.6 336.8
MSMARCOv2 Retrieval s2p 1 138641342 138368101 0 341.4 342.0 0
NFCorpus Retrieval s2p 1 0 0 3956 0 0 1462.7
NQ Retrieval s2p 1 0 0 2684920 0 0 492.7
QuoraRetrieval Retrieval s2s 1 0 0 532931 0 0 62.9
SCIDOCS Retrieval s2p 1 0 0 26657 0 0 1161.9
SciFact Retrieval s2p 1 0 0 5483 0 0 1422.3
Touche2020 Retrieval s2p 1 0 0 382594 0 0 1720.1
TRECCOVID Retrieval s2p 1 0 0 171382 0 0 1117.4
BIOSSES STS s2s 1 200 200 200 156.6 156.6 156.6
SICK-R STS s2s 1 19854 19854 19854 46.1 46.1 46.1
STS12 STS s2s 1 4468 0 6216 100.7 0 64.7
STS13 STS s2s 1 0 0 3000 0 0 54.0
STS14 STS s2s 1 0 0 7500 0 0 54.3
STS15 STS s2s 1 0 0 6000 0 0 57.7
STS16 STS s2s 1 0 0 2372 0 0 65.3
STS17 STS s2s 11 0 0 500 0 0 43.3
STS22 STS p2p 18 0 0 8060 0 0 1992.8
STSBenchmark STS s2s 1 11498 3000 2758 57.6 64.0 53.6
SummEval Summarization p2p 1 0 0 2800 0 0 359.8
Table 2: Tasks in MTEB
AmazonPolarity (McAuley and Leskovec,
2013) A collection of Amazon customer reviews
annotated for polarity classiﬁcation. For each
review the label is either "positive" or "negative".
AmazonReviews (McAuley and Leskovec,
2013) A collection of Amazon reviews designed
to aid research in multilingual text classiﬁcation.
For each review the label is the score given by
the review between 0 and 4 (1-5 stars). This is a



### Claim 7/38

#### Claim Text
It is later extended by MTEB [35], where all major aspects of text embeddings can be comprehensively evaluated.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 0):

MTEB: Massive Text Embedding Benchmark
Niklas Muennighoff1, Nouamane Tazi1, Loïc Magne1, Nils Reimers2*
1Hugging Face 2cohere.ai
1firstname@hf.co 2info@nils-reimers.de
Abstract
Text embeddings are commonly evaluated on
a small set of datasets from a single task not
covering their possible applications to other
tasks. It is unclear whether state-of-the-art em-
beddings on semantic textual similarity (STS)
can be equally well applied to other tasks like
clustering or reranking. This makes progress
in the ﬁeld difﬁcult to track, as various models
are constantly being proposed without proper
evaluation. To solve this problem, we intro-
duce the Massive Text Embedding Benchmark
(MTEB). MTEB spans 8 embedding tasks cov-
ering a total of 58 datasets and 112 languages.
Through the benchmarking of 33 models on
MTEB, we establish the most comprehensive
benchmark of text embeddings to date. We
ﬁnd that no particular text embedding method
dominates across all tasks. This suggests that
the ﬁeld has yet to converge on a universal text
embedding method and scale it up sufﬁciently
to provide state-of-the-art results on all embed-
ding tasks. MTEB comes with open-source
code and a public leaderboard at https:
//github.com/embeddings-benchm
ark/mteb.
1 Introduction
Natural language embeddings power a variety of
use cases from clustering and topic representa-
tion (Aggarwal and Zhai, 2012; Angelov, 2020)
to search systems and text mining (Huang et al.,
2020; Zhu et al., 2021; Nayak, 2019) to feature
representations for downstream models (Saharia
et al., 2022; Borgeaud et al., 2022). Using gener-
ative language models or cross-encoders for these
applications is often intractable, as they may re-
quire exponentially more computations (Reimers
and Gurevych, 2019).
However, the evaluation regime of current text
embedding models rarely covers the breadth of
*Most of the work done while at Hugging Face. Corre-
spondence to n.muennighoff@gmail.com.
their possible use cases. For example, Sim-
CSE (Gao et al., 2021b) or SBERT (Reimers and
Gurevych, 2019) solely evaluate on STS and clas-
siﬁcation tasks, leaving open questions about the
transferability of the embedding models to search
or clustering tasks. STS is known to poorly corre-
late with other real-world use cases (Neelakantan
et al., 2022; Wang et al., 2021). Further, evaluating
embedding methods on many tasks requires imple-
menting multiple evaluation pipelines. Implemen-
tation details like pre-processing or hyperparam-
eters may inﬂuence the results making it unclear
whether performance improvements simply come
from a favorable evaluation pipeline. This leads to
the “blind” application of these models to new use
cases in industry or requires incremental work to
reevaluate them on different tasks.
The Massive Text Embedding Benchmark
(MTEB) aims to provide clarity on how models
perform on a variety of embedding tasks and thus
serves as the gateway to ﬁnding universal text em-
beddings applicable to a variety of tasks. MTEB
consists of 58 datasets covering 112 languages
from 8 embedding tasks: Bitext mining, classi-
ﬁcation, clustering, pair classiﬁcation, reranking,
retrieval, STS and summarization. MTEB software
is available open-source 1 enabling evaluation of
any embedding model by adding less than 10 lines
of code. Datasets and the MTEB leaderboard are
available on the Hugging Face Hub2.
We evaluate over 30 models on MTEB with addi-
tional speed and memory benchmarking to provide
a holistic view of the state of text embedding mod-
els. We cover both models available open-source
as well as models accessible via APIs, such as the
OpenAI Embeddings endpoint. We ﬁnd there to be
no single best solution, with different models dom-
1https://github.com/embeddings-benchm
ark/mteb
2https://huggingface.co/spaces/mteb/l
eaderboard
arXiv:2210.07316v3  [cs.CL]  19 Mar 2023



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 8):

4.4 Multilinguality
MTEB comes with 10 multilingual datasets across
bitext mining, classiﬁcation and STS tasks. We in-
vestigate performance on these in Figure 5. Tabular
results can be found in Tables 12, 13 and 14.
Bitext Mining LaBSE (Feng et al., 2020) per-
forms strongly across a wide array of languages in
bitext mining. Meanwhile, LASER2 shows high
variance across different languages. While there
are additional language-speciﬁc LASER2 models
available for some of the languages we benchmark,
we use the default multilingual LASER2 model
for all languages. This is to provide a fair one-to-
one comparison of models. In practice, however,
the high variance of LASER2’s performance may
be resolved by mixing its model variants. MP-
Net, MiniLM and SGPT-BLOOM-7B1-msmarco
perform poorly on languages they have not been
pre-trained on, such as German for the latter.
Classiﬁcation & STS On multilingual classiﬁ-
cation and STS, the multilingual MPNet provides
the overall strongest performance. It outperforms
the slightly faster multilingual MiniLM on almost
all languages. Both models have been trained
on the same languages, thus bringing decision-
making down to performance vs speed. SGPT-
BLOOM-7B1-msmarco provides state-of-the-art
performance on languages like Hindi, Portuguese,
Chinese or French, which the model has seen ex-
tensively during pre-training. It also performs com-
petitively on languages like Russian or Japanese
that unintentionally leaked into its pre-training
data (Muennighoff et al., 2022). However, it is not
much ahead of the much cheaper MPNet. LASER2
performs consistently worse than other models.
5 Conclusion
In this work, we presented the Massive Text Em-
bedding Benchmark (MTEB). Consisting of 8 text
embedding tasks with up to 15 datasets each and
covering 112 languages, MTEB aims to provide re-
liable embedding performance estimates. By open-
sourcing MTEB alongside a leaderboard, we pro-
vide a foundation for further pushing the state-of-
the-art of available text embeddings.
To introduce MTEB, we have conducted the
most comprehensive benchmarking of text embed-
dings to date. Through the course of close to 5,000
experiments on over 30 different models, we have
set up solid baselines for future research to build
on. We found model performance on different tasks
to vary strongly with no model claiming state-of-
the-art on all tasks. Our studies on scaling behav-
ior, model efﬁciency and multilinguality revealed
various intricacies of models that should ease the
decision-making process for future research or in-
dustry applications of text embeddings.
We welcome task, dataset or metric contributions
to the MTEB codebase7 as well as additions to the
leaderboard via our automatic submission format8.
7https://github.com/embeddings-benchm
ark/mteb
8https://huggingface.co/spaces/mteb/l
eaderboard



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 1):

inating different tasks. Our benchmarking sheds
light on the weaknesses and strengths of individual
models, such as SimCSE’s (Gao et al., 2021b) low
performance on clustering and retrieval despite its
strong performance on STS. We hope our work
makes selecting the right embedding model easier
and simpliﬁes future embedding research.
2 Related Work
2.1 Benchmarks
Benchmarks, such as (Super)GLUE (Wang et al.,
2018, 2019) or Big-BENCH (Srivastava et al.,
2022), and evaluation frameworks (Gao et al.,
2021a) play a key role in driving NLP progress.
Yearly released SemEval datasets (Agirre et al.,
2012, 2013, 2014, 2015, 2016) are commonly used
as the go-to benchmark for text embeddings. Se-
mEval datasets correspond to the task of semantic
textual similarity (STS) requiring models to embed
similar sentences with geometrically close embed-
dings. Due to the limited expressivity of a single Se-
mEval dataset, SentEval (Conneau and Kiela, 2018)
aggregates multiple STS datasets. SentEval focuses
on ﬁne-tuning classiﬁers on top of embeddings. It
lacks tasks like retrieval or clustering, where em-
beddings are directly compared without additional
classiﬁers. Further, the toolkit was proposed in
2018 and thus does not provide easy support for
recent trends like text embeddings from transform-
ers (Reimers and Gurevych, 2019). Due to the
insufﬁciency of STS benchmarking, USEB (Wang
et al., 2021) was introduced consisting mostly of
reranking tasks. Consequently, it does not cover
tasks like retrieval or classiﬁcation. Meanwhile, the
recently released BEIR Benchmark (Thakur et al.,
2021) has become the standard for the evaluation
of embeddings for zero-shot information retrieval.
MTEB uniﬁes datasets from different embed-
ding tasks into a common, accessible evaluation
framework. MTEB incorporates SemEval datasets
(STS11 - STS22) and BEIR alongside a variety of
other datasets from various tasks to provide a holis-
tic performance review of text embedding models.
2.2 Embedding Models
Text embedding models like Glove (Pennington
et al., 2014) lack context awareness and are thus
commonly labeled as Word Embedding Models.
They consist of a layer mapping each input word
to a vector often followed by an averaging layer to
provide a ﬁnal embedding invariant of input length.
Transformers (Vaswani et al., 2017) inject context
awareness into language models via self-attention
and form the foundation of most recent embed-
ding models. BERT (Devlin et al., 2018) uses the
transformer architecture and performs large-scale
self-supervised pre-training. The resulting model
can directly be used to produce text embeddings
via an averaging operation alike Glove. Build-
ing on InferSent (Conneau et al., 2017), SBERT
(Reimers and Gurevych, 2019) demonstrated it to
be beneﬁcial to perform additional ﬁne-tuning of
the transformer for competitive embedding perfor-
mance. Most recent ﬁne-tuned embedding models
use a contrastive loss objective to perform super-
vised ﬁne-tuning on positive and negative text pairs
(Gao et al., 2021b; Wang et al., 2021; Ni et al.,
2021b; Muennighoff, 2022). Due to the large va-
riety of available pre-trained transformers (Wolf
et al., 2020), there is an at least equally large va-
riety of potential text embedding models to be ex-
plored. This leads to confusion about which model
provides practitioners with the best performance
for their embedding use case.
We benchmark both word embedding and trans-
former models on MTEB quantifying gains pro-
vided by often much slower context aware models.
3 The MTEB Benchmark
3.1 Desiderata
MTEB is built on a set of desiderata: (a) Diversity:
MTEB aims to provide an understanding of the
usability of embedding models in various use cases.
The benchmark comprises 8 different tasks, with
up to 15 datasets each. Of the 58 total datasets in
MTEB, 10 are multilingual, covering 112 differ-
ent languages. Sentence-level and paragraph-level
datasets are included to contrast performance on
short and long texts. (b) Simplicity: MTEB pro-
vides a simple API for plugging in any model that
given a list of texts can produce a vector for each
list item with a consistent shape. This makes it
possible to benchmark a diverse set of models. (c)
Extensibility: New datasets for existing tasks can
be benchmarked in MTEB via a single ﬁle that
speciﬁes the task and a Hugging Face dataset name
where the data has been uploaded (Lhoest et al.,
2021). New tasks require implementing a task in-
terface for loading the data and an evaluator for
benchmarking. We welcome dataset, task or metric
contributions from the community via pull requests
to continue the development of MTEB. (d) Repro-



Source: data\tc18_2309.07597v5\referenced_papers\[28]_2308.03281.pdf (Page 5):

Model Params LR GPUs BS Base LM
GTEsmall 30M 3 × 10−4 2 16384 microsoft/MiniLM-L12-H384-uncased
GTEbase 110M 2 × 10−4 4 16384 bert-base-uncased
GTElarge 330M 5 × 10−5 8 16384 bert-large-uncased
Table 3: Pre-training configurations of models of different sizes.
4 Experiments
In this section, we provide an extensive evaluation
of our embedding model, comparing to state-of-
the-art models for each task. Note that an apple-to-
apple comparison is hardly possible since different
models used different in-house data for pre-training
and the base language models vary a lot. We mainly
use the number of model parameters as a criterion
for performance comparison since it is closely re-
lated to the inference speed.
4.1 Zero-shot Text Classification
Model Params Prompting Accuracy
E5base 110M ✓ 81.3
E5large 330M ✓ 85.3
cpt-text 6B 88.1
cpt-text 6B ✓ 89.1
GTEbase 110M 85.1
GTEbase 110M ✓ 87.2
Table 4: Zero shot text classification performance on
SST-2. All compared models are the fine-tuned ones.
One method to assess the quality of learned
representation is through zero-shot classifica-
tion. (Radford et al., 2021; Neelakantan et al., 2022;
Wang et al., 2022b). We recast text classification
into an embedding-based similarity matching prob-
lem. In this setting, inputs texts are converted into
embeddings directly and labels are verbalized to
corresponding text to get label embeddings. Dis-
tances between input embeddings and label embed-
dings are measured by their inner product and label
with the most close embedding distance to the in-
put text is regarded as the classification result. An
example is SST-2 binary sentiment classification
task. We consider two types of label verbalizers
for evaluation. The vanilla version uses the sen-
timent word ‘positive’ or ‘negative’ to denote the
corresponding labels. Prompted version uses fuzzy
prompt template, such as ‘this is an example of
positive/negative movie review’.
Zero-shot text classification accuracy on SST-
2 is shown in Table 4. In the vanilla setting, our
110M model already matches the performance of
prompted E5large with 330M parameters. Using
prompting strategy further improves results signifi-
cantly and closes the gap with large models. Even
without explicit prompt or instruction during train-
ing, our model can somewhat understand the label
context better when formatted as a natural language
text.
4.2 Unsupervised Text Retrieval
Text retrieval requires retrieving most relevant doc-
uments from a large-scale candidate sets. We
use BEIR (Thakur et al., 2021) as our evalua-
tion benchmark for zero-shot unsupervised text
retrieval. BEIR is a heterogeneous information re-
trieval benchmark which contains retrieval tasks of
different formats and from different domains. We
use the open available 15 datasets for evaluation.
We compare our unsupervised pre-trained check-
point to recent unsupervised dense retrievers such
as Contriever (Izacard et al., 2022a) and E5 (Wang
et al., 2022b). According to Table 5, we find that
our base size model significantly outperforms the
models with comparable size, like SimCSE, Con-
triever and E5. Our base model is comparable to
E5large without using human supervision.
4.3 Massive Text Embedding Benchmark
Massive Text Embedding Benchmark (MTEB) is a
comprehensive semi-supervised benchmark that in-
corporates a limited amount of supervision data for
evaluation. In this paper, we evaluate the English
subsets which encompasses 56 English datasets
across seven distinct tasks, including text classi-
fication (Class.), text clustering (Clust.), pairwise
classification (Pair.), text reranking (Rerank.), text
retreival (Retr.), semantic textual similarity (STS)
and summarization (Summ.). The evaluation met-
rics employed in MTEB are accuracy, v-measure,
average precision, MAP, nDCG@10, and Spear-
man coefficients, respectively. For further details



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 8):

Table 8: Comparison of different negative sampling strategies.
# negatives NFCorpus NQ FiQA Quora DBPedia Scifact Avg
In batch 32k 35.8 39.0 40.0 85.7 35.4 73.7 51.6
+ pre-batch 64k 29.4 27.2 29.4 84.6 25.0 64.3 43.3
MoCo 130k 29.7 36.1 32.0 81.6 29.9 63.6 45.5
performance for a wide range of tasks requiring single-vector text representations such as retrieval,
semantic textual similarity, and text matching. When further customized for downstream tasks, E5
achieves superior fine-tuned performance compared to existing embedding models with 40× more
parameters on the large, 56-task MTEB benchmark datasets.
References
[1] Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence
embeddings. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net, 2017. URL
https://openreview.net/forum?id=SyK00v5xx.
[2] Mikel Artetxe and Holger Schwenk. Massively multilingual sentence embeddings for zero-
shot cross-lingual transfer and beyond. Transactions of the Association for Computational
Linguistics, 7:597–610, 2019. doi: 10.1162/tacl_a_00288. URL https://aclanthology.
org/Q19-1038.
[3] David M. Blei, Andrew Y . Ng, and Michael I. Jordan. Latent dirichlet allocation. In
Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani, editors, Advances in Neural
Information Processing Systems 14 [Neural Information Processing Systems: Natural and
Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada] , pages
601–608. MIT Press, 2001. URLhttps://proceedings.neurips.cc/paper/2001/hash/
296472c9542ad4d4788d543508116cbc-Abstract.html.
[4] Alexander Bondarenko, Maik Fröbe, Johannes Kiesel, Shahbaz Syed, Timon Gurcke, Meriem
Beloucif, Alexander Panchenko, Chris Biemann, Benno Stein, Henning Wachsmuth, et al.
Overview of touché 2022: argument retrieval. In International Conference of the Cross-
Language Evaluation Forum for European Languages, pages 311–336. Springer, 2022.
[5] Vera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. A full-text learning to rank
dataset for medical information retrieval. In European Conference on Information Retrieval,
pages 716–722. Springer, 2016.
[6] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference. InProceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing , pages 632–642, Lisbon, Portugal,
2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https:
//aclanthology.org/D15-1075.
[7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learn-
ers. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-
12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.
[8] Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh
Tiwary, Rangan Majumder, Li Deng, and Bhaskar Mitra. Ms marco: A human generated
machine reading comprehension dataset. ArXiv, abs/1611.09268, 2016.
9



### Claim 8/38

#### Claim Text
In the past few years, the community has put forward many datasets for text representation and language understanding tasks in Chinese, such as CMNLI [62], DuReader [20], T2Ranking [60].

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[62]_2004.05986.pdf (Page 13):

TNEWS
sentence: 如果我的世界下架了，你会玩迷你世界吗？
sentence (en): If Minecraft is gone, will you play miniworld?
label: 116(news game)
iFLYTEK
sentence: 《钢铁英雄》是一款角色扮演类游戏。游戏拥有 ...... 带领他们逃出去。修复部分小错误，提升整
体稳定性。
sentence (en): ”Heroes of Steel” is a role-playing game. The game has ...... all four heroes are imprisoned and you
will lead them out. repair part small Errors to improve overall stability.
label: 22(Strategy)
CLUEWSC
text: 这时候放在床上枕头旁边的手机响了，我感到奇怪，因为欠费已被停机两个月，现在它突然响了。
text (en): At this moment, the cellphone on the bed next to the pillow rang. I feel this is quite strange because the
cellphone plan was terminated two months ago since I did not pay the bill. Now it was ringing all of a sudden.
label: true
AFQMC
sentence1: 本月花呗还不上怎么办 sentence2: 花呗超时怎么办
sentence1 (en): What to do if Ant Credit Pay is not available yet this month sentence2 (en): How to deal with Ant
Credit Pay overtime
label: 0(different)
CSL
abst: 不同阶段电子数据的操作都会留下表现各异的轨迹.从操作系统、计算机应用系统 ...... 分析审计电子数
据轨迹在计算机系统中表现形式,可以为审计人员提供有效的审计方法
keyword: [“计算机审计”, “数据轨迹”, “日志文件”]
abst (en): The operation of electronic data in different stages will leave different traces. From operating system,
computer application system ...... provide effective audit methods for auditors by analyzing the expression of audit
electronic data trace in computer system.
keyword (en): [“computer audit”, “data trace”, “log ﬁle”]
label: 0(false)
OCNLI
premise: 但是不光是中国,日本,整个东亚文化都有这个特点就是被权力影响很深 hypothesis: 有超过两个东
亚国家有这个特点
premise (en): But not only China and Japan, the entire East Asian culture has this feature, that is it is deeply inﬂuenced
by the power. hypothesis (en): More than two East Asian countries have this feature.
label: entailment
CMRC 2018
context: 萤火虫工作室是一家总部设在英国伦敦和康涅狄格州坎顿...... 目前，他们正在开发PC和Xbox360上
的次时代游戏。
question: 萤火虫工作室的总部设在哪里？ answer: 英国伦敦和康涅狄格州坎顿
context (en): Fireﬂy Studios is a video game developer based in London, UK and Canton, Connecticut, with a quality
department in Aberdeen, Scotland ...... Currently, they are developing next-generation games on PC and Xbox 360.
question (en): Where is Fireﬂy Studios headquartered? answer (en): London, UK and Canton, Connecticut
ChID
content: 中国青年报：篮协改革联赛切莫#idiom#......
candidates: [“急功近利”, “画蛇添足”, “本末倒置”(answer)]
content (en): China Youth Daily: Chinese Basketball Association should not #idiom# when reforming the league ......
candidates (en): [“seeking instant beneﬁt”, “to overdo it”, “take the branch for the root ”(answer)]
C3 document: 男：我们坐在第七排，应该能看清楚字幕吧? 女：肯定可以，对了，我们得把手机设成振动。
question: 他们最可能在哪儿?
candidates: [“图书馆”, “体育馆”,“电影院”(answer),“火车站”]
document (en): Man: Our seats are in the seventh row. We should be able to see the subtitles clearly, right? Woman:
Absolutely. By the way, we should set the phone to vibrate.
question (en): Where does the conversation most probably take place?
candidates (en): [“In a library”, “In a stadium”,“In a cinema ”(answer),“At a train station”]
Table 5: Development set examples from the tasks in CLUE. Bold text represents part of the example
format for each task. Chinese text is part of the model input, and the corresponding text in italics is
the English version translated from that. Underlined text is specially marked in the input. Text in a
monospaced font represents the expected model output.



Source: data\tc18_2309.07597v5\referenced_papers\[62]_2004.05986.pdf (Page 3):

Corpus |Train| | Dev| | Test| Task Metric Source
Single-Sentence Tasks
TNEWS 53.3k 10k 10k short text classiﬁcation acc. news title and keywords
IFLYTEK 12.1k 2.6k 2.6k long text classiﬁcation acc. app descriptions
CLUEWSC2020 1,244 304 290 coreference resolution acc. Chinese ﬁction books
Sentence Pair Tasks
AFQMC 34.3k 4.3k 3.9k semantic similarity acc. online customer service
CSL 20k 3k 3k keyword recognition acc. academic (CNKI)
OCNLI 50k 3k 3k natural language inference acc. 5 genres
Machine Reading Comprehension Tasks
CMRC 2018 10k 3.4k 4.9k answer span extraction EM. Wikipedia
ChID 577k 23k 23k multiple-choice, idiom acc. novel, essay, and news
C3 11.9k 3.8k 3.9k multiple-choice, free-form acc. mixed-genre
Table 1: Task descriptions and statistics. TNEWS has 15 classes; IFLYTEK has 119 classes; OCNLI has
3 classes, other classiﬁcation tasks are binary classiﬁcation.
more discriminative, we use cross-validation to ﬁlter out some of the easy examples (see Section D Dataset
Filtering in the Appendix for details). We then randomly shufﬂe and split the whole dataset into a training
set, development set and test set.
IFLYTEK IFLYTEK (IFLYTEK CO., 2019) contains 17,332 app descriptions. The task is to assign
each description into one of 119 categories, such as food, car rental, education, etc. A data ﬁltering
technique similar to the one used for the TNEWS dataset has been applied.
CLUEWSC2020 The Chinese Winograd Schema Challenge dataset is an anaphora/coreference resolu-
tion task where the model is asked to decide whether a pronoun and a noun (phrase) in a sentence co-refer
(binary classiﬁcation), built following similar datasets in English (e.g., Levesque et al. (2012) and Wang
et al. (2019)). Sentences in the dataset are hand-picked from 36 contemporary literary works in Chinese.
Their anaphora relations are then hand-annotated by linguists, amounting to 1,838 questions in total.
4.2 Sentence Pair Tasks
Tasks in this section ask a model to predict relations between sentence pairs, or abstract-keyword pairs.
AFQMC The Ant Financial Question Matching Corpus 3 comes from Ant Technology Exploration
Conference (ATEC) Developer competition. It is a binary classiﬁcation task that aims to predict whether
two sentences are semantically similar.
CSL Chinese Scientiﬁc Literature dataset contains Chinese paper abstracts and their keywords from
core journals of China, covering multiple ﬁelds of natural sciences and social sciences. We generate fake
keywords through tf-idf and mix them with real keywords. Given an abstract and some keywords, the
task is to tell whether the keywords are all original keywords of a paper. It mainly evaluates the ability of
models to judge whether keywords can summarize the document.
OCNLI Original Chinese Natural Language Inference (OCNLI, Hu et al. (2020)) is collected closely
following procedures of MNLI (Williams et al., 2018). OCNLI is composed of 56k inference pairs from
ﬁve genres: news, government, ﬁction, TV transcripts and Telephone transcripts, where the premises
are collected from Chinese sources, and universities students in language majors are hired to write the
hypotheses. The annotator agreement is on par with MNLI. We believe the non-translation nature of
OCNLI makes it more suitable than XNLI (Conneau et al., 2018) as an NLU task speciﬁc for Chinese.
3https://dc.cloud.alipay.com/index\#/topic/intro?id=3



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 15):

available. For MS-MARCO passage ranking, MRR@10 and Recall@1k are reported. For the NQ
dataset, Recall@20 and Recall@100 are the main metrics.
Table 12: In-domain results. “target pre-train” refers to intermediate pre-training on the target corpus
before supervised fine-tuning. For NQ, we use the passage retrieval setting from DPR [30].
target pre-train? MS-MARCO NQ
MRR@10 R@1k R@20 R@100
ANCE [61] ✗ 33.0 95.9 81.9 87.5
RocketQAv2 [50] ✗ 38.8 98.1 83.7 89.0
SimLM [58] ✓ 41.1 98.7 85.2 89.7
E5small ✗ 37.5 98.1 84.6 89.8
E5base ✗ 38.5 98.5 86.1 90.7
E5large ✗ 39.4 98.7 86.4 90.5
C Negative Results
Here are some attempts that we eventually give up on:
Adding BM25 hard negatives Similar to DPR [ 30], we add one BM25 hard negative for each
positive pair during training. When using 15M data, this strategy improves the overall results by
∼ 0.5 points on the BEIR benchmark. However, running the BM25 algorithm over a 250M+ dataset
is too time-consuming even with multi-node and multi-process parallelism.
Using RoBERTa instead of BERT for initialization Though RoBERTa shows consistent gains on
many NLP tasks, we empirically find that RoBERTa performs worse than BERT initialization on
most of the BEIR benchmark datasets.
Auxiliary MLM objective We add a masked language modeling loss for25% of the training text
pairs. The numbers are on par with removing this auxiliary objective, but the training cost goes up.
16



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 15):

Preprint
(a) 182539: monotonic function
 (b) 1117099: active margin
Query
Relevant
ANCE Neg
BM25 Neg
Rand Neg (c) 1132213: yoga bow
Figure 7: t-SNE Plots for Losing Cases in Table 9.
margin” is a geographical terminology, not a ﬁnancial one (which we did not know ourselves and took some time
to ﬁgure out when conducting this case study). There are also some cases where the dense retrieved documents
make sense to us but were labeled irrelevant.
The t-SNE plots in Fig. 6 and Fig. 7 show many interesting patterns of the learned representation space. The
ANCE winning cases often correspond to clear separations of different document groups. For losing cases the
representation space is more mixed, or there is too few relevant documents which may cause the variances in
model performances. There are also many different interesting patterns in the ANCE-learned representation
space. We include the t-SNE plots for all 43 TREC DL Track queries in the supplementary material. More future
analyses of the learned patterns in the representation space may help provide more insights on dense retrieval.
16



Source: data\tc18_2309.07597v5\referenced_papers\[14]_1803.05449.pdf (Page 1):

name N task C examples label(s)
MR 11k sentiment (movies) 2 “Too slow for a younger crowd , too shallow for an older one.” neg
CR 4k product reviews 2 “We tried it out christmas night and it worked great .” pos
SUBJ 10k subjectivity/objectivity 2 “A movie that doesn’t aim too high , but doesn’t need to.” subj
MPQA 11k opinion polarity 2 “don’t want”; “would like to tell”; neg, pos
TREC 6k question-type 6 “What are the twin cities ?” LOC:city
SST-2 70k sentiment (movies) 2 “Audrey Tautou has a knack for picking roles that magnify her [..]” pos
SST-5 12k sentiment (movies) 5 “nothing about this movie works.” 0
Table 1: Classiﬁcation tasks. C is the number of classes and N is the number of samples.
name N task output premise hypothesis label
SNLI 560k NLI 3 “A small girl wearing a pink jacket
is riding on a carousel.”
“The carousel is moving.” entailment
SICK-E 10k NLI 3 “A man is sitting on a chair and rub-
bing his eyes”
“There is no man sitting on a chair
and rubbing his eyes”
contradiction
SICK-R 10k STS [0, 5] “A man is singing a song and playing
the guitar”
“A man is opening a package that
contains headphones”
1.6
STS14 4.5k STS [0, 5] “Liquid ammonia leak kills 15 in
Shanghai”
“Liquid ammonia leak kills at least
15 in Shanghai”
4.6
MRPC 5.7k PD 2 “The procedure is generally per-
formed in the second or third
trimester.”
“The technique is used during
the second and, occasionally, third
trimester of pregnancy.”
paraphrase
COCO 565k ICR sim
 3 “A group of people on some horses
riding through the beach.”
rank
Table 2: Natural Language Inference and Semantic Similarity tasks. NLI labels are contradiction, neutral and entail-
ment. STS labels are scores between 0 and 5. PD=paraphrase detection, ICR=image-caption retrieval.
for a broad set of tasks. To evaluate the quality of these
representations, we use them as features in various transfer
tasks.
Binary and multi-class classiﬁcation We use a set of
binary classiﬁcation tasks (see Table 1) that covers var-
ious types of sentence classiﬁcation, including sentiment
analysis (MR and both binary and ﬁne-grained SST) (Pang
and Lee, 2005; Socher et al., 2013), question-type (TREC)
(V oorhees and Tice, 2000), product reviews (CR) (Hu and
Liu, 2004), subjectivity/objectivity (SUBJ) (Pang and Lee,
2004) and opinion polarity (MPQA) (Wiebe et al., 2005).
We generate sentence vectors and classiﬁer on top, either in
the form of a Logistic Regression or an MLP. For MR, CR,
SUBJ and MPQA, we use nested 10-fold cross-validation,
for TREC cross-validation and for SST standard validation.
Entailment and semantic relatedness We also include
the SICK dataset (Marelli et al., 2014) for entailment
(SICK-E), and semantic relatedness datasets including
SICK-R and the STS Benchmark dataset (Cer et al., 2017).
For semantic relatedness, which consists of predicting a se-
mantic score between 0 and 5 from two input sentences, we
follow the approach of Tai et al. (2015a) and learn to pre-
dict the probability distribution of relatedness scores. Sen-
tEval reports Pearson and Spearman correlation. In addi-
tion, we include the SNLI dataset (Bowman et al., 2015),
a collection of 570k human-written English supporting the
task of natural language inference (NLI), also known as rec-
3Antonio Rivera - CC BY 2.0 - ﬂickr
ognizing textual entailment (RTE) which consists of pre-
dicting whether two input sentences are entailed, neutral
or contradictory. SNLI was speciﬁcally designed to serve
as a benchmark for evaluating text representation learning
methods.
Semantic Textual Similarity While semantic related-
ness requires training a model on top of the sentence em-
beddings, we also evaluate embeddings on the unsuper-
vised SemEval tasks. These datasets include pairs of sen-
tences taken from news articles, forum discussions, news
conversations, headlines, image and video descriptions la-
beled with a similarity score between 0 and 5. The goal is
to evaluate how the cosine distance between two sentences
correlate with a human-labeled similarity score through
Pearson and Spearman correlations. We include STS tasks
from 2012 (Agirre et al., 2012), 20134 (Agirre et al., 2013),
2014 (Agirre et al., 2014), 2015 (Agirre et al., 2015) and
2016 (Agirre et al., 2016). Each of these tasks includes
several subtasks. SentEval reports both the average and the
weighted average (by number of samples in each subtask)
of the Pearson and Spearman correlations.
Paraphrase detection The Microsoft Research Para-
phrase Corpus (MRPC) (Dolan et al., 2004) is composed
of pairs of sentences which have been extracted from
news sources on the Web. Sentence pairs have been
human-annotated according to whether they capture a para-
phrase/semantic equivalence relationship. We use the same
4Due to License issues, we do not include the SMT subtask.



#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[20]_1711.05073.pdf (Page 0):

DuReader: a Chinese Machine Reading Comprehension Dataset from
Real-world Applications
Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang,
Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, Haifeng Wang
Baidu Inc., Beijing, China
{hewei06, liukai20, liujing46, lvyajuan, zhaoshiqi, xiaoxinyan, liuyuan04, wangyizhong01,
wu hua, sheqiaoqiao, liuxuan, wutian, wanghaifeng}@baidu.com
Abstract
This paper introduces DuReader, a new
large-scale, open-domain Chinese ma-
chine reading comprehension (MRC)
dataset, designed to address real-world
MRC. DuReader has three advantages
over previous MRC datasets: (1) data
sources: questions and documents are
based on Baidu Search and Baidu Zhi-
dao1; answers are manually generated.
(2) question types: it provides rich
annotations for more question types,
especially yes-no and opinion questions,
that leaves more opportunity for the
research community. (3) scale: it contains
200K questions, 420K answers and 1M
documents; it is the largest Chinese
MRC dataset so far. Experiments show
that human performance is well above
current state-of-the-art baseline systems,
leaving plenty of room for the community
to make improvements. To help the
community make these improvements,
both DuReader 2 and baseline systems 3
have been posted online. We also organize
a shared competition to encourage the
exploration of more models. Since the
release of the task, there are signiﬁcant
improvements over the baselines.
1 Introduction
The task of machine reading comprehension
(MRC) aims to empower machines to answer
questions after reading articles (Rajpurkar et al.,
1Zhidao ( https://zhidao.baidu.com) is the
largest Chinese community-based question answering
(CQA) site in the world.
2http://ai.baidu.com/broad/download?
dataset=dureader
3https://github.com/baidu/DuReader
2016; Nguyen et al., 2016). In recent years, a
number of datasets have been developed for MRC,
as shown in Table 1. These datasets have led to
advances such as Match-LSTM (Wang and Jiang,
2017), BiDAF (Seo et al., 2016), AoA Reader (Cui
et al., 2017), DCN (Xiong et al., 2017) and R-
Net (Wang et al., 2017). This paper hopes to
advance MRC even further with the release of
DuReader, challenging the community to deal
with more realistic data sources, more types of
questions and more scale, as illustrated in Tables
1-4. Table 1 highlights DuReader’s advantages
over previous datasets in terms of data sources and
scale. Tables 2-4 highlight DuReader’s advantages
in the range of questions.
Ideally, a good dataset should be based on ques-
tions from real applications. However, many ex-
isting datasets have been forced to make vari-
ous compromises such as: (1) cloze task: Data
is synthesized missing a keyword. The task is
to ﬁll in the missing keyword (Hermann et al.,
2015; Cui et al., 2016; Hill et al., 2015). (2)
multiple-choice exams:Richardson et al. (2013)
collect both ﬁctional stories and the corresponding
multiple-choice questions by crowdsourcing. Lai
et al. (2017) collect the multiple-choice questions
from English exams. (3) crowdsourcing: Turkers
are given documents (e.g., articles from the news
and/or Wikipedia) and are asked to construct ques-
tions after reading the documents(Trischler et al.,
2017; Rajpurkar et al., 2016; Koˇcisk`y et al., 2017).
The limitations of the datasets lead to build
datasets based on queries that real users submit-
ted to real search engines. MS-MARCO (Nguyen
et al., 2016) is based on Bing logs (in English),
and DuReader (this paper) is based on the logs
of Baidu Search (in Chinese). Besides question
sources, DuReader complements MS-MARCO
and other datasets in the following ways:
question types:DuReader contains a richer in-
arXiv:1711.05073v4  [cs.CL]  11 Jun 2018



Source: data\tc18_2309.07597v5\referenced_papers\[20]_1711.05073.pdf (Page 8):

ings actually summarize answers with their own
comprehension in DuReader. How to summarize
or generate the answers deserves more research.
Forth, as the ﬁrst release of the dataset, it is far
from perfection and it leaves much room for im-
provement. For example, we annotate only opin-
ion tags for yes-no questions, we will also anno-
tate opinion tags for description and entity ques-
tions. We would like to gather feedback from the
community to improve DuReader continually.
Overall, it is necessary to propose new algo-
rithms and models to tackle with real-world read-
ing comprehension problems. We hope that the
DuReader would be a good start for facilitating the
MRC research.
5 A Shared Task
To encourage the exploration of more models from
the research community, we organize an online
competition6. Each participant can submit the re-
sult and evaluate the system performance at the
online website. Since the release of the task, there
are signiﬁcant improvements over the baselines,
For example, a team obtained 51.2 ROUGE-L on
our dataset (when the paper was submitted). The
gap between our BiDAF baseline model (with 39.0
ROUGE-L) and human performance (with 57.4
ROUGE-L) has been signiﬁcantly reduced. It is
expected that the remaining gap the system per-
formances and human performance will be harder
to close, but such efforts will lead to advances in
machine reading comprehension.
6 Conclusion
This paper announced the release of DuReader, a
new dataset for researchers interested in machine
reading comprehension (MRC). DuReader has
three advantages over previous MRC datasets: (1)
data sources (based on search logs and the ques-
tion answering community), (2) question types
(fact/ opinion & entity/ description/ yes-no) and
(3) scale (largest Chinese MRC dataset so far).
We have made our dataset freely available and
organize a shared competition to encourage the ex-
ploration of more models. Since the release of
the task, we have already seen signiﬁcant improve-
ments from more sophisticated models.
6https://ai.baidu.com/broad/
leaderboard?dataset=dureader
Acknowledgements
We would like to thank Dr. Kenneth Ward Church
for his valuable suggestions and revisions on this
paper, Prof. Sujian Li for her supports on this pa-
per, and the anonymous reviewers for their helpful
comments on this work.
References
Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang,
Ting Liu, and Guoping Hu. 2017. Attention-over-
attention neural networks for reading comprehen-
sion. In Proceedings of 55th Annual Meeting of the
Association for Computational Linguistics, pages
593–602.
Yiming Cui, Ting Liu, Zhipeng Chen, Shijin Wang, and
Guoping Hu. 2016. Consensus attention-based neu-
ral networks for chinese reading comprehension.
Matthew Dunn, Levent Sagun, Mike Higgins, Ugur
Guney, V olkan Cirik, and Kyunghyun Cho. 2017.
Searchqa: A new q&a dataset augmented with
context from a search engine. arXiv preprint
arXiv:1704.05179.
Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. InAdvances in Neu-
ral Information Processing Systems, pages 1693–
1701.
Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2015. The goldilocks principle: Reading
children’s books with explicit memory representa-
tions. arXiv preprint arXiv:1511.02301.
Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. CoRR.
Diederik P. Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. CoRR.
Tom´aˇs Ko ˇcisk`y, Jonathan Schwarz, Phil Blunsom,
Chris Dyer, Karl Moritz Hermann, G ´abor Melis,
and Edward Grefenstette. 2017. The narrativeqa
reading comprehension challenge. arXiv preprint
arXiv:1712.07040.
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy. 2017. Race: Large-scale reading
comprehension dataset from examinations. arXiv
preprint arXiv:1704.04683.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81.



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 11):

Preprint
Table 6: Coverage of TREC 2019 DL Track labels on Dense Retrieval methods. Overlap with BM25
is calculated on top 100 retrieved documents.
TREC DL Passage TREC DL Document
Method Recall@1K Hole@10 Overlap w. BM25 Recall@100 Hole@10 Overlap w. BM25
BM25 0.685 5.9% 100% 0.387 0.2% 100%
BM25 Neg 0.569 25.8% 11.9% 0.217 28.1% 17.9%
BM25 + Rand Neg 0.662 20.2% 16.4% 0.240 21.4% 21.0%
ANCE (FirstP) 0.661 14.8% 17.4% 0.266 13.3% 24.4%
ANCE (MaxP) - - - 0.286 11.9% 24.9%
A A PPENDIX
A.1 M ORE EXPERIMENTAL DETAILS
More Details on TREC DL Benchmarks:There are two tasks in the TREC DL 2019 Track: document retrieval
and passage retrieval. The training and development sets are from MS MARCO, which includes passage level
relevance labels for one million Bing queries (Bajaj et al., 2016). The document corpus was post-constructed by
back-ﬁlling the body texts of the passage’s URLs and their labels were inherited from its passages (Craswell
et al., 2020). The testing sets are labeled by NIST accessors on the top 10 ranked results from past Track
participants (Craswell et al., 2020).
TREC DL ofﬁcial metrics include NDCG@10 on test and MRR@10 on MARCO Passage Dev. MARCO
Document Dev is noisy and the recall on the DL Track testing is less meaningful due to low label coverage on
DR results. There is a two-year gap between the construction of the passage training data and the back-ﬁlling of
their full document content. Some original documents were no longer available. There is also a decent amount
of content changes in those documents during the two-year gap, and many no longer contain the passages. This
back-ﬁlling perhaps is the reason why many Track participants found the passage training data is more effective
than the inherited document labels. Note that the TREC testing labels are not inﬂuenced as the annotators were
provided the same document contents when judging.
All the TREC DL runs are trained using these training data. Their inference results on the testing queries of the
document and the passage retrieval tasks were evaluated by NIST assessors in the standard TREC-style pooling
technique (V oorhees, 2000). The pooling depth is set to 10, that is, the top 10 ranked results from all participated
runs are evaluated, and these evaluated labels are released as the ofﬁcial TREC DL benchmarks for passage and
document retrieval tasks.
More Details on OpenQA Experiments: All the DPR related experimental settings, baseline systems, and
DPR Reader are based on their open source libarary1. The RAG-Token reader uses their open-source release in
huggingface2. The RAG-Seq release in huggingface is not yet stable by the time we did our experiment, thus we
choose the RAG-Token in our OpenQA experiment. RAG only releases the NQ models thus we use DPR reader
on TriviaQA. We feed top 20 passages from ANCE to RAG-Token on NQ and top 100 passages to DPR’s BERT
Reader, following the guideline in their open-source codes.
More Details on Baselines: The most representative sparse retrieval baselines in TREC DL include the standard
BM25 (“bm25base” or “bm25base_p”), Best TREC Sparse Retrieval (“bm25tuned_rm3” or “bm25tuned_prf_p”)
with tuned query expansion (Lavrenko & Croft, 2017), and Best DeepCT (“dct_tp_bm25e2”, doc only), which
uses BERT to estimate the term importance for BM25 (Dai & Callan, 2019a). These three runs represent
the standard sparse retrieval, best classical sparse retrieval, and the recent progress of using BERT to im-
prove sparse retrieval. We also include the standard cascade retrieval-and-reranking systems BERT Reranker
(“bm25exp_marcomb” or “p_exp_rm3_bert”), which is the best run using standard BERT on top of query/doc
expansion, from the groups with multiple top MARCO runs (Nogueira & Cho, 2019; Nogueira et al., 2019).
BERT-Siamese Conﬁgurations: We follow the network conﬁgurations in Luan et al. (2020) in all Dense
Retrieval methods, which we found provides the most stable results. More speciﬁcally, we initialize the BERT-
Siamese model with RoBERTa base (Liu et al., 2019) and add a768 ×768 projection layer on top of the last
layer’s “[CLS]” token, followed by a layer norm.
Implementation Details: The training often takes about 1-2 hours per ANCE epoch, which is whenever new
ANCE negative is ready, it immediately replaces existing negatives in training, without waiting. It converges
in about 10 epochs, similar to other DR baselines. The optimization uses LAMB optimizer, learning rate 5e-6
for document and 1e-6 for passage retrieval, and linear warm-up and decay after 5000 steps. More detailed
hyperparameter settings can be found in our code release.
1https://github.com/facebookresearch/DPR.
2https://huggingface.co/transformers/master/model_doc/rag.html
12



Source: data\tc18_2309.07597v5\referenced_papers\[25]_2004.04906.pdf (Page 11):

A Distant Supervision
When training our ﬁnal DPR model using Natural
Questions, we use the passages in our collection
that best match the gold context as the positive
passages. As some QA datasets contain only the
question and answer pairs, it is thus interesting
to see when using the passages that contain the
answers as positives (i.e., the distant supervision
setting), whether there is a signiﬁcant performance
degradation. Using the question and answer to-
gether as the query, we run Lucene-BM25 and pick
the top passage that contains the answer as the pos-
itive passage. Table 5 shows the performance of
DPR when trained using the original setting and
the distant supervision setting.
B Alternative Similarity Functions &
Triplet Loss
In addition to dot product (DP) and negative log-
likelihood based on softmax (NLL), we also exper-
iment with Euclidean distance (L2) and the triplet
loss. We negate L2 similarity scores before ap-
plying softmax and change signs of question-to-
positive and question-to-negative similarities when
applying the triplet loss on dot product scores. The
margin value of the triplet loss is set to 1. Ta-
ble 6 summarizes the results. All these additional
experiments are conducted using the same hyper-
parameters tuned for the baseline (DP, NLL).
Note that the retrieval accuracy for our “baseline”
settings reported in Table 5 (Gold) and Table 6
(DP, NLL) is slightly better than those reported in
Table 3. This is due to a better hyper-parameter
setting used in these analysis experiments, which
is documented in our code release.
C Qualitative Analysis
Although DPR performs better than BM25 in gen-
eral, the retrieved passages of these two retrievers
actually differ qualitatively. Methods like BM25
are sensitive to highly selective keywords and
phrases, but cannot capture lexical variations or se-
mantic relationships well. In contrast, DPR excels
at semantic representation, but might lack sufﬁcient
capacity to represent salient phrases which appear
rarely. Table 7 illustrates this phenomenon with
two examples. In the ﬁrst example, the top scor-
ing passage from BM25 is irrelevant, even though
keywords such as England and Ireland appear mul-
tiple times. In comparison, DPR is able to return
Top-1 Top-5 Top-20 Top-100
Gold 44.9 66.8 78.1 85.0
Dist. Sup. 43.9 65.3 77.1 84.4
Table 5: Retrieval accuracy on the development set of
Natural Questions, trained on passages that match the
gold context (Gold) or the top BM25 passage that con-
tains the answer (Dist. Sup.).
Sim Loss Retrieval Accuracy
Top-1 Top-5 Top-20 Top-100
DP NLL 44.9 66.8 78.1 85.0
Triplet 41.6 65.0 77.2 84.5
L2 NLL 43.5 64.7 76.1 83.1
Triplet 42.2 66.0 78.1 84.9
Table 6: Retrieval Top-kaccuracy on the development
set of Natural Questions using different similarity and
loss functions.
the correct answer, presumably by matching “body
of water” with semantic neighbors such as sea and
channel, even though no lexical overlap exists. The
second example is one where BM25 does better.
The salient phrase “Thoros of Myr” is critical, and
DPR is unable to capture it.
D Joint Training of Retriever and
Reader
We ﬁx the passage encoder in our joint-training
scheme while allowing only the question encoder
to receive backpropagation signal from the com-
bined (retriever + reader) loss function. This allows
us to leverage the HNSW-based FAISS index for
efﬁcient low-latency retrieving, without reindexing
the passages during model updates. Our loss func-
tion largely follows ORQA’s approach, which uses
log probabilities of positive passages selected from
the retriever model, and correct spans and passages
selected from the reader model. Since the passage
encoder is ﬁxed, we could use larger amount of
retrieved passages when calculating the retriever
loss. Speciﬁcally, we get top 100 passages for each
question in a mini-batch and use the method similar
to in-batch negative training: all retrieved passages’
vectors participate in the loss calculation for all
questions in a batch. Our training batch size is set
to 16, which effectively gives 1,600 passages per
question to calculate retriever loss. The reader still
uses 24 passages per question, which are selected



Source: data\tc18_2309.07597v5\referenced_papers\[20]_1711.05073.pdf (Page 2):

Fact Opinion
Entity iphone哪天发布 2017最好看的十部电影
On which day will iphone be released Top 10 movies of 2017
Description 消防车为什么是红的 丰田卡罗拉怎么样
Why are ﬁretrucks red How is Toyota Carola
YesNo 39.5度算高烧吗 学围棋能开发智力吗
Is 39.5 degree a high fever Does learning to play go improve intelligence
Table 2: Examples of the six types of questions in Chinese (with glosses in English). Previous datasets
have focused on fact-entity and fact-description, though all six types are common in search logs.
Fact Opinion Total
Entity 23.4% 8.5% 31.9%
Description 34.6% 17.8% 52.5%
YesNo 8.2% 7.5% 15.6%
Total 66.2% 33.8% 100.0%
Table 3: Pilot Study found that all six types
of question queries are common in search logs.
Previous MRC datasets have emphasized span-
selection methods. Such methods are appropriate
for fact-entity and fact-description. Opinions and
yes-no leave big opportunities (about 33.8% and
15.6% of the sample, respectively).
for a relatively small fraction (23.4%) of the sam-
ple. Fact-descriptions account for a larger fraction
of the sample (34.6%). From this Table, we can
see that opinions (33.8%) are common in search
logs. Yes-No questions account for 15.6%, with
one half about fact, another half about opinion.
Previous MRC datasets have emphasized span-
selection methods. Such methods are appropriate
for fact-entity and fact-description, but it is prob-
lematic when the answer involves a summary of
multiple sentences from multiple documents, es-
pecially for Yes-no and opinion questions. This
requires methods that go beyond currently popu-
lar methods such as span selection, and leave large
opportunity for the community.
3 Scaling up from the Pilot to DuReader
3.1 Data Collection and Annotation
3.1.1 Data Collection
After the successful completion of the pilot study,
we began work on scaling up the relatively small
sample of 1k questions to a more ambitious col-
lection of 200k questions.
The DuReader is a sequence of 4-tuples: {q, t,
D, A}, where q is a question, t is a question type,D
is a set of relevant documents, and A is an answer
set produced by human annotators.
Before labeling question types, we need to col-
lect a set of questions q from search logs. Accord-
ing to our estimation, there are about 21% ques-
tion queries in search logs. It would take too much
time, if human annotators manually label each
query in search logs. Hence, we ﬁrst randomly
sample the most frequent queries from search logs,
and use a pre-trained classiﬁer (with recall higher
than 90%) to automatically select question queries
from search logs. Then, workers will annotate the
question queries selected by the classiﬁer. Since
this annotation task is relatively easy, each query
was annotated by one worker. The experts will
further review all the annotations by workers and
correct them if the annotation is wrong. The accu-
racy of workers’ annotation (judged by experts) is
higher than 98%.
Initially, we have 1M frequent queries sam-
pled from search logs. The classiﬁer automati-
cally selected 280K question queries. After human
annotation, there are 210K question queries left.
Eventually, we uniformly sampled 200K questions
from the 210K question queries.
We then collect the relevant documents, D, by
submitting questions to two sources, Baidu Search
and Baidu Zhidao. Note that the two sources are
very different from one another; Zhidao contains
user-generated content and tends to have more
documents relevant to opinions. Since the two
sources are so different from each another, we de-
cided to randomly split the 200k unique questions
into two subsets. The ﬁrst subset was used to pro-
duce the top 5 ranked documents from one source,
and the second subset was used to produce the top
5 ranked documents from the other source.
We also believe that it is important to keep the
entire document unlike previous work which kept
a single paragraph (Rajpurkar et al., 2016) or a few



#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[60]_2304.03679.pdf (Page 6):

T2Ranking: A large-scale Chinese Benchmark for Passage Ranking SIGIR ’23, July 23–27, 2023, Taipei, Taiwan
General Medical Education Emotion E-commerce IT Government Vehicle
Domains
0.00
0.05
0.10
0.15
0.20
0.25
0.30Proportion(%)
Training
Test
Figure 3: Domain statistics for the training and test queries
in T2Ranking.
Table 4: ILS scores of different datasets. Lower ILS scores re-
fer to higher diversity levels of queries.
Dataset ILS Score
MS-MARCO [16] 0.227
Multi-CPR [15] 0.186
DuReaderretrieval [20] 0.152
T2Ranking (ours) 0.144
43.33%
10.52%
25.78%
20.37%
0
1
2
3
(a) The distribution of relevance
annotations in the training set.
54.35%
15.96%
23.56%
6.13%
0
1
2
3
(b) The distribution of relevance
annotations in the test set.
Figure 4: Pie chart of the annotation distribution.
6 EXPERIMENTS AND RESULTS
Consistent with modern information retrieval systems, theretrieval-
then-re-ranking paradigm is utilized in our experiments. In this
section, we examine the performance of commonly-used retrievers
and re-rankers on T2Ranking.
6.1 Retrieval Performance
Baselines. Existing retrieval models can be broadly divided into
sparse retrieval models and dense retrieval models. Sparse retrieval
models focus on exact matching signals to design a relevance scor-
ing function, with BM25 being the most prominent and widely-
utilized baseline due to its promising performance. Additionally,
dense retrieval models leverage deep neural networks to learn
low-dimensional dense embeddings for queries and documents.
Dual-encoder 
w/ BM25 Neg
Dual-encoder 
w/ Mined NegBM25
negatives
Mined
negatives
Cross-encoder 
w/ Mined Neg
Figure 5: Illustration for the training process of baselines
used in our experiments. First, we train a dual-encoder
with BM25 negatives, which is similar to DPR [12]. Sec-
ond, we train the dual-encoder and cross-encoder with the
global negative sampling strategy proposed in several stud-
ies [15, 20].
P assage 
E ncoder
Q uery 
E ncoder
Q uery P assage 
E m bq E m bp
(a) Dual-encoder.
Cross E ncoder
Q uery P assage 
Fully-connected (b) Cross-encoder.
Figure 6: Illustration for the architecture of dual-encoder
and cross-encoder.
Generally, most existing dense retrieval methods adhere to the cas-
cade training paradigm [15, 20, 21]. Therefore, to facilitate easier
comparison in future studies on our dataset, we simplify the train-
ing process as illustrated in Figure 5 as in [15, 20]. Specifically, we
utilize the dual-encoder (DE) as the architecture of dense retrieval
models, which is illustrated in Figure 6(a). The following methods
are employed as our baselines to evaluate the retrieval performance
on T2Ranking.
•QL (query likelihood) [19] is a representative statistical lan-
guage model that measures the relevance of passages by
modeling the generation of a query.
•BM25 [23] is a widely-used sparse retrieval baseline.
•DE w/ BM25 Neg is equivalent to DPR [12], which is the
first work that uses the pre-trained language model as the
backbone for the passage retrieval task.
•DE w/ Mined Neg enhance the performance of DPR by
sampling hard negatives globally from the entire corpus as
in ANCE [28] and RocketQA [21].
•DPTDR [25] is the first work that employs prompt tuning
for dense retrieval.
Among them, QL and BM25 are sparse retrieval models, whereas
the others are dense retrieval models
Implementation details. BM25 is implemented by Pyserini [14]
with default parameters. The dual-encoder models are implemented



Source: data\tc18_2309.07597v5\referenced_papers\[60]_2304.03679.pdf (Page 5):

SIGIR ’23, July 23–27, 2023, Taipei, Taiwan Xie, et al.
Figure 1: Illustration for a web document from Wikipedia
which is well-written with clearly defined paragraphs.
de-duplication to save the annotation cost while retaining more
diverse training samples for improving model performance.
4.4 Active Learning-based Data Sampling
In practice, we observe that not all training samples can further en-
hance the ranking model’s performance. Training samples, that can
be easily predicted accurately by a model, are unlikely to provide
useful information for model training.
To address this issue, we borrow the light of active learning [22],
using a model to choose more informative training samples for
further annotations. Active learning is a framework that enables
models to participate in the data annotation process. The aim of ac-
tive learning is to minimize the amount of annotated data required
while maintaining or improving model performance. Formally, ac-
tive learning is an iterative process where the model makes pre-
dictions on a pool of unannotated samples. The samples with the
highest uncertainty or informativeness are selected for annotation
by annotators, and the annotated samples are added to the training
data. The model is then updated with the newly annotated data. The
framework of active learning is illustrated in Figure 2. Concretely,
a query-passage re-ranking model, specifically a cross-encoder, is
trained using data constructed from the initial stage. In the second
stage, unannotated query-passage pairs are obtained and evalu-
ated for relevance by the cross-encoder. Pairs with high confidence
scores are filtered out as they do not provide significant information
for further performance improvement, while pairs with low con-
fidence scores, which are typically considered noise samples, are
also eliminated. The remaining pairs are submitted to annotators
for fine-grained annotation. The annotated query-passage pairs are
then added to the training set and the cross-encoder is updated
with newly acquired samples.
5 DATA STATISTICS
This section presents the data statistics of T2Ranking.
Expert
annotators
Annotated 
qry-psg pairs
Unannotated 
qry-psg pairs
Cross-encoder
Sample 
selection
Submiting for 
annotation
Adding to the 
training set
Model training
Figure 2: Illustration for the framework of active learning.
Table 3: Statistic of queries in T2Ranking.
Quantity Max. length Mean.length
Training set 258,042 40 11.1
Test set 49,662 38 10.99
Query. Table 3 provides a summary of the statistics of queries in
T2Ranking. The maximum and mean lengths of queries in the train-
ing and test sets are nearly identical. We further analyze the domain
distribution of queries in the training and test sets, as demonstrated
in Figure 3. Domain tags are provided by the Sogou search engine.
The query domain distribution in the training and test sets is con-
sistent, and the queries cover a broad range of domains. We also
demonstrate the diversity level of queries by resorting to the metric,
intra-list similarity (ILS) [31] which can be defined as
𝑠(q𝑖,q𝑗)=
BERT(q𝑖)[𝑐𝑙𝑠]·BERT(q𝑗)[𝑐𝑙𝑠]
∥BERT(q𝑖)[𝑐𝑙𝑠]∥∥BERT(q𝑗)[𝑐𝑙𝑠]∥, (3)
ILSQ=
Í|Q|
𝑖=1
ÍQ
𝑗=𝑖+1 𝑠(q𝑖,q𝑗)
ÍQ
𝑖=1
ÍQ
𝑗=𝑖+1 1
, (4)
where BERT [13] is a pre-trained language model that is often used
as the backbone model for various tasks [7–9, 12, 18]. A lower ILS
score indicates a lower similarity between queries in the benchmark,
thus indicating a higher level of diversity. We calculated the ILS
scores of T2Ranking, as well as those of several popular datasets,
such as MSMARCO, Multi-CPR and DuReaderretrieval. The results
are shown in Table 4. From the table, it is evident that the queries in
T2Ranking are more diverse, as indicated by a lower ILS score. Note
that, T2Ranking compromises queries with higher diversity even
than those in Multi-CPR, which contains queries from different
vertical search applications.
Document & Passage. T2Ranking comprises passages extracted
from 1,752,482 web documents, with a total of 2,303,643 passages
after segmentation. On average, each web document is divided into
1.31 passages of which the mean length is 632.6.
Relevance Annotation. We display the distribution of the 4-level
relevance annotations in Figure 4. In the training set, on average,
each query is annotated with 6.25 passages, while in the test set,
each query is annotated with an average of 15.75 passages.



Source: data\tc18_2309.07597v5\referenced_papers\[60]_2304.03679.pdf (Page 0):

T2Ranking: A large-scale Chinese Benchmark for Passage
Ranking
Xiaohui Xie
xiexiaohui@mail.tsinghua.edu.cn
DCST, Tsinghua University.
Zhongguancun Lab.
Beijing, China
Qian Dong
dq22@mails.tsinghua.edu.cn
DCST, Tsinghua University.
Zhongguancun Lab.
Beijing, China
Bingning Wang
bryantwwang@tencent.com
Tencent Inc.
Beijing, China
Feiyang Lv
feiyanglv@tencent.com
Tencent Inc.
Beijing, China
Ting Yao
tessieyao@tencent.com
Tencent Inc.
Beijing, China
Weinan Gan
carrygan@tencent.com
Tencent Inc.
Beijing, China
Zhijing Wu
wuzhijing.joyce@gmail.com
Beijing Institute of Technology
Beijing, China
Xiangsheng Li
lixsh6@gmail.com
Tencent Inc.
Beijing, China
Haitao Li
liht22@mails.tsinghua.edu.cn
DCST, Tsinghua University.
Zhongguancun Lab.
Beijing, China
Yiqun Liu
yiqunliu@tsinghua.edu.cn
DCST, Tsinghua University.
Zhongguancun Lab.
Beijing, China
Jin Ma
daniellwang@tencent.com
Tencent Inc.
Beijing, China
ABSTRACT
Passage ranking involves two stages: passage retrieval and pas-
sage re-ranking, which are important and challenging topics for
both academics and industries in the area of Information Retrieval
(IR). However, the commonly-used datasets for passage ranking
usually focus on the English language. For non-English scenarios,
such as Chinese, the existing datasets are limited in terms of data
scale, fine-grained relevance annotation and false negative issues.
To address this problem, we introduce T2Ranking, a large-scale
Chinese benchmark for passage ranking. T2Ranking comprises
more than 300K queries and over 2M unique passages from real-
world search engines. Expert annotators are recruited to provide
4-level graded relevance scores (fine-grained) for query-passage
pairs instead of binary relevance judgments (coarse-grained). To
ease the false negative issues, more passages with higher diversities
are considered when performing relevance annotations, especially
in the test set, to ensure a more accurate evaluation. Apart from
the textual query and passage data, other auxiliary resources are
also provided, such as query types and XML files of documents
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’23, July 23–27, 2023, Taipei, Taiwan
© 2023 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/XXXXXXX.XXXXXXX
which passages are generated from, to facilitate further studies. To
evaluate the dataset, commonly used ranking models are imple-
mented and tested on T2Ranking as baselines. The experimental
results show that T2Ranking is challenging and there is still scope
for improvement. The full data 1 and all codes are available at
https://github.com/THUIR/T2Ranking/
KEYWORDS
Test collection, Passage retrieval, Passage re-ranking, Passage rank-
ing, Search evaluation
ACM Reference Format:
Xiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv, Ting Yao, Weinan
Gan, Zhijing Wu, Xiangsheng Li, Haitao Li, Yiqun Liu, and Jin Ma. 2023.
T2Ranking: A large-scale Chinese Benchmark for Passage Ranking. In Pro-
ceedings of The 46th International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR ’23). ACM, New York, NY, USA,
10 pages. https://doi.org/XXXXXXX.XXXXXXX
1 INTRODUCTION
Passage ranking is a crucial component of information retrieval
systems. The promising performance of passage ranking leads to
satisfaction of search users and benefits multiple IR-related applica-
tions, e.g., question answering [1] and reading comprehension [17].
Typically, passage ranking encapsulates two coherent stages, i.e.,
passage retrieval and passage re-ranking. The goal of passage rank-
ing is to compile a search result list ordered in terms of relevance
1The dataset is licensed under the Apache License 2.0
arXiv:2304.03679v1  [cs.IR]  7 Apr 2023



Source: data\tc18_2309.07597v5\referenced_papers\[60]_2304.03679.pdf (Page 2):

T2Ranking: A large-scale Chinese Benchmark for Passage Ranking SIGIR ’23, July 23–27, 2023, Taipei, Taiwan
Table 1: The data statistics of datasets commonly used in passage ranking. Qrys (Psgs): Queries (Passages). FR(SR):
First (Second)-stage of passage ranking, i.e., passage Retrieval (Re-ranking).
Dataset Lang #Qrys #Psgs Qrys.source Psgs.source Annotation Task
Trec Car [6] EN 2M 30M Wiki doc. Wiki doc. Binary SR
TriviaQA [11] EN 95K 650K Trivia Web. Wiki./Web doc. Binary FR, SR
MS-MARCO [16] EN 516K 8.8M User logs Web doc. Binary FR, SR
Sogou-SRR [29] CN 6K 63K User logs Web doc. Fine-grained SR
Sogou-QCL [30] CN 537K 9M User logs Web doc. Click labels SR
TianGong-PDR [27] CN 70 11K User logs News doc Fine-grained FR, SR
mMarco-Chinese [3] CN 516K 8.8M User logs Web doc. Binary FR, SR
Multi-CPR [15] CN 303K 3M User logs Result Title Binary FR, SR
DuReaderretrieval [20] CN 97K 8.9M User logs Web doc. Binary FR, SR
T2Ranking(Ours) CN 307K 2.3M User logs Web doc. Fine-grained FR, SR
2 RELATED WORK
There are several benchmark datasets developed for passage rank-
ing. For datasets that have relevance annotations for all query-
passage pairs, both passage retrieval and passage re-ranking tasks
can be tested. Other datasets, however, only focus on passage re-
ranking tasks, providing relevance annotations only for query-
passage pairs in which the passages have been extracted from the
initial result lists recalled by the first-stage retrievers. We use FR
to denote the first stage of passage ranking, i.e., passage retrieval
and SR to denote the second stage of passage ranking, i.e., passage
re-ranking as shown in Table 1.
Commonly used datasets for passage ranking are constructed for
the English community. Trec Complex Answer Retrieval (CAR) [6]
uses topics, outlines, and paragraphs extracted from Wikipedia.
For the training set, a passage is considered relevant if it is found
within the Wikipedia pages of the topic and non-relevant otherwise.
The test set, comprised of 113 complex topics, has 50 passages per
topic that are manually annotated. TriviaQA [11] gathers question-
answer pairs from 14 trivia and quiz-league websites and passages
from Wikipedia and web documents. MS-MARCO [16] is widely
utilized due to its large scale. Unlike Trec Car and TriviaQA, queries
in MS-MARCO are sourced from user-generated queries, which are
question-based, from the Bing search engine 3. The Passages are
extracted from realistic web documents returned by the same search
engine. Then human editors are recruited and instructed to create
a natural language answer with the correct information extracted
strictly from the passages provided given particular queries. The
relevance levels of passages in both TriviaQA and MS-MARCO
are determined in a binary fashion, based on whether or not the
passages contain facets of the true answer to a given query.
For the Chinese community, there exist several datasets designed
for training and evaluating passage ranking models. Drawing upon
the Sogou search engine, three datasets have been established,
namely Sogou-SRR [29], Sogou-QCL [30] and TianGong-PDR [27].
Sogou-SRR (Search Result Relevance) consists of 6K queries and
corresponding top 10 search results. For each search result, the
screenshot, title, snippet, HTML source code, parse tree, URL as
well as a 4-grade relevance score and the result type are provided.
3https://www.bing.com
Sogou-QCL is a large-scale dataset compromised of 537K queries
and more than 9 million Chinese web pages. Rather than human-
generated relevance judgments, relevance levels of query-result
pairs are assessed based on click labels. Queries from Tiangong-PDR
are collected from Sogou’s search logs, while passages are obtained
from Web pages data from the Sina news website4. Moreover, four-
grade human-assessed relevance labels for each query-passage pair
are available. Besides, mMarco-Chinese [3] is constructed via ma-
chine translation from MS-MARCO. However, these datasets are
not large-scale and/or human-generated. Recently, Qiu et al. [20]
propose a new dataset, named DuReaderretrieval, for benchmark-
ing the passage retrieval models from Baidu search 5. Similar to
MS-MARCO, queries in DuReaderretrieval are question-based, and
human-generated answers are collected to access the relevance
levels of passages. Long et al . [15] build Multi-CPR which is a
multi-domain dataset for passage ranking. Queries and passages for
Multi-CPR are gathered from three different vertical search systems:
E-commerce, Entertainment Video, and Medical. Rather than being
extracted from web documents, passages in Multi-CPR refer to ti-
tles of search results, such as product titles in E-commerce search,
resulting in shorter passage lengths. Human annotators have been
recruited to judge the relevance level (binary) of the query-passage
pairs. For each query, the most semantically relevant passage is
marked as positive, while the others are marked as negative.
3 TASK DEFINITION
In this section, we formally define the tasks in T2Ranking. Our
proposed dataset focuses on two stages of passage ranking, namely,
passage retrieval and re-ranking. This aligns with the pipeline of
modern information retrieval systems, which follows the retrieval-
then-re-ranking paradigm.
The goal of passage retrieval is to retrieve candidate passages in
response to a given query. Given a query𝑞, a retrieval model is used
to retrieve a candidate set of passages K= {pq
𝑗}𝐾
𝑗=1 from a large
corpus G= {p𝑖}𝐺
𝑖=1, where 𝐾 ≪𝐺. In particular, a passage consists
of a sequence of wordsp = {𝑤𝑝}|p|
𝑝=1, where|p|represents the length
4https://www.sina.com.cn/
5https://www.baidu.com/



Source: data\tc18_2309.07597v5\referenced_papers\[60]_2304.03679.pdf (Page 7):

SIGIR ’23, July 23–27, 2023, Taipei, Taiwan Xie, et al.
Table 5: Performance of retrieval models on the test set of
T2Ranking.
MRR@10 Recall @50 Recall@1K
QL .2803 .3915 .6858
BM25 .3579 .4918 .7426
DE w/ BM25 Neg .4877 .7123 .9104
DE w/ Mined Neg .5191 .7357 .9147
DPTDR .5285 .7423 .9211
by the deep learning framework PyTorch on up to 8 NVIDIA Tesla
A100 GPUs (with 80G RAM). We use the off-the-shelf Chinese
BERTbase to initialize the dual-encoder. The maximal length of
queries and passages are set to 32 and 256, respectively. The nega-
tives are sampled from the top 200 passages recalled by BM25 or
DE w/ BM25 Neg. The ratio of positive:negative is set to 1:1. We
train the dual-encoder for 100 epochs with a learning rate of 3e-5.
Metrics. The following evaluation metrics are used in our ex-
periments to examine the retrieval performance of baselines on
T2Ranking: (1) Mean Reciprocal Rank for the top 10 retrieved
passages (MRR@10), (2) Recall for the top- 𝐾 retrieved passages
(Recall@𝐾). Notably, for the retrieval task, we considerLevel-2 and
Level-3 passages as relevant passages, and all other passages are
regarded as irrelevant passages. For a comprehensive comparison,
we report Recall@50 and Recall@1K on the test queries. Following
the evaluation settings of MS-MARCO and DuReaderretrieval, MRR
is defined as the average of the reciprocal ranks of thefirst relevant
passage for a set of queries. The MRR is a value between 0 and 1,
with a higher value indicating that the system is better at ranking
the most relevant passage higher in the list. Meanwhile, Recall is
defined as the fraction of relevant passages that are retrieved among
all relevant passages, also with a value between 0 and 1, where a
higher value indicates that the system is better at retrieving all
relevant passages. MRR and Recall measure different aspects of
retrieval performance. MRR@𝐾 and Recall@𝐾 can be depicted as:
𝑀𝑅𝑅@𝐾 = 1
|Q|
∑︁
𝑞∈Q
I(𝑟𝑎𝑛𝑘 ≤𝐾)
𝑟𝑎𝑛𝑘 , (5)
𝑅𝑒𝑐𝑎𝑙𝑙@𝐾 =
I(𝑟𝑎𝑛𝑘K𝑞
𝑝 ≤𝐾)
Í
𝑞∈Q
Í
𝑝∈𝑅𝑞 1 . (6)
where I(·)is a indicator function. The 𝑟𝑎𝑛𝑘 in Eq. 5 denotes the
position of the first relevant passage in the retrieved candidates
of query 𝑞. The 𝑅𝑞 and 𝑟𝑎𝑛𝑘K𝑞
𝑝 represent the relevant passages of
query 𝑞and the position of passage 𝑝 in the candidate list K𝑞.
Retrieval performance. We report the retrieval performance of
baselines in Table 5. Compared to the traditional sparse retrieval
method BM25, dual-encoder models significantly boost the retrieval
performance on our dataset. The improvement can be attributed
to the integration of two distinct sources of knowledge, i.e., latent
knowledge obtained through unsupervised pre-training of language
models on a massive corpus and relevance knowledge acquired
through supervised training on our large-scale annotated dataset.
Equipped with the strategy of negative mining proposed in recent
studies [28], the retrieval performance of dual-encoder models could
be further improved on T2Ranking. It is worth noting that the
Recall@𝐾 metrics observed in T2Ranking are lower than those
reported in other benchmarks with coarse-grained annotations. For
instance, the Recall@50 of BM25 is .601 and .700 on MS-MARCO-
DEV Passage and DuReaderretrieval, respectively, and 0.4918 on our
dataset. In the test set of T2Ranking, we have a greater number
of passages annotated with fine-grained relevance labels, leading
to a 4.74 average positive paragraph per query, which makes the
retrieval task more difficult and eases the false negative problem to
some extent. This highlights the challenging nature of T2Ranking
and the potential for further improvement in the future.
6.2 Re-ranking Performance
Baselines. Due to the smaller number of passages considered by
re-rankers, they tend to use the cross-encoder architecture rather
than the dual-encoder architecture. The cross-encoder approach
allows for a more detailed interaction between queries and docu-
ments, resulting in better performance, although at the expense
of lower efficiency. We report the re-ranking performance of the
cross-encoder model, which is trained on the hard negatives mined
from the entire corpus, as depicted in Figure 5. The architecture of
cross-encoder is illustrated in Figure 6(b).
Implementation details. The cross-encoder is implemented in
the same experimental environment as the dual-encoder, with a
maximum input length of 288. Negatives are sampled from the
top 256 passages retrieved by the dual-encoder, and a positive-to-
negative ratio of 1:128 is set. The cross-encoder is then trained for
5 epochs with a learning rate of 3e-5.
Metrics. To evaluate the re-ranking performance of the cross-
encoder, we use two ranking metrics: MRR@10 and nDCG@ 𝐾.
In the test set of T2Ranking, the average number of annotated pas-
sages per query is 15.7, with a maximum of 100 annotated passages.
We report nDCG@20 and nDCG@100 on the test queries. nDCG@𝐾
normalizes DCG@𝐾 by dividing DCG@𝐾 by the iDCG@𝐾, which
is the DCG@𝐾of ideal ordering of the passages. DCG@𝐾discounts
the graded relevance value of a passage according to the rank that
it appears at, which can be defined as:
𝐷𝐶𝐺@𝐾 = 1
|Q|
∑︁
𝑞∈Q
∑︁
𝑝∈K𝑞
𝐿(𝑝)
log2 (𝑟𝑎𝑛𝑘K𝑞
𝑝 +1)
, (7)
𝑛𝐷𝐶𝐺@𝐾 = 𝐷𝐶𝐺@𝐾
𝑖𝐷𝐶𝐺@𝐾, (8)
where 𝐿(𝑝)is the graded relevance of passage 𝑝.
Re-ranking performance. The re-ranking performance of the
cross-encoder is shown in Table 6. The results indicate that re-
ranking the candidates retrieved by the dual-encoder significantly
outperforms re-ranking the candidates retrieved using the BM25
method. The improved performance is attributed to the higher re-
call rate achieved by the dual-encoder method, which is consistent
with previous studies conducted on other benchmarks [15, 20]. The
re-ranking performance onT2Ranking, however, is lower compared
to other benchmarks [15, 20]. This can be explained by the pres-
ence of more fine-grained annotated relevant passages and queries
with higher diversities in T2Ranking, which makes it a more chal-
lenging benchmark but also provides a more accurate reflection of
re-ranking performance.



### Claim 9/38

#### Claim Text
In this work, we use the setting from BEIR [51], using NDCG@10 as the main metric. •Re-ranking.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 3):

Split(→) Train Dev Test Avg. Word Lengths
Task (↓) Domain (↓) Dataset (↓) TitleRelevancy#Pairs#Query#Query #Corpus Avg. D / QQuery Document
Passage-RetrievalMisc. MS MARCO [45] Binary 532,761 —- 6,980 8,841,823 1.1 5.96 55.98
Bio-Medical Bio-MedicalTREC-COVID [65] 3-level —- —- 50 171,332 493.5 10.60 160.77Information Bio-MedicalNFCorpus [7]  3-level 110,575 324 323 3,633 38.2 3.30 232.26Retrieval (IR) Bio-MedicalBioASQ [61]  Binary 32,916 —- 500 14,914,602 4.7 8.05 202.61
Question WikipediaNQ [34]  Binary 132,803 —- 3,452 2,681,468 1.2 9.16 78.88Answering WikipediaHotpotQA [76]  Binary 170,0005,447 7,405 5,233,329 2.0 17.61 46.30(QA) Finance FiQA-2018 [44]  Binary 14,166 500 648 57,638 2.6 10.77 132.32
Tweet-RetrievalTwitter Signal-1M (RT) [59] 3-level —- —- 97 2,866,316 19.6 9.30 13.93
News News TREC-NEWS [58] 5-level —- —- 57 594,977 19.6 11.14 634.79Retrieval News Robust04 [64]  3-level —- —- 249 528,155 69.9 15.27 466.40
Argument Misc. ArguAna [67]  Binary —- —- 1,406 8,674 1.0 192.98 166.80Retrieval Misc. Touché-2020 [6] 3-level —- —- 49 382,545 19.0 6.55 292.37
Duplicate-QuestionStackEx. CQADupStack [25] Binary —- —- 13,145 457,199 1.4 8.59 129.09Retrieval Quora Quora  Binary —- 5,000 10,000 522,931 1.6 9.53 11.44
Entity-RetrievalWikipediaDBPedia [21]  3-level —- 67 400 4,635,922 38.2 5.39 49.68
Citation-PredictionScientiﬁc SCIDOCS [9]  Binary —- —- 1,000 25,657 4.9 9.38 176.19
WikipediaFEVER [60]  Binary 140,0856,666 6,666 5,416,568 1.2 8.13 84.76Fact CheckingWikipediaClimate-FEVER [14] Binary —- —- 1,535 5,416,593 3.0 20.13 84.76Scientiﬁc SciFact [68]  Binary 920 —- 300 5,183 1.1 12.37 213.63
Table 1: Statistics of datasets in BEIR benchmark. Few datasets contain documents without titles. Relevancy
indicates the query-document relation: binary (relevant, non-relevant) or graded into sub-levels. Avg. D/Q
indicates the average relevant documents per query.
datasets in depth. Examples for each dataset are listed in Table 8. We additionally provide dataset
licenses in Appendix E, and links to the datasets in Table 5.
Table 1 summarizes the statistics of the datasets provided in BEIR . A majority of datasets contain
binary relevancy judgements, i.e. relevant or non-relevant, and a few contain ﬁne-grained relevancy
judgements. Some datasets contain few relevant documents for a query (< 2), while other datasets
like TREC-COVID [65] can contain up to even 500 relevant documents for a query. Only 8 out of 19
datasets (including MS MARCO) have training data denoting the practical importance for zero-shot
retrieval benchmarking. All datasets except ArguAna [67] have short queries (either a single sentence
or 2-3 keywords). Figure 1 shows an overview of the tasks and datasets in the BEIR benchmark.
Information Retrieval (IR) is ubiquitous, there are lots of datasets available within each task and
further even more tasks with retrieval. However, it is not feasible to include all datasets within the
benchmark for evaluation. We tried to cover a balanced mixture of a wide range of tasks and datasets
and paid importance not to overweight a speciﬁc task like question-answering. Future datasets can
easily be integrated in BEIR , and existing models can be evaluated on any new dataset quickly. The
BEIR website will host an actively maintained leaderboard2 with all datasets and models.
3.1 Dataset and Diversity Analysis
The datasets present in BEIR are selected from diverse domains ranging from Wikipedia, scientiﬁc
publications, Twitter, news, to online user communities, and many more. To measure the diversity in
domains, we compute the domain overlap between the pairwise datasets using a pairwise weighted
Jaccard similarity [26] score on unigram word overlap between all dataset pairs. For more details
on the theoretical formulation of the similarity score, please refer to Appendix F. Figure 2 shows a
heatmap denoting the pairwise weighted jaccard scores and the clustered force-directed placement
diagram. Nodes (or datasets) close in this graph have a high word overlap, while nodes far away in
the graph have a low overlap. From Figure 2, we observe a rather low weighted Jaccard word overlap
across different domains, indicating that BEIR is a challenging benchmark where approaches must
generalize well to diverse out-of-distribution domains.
3.2 BEIR Software and Framework
The BEIR software3 provides an is an easy to use Python framework (pip install beir) for model
evaluation. It contains extensive wrappers to replicate experiments and evaluate models from well-
known repositories including Sentence-Transformers [53], Transformers [72], Anserini [74], DPR
[31], Elasticsearch, ColBERT [32], and Universal Sentence Encoder [75]. This makes the software
useful for both academia and industry. The software also provides you with all IR-based metrics
from Precision, Recall, MAP (Mean Average Precision), MRR (Mean Reciprocal Rate) to nDCG
2 BEIR Leaderboard: https://tinyurl.com/beir-leaderboard
3 BEIR Code & documentation: https://github.com/UKPLab/beir
4



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 22):

Corpus Website (Link)
CORD-19 https://www.semanticscholar.org/cord19
NutritionFacts https://nutritionfacts.org
PubMed https://pubmed.ncbi.nlm.nih.gov
Signal-1M https://research.signal-ai.com/datasets/signal1m.html
TREC Washington Posthttps://ir.nist.gov/wapo/
TREC disks 4 and 5 https://trec.nist.gov/data/cd45/
Args.me https://zenodo.org/record/4139439/
DBPedia (2015-10) http://downloads.dbpedia.org/wiki-archive/Downloads2015-10.html
TREC-COVID (Annotated)https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid-beir.zip
Table 7: Corpus Name and Link used for datasets in BEIR .
Dataset Query Relevant-Document
MS MARCOwhat fruit is native to australia <Paragraph>Passiﬂora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned,white ﬂeshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, whileothers list the fruits as being bitter and inedible. assiﬂora herbertiana. A rare passion fruit native toAustralia...
TREC-COVIDwhat is the origin of COVID-19<Title>Origin of Novel Coronavirus (COVID-19): A Computational Biology Study using ArtiﬁcialIntelligence<Paragraph>Origin of the COVID-19 virus has been intensely debated in the community...
BioASQ What is the effect of HMGB2 loss on CTCFclustering <Title>HMGB2 Loss upon Senescence Entry Disrupts Genomic Organization and Induces CTCFClustering across Cell Types.<Paragraph>Processes like cellular senescence are characterized bycomplex events giving rise to heterogeneous cell populations. However, the early molecular eventsdriving this cascade remain elusive....
NFCorpus Titanium Dioxide & Inﬂammatory Bowel Dis-ease <Title>Titanium Dioxide Nanoparticles in Food and Personal Care Products<Paragraph>Titaniumdioxide is a common additive in many food, personal care, and other consumer products used by people,which after use can enter the sewage system, and subsequently enter the environment as treated efﬂuentdischarged to surface waters or biosolids applied to agricultural land, or incinerated wastes...
NQ when did they stop cigarette advertising on tele-vision? <Title>Tobacco advertising<Paragraph>The ﬁrst calls to restrict advertising came in 1962 from theRoyal College of Physicians, who highlighted the health problems and recommended stricter laws...
HotpotQA Stockely Webster has paintings hanging in whathome (that serves as the residence for the Mayorof New York)?
<Title>Stokely Webster<Paragraph>Stokely Webster (1912 – 2001) was best known as an Americanimpressionist painter who studied in Paris. His paintings can be found in the permanent collections ofmany museums, including the Metropolitan Museum of Art in New York, the National Museum...
FiQA-2018 What is the PEG ratio? How is the PEG ratiocalculated? How is the PEG ratio useful forstock investing?
<Paragraph>PEG is Price/Earnings to Growth. It is calculated as Price/Earnings/Annual EPS Growth.It represents how good a stock is to buy, factoring in growth of earnings, which P/E does not. Obviouslywhen PEG is lower, a stock is more undervalued, which means that it is a better buy, and more likely...
Signal-1M (RT)Genvoya, a Gentler Anti-HIV Cocktail, Okayedby EU Regulators <Paragraph>All people with #HIV should get anti-retroviral drugs: @WHO, by @kkelland via@Reuters_Health #AIDS #TasP
TREC-NEWSWebsites where children are prostituted are im-mune from prosecution. But why?<Title>Senate launches bill to remove immunity for websites hosting illegal content, spurred byBackpage.com<Paragraph>The legislation, along with a similar bill in the House, sets the stage for abattle between Congress and some of the Internet’s most powerful players, including Google and variousfree-speech advocates, who believe that Congress shouldn’t regulate Web content or try to force websitesto police themselves more rigorously...
Robust04 What were the causes for the Islamic Revolutionrelative to relations with the U.S.?<Paragraph>BFN [Editorial: "Sow the Wind and Reap the Whirlwind"] Yesterday marked the 14thanniversary of severing of diplomatic relations between the Islamic Republic and the United States ofAmerica. Several occasions arose in the last decade and a half for improving Irano-American relations...
Touché-2020Should the government allow illegal immigrantsto become citizens? <Title>America should support blanket amnesty for illegal immigrants.<Paragraph>Undocumentedworkers do not receive full Social Security beneﬁts because they are not United States citizens " norshould they be until they seek citizenship legally. Illegal immigrants are legally obligated to pay taxes...
CQADupStackCommand to display ﬁrst few and last few linesof a ﬁle <Title>Combing head and tail in a single call via pipe<Paragraph>On a regular basis, I am piping theoutput of some program to either ‘head‘ or ‘tail‘. Now, suppose that I want to see the ﬁrst AND last 10lines of piped output, such that I could do something like ./lotsofoutput | headtail...
Quora How long does it take to methamphetamine outof your blood? <Paragraph>How long does it take the body to get rid of methamphetamine?
DBPedia Paul Auster novels <Title>The New York Trilogy<Paragraph>The New York Trilogy is a series of novels by Paul Auster.Originally published sequentially as City of Glass (1985), Ghosts (1986) and The Locked Room (1986),it has since been collected into a single volume.
SCIDOCS CFD Analysis of Convective Heat Transfer Co-efﬁcient on External Surfaces of Buildings<Title>Application of CFD in building performance simulation for the outdoor environment: an overview<Paragraph>This paper provides an overview of the application of CFD in building performancesimulation for the outdoor environment, focused on four topics...
FEVER DodgeBall: A True Underdog Story is an Amer-ican movie from 2004 <Title>DodgeBall: A True Underdog Story<Paragraph>DodgeBall: A True Underdog Story is a2004 American sports comedy ﬁlm written and directed by Rawson Marshall Thurber and starring VinceVaughn and Ben Stiller. The ﬁlm follows friends who enter a dodgeball tournament...
Climate-FEVERSea level rise is now increasing faster than pre-dicted due to unexpectedly rapid ice melting.<Title>Sea level rise<Paragraph>A sea level rise is an increase in the volume of water in the world ’soceans, resulting in an increase in global mean sea level. The rise is usually attributed to global climatechange by thermal expansion of the water in the oceans and by melting of Ice sheets and glaciers...
Table 8: Examples of queries and relevant documents for all datasets included in BEIR . ( <Title>) and
(<Paragraph>) are used to distinguish the title separately from the paragraph within a document in the table
above. These tokens were not passed to the respective models.
23



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 4):

TREC-COVID
BioASQ
NFCorpus
NQ
HotpotQA
Signal-1M (RT)
FiQA-2018
ArguAna
Touché-2020
CQADupStack
Quora
DBPedia
SCIDOCS
FEVER
SciFact
TREC-NEWS
Robust04
TREC-COVID
BioASQ
NFCorpus
NQ
HotpotQA
Signal-1M (RT)
FiQA-2018
ArguAna
Touché-2020
CQADupStack
Quora
DBPedia
SCIDOCS
FEVER
SciFact
TREC-NEWS
Robust04
0.51
0.39 0.44
0.24 0.22 0.19
0.16 0.16 0.13 0.46
0.14 0.13 0.11 0.34 0.25
0.18 0.17 0.15 0.28 0.17 0.26
0.21 0.19 0.17 0.34 0.22 0.26 0.33
0.2 0.19 0.17 0.36 0.21 0.29 0.41 0.44
0.18 0.18 0.14 0.24 0.16 0.2 0.3 0.22 0.29
0.18 0.17 0.15 0.33 0.22 0.3 0.36 0.29 0.37 0.29
0.16 0.15 0.13 0.44 0.89 0.24 0.17 0.21 0.2 0.15 0.22
0.32 0.29 0.23 0.26 0.16 0.16 0.22 0.24 0.23 0.26 0.22 0.16
0.18 0.18 0.15 0.52 0.8 0.27 0.2 0.24 0.24 0.18 0.25 0.78 0.19
0.44 0.53 0.41 0.19 0.13 0.11 0.15 0.18 0.17 0.16 0.15 0.13 0.26 0.15
0.18 0.16 0.15 0.42 0.29 0.4 0.32 0.34 0.37 0.23 0.32 0.28 0.2 0.32 0.14
0.23 0.21 0.18 0.46 0.31 0.34 0.35 0.38 0.35 0.23 0.31 0.3 0.24 0.35 0.18 0.45
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Figure 2: Domain overlap across each pairwise dataset in the BEIR benchmark. Heatmap (left) shows the
pairwise weighted jaccard similarity scores between BEIR datasets. 2D representation (right) using a force-
directed placement algorithm with NetworkX [20]. We color and mark datasets differently for different domains.
(Normalised Cumulative Discount Gain) for any top-k hits. One can use the BEIR benchmark for
evaluating existing models on new retrieval datasets and for evaluating new models on the included
datasets.
Datasets are often scattered online and are provided in various ﬁle-formats, making the evaluation of
models on various datasets difﬁcult. BEIR introduces a standard format (corpus, queries and qrels)
and converts existing datasets in this easy universal data format, allowing to evaluate faster on an
increasing number of datasets.
3.3 Evaluation Metric
Depending upon the nature and requirements of real-world applications, retrieval tasks can be either
be precision or recall focused. To obtain comparable results across models and datasets in BEIR , we
argue that it is important to leverage a single evaluation metric that can be computed comparably
across all tasks. Decision support metrics such as Precision and Recall which are both rank unaware
are not suitable. Binary rank-aware metrics such as MRR (Mean Reciprocal Rate) and MAP (Mean
Average Precision) fail to evaluate tasks with graded relevance judgements. We ﬁnd thatNormalised
Cumulative Discount Gain (nDCG@k) provides a good balance suitable for both tasks involving
binary and graded relevance judgements. We refer the reader to Wang et al. [71] for understanding
the theoretical advantages of the metric. For our experiments, we utilize the Python interface of the
ofﬁcial TREC evaluation tool [63] and compute nDCG@10 for all datasets.
4 Experimental Setup
We use BEIR to compare diverse, recent, state-of-the-art retrieval architectures with a focus on
transformer-based neural approaches. We evaluate on publicly available pre-trained checkpoints,
which we provide in Table 6. Due to the length limitations of transformer-based networks, we use
only the ﬁrst 512 word pieces within all documents in our experiments across all neural architectures.
We group the models based on their architecture: (i) lexical, (ii) sparse, (iii) dense, (iv) late-interaction,
and (v) re-ranking. Besides the included models, the BEIR benchmark is model agnostic and in future
different model conﬁgurations can be easily incorporated within the benchmark.
(i) Lexical Retrieval: (a) BM25 [55] is a commonly-used bag-of-words retrieval function based on
token-matching between two high-dimensional sparse vectors with TF-IDF token weights. We use
Anserini [36] with the default Lucene parameters (k=0.9 and b=0.4). We index the title (if available)
and passage as separate ﬁelds for documents. In our leaderboard, we also tested Elasticsearch BM25
and Anserini + RM3 expansion, but found Anserini BM25 to perform the best.
5



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 16):

A Complementing Information
We provide the following additional sections in detail and information that complement discussions
in the main paper:
• Limitations of the BEIR benchmark in Appendix B.
• Training and in-domain evaluation task details in Appendix C.
• Description of all zero-shot tasks and datasets used in BEIR in Appendix D.
• Details of dataset licenses in Appendix E.
• Overview of the weighted jaccard similarity metric in Appendix F.
• Overview of the capped recall at k metric in Appendix G.
• Length preference for dense retrieval system in Appendix H.
B Limitations of the BEIR Benchmark
Even though we cover a wide range of tasks and domains in BEIR , no benchmark is perfect and has
its limitations. Making those explicit is a critical point in understanding the results on the benchmark
and, for future work, to improve up-on the benchmark.
1. Multilingual Tasks: Although we aim for a diverse retrieval evaluation benchmark, due to the
limited availability of multilingual retrieval datasets, all datasets covered in the BEIR benchmark
are currently English. It is worthwhile to add more multilingual datasets [ 2, 77] (in consideration
of the selection criteria) as a next step for the benchmark. Future work could include multi- and
cross-lingual tasks and models.
2. Long Document Retrieval: Most of our tasks have average document lengths up-to a few hundred
words roughly equivalent to a few paragraphs. Including tasks that require the retrieval of longer
documents would be highly relevant. However, as transformer-based approaches often have a length
limit of 512 word pieces, a fundamental different setup would be required to compare approaches.
3. Multi-factor Search: Until now, we focused on pure textual search in BEIR . In many real-world
applications, further signals are used to estimate the relevancy of documents, such as PageRank
[49], recency [16], authority score [33] or user-interactions such as click-through rates [ 51]. The
integration of such signals in the tested approaches is often not straight-forward and is an interesting
direction for research.
4. Multi-ﬁeld Retrieval: Retrieval can often be performed over multiple ﬁelds. For example, for
scientiﬁc publication we have the title, the abstract, the document body, the authors list, and the
journal name. So far we focused only on datasets that have one or two ﬁelds.
5. Task-speciﬁc Models: In our benchmark, we focus on evaluating models that are able to generalize
well for a broad range of retrieval tasks. Naturally in real-world, for some few tasks or domains,
specialized models are available which can easily outperform generic models as they focus and
perform well on a single task, lets say on question-answering. Such task-speciﬁc models do not
necessarily need to generalize across all diverse tasks.
C Training and In-domain Evaluation
We use the MS MARCO Passage Ranking dataset [45], which contains 8.8M Passages and an ofﬁcial
training set of 532,761 query-passage pairs for ﬁne-tuning for a majority of retrievers. The dataset
contains queries from Bing search logs with one text passage from various web sources annotated as
relevant. We ﬁnd the dataset useful for training, in terms of covering a wide variety of topics and
providing the highest number of training pairs. It has been extensively explored and used for ﬁne-
tuning dense retrievers in recent works [46, 17, 15]. We use the ofﬁcial MS MARCO development
set for our in-domain evaluation which has been widely used in prior research [46, 17, 15]. It has
6,980 queries. Most of the queries have only 1 document judged relevant; the labels are binary.
D Zero-shot Evaluation Tasks
Following the selection criteria mentioned in Section 3, we include 18 evaluation datasets that span
across 9 heterogeneous tasks. Each dataset mentioned below contains a document corpus denoted
17



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 1):

Fact Checking
Citation-Prediction
W iki 
FEVER
QUERY
DOCS
Natural Claim
Wikipedia Articles
W iki 
Climate-FEVER
QUERY
DOCS
Climate-based Claim
Wikipedia Articles
SciFact
QUERY
DOCS
Scientific claim
PubMed ArticlesScientific 
SCIDOCS
QUERY
DOCS
Article Title
PubMed ArticlesScientific 
Dup. Question Retrieval
Quora 
Quora
QUERY
DOCS
StackEx. 
CQADupStack
QUERY
DOCS
Argument Retrieval
Misc. 
QUERY
DOCS
Misc. 
ArguAna
QUERY
DOCS
Tóuche-2020
Query Title
Query Title + Body
Query Title
Quora Questions
Argument
Idebate Arguments
Args.me Arguments
News Retrieval
TREC-NEWS
QUERY
DOCS News ArticlesNews Tweet Retrieval
Signal-1M
QUERY
DOCS
News Headline
Twitter TweetsT witter 
Question-Answering
W iki 
NQ
QUERY
DOCS
W iki 
HotpotQA
QUERY
DOCS
FiQA-2018
QUERY
DOCSFinance 
Bio-Medical IR
QUERY
DOCS
Scientific 
BioASQ
QUERY
DOCS
NFCorpus
QUERY
DOCSScientific 
Entity Retrieval
DBPedia
QUERY
DOCS
Entity-based Query
DBPedia ArticlesW iki 
TREC-COVID
Scientific 
Wikipedia Articles
Wikipedia Articles
Natural Query
Multi-Hop Query
CORD-19 Articles
PubMed Articles
PubMed Articles
COVID-19 Query
Nutrition Facts
Bio-Medical Query
Financial Query
Investment Articles
Controversial Query
9 Tasks
18 Datasets
News Headline
Robust04
QUERY
DOCSNews 
 News Articles
News Query
Figure 1: An overview of the diverse tasks and datasets in BEIR benchmark.
So far, it is unclear how well existing trained neural models will perform for other text domains or
textual retrieval tasks. Even more important, it is unclear how well different approaches, like sparse
embeddings vs. dense embeddings, generalize to out-of-distribution data.
In this work, we present a novel robust and heterogeneous benchmark called BEIR (Benchmarking
IR), comprising of 18 retrieval datasets for comparison and evaluation of model generalization. Prior
retrieval benchmarks [19, 50] have issues of a comparatively narrow evaluation focusing either only
on a single task, like question-answering, or on a certain domain. In BEIR , we focus on Diversity, we
include nine different retrieval tasks: Fact checking, citation prediction, duplicate question retrieval,
argument retrieval, news retrieval, question answering, tweet retrieval, bio-medical IR, and entity
retrieval. Further, we include datasets from diverse text domains, datasets that cover broad topics (like
Wikipedia) and specialized topics (like COVID-19 publications), different text types (news articles vs.
Tweets), datasets of various sizes (3.6k - 15M documents), and datasets with different query lengths
(average query length between 3 and 192 words) and document lengths (average document length
between 11 and 635 words).
We use BEIR to evaluate ten diverse retrieval methods from ﬁve broad architectures: lexical, sparse,
dense, late interaction, and re-ranking. From our analysis, we ﬁnd that no single approach consistently
outperforms other approaches on all datasets. Further, we notice that the in-domain performance of a
model does not correlate well with its generalization capabilities: models ﬁne-tuned with identical
training data might generalize differently. In terms of efﬁciency, we ﬁnd a trade-off between the
performances and the computational cost: computationally expensive models, like re-ranking models
and late interaction model perform the best. More efﬁcient approaches e.g. based on dense or sparse
embeddings can substantially underperform traditional lexical models like BM25. Overall, BM25
remains a strong baseline for zero-shot text retrieval.
Finally, we notice that there can be a strong lexical bias present in datasets included within the
benchmark, likely as lexical models are pre-dominantly used during the annotation or creation of
datasets. This can give an unfair disadvantage to non-lexical approaches. We analyze this for the
TREC-COVID [65] dataset: We manually annotate the missing relevance judgements for the tested
systems and see a signiﬁcant performance improvement for non-lexical approaches. Hence, future
work requires better unbiased datasets that allow a fair comparison for all types of retrieval systems.
With BEIR , we take an important step towards a single and uniﬁed benchmark to evaluate the zero-shot
capabilities of retrieval systems. It allows to study when and why certain approaches perform well,
and hopefully steers innovation to more robust retrieval systems. We release BEIR and an integration
of diverse retrieval systems and datasets in a well-documented, easy to use and extensible open-source
package. BEIR is model-agnostic, welcomes methods of all kinds, and also allows easy integration of
new tasks and datasets. More details are available at https://github.com/UKPLab/beir.
2 Related Work and Background
To our knowledge, BEIR is the ﬁrst broad, zero-shot information retrieval benchmark. Existing works
[19, 50] do not evaluate retrieval in a zero-shot setting in depth, they either focus over a single task,
small corpora or on a certain domain. This setting hinders for investigation of model generalization
across diverse set of domains and task types. MultiReQA [19] consists of eight Question-Answering
(QA) datasets and evaluates sentence-level answer retrieval given a question. It only tests a single
task and ﬁve out of eight datasets are from Wikipedia. Further, MultiReQA evaluates retrieval over
rather small corpora: six out of eight tasks have less than 100k candidate sentences, which beneﬁts
dense retrieval over lexical as previously shown [54]. KILT [50] consists of ﬁve knowledge-intensive
2



### Claim 10/38

#### Claim Text
The classification task re-uses the logistic regression classifier from MTEB [35], where the average precision is used as the main metric. •Pair-classification.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 2):

MTEB
8 Tasks
58 Datasets
Massive Text  
Embedding Benchmark
Classification
AmazonCounterfactual
Retrieval
Pair Classification
AmazonPolarity
AmazonReviews Banking77 Emotion
Imdb MassiveIntent MassiveScenario
MTOPDomain MTOPIntent
ToxicConversations TweetSentimentExtraction
SprintDuplicateQuestionsTwitterSemEval2015
TwitterURLCorpus
Clustering
ArxivP2P ArxivS2S
STS
BIOSESS SICK-R
STS11 STS12 STS13
STS14
Reranking
Summarization
STS15 STS16
AskUbuntuDupQuestionsMindSmallReranking
SciDocsRR StackOverFlowDupQuestionsSummEval 
STSBSTS17 STS22ArguAna ClimateFEVER
CQADupstackRetrieval FEVER
DBPedia
FiQA2018
HotpotQAMSMARCONFCorpus NQ Quora
SCIDOCS SciFact Touche2020TRECCOVID
MedrxivP2PMedrxivS2S Reddit
StackExchange
RedditP2P
StackExchangeP2P
TwentyNewsgroup
BiorxivP2PBiorxivS2S
Bitext Mining
BUCC Tatoeba
Figure 1: An overview of tasks and datasets in MTEB. Multilingual datasets are marked with a purple shade.
ducibility: Through versioning at a dataset and
software level, we aim to make it easy to repro-
duce results in MTEB. JSON ﬁles corresponding
to all results available in this paper have been made
available together with the MTEB benchmark3.
3.2 Tasks and Evaluation
Figure 1 provides an overview of tasks and datasets
available in MTEB. Dataset statistics are available
in Table 2. The benchmark consists of the follow-
ing 8 task types:
Bitext Mining Inputs are two sets of sentences
from two different languages. For each sentence
in the ﬁrst set, the best match in the second set
needs to be found. The matches are commonly
translations. The provided model is used to embed
each sentence and the closest pairs are found via
cosine similarity. F1 serves as the main metric for
bitext mining. Accuracy, precision and recall are
also computed.
Classiﬁcation A train and test set are embedded
with the provided model. The train set embeddings
are used to train a logistic regression classiﬁer with
100 maximum iterations, which is scored on the
test set. The main metric is accuracy with average
precision and f1 additionally provided.
3https://huggingface.co/datasets/mteb
/results
Clustering Given a set of sentences or para-
graphs, the goal is to group them into meaning-
ful clusters. A mini-batch k-means model with
batch size 32 and k equal to the number of dif-
ferent labels (Pedregosa et al., 2011) is trained on
the embedded texts. The model is scored using
v-measure (Rosenberg and Hirschberg, 2007). V-
measure does not depend on the cluster label, thus
the permutation of labels does not affect the score.
Pair Classiﬁcation A pair of text inputs is pro-
vided and a label needs to be assigned. Labels
are typically binary variables denoting duplicate
or paraphrase pairs. The two texts are embedded
and their distance is computed with various metrics
(cosine similarity, dot product, euclidean distance,
manhattan distance). Using the best binary thresh-
old accuracy, average precision, f1, precision and
recall are computed. The average precision score
based on cosine similarity is the main metric.
Reranking Inputs are a query and a list of rele-
vant and irrelevant reference texts. The aim is to
rank the results according to their relevance to the
query. The model is used to embed the references
which are then compared to the query using cosine
similarity. The resulting ranking is scored for each
query and averaged across all queries. Metrics are
mean MRR@k and MAP with the latter being the
main metric.



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 13):

A Datasets
Table 2 provides a summary along with statistics of
all MTEB tasks. In the following, we give a brief
description of each dataset included in MTEB.
A.1 Clustering
ArxivClusteringS2S, ArxivClusteringP2P,
BiorxivClusteringS2S, BiorxivClusteringP2P,
MedrxivClusteringP2P, MedrxivCluster-
ingS2S These datasets are custom-made for
MTEB using the public APIs from arXiv 9 and
bioRxiv/medRxiv10. For S2S datasets, the input
text is simply the title of the paper, while for
P2P the input text is the concatenation of the
title and the abstract. The cluster labels are
generated using categories given to the papers by
humans. For bioRxiv and medRxiv this category
is unique, but for arXiv multiple categories can
be given to a single paper so we only use the
ﬁrst one. For bioRxiv and medRxiv there is
only one level of category (e.g. biochemistry,
genetics, microbiology, etc.) hence we only
perform clustering based on that label. For arXiv
there is a main category and secondary category:
for example "cs.AI" means the main category is
Computer Science and the sub-category is AI,
math.AG means the main category is Mathematics
and the sub-category is Algrebraic Geometry etc.
Hence, we create three types of splits:
(a) Main category clustering Articles are only
clustered based on the main category (Math,
Physics, Computer Science etc.). This split evalu-
ates coarse clustering capacity of a model.
(b) Secondary category clustering within the
same main category Articles are clustered
based on their secondary category, but within a
given main category, for example only Math papers
that need to be clustered into Algebraic Geometry,
Functional Analysis, Numerical Analysis etc. This
split evaluates ﬁne-grained clustering capacity of a
model, as differentiating some sub-categories can
be very difﬁcult.
(c) Secondary category clustering Articles are
clustered based on their secondary category for all
main categories, so the labels can be Number The-
ory, Computational Complexity, Astrophysics of
Galaxies etc. These splits evaluate ﬁne-grained
9https://arxiv.org/help/api/
10https://api.biorxiv.org/
clustering capacity, as well as multi-scale capac-
ities i.e. is a model able to both separate Maths
from Physics as well as Probability from Algebraic
Topology at the same time.
For every dataset, split and strategy, we select
subsets of all labels and then sample articles from
those labels. This yields splits with a varying
amount and size of clusters.
RedditClustering (Geigle et al., 2021): Cluster-
ing of titles from 199 subreddits. Clustering of 25
splits, each with 10-50 classes, and each class with
100 - 1000 sentences
RedditClusteringP2P Dataset created for
MTEB using available data from Reddit posts 11.
The task consists of clustering the concatenation of
title+post according to their subreddit. It contains
10 splits, with 10 and 100 clusters per split and
1,000 to 100,000 posts.
StackExchangeClustering (Geigle et al., 2021)
Clustering of titles from 121 stackexchanges. Clus-
tering of 25 splits, each with 10-50 classes, and
each class with 100-1000 sentences.
StackExchangeClusteringP2P Dataset created
for MTEB using available data from StackEx-
change posts 12. The task consists of clustering
the concatenation of title and post according to
their subreddit. It contains 10 splits, with 10 to 100
clusters and 5,000 to 10,000 posts per split.
TwentyNewsgroupsClustering13 Clustering of
the 20 Newsgroups dataset, given titles of article
the goal is to ﬁnd the newsgroup (20 in total). Con-
tains 10 splits, each with 20 classes, with each split
containing between 1,000 and 10,000 titles.
A.2 Classiﬁcation
AmazonCounterfactual (O’Neill et al., 2021) A
collection of Amazon customer reviews annotated
for counterfactual detection pair classiﬁcation. For
each review the label is either "counterfactual" or
"not-counterfactual". This is a multilingual dataset
with 4 available languages.
11https://huggingface.co/datasets/sent
ence-transformers/reddit-title-body
12https://huggingface.co/datasets/flax
-sentence-embeddings/stackexchange_title
_body_jsonl
13https://scikit-learn.org/0.19/datase
ts/twenty_newsgroups.html



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 16):

STSBenchmark are monolingual english bench-
marks. STS17 and STS22 contain crosslingual
pairs of sentences, where the goal is to assess the
similarity of two sentences in different languages.
STS17 has 11 language pairs (among Korean, Ara-
bic, English, French, German, Turkish, Spanish,
Italian and Dutch) and STS22 has 18 language pairs
(among Arabic, English, French, German, Turkish,
Spanish, Polish, Italian, Russian and Chinese).
BIOSSES21 Contains 100 sentence pairs from
the biomedical ﬁeld.
SICK-R (Agirre et al., 2014) Sentences Involv-
ing Compositional Knowledge (SICK) contains a
large number of sentence pairs (10 0000) that are
lexically, syntactically and semantically rich.
A.7 Summarization
SummEval (Fabbri et al., 2020) Summaries gen-
erated by recent summarization models trained on
CNN or DailyMail alongside human annotations.
A.8 Retrieval
We refer to the BEIR paper (Thakur et al., 2021),
which contains description of each dataset. For
MTEB, we include all publicly available datasets:
ArguAna, ClimateFEVER, CQADupstack, DB-
Pedia, FEVER, FiQA2018, HotpotQA, MS-
MARCO, NFCorpus, NQ, Quora, SCIDOCS,
SciFact, Touche2020, TRECCOVID.
B Limitations of MTEB
While MTEB aims to be a diverse benchmark to
provide holistic performance reviews, the bench-
mark has its limitations. We list them here:
1. Long document datasets MTEB covers mul-
tiple text lengths (S2S, P2P, S2P), but very long
documents are still missing. The longest datasets in
MTEB have a few hundred words, and longer text
sizes could be relevant for use cases like retrieval.
2. Task imbalance Tasks in MTEB have a differ-
ent amount of datasets with summarization consist-
ing of only a single dataset. This means MTEB av-
erage scores, which are computed over all datasets,
are biased towards tasks with many datasets, no-
tably retrieval, classiﬁcation and clustering. As
MTEB grows, we hope to add more datasets to cur-
rently underrepresented tasks like summarization
or pair classiﬁcation.
21https://tabilab.cmpe.boun.edu.tr/BIO
SSES/DataSet.html
3. Multinguality MTEB contains multilingual
classiﬁcation, STS and bitext mining datasets.
However, retrieval and clustering are English-only.
SGPT-BLOOM-7B1-msmarco is geared towards
multilingual retrieval datasets and due to the lack
thereof cannot be comprehensively benchmarked
in MTEB. Further, MTEB does not contain any
code datasets that could be used to benchmark code
models (Neelakantan et al., 2022; Allal et al., 2023).
It should be easy to extend MTEB with datasets,
such as CodeSearchNet (Husain et al., 2019), TyDI
QA (Clark et al., 2020), XOR QA (Asai et al., 2020)
or MIRACL (Zhang et al., 2022).
4. Additional modalities Text embeddings are
commonly used as input features for downstream
models, such as in our classiﬁcation task. This
can involve other modalities, notably image con-
tent (Carvalho et al., 2018; Tan and Bansal, 2019;
Muennighoff, 2020; Nichol et al., 2021; Saharia
et al., 2022; Weinbach et al., 2022). We have fo-
cused solely on natural language applications and
leave extensive benchmarking of text embeddings
as inputs for other modalities to future work.
C Examples
Tables 3-9 provide examples for each dataset for
each task. For retrieval datasets, we refer to the
BEIR paper (Thakur et al., 2021).
D Correlations
Figure 6 provides correlation heatmaps for model
performance and MTEB tasks.
E Models
Table 10 provides publicly available model check-
points used for MTEB evaluation.
F Additional results
Tables 11 until the end provide results on individ-
ual datasets of MTEB. The results are additionally
available in json format on the Hugging Face Hub22
and can be inspected on the leaderboard23.
22https://huggingface.co/datasets/mteb
/results
23https://huggingface.co/spaces/mteb/l
eaderboard



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 4):

Zero-shot Text Classification The input and label texts are converted to sentences based on manually
written prompt templates. The predicted label is the one closest to the input text in the embedding
space. Take the sentiment classification of movie reviews as an example, with the original input “I
enjoy watching it”, the label text is “it is an example of terrible/great movie review” and the input text
becomes “movie review: I enjoy watching it”.
Semantic Textual Similarity Given two text embeddings, we use the cosine function to measure
their semantic similarity. Since the absolute similarity scores do not enable an easy interpretation, the
evaluation is usually based on rank correlation coefficients.
Text Clustering Standard clustering algorithms such as k-means can be applied straightforwardly.
Texts belonging to the same category are expected to be close in the embedding space.
For tasks other than zero-shot text classification and retrieval, we use the query embeddings by
default.
5 Experiments
5.1 Pre-training and Fine-tuning Configurations
Pre-training We pre-train on our proposed text pair dataset for three model sizes: E5small, E5base
and E5large initialized from MiniLM [ 59], bert-base-uncased, and bert-large-uncased-whole-word-
masking respectively. The batch size is set to a large value of 32, 768 to increase the number of
negatives. The learning rate is {3, 2, 1}×10−4 for the {small, base, large} models, with linear decay
and the first 1, 000 steps for warmup. We pre-train for 20k steps in total with AdamW optimizer,
which is approximately 2.5 epochs over the dataset. It takes {16, 32, 64} V100 GPUs and {1, 1, 2}
days for the {small, base, large} models. To improve training efficiency and reduce GPU memory
usage, we adopt mixed precision training and gradient checkpointing.
Fine-tuning is performed on the concatenation of 3 datasets: MS-MARCO passage ranking [ 8],
NQ [ 32, 30], and NLI [ 22] datasets. We reuse the mined hard negatives and re-ranker scores from
SimLM [ 58] for the first two datasets. Models are fine-tuned for 3 epochs with batch size 256 on 8
GPUs. Learning rate is {3, 2, 1}×10−5 for the {small, base, large} models with 400 steps warmup.
For each example, we use 7 hard negatives. Since the NLI dataset only has 1 hard negative for each
example, 6 sentences are randomly sampled from the entire corpus.
We use E5-PT to denote models with contrastive pre-training only. More implementation details can
be found in Appendix B.
5.2 Evaluation Datasets
BEIR Benchmark [ 53] is a collection of 19 information retrieval datasets, ranging across ad-hoc
web search, question answering, fact verification and duplicate question retrieval, etc. We evaluate
the 15 datasets that provide public downloads. The main metric is nDCG@10.
MTEB Benchmark [ 40] is recently proposed for benchmarking massive text embedding tasks.
Though MTEB is multilingual due to the inclusion of bitext mining datasets, most datasets are
still only available in English. In this paper, we evaluate the English subsets, which have 56
datasets spanning across 6 categories: Classification (Class.), Clustering (Clust.), Pair Classification
(PairClass.), Rerank, Retrieval (Retr.), STS, and Summarization (Summ.). The evaluation metrics are
accuracy, v-measure, average precision, MAP, nDCG@10, and Spearman coefficients, respectively.
Please refer to the MTEB paper for details.
5.3 Results on BEIR benchmark
Results with Unsupervised Methods In Table 1, we show model results that do not use any labeled
data. When averaged over all 15 datasets, E5-PTbase outperforms the classic BM25 algorithm by 1.2
points. To the best of our knowledge, this is the first reported result that an unsupervised model can
beat BM25 on the BEIR benchmark. When scaling up to E5-PT large, we see further benefits from
42.9 to 44.2.
5



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 3):

AmazonCounterfactualClassification
AmazonPolarityClassification
AmazonReviewsClassification
Banking77Classification
EmotionClassification
ImdbClassification
MassiveIntentClassification
MassiveScenarioClassification
MTOPDomainClassification
MTOPIntentClassification
T oxicConversationsClassification
T weetSentimentExtractionClassification
ArxivClusteringP2P
ArxivClusteringS2S
BiorxivClusteringP2P
BiorxivClusteringS2S
MedrxivClusteringP2P
MedrxivClusteringS2S
RedditClustering
RedditClusteringP2P
StackExchangeClustering
StackExchangeClusteringP2P
T wentyNewsgroupsClustering
SprintDuplicateQuestions
T witterSemEval2015
T witterURLCorpus
AskUbuntuDupQuestions
MindSmallReranking
SciDocsRR
StackOverflowDupQuestions
ArguAna
ClimateFEVER
CQADupstackAndroidRetrieval
CQADupstackEnglishRetrieval
CQADupstackGamingRetrieval
CQADupstackGisRetrieval
CQADupstackMathematicaRetrieval
CQADupstackPhysicsRetrieval
CQADupstackProgrammersRetrieval
CQADupstackStatsRetrieval
CQADupstackT exRetrieval
CQADupstackUnixRetrieval
CQADupstackWebmastersRetrieval
CQADupstackWordpressRetrieval
DBPedia
FEVER
FiQA2018
HotpotQA
MSMARCO
NFCorpus
NQ
QuoraRetrieval
SCIDOCS
SciFact
T ouche2020
TRECCOVID
BIOSSES
SICK-R
STS12
STS13
STS14
STS15
STS16
STS17
STS22
STSBenchmark
SummEval
AmazonCounterfactualClassification
AmazonPolarityClassification
AmazonReviewsClassification
Banking77Classification
EmotionClassification
ImdbClassification
MassiveIntentClassification
MassiveScenarioClassification
MTOPDomainClassification
MTOPIntentClassification
T oxicConversationsClassification
T weetSentimentExtractionClassification
ArxivClusteringP2P
ArxivClusteringS2S
BiorxivClusteringP2P
BiorxivClusteringS2S
MedrxivClusteringP2P
MedrxivClusteringS2S
RedditClustering
RedditClusteringP2P
StackExchangeClustering
StackExchangeClusteringP2P
T wentyNewsgroupsClustering
SprintDuplicateQuestions
T witterSemEval2015
T witterURLCorpus
AskUbuntuDupQuestions
MindSmallReranking
SciDocsRR
StackOverflowDupQuestions
ArguAna
ClimateFEVER
CQADupstackAndroidRetrieval
CQADupstackEnglishRetrieval
CQADupstackGamingRetrieval
CQADupstackGisRetrieval
CQADupstackMathematicaRetrieval
CQADupstackPhysicsRetrieval
CQADupstackProgrammersRetrieval
CQADupstackStatsRetrieval
CQADupstackT exRetrieval
CQADupstackUnixRetrieval
CQADupstackWebmastersRetrieval
CQADupstackWordpressRetrieval
DBPedia
FEVER
FiQA2018
HotpotQA
MSMARCO
NFCorpus
NQ
QuoraRetrieval
SCIDOCS
SciFact
T ouche2020
TRECCOVID
BIOSSES
SICK-R
STS12
STS13
STS14
STS15
STS16
STS17
STS22
STSBenchmark
SummEval
97
85 84
90 89 83
90 89 84 87
91 94 81 85 85
92 92 89 91 89 88
92 92 89 91 89 88 100
91 92 87 92 88 88 98 98
91 92 87 92 88 88 98 98 100
93 93 87 90 89 90 96 96 95 95
94 94 89 91 92 90 97 97 96 96 98
91 91 83 87 83 86 90 90 89 89 89 89
92 93 87 90 87 89 97 97 95 95 96 96 93
88 88 81 85 82 85 87 87 87 87 87 87 95 90
91 91 85 87 85 88 93 93 92 92 92 92 95 96 94
87 87 81 84 81 84 87 87 87 87 87 87 92 89 96 93
89 89 83 85 83 85 90 90 89 89 89 90 93 93 93 97 96
94 94 88 92 90 89 95 95 95 95 95 96 90 95 88 93 88 91
94 95 86 93 92 91 95 95 95 95 96 97 92 95 90 93 89 91 96
92 92 89 91 88 88 95 95 94 94 94 94 92 96 89 94 89 92 95 95
87 87 79 86 82 83 90 90 89 89 89 88 89 91 86 90 85 88 89 91 92
93 93 88 91 87 88 96 96 95 95 95 96 92 98 89 95 90 93 95 95 96 91
74 74 69 78 72 69 77 77 79 79 74 75 73 75 71 75 71 74 77 76 77 74 77
88 89 83 85 85 85 91 91 90 90 92 92 85 91 83 88 83 85 91 92 89 84 90 71
92 92 84 88 87 89 92 92 92 92 93 93 89 92 87 91 87 89 93 93 92 86 93 74 88
88 87 85 89 84 84 92 92 91 91 89 91 89 91 86 90 85 88 90 90 92 88 92 77 85 87
84 86 80 81 80 84 89 89 88 88 88 88 85 88 82 88 82 86 88 88 87 82 89 67 84 88 83
91 92 86 89 85 88 95 95 93 93 93 93 94 97 91 97 91 95 94 94 95 92 96 76 89 92 91 89
88 88 84 87 83 83 92 92 91 91 89 90 90 92 86 92 86 90 90 90 93 92 92 75 85 87 92 84 93
92 91 84 87 85 89 90 90 90 90 91 90 92 91 91 91 91 90 91 93 92 87 92 72 86 91 87 86 92 88
87 88 83 86 83 84 91 91 90 90 90 91 88 91 85 90 86 88 91 90 90 85 92 72 85 88 86 84 90 86 87
88 87 80 89 82 84 90 90 90 90 88 88 87 89 85 88 85 86 89 90 91 92 90 79 84 86 90 81 90 90 87 85
91 91 83 89 86 87 92 92 92 92 92 92 90 93 88 91 88 89 93 93 96 91 93 74 86 90 87 83 92 89 91 88 91
91 90 82 90 85 87 93 93 92 92 91 91 89 93 87 91 87 89 93 94 94 95 93 75 87 89 90 84 92 91 90 88 94 94
86 85 79 86 80 81 87 87 87 87 86 86 88 89 85 89 86 88 87 88 91 93 89 74 81 84 88 80 90 91 86 84 91 90 92
88 87 80 87 82 83 89 89 89 89 87 87 91 91 88 91 87 89 89 90 93 94 90 77 82 86 89 81 92 92 88 85 92 92 93 94
88 88 80 87 82 83 89 89 88 88 89 88 93 92 88 92 87 89 90 91 93 92 91 73 83 87 87 82 92 88 90 86 91 94 93 90 93
88 88 81 87 82 85 90 90 89 89 88 88 90 91 88 91 87 89 90 91 94 95 92 75 83 87 88 81 93 92 91 85 92 95 94 93 94 94
87 87 80 86 81 82 88 88 88 88 87 87 92 91 89 92 90 91 89 90 92 93 91 74 83 86 87 81 93 90 89 85 90 93 92 93 96 94 95
87 87 80 86 80 82 88 88 88 88 86 87 90 90 87 90 86 88 89 89 93 92 90 75 82 85 88 81 90 91 87 84 90 92 91 92 96 91 93 93
88 87 81 88 82 84 90 90 89 89 88 88 89 90 87 90 86 88 90 90 93 93 91 76 83 86 93 80 91 91 88 85 94 93 94 93 95 93 95 94 94
88 88 80 88 82 84 90 90 89 89 89 88 89 91 87 90 87 89 90 91 93 93 91 74 83 87 89 83 92 91 89 85 93 93 94 93 93 92 96 93 93 94
87 87 80 87 82 83 89 89 88 88 88 88 89 89 86 90 86 88 89 90 92 92 90 75 83 87 89 81 91 92 88 84 92 92 93 92 93 91 93 92 93 94 96
90 90 86 87 84 86 93 93 92 92 92 93 90 95 89 93 88 91 93 92 93 87 94 73 88 89 88 85 93 89 89 91 86 91 89 86 87 88 87 88 87 87 88 87
87 88 83 86 83 84 91 91 90 90 90 91 88 91 85 90 86 88 91 90 90 85 92 72 85 88 86 84 90 86 87 100 85 88 88 84 85 86 85 85 84 85 85 84 91
92 92 87 90 87 88 95 95 93 93 95 94 91 95 88 92 88 90 94 94 95 90 95 75 89 92 91 87 94 91 92 89 89 92 92 88 90 90 91 90 90 90 91 90 91 89
90 90 86 89 85 88 95 95 94 94 94 94 90 96 88 93 88 92 94 93 94 88 95 74 90 91 90 88 94 90 89 93 88 91 90 88 88 88 88 88 87 88 89 87 96 93 93
90 91 87 91 86 87 94 94 95 95 93 94 89 94 87 91 88 90 94 93 93 87 94 77 88 90 89 85 92 89 89 91 89 91 90 87 88 88 88 88 87 88 89 87 93 91 93 93
89 89 83 85 84 87 89 89 88 88 90 89 92 91 94 94 95 94 90 91 92 87 92 72 85 89 86 84 92 87 93 87 86 90 89 85 88 89 89 90 86 87 88 87 90 87 90 89 89
91 92 86 89 86 88 95 95 94 94 94 95 91 96 88 94 88 92 95 94 95 89 97 74 90 92 90 89 95 91 91 93 88 92 91 87 89 90 89 88 88 89 90 89 95 93 93 97 92 90
92 92 88 92 88 89 97 97 96 96 98 97 91 97 88 93 88 91 96 96 97 91 96 76 90 93 91 88 94 91 91 91 91 94 94 89 90 92 92 91 90 91 92 91 93 91 95 95 95 91 95
89 90 85 86 82 85 91 91 89 89 89 89 95 93 92 97 92 96 90 91 93 90 93 74 85 89 88 86 97 91 91 88 87 90 89 90 92 90 92 93 90 90 90 89 91 88 91 91 90 92 92 91
89 90 83 85 83 86 89 89 89 89 89 89 93 92 95 96 95 95 90 91 92 88 92 73 85 89 86 83 94 88 92 87 87 90 90 87 90 91 90 92 88 89 89 88 91 87 90 89 89 97 90 91 95
92 93 85 88 88 90 93 93 92 92 94 93 91 93 89 92 89 90 94 95 93 88 93 74 89 93 88 87 92 89 96 88 88 92 91 86 87 89 90 88 88 88 89 88 90 88 93 91 91 91 93 94 90 90
88 89 84 84 83 86 90 90 89 89 89 90 93 92 94 96 96 97 90 91 91 86 92 72 85 89 88 86 94 88 91 88 86 89 88 86 88 88 88 90 87 88 88 87 91 88 90 91 89 96 91 90 95 96 90
86 86 80 82 80 83 85 85 85 85 85 85 91 88 93 93 92 92 86 87 88 84 88 71 82 85 83 80 90 85 90 83 84 87 86 83 86 87 86 88 85 85 85 84 87 83 86 85 85 93 86 86 91 97 87 92
83 84 81 82 81 84 89 89 88 88 89 89 81 88 78 84 78 82 88 88 86 81 88 68 89 85 83 83 86 83 82 84 81 82 83 77 79 81 80 79 78 80 80 79 84 84 85 88 86 82 88 89 81 80 86 81 76
92 92 89 90 88 88 97 97 95 95 95 96 90 96 87 93 87 91 95 94 95 89 96 76 91 93 91 87 94 91 91 91 89 92 92 87 88 89 89 88 88 89 89 88 93 91 94 95 94 90 95 96 90 90 93 90 86 90
92 91 89 90 88 88 96 96 94 94 94 95 91 95 89 93 89 91 95 94 95 90 95 75 90 92 91 87 94 91 92 90 89 94 92 88 89 91 91 89 89 90 90 89 92 90 93 93 92 91 94 95 91 91 93 91 87 86 97
94 94 90 92 91 90 97 97 96 96 97 98 92 96 90 94 90 92 96 97 96 90 97 76 91 94 92 89 95 92 94 92 90 94 94 89 90 91 91 90 89 91 91 90 93 92 95 95 94 92 95 97 92 92 95 92 88 89 98 98
93 92 89 90 89 89 96 96 95 95 95 96 91 95 89 93 88 90 95 95 95 89 95 74 90 92 91 88 94 91 92 90 89 92 92 87 88 90 89 88 87 89 89 88 92 90 94 94 93 90 94 96 90 90 94 90 87 89 96 96 98
93 93 89 91 91 89 95 95 94 94 95 95 90 93 88 91 88 89 95 95 95 90 94 75 88 93 91 85 92 90 92 88 90 93 92 87 89 90 91 89 89 90 91 90 91 88 95 92 93 91 92 96 90 90 93 89 86 85 95 94 97 95
91 90 86 88 87 88 95 95 95 95 95 96 87 94 85 91 85 88 94 94 92 87 94 73 92 91 89 88 92 89 89 89 87 89 90 85 86 87 87 85 84 86 87 86 90 89 91 94 92 88 93 95 88 87 92 88 83 93 95 93 95 95 92
89 89 91 88 85 85 94 94 92 92 93 93 89 93 87 91 87 90 92 93 94 87 93 75 88 90 90 86 92 90 89 89 87 89 89 86 87 87 87 87 87 87 88 87 91 89 97 92 92 88 93 94 90 88 90 89 85 85 93 92 94 92 92 90
93 93 87 90 89 91 96 96 95 95 96 97 89 95 87 92 87 90 96 96 94 89 95 75 92 93 90 88 93 90 92 90 89 92 92 86 88 89 90 88 87 89 89 89 91 90 94 94 93 90 94 96 89 89 94 89 85 94 96 95 97 96 96 97 91
93 92 85 89 87 90 94 94 93 93 94 94 91 94 90 92 90 91 94 95 93 88 94 73 92 93 88 89 92 89 94 91 88 91 92 86 88 88 89 88 87 87 89 88 91 91 92 93 92 92 93 94 90 91 93 91 88 89 94 94 95 94 92 95 91 95
70
75
80
85
90
95
100
Figure 2: Similarity of MTEB datasets. We use the best model on MTEB STS (ST5-XXL, see Table 1) to embed
100 samples for each dataset. Cosine similarities between the averaged embeddings are computed and visualized.
Retrieval Each dataset consists of a corpus,
queries and a mapping for each query to relevant
documents from the corpus. The aim is to ﬁnd these
relevant documents. The provided model is used
to embed all queries and all corpus documents and
similarity scores are computed using cosine simi-
larity. After ranking the corpus documents for each
query based on the scores, nDCG@k, MRR@k,
MAP@k, precision@k and recall@k are computed
for several values of k. nDCG@10 serves as the
main metric. MTEB reuses datasets and evaluation
from BEIR (Thakur et al., 2021).
Semantic Textual Similarity (STS) Given a
sentence pair the aim is to determine their simi-
larity. Labels are continuous scores with higher
numbers indicating more similar sentences. The
provided model is used to embed the sentences and
their similarity is computed using various distance
metrics. Distances are benchmarked with ground
truth similarities using Pearson and Spearman cor-
relations. Spearman correlation based on cosine
similarity serves as the main metric (Reimers et al.,
2016).
Summarization A set of human-written and
machine-generated summaries are provided. The
aim is to score the machine summaries. The pro-
vided model is ﬁrst used to embed all summaries.
For each machine summary embedding, distances
to all human summary embeddings are computed.
The closest score (e.g. highest cosine similarity)
is kept and used as the model’s score of a single
machine-generated summary. Pearson and Spear-
man correlations with ground truth human assess-
ments of the machine-generated summaries are
computed. Like for STS, Spearman correlation
based on cosine similarity serves as the main met-
ric (Reimers et al., 2016).
3.3 Datasets
To further the diversity of MTEB, datasets of vary-
ing text lengths are included. All datasets are
grouped into three categories:
Sentence to sentence (S2S) A sentence is com-
pared with another sentence. An example of S2S
are all current STS tasks in MTEB, where the simi-
larity between two sentences is assessed.



### Claim 11/38

#### Claim Text
Following the original setting in MTEB [35], it uses the mini-batch k-means method for the evaluation, with batch size equal to 32 and k equal to the number of labels within the mini-batch.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[14]_1803.05449.pdf (Page 2):

approach as with SICK-E, except that our classiﬁer has only
2 classes, i.e., the aim is to predict whether the sentences are
paraphrases or not.
Caption-Image retrieval The caption-image retrieval
task evaluates joint image and language feature models (Lin
et al., 2014). The goal is either to rank a large collection
of images by their relevance with respect to a given query
caption (Image Retrieval), or ranking captions by their rel-
evance for a given query image (Caption Retrieval). The
COCO dataset provides a training set of 113k images with 5
captions each. The objective consists of learning a caption-
image compatibility score Lcir(x,y) from a set of aligned
image-caption pairs as training data. We use a pairwise
ranking-loss Lcir(x,y):
∑
y
∑
k
max(0,α −s(Vy,Ux ) +s(Vy,Ux k)) +
∑
x
∑
k′
max(0,α −s(Ux,Vy ) +s(Ux,Vy k′ )),
where (x,y) consists of an image y with one of its asso-
ciated captions x, (yk)k and (yk′ )k′ are negative examples
of the ranking loss, α is the margin and s corresponds to
the cosine similarity. U and V are learned linear trans-
formations that project the caption x and the image y to
the same embedding space. We measure Recall@K, with
K ∈{1,5,10}, i.e., the percentage of images/captions for
which the corresponding caption/image is one of the ﬁrst
K retrieved; and median rank. We use the same splits
as Karpathy and Fei-Fei (2015), i.e., we use 113k images
(each containing 5 captions) for training, 5k images for
validation and 5k images for test. For evaluation, we split
the 5k images in 5 random sets of 1k images on which we
compute the mean R@1, R@5, R@10 and median (Med r)
over the 5 splits. We include 2048-dimensional pretrained
ResNet-101 (He et al., 2016) features for all images.
4. Usage and Requirements
Our evaluations comprise two different types: ones where
we need to learn on top of the provided sentence represen-
tations (e.g. classiﬁcation/regression) and ones where we
simply take the cosine similarity between the two represen-
tations, as in the STS tasks. In the binary and multi-class
classiﬁcation tasks, we ﬁt either a Logistic Regression clas-
siﬁer or an MLP with one hidden layer on top of the sen-
tence representations. For the natural language inference
tasks, where we are given two sentences uand v, we pro-
vide the classiﬁer with the input ⟨u,v, |u−v|,u ∗v⟩. To ﬁt
the Pytorch models, we use Adam (Kingma and Ba, 2014),
with a batch size 64. We tune the L2 penalty of the classiﬁer
with grid-search on the validation set. When using Sent-
Eval, two functions should be implemented by the user:
• prepare(params, dataset): sees the whole
dataset and applies any necessary preprocessing, such
as constructing a lookup table of word embeddings
(this function is optional); and
• batcher(params, batch): given a batch of in-
put sentences, returns an array of the sentence embed-
dings for the respective inputs.
The main batcherfunction allows the user to encode text
sentences using any Python framework. For example, the
batcher function might be a wrapper around a model writ-
ten in Pytorch, TensorFlow, Theano, DyNet, or any other
framework5. To illustrate the use, here is an example of
what an evaluation script looks like, having deﬁned the pre-
pare and batcher functions:
import senteval
se = senteval.engine.SE(
params, batcher, prepare)
transfer_tasks = ['MR', 'CR']
results = se.eval(transfer_tasks)
Parameters Both functions make use of a params ob-
ject, which contains the settings of the network and the
evaluation. SentEval has several parameters that inﬂuence
the evaluation procedure. These include the following:
• task path(str, required): path to the data.
• seed(int): random seed for reproducibility.
• batch size (int): size of minibatch of text sen-
tences provided to batcher (sentences are sorted by
length).
• kfold(int): k in the kfold-validation (default: 10).
The default conﬁg is:
params = {'task_path': PATH_TO_DATA,
'usepytorch': True,
'kfold': 10}
We also give the user the ability to customize the classiﬁer
used for the classiﬁcation tasks.
Classiﬁer To be comparable to the results published in
the literature, users should use the following parameters for
Logistic Regression:
params['classifier'] =
{'nhid': 0, 'optim': 'adam',
'batch_size': 64, 'tenacity': 5,
'epoch_size': 4}
The parameters of the classiﬁer include:
• nhid (int): number of hidden units of the MLP; if
nhid> 0, a Multi-Layer Perceptron with one hidden
layer and a Sigmoid nonlinearity is used.
• optim(str): classiﬁer optimizer (default: adam).
• batch size(int): batch size for training the classi-
ﬁer (default: 64).
• tenacity(int): stopping criterion; maximum num-
ber of times the validation error does not decrease.
• epoch size (int): number of passes through the
training set for one epoch.
5Or any other programming language, as long as the vectors
can be passed to, or loaded from, code written in Python.



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 4):

Class. Clust. PairClass. Rerank. Retr. STS Summ. Avg.
Num. Datasets (→) 12 11 3 4 15 10 1 56
Self-supervised methods
Glove 57.29 27.73 70.92 43.29 21.62 61.85 28.87 41.97
Komninos 57.65 26.57 72.94 44.75 21.22 62.47 30.49 42.06
BERT 61.66 30.12 56.33 43.44 10.59 54.36 29.82 38.33
SimCSE-BERT-unsup 62.50 29.04 70.33 46.47 20.29 74.33 31.15 45.45
Supervised methods
SimCSE-BERT-sup 67.32 33.43 73.68 47.54 21.82 79.12 23.31 48.72
coCondenser-msmarco 64.71 37.64 81.74 51.84 32.96 76.47 29.50 52.35
Contriever 66.68 41.10 82.53 53.14 41.88 76.51 30.36 56.00
SPECTER 52.37 34.06 61.37 48.10 15.88 61.02 27.66 40.28
LaBSE 62.71 29.55 78.87 48.42 18.99 70.80 31.05 45.21
LASER2 53.65 15.28 68.86 41.44 7.93 55.32 26.80 33.63
MiniLM-L6 63.06 42.35 82.37 58.04 41.95 78.90 30.81 56.26
MiniLM-L12 63.21 41.81 82.41 58.44 42.69 79.80 27.90 56.53
MiniLM-L12-multilingual 64.30 37.14 78.45 53.62 32.45 78.92 30.67 52.44
MPNet 65.07 43.69 83.04 59.36 43.81 80.28 27.49 57.78
MPNet-multilingual 67.91 38.40 80.81 53.80 35.34 80.73 31.57 54.71
OpenAI Ada Similarity 70.44 37.52 76.86 49.02 18.36 78.60 26.94 49.52
SGPT-125M-nli 61.46 30.95 71.78 47.56 20.90 74.71 30.26 45.97
SGPT-5.8B-nli 70.14 36.98 77.03 52.33 32.34 80.53 30.38 53.74
SGPT-125M-msmarco 60.72 35.79 75.23 50.58 37.04 73.41 28.90 51.23
SGPT-1.3B-msmarco 66.52 39.92 79.58 54.00 44.49 75.74 25.44 56.11
SGPT-2.7B-msmarco 67.13 39.83 80.65 54.67 46.54 76.83 27.87 57.12
SGPT-5.8B-msmarco 68.13 40.35 82.00 56.56 50.25 78.10 24.75 58.81
SGPT-BLOOM-7.1B-msmarco66.19 38.93 81.90 55.65 48.21 77.74 24.99 57.44
GTR-Base 65.25 38.63 83.85 54.23 44.67 77.07 29.67 56.19
GTR-Large 67.14 41.60 85.33 55.36 47.42 78.19 29.50 58.28
GTR-XL 67.11 41.51 86.13 55.96 47.96 77.80 30.21 58.42
GTR-XXL 67.41 42.42 86.12 56.65 48.48 78.38 30.64 58.97
ST5-Base 69.81 40.21 85.17 53.09 33.63 81.14 31.39 55.27
ST5-Large 72.31 41.65 84.97 54.00 36.71 81.83 29.64 57.06
ST5-XL 72.84 42.34 86.06 54.71 38.47 81.66 29.91 57.87
ST5-XXL 73.42 43.71 85.06 56.43 42.24 82.63 30.08 59.51
Table 1: Average of the main metric (see Section 3.2) per task per model on MTEB English subsets.
Paragraph to paragraph (P2P) A paragraph is
compared with another paragraph. MTEB imposes
no limit on the input length, leaving it up to the
models to truncate if necessary. Several clustering
tasks are framed as both S2S and P2P tasks. The
former only compare titles, while the latter include
both title and content. For ArxivClustering, for
example, abstracts are concatenated to the title in
the P2P setting.
Sentence to paragraph (S2P) A few retrieval
datasets are mixed in a S2P setting. Here a query
is a single sentence, while documents are long
paragraphs consisting of multiple sentences.
Similarities across 56 MTEB datasets are vi-
sualized in Figure 2. Several datasets rely on
the same corpora, such as ClimateFEVER and
FEVER, resulting in a score of 1. Clusters of simi-
lar datasets can be seen among CQADupstack vari-
ations and STS datasets. S2S and P2P variations of
the same dataset tend to also be similar. Scientiﬁc
datasets, such as SciDocsRR, SciFact, ArxivClus-
tering, show high similarities among each other
even when coming from different tasks (Reranking,
Retrieval and Clustering in this case).
4 Results
4.1 Models
We evaluate on the test splits of all datasets except
for MSMARCO, where the dev split is used follow-
ing Thakur et al. (2021). We benchmark models
claiming state-of-the-art results on various embed-



Source: data\tc18_2309.07597v5\referenced_papers\[29]_1907.11692.pdf (Page 4):

Model SQuAD 1.1/2.0 MNLI-m SST -2 RACE
Our reimplementation (with NSP loss):
SEG M EN T -PA I R 90.4/78.7 84.0 92.9 64.2
SEN TEN C E -PA I R 88.7/76.2 82.9 92.1 63.0
Our reimplementation (without NSP loss):
FU LL -SEN T E N C E S 90.4/79.1 84.7 92.5 64.8
D O C-SEN TEN C ES 90.6/79.7 84.7 92.7 65.6
BERTBA S E 88.5/76.3 84.3 92.8 64.3
XLNetBA S E (K = 7) –/81.3 85.8 92.7 66.1
XLNetBA S E (K = 6) –/81.0 85.6 93.4 66.7
T able 2: Development set results for base models pretrained over B O O KCO RP U S and W IK IP E D IA . All models are
trained for 1M steps with a batch size of 256 sequences. W e rep ort F1 for SQuAD and accuracy for MNLI-m,
SST -2 and RACE. Reported results are medians over ﬁve random initializations (seeds). Results for BER T BASE and
XLNetBASE are from Y ang et al. (2019).
• SEN TEN C E -PA I R+N S P: Each input contains a
pair of natural sentences, either sampled from
a contiguous portion of one document or from
separate documents. Since these inputs are sig-
niﬁcantly shorter than 512 tokens, we increase
the batch size so that the total number of tokens
remains similar toSEG M EN T -PA IR +N S P. W e re-
tain the NSP loss.
• FU LL -SEN T EN C E S : Each input is packed with
full sentences sampled contiguously from one
or more documents, such that the total length is
at most 512 tokens. Inputs may cross document
boundaries. When we reach the end of one doc-
ument, we begin sampling sentences from the
next document and add an extra separator token
between documents. W e remove the NSP loss.
• D O C-SEN TEN C ES : Inputs are constructed sim-
ilarly to FU LL -SEN T E N C E S , except that they
may not cross document boundaries. Inputs
sampled near the end of a document may be
shorter than 512 tokens, so we dynamically in-
crease the batch size in these cases to achieve
a similar number of total tokens asFU LL -
SEN TEN C ES . W e remove the NSP loss.
Results T able
2 shows results for the four dif-
ferent settings. W e ﬁrst compare the original
SEG M EN T -PA IR input format from Devlin et al.
(2019) to the SEN TEN C E -PA I R format; both for-
mats retain the NSP loss, but the latter uses sin-
gle sentences. W e ﬁnd thatusing individual
sentences hurts performance on downstream
tasks, which we hypothesize is because the model
is not able to learn long-range dependencies.
W e next compare training without the NSP
loss and training with blocks of text from a sin-
gle document (D O C-SEN TEN C ES ). W e ﬁnd that
this setting outperforms the originally published
BERTBA S E results and that removing the NSP loss
matches or slightly improves downstream task
performance, in contrast to
Devlin et al. (2019).
It is possible that the original BERT implementa-
tion may only have removed the loss term while
still retaining theSEG M EN T -PA IR input format.
Finally we ﬁnd that restricting sequences to
come from a single document ( D O C-SEN TEN C ES )
performs slightly better than packing sequences
from multiple documents (FU LL -SEN T E N C E S ).
However, because the D O C-SEN TEN C ES format
results in variable batch sizes, we use FU LL -
SEN TEN C ES in the remainder of our experiments
for easier comparison with related work.
4.3 T raining with large batches
Past work in Neural Machine Translation has
shown that training with very large mini-batches
can both improve optimization speed and end-task
performance when the learning rate is increased
appropriately (
Ott et al. , 2018). Recent work has
shown that BERT is also amenable to large batch
training (
Y ou et al. , 2019).
Devlin et al. (2019) originally trained
BERTBA S E for 1M steps with a batch size of
256 sequences. This is equivalent in computa-
tional cost, via gradient accumulation, to training
for 125K steps with a batch size of 2K sequences,
or for 31K steps with a batch size of 8K.
In T able
3 we compare perplexity and end-



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 2):

MTEB
8 Tasks
58 Datasets
Massive Text  
Embedding Benchmark
Classification
AmazonCounterfactual
Retrieval
Pair Classification
AmazonPolarity
AmazonReviews Banking77 Emotion
Imdb MassiveIntent MassiveScenario
MTOPDomain MTOPIntent
ToxicConversations TweetSentimentExtraction
SprintDuplicateQuestionsTwitterSemEval2015
TwitterURLCorpus
Clustering
ArxivP2P ArxivS2S
STS
BIOSESS SICK-R
STS11 STS12 STS13
STS14
Reranking
Summarization
STS15 STS16
AskUbuntuDupQuestionsMindSmallReranking
SciDocsRR StackOverFlowDupQuestionsSummEval 
STSBSTS17 STS22ArguAna ClimateFEVER
CQADupstackRetrieval FEVER
DBPedia
FiQA2018
HotpotQAMSMARCONFCorpus NQ Quora
SCIDOCS SciFact Touche2020TRECCOVID
MedrxivP2PMedrxivS2S Reddit
StackExchange
RedditP2P
StackExchangeP2P
TwentyNewsgroup
BiorxivP2PBiorxivS2S
Bitext Mining
BUCC Tatoeba
Figure 1: An overview of tasks and datasets in MTEB. Multilingual datasets are marked with a purple shade.
ducibility: Through versioning at a dataset and
software level, we aim to make it easy to repro-
duce results in MTEB. JSON ﬁles corresponding
to all results available in this paper have been made
available together with the MTEB benchmark3.
3.2 Tasks and Evaluation
Figure 1 provides an overview of tasks and datasets
available in MTEB. Dataset statistics are available
in Table 2. The benchmark consists of the follow-
ing 8 task types:
Bitext Mining Inputs are two sets of sentences
from two different languages. For each sentence
in the ﬁrst set, the best match in the second set
needs to be found. The matches are commonly
translations. The provided model is used to embed
each sentence and the closest pairs are found via
cosine similarity. F1 serves as the main metric for
bitext mining. Accuracy, precision and recall are
also computed.
Classiﬁcation A train and test set are embedded
with the provided model. The train set embeddings
are used to train a logistic regression classiﬁer with
100 maximum iterations, which is scored on the
test set. The main metric is accuracy with average
precision and f1 additionally provided.
3https://huggingface.co/datasets/mteb
/results
Clustering Given a set of sentences or para-
graphs, the goal is to group them into meaning-
ful clusters. A mini-batch k-means model with
batch size 32 and k equal to the number of dif-
ferent labels (Pedregosa et al., 2011) is trained on
the embedded texts. The model is scored using
v-measure (Rosenberg and Hirschberg, 2007). V-
measure does not depend on the cluster label, thus
the permutation of labels does not affect the score.
Pair Classiﬁcation A pair of text inputs is pro-
vided and a label needs to be assigned. Labels
are typically binary variables denoting duplicate
or paraphrase pairs. The two texts are embedded
and their distance is computed with various metrics
(cosine similarity, dot product, euclidean distance,
manhattan distance). Using the best binary thresh-
old accuracy, average precision, f1, precision and
recall are computed. The average precision score
based on cosine similarity is the main metric.
Reranking Inputs are a query and a list of rele-
vant and irrelevant reference texts. The aim is to
rank the results according to their relevance to the
query. The model is used to embed the references
which are then compared to the query using cosine
similarity. The resulting ranking is scored for each
query and averaged across all queries. Metrics are
mean MRR@k and MAP with the latter being the
main metric.



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 4):

Zero-shot Text Classification The input and label texts are converted to sentences based on manually
written prompt templates. The predicted label is the one closest to the input text in the embedding
space. Take the sentiment classification of movie reviews as an example, with the original input “I
enjoy watching it”, the label text is “it is an example of terrible/great movie review” and the input text
becomes “movie review: I enjoy watching it”.
Semantic Textual Similarity Given two text embeddings, we use the cosine function to measure
their semantic similarity. Since the absolute similarity scores do not enable an easy interpretation, the
evaluation is usually based on rank correlation coefficients.
Text Clustering Standard clustering algorithms such as k-means can be applied straightforwardly.
Texts belonging to the same category are expected to be close in the embedding space.
For tasks other than zero-shot text classification and retrieval, we use the query embeddings by
default.
5 Experiments
5.1 Pre-training and Fine-tuning Configurations
Pre-training We pre-train on our proposed text pair dataset for three model sizes: E5small, E5base
and E5large initialized from MiniLM [ 59], bert-base-uncased, and bert-large-uncased-whole-word-
masking respectively. The batch size is set to a large value of 32, 768 to increase the number of
negatives. The learning rate is {3, 2, 1}×10−4 for the {small, base, large} models, with linear decay
and the first 1, 000 steps for warmup. We pre-train for 20k steps in total with AdamW optimizer,
which is approximately 2.5 epochs over the dataset. It takes {16, 32, 64} V100 GPUs and {1, 1, 2}
days for the {small, base, large} models. To improve training efficiency and reduce GPU memory
usage, we adopt mixed precision training and gradient checkpointing.
Fine-tuning is performed on the concatenation of 3 datasets: MS-MARCO passage ranking [ 8],
NQ [ 32, 30], and NLI [ 22] datasets. We reuse the mined hard negatives and re-ranker scores from
SimLM [ 58] for the first two datasets. Models are fine-tuned for 3 epochs with batch size 256 on 8
GPUs. Learning rate is {3, 2, 1}×10−5 for the {small, base, large} models with 400 steps warmup.
For each example, we use 7 hard negatives. Since the NLI dataset only has 1 hard negative for each
example, 6 sentences are randomly sampled from the entire corpus.
We use E5-PT to denote models with contrastive pre-training only. More implementation details can
be found in Appendix B.
5.2 Evaluation Datasets
BEIR Benchmark [ 53] is a collection of 19 information retrieval datasets, ranging across ad-hoc
web search, question answering, fact verification and duplicate question retrieval, etc. We evaluate
the 15 datasets that provide public downloads. The main metric is nDCG@10.
MTEB Benchmark [ 40] is recently proposed for benchmarking massive text embedding tasks.
Though MTEB is multilingual due to the inclusion of bitext mining datasets, most datasets are
still only available in English. In this paper, we evaluate the English subsets, which have 56
datasets spanning across 6 categories: Classification (Class.), Clustering (Clust.), Pair Classification
(PairClass.), Rerank, Retrieval (Retr.), STS, and Summarization (Summ.). The evaluation metrics are
accuracy, v-measure, average precision, MAP, nDCG@10, and Spearman coefficients, respectively.
Please refer to the MTEB paper for details.
5.3 Results on BEIR benchmark
Results with Unsupervised Methods In Table 1, we show model results that do not use any labeled
data. When averaged over all 15 datasets, E5-PTbase outperforms the classic BM25 algorithm by 1.2
points. To the best of our knowledge, this is the first reported result that an unsupervised model can
beat BM25 on the BEIR benchmark. When scaling up to E5-PT large, we see further benefits from
42.9 to 44.2.
5



### Claim 12/38

#### Claim Text
Our models are based on the BERT-like architecture [15], which go through three-stage of training (to be discussed in the next section).

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[29]_1907.11692.pdf (Page 5):

bsz steps lr ppl MNLI-m SST -2
256 1M 1e-4 3.99 84.7 92.7
2K 125K 7e-4 3.68 85.2 92.9
8K 31K 1e-3 3.77 84.6 92.8
T able 3: Perplexity on held-out training data ( ppl) and
development set accuracy for base models trained over
BO O KCO RP U S and W IK IP E D IA with varying batch
sizes ( bsz). W e tune the learning rate ( lr) for each set-
ting. Models make the same number of passes over the
data (epochs) and have the same computational cost.
task performance of BERTBA S E as we increase the
batch size, controlling for the number of passes
through the training data. W e observe that train-
ing with large batches improves perplexity for the
masked language modeling objective, as well as
end-task accuracy . Large batches are also easier to
parallelize via distributed data parallel training,
8
and in later experiments we train with batches of
8K sequences.
Notably
Y ou et al. (2019) train BERT with even
larger batche sizes, up to 32K sequences. W e leave
further exploration of the limits of large batch
training to future work.
4.4 T ext Encoding
Byte-Pair Encoding (BPE) (
Sennrich et al. , 2016)
is a hybrid between character- and word-level rep-
resentations that allows handling the large vocab-
ularies common in natural language corpora. In-
stead of full words, BPE relies on subwords units,
which are extracted by performing statistical anal-
ysis of the training corpus.
BPE vocabulary sizes typically range from
10K-100K subword units. However, unicode char-
acters can account for a sizeable portion of this
vocabulary when modeling large and diverse cor-
pora, such as the ones considered in this work.
Radford et al. (2019) introduce a clever imple-
mentation of BPE that uses bytes instead of uni-
code characters as the base subword units. Using
bytes makes it possible to learn a subword vocab-
ulary of a modest size (50K units) that can still en-
code any input text without introducing any “un-
known” tokens.
8 Large batch training can improve training efﬁciency even
without large scale parallel hardware through gradient ac-
cumulation, whereby gradients from multiple mini-batches
are accumulated locally before each optimization step. Thi s
functionality is supported natively in FA I R S E Q (Ott et al. ,
2019).
The original BERT implementa-
tion ( Devlin et al. , 2019) uses a character-level
BPE vocabulary of size 30K, which is learned
after preprocessing the input with heuristic tok-
enization rules. Following
Radford et al. (2019),
we instead consider training BERT with a larger
byte-level BPE vocabulary containing 50K sub-
word units, without any additional preprocessing
or tokenization of the input. This adds approxi-
mately 15M and 20M additional parameters for
BERTBA S E and BERT L A R G E, respectively .
Early experiments revealed only slight dif-
ferences between these encodings, with the
Radford et al. (2019) BPE achieving slightly
worse end-task performance on some tasks. Nev-
ertheless, we believe the advantages of a univer-
sal encoding scheme outweighs the minor degre-
dation in performance and use this encoding in
the remainder of our experiments. A more de-
tailed comparison of these encodings is left to fu-
ture work.
5 RoBERT a
In the previous section we propose modiﬁcations
to the BERT pretraining procedure that improve
end-task performance. W e now aggregate these
improvements and evaluate their combined im-
pact. W e call this conﬁgurationRoBERT a for
R
obustly optimized BERT approach. Speciﬁ-
cally , RoBERT a is trained with dynamic mask-
ing (Section
4.1), FU LL -SEN TE N C E S without NSP
loss (Section 4.2), large mini-batches (Section 4.3)
and a larger byte-level BPE (Section 4.4).
Additionally , we investigate two other impor-
tant factors that have been under-emphasized in
previous work: (1) the data used for pretraining,
and (2) the number of training passes through the
data. For example, the recently proposed XLNet
architecture (
Y ang et al. , 2019) is pretrained us-
ing nearly 10 times more data than the original
BERT (
Devlin et al. , 2019). It is also trained with
a batch size eight times larger for half as many op-
timization steps, thus seeing four times as many
sequences in pretraining compared to BERT .
T o help disentangle the importance of these fac-
tors from other modeling choices (e.g., the pre-
training objective), we begin by training RoBERT a
following the BERTL A R G E architecture ( L = 24,
H = 1024, A = 16, 355M parameters). W e
pretrain for 100K steps over a comparable B O O K-
CO R PU S plus W IK IPED IA dataset as was used in



Source: data\tc18_2309.07597v5\referenced_papers\[15]_1810.04805.pdf (Page 2):

BERT BERT
E[CLS] E1  E[SEP]... EN E1’ ... EM’
C
 T1
 T[SEP]...
 TN
 T1’ ...
 TM’
[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM
Question Paragraph
Start/End Span
BERT
E[CLS] E1  E[SEP]... EN E1’ ... EM’
C
 T1
 T[SEP]...
 TN
 T1’ ...
 TM’
[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM
Masked Sentence A Masked Sentence B
Pre-training Fine-Tuning
NSP Mask LM Mask LM
Unlabeled Sentence A and B Pair 
SQuAD
Question Answer Pair
NERMNLI
Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-
tures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize
models for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special
symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-
tions/answers).
ing and auto-encoder objectives have been used
for pre-training such models (Howard and Ruder,
2018; Radford et al., 2018; Dai and Le, 2015).
2.3 Transfer Learning from Supervised Data
There has also been work showing effective trans-
fer from supervised tasks with large datasets, such
as natural language inference (Conneau et al.,
2017) and machine translation (McCann et al.,
2017). Computer vision research has also demon-
strated the importance of transfer learning from
large pre-trained models, where an effective recipe
is to ﬁne-tune models pre-trained with Ima-
geNet (Deng et al., 2009; Yosinski et al., 2014).
3 BERT
We introduce BERT and its detailed implementa-
tion in this section. There are two steps in our
framework: pre-training and ﬁne-tuning. Dur-
ing pre-training, the model is trained on unlabeled
data over different pre-training tasks. For ﬁne-
tuning, the BERT model is ﬁrst initialized with
the pre-trained parameters, and all of the param-
eters are ﬁne-tuned using labeled data from the
downstream tasks. Each downstream task has sep-
arate ﬁne-tuned models, even though they are ini-
tialized with the same pre-trained parameters. The
question-answering example in Figure 1 will serve
as a running example for this section.
A distinctive feature of BERT is its uniﬁed ar-
chitecture across different tasks. There is mini-
mal difference between the pre-trained architec-
ture and the ﬁnal downstream architecture.
Model Architecture BERT’s model architec-
ture is a multi-layer bidirectional Transformer en-
coder based on the original implementation de-
scribed in Vaswani et al. (2017) and released in
the tensor2tensor library.1 Because the use
of Transformers has become common and our im-
plementation is almost identical to the original,
we will omit an exhaustive background descrip-
tion of the model architecture and refer readers to
Vaswani et al. (2017) as well as excellent guides
such as “The Annotated Transformer.”2
In this work, we denote the number of layers
(i.e., Transformer blocks) as L, the hidden size as
H, and the number of self-attention heads as A.3
We primarily report results on two model sizes:
BERTBASE (L=12, H=768, A=12, Total Param-
eters=110M) and BERTLARGE (L=24, H=1024,
A=16, Total Parameters=340M).
BERTBASE was chosen to have the same model
size as OpenAI GPT for comparison purposes.
Critically, however, the BERT Transformer uses
bidirectional self-attention, while the GPT Trans-
former uses constrained self-attention where every
token can only attend to context to its left.4
1https://github.com/tensorﬂow/tensor2tensor
2http://nlp.seas.harvard.edu/2018/04/03/attention.html
3In all cases we set the feed-forward/ﬁlter size to be 4H,
i.e., 3072 for the H = 768and 4096 for the H = 1024.
4We note that in the literature the bidirectional Trans-



Source: data\tc18_2309.07597v5\referenced_papers\[15]_1810.04805.pdf (Page 0):

BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding
Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova
Google AI Language
{jacobdevlin,mingweichang,kentonl,kristout}@google.com
Abstract
We introduce a new language representa-
tion model called BERT, which stands for
Bidirectional Encoder Representations from
Transformers. Unlike recent language repre-
sentation models (Peters et al., 2018a; Rad-
ford et al., 2018), BERT is designed to pre-
train deep bidirectional representations from
unlabeled text by jointly conditioning on both
left and right context in all layers. As a re-
sult, the pre-trained BERT model can be ﬁne-
tuned with just one additional output layer
to create state-of-the-art models for a wide
range of tasks, such as question answering and
language inference, without substantial task-
speciﬁc architecture modiﬁcations.
BERT is conceptually simple and empirically
powerful. It obtains new state-of-the-art re-
sults on eleven natural language processing
tasks, including pushing the GLUE score to
80.5% (7.7% point absolute improvement),
MultiNLI accuracy to 86.7% (4.6% absolute
improvement), SQuAD v1.1 question answer-
ing Test F1 to 93.2 (1.5 point absolute im-
provement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
1 Introduction
Language model pre-training has been shown to
be effective for improving many natural language
processing tasks (Dai and Le, 2015; Peters et al.,
2018a; Radford et al., 2018; Howard and Ruder,
2018). These include sentence-level tasks such as
natural language inference (Bowman et al., 2015;
Williams et al., 2018) and paraphrasing (Dolan
and Brockett, 2005), which aim to predict the re-
lationships between sentences by analyzing them
holistically, as well as token-level tasks such as
named entity recognition and question answering,
where models are required to produce ﬁne-grained
output at the token level (Tjong Kim Sang and
De Meulder, 2003; Rajpurkar et al., 2016).
There are two existing strategies for apply-
ing pre-trained language representations to down-
stream tasks: feature-based and ﬁne-tuning. The
feature-based approach, such as ELMo (Peters
et al., 2018a), uses task-speciﬁc architectures that
include the pre-trained representations as addi-
tional features. The ﬁne-tuning approach, such as
the Generative Pre-trained Transformer (OpenAI
GPT) (Radford et al., 2018), introduces minimal
task-speciﬁc parameters, and is trained on the
downstream tasks by simply ﬁne-tuning all pre-
trained parameters. The two approaches share the
same objective function during pre-training, where
they use unidirectional language models to learn
general language representations.
We argue that current techniques restrict the
power of the pre-trained representations, espe-
cially for the ﬁne-tuning approaches. The ma-
jor limitation is that standard language models are
unidirectional, and this limits the choice of archi-
tectures that can be used during pre-training. For
example, in OpenAI GPT, the authors use a left-to-
right architecture, where every token can only at-
tend to previous tokens in the self-attention layers
of the Transformer (Vaswani et al., 2017). Such re-
strictions are sub-optimal for sentence-level tasks,
and could be very harmful when applying ﬁne-
tuning based approaches to token-level tasks such
as question answering, where it is crucial to incor-
porate context from both directions.
In this paper, we improve the ﬁne-tuning based
approaches by proposing BERT: Bidirectional
Encoder Representations from Transformers.
BERT alleviates the previously mentioned unidi-
rectionality constraint by using a “masked lan-
guage model” (MLM) pre-training objective, in-
spired by the Cloze task (Taylor, 1953). The
masked language model randomly masks some of
the tokens from the input, and the objective is to
predict the original vocabulary id of the masked
arXiv:1810.04805v2  [cs.CL]  24 May 2019



Source: data\tc18_2309.07597v5\referenced_papers\[15]_1810.04805.pdf (Page 8):

mixed results on the downstream task impact of
increasing the pre-trained bi-LM size from two
to four layers and Melamud et al. (2016) men-
tioned in passing that increasing hidden dimen-
sion size from 200 to 600 helped, but increasing
further to 1,000 did not bring further improve-
ments. Both of these prior works used a feature-
based approach — we hypothesize that when the
model is ﬁne-tuned directly on the downstream
tasks and uses only a very small number of ran-
domly initialized additional parameters, the task-
speciﬁc models can beneﬁt from the larger, more
expressive pre-trained representations even when
downstream task data is very small.
5.3 Feature-based Approach with BERT
All of the BERT results presented so far have used
the ﬁne-tuning approach, where a simple classiﬁ-
cation layer is added to the pre-trained model, and
all parameters are jointly ﬁne-tuned on a down-
stream task. However, the feature-based approach,
where ﬁxed features are extracted from the pre-
trained model, has certain advantages. First, not
all tasks can be easily represented by a Trans-
former encoder architecture, and therefore require
a task-speciﬁc model architecture to be added.
Second, there are major computational beneﬁts
to pre-compute an expensive representation of the
training data once and then run many experiments
with cheaper models on top of this representation.
In this section, we compare the two approaches
by applying BERT to the CoNLL-2003 Named
Entity Recognition (NER) task (Tjong Kim Sang
and De Meulder, 2003). In the input to BERT, we
use a case-preserving WordPiece model, and we
include the maximal document context provided
by the data. Following standard practice, we for-
mulate this as a tagging task but do not use a CRF
Hyperparams Dev Set Accuracy
#L #H #A LM (ppl) MNLI-m MRPC SST-2
3 768 12 5.84 77.9 79.8 88.4
6 768 3 5.24 80.6 82.2 90.7
6 768 12 4.68 81.9 84.8 91.3
12 768 12 3.99 84.4 86.7 92.9
12 1024 16 3.54 85.7 86.9 93.3
24 1024 16 3.23 86.6 87.8 93.7
Table 6: Ablation over BERT model size. #L = the
number of layers; #H = hidden size; #A = number of at-
tention heads. “LM (ppl)” is the masked LM perplexity
of held-out training data.
System Dev F1 Test F1
ELMo (Peters et al., 2018a) 95.7 92.2
CVT (Clark et al., 2018) - 92.6
CSE (Akbik et al., 2018) - 93.1
Fine-tuning approach
BERTLARGE 96.6 92.8
BERTBASE 96.4 92.4
Feature-based approach (BERTBASE )
Embeddings 91.0 -
Second-to-Last Hidden 95.6 -
Last Hidden 94.9 -
Weighted Sum Last Four Hidden 95.9 -
Concat Last Four Hidden 96.1 -
Weighted Sum All 12 Layers 95.5 -
Table 7: CoNLL-2003 Named Entity Recognition re-
sults. Hyperparameters were selected using the Dev
set. The reported Dev and Test scores are averaged over
5 random restarts using those hyperparameters.
layer in the output. We use the representation of
the ﬁrst sub-token as the input to the token-level
classiﬁer over the NER label set.
To ablate the ﬁne-tuning approach, we apply the
feature-based approach by extracting the activa-
tions from one or more layers without ﬁne-tuning
any parameters of BERT. These contextual em-
beddings are used as input to a randomly initial-
ized two-layer 768-dimensional BiLSTM before
the classiﬁcation layer.
Results are presented in Table 7. BERT LARGE
performs competitively with state-of-the-art meth-
ods. The best performing method concatenates the
token representations from the top four hidden lay-
ers of the pre-trained Transformer, which is only
0.3 F1 behind ﬁne-tuning the entire model. This
demonstrates that BERT is effective for both ﬁne-
tuning and feature-based approaches.
6 Conclusion
Recent empirical improvements due to transfer
learning with language models have demonstrated
that rich, unsupervised pre-training is an integral
part of many language understanding systems. In
particular, these results enable even low-resource
tasks to beneﬁt from deep unidirectional architec-
tures. Our major contribution is further general-
izing these ﬁndings to deep bidirectional architec-
tures, allowing the same pre-trained model to suc-
cessfully tackle a broad set of NLP tasks.



Source: data\tc18_2309.07597v5\referenced_papers\[46]_1908.10084.pdf (Page 0):

Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks
Nils Reimers and Iryna Gurevych
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universit¨at Darmstadt
www.ukp.tu-darmstadt.de
Abstract
BERT (Devlin et al., 2018) and RoBERTa (Liu
et al., 2019) has set a new state-of-the-art
performance on sentence-pair regression tasks
like semantic textual similarity (STS). How-
ever, it requires that both sentences are fed
into the network, which causes a massive com-
putational overhead: Finding the most sim-
ilar pair in a collection of 10,000 sentences
requires about 50 million inference computa-
tions (~65 hours) with BERT. The construction
of BERT makes it unsuitable for semantic sim-
ilarity search as well as for unsupervised tasks
like clustering.
In this publication, we present Sentence-BERT
(SBERT), a modiﬁcation of the pretrained
BERT network that use siamese and triplet net-
work structures to derive semantically mean-
ingful sentence embeddings that can be com-
pared using cosine-similarity. This reduces the
effort for ﬁnding the most similar pair from 65
hours with BERT / RoBERTa to about 5 sec-
onds with SBERT, while maintaining the ac-
curacy from BERT.
We evaluate SBERT and SRoBERTa on com-
mon STS tasks and transfer learning tasks,
where it outperforms other state-of-the-art
sentence embeddings methods.1
1 Introduction
In this publication, we present Sentence-BERT
(SBERT), a modiﬁcation of the BERT network us-
ing siamese and triplet networks that is able to
derive semantically meaningful sentence embed-
dings2. This enables BERT to be used for certain
new tasks, which up-to-now were not applicable
for BERT. These tasks include large-scale seman-
1Code available: https://github.com/UKPLab/
sentence-transformers
2With semantically meaningfulwe mean that semantically
similar sentences are close in vector space.
tic similarity comparison, clustering, and informa-
tion retrieval via semantic search.
BERT set new state-of-the-art performance on
various sentence classiﬁcation and sentence-pair
regression tasks. BERT uses a cross-encoder: Two
sentences are passed to the transformer network
and the target value is predicted. However, this
setup is unsuitable for various pair regression tasks
due to too many possible combinations. Finding
in a collection of n = 10 000sentences the pair
with the highest similarity requires with BERT
n·(n−1)/2 = 49 995 000inference computations.
On a modern V100 GPU, this requires about 65
hours. Similar, ﬁnding which of the over 40 mil-
lion existent questions of Quora is the most similar
for a new question could be modeled as a pair-wise
comparison with BERT, however, answering a sin-
gle query would require over 50 hours.
A common method to address clustering and se-
mantic search is to map each sentence to a vec-
tor space such that semantically similar sentences
are close. Researchers have started to input indi-
vidual sentences into BERT and to derive ﬁxed-
size sentence embeddings. The most commonly
used approach is to average the BERT output layer
(known as BERT embeddings) or by using the out-
put of the ﬁrst token (the [CLS] token). As we
will show, this common practice yields rather bad
sentence embeddings, often worse than averaging
GloVe embeddings (Pennington et al., 2014).
To alleviate this issue, we developed SBERT.
The siamese network architecture enables that
ﬁxed-sized vectors for input sentences can be de-
rived. Using a similarity measure like cosine-
similarity or Manhatten / Euclidean distance, se-
mantically similar sentences can be found. These
similarity measures can be performed extremely
efﬁcient on modern hardware, allowing SBERT
to be used for semantic similarity search as well
as for clustering. The complexity for ﬁnding the
arXiv:1908.10084v1  [cs.CL]  27 Aug 2019



### Claim 13/38

#### Claim Text
Instead of mining hard negative samples on purpose, we purely rely on in-batch negative samples [25] and resort to a big batch size (as large as 19,200) to improve the discriminativeness of the embedding. •Task-specific fine-tuning.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[22]_2112.09118.pdf (Page 4):

Published in Transactions on Machine Learning Research (08/2022)
Trec-COVIDTóuche-2020
NFCorpusScidocsDBPedia
Climate-fever
FiQA
CQADupStackMSMARCOHotpotQA
NQ
ArguAnaScifactFeverQuora
Avg.0
50
100Recall@100
REALM SimCSE BM25 Contriever
Figure 1: Unsupervised retrieval. We compare our pre-training without usingany annotated data to
REALM (Guu et al., 2020), SimCSE (Gao et al., 2021) and BM25. For SimCSE we report results of the
model using RoBERTa large. REALM uses annotated entity recognition data for training. We highlight that
our unsupervised pre-training is on par with BM25 but on 2 datasets.
tokens of the span as the query and the complement(w1,...,wa−1,wb+1,...,wn) as the key. In the original
implementation by Lee et al. (2019) the span corresponds to a sentence, and is kept in the document 10% of
the time to encourage lexical matching. The Inverse Cloze Task is closely related to the Cloze task which
uses the span complement(w1,...,wa−1,wb+1,...,wn) as the query.
Independent cropping is a common independent data augmentation used for images where views are
generated independently by cropping the input. In the context of text, cropping is equivalent to sampling a
span of tokens. This strategy thus samples independently two spans from a document to form a positive
pair. As opposed to the inverse Cloze task, incropping both views of the example correspond to contiguous
subsequence of the original data. A second diﬀerence between cropping and ICT is the fact that independent
random cropping is symmetric: both the queries and documents follow the same distribution. Independent
cropping also lead to overlap between the two views of the data, hence encouraging the network to learn
exact matches between the query and document, in a way that is similar to lexical matching methods like
BM25. In practice, we can either ﬁx the length of the span for the query and the key, or sample them.
Additional data augmentation. Finally, we also consider additional data augmentations such as random
word deletion, replacement or masking. We use these perturbations in addition to random cropping.
3.1.3 Building large set of negative pairs
An important aspect of contrastive learning is to sample a large set of negatives. Most standard frameworks
diﬀer from each other in terms of how the negatives are handled, and we brieﬂy describe two of them, in-batch
negative sampling and MoCo, that we use in this work.
Negatives within a batch. A ﬁrst solution is to generate the negatives by using the other examples from
the same batch: each example in a batch is transformed twice to generate positive pairs, and we generate
negatives by using the views from the other examples in the batch. We will refer to this technique as “in-batch
negatives”. In that case, the gradient is back-propagated through the representations of both the queries
and the keys. A downside of this approach is that it requires extremely large batch sizes to work well Chen
et al. (2020), with Qu et al. (2021) reporting improvement in the context of information retrieval up to 8192
negatives. This method has been widely used to train information retrieval models with supervised data Chen
et al. (2017b); Karpukhin et al. (2020) and was also considered when using ICT to pre-train retrievers by Lee
et al. (2019).
5



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 3):

Preprint
q
𝑑!
𝐷!! "#
"
Trainer
Inferencer
q
𝑑!
𝐷!! "$
"
Checkpoint k-1
…
Checkpoint k
q
𝑑!
𝐷!! "$
"
q
𝑑!
𝐷!!
"
…
Checkpoint k+1
q
𝑑!
𝐷!! "#
"
…
Inferencing
Index & 
Search
Training 
Positives
ANCE 
Negatives
Index & 
Search
Figure 2: ANCE Asynchronous Training. The Trainer learns the representation using negatives from
the ANN index. The Inferencer uses a recent checkpoint to update the representation of documents in
the corpus and once ﬁnished, refreshes the ANN index with most up-to-date encodings.
where L is the number of layers, ρis composed by pre-activation weights and gradients in inter-
mediate layers, and ||∇φLl(d+,d−)||2 is the gradient w.r.t. the last layer. Intuitively, the inter-
mediate layers are more regulated by various normalization techniques; the main moving piece is
||∇φLl(d+,d−)||2 (Katharopoulos & Fleuret, 2018).
For common learning to rank loss functions, for example, BCE loss and pairwise hinge loss, we can
veriﬁed that (Katharopoulos & Fleuret, 2018):
l(d+,d−) →0 ⇒||∇φLl(d+,d−)||2 →0 ⇒||∇θtl(d+,d−)||2 →0. (12)
Intuitively, negative samples with near zero loss have near zero gradients and contribute little to
model convergence. The convergence of dense retrieval model training relies on the informativeness
of constructed negatives.
Inefﬁcacy of Local In-Batch Negatives: We argue that the in-batch local negatives are unlikely to
provide informative samples due to two common properties of text retrieval.
Let D−∗be the set of informative negatives that are hard to distinguish from D+, and bbe the batch
size, we have (1) b≪|C|, the batch size is far smaller than the corpus size; (2) |D−∗|≪| C|, that
only a few negatives are informative and the majority of corpus is trivially unrelated.
Both conditions are easy to verify empirically in dense retrieval benchmarks. The two together make
the probability that a random mini-batch includes meaningful negatives p= b|D−∗|
|C|2 close to zero.
Selecting negatives from local training batches is unlikely to provide optimal training signals for
dense retrieval.
4 A PPROXIMATE NEAREST NEIGHBOR NOISE CONTRASTIVE ESTIMATION
Our analyses show the importance, if not necessity, to construct negativesglobally from the corpus.
In this section, we propose Approximate nearest neighbor Negative Contrastive Estimation, (ANCE),
which selects negatives from the entire corpus using an asynchronously updated ANN index.
ANCE samples negatives from the top retrieved documents via the DR model from the ANN index:
θ∗= argminθ
∑
q
∑
d+∈D+
∑
d−∈D−
ANCE
l(f(q,d+),f(q,d−)), (13)
with D−
ANCE = ANNf(q,d) \D+ and ANNf(q,d) the top retrieved documents by f() from the ANN
index. By deﬁnition, D−
ANCE are the hardest negatives for the current DR model: D−
ANCE ≈D−∗.
In theory, these more informative negatives have higher training loss, higher upper bound on the
gradient norms, and will improve training convergence.
ANCE can be used to train any dense retrieval model. For simplicity, we use a simple set up in recent
research (Luan et al., 2020) with BERT Siamese/Dual Encoder (shared between qand d), dot product
similarity, and negative log likelihood (NLL) loss.
4



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 2):

Preprint
instances, which have to be sampled in training:
θ∗= argminθ
∑
q
∑
d+∈D+
∑
d−∈ˆD−
l(f(q,d+),f(q,d−)). (3)
A natural choice is to sample negatives ˆD−from top documents retrieved by BM25. However, they
may bias the DR model to merely learn sparse retrieval and do not elevate DR models much beyond
BM25 (Luan et al., 2020). Another way is to sample negatives in local mini-batches, e.g., as in
contrastive learning (Oord et al., 2018; Chen et al., 2020a), however, these local negatives do not
signiﬁcantly outperform BM25 negatives (Karpukhin et al., 2020; Luan et al., 2020).
3 A NALYSES ON THE CONVERGENCE OF DENSE RETRIEVAL TRAINING
In this section, we provide theoretical analyses on the convergence of representation training in dense
retrieval. We ﬁrst show the connections between learning convergence and gradient norms, then the
bounded gradient norms by uninformative negatives, and ﬁnally, how in-batch local negatives are
ineffective under common conditions in dense retrieval.
Convergence Rate and Gradient Norms: Let l(d+,d−) = l(f(q,d+),f(q,d−) be the loss func-
tion on the training triple (q,d+,d−), PD− the negative sampling distribution for the given (q,d+),
and pd− the sampling probability of negative instance d−, a stochastic gradient decent (SGD) step
with importance sampling (Alain et al., 2015) is:
θt+1 = θt −η 1
Npd−
∇θtl(d+,d−), (4)
with θt the parameter at t-th step, θt+1 the one after, and N the total number of negatives. The scaling
factor 1
Npd−
is to make sure Eqn. 4 is an unbiased estimator of the full gradient.
Then we can characterize the converge rate of this SGD step as the movement to optimalθ∗. Following
derivations in variance reduction (Katharopoulos & Fleuret, 2018; Johnson & Guestrin, 2018), let
gd− = 1
Npd−
∇θtl(d+,d−) the weighted gradient, the convergence rate is:
E∆t = ||θt −θ∗||2 −EPD− (||θt+1 −θ∗||2) (5)
= ||θt||2 −2θT
t θ∗−EPD− (||θt −ηgd−||2) + 2θ∗TEPD− (θt −ηgd−) (6)
= −η2EPD− (||gd−||2) + 2ηθT
t EPD− (gd−) −2ηθ∗TEPD− (gd−) (7)
= 2ηEPD− (gd−)T(θt −θ∗) −η2EPD− (||gd−||2) (8)
= 2ηEPD− (gd−)T(θt −θ∗) −η2EPD− (gd−)TEPD− (gd−) −η2Tr(VPD− (gd−)). (9)
This shows we can obtain better convergence rate by sampling from a distributionPD− that minimizes
the variance of the gradient estimator,EPD− (||gd−||2), or Tr(VPD− (gd−)) as the estimator is unbiased.
There exists an optimal distribution that:
p∗
d− = argminpd− Tr(VPD− (gd−)) ∝||∇θtl(d+,d−)||2, (10)
which is to sample proportionally to per instance gradient norm. This is a well known result in
importance sampling (Alain et al., 2015; Johnson & Guestrin, 2018). It can be proved by applying
Jensen’s inequality on the gradient variance and then verifying that Eqn. 10 achieves the minimum.
We do not repeat this proof and refer to Johnson & Guestrin (2018) for exact derivations.
Intuitively, an negative instance with larger gradient norm is more likely to reduce the training loss
more, while those with diminishing gradients are not informative. Empirically, the correlation of
gradient norm and training convergence is also observed in BERT ﬁne-tuning (Mosbach et al., 2020).
Diminishing Gradients of Uninformative Negatives: The oracle distribution in Eqn. 10 is too
expensive to compute and the closed form of gradient norms can be complicated in deep neural
networks. Nevertheless, for MLP networks, Katharopoulos & Fleuret (2018) derives an upper bound
of the per sample gradient norm:
||∇θtl(d+,d−)||2 ≤Lρ||∇φLl(d+,d−)||2, (11)
3



Source: data\tc18_2309.07597v5\referenced_papers\[52]_2207.02578.pdf (Page 3):

distillationBM25 negatives mined negatives
Retriever 1 Retriever 2
Data
Re-ranker
mined negatives
Retrieverdistill
Pre-trained model
initializeinitialize initialize
Figure 2: Illustration of our supervised ﬁne-tuning pipeline. Note that we only use SimLM to initialize the
biencoder-based retrievers. For cross-encoder based re-ranker, we use off-the-shelf pre-trained models such as
ELECTRAbase.
supervised ﬁne-tuning pipeline. In contrast to
previous approaches, our proposed pipeline is
relatively straightforward and does not require
joint training (Ren et al., 2021b) or re-building
index periodically (Xiong et al., 2021). Each
stage takes the outputs from the previous stage
as inputs and can be trained in a standalone fashion.
Retriever1 Given a labeled query-passage pair
(q+,d+), we take the last-layer [CLS] vector of
the pre-trained encoder as their representations
(hq+,hd+). Both the in-batch negatives and BM25
hard negatives are used to compute the contrastive
loss Lcont:
−log φ(q+,d+)
φ(q+,d+) +
∑
ni∈N
(φ(q+,ni) +φ(d+,ni))
(3)
Where N denotes all the negatives, and φ(q,d) is a
function to compute the matching score between
query q and passage d. In this paper, we use
temperature-scaled cosine similarity function:
φ(q,d) =exp(1
τ cos(hq,hd)). τ is a temperature
hyper-parameter and set to a constant 0.02 in our
experiments.
Retriever2 It is trained in the same way as
Retriever1 except that the hard negatives are mined
based on a well-trained Retriever1 checkpoint.
Re-ranker is a cross-encoder that re-ranks the top-
kresults of Retriever2. It takes the concatenation
of query qand passage das input and outputs a real-
valued score θ(q,d). Given a labeled positive pair
(q+,d+) and n−1 hard negative passages randomly
sampled from top-kpredictions of Retriever2, we
adopt a listwise loss to train the re-ranker:
−log exp(θ(q+,d+))
exp(θ(q+,d+)) +∑n−1
i=1 exp(θ(q+,d−
i ))
(4)
The cross-encoder architecture can model the
full interaction between the query and the passage,
making it suitable to be a teacher model for
knowledge distillation.
Retrieverdistill Although cross-encoder based re-
ranker is powerful, it is not scalable enough for
ﬁrst-stage retrieval. To combine the scalability of
biencoder and the effectiveness of cross-encoder,
we can train a biencoder-based retriever by dis-
tilling the knowledge from the re-ranker. The re-
ranker from the previous stage is employed to com-
pute scores for both positive pairs and mined nega-
tives from Retriever2. These scores are then used
as training data for knowledge distillation. With
n−1 mined hard negatives, we use KL (Kullback-
Leibler) divergence Lkl as the loss function for
distilling the soft labels:
Lkl =
n∑
i=1
pi
ranker log pi
ranker
piret
(5)
where pranker and pret are normalized probabili-
ties from the re-ranker teacher and Retriever distill
student. For training with the hard labels, we
use the contrastive loss Lcont as deﬁned in Equa-
tion 3. The ﬁnal loss is their linear interpolation:
L= Lkl + αLcont.
Our pre-trained SimLM model is used to ini-
tialize all three biencoder-based retrievers but not
the cross-encoder re-ranker. Since our pre-training



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 1):

Preprint
Query
Relevant
DR Neg
BM25 Neg
Rand Neg
Figure 1: T-SNE (Maaten & Hinton,
2008) representations of query, relevant
documents, negative training instances
from BM25 (BM25 Neg) or randomly
sampled (Rand Neg), and testing nega-
tives (DR Neg) in dense retrieval.
In this paper, we ﬁrst theoretically analyze the convergence
of dense retrieval training with negative sampling. Us-
ing the variance reduction framework (Alain et al., 2015;
Katharopoulos & Fleuret, 2018), we show that, under con-
ditions commonly met in dense retrieval, local in-batch
negatives lead to diminishing gradient norms, resulted in
high stochastic gradient variances and slow training con-
vergence — the local negative sampling is the bottleneck
of dense retrieval’s effectiveness.
Based on our analysis, we propose Approximate near-
est neighbor Negative Contrastive Estimation (ANCE),
a new contrastive representation learning mechanism for
dense retrieval. Instead of random or in-batch local neg-
atives, ANCE constructs global negatives using the being-
optimized DR model to retrieve from the entire corpus.
This fundamentally aligns the distribution of negative sam-
ples in training and of irrelevant documents to separate in
testing. From the variance reduction point of view, these ANCE negatives lift the upper bound of per
instance gradient norm, reduce the variance of the stochastic gradient estimation, and lead to faster
learning convergence.
We implement ANCE using an asynchronously updated ANN index of the corpus representation.
Similar to Guu et al. (2020), we maintain an Inferencer that parallelly computes the document
encodings with a recent checkpoint from the being optimized DR model, and refresh the ANN index
used for negative sampling once it ﬁnishes, to keep up with the model training. Our experiments
demonstrate the advantage of ANCE in three text retrieval scenarios: standard web search (Craswell
et al., 2020), OpenQA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), and in a commercial search
engine’s retrieval system. We also empirically validate our theory that the gradient norms on ANCE
sampled negatives are much bigger than local negatives and thus improve the convergence of dense
retrieval models. Our code and trained models are available at https://aka.ms/ance.
2 P RELIMINARIES
In this section, we discuss the preliminaries of dense retrieval and its representation learning.
Task Deﬁnition: Given a query qand a corpus C, the ﬁrst stage retrieval is to ﬁnd a set of documents
relevant to the query D+ = {d1,...,d i,...,d n}from C(|D+|≪| C|), which then serve as input to
later more complex models (Croft et al., 2010). Instead of using sparse term matches and inverted
index, Dense Retrieval calculates the retrieval score f() using similarities in a learned embedding
space (Lee et al., 2019; Luan et al., 2020; Karpukhin et al., 2020):
f(q,d) = sim(g(q; θ),g(d; θ)), (1)
where g() is the representation model that encodes the query or document to dense embeddings. The
encoder parameter θprovides the main capacity, often ﬁne-tuned from pretrained transformers, e.g.,
BERT (Lee et al., 2019). The similarity function (sim()) is often simply cosine or dot product, to
leverage efﬁcient ANN retrieval (Johnson et al., 2019; Guo et al., 2020).
Learning with Negative Sampling: The effectiveness of DR resides in learning a good representa-
tion space that maps query and relevant documents together, while separating irrelevant ones. The
learning of this representation often follows standard learning to rank (Liu, 2009): Given a query q, a
set of relevant document D+ and irrelevant ones D−, ﬁnd the best θ∗that:
θ∗= argminθ
∑
q
∑
d+∈D+
∑
d−∈D−
l(f(q,d+),f(q,d−)). (2)
The loss l() can be binary cross entropy (BCE), hinge loss, or negative log likelihood (NLL).
A unique challenge in dense retrieval, targeting ﬁrst stage retrieval, is that the irrelevant documents
to separate are from the entire corpus (D− = C \D+). This often leads to millions of negative
2



### Claim 14/38

#### Claim Text
The hard negative sample is mined from the task’s original corpus, following the ANN-style sampling strategy in [61]. 4 EXPERIMENTS In this section, we conduct experimental studies for the exploration of following problems.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 3):

Preprint
q
𝑑!
𝐷!! "#
"
Trainer
Inferencer
q
𝑑!
𝐷!! "$
"
Checkpoint k-1
…
Checkpoint k
q
𝑑!
𝐷!! "$
"
q
𝑑!
𝐷!!
"
…
Checkpoint k+1
q
𝑑!
𝐷!! "#
"
…
Inferencing
Index & 
Search
Training 
Positives
ANCE 
Negatives
Index & 
Search
Figure 2: ANCE Asynchronous Training. The Trainer learns the representation using negatives from
the ANN index. The Inferencer uses a recent checkpoint to update the representation of documents in
the corpus and once ﬁnished, refreshes the ANN index with most up-to-date encodings.
where L is the number of layers, ρis composed by pre-activation weights and gradients in inter-
mediate layers, and ||∇φLl(d+,d−)||2 is the gradient w.r.t. the last layer. Intuitively, the inter-
mediate layers are more regulated by various normalization techniques; the main moving piece is
||∇φLl(d+,d−)||2 (Katharopoulos & Fleuret, 2018).
For common learning to rank loss functions, for example, BCE loss and pairwise hinge loss, we can
veriﬁed that (Katharopoulos & Fleuret, 2018):
l(d+,d−) →0 ⇒||∇φLl(d+,d−)||2 →0 ⇒||∇θtl(d+,d−)||2 →0. (12)
Intuitively, negative samples with near zero loss have near zero gradients and contribute little to
model convergence. The convergence of dense retrieval model training relies on the informativeness
of constructed negatives.
Inefﬁcacy of Local In-Batch Negatives: We argue that the in-batch local negatives are unlikely to
provide informative samples due to two common properties of text retrieval.
Let D−∗be the set of informative negatives that are hard to distinguish from D+, and bbe the batch
size, we have (1) b≪|C|, the batch size is far smaller than the corpus size; (2) |D−∗|≪| C|, that
only a few negatives are informative and the majority of corpus is trivially unrelated.
Both conditions are easy to verify empirically in dense retrieval benchmarks. The two together make
the probability that a random mini-batch includes meaningful negatives p= b|D−∗|
|C|2 close to zero.
Selecting negatives from local training batches is unlikely to provide optimal training signals for
dense retrieval.
4 A PPROXIMATE NEAREST NEIGHBOR NOISE CONTRASTIVE ESTIMATION
Our analyses show the importance, if not necessity, to construct negativesglobally from the corpus.
In this section, we propose Approximate nearest neighbor Negative Contrastive Estimation, (ANCE),
which selects negatives from the entire corpus using an asynchronously updated ANN index.
ANCE samples negatives from the top retrieved documents via the DR model from the ANN index:
θ∗= argminθ
∑
q
∑
d+∈D+
∑
d−∈D−
ANCE
l(f(q,d+),f(q,d−)), (13)
with D−
ANCE = ANNf(q,d) \D+ and ANNf(q,d) the top retrieved documents by f() from the ANN
index. By deﬁnition, D−
ANCE are the hardest negatives for the current DR model: D−
ANCE ≈D−∗.
In theory, these more informative negatives have higher training loss, higher upper bound on the
gradient norms, and will improve training convergence.
ANCE can be used to train any dense retrieval model. For simplicity, we use a simple set up in recent
research (Luan et al., 2020) with BERT Siamese/Dual Encoder (shared between qand d), dot product
similarity, and negative log likelihood (NLL) loss.
4



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 0):

Preprint
APPROXIMATE NEAREST NEIGHBOR NEGATIVE CON-
TRASTIVE LEARNING FOR DENSE TEXT RETRIEVAL
Lee Xiong∗, Chenyan Xiong∗, Ye Li, Kwok-Fung Tang, Jialin Liu,
Paul Bennett, Junaid Ahmed, Arnold Overwijk
Microsoft
lexion, chenyan.xiong, yeli1, kwokfung.tang, jialliu,
paul.n.bennett, jahmed, arnold.overwijk@microsoft.com
ABSTRACT
Conducting text retrieval in a dense representation space has many intriguing ad-
vantages. Yet the end-to-end learned dense retrieval (DR) often underperforms
word-based sparse retrieval. In this paper, we ﬁrst theoretically show the learning
bottleneck of dense retrieval is due to the domination of uninformative negatives
sampled locally in batch, which yield diminishing gradient norms, large stochastic
gradient variances, and slow learning convergence. We then propose Approximate
nearest neighbor Negative Contrastive Learning (ANCE), a learning mechanism
that selects hard training negatives globally from the entire corpus, using an asyn-
chronously updated ANN index. Our experiments demonstrate the effectiveness
of ANCE on web search, question answering, and in a commercial search envi-
ronment, showing ANCE dot-product retrieval nearly matches the accuracy of
BERT-based cascade IR pipeline, while being 100x more efﬁcient.
1 I NTRODUCTION
Many language systems rely on text retrieval as their ﬁrst step to ﬁnd relevant information. For
example, search ranking (Nogueira & Cho, 2019), open domain question answering (OpenQA) (Chen
et al., 2017), and fact veriﬁcation (Thorne et al., 2018) all ﬁrst retrieve relevant documents for their
later stage reranking, machine reading, and reasoning models. All these later-stage models enjoy the
advancements of deep learning techniques (Rajpurkar et al., 2016; Wang et al., 2018), while, the ﬁrst
stage retrieval still mainly relies on matching discrete bag-of-words, e.g., BM25, which has become
the bottleneck of many systems (Nogueira & Cho, 2019; Luan et al., 2020; Zhao et al., 2020).
Dense Retrieval (DR) aims to overcome the sparse retrieval bottleneck by matching texts in a contin-
uous representation space learned via deep neural networks (Lee et al., 2019; Karpukhin et al., 2020;
Luan et al., 2020). It has many desired properties: fully learnable representation, easy integration
with pretraining, and efﬁciency support from approximate nearest neighbor (ANN) search (Johnson
et al., 2019). These make dense retrieval an intriguing potential choice to fundamentally overcome
some intrinsic limitations of sparse retrieval, for example, vocabulary mismatch (Croft et al., 2010).
A key challenge in DR is to construct proper negative instances during its representation learn-
ing (Karpukhin et al., 2020). Unlike in reranking where negatives are naturally the irrelevant
documents from previous retrieval stages, in ﬁrst stage retrieval, DR models have to distinguish
relevant documents from all irrelevant ones in the entire corpus. As illustrated in Fig. 1, these global
negatives are quite different from negatives retrieved by sparse models.
Recent research explored various ways to construct negative training instances for dense re-
trieval (Huang et al., 2020; Karpukhin et al., 2020)., e.g., using contrastive learning (Faghri et al.,
2017; Oord et al., 2018; He et al., 2019; Chen et al., 2020a) to select hard negatives in current or
recent mini-batches. However, as observed in recent research (Karpukhin et al., 2020), the in-batch
local negatives, though effective in learning word or visual representations, are not signiﬁcantly better
than spare-retrieved negatives in representation learning for dense retrieval. In addition, the accuracy
of dense retrieval models often underperform BM25, especially on documents (Lee et al., 2019; Gao
et al., 2020b; Luan et al., 2020).
∗Lee and Chenyan contributed equally.
1
arXiv:2007.00808v2  [cs.IR]  20 Oct 2020



Source: data\tc18_2309.07597v5\referenced_papers\[22]_2112.09118.pdf (Page 4):

Published in Transactions on Machine Learning Research (08/2022)
Trec-COVIDTóuche-2020
NFCorpusScidocsDBPedia
Climate-fever
FiQA
CQADupStackMSMARCOHotpotQA
NQ
ArguAnaScifactFeverQuora
Avg.0
50
100Recall@100
REALM SimCSE BM25 Contriever
Figure 1: Unsupervised retrieval. We compare our pre-training without usingany annotated data to
REALM (Guu et al., 2020), SimCSE (Gao et al., 2021) and BM25. For SimCSE we report results of the
model using RoBERTa large. REALM uses annotated entity recognition data for training. We highlight that
our unsupervised pre-training is on par with BM25 but on 2 datasets.
tokens of the span as the query and the complement(w1,...,wa−1,wb+1,...,wn) as the key. In the original
implementation by Lee et al. (2019) the span corresponds to a sentence, and is kept in the document 10% of
the time to encourage lexical matching. The Inverse Cloze Task is closely related to the Cloze task which
uses the span complement(w1,...,wa−1,wb+1,...,wn) as the query.
Independent cropping is a common independent data augmentation used for images where views are
generated independently by cropping the input. In the context of text, cropping is equivalent to sampling a
span of tokens. This strategy thus samples independently two spans from a document to form a positive
pair. As opposed to the inverse Cloze task, incropping both views of the example correspond to contiguous
subsequence of the original data. A second diﬀerence between cropping and ICT is the fact that independent
random cropping is symmetric: both the queries and documents follow the same distribution. Independent
cropping also lead to overlap between the two views of the data, hence encouraging the network to learn
exact matches between the query and document, in a way that is similar to lexical matching methods like
BM25. In practice, we can either ﬁx the length of the span for the query and the key, or sample them.
Additional data augmentation. Finally, we also consider additional data augmentations such as random
word deletion, replacement or masking. We use these perturbations in addition to random cropping.
3.1.3 Building large set of negative pairs
An important aspect of contrastive learning is to sample a large set of negatives. Most standard frameworks
diﬀer from each other in terms of how the negatives are handled, and we brieﬂy describe two of them, in-batch
negative sampling and MoCo, that we use in this work.
Negatives within a batch. A ﬁrst solution is to generate the negatives by using the other examples from
the same batch: each example in a batch is transformed twice to generate positive pairs, and we generate
negatives by using the views from the other examples in the batch. We will refer to this technique as “in-batch
negatives”. In that case, the gradient is back-propagated through the representations of both the queries
and the keys. A downside of this approach is that it requires extremely large batch sizes to work well Chen
et al. (2020), with Qu et al. (2021) reporting improvement in the context of information retrieval up to 8192
negatives. This method has been widely used to train information retrieval models with supervised data Chen
et al. (2017b); Karpukhin et al. (2020) and was also considered when using ICT to pre-train retrievers by Lee
et al. (2019).
5



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 4):

Preprint
Asynchronous Index Refresh: During stochastic training, the DR model f() is updated each mini-
batch. Maintaining an update-to-date ANN index to select fresh ANCE negatives is challenging, as
the index update requires two operations: 1) Inference: refresh the representations of all documents in
the corpus with an updated DR model; 2)Index: rebuild the ANN index using updated representations.
Although Index is efﬁcient (Johnson et al., 2019), Inference is too expensive to compute per batch as
it requires a forward pass on the entire corpus which is much bigger than the training batch.
Thus we implement an asynchronous index refresh similar to Guu et al. (2020), and update the ANN
index once every mbatches, i.e., with checkpoint fk. As illustrated in Fig. 2, besides the Trainer, we
run an Inferencer that takes the latest checkpoint (e.g., fk) and recomputes the encodings of the entire
corpus. In parallel, the Trainer continues its stochastic learning using D−
fk−1
from ANNfk−1 . Once
the corpus is re-encoded, Inferencer updates the ANN index (ANNfk ) and feed it to the Trainer.
In this process, the ANCE negatives ( D−
ANCE) are asynchronously updated to “catch up” with the
stochastic training. The gap between the ANN index and the DR model optimization depends on the
allocation of computing resources between Trainer and Inferencer. Appendix A.3 shows an 1:1 GPU
split is sufﬁcient to minimize the inﬂuence of this gap.
5 E XPERIMENTAL METHODOLOGIES
This section describes our experimental setups. More details can be found in Appendix A.1 and A.2.
Benchmarks: The web search experiments use the TREC 2019 Deep Learning (DL) Track bench-
mark (Craswell et al., 2020), a large scale ad hoc retrieval dataset. We follow the ofﬁcial guideline
and evaluate mainly in the retrieval setting, but also results when reranking top 100 BM25 candidates.
The OpenQA experiments use the Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA
(TQA) (Joshi et al., 2017), following the exact settings from Karpukhin et al. (2020). The metrics are
Coverage@20/100, which evaluate whether the Top-20/100 retrieved passages include the answer.
We also evaluate whether ANCE’s better retrieval can propagate to better answer accuracy, by running
the state-of-the-art systems’ readers on top of ANCE instead of DPR retrieval. The readers are
RAG-Token (Lewis et al., 2020b) on NQ and DPR Reader on TQA, in their suggested settings.
We also study the effectiveness of ANCE in the ﬁrst stage retrieval of a commercial search engine’s
production system. We change the training of a production-quality DR model to ANCE, and evaluate
the ofﬂine gains in various corpus sizes, encoding dimensions, and exact/approximate search.
Baselines: In TREC DL, we include best runs in relevant categories and refer to Craswell et al.
(2020) for more baseline scores. We implement recent DR baselines that use the same BERT-Siamese,
but vary in negative construction: random sampling in batch (Rand Neg), random sampling from
BM25 top 100 (BM25 Neg) (Lee et al., 2019; Gao et al., 2020b) and the 1:1 combination of BM25
and Random negatives (BM25 + Rand Neg) (Karpukhin et al., 2020; Luan et al., 2020). We also
compare with contrastive learning/Noise Contrastive Estimation, which uses hardest negatives in
batch (NCE Neg) (Gutmann & Hyvärinen, 2010; Oord et al., 2018; Chen et al., 2020a). In OpenQA,
we compare with DPR, BM25, and their combinations (Karpukhin et al., 2020).
Implementation Details: In TREC DL, recent research found MARCO passage training labels
cleaner (Yan et al., 2019) and BM25 negatives can help train dense retrieval (Karpukhin et al., 2020;
Luan et al., 2020). Thus, we include a “BM25 Warm Up” setting (BM25 →∗), where the DR
models are ﬁrst trained using MARCO ofﬁcial BM25 Negatives. ANCE is also warmed up by BM25
negatives. All DR models in TREC DL are ﬁne-tuned from RoBERTa base (Liu et al., 2019). In
OpenQA, we warm up ANCE using the released DPR checkpoints (Karpukhin et al., 2020).
To ﬁt long documents in BERT-Siamese, ANCE uses the two settings from Dai & Callan (2019b),
FirstP which uses the ﬁrst 512 tokens of the document, and MaxP, where the document is split to
512-token passages (maximum 4) and the passage level scores are max-pooled. The max-pooling
operation is natively supported in ANN. The ANN search uses the Faiss IndexFlatIP Index (Johnson
et al., 2019). We use 1:1 Trainer:Inference GPU allocation, index refreshing per 10k training batches,
batch size 8, and gradient accumulation step 2 on 4 GPUs. For each positive, we uniformly sample
one negative from ANN top 200. We measured ANCE efﬁciency using a single 32GB V100 GPU, on
a cloud VM with Intel(R) Xeon(R) Platinum 8168 CPU and 650GB of RAM memory.
5



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 1):

Preprint
Query
Relevant
DR Neg
BM25 Neg
Rand Neg
Figure 1: T-SNE (Maaten & Hinton,
2008) representations of query, relevant
documents, negative training instances
from BM25 (BM25 Neg) or randomly
sampled (Rand Neg), and testing nega-
tives (DR Neg) in dense retrieval.
In this paper, we ﬁrst theoretically analyze the convergence
of dense retrieval training with negative sampling. Us-
ing the variance reduction framework (Alain et al., 2015;
Katharopoulos & Fleuret, 2018), we show that, under con-
ditions commonly met in dense retrieval, local in-batch
negatives lead to diminishing gradient norms, resulted in
high stochastic gradient variances and slow training con-
vergence — the local negative sampling is the bottleneck
of dense retrieval’s effectiveness.
Based on our analysis, we propose Approximate near-
est neighbor Negative Contrastive Estimation (ANCE),
a new contrastive representation learning mechanism for
dense retrieval. Instead of random or in-batch local neg-
atives, ANCE constructs global negatives using the being-
optimized DR model to retrieve from the entire corpus.
This fundamentally aligns the distribution of negative sam-
ples in training and of irrelevant documents to separate in
testing. From the variance reduction point of view, these ANCE negatives lift the upper bound of per
instance gradient norm, reduce the variance of the stochastic gradient estimation, and lead to faster
learning convergence.
We implement ANCE using an asynchronously updated ANN index of the corpus representation.
Similar to Guu et al. (2020), we maintain an Inferencer that parallelly computes the document
encodings with a recent checkpoint from the being optimized DR model, and refresh the ANN index
used for negative sampling once it ﬁnishes, to keep up with the model training. Our experiments
demonstrate the advantage of ANCE in three text retrieval scenarios: standard web search (Craswell
et al., 2020), OpenQA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), and in a commercial search
engine’s retrieval system. We also empirically validate our theory that the gradient norms on ANCE
sampled negatives are much bigger than local negatives and thus improve the convergence of dense
retrieval models. Our code and trained models are available at https://aka.ms/ance.
2 P RELIMINARIES
In this section, we discuss the preliminaries of dense retrieval and its representation learning.
Task Deﬁnition: Given a query qand a corpus C, the ﬁrst stage retrieval is to ﬁnd a set of documents
relevant to the query D+ = {d1,...,d i,...,d n}from C(|D+|≪| C|), which then serve as input to
later more complex models (Croft et al., 2010). Instead of using sparse term matches and inverted
index, Dense Retrieval calculates the retrieval score f() using similarities in a learned embedding
space (Lee et al., 2019; Luan et al., 2020; Karpukhin et al., 2020):
f(q,d) = sim(g(q; θ),g(d; θ)), (1)
where g() is the representation model that encodes the query or document to dense embeddings. The
encoder parameter θprovides the main capacity, often ﬁne-tuned from pretrained transformers, e.g.,
BERT (Lee et al., 2019). The similarity function (sim()) is often simply cosine or dot product, to
leverage efﬁcient ANN retrieval (Johnson et al., 2019; Guo et al., 2020).
Learning with Negative Sampling: The effectiveness of DR resides in learning a good representa-
tion space that maps query and relevant documents together, while separating irrelevant ones. The
learning of this representation often follows standard learning to rank (Liu, 2009): Given a query q, a
set of relevant document D+ and irrelevant ones D−, ﬁnd the best θ∗that:
θ∗= argminθ
∑
q
∑
d+∈D+
∑
d−∈D−
l(f(q,d+),f(q,d−)). (2)
The loss l() can be binary cross entropy (BCE), hinge loss, or negative log likelihood (NLL).
A unique challenge in dense retrieval, targeting ﬁrst stage retrieval, is that the irrelevant documents
to separate are from the entire corpus (D− = C \D+). This often leads to millions of negative
2



### Claim 15/38

#### Claim Text
We consider the following popular Chinese text embedding models as the baselines for our experiments: Text2Vec-Chinese10 base and large; Luotuo11; M3E12 base and large; multilingual E5 [53] and OpenAI text embedding ada.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 21):

Dataset LanguageLASER2 LaBSE MiniLM-L12-multilingual MPNet-multilingual SGPT-BLOOM-7.1B-msmarco
BUCC de-en99.21 99.35 97.11 98.59 54.00BUCC fr-en98.39 98.72 94.99 96.89 97.06BUCC ru-en97.62 97.78 95.06 96.44 45.30BUCC zh-en97.70 99.16 95.63 97.56 97.96Tatoeba sqi-eng97.22 96.76 98.17 98.57 10.38Tatoeba fry-eng42.07 89.31 31.13 43.54 24.62Tatoeba kur-eng19.09 83.59 46.94 61.44 8.26Tatoeba tur-eng98.03 98.00 95.08 96.17 6.15Tatoeba deu-eng99.07 99.20 97.02 97.73 70.10Tatoeba nld-eng95.35 96.07 94.58 95.50 29.74Tatoeba ron-eng96.52 96.92 95.30 96.43 27.23Tatoeba ang-eng25.22 59.28 10.24 16.72 28.76Tatoeba ido-eng80.86 89.42 40.25 43.91 43.91Tatoeba jav-eng9.95 79.77 17.04 23.39 15.02Tatoeba isl-eng94.32 94.75 24.07 59.25 6.29Tatoeba slv-eng95.40 96.03 96.92 97.08 10.14Tatoeba cym-eng5.85 92.00 13.25 22.31 6.97Tatoeba kaz-eng53.30 87.49 34.89 61.49 3.32Tatoeba est-eng96.43 96.55 97.33 98.40 4.76Tatoeba heb-eng0.00 91.53 86.88 88.26 1.69Tatoeba gla-eng1.52 85.66 3.61 4.72 2.09Tatoeba mar-eng92.93 92.65 92.38 93.83 45.53Tatoeba lat-eng64.81 80.07 19.47 24.25 28.76Tatoeba bel-eng79.54 95.00 67.73 79.94 8.03Tatoeba pms-eng36.23 64.57 30.70 34.19 31.94Tatoeba gle-eng4.20 93.80 11.62 16.85 3.26Tatoeba pes-eng93.13 94.70 92.59 93.47 12.13Tatoeba nob-eng95.77 98.40 97.73 98.53 21.07Tatoeba bul-eng93.57 94.58 92.65 93.52 20.09Tatoeba cbk-eng77.17 79.44 55.37 58.68 64.63Tatoeba hun-eng95.20 96.55 91.58 94.18 5.07Tatoeba uig-eng56.49 92.40 24.39 48.35 1.27Tatoeba rus-eng92.58 93.75 91.87 92.92 59.84Tatoeba spa-eng97.33 98.40 95.42 97.00 94.48Tatoeba hye-eng88.72 94.09 93.28 94.38 0.50Tatoeba tel-eng96.72 97.86 36.40 79.73 64.62Tatoeba afr-eng92.59 96.18 58.22 72.96 16.62Tatoeba mon-eng3.42 95.91 95.04 96.14 2.85Tatoeba arz-eng66.16 76.00 51.26 55.69 70.66Tatoeba hrv-eng96.72 96.95 95.98 97.00 12.79Tatoeba nov-eng60.02 74.38 47.99 50.23 52.23Tatoeba gsw-eng27.52 46.50 25.74 25.12 21.03Tatoeba nds-eng77.13 79.42 32.16 38.88 23.92Tatoeba ukr-eng93.52 93.97 92.82 92.67 22.06Tatoeba uzb-eng23.20 84.23 17.14 23.19 4.71Tatoeba lit-eng96.20 96.47 93.16 95.37 4.49Tatoeba ina-eng93.93 95.37 79.13 84.32 73.67Tatoeba lfn-eng63.39 67.54 47.02 49.56 44.85Tatoeba zsm-eng95.41 95.62 95.31 95.80 79.95Tatoeba ita-eng94.32 92.72 93.05 93.76 65.04Tatoeba cmn-eng85.62 95.10 94.93 95.83 91.45Tatoeba lvs-eng95.33 95.88 97.87 97.53 6.55Tatoeba glg-eng96.14 96.82 94.00 95.32 79.86Tatoeba ceb-eng9.93 64.42 8.05 7.39 6.64Tatoeba bre-eng31.2 15.07 5.56 6.42 4.67Tatoeba ben-eng89.43 88.55 36.48 64.90 75.98Tatoeba swg-eng33.10 59.36 26.31 22.80 16.89Tatoeba arq-eng26.63 42.69 18.60 19.84 27.75Tatoeba kab-eng65.88 4.31 1.16 1.41 1.69Tatoeba fra-eng94.28 94.86 91.72 93.12 91.44Tatoeba por-eng94.54 94.14 92.13 93.02 92.62Tatoeba tat-eng34.74 85.92 10.25 10.89 3.59Tatoeba oci-eng58.13 65.81 38.57 43.49 40.17Tatoeba pol-eng97.32 97.22 94.28 96.95 14.09Tatoeba war-eng8.25 60.29 7.25 7.42 10.38Tatoeba aze-eng82.41 94.93 62.10 76.36 6.32Tatoeba vie-eng96.73 97.20 95.12 97.23 94.20Tatoeba nno-eng72.75 94.48 76.34 81.41 16.28Tatoeba cha-eng14.86 31.77 15.98 12.59 23.26Tatoeba mhr-eng6.86 15.74 6.89 7.57 1.56Tatoeba dan-eng95.22 95.71 94.80 96.17 23.52Tatoeba ell-eng96.20 95.35 95.43 94.93 5.34Tatoeba amh-eng80.82 91.47 36.21 53.49 0.03Tatoeba pam-eng3.24 10.73 5.41 5.39 5.85Tatoeba hsb-eng45.75 67.11 36.10 44.32 9.68Tatoeba srp-eng93.64 94.43 92.24 94.12 11.69Tatoeba epo-eng96.61 98.20 41.73 55.12 26.20Tatoeba kzj-eng4.46 11.33 6.24 5.88 5.17Tatoeba awa-eng33.74 71.70 33.43 42.83 35.01Tatoeba fao-eng57.04 87.40 27.51 38.24 12.61Tatoeba mal-eng98.16 98.45 32.20 88.46 83.30Tatoeba ile-eng87.88 85.58 57.71 60.36 59.59Tatoeba bos-eng95.86 94.92 93.27 94.02 13.65Tatoeba cor-eng4.45 10.11 3.42 3.53 2.83Tatoeba cat-eng95.80 95.38 94.42 96.05 88.31Tatoeba eus-eng93.32 95.01 23.18 31.33 53.38Tatoeba yue-eng87.75 89.58 71.45 77.58 77.03Tatoeba swe-eng95.31 95.63 94.42 95.45 19.53Tatoeba dtp-eng7.39 10.85 5.69 5.03 3.41Tatoeba kat-eng81.16 95.02 95.44 95.46 0.42Tatoeba jpn-eng93.78 95.38 90.41 92.51 71.36Tatoeba csb-eng27.03 52.57 21.56 23.73 10.03Tatoeba xho-eng4.68 91.55 4.52 6.53 5.51Tatoeba orv-eng23.24 38.93 15.10 23.77 5.79Tatoeba ind-eng92.98 93.66 92.74 93.50 88.04Tatoeba tuk-eng16.35 75.27 15.16 14.91 5.48Tatoeba max-eng36.96 63.26 45.25 48.77 36.14Tatoeba swh-eng55.66 84.50 14.48 16.02 16.74Tatoeba hin-eng95.32 96.87 97.62 97.75 85.23Tatoeba dsb-eng42.34 64.81 33.43 36.85 8.78Tatoeba ber-eng77.63 8.40 4.43 4.88 4.92Tatoeba tam-eng87.32 89.0 24.64 73.60 72.76Tatoeba slk-eng95.82 96.5 95.15 96.62 9.98Tatoeba tgl-eng63.19 96.02 13.09 17.67 10.70Tatoeba ast-eng76.35 90.68 62.17 70.08 71.13Tatoeba mkd-eng93.63 93.6 91.00 93.02 10.47Tatoeba khm-eng74.19 78.37 32.11 58.80 0.37Tatoeba ces-eng95.52 96.68 95.12 95.73 9.55Tatoeba tzl-eng36.56 58.88 25.46 34.21 27.82Tatoeba urd-eng84.23 93.22 94.57 95.12 70.10Tatoeba ara-eng90.14 88.80 87.93 90.19 85.37Tatoeba kor-eng87.97 90.95 92.52 93.07 22.39Tatoeba yid-eng2.49 88.79 14.38 30.73 0.16Tatoeba ﬁn-eng96.98 96.37 93.10 95.92 3.41Tatoeba tha-eng96.38 96.14 96.72 95.99 2.22Tatoeba wuu-eng75.09 90.18 76.00 78.25 79.58
Average mix67.42 81.75 57.98 63.38 31.08
Table 12: Multilingual bitext mining results. Scores are f1.



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 0):

Text Embeddings by Weakly-Supervised
Contrastive Pre-training
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao
Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei
Microsoft Corporation
https://github.com/microsoft/unilm
Abstract
This paper presents E5 1, a family of state-of-the-art text embeddings that transfer
well to a wide range of tasks. The model is trained in a contrastive manner with
weak supervision signals from our curated large-scale text pair dataset (called
CCPairs). E5 can be readily used as a general-purpose embedding model for any
tasks requiring a single-vector representation of texts such as retrieval, clustering,
and classification, achieving strong performance in both zero-shot and fine-tuned
settings. We conduct extensive evaluations on 56 datasets from the BEIR and
MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms
the strong BM25 baseline on the BEIR retrieval benchmark without using any
labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark,
beating existing embedding models with 40× more parameters.
1 Introduction
Text embeddings are low-dimensional vector representations for arbitrary-length texts and play key
roles in many NLP tasks such as large-scale retrieval. Compared to the high-dimensional and sparse
representations like TF-IDF, text embeddings have the potential to overcome the lexical mismatch
issue and facilitate efficient retrieval and matching between texts. It also offers a versatile interface
easily consumable by downstream applications.
While pre-trained language models such as BERT [ 17] and GPT [ 7] can produce transferrable
text representations, they are not ideal for tasks such as retrieval and text matching where a single-
vector embedding of texts is more desired due to its efficiency and versatility. To obtain better
text embeddings, contrastive learning is often the go-to framework to enhance the sequence-level
representations from text pairs. Along this line of research, some works are geared towards learning
task-specific embeddings. For example, GTR [ 43] and Sentence-T5 [ 44] fine-tune pre-trained
models with supervised datasets to learn embeddings customized for passage retrieval and semantic
textual similarity, respectively. Other works learn unsupervised embeddings from automatically
constructed text pairs. Typical methods to construct text pairs include Inverse Close Task (ICT)
[9], random cropping [ 28] and neighboring text spans [ 41], etc. While such synthetic data are
of unlimited quantity, they are often poor in quality and the resulted embeddings fail to match the
performance of the classic BM25 baseline without further fine-tuning [40].
In this work, we learn a high-quality general-purpose text embedding termed E5, EmbEddings from
bidirEctional Encoder rEpresentations. E5 aims to provide strong off-the-shelf text embeddings
suitable for any tasks requiring single-vector representations in both zero-shot or fine-tuned settings.
To achieve this goal, instead of relying on limited labeled data or low-quality synthetic text pairs, we
contrastively train E5 embeddings from CCPairs, a curated web-scale text pair dataset containing
1E5: EmbEddings from bidirEctional Encoder rEpresentations
Work in progress.
arXiv:2212.03533v2  [cs.CL]  22 Feb 2024



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 7):

Table 5: Impacts of different batch sizes for contrastive pre-training.
batch size NFCorpus NQ FiQA Quora DBPedia Scifact Avg
32k 35.8 39.0 40.0 85.7 35.4 73.7 51.6
8k 33.3 38.5 37.6 85.7 34.0 71.8 50.2
1k 28.2 33.1 30.4 84.0 30.1 69.1 45.8
Table 5, increasing batch size from 1K to 32K leads to consistent gains across all 6 datasets. It is also
possible to train with smaller batch sizes by adding hard negatives [ 50]. However, the engineering
efforts of mining hard negatives for large datasets (>100M) are non-trivial.
Table 6: Fine-tuning with different combinations of labeled data.
Fine-tuned on Retrieval STS Classification Summ. MTEB Avg
No fine-tuning 42.9 69.5 67.9 31.1 55.6
MS-MARCO + NQ 50.3 78.3 68.3 30.6 59.0
NLI 38.3 81.1 72.6 31.6 57.3
All above 48.7 81.0 73.1 31.0 60.4
Fine-tuning Datasets GTR models are fine-tuned with “MS-MARCO + NQ”, while Sentence-T5
models use NLI instead. In Table 6, we can see that the “MS-MARCO + NQ” setting performs best
on retrieval tasks, and the NLI data is beneficial for STS and linear probing classification. Similar
observations are also made by Muennighoff et al. [40]. Combining all of them leads to the best
overall scores on the MTEB benchmark. This also illustrates the importance of dataset diversity for
learning text embeddings.
Table 7: Data filtering. For the top 2 rows, we train with 1M random text pairs.
# of pairs NFCorpus NQ FiQA Quora DBPedia Scifact Avg
1M w/o filter 23.0 15.1 18.5 83.1 18.2 51.4 34.9
w/ filter 26.8 22.7 24.5 85.0 27.5 57.5 40.7
All w/o filter 34.5 35.4 39.1 85.7 32.9 72.5 50.0
w/ filter 35.8 39.0 40.0 85.7 35.4 73.7 51.6
Data Filtering One crucial step in our dataset curation pipeline is filtering out low-quality text pairs.
In Table 7, when training with 1M pairs, using filtered data has a nearly 6 points advantage. When
all the text pairs are used, the “w/o filter” setting has about 4× more data but is still behind by 1.6
points. Though recent studies [ 29, 47] show that deep learning models are quite robust to dataset
noises, data filtering still has benefits in improving training efficiency and model quality.
Negative Sampling We explore two alternative methods to enlarge the number of negatives: Pre-
batch negatives [ 33] reuse embeddings from previous batches as additional negatives, while MoCo
[25] introduces a momentum encoder and uses a FIFO queue to store negatives. For both approaches,
the negative size can be easily scaled up without incurring much GPU memory overhead. The
downside is that most negatives are produced by an older version of model parameters. In Table 8,
in-batch negatives still perform favorably. Empirically, we find that MoCo is more sensitive to certain
hyperparameters such as temperature, better results are possible with more tuning.
BM25 vs Dense Retrieval With the rapid development of dense retrieval models, can we replace
the long-standing BM25 algorithm from now on? The answer is likely “not yet”. BM25 still holds
obvious advantages in terms of simplicity, efficiency, and interpretability. For long-tail domains such
as Trec-Covid [ 55] and retrieval tasks that involve long documents (Touche-2020) [4] or rely heavily
on exact lexical match (Fever) [ 54], further research efforts are still necessary to improve current
dense retrievers.
6 Conclusion
In this work, we train a general-purpose text embedding model E5 from weak supervision signals. We
adopt a simple contrastive training framework with in-batch negatives and learn from a large-scale text
pair dataset we harvest from heterogeneous data sources across the web. E5 offers strong off-the-shelf
8



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 23):

Dataset LanguageKomninos LASER2 LaBSE MiniLM-L12-multilingual MPNet-multilingual SGPT-BLOOM-7.1B-msmarco
STS17 ko-ko 2.54 70.52 71.32 77.03 83.41 66.89
STS17 ar-ar 13.78 67.47 69.07 79.16 79.10 76.42
STS17 en-ar 9.08 65.05 74.51 81.22 80.85 78.07
STS17 en-de -3.11 66.66 73.85 84.22 83.28 59.10
STS17 en-tr -0.45 70.05 72.07 76.74 74.90 11.80
STS17 es-en -8.18 55.30 65.71 84.44 86.11 78.22
STS17 es-es 48.23 79.67 80.83 85.56 85.14 86.00
STS17 fr-en 5.81 70.82 76.98 76.59 81.17 80.46
STS17 it-en 3.64 70.98 76.99 82.35 84.24 51.58
STS17 nl-en -0.44 68.12 75.22 81.71 82.51 45.85
STS22 de 33.04 25.69 48.58 44.64 46.70 30.05
STS22 es 48.53 54.92 63.18 56.56 59.91 65.41
STS22 pl 12.47 18.34 39.30 33.74 33.65 31.13
STS22 tr 47.38 36.97 58.15 53.39 56.30 47.14
STS22 ar 32.42 42.57 57.67 46.2 52.19 58.67
STS22 ru 19.44 39.24 57.49 57.08 58.74 43.36
STS22 zh 4.78 49.41 63.02 58.75 61.75 66.78
STS22 fr 49.43 58.61 77.95 70.55 74.30 80.38
STS22 de-en 28.65 32.35 50.14 52.65 50.81 51.16
STS22 es-en 26.97 54.34 71.86 67.33 70.26 75.06
STS22 it 57.77 60.31 72.22 55.22 60.65 65.65
STS22 pl-en 45.55 53.63 69.41 69.02 73.07 53.31
STS22 zh-en 14.05 46.19 64.02 65.71 67.96 68.45
STS22 es-it 41.10 42.21 69.69 47.67 53.70 65.50
STS22 de-fr 14.77 37.41 53.28 51.73 62.34 53.28
STS22 de-pl 11.21 15.67 58.69 44.22 40.53 43.05
STS22 fr-pl 39.44 39.44 61.98 50.71 84.52 28.17
Average mix 22.14 51.55 65.67 64.23 67.71 57.81
Table 14: Multilingual STS Results. Scores are Spearman correlations of cosine similarities.



Source: data\tc18_2309.07597v5\referenced_papers\[22]_2112.09118.pdf (Page 19):

Published in Transactions on Machine Learning Research (08/2022)
Table 12:List of languages used for multilingual retrieval.
ar bn da de en es
Language Arabic Bengali Danish German English Spanish
Pre-training      
Mr. TyDi      
MKQA      
ﬁ fr he hu it id
Language Finnish French Hebrew Hungarian Italian Indonesian
Pre-training      
Mr. TyDi      
MKQA      
ja km ko ms nl no
Language Japanese Khmer Korean Malay Dutch Norwegian
Pre-training      
Mr. TyDi      
MKQA      
pl pt ru sv sw te
Language Polish Portugese Russian Swedish Swahili Telugu
Pre-training      
Mr. TyDi      
MKQA      
th tr vi zh-cn zh-hk zh-tw
Language Thai Turkish Vietnamese Chinese (Simpliﬁed) Chinese (Hong Kong) Chinese (Traditional)
Pre-training      
Mr. TyDi      
MKQA      
Table 13:Recall@100 on MKQA for cross-lingual retrievalin the setting described in Section 5.3.
avg en ar ﬁ ja ko ru es sv he th da de fr
CORA 59.8 75.6 44.5 61.3 47.0 45.5 58.6 69.2 68.0 48.3 44.4 68.9 68.1 70.2
mBERT + MS MARCO 57.9 74.2 44.0 51.7 55.7 48.2 57.4 63.9 62.7 46.8 51.7 63.7 59.6 65.2
XLM-R + MS MARCO 59.2 73.4 42.4 57.7 53.1 48.6 58.5 62.9 67.5 46.9 61.5 66.9 60.9 62.4
Contriever 49.2 65.3 43.0 43.1 47.1 44.8 51.8 37.2 54.5 44.7 51.4 49.3 49.0 50.2
+ MS MARCO 65.6 75.6 53.3 66.6 60.4 55.4 64.7 70.0 70.8 59.6 63.5 72.0 66.6 70.1
it nl pl pt hu vi ms km no tr zh-cn zh-hk zh-tw
CORA 68.3 72.0 65.6 67.9 59.5 61.2 67.9 35.6 68.3 61.5 52.0 52.8 52.8
mBERT + MS MARCO 64.1 66.7 59.0 61.9 57.5 58.6 62.8 32.9 63.2 56.0 58.4 59.3 59.3
XLM-R + MS MARCO 58.1 66.4 61.0 62.0 60.1 62.4 66.1 46.6 65.9 60.6 55.8 55.5 55.7
Contriever 56.7 61.7 44.4 54.5 47.7 45.1 56.7 27.8 50.2 44.3 54.3 51.9 52.5
+ MS MARCO 70.3 71.4 68.8 68.5 66.7 67.8 71.6 37.8 71.5 68.7 64.1 64.5 64.3
Table 14:Recall@20 on MKQA for cross-lingual retrievalin the setting described in Section 5.3.
avg en ar ﬁ ja ko ru es sv he th da de fr
CORA 49.0 68.5 31.7 49.7 34.1 33.1 46.5 60.3 58.1 36.8 33.6 59.4 58.5 61.6
mBERT + MS MARCO 45.3 65.5 30.2 38.9 41.7 34.5 44.3 52.4 50.5 32.6 38.5 52.5 46.6 53.8
XLM-R + MS MARCO 46.7 64.5 29.0 45.1 39.7 34.9 45.9 51.4 56.1 32.5 49.4 55.8 48.3 50.5
Contriever 31.4 50.2 26.6 26.7 29.4 27.9 32.7 20.7 37.6 22.2 31.1 31.2 31.2 30.7
+ MS MARCO 53.9 67.2 40.1 55.1 46.2 41.7 52.3 59.3 60.0 45.6 52.0 62.0 54.8 59.3
it nl pl pt hu vi ms km no tr zh-cn zh-hk zh-tw
CORA 58.2 63.5 54.3 58.4 47.6 49.8 57.6 24.8 58.8 49.1 38.6 40.5 39.6
mBERT + MS MARCO 52.1 55.3 45.6 49.5 44.6 46.9 49.9 21.5 51.3 42.7 44.6 45.3 45.5
XLM-R + MS MARCO 45.4 54.5 48.5 49.6 47.3 49.7 54.0 33.4 53.7 48.7 42.4 42.4 42.0
Contriever 38.6 45.1 25.1 37.6 28.3 27.3 39.6 15.7 33.2 26.5 35.0 32.7 32.5
+ MS MARCO 59.4 60.9 58.1 56.9 55.2 55.9 60.9 26.2 61.0 56.7 50.9 51.9 51.2
20



### Claim 16/38

#### Claim Text
At the time of public release, our English BGE models achieved the state-of-the-art performance on the English MTEB benchmark [35] across its 56 datasets.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 8):

4.4 Multilinguality
MTEB comes with 10 multilingual datasets across
bitext mining, classiﬁcation and STS tasks. We in-
vestigate performance on these in Figure 5. Tabular
results can be found in Tables 12, 13 and 14.
Bitext Mining LaBSE (Feng et al., 2020) per-
forms strongly across a wide array of languages in
bitext mining. Meanwhile, LASER2 shows high
variance across different languages. While there
are additional language-speciﬁc LASER2 models
available for some of the languages we benchmark,
we use the default multilingual LASER2 model
for all languages. This is to provide a fair one-to-
one comparison of models. In practice, however,
the high variance of LASER2’s performance may
be resolved by mixing its model variants. MP-
Net, MiniLM and SGPT-BLOOM-7B1-msmarco
perform poorly on languages they have not been
pre-trained on, such as German for the latter.
Classiﬁcation & STS On multilingual classiﬁ-
cation and STS, the multilingual MPNet provides
the overall strongest performance. It outperforms
the slightly faster multilingual MiniLM on almost
all languages. Both models have been trained
on the same languages, thus bringing decision-
making down to performance vs speed. SGPT-
BLOOM-7B1-msmarco provides state-of-the-art
performance on languages like Hindi, Portuguese,
Chinese or French, which the model has seen ex-
tensively during pre-training. It also performs com-
petitively on languages like Russian or Japanese
that unintentionally leaked into its pre-training
data (Muennighoff et al., 2022). However, it is not
much ahead of the much cheaper MPNet. LASER2
performs consistently worse than other models.
5 Conclusion
In this work, we presented the Massive Text Em-
bedding Benchmark (MTEB). Consisting of 8 text
embedding tasks with up to 15 datasets each and
covering 112 languages, MTEB aims to provide re-
liable embedding performance estimates. By open-
sourcing MTEB alongside a leaderboard, we pro-
vide a foundation for further pushing the state-of-
the-art of available text embeddings.
To introduce MTEB, we have conducted the
most comprehensive benchmarking of text embed-
dings to date. Through the course of close to 5,000
experiments on over 30 different models, we have
set up solid baselines for future research to build
on. We found model performance on different tasks
to vary strongly with no model claiming state-of-
the-art on all tasks. Our studies on scaling behav-
ior, model efﬁciency and multilinguality revealed
various intricacies of models that should ease the
decision-making process for future research or in-
dustry applications of text embeddings.
We welcome task, dataset or metric contributions
to the MTEB codebase7 as well as additions to the
leaderboard via our automatic submission format8.
7https://github.com/embeddings-benchm
ark/mteb
8https://huggingface.co/spaces/mteb/l
eaderboard



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 16):

STSBenchmark are monolingual english bench-
marks. STS17 and STS22 contain crosslingual
pairs of sentences, where the goal is to assess the
similarity of two sentences in different languages.
STS17 has 11 language pairs (among Korean, Ara-
bic, English, French, German, Turkish, Spanish,
Italian and Dutch) and STS22 has 18 language pairs
(among Arabic, English, French, German, Turkish,
Spanish, Polish, Italian, Russian and Chinese).
BIOSSES21 Contains 100 sentence pairs from
the biomedical ﬁeld.
SICK-R (Agirre et al., 2014) Sentences Involv-
ing Compositional Knowledge (SICK) contains a
large number of sentence pairs (10 0000) that are
lexically, syntactically and semantically rich.
A.7 Summarization
SummEval (Fabbri et al., 2020) Summaries gen-
erated by recent summarization models trained on
CNN or DailyMail alongside human annotations.
A.8 Retrieval
We refer to the BEIR paper (Thakur et al., 2021),
which contains description of each dataset. For
MTEB, we include all publicly available datasets:
ArguAna, ClimateFEVER, CQADupstack, DB-
Pedia, FEVER, FiQA2018, HotpotQA, MS-
MARCO, NFCorpus, NQ, Quora, SCIDOCS,
SciFact, Touche2020, TRECCOVID.
B Limitations of MTEB
While MTEB aims to be a diverse benchmark to
provide holistic performance reviews, the bench-
mark has its limitations. We list them here:
1. Long document datasets MTEB covers mul-
tiple text lengths (S2S, P2P, S2P), but very long
documents are still missing. The longest datasets in
MTEB have a few hundred words, and longer text
sizes could be relevant for use cases like retrieval.
2. Task imbalance Tasks in MTEB have a differ-
ent amount of datasets with summarization consist-
ing of only a single dataset. This means MTEB av-
erage scores, which are computed over all datasets,
are biased towards tasks with many datasets, no-
tably retrieval, classiﬁcation and clustering. As
MTEB grows, we hope to add more datasets to cur-
rently underrepresented tasks like summarization
or pair classiﬁcation.
21https://tabilab.cmpe.boun.edu.tr/BIO
SSES/DataSet.html
3. Multinguality MTEB contains multilingual
classiﬁcation, STS and bitext mining datasets.
However, retrieval and clustering are English-only.
SGPT-BLOOM-7B1-msmarco is geared towards
multilingual retrieval datasets and due to the lack
thereof cannot be comprehensively benchmarked
in MTEB. Further, MTEB does not contain any
code datasets that could be used to benchmark code
models (Neelakantan et al., 2022; Allal et al., 2023).
It should be easy to extend MTEB with datasets,
such as CodeSearchNet (Husain et al., 2019), TyDI
QA (Clark et al., 2020), XOR QA (Asai et al., 2020)
or MIRACL (Zhang et al., 2022).
4. Additional modalities Text embeddings are
commonly used as input features for downstream
models, such as in our classiﬁcation task. This
can involve other modalities, notably image con-
tent (Carvalho et al., 2018; Tan and Bansal, 2019;
Muennighoff, 2020; Nichol et al., 2021; Saharia
et al., 2022; Weinbach et al., 2022). We have fo-
cused solely on natural language applications and
leave extensive benchmarking of text embeddings
as inputs for other modalities to future work.
C Examples
Tables 3-9 provide examples for each dataset for
each task. For retrieval datasets, we refer to the
BEIR paper (Thakur et al., 2021).
D Correlations
Figure 6 provides correlation heatmaps for model
performance and MTEB tasks.
E Models
Table 10 provides publicly available model check-
points used for MTEB evaluation.
F Additional results
Tables 11 until the end provide results on individ-
ual datasets of MTEB. The results are additionally
available in json format on the Hugging Face Hub22
and can be inspected on the leaderboard23.
22https://huggingface.co/datasets/mteb
/results
23https://huggingface.co/spaces/mteb/l
eaderboard



Source: data\tc18_2309.07597v5\referenced_papers\[28]_2308.03281.pdf (Page 7):

Params Class. Clust. Pair. Rerank Retr. STS Summ. Avg
# of datasets → 12 11 3 4 15 10 1 56
Unsupervised models
Glove 120M 57.3 27.7 70.9 43.3 21.6 61.9 28.9 42.0
BERT 110M 61.7 30.1 56.3 43.4 10.6 54.4 29.8 38.3
SimCSE 110M 62.5 29.0 70.3 46.5 20.3 74.3 31.2 45.5
E5small 30M 67.0 41.7 78.2 53.1 40.8 68.8 25.2 54.2
E5base 110M 67.9 43.4 79.2 53.5 42.9 69.5 24.3 55.5
E5large 330M 69.0 44.3 80.3 54.4 44.2 69.9 24.8 56.4
GTEsmall 30M 71.0 44.9 82.4 57.5 43.4 77.2 30.4 58.5
GTEbase 110M 71.5 46.0 83.3 58.4 44.2 76.5 29.5 59.0
GTElarge 330M 71.8 46.4 83.3 58.8 44.6 76.3 30.1 59.3
Supervised models
SimCSE 110M 67.3 33.4 73.7 47.5 21.8 79.1 23.3 48.7
Contriever 110M 66.7 41.1 82.5 53.1 41.9 76.5 30.4 56.0
GTRlarge 330M 67.1 41.6 85.3 55.4 47.4 78.2 29.5 58.3
Sentence-T5large 330M 72.3 41.7 85.0 54.0 36.7 81.8 29.6 57.1
E5small 30M 71.7 39.5 85.1 54.5 46.0 80.9 31.4 58.9
E5base 110M 72.6 42.1 85.1 55.7 48.7 81.0 31.0 60.4
E5large 330M 73.1 43.3 85.9 56.5 50.0 82.1 31.0 61.4
InstructORbase 110M 72.6 42.1 85.1 55.7 48.8 81.0 31.0 60.4
InstructORlarge 330M 73.9 45.3 85.9 57.5 47.6 83.2 31.8 61.6
OpenAIada-001 n.a. 70.4 37.5 76.9 49.0 18.4 78.6 26.9 49.5
OpenAIada-002 n.a. 70.9 45.9 84.9 56.3 49.3 81.0 30.8 61.0
GTEsmall 30M 72.3 44.9 83.5 57.7 49.5 82.1 30.4 61.4
GTEbase 110M 73.0 46.1 84.3 58.6 51.2 82.3 30.7 62.4
GTElarge 330M 73.3 46.8 85.0 59.1 52.2 83.4 31.7 63.1
Larger models
InstructORxl 1.5B 73.1 44.7 86.6 57.3 49.3 83.1 32.3 61.8
GTRxxl 4.5B 67.4 42.4 86.1 56.7 48.5 78.4 30.6 59.0
Sentence-T5xxl 4.5B 73.4 43.7 85.1 56.4 42.2 82.6 30.1 59.5
Table 6: Results on the MTEB (Muennighoff et al., 2023) (56 datasets in English subset). Compared models include
SimCSE (Gao et al., 2021), Sentence-T5 (Ni et al., 2022a), GTR (Ni et al., 2022b), Contriever (Izacard et al., 2022a),
OpenAI text embedding API (Neelakantan et al., 2022), E5 (Wang et al., 2022b) and InstructOR (Su et al., 2023).
Exact parameter amount of OpenAI ada model is not available, but is suspected to be ∼300M, comparable to the
BERT large size model.
by a large margin despite using a modest model
size. GTE small is comparable to E5 large while be-
ing 10× smaller. GTE large establishes new state-
of-the-art performance on the MTEB benchmark,
outperforming the multi-task instruction-finetuned
embedding model, InstructOR large, by 1.5 points
on average.
4.4 Code Search
Programming languages can be regarded as a dis-
tinct form of text. To assess the effectiveness of our
approach in code search, we conduct a comparative
analysis with other code-based language models,
such as CodeBERT (Guo et al., 2021) and Graph-
CodeBERT (Guo et al., 2021). We also compare
our approach with a more recent code language
model called UniXcoder (Guo et al., 2022), which
aims to integrate various pre-training tasks into a
unified model. CodeRetriever (Li et al., 2022) is ini-
tialized from GraphCodeBERT and pre-trained on
large-scale multi-modal code-text pairs mined and
cleaned by heuristics. It is important to note that
while the baseline models are individually trained
and evaluated for each programming language, our
model is directly evaluated across all the languages.
In line with recent work (Guo et al., 2021, 2022;



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 1):

inating different tasks. Our benchmarking sheds
light on the weaknesses and strengths of individual
models, such as SimCSE’s (Gao et al., 2021b) low
performance on clustering and retrieval despite its
strong performance on STS. We hope our work
makes selecting the right embedding model easier
and simpliﬁes future embedding research.
2 Related Work
2.1 Benchmarks
Benchmarks, such as (Super)GLUE (Wang et al.,
2018, 2019) or Big-BENCH (Srivastava et al.,
2022), and evaluation frameworks (Gao et al.,
2021a) play a key role in driving NLP progress.
Yearly released SemEval datasets (Agirre et al.,
2012, 2013, 2014, 2015, 2016) are commonly used
as the go-to benchmark for text embeddings. Se-
mEval datasets correspond to the task of semantic
textual similarity (STS) requiring models to embed
similar sentences with geometrically close embed-
dings. Due to the limited expressivity of a single Se-
mEval dataset, SentEval (Conneau and Kiela, 2018)
aggregates multiple STS datasets. SentEval focuses
on ﬁne-tuning classiﬁers on top of embeddings. It
lacks tasks like retrieval or clustering, where em-
beddings are directly compared without additional
classiﬁers. Further, the toolkit was proposed in
2018 and thus does not provide easy support for
recent trends like text embeddings from transform-
ers (Reimers and Gurevych, 2019). Due to the
insufﬁciency of STS benchmarking, USEB (Wang
et al., 2021) was introduced consisting mostly of
reranking tasks. Consequently, it does not cover
tasks like retrieval or classiﬁcation. Meanwhile, the
recently released BEIR Benchmark (Thakur et al.,
2021) has become the standard for the evaluation
of embeddings for zero-shot information retrieval.
MTEB uniﬁes datasets from different embed-
ding tasks into a common, accessible evaluation
framework. MTEB incorporates SemEval datasets
(STS11 - STS22) and BEIR alongside a variety of
other datasets from various tasks to provide a holis-
tic performance review of text embedding models.
2.2 Embedding Models
Text embedding models like Glove (Pennington
et al., 2014) lack context awareness and are thus
commonly labeled as Word Embedding Models.
They consist of a layer mapping each input word
to a vector often followed by an averaging layer to
provide a ﬁnal embedding invariant of input length.
Transformers (Vaswani et al., 2017) inject context
awareness into language models via self-attention
and form the foundation of most recent embed-
ding models. BERT (Devlin et al., 2018) uses the
transformer architecture and performs large-scale
self-supervised pre-training. The resulting model
can directly be used to produce text embeddings
via an averaging operation alike Glove. Build-
ing on InferSent (Conneau et al., 2017), SBERT
(Reimers and Gurevych, 2019) demonstrated it to
be beneﬁcial to perform additional ﬁne-tuning of
the transformer for competitive embedding perfor-
mance. Most recent ﬁne-tuned embedding models
use a contrastive loss objective to perform super-
vised ﬁne-tuning on positive and negative text pairs
(Gao et al., 2021b; Wang et al., 2021; Ni et al.,
2021b; Muennighoff, 2022). Due to the large va-
riety of available pre-trained transformers (Wolf
et al., 2020), there is an at least equally large va-
riety of potential text embedding models to be ex-
plored. This leads to confusion about which model
provides practitioners with the best performance
for their embedding use case.
We benchmark both word embedding and trans-
former models on MTEB quantifying gains pro-
vided by often much slower context aware models.
3 The MTEB Benchmark
3.1 Desiderata
MTEB is built on a set of desiderata: (a) Diversity:
MTEB aims to provide an understanding of the
usability of embedding models in various use cases.
The benchmark comprises 8 different tasks, with
up to 15 datasets each. Of the 58 total datasets in
MTEB, 10 are multilingual, covering 112 differ-
ent languages. Sentence-level and paragraph-level
datasets are included to contrast performance on
short and long texts. (b) Simplicity: MTEB pro-
vides a simple API for plugging in any model that
given a list of texts can produce a vector for each
list item with a consistent shape. This makes it
possible to benchmark a diverse set of models. (c)
Extensibility: New datasets for existing tasks can
be benchmarked in MTEB via a single ﬁle that
speciﬁes the task and a Hugging Face dataset name
where the data has been uploaded (Lhoest et al.,
2021). New tasks require implementing a task in-
terface for loading the data and an evaluator for
benchmarking. We welcome dataset, task or metric
contributions from the community via pull requests
to continue the development of MTEB. (d) Repro-



Source: data\tc18_2309.07597v5\referenced_papers\[28]_2308.03281.pdf (Page 5):

Model Params LR GPUs BS Base LM
GTEsmall 30M 3 × 10−4 2 16384 microsoft/MiniLM-L12-H384-uncased
GTEbase 110M 2 × 10−4 4 16384 bert-base-uncased
GTElarge 330M 5 × 10−5 8 16384 bert-large-uncased
Table 3: Pre-training configurations of models of different sizes.
4 Experiments
In this section, we provide an extensive evaluation
of our embedding model, comparing to state-of-
the-art models for each task. Note that an apple-to-
apple comparison is hardly possible since different
models used different in-house data for pre-training
and the base language models vary a lot. We mainly
use the number of model parameters as a criterion
for performance comparison since it is closely re-
lated to the inference speed.
4.1 Zero-shot Text Classification
Model Params Prompting Accuracy
E5base 110M ✓ 81.3
E5large 330M ✓ 85.3
cpt-text 6B 88.1
cpt-text 6B ✓ 89.1
GTEbase 110M 85.1
GTEbase 110M ✓ 87.2
Table 4: Zero shot text classification performance on
SST-2. All compared models are the fine-tuned ones.
One method to assess the quality of learned
representation is through zero-shot classifica-
tion. (Radford et al., 2021; Neelakantan et al., 2022;
Wang et al., 2022b). We recast text classification
into an embedding-based similarity matching prob-
lem. In this setting, inputs texts are converted into
embeddings directly and labels are verbalized to
corresponding text to get label embeddings. Dis-
tances between input embeddings and label embed-
dings are measured by their inner product and label
with the most close embedding distance to the in-
put text is regarded as the classification result. An
example is SST-2 binary sentiment classification
task. We consider two types of label verbalizers
for evaluation. The vanilla version uses the sen-
timent word ‘positive’ or ‘negative’ to denote the
corresponding labels. Prompted version uses fuzzy
prompt template, such as ‘this is an example of
positive/negative movie review’.
Zero-shot text classification accuracy on SST-
2 is shown in Table 4. In the vanilla setting, our
110M model already matches the performance of
prompted E5large with 330M parameters. Using
prompting strategy further improves results signifi-
cantly and closes the gap with large models. Even
without explicit prompt or instruction during train-
ing, our model can somewhat understand the label
context better when formatted as a natural language
text.
4.2 Unsupervised Text Retrieval
Text retrieval requires retrieving most relevant doc-
uments from a large-scale candidate sets. We
use BEIR (Thakur et al., 2021) as our evalua-
tion benchmark for zero-shot unsupervised text
retrieval. BEIR is a heterogeneous information re-
trieval benchmark which contains retrieval tasks of
different formats and from different domains. We
use the open available 15 datasets for evaluation.
We compare our unsupervised pre-trained check-
point to recent unsupervised dense retrievers such
as Contriever (Izacard et al., 2022a) and E5 (Wang
et al., 2022b). According to Table 5, we find that
our base size model significantly outperforms the
models with comparable size, like SimCSE, Con-
triever and E5. Our base model is comparable to
E5large without using human supervision.
4.3 Massive Text Embedding Benchmark
Massive Text Embedding Benchmark (MTEB) is a
comprehensive semi-supervised benchmark that in-
corporates a limited amount of supervision data for
evaluation. In this paper, we evaluate the English
subsets which encompasses 56 English datasets
across seven distinct tasks, including text classi-
fication (Class.), text clustering (Clust.), pairwise
classification (Pair.), text reranking (Rerank.), text
retreival (Retr.), semantic textual similarity (STS)
and summarization (Summ.). The evaluation met-
rics employed in MTEB are accuracy, v-measure,
average precision, MAP, nDCG@10, and Spear-
man coefficients, respectively. For further details



### Claim 17/38

#### Claim Text
Although there were many strong competitors in the English community, such as E5 [53], SGPT [32], GTE [28], GTR [40], and OpenAI Ada-002 [37], we were able to notably advance the prior SOTA by an absolute 1.1 points in total average, which further verify the effectiveness of our data curation and training method. 4.2 Detailed Analysis We investigate the detailed impact of C-MTP and our training recipe.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 5):

0.1B 1B 2B 4B
Model Parameters (Billions)
0.62
0.64
0.66
0.68
0.70
0.72
0.74Average Performance (accuracy)
Classification
0.1B 1B 2B 4B
Model Parameters (Billions)
0.36
0.37
0.38
0.39
0.40
0.41
0.42
0.43
0.44Average Performance (v_measure)
Clustering
0.1B 1B 2B 4B
Model Parameters (Billions)
0.76
0.78
0.80
0.82
0.84
0.86Average Performance (ap)
PairClassification
0.1B 1B 2B 4B
Model Parameters (Billions)
0.51
0.52
0.53
0.54
0.55
0.56Average Performance (map)
Reranking
0.1B 1B 2B 4B
Model Parameters (Billions)
0.350
0.375
0.400
0.425
0.450
0.475
0.500Average Performance (nDCG@10)
Retrieval
0.1B 1B 2B 4B
Model Parameters (Billions)
0.74
0.76
0.78
0.80
0.82Average Performance (cos. sim. spearman corr.)
STS
GTR ST5 SGPT
Figure 3: MTEB performance scales with model
size. The smallest SGPT variant underperforms similar-
sized GTR and ST5 variants. This may be due to the
bias-only ﬁne-tuning SGPT employs, which catches
up with full ﬁne-tuning only as model size and thus
the number of bias parameters increases (Muennighoff,
2022).
ding tasks leading to a high representation of trans-
formers (Vaswani et al., 2017). We group models
into self-supervised and supervised methods.
Self-supervised methods (a) Transformer-
based BERT (Devlin et al., 2018) is trained using
self-supervised mask and sentence prediction tasks.
By taking the mean across the sequence length
(mean-pooling) the model can directly be used
to produce text embeddings. SimCSE-Unsup
(Gao et al., 2021b) uses BERT as a foundation
and performs additional self-supervised training.
(b) Non-transformer : Komninos (Komninos
and Manandhar, 2016) and Glove (Pennington
et al., 2014) are two word embedding models
that directly map words to vectors. Hence, their
embeddings lack context awareness, but provide
signiﬁcant speed-ups.
Supervised methods The original transformer
model (Vaswani et al., 2017) consists of an encoder
and decoder network. Subsequent transformers
often train only encoders like BERT (Devlin et al.,
2018) or decoders like GPT (Radford et al., 2019).
(a) Transformer encoder methods coCon-
denser (Gao and Callan, 2021), Contriever (Izac-
ard et al., 2021), LaBSE (Feng et al., 2020) and
SimCSE-BERT-sup (Gao et al., 2021b) are based
on the pre-trained BERT model (Devlin et al.,
2018). coCondenser and Contriever add a self-
supervised stage prior to supervised ﬁne-tuning
for a total of three training stages. LaBSE uses
BERT to perform additional pre-training on par-
allel data to produce a competitive bitext mining
model. SPECTER (Cohan et al., 2020a) relies on
the pre-trained SciBERT (Beltagy et al., 2019) vari-
ant instead and ﬁne-tunes on citation graphs. GTR
(Ni et al., 2021b) and ST5 (Ni et al., 2021a) are
based on the encoder part of the T5 model (Raf-
fel et al., 2020) and only differ in their ﬁne-tuning
datasets. After additional self-supervised training,
ST5 does contrastive ﬁne-tuning on NLI (Ni et al.,
2021a; Gao et al., 2021b) being geared towards
STS tasks. Meanwhile, GTR ﬁne-tunes on MS-
MARCO and focuses on retrieval tasks. MPNet
and MiniLM correspond to ﬁne-tuned embedding
models (Reimers and Gurevych, 2019) of the pre-
trained MPNet (Song et al., 2020) and MiniLM
(Wang et al., 2020) models using diverse datasets
to target any embedding use case.
(b) Transformer decoder methods SGPT Bi-
Encoders (Muennighoff, 2022) perform contrastive
ﬁne-tuning of <0.1% of pre-trained parameters us-
ing weighted-mean pooling. Similar to ST5 and
GTR, SGPT-nli models are geared towards STS,
while SGPT-msmarco models towards retrieval.
SGPT-msmarco models embed queries and doc-
uments for retrieval with different special tokens
to help the model distinguish their role. For non-
retrieval tasks, we use its query representations.
We benchmark publicly available SGPT models
based on GPT-NeoX (Andonian et al., 2021), GPT-
J (Wang and Komatsuzaki, 2021) and BLOOM
(Scao et al., 2022). Alternatively, cpt-text (Nee-
lakantan et al., 2022) passes pre-trained GPT de-
coders through a two-stage process using last token
pooling to provide embeddings from decoders. We
benchmark their models via the OpenAI Embed-
dings API4.
(c) Non-transformer LASER (Heffernan et al.,
2022) is the only context aware non-transformer
model we benchmark, relying on an LSTM
4https://beta.openai.com/docs/guides/
embeddings



Source: data\tc18_2309.07597v5\referenced_papers\[28]_2308.03281.pdf (Page 6):

Trec-COVIDNFCorpus
Tóuche-2020
DBPediaScidocs
Climate-fever
HotpotQA
FiQA
CQADupStackMSMARCO
NQ FeverScifactArguAna
Quora
Avg.0
50
100Recall@100
SimCSE Contriever BM25 GTE
Figure 2: Recall@100 of unsupervised text retrieval methods on BEIR benchmark (Thakur et al., 2021). We
compare our model GTEbase (based on BERTbase) without using any annotated data to SimCSE (Gao et al., 2021)
(based on RoBERTalarge), Contriever (Izacard et al., 2022a) (based on BERTbase) and BM25. Baseline results are
borrowed from the Contriever paper (Izacard et al., 2022a) with dot product being the similarity function.
Dataset BM25 SimCSE Contriever CPT-S E5 small E5base E5large GTEsmall GTEbase GTElarge
MS MARCO 22.8 9.4 20.6 19.9 25.4 26.0 26.2 31.3 31.8 31.7
Trec-Covid 65.6 26.2 27.4 52.9 52.0 61.0 61.8 61.8 64.0 64.8
NFCorpus 32.5 9.9 31.7 32.0 29.3 35.8 33.7 34.9 36.2 38.1
NQ 32.9 11.7 25.4 - 37.3 39.0 41.7 32.0 35.3 34.5
HotpotQA 60.3 19.8 48.1 51.5 46.0 52.4 52.2 49.3 50.8 49.2
FiQA 23.6 9.8 24.5 34.1 38.3 40.0 43.2 37.0 36.9 40.6
ArguAna 31.5 38.3 37.9 38.7 42.5 42.2 44.4 41.6 41.0 41.3
Touche-2020 36.7 8.9 19.3 21.0 19.9 16.9 19.8 17.7 18.2 18.5
CQADupStack 29.9 13.2 28.4 - 35.0 35.4 38.9 38.1 39.9 39.8
Quora 78.9 78.0 83.5 68.1 85.8 85.7 86.1 86.1 85.0 84.8
DBPedia 31.3 15.0 29.2 27.2 34.5 35.4 37.1 33.5 33.2 33.6
Scidocs 15.8 5.5 14.9 - 19.9 21.1 21.8 21.5 22.5 22.7
Fever 75.3 21.1 68.2 57.1 62.5 63.4 68.6 71.3 72.7 70.5
Climate-Fever 21.3 11.8 15.5 15.8 14.5 15.4 15.7 21.4 21.0 25.4
Scifact 66.5 25.7 64.9 65.4 68.5 73.7 72.3 72.7 74.1 74.1
Average 41.7 20.3 36.0 - 40.8 42.9 44.2 43.4 44.2 44.6
Table 5: nDCG@10 of different unsupervised methods on the BEIR benchmark (Thakur et al., 2021). SimCSE is
based on BERTbase backbone. CPT-S (Neelakantan et al., 2022) is of similar size to BERTlarge. Baseline results are
borrowed from E5 paper (Wang et al., 2022b). Note that Contriever uses dot product as the similarity metric while
other models uses cosine similarity.
on the tasks covered in the MTEB benchmark,
please refer to the Appendix B.
Two settings are considered for comparison: the
unsupervised setting and the supervised setting. In
the unsupervised setting, models are trained us-
ing unlabeled data, while supervised models are
fine-tuned using high-quality datasets with human
labels. The results of strong baseline models are
presented in Table 6.
In the unsupervised setting, our model outper-
forms the previous best model, E5, by a signifi-
cant margin across all considered tasks, without
the use of task-specific prompts. This improve-
ment can be attributed to the inclusion of more
training data formats and various sources of self-
supervision signals. Furthermore, it is worth noting
that our unsupervised pre-trained model narrows
the gap even further with larger supervised base-
lines, such as GTR and Sentence-T5. In the super-
vised setting, our model surpasses OpenAI results



Source: data\tc18_2309.07597v5\referenced_papers\[28]_2308.03281.pdf (Page 0):

Towards General Text Embeddings with Multi-stage Contrastive Learning
Zehan Li1, Xin Zhang1, Yanzhao Zhang1, Dingkun Long1, Pengjun Xie1, Meishan Zhang
1Alibaba Group
{lizehan.lzh,linzhang.zx,zhangyanzhao.zyz,
dingkun.ldk,pengjun.xpj}@alibaba-inc.com
Abstract
We present GTE, a general-purpose text embed-
ding model trained with multi-stage contrastive
learning. In line with recent advancements in
unifying various NLP tasks into a single for-
mat, we train a unified text embedding model
by employing contrastive learning over a di-
verse mixture of datasets from multiple sources.
By significantly increasing the number of train-
ing data during both unsupervised pre-training
and supervised fine-tuning stages, we achieve
substantial performance gains over existing em-
bedding models. Notably, even with a relatively
modest parameter count of 110M, GTEbase out-
performs the black-box embedding API pro-
vided by OpenAI and even surpasses 10x larger
text embedding models on the massive text
embedding benchmark. Furthermore, without
additional fine-tuning on each programming
language individually, our model outperforms
previous best code retrievers of similar size by
treating code as text. In summary, our model
achieves impressive results by effectively har-
nessing multi-stage contrastive learning, offer-
ing a powerful and efficient text embedding
model with broad applicability across various
NLP and code-related tasks.1
1 Introduction
Text embeddings have became an indispensable
component in many natural language processing
tasks, such as text classification, text retrieval, ques-
tion answering and dialogue systems (Karpukhin
et al., 2020; Humeau et al., 2020; Choi et al., 2021;
Izacard et al., 2022a; Long et al., 2022a; Rajapakse,
2023). These embedding models represent texts us-
ing low-dimensional vectors and capture their sim-
ilarity through vector operations. The emergence
of recent large language models (LLMs) (Radford
et al., 2018; Touvron et al., 2023; OpenAI, 2023)
has generated considerable interest in retrieval-
1The GTE model is publicly available at https://
huggingface.co/thenlper/gte-large
UnsupervisedContrastive Pre-training on Massive Text Pairs mined from the WebSupervisedContrastive Fine-tuningon Annotated Text Triples from Multiple Tasks
… …
MSMARCO
NaturalQuestionsTriviaQAWebQuestionsHotpotQAMNLI
QuoraStackExchangeDupWebSearch
OpenQA NaturalLanguageInferenceSNLI
FactVerificationFEVER
Paraphrase
MEDIOthersBERRI
Figure 1: Illustration of the multi-stage contrastive learn-
ing pipeline used to train our text embedding model.
augmented systems based on text embedding mod-
els that integrate the reasoning and comprehension
capabilities of LLMs (Izacard et al., 2022b; Ram
et al., 2023; Shi et al., 2023). Consequently, there
has been a growing focus on general text represen-
tation in both industry and academia.
The pursuit of developing a unified model to ad-
dress a multitude of downstream tasks has been
long-standing due to the diverse formats, domains
and downstream applications of natural language.
The emergence of pre-trained language models has
further opened up possibilities for training such a
universal model. Nonetheless, within the realm
of text representation research, previous text em-
bedding models have primarily focused on specific
tasks, and their training strategies or models, tai-
lored to a single task, may not perform optimally
in other contexts. For example, the text represen-
tation model SimCSE (Gao et al., 2021), trained
on symmetric text pairs, demonstrates limitations
in text retrieval tasks. Similarly, certain text rep-
resentation models specifically designed for dense
retrieval tasks do not exhibit robust performance
in sentence textual similarity tasks. Recently, there
has been a shift in research focus towards develop-
ing more comprehensive models for text represen-
tation leveraging large quantities of unlabeled web
data through unsupervised contrastive pre-training,
coupled with task-specific data, prompts, or in-
structions to mitigate task conflicts during fine-
tuning (Ni et al., 2022a,b; Neelakantan et al., 2022;
arXiv:2308.03281v1  [cs.CL]  7 Aug 2023



Source: data\tc18_2309.07597v5\referenced_papers\[15]_1810.04805.pdf (Page 14):

BERT
E[CLS] E1  E[SEP]... EN E1’ ... EM’
C
 T1
 T[SEP]...
 TN
 T1’ ...
 TM’
[CLS] Tok 
1
 [SEP]... Tok 
N
Tok 
1 ... Tok
M
Question Paragraph
BERT
E[CLS] E1  E2  EN
C
 T1
  T2
  TN
Single Sentence 
...
...
BERT
Tok 1  Tok 2  Tok N...[CLS]
E[CLS] E1  E2  EN
C
 T1
  T2
  TN
Single Sentence 
B-PERO O
...
...E[CLS] E1  E[SEP]
Class 
Label
... EN E1’ ... EM’
C
 T1
 T[SEP]...
 TN
 T1’ ...
 TM’
Start/End Span
Class 
Label
BERT
Tok 1  Tok 2  Tok N...[CLS] Tok 1[CLS][CLS] Tok 
1
 [SEP]... Tok 
N
Tok 
1 ... Tok
M
Sentence 1
...
Sentence 2
Figure 4: Illustrations of Fine-tuning BERT on Different Tasks.
SST-2 The Stanford Sentiment Treebank is a
binary single-sentence classiﬁcation task consist-
ing of sentences extracted from movie reviews
with human annotations of their sentiment (Socher
et al., 2013).
CoLA The Corpus of Linguistic Acceptability is
a binary single-sentence classiﬁcation task, where
the goal is to predict whether an English sentence
is linguistically “acceptable” or not (Warstadt
et al., 2018).
STS-B The Semantic Textual Similarity Bench-
mark is a collection of sentence pairs drawn from
news headlines and other sources (Cer et al.,
2017). They were annotated with a score from 1
to 5 denoting how similar the two sentences are in
terms of semantic meaning.
MRPC Microsoft Research Paraphrase Corpus
consists of sentence pairs automatically extracted
from online news sources, with human annotations
for whether the sentences in the pair are semanti-
cally equivalent (Dolan and Brockett, 2005).
RTE Recognizing Textual Entailment is a bi-
nary entailment task similar to MNLI, but with
much less training data (Bentivogli et al., 2009).14
WNLI Winograd NLI is a small natural lan-
guage inference dataset (Levesque et al., 2011).
The GLUE webpage notes that there are issues
with the construction of this dataset, 15 and every
trained system that’s been submitted to GLUE has
performed worse than the 65.1 baseline accuracy
of predicting the majority class. We therefore ex-
clude this set to be fair to OpenAI GPT. For our
GLUE submission, we always predicted the ma-
14Note that we only report single-task ﬁne-tuning results
in this paper. A multitask ﬁne-tuning approach could poten-
tially push the performance even further. For example, we
did observe substantial improvements on RTE from multi-
task training with MNLI.
15https://gluebenchmark.com/faq



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 7):

deu-engmal-engnob-engspa-engepo-engtur-engtel-engpol-engvie-enghrv-engron-enghin-engglg-engsqi-engces-engest-enghun-engslk-englit-engfin-engafr-engtha-engnld-engslv-engtgl-engmon-englvs-engdan-engswe-engzsm-engcat-engjpn-engina-engell-engcmn-engkat-engeus-engbel-engaze-engbos-engfra-engisl-engpes-engbul-engnno-engsrp-engpor-enghye-engukr-enggle-engrus-engind-engmkd-engurd-engita-engmar-enguig-engcym-engxho-engheb-engamh-engkor-engast-engwuu-engyue-engido-engfry-engtam-engara-engyid-engben-engkaz-engfao-engtat-enggla-engile-engswh-enguzb-engkur-englat-engjav-engcbk-engnds-engkhm-engarz-engtuk-engnov-engawa-englfn-enghsb-engoci-engdsb-engpms-engceb-engmax-engwar-engswg-engang-engtzl-engcsb-enggsw-engarq-engorv-engcha-engmhr-engbre-engkzj-engdtp-engpam-engcor-engber-engkab-eng
0.0
0.2
0.4
0.6
0.8
1.0
F1 score
LaBSE
LASER2
MiniLM-L12-multilingual
MPNet-multilingual
SGPT-BLOOM-7.1B-msmarco
(a) Bitext Mining on Tatoeba
en hi
zh-CN
pt id es th it fr ru de fa sv vi
zh-TW
nl ms da pl tr sq el ro hu sl ko fi ja nb ml lv he ur bn ar te af ta hy my az mn is kn tl jv sw ka kmam cy zh
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Accuracy
(b) Multilingual Classiﬁcation
ko fr es en ar it zh ru tr de pl
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8Cos. Sim. Spearman Corr.
fr-en en-aren-de it-en es-en nl-en pl-en zh-en en-tr es-it fr-pl de-fr de-ende-pl
 (c) Multi- and Crosslingual STS
Figure 5: MTEB multilingual performance. Bitext mining is dominated by LaBSE, while classiﬁcation and STS
results are mixed. SGPT-BLOOM-7B1-msmarco tends to perform well on the languages BLOOM has been pre-
trained on, such as Chinese, French and Portuguese.
Pair Classiﬁcation GTR-XL and GTR-XXL
have the strongest performance. Pair classiﬁca-
tion is closest to STS in its framing, yet models
rank signiﬁcantly differently on the two tasks. This
highlights the importance of benchmarking on a
diverse set of tasks to avoid blindly reusing a model
for a different task.
Reranking MPNet and MiniLM models perform
strongly on reranking tasks. On SciDocsRR (Co-
han et al., 2020a) they perform far better than big-
ger models, which is likely due to parts of Sci-
DocsRR being included in their training data. Our
scale of experiments and that of model pre-training
make controlling for data contamination challeng-
ing. Thus, we ignore overlap of MTEB datasets
with model training datasets in MTEB scores. As
long as enough datasets are averaged, we believe
these effects to be insigniﬁcant.
Retrieval SGPT-5.8B-msmarco is the best em-
bedding model on the BEIR subset in MTEB
as well as on the full BEIR benchmark (Thakur
et al., 2021; Muennighoff, 2022). The even larger
7.1B SGPT model making use of BLOOM (Scao
et al., 2022) performs signiﬁcantly weaker, which
is likely due to the multilinguality of BLOOM.
Models geared towards STS (SimCSE, ST5, SGPT-
nli) perform badly on retrieval tasks. Retrieval
tasks are unique in that there are two distinct types
of texts: Queries and documents (“asymmetric”),
while other tasks only have a single type of text
(“symmetric”). On the QuoraRetrieval dataset,
which has been shown to be largely symmetric
(Muennighoff, 2022), the playing ﬁeld is more
even with SGPT-5.8B-nli outperforming SGPT-
5.8B-msmarco, see Table 11.
STS & Summarization Retrieval models (GTR,
SGPT-msmarco) perform badly on STS, while ST5-
XXL has the highest performance. This highlights
the bifurcation of the ﬁeld into separate embedding
models for retrieval (asymmetric) and similarity
(symmetric) use cases (Muennighoff, 2022).
4.3 Efﬁciency
We investigate the latency-performance trade-off
of models in Figure 4. The graph allows for signiﬁ-
cant elimination of model candidates in the model
selection process. It brings model selection down
to three clusters:
Maximum speed Word Embedding models offer
maximum speed with Glove taking the lead on both
performance and speed, thus making the choice
simple in this case.
Maximum performance If latency is less impor-
tant than performance, the left-hand side of the
graph offers a cluster of highly performant, but
slow models. Depending on the task at hand, GTR-
XXL, ST5-XXL or SGPT-5.8B may be the right
choice, see Section 4.2. SGPT-5.8B comes with
the additional caveat of its high-dimensional em-
beddings requiring more storage.
Speed and performance The ﬁne-tuned MPNet
and MiniLM models lead the middle cluster mak-
ing the choice easy.



#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 7):

173 176 173 172
ArguAna
176 179
173
182
BioASQ
206
228
244
234
Climate-Fever
57
60
62 62
DBPedia-entity
88
97
104 105
Fever
136
156
151 154
FiQA-2018
53
56
61
63
HotpotQA
49
50 50 50
MS Marco
236
243 245 243
NFCorpus
75
78
80 81
NQ
8
9 9 9
Quora
410
453
525
490
Robust04
166
170 169 168
SCIDOCS
211
216
221 219
SciFact
10 10 10 10
Signal-1M
BaseLarge XL XXL
15
10
12
9
Trec-Covid
BaseLarge XL XXL
678
707 700 708
Trec-News
BaseLarge XL XXL
32 32
41
62
Touché-2020
BaseLarge XL XXL
111
116 117 116
CQADupStack
Figure 5: Median lengths (in words) of top-10 retrieved
documents for all queries.
Touche2020 are longer.
On the other hand, the only exception we ob-
serve is the Trec-Covid dataset, where GTR-XXL
model retrieves much shorter documents than those
retrieved by the smaller size counterparts. This may
explain the inferior performance of GTR-XXL on
Trec-Covid shown in table 3 and table 8. We leave
it as future work to explore the effects of using the
dot-product as similarity function for large dual
encoders.
7 Related Work
Neural information retrieval. Document re-
trieval is an important task in the NLP and informa-
tion retrieval (IR) communities. The goal is to ﬁnd
the relevant document from a large corpus given a
query. Traditionally, lexical based approaches try-
ing to match the query and document based on term
overlap, such as TF-IDF and BM25 (Robertson
and Zaragoza, 2009), have achieved great success
in this task. Recently, neural based approaches,
which go beyond the simple term matching, are be-
ing quickly adopted by the community and achieve
state-of-the-art performance on multiple retrieval
tasks, such as passage retrieval (Karpukhin et al.,
2020), question answering (Ahmad et al., 2019),
conversational question answering (Qu et al., 2020)
and bitext retrieval (Feng et al., 2020).
Dual encoders for neural retrieval. Dual en-
coders have demonstrated to be one type of neural
retrievers that can achieve great performance com-
pared to traditional sparse models such as BM25
for a wide range of retrieval tasks (Karpukhin et al.,
2020; Gillick et al., 2018). One key aspect to their
success is the adoption of pre-trained language
models, which enables the dual encoders to have
backbone contextual embeddings to initialize from.
Other techniques such as negative mining (Xiong
et al., 2020; Lu et al., 2021; Sachan et al., 2021)
and large training batch sizes (Qu et al., 2021) have
also shown great effectiveness. However, few of
the previous works have discussed the effect of the
backbone model’s capacity.
Zero-shot neural retrieval. Recent works have
shown great improvement under the zero-shot set-
ting for dual encoders by leveraging distillation
and synthetic data generation (Thakur et al., 2021;
Hofstätter et al., 2021; Ma et al., 2020). Both of
these techniques, and scaling up backbone mod-
els, are effective ways to close the gap between
dual encoders and the upper bound of the single-
product approaches with ﬁxed-dimension embed-
dings. On the other hand, multi-vector approaches
introduce more interactions between dense embed-
dings, which could also beneﬁt from scaling up the
backbone multi-vector encoders. We hope that our
observation about scaling up model sizes for single
dot-product based methods can be combined with
these techniques and further push the frontier of
neural retrieval models.
8 Inference latency
One caveat for scaling up model size is the incre-
ment in the latency overhead. We investigate the
inference speed in terms of microseconds (ms) for
all GTR models with batch size 1 and input length
128. We found the latency increases from 17 ms,
34 ms, 96 ms to 349 ms. The GTR-Base model
has close latency compared to TAS-B while the
largest GTR-XXL model has a similar latency to
the re-ranking models (Thakur et al., 2021). With
the recent work towards making large models efﬁ-
cient from angles such as sparsity, distillation and
prompt-tuning, we hope the inference time for large
dual encoders can be signiﬁcantly reduced in the
future.
9 Conclusion
This paper presents the Generalizable T5 Retriever
(GTR), a scaled-up dual encoder model with a
ﬁxed-size bottleneck layer. We show that scal-
ing up the model size brings signiﬁcant improve-
ment on retrieval performance across the board on
the BEIR zero-shot retrieval benchmark, especially



Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 6):

GTR-FT GTR-PT GTR
Fine-tuning   
NDCG@10 on MS Marco
Base 0.400 0.258 0.420
Large 0.415 0.262 0.430
XL 0.418 0.259 0.439
XXL 0.422 0.252 0.442
Zero-shot average NDCG@10 w/o MS Marco
Base 0.387 0.295 0.416
Large 0.412 0.315 0.445
XL 0.433 0.315 0.453
XXL 0.430 0.332 0.458
Table 5: Comparisons (NDCG@10) of the models
trained with and without pre-training and ﬁne-tuning.
Notably, the GTR-FT XL model already achieves an
average zero-shot NDCG@10 of 0.433, which outper-
forms the previous best dual encoder model TAS-B
(NDCG@10=0.415).
6.1 Effect of scaling up for different training
stages
The ﬁrst ablation study aims to investigate how
scaling up effects dual encoder pre-training and
ﬁne-tuning. Results are listed in table 5.
For ﬁne-tuning only models, scaling up beneﬁts
both in-domain and out-of-domain performance.
For pre-training only models, the improvement on
in-domain performance is not obvious; meanwhile
for out-of-domain tasks, scaling up also improves
the generalization. Finally with both pre-training
and ﬁne-tuning, GTR models consistently improve
over GTR-FT models of all sizes. This shows the
power of combining scaling up and a generic pre-
training stage.
6.2 Importance of the ﬁne-tuning dataset
In table 5, we compare GTR and GTR-PT on the
BEIR benchmark to understand the importance of
ﬁne-tuning on MS Marco. The table shows that
there is a clear gap between GTR models before
and after ﬁne-tuning. The result shows the neces-
sity of leveraging a high quality dataset (e.g. search
data) to ﬁne-tune the dual encoders.
In table 6, we compare ﬁne-tuning GTR on NQ
instead of MS Marco. Compared to MS Marco,
NQ only covers Wikipedia documents and is much
smaller in size, which allows us to investigate the
performance of GTR when ﬁne-tuned on a less
generalizable dataset. In addition, ﬁne-tuning on
NQ can give us a fair comparison with DPR.
As shown in table 6, the GTR-base model ﬁne-
Model Fine-tuning dataset Zero-shot aver-
age NDCG@10
DPR NQ 0.237
GTR-Base NQ 0.360
GTR-Large NQ 0.379
GTR-XL NQ 0.407
GTR-Large MS Marco 0.445
GTR-XL MS Marco 0.453
Table 6: Comparisons of GTR models ﬁne-tuned on
MS Marco and NQ. We report the zero-shot average
NDCG@10. Scaling up improves model performance
both on NQ and MS Marco.
tuned on NQ outperforms the original DPR model,
which uses a BERT-Base model as the encoder
backbone. This demonstrates the effectiveness of
our pre-training on the Web dataset as well as the
hard negatives introduced from Lu et al. (2021)
for NQ. Fine-tuning on NQ leads to inferior per-
formance compared to ﬁne-tuning on MS Marco,
which is consistent with prior work (Thakur et al.,
2021). However, importantly, scaling up GTR size
improves zero-shot performance on BEIR when
ﬁne-tuning on NQ. This shows that the beneﬁt of
scaling up holds for different ﬁne-tuning datasets.
Furthermore, when scaling from Large to XL, we
observe a larger gain when ﬁne-tuning with NQ
than with MS Marco, indicating that scaling up
helps more when using weaker ﬁne-tuning data.
6.3 Document length vs model capacity
Previously, BEIR has shown that models trained
with cosine similarity prefer short documents while
those trained with dot-product prefer long docu-
ments (Thakur et al., 2021). We investigate whether
scaling up affect this observation. Speciﬁcally, we
compute the median lengths (in words) of the top-
10 retrieved documents for all queries. Results are
shown in ﬁg. 5.
Though all GTR models are trained using co-
sine similarity, we found that scaling up the model
size has inﬂuence over the lengths of retrieved
documents. We observe an increasing trend of
document length for DB-Pedia, Fever, HotpotQA,
Signal-1M, Trec-News, and Web-Touche2020 with
scaling up. In particular, for Web-Touche2020, the
lengths of the retrieved documents grow drastically
as the models scale up: The largest GTR-XXL
retrieves documents that are on average twice as
long compared with the smallest GTR-Base. This
plays in our favor since Thakur et al. (2021) show
that the majority of relevant documents in Web-



Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 10):

Model NDCG@10 MRR@10 Recall@1000
ANCE 0.388 0.330 0.959
TAS-Balanced 0.408 0.340 0.975
ColBERT 0.401 0.360 0.968
RocketQA / 0.370 0.979
GTR-Base 0.420 0.366 0.983
GTR-Large 0.430 0.379 0.991
GTR-XL 0.439 0.385 0.989
GTR-XXL 0.442 0.388 0.990
Table 7: Comparisons of different models on MS
Marco. Scaling up can improve GTR models’ in-
domain performance.
A More results
A.1 Comparisons on MS Marco
Table 7 shows the comparisons of GTR models and
the baselines. Note that the best RocketQA model
used additional augmented data other than MS
Marco to improve the model performance while
all others do not. Our best GTR-XXL models out-
performs RocketQA on both MRR and recall.
A.2 Comparison of different dual encoder
pre-training strategies
In a concurrent work (Anonymous, 2022), re-
searchers proposed to conduct contrastive learning
(CL) pre-training for improving the generalizability
of neural retrievers. The paired data for contrastive
training is constructed from C4 and Wiki dataset
in an unsupervised way. In particular, they con-
struct pairs by randomly choosing two spans from
a single document and conduct word deletion or
replacement to each span. We compare the perfor-
mance of our GTR models to their models to gain
insights into different pretraining strategies for dual
encoders.
As shown in Figure 6, on over half of the
datasets, models with our pre-training approach
under-perform CL-Pretrain with the base size;
while as the model size increases, GTR-Large
and -XXL models show signiﬁcant gains over CL-
Pretrain. The best GTR-XXL model achieves
0.49 for NDCG@10 on average while CL-Pretrain
achieves 0.46. This demonstrates that scaling up
can mitigate the disadvantage of the potentially
inferior pre-training approach. Note that our pre-
training is additive to CL-Pretrain and we can lever-
age the pre-training on C4 and Wiki to further im-
prove the results. We leave this exploration as
future work.
0.66
0.54
0.56
0.58
0.5
Trec-Covid
0.32
0.31
0.33
0.340.34
NFCorpus
0.33
0.5
0.550.560.57
NQ
0.6
0.54
0.58
0.590.6
HotpotQA
0.24
0.35
0.420.44
0.47
FiQA-2018
0.32
0.510.520.530.54
ArguAna
0.37
0.2 0.220.23
0.26
Touché-2020
0.79
0.880.890.890.89
Quora
0.31
0.35
0.390.4
0.41
DBPedia-entity
0.16
0.15
0.160.160.16
SCIDOCS
CL
BaseLarge
XLXXL
0.75
0.66
0.710.72
0.74
Fever
CL
BaseLarge
XLXXL
0.21
0.24
0.26
0.270.27
Climate-Fever
CL
BaseLarge
XLXXL
0.66
0.6
0.640.64
0.66
SciFact
CL
BaseLarge
XLXXL
0.3
0.36
0.380.390.4
CQADupStack
CL
BaseLarge
XLXXL
0.410.42
0.440.450.46
Avg
Figure 6: Comparison with Anonymous (2022) on
NDCG@10. “CL” denotes Anonymous (2022) with
contrastive learning on C4 and Wiki while others de-
note our GTR models with different sizes. Note that
they only report results on 15 datasets of the BEIR
benchmark.
A.3 Recall on BEIR
Table 8 presents the Recall@100 of GTR mod-
els and the baselines. Similar to NDCG@10, we
observe that scaling up dual encoders lead to sig-
niﬁcant gains on the BEIR benchmark in terms of
recall.



Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 3):

The training process includes a pre-training stage
on a web-mined corpus and a ﬁne-tuning stage on
search datasets. The web-mined corpus provides a
large amount of semi-structured data pairs (such as
question-answer pairs and conversations), which
can provide rich semantic relevance information. It
is easy to collect but it is often not well annotated,
if at all. The search datasets are often annotated by
humans, and the queries and documents are also
authored by humans. These datasets are of high
quality but costly to collect.
In this work, for dual encoder pre-training, we
initialize the dual encoders from the T5 models
and train on question-answer pairs collected from
the Web. Recently, Sentence-T5 (Ni et al., 2021)
explored different ways to extract strong text em-
beddings and achieved remarkable performance
on SentEval and Sentence Textual Similarity tasks.
We follow that setting to encode queries and pas-
sages via mean pooling from the T5 encoders and
focus on the dense retrieval tasks.
For ﬁne-tuning, our aim is to adapt the model to
retrieval using a high quality search corpus so the
model can learn to better match generic queries to
documents. In this paper, we consider two datasets
for ﬁne-tuning: MS Marco (Nguyen et al., 2016)
and Natural Questions (Kwiatkowski et al., 2019).
4 Experimental setup
4.1 Training Data
Community QA. In order to leverage most of
the power from the large scale models, we col-
lect input-response pairs and question-answer pairs
from online forums and QA websites including
Reddit, Stack-Overﬂow, etc. This results in 2 bil-
lion question-answer pairs that we use to pre-train
the dual encoder.
MS Marco. We consider the MS Marco
dataset (Nguyen et al., 2016), which includes 532K
query and document pairs, as search data for ﬁne-
tuning. The dataset is sampled from Bing search
logs, which covers a broad range of domains
and concepts. Most of the neural models com-
pared in (Thakur et al., 2021) are trained on MS
Marco, including DeepCT (Dai and Callan, 2020),
DocT5Query (Nogueira, 2019), ANCE (Xiong
et al., 2020) and ColBERT (Khattab and Zaharia,
2020). Some of these models have shown great
generalization with comparable or even better per-
formance relative to BM25.
GTR Models Base Large XL XXL
# of params 110M 335M 1.24B 4.8B
Table 1: Number of parameters in the GTR models.
Models Dim. size
ColBERT 128
DPR, ANCE, TAS-B, GenQ, GTR 768
BM25, DocT5Query -
Table 2: Dimension size of different models. Most dual
encoder models set the embedding dimension size to
768.
Natural Questions. In the ﬁne-tuning stage,
we also consider the Natural Questions dataset
(Kwiatkowski et al., 2019) , which has been widely
used in the dense retrieval literature (Karpukhin
et al., 2020; Xiong et al., 2020). It consists of 130k
query and passage pairs which are also human-
annotated.
4.2 Conﬁgurations
We implement GTR models in JAX4 and train them
on Cloud TPU-V8. We consider different sizes of
the T5 transformer (Vaswani et al., 2017) architec-
ture including Base, Large, XL and XXL. Their
number of parameters are listed in table 1.
Note that we only use the encoder portion of the
T5 models and thus the number of parameters are
less than half of the full model size. We use the off-
the-shelf checkpoints as the initial parameters and
use the same sentencepiece vocabulary model.5
During pre-training and ﬁne-tuning, we set the
batch size to 2048 and use a softmax temperature
τ of 0.01. We use Adafactor optimizer (Shazeer
and Stern, 2018) and set the initial learning rate to
1e-3 with a linear decay. We train the model for
800K steps and 20K steps for the pre-training and
ﬁne-tuning stages, respectively.
For ﬁne-tuning, we use the hard negatives re-
leased by RocketQA (Qu et al., 2021) when ﬁne-
tuning with MS Marco data and the hard negatives
release by (Lu et al., 2021) for Natural Questions,
which were proven to lead to better retriever perfor-
mance. By default, we use the complete MS Marco
dataset and the NQ dataset for ﬁne-tuning.
When evaluating on the BEIR benchmark, we
use sequences of 64 tokens for the questions and
512 for the documents in all datasets except Trec-
4https://github.com/google/jax
5https://github.com/google-research/
text-to-text-transfer-transformer



Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 5):

0.420.430.440.44
0.23
MS MARCO
0.540.56
0.58
0.5
0.66
Trec-Covid
0.27
0.320.320.32
0.46
BioASQ
0.31
0.33
0.340.34
0.32
NFCorpus
0.5
0.550.560.57
0.33
NQ
0.54
0.580.590.6 0.6
HotpotQA
0.35
0.420.44
0.47
0.24
FiQA-2018
0.260.260.270.27
0.33
Signal-1M
0.340.340.350.35
0.4
Trec-News
0.44
0.470.48
0.51
0.41
Robust04
0.510.520.530.54
0.32
ArguAna
0.2 0.220.23
0.26
0.37
Touché-2020
0.880.890.890.89
0.79
Quora
0.35
0.390.4 0.41
0.31
DBPedia-entity
0.15
0.160.160.160.16
SCIDOCS
BaseLarge
XLXXLBM25
0.66
0.710.72
0.740.75
Fever
BaseLarge
XLXXLBM25
0.24
0.260.270.27
0.21
Climate-Fever
BaseLarge
XLXXLBM25
0.6
0.640.64
0.660.66
SciFact
BaseLarge
XLXXLBM25
0.36
0.380.390.4
0.3
CQADupStack
BaseLarge
XLXXLBM25
0.42
0.440.450.46
0.41
Avg
Figure 4: Comparison with BM25 on NDCG@10. The
GTR-Base model outperforms BM25 on 9 datasets and
the larger GTR models continue to improve on these 9
tasks. The GTR-XXL model catches up or surpasses
BM25 on the other 5 datasets and only under-performs
on 5 of the remaining tasks.
model already outperforms the previous best dense
retrieval model TAS-B as well as the best sparse
model DocT5Query. Scaling up to GTR-XXL
leads to another jump in retrieval performance.
Similar improvements are found on Recall@100
as shown in the Appendix’s table 8. On average,
the scaling up process demonstrates an encourag-
ing ascending trend that eventually outperforms all
baseline methods on all evaluation metrics. This
conﬁrms that scaling up is a valid path towards
generalizability.
Previously, dual encoders failed to match the
performance of BM25 for tasks that require better
lexical matching capabilities. Thus, we wanted to
investigate what kind of tasks can get improved
by scaling up the model size. Figure 4 presents a
detailed comparison of all sizes of GTR models
against the BM25 baseline.
For tasks like NQ where dual encoders have been
previously shown to be more effective than BM25,
increasing the model size continues to advance the
performance of dual encoders. This suggests scal-
ing up can further boost the head start of dense
models over sparse models on these datasets.
For tasks like BioASQ and NFCorpus, where
dual encoders previously struggled to match the
performance of BM25 for inherent reasons, we dis-
covered that scaling up consistently improves the
retrieval performance. In particular, for NFCor-
pus, our Base model under-performs BM25 but the
GTR-FT GTR
Ratio of data Large XL Large XL XXL
NDCG@10 on MS Marco
10% 0.402 0.397 0.428 0.426 -
100% 0.415 0.418 0.430 0.439 0.430
Zero-shot average NDCG@10 w/o MS Marco
10% 0.413 0.418 0.452 0.462 0.465
100% 0.412 0.433 0.445 0.453 0.458
Table 4: Comparisons of NDCG@10 for GTR models
trained with different amount of ﬁne-tuning data. With
only 10% of the MS Marco data, both GTR-FT and
GTR large and XL models achieve slightly worse in-
domain performance; meanwhile they obtain compara-
ble or even superior out-of-domain performance than
using the complete MS Marco data.
XL model outperforms BM25 by 5.5% (0.343 vs.
0.325). This exciting ﬁnding veriﬁes our assump-
tion that scaling up can further exploit the powerful
semantic matching capabilities of the dual encoder
models and enable them to ultimately outperform
BM25.
5.3 Data efﬁciency for large retrievers
To better understand the data efﬁciency for large
dual encoders, we trained models using different
portions of the MS Marco dataset during ﬁne-
tuning. In particular, we sampled a subset of the
training data by keeping only 10% of the training
queries as well as their relevant (positive) passages
and irrelevant (hard negative) passages.
As shown in table 4, using 10% of training data
reduces the in-domain performance of the GTR
models on MS Marco. For the GTR-FT (ﬁne-
tuning only) models, using 10% of the data leads
to a mixed result of out-of-domain performance.
On the other hand, for full GTR models, using
10% of the MS Marco dataset is sufﬁcient for ﬁne-
tuning. In particular, the GTR-Large, XL and XXL
models achieve comparable or even better OOD
performance than ﬁne-tuning on the complete MS
Marco dataset. This might suggest that GTR mod-
els have the beneﬁt of data efﬁciency and could use
less training data for domain adaptation.
6 Ablation Study and Analysis
In this section we present ablations and analysis to
further understand the effects of scaling up, the im-
pact of ﬁne-tuning and pre-training, and the trends
of the GTR model on different experimental condi-
tions.



#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[32]_2202.08904.pdf (Page 6):

Model Name SBERT-Base SGPT-125M SGPT-1.3B SGPT-2.7B SGPT-5.8B
Transformer (T.) BERT GPT-Neo GPT-Neo GPT-Neo GPT-J
Total params 109M 125M 1.3B 2.7B 5.8B
Bias tensors per T. layer 8 5 5 5 3
Bias params 103K 74K 395K 658K 692K
Bias params % 0.094% 0.060% 0.030% 0.025% 0.012%
Table 4: SGPT parameter overview. Due to the removal of the ﬁnal language modeling head SGPT-
BE-5.8B has 206M parameters less than SGPT-CE-6.1B or GPT-J-6.1B. GPT-Neo models tie the
language modeling head weights with the input embeddings, hence there is no parameter difference.
Figure 3: Performance on USEB [49] by taking the embeddings from certain layers. S models are
ﬁne-tuned on the same data with the same hyperparameters. Dashed, solid and dotted lines are last
token, mean and weighted mean pooling, respectively. Shades of red are transformer encoders, while
shades of blue are decoders. The 0th layer is the embeddings prior to the ﬁrst transformer layer.
4.1.2 Results
Figure 3 shows average precisions on USEB [49] across different methods and layers. Similar to
previous work [11], we ﬁnd that in the unsupervised setting, decoder transformers (GPT) strongly
underperform encoders (BERT). However, after ﬁne-tuning on the same dataset with the same
hyperparameters, decoders (SGPT) with 125M parameters closely trail the 110M parameter encoder
(SBERT) for the 12th layer. Weighted mean pooling outperforms mean and last token pooling for
SGPT 125M. When increasing SGPT size ten-fold, the last layer performance (24th layer) increases
beyond that of SBERT models. The performance difference of weighted mean pooling compared to
mean pooling further widens for SGPT 1.3B.
Table 5 provides performance on the individual USEB datasets, Quora and STS-B. STS-B scores
should not be the focus of comparison due to the drawbacks highlighted in [ 49]. Despite training
on less than 0.1% of parameters BitFit models are within +2 to -2% of fully ﬁne-tuned ones. BitFit
degrades performance more for decoders than encoders. This could be due to the missing bias
parameters, see Table 4. [53] highlights the importance of the query bias vector for BERT, which is
not present for SGPT models. SGPT-5.8B-weightedmean-nli-bitﬁt sets an out-of-domain state-of-the-
art on USEB, but is outperformed by models trained in-domain in [49]. We observed performance
gains by increasing the training batch size. SGPT-5.8B-weightedmean-nli-bitﬁt is trained with a
7



Source: data\tc18_2309.07597v5\referenced_papers\[32]_2202.08904.pdf (Page 5):

Id Python 125M 6.1B
G Documents are searched to ﬁnd matches with the same content.\nThe
document "{doc}" is a good search result for "{query}
0.764 0.794
quoraA Questions are searched to ﬁnd matches with the same content.\nThe
question "{doc}" is a good search result for "{query}
0.766
quoraB Below are two similar questions asking the same thing.\nThe question
"{doc}" is similar to "{query}
0.751
quoraC These two questions are the same: 1. {doc} 2.{query} 0.740
quoraD Question Body: {doc} Question Title:{query} 0.782 0.830
quoraE Question Body: {shortdoc} Question Title: {shortquery}\n Question
Body: {doc} Question Title: {query}
0.773
Table 3: SGPT-CE symmetric search results on Quora. The sum of log probabilities from {query}
is used as the re-rank score. Overﬂowing tokens are truncated from the left of {doc}. Top 100
documents are re-ranked. Scores are nDCG@10.
4 SGPT Bi-Encoder
4.1 Symmetric Search
4.1.1 Method
Like in §3.1.1, we ﬁrst experiment with decoder transformers that have only gone through unsuper-
vised pre-training. In the Bi-Encoder setting, a pooling operation is commonly applied to the model’s
hidden states to reduce them to a vector whose size is irrespective of sequence length. SBERT [39]
showed that a MEAN pooling mechanism outperforms [CLS] and MAX strategies for a BERT encoder.
Due to the causal attention mask in an auto-regressive decoder transformer, tokens do not attend to
future tokens like in an encoder transformer. Hence, only the last token has attended to all tokens in a
sequence. To account for this information mismatch, we propose to give later tokens a higher weight
using a position-weighted mean pooling method:
v=
S∑
i=1
wihi where wi = i∑S
i=1 i
(2)
where Sis the sequence length, hi the ith hidden state and vthe query or document embedding. We
compare weighted mean pooling with last token pooling, where the hidden state of the ﬁnal token is
the embedding, and regular mean pooling.
We follow recent work [16, 15, 20, 27] and perform supervised contrastive learning with in-batch
negatives. Given matching query-doc pairs {q(i),d(i)}M
i=1, we optimize the cost function:
JCL(θ) = 1
M
M∑
i=1
log exp(τ ·σ(fθ(q(i)),fθ(d(i))))∑M
j=1 exp(τ ·σ(fθ(q(i)),fθ(d(j))))
(3)
where fθ is the SGPT model outputting a ﬁxed-size vector, σcosine similarity and τ a temperature
parameter set to 20 in our experiments. We use GradCache [14] to train with large batch sizes in a
limited memory setting. We train on SNLI [4] and MNLI [51]. We limit the model sequence length
to 75 tokens during both training and inference.
We ﬁne-tune only bias parameters and freeze the rest of the model. This has been recently proposed
as BitFit [53] for BERT encoders. It has been shown to be competitive with full ﬁne-tuning in various
scenarios [18, 50, 24]. Table 4 shows the number of parameters trained for BitFit models. Due to
fewer gradient updates, BitFit signiﬁcantly reduces GPU memory and time required per step. Further,
adding a BitFit checkpoint to an instance with an existing full model will only require storing the
different biases. An instance already serving a 22.5GB fp32 GPT-J-6B model requires an additional
22MB of storage to serve an SGPT-5.8B-bitﬁt model.
6



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 5):

0.1B 1B 2B 4B
Model Parameters (Billions)
0.62
0.64
0.66
0.68
0.70
0.72
0.74Average Performance (accuracy)
Classification
0.1B 1B 2B 4B
Model Parameters (Billions)
0.36
0.37
0.38
0.39
0.40
0.41
0.42
0.43
0.44Average Performance (v_measure)
Clustering
0.1B 1B 2B 4B
Model Parameters (Billions)
0.76
0.78
0.80
0.82
0.84
0.86Average Performance (ap)
PairClassification
0.1B 1B 2B 4B
Model Parameters (Billions)
0.51
0.52
0.53
0.54
0.55
0.56Average Performance (map)
Reranking
0.1B 1B 2B 4B
Model Parameters (Billions)
0.350
0.375
0.400
0.425
0.450
0.475
0.500Average Performance (nDCG@10)
Retrieval
0.1B 1B 2B 4B
Model Parameters (Billions)
0.74
0.76
0.78
0.80
0.82Average Performance (cos. sim. spearman corr.)
STS
GTR ST5 SGPT
Figure 3: MTEB performance scales with model
size. The smallest SGPT variant underperforms similar-
sized GTR and ST5 variants. This may be due to the
bias-only ﬁne-tuning SGPT employs, which catches
up with full ﬁne-tuning only as model size and thus
the number of bias parameters increases (Muennighoff,
2022).
ding tasks leading to a high representation of trans-
formers (Vaswani et al., 2017). We group models
into self-supervised and supervised methods.
Self-supervised methods (a) Transformer-
based BERT (Devlin et al., 2018) is trained using
self-supervised mask and sentence prediction tasks.
By taking the mean across the sequence length
(mean-pooling) the model can directly be used
to produce text embeddings. SimCSE-Unsup
(Gao et al., 2021b) uses BERT as a foundation
and performs additional self-supervised training.
(b) Non-transformer : Komninos (Komninos
and Manandhar, 2016) and Glove (Pennington
et al., 2014) are two word embedding models
that directly map words to vectors. Hence, their
embeddings lack context awareness, but provide
signiﬁcant speed-ups.
Supervised methods The original transformer
model (Vaswani et al., 2017) consists of an encoder
and decoder network. Subsequent transformers
often train only encoders like BERT (Devlin et al.,
2018) or decoders like GPT (Radford et al., 2019).
(a) Transformer encoder methods coCon-
denser (Gao and Callan, 2021), Contriever (Izac-
ard et al., 2021), LaBSE (Feng et al., 2020) and
SimCSE-BERT-sup (Gao et al., 2021b) are based
on the pre-trained BERT model (Devlin et al.,
2018). coCondenser and Contriever add a self-
supervised stage prior to supervised ﬁne-tuning
for a total of three training stages. LaBSE uses
BERT to perform additional pre-training on par-
allel data to produce a competitive bitext mining
model. SPECTER (Cohan et al., 2020a) relies on
the pre-trained SciBERT (Beltagy et al., 2019) vari-
ant instead and ﬁne-tunes on citation graphs. GTR
(Ni et al., 2021b) and ST5 (Ni et al., 2021a) are
based on the encoder part of the T5 model (Raf-
fel et al., 2020) and only differ in their ﬁne-tuning
datasets. After additional self-supervised training,
ST5 does contrastive ﬁne-tuning on NLI (Ni et al.,
2021a; Gao et al., 2021b) being geared towards
STS tasks. Meanwhile, GTR ﬁne-tunes on MS-
MARCO and focuses on retrieval tasks. MPNet
and MiniLM correspond to ﬁne-tuned embedding
models (Reimers and Gurevych, 2019) of the pre-
trained MPNet (Song et al., 2020) and MiniLM
(Wang et al., 2020) models using diverse datasets
to target any embedding use case.
(b) Transformer decoder methods SGPT Bi-
Encoders (Muennighoff, 2022) perform contrastive
ﬁne-tuning of <0.1% of pre-trained parameters us-
ing weighted-mean pooling. Similar to ST5 and
GTR, SGPT-nli models are geared towards STS,
while SGPT-msmarco models towards retrieval.
SGPT-msmarco models embed queries and doc-
uments for retrieval with different special tokens
to help the model distinguish their role. For non-
retrieval tasks, we use its query representations.
We benchmark publicly available SGPT models
based on GPT-NeoX (Andonian et al., 2021), GPT-
J (Wang and Komatsuzaki, 2021) and BLOOM
(Scao et al., 2022). Alternatively, cpt-text (Nee-
lakantan et al., 2022) passes pre-trained GPT de-
coders through a two-stage process using last token
pooling to provide embeddings from decoders. We
benchmark their models via the OpenAI Embed-
dings API4.
(c) Non-transformer LASER (Heffernan et al.,
2022) is the only context aware non-transformer
model we benchmark, relying on an LSTM
4https://beta.openai.com/docs/guides/
embeddings



Source: data\tc18_2309.07597v5\referenced_papers\[32]_2202.08904.pdf (Page 7):

Dataset (→) AskU CQA TwitterP SciDocs Avg Quora STS-B
Method (↓) TURL PIT Avg Cite CC CR CV Avg
OOD Unsupervised
BM25♦ 53.4 13.3 71.9 70.5 71.2 58.9 61.3 67.3 66.9 63.6 50.4 80.8†
OOD Unsupervised + OOD Supervised (NLI)
SBERT-base-nli-v2-bs64 52.8 11.6 75.5 71.5 73.5 68.0 70.6 71.1 73.5 70.8 52.2 78.9 83.9
SBERT-base-nli-v2-bf-bs64 53.8 11.7 76.6 72.9 74.8 67.5 70.6 70.8 73.0 70.5 52.7 78.8 81.8
SGPT-0.1B-weightedmean-nli-bs64 54.9 11.2 72.7 66.0 69.3 66.2 68.9 68.9 71.7 68.9 51.1 79.5 81.0
SGPT-0.1B-weightedmean-nli-bf-bs64 54.9 10.8 72.3 65.3 68.8 64.7 67.4 68.0 70.8 67.8 50.6 77.5 78.6
SGPT-0.1B-weightedmean-nli-bf-bs1024 55.7 11.1 72.8 66.5 69.6 65.1 67.8 68.6 70.5 68.0 51.1 79.0 79.5
SGPT-1.3B-weightedmean-nli-bf-bs1024 56.0 13.5 75.4 70.7 73.1 70.1 72.9 73.2 75.0 72.8 53.8 82.3 83.9
SGPT-2.7B-weightedmean-nli-bf-bs1024 57.5 14.0 75.8 71.0 73.4 72.3 75.4 74.7 76.5 74.7 54.9 82.6 84.7
SGPT-5.8B-weightedmean-nli-bf-bs1024 57.1 16.0 76.5 76.0 76.3 75.0 78.2 77.1 78.3 77.2 56.6 84.7 85.7
OOD Unsupervised + OOD Unsupervised + OOD Supervised (NLI, [27])
Ada Similarity (Mar 2022) 55.9 13.6 76.6 76.2 76.4 66.7 71.0 71.3 73.7 70.7 54.1 82.2
Curie Similarity (Mar 2022) 57.3 15.8 76.3 78.9 77.6 66.3 71.7 71.3 73.0 70.6 55.3 83.1
Davinci Similarity (June 2022) 55.9 76.3 78.0 77.1
Table 5: Results on USEB, Quora and STS-B. Metrics are average precision for USEB, nDCG@10
for Quora and Spearman correlation for STS-B. bf=BitFit. bs=Batch Size. OOD=Out-of-domain,
to contrast these numbers from in-domain numbers in [49]. However, fragments may be in-domain
due to the large pre-training data of the transformer models. SGPT-0.1B-weightedmean-nli performs
2% worse than SBERT-base-nli-v2 on USEB, but improves on Quora by 1%. Note that there is still a
size difference of 14% between the two models. ♦: Results from [49] except when marked with †.
CQADupstack and SciDocs differ from the same-name datasets in BEIR.
batch size of 1024. In Appendix §A, we provide results using a lower batch size and additional
ablations. Results for Ada and Curie were obtained by querying the OpenAI Similarity Embeddings
API in March 2022. They correspond to the cpt-text similarity models from [27] and we provide their
parameters in Table 2.
4.2 Asymmetric Search
4.2.1 Method
If not otherwise speciﬁed, we follow the same setup as in §4.1.1. For asymmetric search, we train
on MS-MARCO [28]. We limit the model sequence length to 300 tokens during both training and
inference. We follow concurrent work [27] and add enclosing brackets to help the model distinguish
between query and document. We embed the tokens of query q in two brackets as [q0−n]. For
documents, we use curly brackets: {d0−n}. We add the token ids of the brackets to the already
tokenized text to avoid the tokens intermingling. We refer to these special brackets as specb.
4.2.2 Results
Table 6 benchmarksSGPT-BE-5.8B(SGPT-5.8B-weightedmean-msmarco-specb-bitﬁt) on BEIR [44]
with: (a) BM25 [41], a non-semantic fast baseline (b) SGPT-CE-6.1B from §3 (c) BM25+CE [44],
the current overall state-of-the-art on BEIR (d) TAS-B [17], the original Bi-Encoder state-of-the-art
on BEIR (e) Contriever [20], a similar training scheme as [ 27] but using an encoder transformer
(f ) GTR-XXL [29], the current Bi-Encoder state-of-the-art on BEIR with 4.8 billion parameters
using the BERT-like encoder transformer of T5 [38] (g) cpt-text, a GPT-like decoder transformer
architecture concurrently proposed in [27]. Corresponding parameter estimates are in Table 2.
SGPT-5.8B achieves the best average nDCG@10 both on the BEIR subset selected in [27] and on the
full BEIR benchmark. It outperforms the roughly same-sized cpt-text-L and the 30x larger cpt-text-XL
by 8.1% and 4.2%, respectively. Yet, cpt-text models have gone through an additional unsupervised
training stage [27] and are fully trained. SGPT-BE-5.8B ﬁne-tunes just 700K parameters, 0.0004%
of the parameters ﬁne-tuned for cpt-text-XL [ 27]. See Table 2 for sizes. We suspect much of
the difference to come from the cpt-text model’s inferior last token pooling as shown in Figure
3. Further, we suspect that the beneﬁts of the additional unsupervised contrastive pre-training
stage diminish when followed by supervised contrastive ﬁne-tuning. SGPT-BE-5.8B improves on
the overall state-of-the-art, a Cross-Encoder, by 3%. It improves on the previously best sentence
embeddings (Bi-Encoder) on BEIR, GTR-XXL, by 7%. However, these improvements come at a
signiﬁcant cost. GTR-XXL has 20% fewer parameters and its embeddings have 768 dimensions.
SGPT-BE-5.8B produces embeddings with 4096 dimensions, hence requiring about 5x more storage.
It took the model six days on one Nvidia A100 GPU to encode the entire BioASQ corpus with 15M
8



Source: data\tc18_2309.07597v5\referenced_papers\[32]_2202.08904.pdf (Page 14):

Model ( → ) [41] SGPT-BE
Dataset ( ↓ ) BM25 125M 1.3B 2.7B
MS MARCO 0.228 0.279 0.361 0.388
TREC-COVID 0.688 0.738 0.785 0.807
BioASQ 0.488 0.272 0.347 0.384
NFCorpus 0.306 0.228 0.321 0.339
NQ 0.326 0.297 0.430 0.467
HotpotQA 0.602 0.409 0.499 0.528
FiQA-2018 0.254 0.211 0.300 0.333
Signal-1M (RT) 0.330 0.236 0.250 0.249
TREC-NEWS 0.405 0.319 0.424 0.438
Robust04 0.425 0.313 0.421 0.449
ArguAna 0.472 0.455 0.497 0.505
Touché-2020 0.347 0.230 0.245 0.235
CQADupStack 0.326 0.249 0.320 0.349
Quora 0.808 0.730 0.853 0.856
DBPedia 0.320 0.227 0.315 0.347
SCIDOCS 0.165 0.121 0.161 0.165
FEVER 0.649 0.605 0.682 0.728
Climate-FEVER 0.186 0.218 0.266 0.272
SciFact 0.611 0.569 0.683 0.702
Sub-Average 0.477 0.420 0.495 0.514
Average 0.428 0.357 0.433 0.453
Table 8: Additional SGPT Bi-Encoder scores on BEIR. Scores are nDCG@10. Average scores do
not include MS MARCO.
15



#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 0):

Text Embeddings by Weakly-Supervised
Contrastive Pre-training
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao
Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei
Microsoft Corporation
https://github.com/microsoft/unilm
Abstract
This paper presents E5 1, a family of state-of-the-art text embeddings that transfer
well to a wide range of tasks. The model is trained in a contrastive manner with
weak supervision signals from our curated large-scale text pair dataset (called
CCPairs). E5 can be readily used as a general-purpose embedding model for any
tasks requiring a single-vector representation of texts such as retrieval, clustering,
and classification, achieving strong performance in both zero-shot and fine-tuned
settings. We conduct extensive evaluations on 56 datasets from the BEIR and
MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms
the strong BM25 baseline on the BEIR retrieval benchmark without using any
labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark,
beating existing embedding models with 40× more parameters.
1 Introduction
Text embeddings are low-dimensional vector representations for arbitrary-length texts and play key
roles in many NLP tasks such as large-scale retrieval. Compared to the high-dimensional and sparse
representations like TF-IDF, text embeddings have the potential to overcome the lexical mismatch
issue and facilitate efficient retrieval and matching between texts. It also offers a versatile interface
easily consumable by downstream applications.
While pre-trained language models such as BERT [ 17] and GPT [ 7] can produce transferrable
text representations, they are not ideal for tasks such as retrieval and text matching where a single-
vector embedding of texts is more desired due to its efficiency and versatility. To obtain better
text embeddings, contrastive learning is often the go-to framework to enhance the sequence-level
representations from text pairs. Along this line of research, some works are geared towards learning
task-specific embeddings. For example, GTR [ 43] and Sentence-T5 [ 44] fine-tune pre-trained
models with supervised datasets to learn embeddings customized for passage retrieval and semantic
textual similarity, respectively. Other works learn unsupervised embeddings from automatically
constructed text pairs. Typical methods to construct text pairs include Inverse Close Task (ICT)
[9], random cropping [ 28] and neighboring text spans [ 41], etc. While such synthetic data are
of unlimited quantity, they are often poor in quality and the resulted embeddings fail to match the
performance of the classic BM25 baseline without further fine-tuning [40].
In this work, we learn a high-quality general-purpose text embedding termed E5, EmbEddings from
bidirEctional Encoder rEpresentations. E5 aims to provide strong off-the-shelf text embeddings
suitable for any tasks requiring single-vector representations in both zero-shot or fine-tuned settings.
To achieve this goal, instead of relying on limited labeled data or low-quality synthetic text pairs, we
contrastively train E5 embeddings from CCPairs, a curated web-scale text pair dataset containing
1E5: EmbEddings from bidirEctional Encoder rEpresentations
Work in progress.
arXiv:2212.03533v2  [cs.CL]  22 Feb 2024



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 7):

Table 5: Impacts of different batch sizes for contrastive pre-training.
batch size NFCorpus NQ FiQA Quora DBPedia Scifact Avg
32k 35.8 39.0 40.0 85.7 35.4 73.7 51.6
8k 33.3 38.5 37.6 85.7 34.0 71.8 50.2
1k 28.2 33.1 30.4 84.0 30.1 69.1 45.8
Table 5, increasing batch size from 1K to 32K leads to consistent gains across all 6 datasets. It is also
possible to train with smaller batch sizes by adding hard negatives [ 50]. However, the engineering
efforts of mining hard negatives for large datasets (>100M) are non-trivial.
Table 6: Fine-tuning with different combinations of labeled data.
Fine-tuned on Retrieval STS Classification Summ. MTEB Avg
No fine-tuning 42.9 69.5 67.9 31.1 55.6
MS-MARCO + NQ 50.3 78.3 68.3 30.6 59.0
NLI 38.3 81.1 72.6 31.6 57.3
All above 48.7 81.0 73.1 31.0 60.4
Fine-tuning Datasets GTR models are fine-tuned with “MS-MARCO + NQ”, while Sentence-T5
models use NLI instead. In Table 6, we can see that the “MS-MARCO + NQ” setting performs best
on retrieval tasks, and the NLI data is beneficial for STS and linear probing classification. Similar
observations are also made by Muennighoff et al. [40]. Combining all of them leads to the best
overall scores on the MTEB benchmark. This also illustrates the importance of dataset diversity for
learning text embeddings.
Table 7: Data filtering. For the top 2 rows, we train with 1M random text pairs.
# of pairs NFCorpus NQ FiQA Quora DBPedia Scifact Avg
1M w/o filter 23.0 15.1 18.5 83.1 18.2 51.4 34.9
w/ filter 26.8 22.7 24.5 85.0 27.5 57.5 40.7
All w/o filter 34.5 35.4 39.1 85.7 32.9 72.5 50.0
w/ filter 35.8 39.0 40.0 85.7 35.4 73.7 51.6
Data Filtering One crucial step in our dataset curation pipeline is filtering out low-quality text pairs.
In Table 7, when training with 1M pairs, using filtered data has a nearly 6 points advantage. When
all the text pairs are used, the “w/o filter” setting has about 4× more data but is still behind by 1.6
points. Though recent studies [ 29, 47] show that deep learning models are quite robust to dataset
noises, data filtering still has benefits in improving training efficiency and model quality.
Negative Sampling We explore two alternative methods to enlarge the number of negatives: Pre-
batch negatives [ 33] reuse embeddings from previous batches as additional negatives, while MoCo
[25] introduces a momentum encoder and uses a FIFO queue to store negatives. For both approaches,
the negative size can be easily scaled up without incurring much GPU memory overhead. The
downside is that most negatives are produced by an older version of model parameters. In Table 8,
in-batch negatives still perform favorably. Empirically, we find that MoCo is more sensitive to certain
hyperparameters such as temperature, better results are possible with more tuning.
BM25 vs Dense Retrieval With the rapid development of dense retrieval models, can we replace
the long-standing BM25 algorithm from now on? The answer is likely “not yet”. BM25 still holds
obvious advantages in terms of simplicity, efficiency, and interpretability. For long-tail domains such
as Trec-Covid [ 55] and retrieval tasks that involve long documents (Touche-2020) [4] or rely heavily
on exact lexical match (Fever) [ 54], further research efforts are still necessary to improve current
dense retrievers.
6 Conclusion
In this work, we train a general-purpose text embedding model E5 from weak supervision signals. We
adopt a simple contrastive training framework with in-batch negatives and learn from a large-scale text
pair dataset we harvest from heterogeneous data sources across the web. E5 offers strong off-the-shelf
8



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 16):

Table 13: Results for each dataset in the MTEB benchmark [ 40]. The numbers for the Retrieval
category are not included here since the datasets are the same as the BEIR benchmark.
unsupervised supervised
E5-PTsmall E5-PTbase E5-PTlarge E5small E5base E5large
AmazonCounterfactualClassification 71.7 73.6 70.4 76.2 79.7 77.7
AmazonPolarityClassification 76.1 77.0 83.2 87.5 88.0 90.1
AmazonReviewsClassification 35.0 35.8 37.4 42.6 42.7 43.0
Banking77Classification 82.1 82.9 83.5 81.9 83.3 84.1
EmotionClassification 42.2 44.2 43.5 46.9 49.4 48.1
ImdbClassification 67.9 67.3 77.7 75.6 76.0 82.1
MassiveIntentClassification 70.2 71.1 70.8 72.2 72.3 73.2
MassiveScenarioClassification 74.6 75.4 75.9 75.8 76.8 77.4
MTOPDomainClassification 91.3 92.3 93.2 92.1 93.2 93.9
MTOPIntentClassification 71.9 74.0 74.2 73.2 74.8 76.4
ToxicConversationsClassification 67.0 67.4 66.1 72.8 74.1 70.6
TweetSentimentExtractionClass. 54.4 53.3 52.5 63.3 61.4 61.2
ArxivClusteringP2P 47.9 49.3 49.4 44.1 44.6 46.2
ArxivClusteringS2S 39.9 42.8 43.6 37.1 40.5 41.4
BiorxivClusteringP2P 38.5 38.8 39.2 35.8 36.2 37.6
BiorxivClusteringS2S 35.4 36.5 36.7 31.9 32.7 35.1
MedrxivClusteringP2P 34.4 33.7 33.3 31.3 31.5 32.3
MedrxivClusteringS2S 32.0 32.1 32.2 28.2 28.3 29.7
RedditClustering 46.9 49.3 52.4 42.9 48.2 50.7
RedditClusteringP2P 60.2 64.4 64.6 56.4 62.2 61.4
StackExchangeClustering 57.7 60.2 63.3 59.1 63.9 65.0
StackExchangeClusteringP2P 32.0 34.0 34.7 30.3 32.6 33.6
TwentyNewsgroupsClustering 34.4 36.2 37.9 37.5 42.6 43.8
SprintDuplicateQuestions 91.6 90.8 92.0 95.3 94.9 95.4
TwitterSemEval2015 60.0 62.8 64.7 74.2 74.4 76.1
TwitterURLCorpus 83.2 84.0 84.1 85.8 86.0 86.3
AskUbuntuDupQuestions 57.8 57.6 58.3 59.4 59.7 60.1
MindSmallReranking 29.0 29.6 29.2 29.6 30.1 30.8
SciDocsRR 81.1 82.6 84.3 79.8 82.9 83.9
StackOverflowDupQuestions 44.4 44.2 45.8 49.1 50.1 51.3
BIOSSES 69.2 71.9 69.7 84.2 85.1 84.7
SICK-R 66.6 68.7 69.7 78.9 79.7 80.5
STS12 60.7 57.9 54.7 75.2 74.2 75.9
STS13 71.1 73.5 74.0 81.8 83.3 85.2
STS14 64.2 64.0 65.3 78.5 78.5 80.5
STS15 74.3 75.4 75.8 87.5 88.4 88.8
STS16 76.6 79.8 80.1 84.6 84.2 85.3
STS17 78.3 77.2 76.0 87.9 87.2 89.4
STS22 59.2 56.2 62.8 63.8 62.9 63.0
STSBenchmark 67.7 70.5 70.9 86.4 86.2 87.2
SummEval 32.7 31.1 32.6 31.4 31.0 31.0
17



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 4):

Zero-shot Text Classification The input and label texts are converted to sentences based on manually
written prompt templates. The predicted label is the one closest to the input text in the embedding
space. Take the sentiment classification of movie reviews as an example, with the original input “I
enjoy watching it”, the label text is “it is an example of terrible/great movie review” and the input text
becomes “movie review: I enjoy watching it”.
Semantic Textual Similarity Given two text embeddings, we use the cosine function to measure
their semantic similarity. Since the absolute similarity scores do not enable an easy interpretation, the
evaluation is usually based on rank correlation coefficients.
Text Clustering Standard clustering algorithms such as k-means can be applied straightforwardly.
Texts belonging to the same category are expected to be close in the embedding space.
For tasks other than zero-shot text classification and retrieval, we use the query embeddings by
default.
5 Experiments
5.1 Pre-training and Fine-tuning Configurations
Pre-training We pre-train on our proposed text pair dataset for three model sizes: E5small, E5base
and E5large initialized from MiniLM [ 59], bert-base-uncased, and bert-large-uncased-whole-word-
masking respectively. The batch size is set to a large value of 32, 768 to increase the number of
negatives. The learning rate is {3, 2, 1}×10−4 for the {small, base, large} models, with linear decay
and the first 1, 000 steps for warmup. We pre-train for 20k steps in total with AdamW optimizer,
which is approximately 2.5 epochs over the dataset. It takes {16, 32, 64} V100 GPUs and {1, 1, 2}
days for the {small, base, large} models. To improve training efficiency and reduce GPU memory
usage, we adopt mixed precision training and gradient checkpointing.
Fine-tuning is performed on the concatenation of 3 datasets: MS-MARCO passage ranking [ 8],
NQ [ 32, 30], and NLI [ 22] datasets. We reuse the mined hard negatives and re-ranker scores from
SimLM [ 58] for the first two datasets. Models are fine-tuned for 3 epochs with batch size 256 on 8
GPUs. Learning rate is {3, 2, 1}×10−5 for the {small, base, large} models with 400 steps warmup.
For each example, we use 7 hard negatives. Since the NLI dataset only has 1 hard negative for each
example, 6 sentences are randomly sampled from the entire corpus.
We use E5-PT to denote models with contrastive pre-training only. More implementation details can
be found in Appendix B.
5.2 Evaluation Datasets
BEIR Benchmark [ 53] is a collection of 19 information retrieval datasets, ranging across ad-hoc
web search, question answering, fact verification and duplicate question retrieval, etc. We evaluate
the 15 datasets that provide public downloads. The main metric is nDCG@10.
MTEB Benchmark [ 40] is recently proposed for benchmarking massive text embedding tasks.
Though MTEB is multilingual due to the inclusion of bitext mining datasets, most datasets are
still only available in English. In this paper, we evaluate the English subsets, which have 56
datasets spanning across 6 categories: Classification (Class.), Clustering (Clust.), Pair Classification
(PairClass.), Rerank, Retrieval (Retr.), STS, and Summarization (Summ.). The evaluation metrics are
accuracy, v-measure, average precision, MAP, nDCG@10, and Spearman coefficients, respectively.
Please refer to the MTEB paper for details.
5.3 Results on BEIR benchmark
Results with Unsupervised Methods In Table 1, we show model results that do not use any labeled
data. When averaged over all 15 datasets, E5-PTbase outperforms the classic BM25 algorithm by 1.2
points. To the best of our knowledge, this is the first reported result that an unsupervised model can
beat BM25 on the BEIR benchmark. When scaling up to E5-PT large, we see further benefits from
42.9 to 44.2.
5



Source: data\tc18_2309.07597v5\referenced_papers\[28]_2308.03281.pdf (Page 6):

Trec-COVIDNFCorpus
Tóuche-2020
DBPediaScidocs
Climate-fever
HotpotQA
FiQA
CQADupStackMSMARCO
NQ FeverScifactArguAna
Quora
Avg.0
50
100Recall@100
SimCSE Contriever BM25 GTE
Figure 2: Recall@100 of unsupervised text retrieval methods on BEIR benchmark (Thakur et al., 2021). We
compare our model GTEbase (based on BERTbase) without using any annotated data to SimCSE (Gao et al., 2021)
(based on RoBERTalarge), Contriever (Izacard et al., 2022a) (based on BERTbase) and BM25. Baseline results are
borrowed from the Contriever paper (Izacard et al., 2022a) with dot product being the similarity function.
Dataset BM25 SimCSE Contriever CPT-S E5 small E5base E5large GTEsmall GTEbase GTElarge
MS MARCO 22.8 9.4 20.6 19.9 25.4 26.0 26.2 31.3 31.8 31.7
Trec-Covid 65.6 26.2 27.4 52.9 52.0 61.0 61.8 61.8 64.0 64.8
NFCorpus 32.5 9.9 31.7 32.0 29.3 35.8 33.7 34.9 36.2 38.1
NQ 32.9 11.7 25.4 - 37.3 39.0 41.7 32.0 35.3 34.5
HotpotQA 60.3 19.8 48.1 51.5 46.0 52.4 52.2 49.3 50.8 49.2
FiQA 23.6 9.8 24.5 34.1 38.3 40.0 43.2 37.0 36.9 40.6
ArguAna 31.5 38.3 37.9 38.7 42.5 42.2 44.4 41.6 41.0 41.3
Touche-2020 36.7 8.9 19.3 21.0 19.9 16.9 19.8 17.7 18.2 18.5
CQADupStack 29.9 13.2 28.4 - 35.0 35.4 38.9 38.1 39.9 39.8
Quora 78.9 78.0 83.5 68.1 85.8 85.7 86.1 86.1 85.0 84.8
DBPedia 31.3 15.0 29.2 27.2 34.5 35.4 37.1 33.5 33.2 33.6
Scidocs 15.8 5.5 14.9 - 19.9 21.1 21.8 21.5 22.5 22.7
Fever 75.3 21.1 68.2 57.1 62.5 63.4 68.6 71.3 72.7 70.5
Climate-Fever 21.3 11.8 15.5 15.8 14.5 15.4 15.7 21.4 21.0 25.4
Scifact 66.5 25.7 64.9 65.4 68.5 73.7 72.3 72.7 74.1 74.1
Average 41.7 20.3 36.0 - 40.8 42.9 44.2 43.4 44.2 44.6
Table 5: nDCG@10 of different unsupervised methods on the BEIR benchmark (Thakur et al., 2021). SimCSE is
based on BERTbase backbone. CPT-S (Neelakantan et al., 2022) is of similar size to BERTlarge. Baseline results are
borrowed from E5 paper (Wang et al., 2022b). Note that Contriever uses dot product as the similarity metric while
other models uses cosine similarity.
on the tasks covered in the MTEB benchmark,
please refer to the Appendix B.
Two settings are considered for comparison: the
unsupervised setting and the supervised setting. In
the unsupervised setting, models are trained us-
ing unlabeled data, while supervised models are
fine-tuned using high-quality datasets with human
labels. The results of strong baseline models are
presented in Table 6.
In the unsupervised setting, our model outper-
forms the previous best model, E5, by a signifi-
cant margin across all considered tasks, without
the use of task-specific prompts. This improve-
ment can be attributed to the inclusion of more
training data formats and various sources of self-
supervision signals. Furthermore, it is worth noting
that our unsupervised pre-trained model narrows
the gap even further with larger supervised base-
lines, such as GTR and Sentence-T5. In the super-
vised setting, our model surpasses OpenAI results



#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[29]_1907.11692.pdf (Page 11):

Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. InAdvances in neural information pro-
cessing systems .
Alex W ang, Y ada Pruksachatkun, Nikita Nangia,
Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, and Samuel R. Bowman. 2019a. SuperGLUE:
A stickier benchmark for general-purpose language
understanding systems.arXiv preprint 1905.00537 .
Alex W ang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019b.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. InInter-
national Conference on Learning Representations
(ICLR).
Alex W arstadt, Amanpreet Singh, and Samuel R. Bow-
man. 2018. Neural network acceptability judg-
ments.arXiv preprint 1805.12471 .
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. InNorth
American Association for Computational Linguis-
tics (NAACL).
Zhilin Y ang, Zihang Dai, Y iming Y ang, Jaime Car-
bonell, Ruslan Salakhutdinov, and Quoc V Le.
2019. Xlnet: Generalized autoregressive pretrain-
ing for language understanding.arXiv preprint
arXiv:1906.08237 .
Y ang Y ou, Jing Li, Jonathan Hseu, Xiaodan Song,
James Demmel, and Cho-Jui Hsieh. 2019. Reduc-
ing bert pre-training time from 3 days to 76 minutes.
arXiv preprint arXiv:1904.00962.
Rowan Zellers, Ari Holtzman, Hannah Rashkin,
Y onatan Bisk, Ali Farhadi, Franziska Roesner, and
Y ejin Choi. 2019. Defending against neural fake
news.arXiv preprint arXiv:1905.12616 .
Y ukun Zhu, Ryan Kiros, Richard Zemel, Ruslan
Salakhutdinov, Raquel Urtasun, Antonio T orralba,
and Sanja Fidler. 2015. Aligning books and movies:
T owards story-like visual explanations by watch-
ing movies and reading books. InarXiv preprint
arXiv:1506.06724 .
Appendix for “RoBERT a: A Robustly
Optimized BERT Pretraining Approach”
A Full results on GLUE
In T able
8 we present the full set of development
set results for RoBERT a. W e present results for
aLA R G E conﬁguration that follows BERT L A R G E,
as well as a BA SE conﬁguration that follows
BERTBA S E .
B Pretraining Hyperparameters
T able
9 describes the hyperparameters for pre-
training of RoBERT a L A R G E and RoBERT aBA S E
C Finetuning Hyperparameters
Finetuning hyperparameters for RACE, SQuAD
and GLUE are given in T able
10. W e select the
best hyperparameter values based on the median
of 5 random seeds for each task.



Source: data\tc18_2309.07597v5\referenced_papers\[32]_2202.08904.pdf (Page 11):

[34] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong,
Hua Wu, and Haifeng Wang. 2020. RocketQA: An optimized training approach to dense passage
retrieval for open-domain question answering. arXiv preprint arXiv:2010.08191.
[35] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving
language understanding by generative pre-training.
[36] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.
Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.
[37] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,
John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language
models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446.
[38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a
uniﬁed text-to-text transformer. arXiv preprint arXiv:1910.10683.
[39] Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese
bert-networks. arXiv preprint arXiv:1908.10084.
[40] Nils Reimers, Benjamin Schiller, Tilman Beck, Johannes Daxenberger, Christian Stab, and
Iryna Gurevych. 2019. Classiﬁcation and clustering of arguments with contextualized word
embeddings. arXiv preprint arXiv:1906.09821.
[41] Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25
and beyond. Now Publishers Inc.
[42] Timo Schick and Hinrich Schütze. 2021. Generating Datasets with Pretrained Language Models.
arXiv preprint arXiv:2104.07540.
[43] Nandan Thakur, Nils Reimers, Johannes Daxenberger, and Iryna Gurevych. 2020. Augmented
sbert: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks.
arXiv preprint arXiv:2010.08240.
[44] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021.
BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models.
arXiv preprint arXiv:2104.08663.
[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural
information processing systems, pages 5998–6008.
[46] Ellen V oorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R Hersh,
Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2021. TREC-COVID: constructing a
pandemic information retrieval test collection. In ACM SIGIR Forum, volume 54, pages 1–12.
ACM New York, NY , USA.
[47] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose
language understanding systems. Advances in neural information processing systems, 32.
[48] Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive
Language Model. https://github.com/kingoflolz/mesh-transformer-jax.
[49] Kexin Wang, Nils Reimers, and Iryna Gurevych. 2021. TSDAE: Using Transformer-based
Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning. arXiv
preprint arXiv:2104.06979.
[50] Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and
Jianfeng Gao. 2021. LiST: Lite Self-training Makes Efﬁcient Few-shot Learners. arXiv preprint
arXiv:2110.06274.
[51] Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge
corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426.
12



Source: data\tc18_2309.07597v5\referenced_papers\[15]_1810.04805.pdf (Page 6):

System Dev Test
EM F1 EM F1
Top Leaderboard Systems (Dec 10th, 2018)
Human - - 82.3 91.2
#1 Ensemble - nlnet - - 86.0 91.7
#2 Ensemble - QANet - - 84.5 90.5
Published
BiDAF+ELMo (Single) - 85.6 - 85.8
R.M. Reader (Ensemble) 81.2 87.9 82.3 88.5
Ours
BERTBASE (Single) 80.8 88.5 - -
BERTLARGE (Single) 84.1 90.9 - -
BERTLARGE (Ensemble) 85.8 91.8 - -
BERTLARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8
BERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2
Table 2: SQuAD 1.1 results. The BERT ensemble
is 7x systems which use different pre-training check-
points and ﬁne-tuning seeds.
System Dev Test
EM F1 EM F1
Top Leaderboard Systems (Dec 10th, 2018)
Human 86.3 89.0 86.9 89.5
#1 Single - MIR-MRC (F-Net) - - 74.8 78.0
#2 Single - nlnet - - 74.2 77.1
Published
unet (Ensemble) - - 71.4 74.9
SLQA+ (Single) - 71.4 74.4
Ours
BERTLARGE (Single) 78.7 81.9 80.0 83.1
Table 3: SQuAD 2.0 results. We exclude entries that
use BERT as one of their components.
tuning data, we only lose 0.1-0.4 F1, still outper-
forming all existing systems by a wide margin.12
4.3 SQuAD v2.0
The SQuAD 2.0 task extends the SQuAD 1.1
problem deﬁnition by allowing for the possibility
that no short answer exists in the provided para-
graph, making the problem more realistic.
We use a simple approach to extend the SQuAD
v1.1 BERT model for this task. We treat ques-
tions that do not have an answer as having an an-
swer span with start and end at the [CLS] to-
ken. The probability space for the start and end
answer span positions is extended to include the
position of the [CLS] token. For prediction, we
compare the score of the no-answer span: snull =
S·C+ E·C to the score of the best non-null span
12The TriviaQA data we used consists of paragraphs from
TriviaQA-Wiki formed of the ﬁrst 400 tokens in documents,
that contain at least one of the provided possible answers.
System Dev Test
ESIM+GloVe 51.9 52.7
ESIM+ELMo 59.1 59.2
OpenAI GPT - 78.0
BERTBASE 81.6 -
BERTLARGE 86.6 86.3
Human (expert)† - 85.0
Human (5 annotations)† - 88.0
Table 4: SW AG Dev and Test accuracies.†Human per-
formance is measured with 100 samples, as reported in
the SW AG paper.
ˆsi,j = maxj≥iS·Ti + E·Tj. We predict a non-null
answer when ˆsi,j > snull + τ, where the thresh-
old τ is selected on the dev set to maximize F1.
We did not use TriviaQA data for this model. We
ﬁne-tuned for 2 epochs with a learning rate of 5e-5
and a batch size of 48.
The results compared to prior leaderboard en-
tries and top published work (Sun et al., 2018;
Wang et al., 2018b) are shown in Table 3, exclud-
ing systems that use BERT as one of their com-
ponents. We observe a +5.1 F1 improvement over
the previous best system.
4.4 SWAG
The Situations With Adversarial Generations
(SW AG) dataset contains 113k sentence-pair com-
pletion examples that evaluate grounded common-
sense inference (Zellers et al., 2018). Given a sen-
tence, the task is to choose the most plausible con-
tinuation among four choices.
When ﬁne-tuning on the SW AG dataset, we
construct four input sequences, each containing
the concatenation of the given sentence (sentence
A) and a possible continuation (sentence B). The
only task-speciﬁc parameters introduced is a vec-
tor whose dot product with the [CLS] token rep-
resentation C denotes a score for each choice
which is normalized with a softmax layer.
We ﬁne-tune the model for 3 epochs with a
learning rate of 2e-5 and a batch size of 16. Re-
sults are presented in Table 4. BERT LARGE out-
performs the authors’ baseline ESIM+ELMo sys-
tem by +27.1% and OpenAI GPT by 8.3%.
5 Ablation Studies
In this section, we perform ablation experiments
over a number of facets of BERT in order to better
understand their relative importance. Additional



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 9):

[9] Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. Pre-training
tasks for embedding-based large-scale retrieval. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
URL https://openreview.net/forum?id=rkg-mA4FDr.
[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework
for contrastive learning of visual representations. In Proceedings of the 37th International
Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119
of Proceedings of Machine Learning Research, pages 1597–1607. PMLR, 2020. URL http:
//proceedings.mlr.press/v119/chen20j.html.
[11] Xilun Chen, Kushal Lakhotia, Barlas O ˘guz, Anchit Gupta, Patrick Lewis, Stan Peshterliev,
Yashar Mehdad, Sonal Gupta, and Wen-tau Yih. Salient phrase aware dense retrieval: Can a
dense retriever imitate a sparse one? arXiv preprint arXiv:2110.06918, 2021.
[12] Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld. Specter:
Document-level representation learning using citation-informed transformers. In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2270–2282,
2020.
[13] Alexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence
representations. In Proceedings of the Eleventh International Conference on Language Re-
sources and Evaluation (LREC 2018), Miyazaki, Japan, 2018. European Language Resources
Association (ELRA). URL https://aclanthology.org/L18-1269.
[14] Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. Super-
vised learning of universal sentence representations from natural language inference data. In
Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,
pages 670–680, Copenhagen, Denmark, 2017. Association for Computational Linguistics. doi:
10.18653/v1/D17-1070. URL https://aclanthology.org/D17-1070.
[15] Zhuyun Dai, Vincent Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu,
Keith B. Hall, and Ming-Wei Chang. Promptagator: Few-shot dense retrieval from 8 examples.
ArXiv, abs/2209.11755, 2022.
[16] Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard
Harshman. Indexing by latent semantic analysis.Journal of the American society for information
science, 41(6):391–407, 1990.
[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer-
ence of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,
Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.
URL https://aclanthology.org/N19-1423.
[18] Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, and Markus
Leippold. Climate-fever: A dataset for verification of real-world climate claims. arXiv preprint
arXiv:2012.00614, 2020.
[19] Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering
the long tail via influence estimation. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/
paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html.
[20] Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Language-
agnostic bert sentence embedding. In Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pages 878–891, 2022.
[21] Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,
Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The
pile: An 800gb dataset of diverse text for language modeling. ArXiv, abs/2101.00027, 2021.
10



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 11):

[32] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris
Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion
Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav
Petrov. Natural questions: A benchmark for question answering research. Transactions of the
Association for Computational Linguistics, 7:452–466, 2019. doi: 10.1162/tacl_a_00276. URL
https://aclanthology.org/Q19-1026.
[33] Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. Learning dense representations of
phrases at scale. InProceedings of the 59th Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 6634–6647, Online, 2021. Association for Computational
Linguistics. doi: 10.18653/v1/2021.acl-long.518. URL https://aclanthology.org/2021.
acl-long.518.
[34] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris
Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models
better. In ACL, 2022.
[35] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. ArXiv, abs/1907.11692, 2019.
[36] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC:
The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics , pages 4969–4983, Online, 2020. Associ-
ation for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.447. URL https:
//aclanthology.org/2020.acl-main.447.
[37] Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott, Manel
Zarrouk, and Alexandra Balahur. Www’18 open challenge: financial opinion mining and
question answering. In Companion proceedings of the the web conference 2018, pages 1941–
1942, 2018.
[38] Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Efficient estimation of word
representations in vector space. In ICLR, 2013.
[39] Niklas Muennighoff. Sgpt: Gpt sentence embeddings for semantic search. ArXiv,
abs/2202.08904, 2022.
[40] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. Mteb: Massive text
embedding benchmark. ArXiv, abs/2210.07316, 2022.
[41] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek,
Qiming Yuan, Nikolas A. Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav
Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David P. Schnurr,
Felipe Petroski Such, Kenny Sai-Kin Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov,
Joanne Jang, Peter Welinder, and Lilian Weng. Text and code embeddings by contrastive pre-
training. ArXiv, abs/2201.10005, 2022.
[42] Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi-Phuong-Nhung Ngo, Thi Hoai Phuong
Nguyen, Laura Beggel, and Thomas Brox. SELF: learning to filter noisy labels with self-
ensembling. In 8th International Conference on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020. URL https://openreview.
net/forum?id=HkgsPhNYPS.
[43] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern’andez ’Abrego, Ji Ma, Vincent
Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are
generalizable retrievers. ArXiv, abs/2112.07899, 2021.
[44] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and
Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In
Findings of the Association for Computational Linguistics: ACL 2022, pages 1864–1874, 2022.
12



### Claim 18/38

#### Claim Text
In our implementation, we use a compound strategy of gradient checkpointing and cross-device embedding sharing [18], which results in a maximum batch size of 19,.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[18]_2101.06983.pdf (Page 4):

twice the amount of time than accumulation3.
5 Extend to Deep Distance Function
Previous discussion assumes a simple parameter-
less dot product similarity. In general it can also be
deep distance function Φ richly parameterized by
Ω, formally,
dij = d(si,tj) = Φ(f(si),g(tj)) (10)
This can still scale by introducing an extraDistance
Gradient Cache. In the ﬁrst forward we collect
all representations as well as all distances. We
compute loss with dijs and back propagate to get
wij = ∂L
∂dij
, and store them in Distance Gradient
Cache, [w00,w01,..,w 10,..]. We can then update
Ω in a sub-batch manner,
∂L
∂Ω =
∑
ˆS∈S
∑
ˆT∈T
∑
si∈ˆS
∑
tj ∈ ˆT
wij
∂Φ(f(si),g(tj))
∂Ω
(11)
Additionally, we simultaneously compute with the
constructed computation graph ∂dij
∂f (si) and ∂dij
∂g(tj )
and accumulate across batches,
ui = ∂L
∂f(si) =
∑
j
wij
∂dij
∂f(si) (12)
and,
vj = ∂L
∂g(tj) =
∑
i
wij
∂dij
∂g(tj) (13)
with which we can build up the Representation
Gradient Cache. When all representations’ gra-
dients are computed and stored, encoder gradi-
ent can be computed with Step3 described in sub-
section 3.3. In philosophy this method links up
two caches. Note this covers early interaction
f(s) = s,g(t) = tas a special case.
6 Conclusion
In this paper, we introduce a gradient cache tech-
nique that breaks GPU memory limitations for
large batch contrastive learning. We propose to con-
struct a representation gradient cache that removes
in-batch data dependency in encoder optimization.
Our method produces the exact same gradient up-
date as training with a large batch. We show the
3We used the gradient checkpoint implemented in Hug-
gingface transformers package
method is efﬁcient and capable of preserving accu-
racy on resource-limited hardware. We believe a
critical contribution of our work is providing a large
population in the NLP community with access to
batch-wise contrastive learning. While many previ-
ous works come from people with industry-grade
hardware, researchers with limited hardware can
now use our technique to reproduce state-of-the-art
models and further advance the research without
being constrained by available GPU memory.
Acknowledgments
The authors would like to thank Zhuyun Dai and
Chenyan Xiong for comments on the paper, and
the anonymous reviewers for their reviews.



Source: data\tc18_2309.07597v5\referenced_papers\[18]_2101.06983.pdf (Page 2):

• Computing partial derivatives ∂L
∂f (si) and
∂L
∂g(tj ) requires only encoded representations,
but not Θ or Λ.
These observations mean back propagation of
f(si) for data si can be run independently with
its own computation graph and activation if the
numerical value of the partial derivative ∂L
∂si
is
known. Meanwhile the derivation of ∂L
∂si
requires
only numerical values of two sets of representa-
tion vectors F = {f(s1),f(s2),..,f (s|S|)}and
G = {g(t1),g(t2),...,g (t|T|)}. A similar argu-
ment holds true for g, where we can use represen-
tation vectors to compute ∂L
∂tj
and back propagate
for each g(tj) independently. In the next section,
we will describe how to scale up batch size by pre-
computing these representation vectors.
3.3 Gradient Cache Technique
Given a large batch that does not ﬁt into the avail-
able GPU memory for training, we ﬁrst divide it
into a set of sub-batches each of which can ﬁt
into memory for gradient computation, denoted as
S = {ˆS1, ˆS2,..},T = {ˆT1, ˆT2,..}. The full-batch
gradient update is computed by the following steps.
Step1: Graph-less Forward Before gradient
computation, we ﬁrst run an extra encoder forward
pass for each batch instance to get its representa-
tion. Importantly, this forward pass runs without
constructing the computation graph. We collect
and store all representations computed.
Step2: Representation Gradient Computation
and Caching We then compute the contrastive
loss for the batch based on the representation from
Step1 and have a corresponding computation graph
constructed. Despite the mathematical derivation,
automatic differentiation system is used in actual
implementation, which automatically supports vari-
ations of contrastive loss. A backward pass is
then run to populate gradients for each represen-
tation. Note that the encoder is not included in
this gradient computation. Let ui = ∂L
∂f (si) and
vi = ∂L
∂g(ti) , we take these gradient tensors and
store them as a Representation Gradient Cache,
[u1,u2,.., v1,v2,..].
Step3: Sub-batch Gradient AccumulationWe
run encoder forward one sub-batch at a time to
compute representations and build the correspond-
ing computation graph. We take the sub-batch’s
representation gradients from the cache and run
back propagation through the encoder. Gradients
are accumulated for encoder parameters across all
sub-batches. Effectively for f we have,
∂L
∂Θ =
∑
ˆSj ∈S
∑
si∈ˆSj
∂L
∂f(si)
∂f(si)
∂Θ
=
∑
ˆSj ∈S
∑
si∈ˆSj
ui
∂f(si)
∂Θ
(8)
where the outer summation enumerates each sub-
batch and the entire internal summation corre-
sponds to one step of accumulation. Similarly, for
g, gradients accumulate based on,
∂L
∂Λ =
∑
ˆTj ∈T
∑
ti∈ ˆTj
vi
∂g(ti)
∂Λ (9)
Here we can see the equivalence with direct large
batch update by combining the two summations.
Step4: Optimization When all sub-batches are
processed, we can step the optimizer to update
model parameters as if the full batch is processed
in a single forward-backward pass.
Compared to directly updating with the full
batch, which requires memory linear to the number
of examples, our method ﬁxes the number of exam-
ples in each encoder gradient computation to be the
size of sub-batch and therefore requires constant
memory for encoder forward-backward pass. The
extra data pieces introduced by our method that re-
main persistent across steps are the representations
and their corresponding gradients with the former
turned into the latter after representation gradient
computation. Consequently, in a general case with
data from Sand T each represented with ddimen-
sion vectors, we only need to store (|S|d+ |T|d)
ﬂoating points in the cache on top of the computa-
tion graph. To remind our readers, this is several
orders smaller than million-size model parameters.
3.4 Multi-GPU Training
When training on multiple GPUs, we need to com-
pute the gradients with all examples across all
GPUs. This requires a single additional cross GPU
communication after Step1 when all representa-
tions are computed. We use an all-gather opera-
tion to make all representations available on all
GPUs. Denote Fn,Gn representations on n-th
GPU and a total of N device. Step2 runs with
gathered representations Fall = F1 ∪..∪FN and
Gall = G1 ∪..∪GN . While Fall and Gall are used



Source: data\tc18_2309.07597v5\referenced_papers\[18]_2101.06983.pdf (Page 1):

Noise Contrastive Estimation (NCE) was later used
by Word2Vec (Mikolov et al., 2013) to learn word
embedding. Recent works use contrastive learning
to unsupervisedly pre-train (Lee et al., 2019; Chang
et al., 2020) as well as supervisedly train dense re-
triever (Karpukhin et al., 2020), where contrastive
loss is used to estimate retrieval probability over
the entire corpus. Inspired by SimCLR (Chen et al.,
2020), constrastive learning is used to learn better
sentence representation (Giorgi et al., 2020) and
pre-trained language model (Wu et al., 2020).
Deep Network Memory Reduction Many ex-
isting techniques deal with large and deep mod-
els. The gradient checkpoint method attempts to
emulate training deep networks by training shal-
lower layers and connecting them with gradient
checkpoints and re-computation (Chen et al., 2016).
Some methods also use reversible activation func-
tions, allowing internal activation in the network to
be recovered throughout back propagation (Gomez
et al., 2017; MacKay et al., 2018). However, their
effectiveness as part of contrastive encoders has
not been conﬁrmed. Recent work also attempts
to remove the redundancy in optimizer tracked pa-
rameters on each GPU (Rajbhandari et al., 2020).
Compared with the aforementioned methods, our
method is designed for scaling over the batch size
dimension for contrastive learning.
3 Methodologies
In this section, we formally introduce the notations
for contrastive loss and analyze the difﬁculties of
using it on limited hardware. We then show how
we can use a Gradient Cache technique to factor
the loss so that large batch gradient update can be
broken into several sub-updates.
3.1 Preliminaries
Under a general formulation, given two classes of
data S,T, we want to learn encoders f and gfor
each such that, given s∈S,t ∈T , encoded repre-
sentations f(s) and g(t) are close if related and far
apart if not related by some distance measurement.
For large Sand T and deep neural network based
f and g, direct training is not tractable, so a com-
mon approach is to use a contrastive loss: sample
anchors S ⊂S and targets T ⊂T as a training
batch, where each element si ∈S has a related
element tri ∈T as well as zero or more specially
sampled hard negatives. The rest of the random
samples in T will be used as in-batch negatives.
Deﬁne loss based on dot product as follows:
L= −1
|S|
∑
si∈S
log exp(f(si)⊤g(tri )/τ)∑
tj ∈T exp(f(si)⊺g(tj)/τ)
(1)
where each summation term depends on the entire
set T and requires ﬁtting all of them into memory.
We set temperature τ = 1 in the following dis-
cussion for simplicity as in general it only adds a
constant multiplier to the gradient.
3.2 Analysis of Computation
In this section, we give a mathematical analysis
of contrastive loss computation and its gradient.
We show that the back propagation process can be
divided into two parts, from loss to representation,
and from representation to encoder model. The
separation then enables us to devise a technique
that removes data dependency in encoder parameter
update. Suppose the function f is parameterized
with Θ and gis parameterized with Λ.
∂L
∂Θ =
∑
si∈S
∂L
∂f(si)
∂f(si)
∂Θ (2)
∂L
∂Λ =
∑
tj ∈T
∂L
∂g(tj)
∂g(tj)
∂Λ (3)
As an extra notation, denote normalized similarity,
pij = exp(f(si)⊺g(tj))∑
t∈T exp(f(si)⊺g(t)) (4)
We note that the summation term for a particularsi
or ti is a function of the batch, as,
∂L
∂f(si) = −1
|S|

g(tri ) −
∑
tj ∈T
pijg(tj)

, (5)
∂L
∂g(tj) = −1
|S|

ϵj −
∑
si∈S
pijf(si)

, (6)
where
ϵj =
{
f(sk) if ∃ks.t. rk = j
0 otherwise (7)
which prohibits the use of gradient accumulation.
We make two observations here:
• The partial derivative ∂f (si)
∂Θ depends only on
si and Θ while ∂g(tj )
∂Λ depends only on tj and
Λ; and



Source: data\tc18_2309.07597v5\referenced_papers\[18]_2101.06983.pdf (Page 3):

Method Top-5 Top-20 Top-100
DPR - 78.4 85.4
Sequential 59.3 71.9 80.9
Accumulation 64.3 77.2 84.9
Cache 68.6 79.3 86.0
- BSZ = 512 68.3 79.9 86.6
Table 1: Retrieval: We compare top-5/20/100 hit accu-
racy of small batch update (Sequential), accumulated
small batch (Accumulation) and gradient cache (Cache)
systems with DPR reference.
to compute loss, the n-th GPU only computes gra-
dient of its local representations Fn,Gn and stores
them into cache. No communication happens in
Step3, when each GPU independently computes
gradient for local representations. Step4 will then
perform gradient reduction across GPUs as with
standard parallel training.
4 Experiments
To examine the reliability and computation cost of
our method, we implement our method into dense
passage retriever (DPR; Karpukhin et al. (2020))2.
We use gradient cache to compute DPR’s super-
vised contrastive loss on a single GPU. Following
DPR paper, we measure top hit accuracy on the Nat-
ural Question Dataset (Kwiatkowski et al., 2019)
for different methods. We then examine the train-
ing speed of various batch sizes.
4.1 Retrieval Accuracy
Compared Systems 1) DPR: the reference num-
ber taken from the original paper trained on 8
GPUs, 2) Sequential: update with max batch size
that ﬁts into 1 GPU, 3) Accumulation: similar
to Sequential but accumulate gradients and up-
date until number of examples matches DPR setup,
4) Cache: training with DPR setup using our gra-
dient cache on 1 GPU. We attempted to run with
gradient checkpointing but found it cannot scale to
standard DPR batch size on our hardware.
Implementations All runs start with the same
random seed and follow DPR training hyperparam-
eters except batch size. Cache uses a batch size of
128 same as DPR and runs with a sub-batch size
of 16 for questions and 8 for passages. We also
run Cache with a batch size of 512 (BSZ=512) to
2Our implementation is at: https://github.com/
luyug/GC-DPR
Examples / Update
Time / Update (seconds)
0
50
100
150
1000 2000 3000 4000
Cache Accumulation
Figure 1: We compare training speed versus the num-
ber of examples per update for gradient cache (Cache)
and gradient accumulation (Accumulation).
examine the behavior of even larger batches. Se-
quential uses a batch size of 8, the largest that ﬁts
into memory. Accumulation will accumulate 16 of
size-8 batches. Each question is paired with a posi-
tive and a BM25 negative passage. All experiments
use a single RTX 2080ti.
Results Accuracy results are shown in Table 1.
We observe that Cache performs better than DPR
reference due to randomness in training. Further in-
creasing batch size to 512 can bring in some advan-
tage at top 20/100. Accumulation and Sequential
results conﬁrm the importance of a bigger batch
and more negatives. For Accumulation which tries
to match the batch size but has fewer negatives,
we see a drop in performance which is larger to-
wards the top. In the sequential case, a smaller
batch incurs higher variance, and the performance
further drops. In summary, our Cache method im-
proves over standard methods and matches the per-
formance of large batch training.
4.2 Training Speed
In Figure 1, we compare update speed of gradient
cache and accumulation with per update example
number of {64,128,256,512,1024,2048,4096}.
We observe gradient cache method can steadily
scale up to larger batch update and uses 20% more
time for representation pre-computation. This extra
cost enables it to create an update of a much larger
batch critical for the best performance, as shown
by previous experiments and many early works.
While the original DPR reports a training time of
roughly one day on 8 V100 GPUs, in practice, with
improved data loading, our gradient cache code can
train a dense retriever in a practical 31 hours on
a single RTX2080ti. We also ﬁnd gradient check-
point only runs up to batch of 64 and consumes



Source: data\tc18_2309.07597v5\referenced_papers\[18]_2101.06983.pdf (Page 0):

Scaling Deep Contrastive Learning Batch Size
under Memory Limited Setup
Luyu Gao1, Yunyi Zhang2, Jiawei Han2, Jamie Callan1
1 Language Technologies Institute, Carnegie Mellon University
2 Department of Computer Science, University of Illinois Urbana-Champaign
1{luyug, callan}@cs.cmu.edu 2{yzhan238, hanj}@illionis.edu
Abstract
Contrastive learning has been applied suc-
cessfully to learn vector representations of
text. Previous research demonstrated that
learning high-quality representations beneﬁts
from batch-wise contrastive loss with a large
number of negatives. In practice, the technique
of in-batch negative is used, where for each ex-
ample in a batch, other batch examples’ pos-
itives will be taken as its negatives, avoiding
encoding extra negatives. This, however, still
conditions each example’s loss on all batch
examples and requires ﬁtting the entire large
batch into GPU memory. This paper intro-
duces a gradient caching technique that decou-
ples backpropagation between contrastive loss
and the encoder, removing encoder backward
pass data dependency along the batch dimen-
sion. As a result, gradients can be computed
for one subset of the batch at a time, leading to
almost constant memory usage. 1
1 Introduction
Contrastive learning learns to encode data into an
embedding space such that related data points have
closer representations and unrelated ones have fur-
ther apart ones. Recent works in NLP adopt deep
neural nets as encoders and use unsupervised con-
trastive learning on sentence representation (Giorgi
et al., 2020), text retrieval (Lee et al., 2019),
and language model pre-training tasks (Wu et al.,
2020). Supervised contrastive learning (Khosla
et al., 2020) has also been shown effective in train-
ing dense retrievers (Karpukhin et al., 2020; Qu
et al., 2020). These works typically use batch-wise
contrastive loss, sharing target texts as in-batch
negatives. With such a technique, previous works
have empirically shown that larger batches help
learn better representations. However, computing
loss and updating model parameters with respect
1Our code is at github.com/luyug/GradCache.
to a big batch require encoding all batch data and
storing all activation, so batch size is limited by to-
tal available GPU memory. This limits application
and research of contrastive learning methods under
memory limited setup, e.g. academia. For example,
Lee et al. (2019) pre-train a BERT (Devlin et al.,
2019) passage encoder with a batch size of 4096
while a high-end commercial GPU RTX 2080ti can
only ﬁt a batch of 8. The gradient accumulation
technique, splitting a large batch into chunks and
summing gradients across several backwards, can-
not emulate a large batch as each smaller chunk
has fewer in-batch negatives.
In this paper, we present a simple technique
that thresholds peak memory usage for contrastive
learning to almost constant regardless of the batch
size. For deep contrastive learning, the memory
bottlenecks are at the deep neural network based
encoder. We observe that we can separate the back-
propagation process of contrastive loss into two
parts, from loss to representation, and from repre-
sentation to model parameter, with the latter being
independent across batch examples given the for-
mer, detailed in subsection 3.2. We then show in
subsection 3.3 that by separately pre-computing
the representations’ gradient and store them in a
cache, we can break the update of the encoder into
multiple sub-updates that can ﬁt into the GPU mem-
ory. This pre-computation of gradients allows our
method to produce the exact samegradient update
as training with large batch. Experiments show that
with about 20% increase in runtime, our technique
enables a single consumer-grade GPU to reproduce
the state-of-the-art large batch trained models that
used to require multiple professional GPUs.
2 Related Work
Contrastive Learning First introduced for prob-
ablistic language modeling (Mnih and Teh, 2012),
arXiv:2101.06983v2  [cs.LG]  14 Jun 2021



### Claim 19/38

#### Claim Text
One more characteristic is that we use a specifically pre-trained text encoder to train BGE, rather than using common choices, like BERT [15] and RoBERTa [29].

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[29]_1907.11692.pdf (Page 5):

bsz steps lr ppl MNLI-m SST -2
256 1M 1e-4 3.99 84.7 92.7
2K 125K 7e-4 3.68 85.2 92.9
8K 31K 1e-3 3.77 84.6 92.8
T able 3: Perplexity on held-out training data ( ppl) and
development set accuracy for base models trained over
BO O KCO RP U S and W IK IP E D IA with varying batch
sizes ( bsz). W e tune the learning rate ( lr) for each set-
ting. Models make the same number of passes over the
data (epochs) and have the same computational cost.
task performance of BERTBA S E as we increase the
batch size, controlling for the number of passes
through the training data. W e observe that train-
ing with large batches improves perplexity for the
masked language modeling objective, as well as
end-task accuracy . Large batches are also easier to
parallelize via distributed data parallel training,
8
and in later experiments we train with batches of
8K sequences.
Notably
Y ou et al. (2019) train BERT with even
larger batche sizes, up to 32K sequences. W e leave
further exploration of the limits of large batch
training to future work.
4.4 T ext Encoding
Byte-Pair Encoding (BPE) (
Sennrich et al. , 2016)
is a hybrid between character- and word-level rep-
resentations that allows handling the large vocab-
ularies common in natural language corpora. In-
stead of full words, BPE relies on subwords units,
which are extracted by performing statistical anal-
ysis of the training corpus.
BPE vocabulary sizes typically range from
10K-100K subword units. However, unicode char-
acters can account for a sizeable portion of this
vocabulary when modeling large and diverse cor-
pora, such as the ones considered in this work.
Radford et al. (2019) introduce a clever imple-
mentation of BPE that uses bytes instead of uni-
code characters as the base subword units. Using
bytes makes it possible to learn a subword vocab-
ulary of a modest size (50K units) that can still en-
code any input text without introducing any “un-
known” tokens.
8 Large batch training can improve training efﬁciency even
without large scale parallel hardware through gradient ac-
cumulation, whereby gradients from multiple mini-batches
are accumulated locally before each optimization step. Thi s
functionality is supported natively in FA I R S E Q (Ott et al. ,
2019).
The original BERT implementa-
tion ( Devlin et al. , 2019) uses a character-level
BPE vocabulary of size 30K, which is learned
after preprocessing the input with heuristic tok-
enization rules. Following
Radford et al. (2019),
we instead consider training BERT with a larger
byte-level BPE vocabulary containing 50K sub-
word units, without any additional preprocessing
or tokenization of the input. This adds approxi-
mately 15M and 20M additional parameters for
BERTBA S E and BERT L A R G E, respectively .
Early experiments revealed only slight dif-
ferences between these encodings, with the
Radford et al. (2019) BPE achieving slightly
worse end-task performance on some tasks. Nev-
ertheless, we believe the advantages of a univer-
sal encoding scheme outweighs the minor degre-
dation in performance and use this encoding in
the remainder of our experiments. A more de-
tailed comparison of these encodings is left to fu-
ture work.
5 RoBERT a
In the previous section we propose modiﬁcations
to the BERT pretraining procedure that improve
end-task performance. W e now aggregate these
improvements and evaluate their combined im-
pact. W e call this conﬁgurationRoBERT a for
R
obustly optimized BERT approach. Speciﬁ-
cally , RoBERT a is trained with dynamic mask-
ing (Section
4.1), FU LL -SEN TE N C E S without NSP
loss (Section 4.2), large mini-batches (Section 4.3)
and a larger byte-level BPE (Section 4.4).
Additionally , we investigate two other impor-
tant factors that have been under-emphasized in
previous work: (1) the data used for pretraining,
and (2) the number of training passes through the
data. For example, the recently proposed XLNet
architecture (
Y ang et al. , 2019) is pretrained us-
ing nearly 10 times more data than the original
BERT (
Devlin et al. , 2019). It is also trained with
a batch size eight times larger for half as many op-
timization steps, thus seeing four times as many
sequences in pretraining compared to BERT .
T o help disentangle the importance of these fac-
tors from other modeling choices (e.g., the pre-
training objective), we begin by training RoBERT a
following the BERTL A R G E architecture ( L = 24,
H = 1024, A = 16, 355M parameters). W e
pretrain for 100K steps over a comparable B O O K-
CO R PU S plus W IK IPED IA dataset as was used in



Source: data\tc18_2309.07597v5\referenced_papers\[15]_1810.04805.pdf (Page 2):

BERT BERT
E[CLS] E1  E[SEP]... EN E1’ ... EM’
C
 T1
 T[SEP]...
 TN
 T1’ ...
 TM’
[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM
Question Paragraph
Start/End Span
BERT
E[CLS] E1  E[SEP]... EN E1’ ... EM’
C
 T1
 T[SEP]...
 TN
 T1’ ...
 TM’
[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM
Masked Sentence A Masked Sentence B
Pre-training Fine-Tuning
NSP Mask LM Mask LM
Unlabeled Sentence A and B Pair 
SQuAD
Question Answer Pair
NERMNLI
Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-
tures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize
models for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special
symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-
tions/answers).
ing and auto-encoder objectives have been used
for pre-training such models (Howard and Ruder,
2018; Radford et al., 2018; Dai and Le, 2015).
2.3 Transfer Learning from Supervised Data
There has also been work showing effective trans-
fer from supervised tasks with large datasets, such
as natural language inference (Conneau et al.,
2017) and machine translation (McCann et al.,
2017). Computer vision research has also demon-
strated the importance of transfer learning from
large pre-trained models, where an effective recipe
is to ﬁne-tune models pre-trained with Ima-
geNet (Deng et al., 2009; Yosinski et al., 2014).
3 BERT
We introduce BERT and its detailed implementa-
tion in this section. There are two steps in our
framework: pre-training and ﬁne-tuning. Dur-
ing pre-training, the model is trained on unlabeled
data over different pre-training tasks. For ﬁne-
tuning, the BERT model is ﬁrst initialized with
the pre-trained parameters, and all of the param-
eters are ﬁne-tuned using labeled data from the
downstream tasks. Each downstream task has sep-
arate ﬁne-tuned models, even though they are ini-
tialized with the same pre-trained parameters. The
question-answering example in Figure 1 will serve
as a running example for this section.
A distinctive feature of BERT is its uniﬁed ar-
chitecture across different tasks. There is mini-
mal difference between the pre-trained architec-
ture and the ﬁnal downstream architecture.
Model Architecture BERT’s model architec-
ture is a multi-layer bidirectional Transformer en-
coder based on the original implementation de-
scribed in Vaswani et al. (2017) and released in
the tensor2tensor library.1 Because the use
of Transformers has become common and our im-
plementation is almost identical to the original,
we will omit an exhaustive background descrip-
tion of the model architecture and refer readers to
Vaswani et al. (2017) as well as excellent guides
such as “The Annotated Transformer.”2
In this work, we denote the number of layers
(i.e., Transformer blocks) as L, the hidden size as
H, and the number of self-attention heads as A.3
We primarily report results on two model sizes:
BERTBASE (L=12, H=768, A=12, Total Param-
eters=110M) and BERTLARGE (L=24, H=1024,
A=16, Total Parameters=340M).
BERTBASE was chosen to have the same model
size as OpenAI GPT for comparison purposes.
Critically, however, the BERT Transformer uses
bidirectional self-attention, while the GPT Trans-
former uses constrained self-attention where every
token can only attend to context to its left.4
1https://github.com/tensorﬂow/tensor2tensor
2http://nlp.seas.harvard.edu/2018/04/03/attention.html
3In all cases we set the feed-forward/ﬁlter size to be 4H,
i.e., 3072 for the H = 768and 4096 for the H = 1024.
4We note that in the literature the bidirectional Trans-



Source: data\tc18_2309.07597v5\referenced_papers\[29]_1907.11692.pdf (Page 12):

MNLI QNLI QQP RTE SST MRPC CoLA STS
RoBERT aBA S E
+ all data + 500k steps 87.6 92.8 91.9 78.7 94.8 90.2 63.6 91.2
RoBERT aL A R G E
with B O O K S + W IK I 89.0 93.9 91.9 84.5 95.3 90.2 66.3 91.6
+ additional data ( §3.2) 89.3 94.0 92.0 82.7 95.6 91.4 66.1 92.2
+ pretrain longer 300k 90.0 94.5 92.2 83.3 96.1 91.1 67.4 92.3
+ pretrain longer 500k 90.2 94.7 92.2 86.6 96.4 90.9 68.0 92.4
T able 8: Development set results on GLUE tasks for various co nﬁgurations of RoBER T a.
Hyperparam RoBERT a L A R G E RoBERT aBA S E
Number of Layers 24 12
Hidden size 1024 768
FFN inner hidden size 4096 3072
Attention heads 16 12
Attention head size 64 64
Dropout 0.1 0.1
Attention Dropout 0.1 0.1
W armup Steps 30k 24k
Peak Learning Rate 4e-4 6e-4
Batch Size 8k 8k
W eight Decay 0.01 0.01
Max Steps 500k 500k
Learning Rate Decay Linear Linear
Adamǫ 1e-6 1e-6
Adam β1 0.9 0.9
Adam β2 0.98 0.98
Gradient Clipping 0.0 0.0
T able 9: Hyperparameters for pretraining RoBER T a LARGE and RoBER T aBASE .
Hyperparam RACE SQuAD GLUE
Learning Rate 1e-5 1.5e-5 {1e-5, 2e-5, 3e-5 }
Batch Size 16 48 {16, 32 }
W eight Decay 0.1 0.01 0.1
Max Epochs 4 2 10
Learning Rate Decay Linear Linear Linear
W armup ratio 0.06 0.06 0.06
T able 10: Hyperparameters for ﬁnetuning RoBER T a LARGE on RACE, SQuAD and GLUE.



Source: data\tc18_2309.07597v5\referenced_papers\[29]_1907.11692.pdf (Page 0):

arXiv:1907.11692v1  [cs.CL]  26 Jul 2019
RoBERT a: A Robustly Optimized BERT Pretraining Approach
Yinhan Liu ∗§ Myle Ott ∗§ Naman Goyal ∗§ Jingfei Du ∗§ Mandar Joshi †
Danqi Chen § Omer Levy § Mike Lewis § Luke Zettlemoyer †§ V eselin Stoyanov§
† Paul G. Allen School of Computer Science & Engineering,
University of W ashington, Seattle, W A
{mandar90,lsz}@cs.washington.edu
§ Facebook AI
{yinhanliu,myleott,naman,jingfeidu,
danqi,omerlevy,mikelewis,lsz,ves}@fb.com
Abstract
Language model pretraining has led to sig-
niﬁcant performance gains but careful com-
parison between different approaches is chal-
lenging. Training is computationally expen-
sive, often done on private datasets of different
sizes, and, as we will show , hyperparameter
choices have signiﬁcant impact on the ﬁnal re-
sults. W e present a replication study of BER T
pretraining (
Devlin et al. , 2019) that carefully
measures the impact of many key hyperparam-
eters and training data size. W e ﬁnd that BER T
was signiﬁcantly undertrained, and can match
or exceed the performance of every model
published after it. Our best model achieves
state-of-the-art results on GLUE, RACE and
SQuAD. These results highlight the impor-
tance of previously overlooked design choices,
and raise questions about the source of re-
cently reported improvements. W e release our
models and code.
1
1 Introduction
Self-training methods such as ELMo (
Peters et al. ,
2018), GPT ( Radford et al. , 2018), BERT
(Devlin et al. , 2019), XLM ( Lample and Conneau ,
2019), and XLNet ( Y ang et al. , 2019) have
brought signiﬁcant performance gains, but it can
be challenging to determine which aspects of
the methods contribute the most. Training is
computationally expensive, limiting the amount
of tuning that can be done, and is often done with
private training data of varying sizes, limiting
our ability to measure the effects of the modeling
advances.
∗ Equal contribution.
1 Our models and code are available at:
https://github.com/pytorch/fairseq
W e present a replication study of BERT pre-
training ( Devlin et al. , 2019), which includes a
careful evaluation of the effects of hyperparmeter
tuning and training set size. W e ﬁnd that BERT
was signiﬁcantly undertrained and propose an im-
proved recipe for training BERT models, which
we call RoBERT a, that can match or exceed the
performance of all of the post-BERT methods.
Our modiﬁcations are simple, they include: (1)
training the model longer, with bigger batches,
over more data; (2) removing the next sentence
prediction objective; (3) training on longer se-
quences; and (4) dynamically changing the mask-
ing pattern applied to the training data. W e also
collect a large new dataset (CC-NEW S ) of compa-
rable size to other privately used datasets, to better
control for training set size effects.
When controlling for training data, our im-
proved training procedure improves upon the pub-
lished BERT results on both GLUE and SQuAD.
When trained for longer over additional data, our
model achieves a score of 88.5 on the public
GLUE leaderboard, matching the 88.4 reported
by
Y ang et al. (2019). Our model establishes a
new state-of-the-art on 4/9 of the GLUE tasks:
MNLI, QNLI, RTE and STS-B. W e also match
state-of-the-art results on SQuAD and RACE.
Overall, we re-establish that BERT’s masked lan-
guage model training objective is competitive
with other recently proposed training objectives
such as perturbed autoregressive language model-
ing (
Y ang et al. , 2019).2
In summary , the contributions of this paper
are: (1) W e present a set of important BERT de-
sign choices and training strategies and introduce
2 It is possible that these other methods could also improve
with more tuning. W e leave this exploration to future work.



Source: data\tc18_2309.07597v5\referenced_papers\[62]_2004.05986.pdf (Page 14):

Masking Type Data
Source
Training
Tokens #
Device Training
Steps
Batch
Size
Optimizer V ocabularyInit Ckpt
BERT-base WordPiecebase wiki 0.4B TPU Pod
v2
- - AdamW 21,128 Random
Init
BERT-wwm-ext-base WWM base wiki+ext 5.4B TPU v3 1M 384 LAMB ∼BERT ∼BERT
ALBERT-tiny WWM tiny CLUE
corpus
5B TPU Pod
v3
500k 4k LAMB ∼BERT Random
Init
ALBERT-xxlarge Span large CLUE
corpus
5B TPU Pod
v3
1M 8k AdamW ∼BERT Random
Init
ERNIE-base Knowledge
Masking
base wiki+ext 15B NVidia
v100
1M 8192 Adam 17964 Random
Init
XLNet-mid Sentence
Piece
mid wiki+ext 5.4B TPU v3 2M 32 Adam 32000 Random
Init
RoBERTa-large WWM large CLUE
corpus
5B TPU Pod 100k 8k AdamW ∼BERT Random
Init
RoBERTa-wwm-ext-base WWM base wiki+ext 5.4B TPU v3 1M 384 AdamW ∼BERT ∼BERT
RoBERTa-wwm-ext-large WWM large wiki+ext 5.4BTPU Pod
v3-32
2M 512 AdamW ∼BERT Random
Init
Table 6: Parameters for pre-training. ”BERT-base” is released by google (Devlin et al., 2019). “WWM”
stands for whole word masking. “ext” presents for extended data, different models may use different
extended data. “∼BERT” means similar to Google’s Chinese BERT.
Model Batch Size Max Length Epoch Learning Rate
AFQMC All* 16 128 3 2e-5
TNEWS All* 16 128 3 2e-5
IFLYTEK ALBERT-tiny 32 128 10 2e-5
RoBERT-large, RoBERTa-wwm-ext-large 24 128 3 2e-5
All* except above 32 128 3 2e-5
OCNLI BERT-base, RoBERTa-wwm-ext-large 32 128 3 2e-5
RoBERTa-wwm-ext, ERNIE 32 128 3 3e-5
ALBERT-tiny 32 128 4 5e-5
XLNET-mid 32 128 3 5e-5
CLUEWSC2020 ALBERT-tiny 8 128 50 1e-4
All* except ALBERT-tiny 8 128 50 2e-5
CSL RoBERTa-large 4 256 5 5e-6
All* except above 4 256 5 1e-5
CMRC* ALBERT-tiny 32 512 3 2e-4
RoBERTa-wwm-ext-large 32 512 2 2.5e-5
RoBERTa-large 32 256 2 3e-5
XLNET-mid, RoBERTa-wwm-ext-base 32 512 2 3e-5
All* except above 32 512 2 3e-5
CHID All* 24 64 3 2e-5
C3 All* 24 512 8 2e-5
Table 7: Parameters for ﬁne-tuning. CMRC* presents for CMRC dataset in 2018. All* means ALBERT-
tiny, BERT-base, BERT-wwm-ext-base, ERNIE-base, RoBERTa-large, XLNet-mid, RoBERTa-wwm-ext-
base and RoBERTa-wwm-ext-large namely. It should be noted that RoBERTa-large is pre-trained with
256 sequence length, which is shorter than 512 length pre-trained for others. So we individually limit
the length of RoBERTa-large to 256 for CMRC*, and use the striding text span to relieve this problem.
However, this drawback of RoBERTa-large may decrease performances of some datasets whose length
can not be effectively cut down, such as C3.



#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[29]_1907.11692.pdf (Page 5):

bsz steps lr ppl MNLI-m SST -2
256 1M 1e-4 3.99 84.7 92.7
2K 125K 7e-4 3.68 85.2 92.9
8K 31K 1e-3 3.77 84.6 92.8
T able 3: Perplexity on held-out training data ( ppl) and
development set accuracy for base models trained over
BO O KCO RP U S and W IK IP E D IA with varying batch
sizes ( bsz). W e tune the learning rate ( lr) for each set-
ting. Models make the same number of passes over the
data (epochs) and have the same computational cost.
task performance of BERTBA S E as we increase the
batch size, controlling for the number of passes
through the training data. W e observe that train-
ing with large batches improves perplexity for the
masked language modeling objective, as well as
end-task accuracy . Large batches are also easier to
parallelize via distributed data parallel training,
8
and in later experiments we train with batches of
8K sequences.
Notably
Y ou et al. (2019) train BERT with even
larger batche sizes, up to 32K sequences. W e leave
further exploration of the limits of large batch
training to future work.
4.4 T ext Encoding
Byte-Pair Encoding (BPE) (
Sennrich et al. , 2016)
is a hybrid between character- and word-level rep-
resentations that allows handling the large vocab-
ularies common in natural language corpora. In-
stead of full words, BPE relies on subwords units,
which are extracted by performing statistical anal-
ysis of the training corpus.
BPE vocabulary sizes typically range from
10K-100K subword units. However, unicode char-
acters can account for a sizeable portion of this
vocabulary when modeling large and diverse cor-
pora, such as the ones considered in this work.
Radford et al. (2019) introduce a clever imple-
mentation of BPE that uses bytes instead of uni-
code characters as the base subword units. Using
bytes makes it possible to learn a subword vocab-
ulary of a modest size (50K units) that can still en-
code any input text without introducing any “un-
known” tokens.
8 Large batch training can improve training efﬁciency even
without large scale parallel hardware through gradient ac-
cumulation, whereby gradients from multiple mini-batches
are accumulated locally before each optimization step. Thi s
functionality is supported natively in FA I R S E Q (Ott et al. ,
2019).
The original BERT implementa-
tion ( Devlin et al. , 2019) uses a character-level
BPE vocabulary of size 30K, which is learned
after preprocessing the input with heuristic tok-
enization rules. Following
Radford et al. (2019),
we instead consider training BERT with a larger
byte-level BPE vocabulary containing 50K sub-
word units, without any additional preprocessing
or tokenization of the input. This adds approxi-
mately 15M and 20M additional parameters for
BERTBA S E and BERT L A R G E, respectively .
Early experiments revealed only slight dif-
ferences between these encodings, with the
Radford et al. (2019) BPE achieving slightly
worse end-task performance on some tasks. Nev-
ertheless, we believe the advantages of a univer-
sal encoding scheme outweighs the minor degre-
dation in performance and use this encoding in
the remainder of our experiments. A more de-
tailed comparison of these encodings is left to fu-
ture work.
5 RoBERT a
In the previous section we propose modiﬁcations
to the BERT pretraining procedure that improve
end-task performance. W e now aggregate these
improvements and evaluate their combined im-
pact. W e call this conﬁgurationRoBERT a for
R
obustly optimized BERT approach. Speciﬁ-
cally , RoBERT a is trained with dynamic mask-
ing (Section
4.1), FU LL -SEN TE N C E S without NSP
loss (Section 4.2), large mini-batches (Section 4.3)
and a larger byte-level BPE (Section 4.4).
Additionally , we investigate two other impor-
tant factors that have been under-emphasized in
previous work: (1) the data used for pretraining,
and (2) the number of training passes through the
data. For example, the recently proposed XLNet
architecture (
Y ang et al. , 2019) is pretrained us-
ing nearly 10 times more data than the original
BERT (
Devlin et al. , 2019). It is also trained with
a batch size eight times larger for half as many op-
timization steps, thus seeing four times as many
sequences in pretraining compared to BERT .
T o help disentangle the importance of these fac-
tors from other modeling choices (e.g., the pre-
training objective), we begin by training RoBERT a
following the BERTL A R G E architecture ( L = 24,
H = 1024, A = 16, 355M parameters). W e
pretrain for 100K steps over a comparable B O O K-
CO R PU S plus W IK IPED IA dataset as was used in



Source: data\tc18_2309.07597v5\referenced_papers\[15]_1810.04805.pdf (Page 2):

BERT BERT
E[CLS] E1  E[SEP]... EN E1’ ... EM’
C
 T1
 T[SEP]...
 TN
 T1’ ...
 TM’
[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM
Question Paragraph
Start/End Span
BERT
E[CLS] E1  E[SEP]... EN E1’ ... EM’
C
 T1
 T[SEP]...
 TN
 T1’ ...
 TM’
[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM
Masked Sentence A Masked Sentence B
Pre-training Fine-Tuning
NSP Mask LM Mask LM
Unlabeled Sentence A and B Pair 
SQuAD
Question Answer Pair
NERMNLI
Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-
tures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize
models for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special
symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-
tions/answers).
ing and auto-encoder objectives have been used
for pre-training such models (Howard and Ruder,
2018; Radford et al., 2018; Dai and Le, 2015).
2.3 Transfer Learning from Supervised Data
There has also been work showing effective trans-
fer from supervised tasks with large datasets, such
as natural language inference (Conneau et al.,
2017) and machine translation (McCann et al.,
2017). Computer vision research has also demon-
strated the importance of transfer learning from
large pre-trained models, where an effective recipe
is to ﬁne-tune models pre-trained with Ima-
geNet (Deng et al., 2009; Yosinski et al., 2014).
3 BERT
We introduce BERT and its detailed implementa-
tion in this section. There are two steps in our
framework: pre-training and ﬁne-tuning. Dur-
ing pre-training, the model is trained on unlabeled
data over different pre-training tasks. For ﬁne-
tuning, the BERT model is ﬁrst initialized with
the pre-trained parameters, and all of the param-
eters are ﬁne-tuned using labeled data from the
downstream tasks. Each downstream task has sep-
arate ﬁne-tuned models, even though they are ini-
tialized with the same pre-trained parameters. The
question-answering example in Figure 1 will serve
as a running example for this section.
A distinctive feature of BERT is its uniﬁed ar-
chitecture across different tasks. There is mini-
mal difference between the pre-trained architec-
ture and the ﬁnal downstream architecture.
Model Architecture BERT’s model architec-
ture is a multi-layer bidirectional Transformer en-
coder based on the original implementation de-
scribed in Vaswani et al. (2017) and released in
the tensor2tensor library.1 Because the use
of Transformers has become common and our im-
plementation is almost identical to the original,
we will omit an exhaustive background descrip-
tion of the model architecture and refer readers to
Vaswani et al. (2017) as well as excellent guides
such as “The Annotated Transformer.”2
In this work, we denote the number of layers
(i.e., Transformer blocks) as L, the hidden size as
H, and the number of self-attention heads as A.3
We primarily report results on two model sizes:
BERTBASE (L=12, H=768, A=12, Total Param-
eters=110M) and BERTLARGE (L=24, H=1024,
A=16, Total Parameters=340M).
BERTBASE was chosen to have the same model
size as OpenAI GPT for comparison purposes.
Critically, however, the BERT Transformer uses
bidirectional self-attention, while the GPT Trans-
former uses constrained self-attention where every
token can only attend to context to its left.4
1https://github.com/tensorﬂow/tensor2tensor
2http://nlp.seas.harvard.edu/2018/04/03/attention.html
3In all cases we set the feed-forward/ﬁlter size to be 4H,
i.e., 3072 for the H = 768and 4096 for the H = 1024.
4We note that in the literature the bidirectional Trans-



Source: data\tc18_2309.07597v5\referenced_papers\[29]_1907.11692.pdf (Page 12):

MNLI QNLI QQP RTE SST MRPC CoLA STS
RoBERT aBA S E
+ all data + 500k steps 87.6 92.8 91.9 78.7 94.8 90.2 63.6 91.2
RoBERT aL A R G E
with B O O K S + W IK I 89.0 93.9 91.9 84.5 95.3 90.2 66.3 91.6
+ additional data ( §3.2) 89.3 94.0 92.0 82.7 95.6 91.4 66.1 92.2
+ pretrain longer 300k 90.0 94.5 92.2 83.3 96.1 91.1 67.4 92.3
+ pretrain longer 500k 90.2 94.7 92.2 86.6 96.4 90.9 68.0 92.4
T able 8: Development set results on GLUE tasks for various co nﬁgurations of RoBER T a.
Hyperparam RoBERT a L A R G E RoBERT aBA S E
Number of Layers 24 12
Hidden size 1024 768
FFN inner hidden size 4096 3072
Attention heads 16 12
Attention head size 64 64
Dropout 0.1 0.1
Attention Dropout 0.1 0.1
W armup Steps 30k 24k
Peak Learning Rate 4e-4 6e-4
Batch Size 8k 8k
W eight Decay 0.01 0.01
Max Steps 500k 500k
Learning Rate Decay Linear Linear
Adamǫ 1e-6 1e-6
Adam β1 0.9 0.9
Adam β2 0.98 0.98
Gradient Clipping 0.0 0.0
T able 9: Hyperparameters for pretraining RoBER T a LARGE and RoBER T aBASE .
Hyperparam RACE SQuAD GLUE
Learning Rate 1e-5 1.5e-5 {1e-5, 2e-5, 3e-5 }
Batch Size 16 48 {16, 32 }
W eight Decay 0.1 0.01 0.1
Max Epochs 4 2 10
Learning Rate Decay Linear Linear Linear
W armup ratio 0.06 0.06 0.06
T able 10: Hyperparameters for ﬁnetuning RoBER T a LARGE on RACE, SQuAD and GLUE.



Source: data\tc18_2309.07597v5\referenced_papers\[29]_1907.11692.pdf (Page 0):

arXiv:1907.11692v1  [cs.CL]  26 Jul 2019
RoBERT a: A Robustly Optimized BERT Pretraining Approach
Yinhan Liu ∗§ Myle Ott ∗§ Naman Goyal ∗§ Jingfei Du ∗§ Mandar Joshi †
Danqi Chen § Omer Levy § Mike Lewis § Luke Zettlemoyer †§ V eselin Stoyanov§
† Paul G. Allen School of Computer Science & Engineering,
University of W ashington, Seattle, W A
{mandar90,lsz}@cs.washington.edu
§ Facebook AI
{yinhanliu,myleott,naman,jingfeidu,
danqi,omerlevy,mikelewis,lsz,ves}@fb.com
Abstract
Language model pretraining has led to sig-
niﬁcant performance gains but careful com-
parison between different approaches is chal-
lenging. Training is computationally expen-
sive, often done on private datasets of different
sizes, and, as we will show , hyperparameter
choices have signiﬁcant impact on the ﬁnal re-
sults. W e present a replication study of BER T
pretraining (
Devlin et al. , 2019) that carefully
measures the impact of many key hyperparam-
eters and training data size. W e ﬁnd that BER T
was signiﬁcantly undertrained, and can match
or exceed the performance of every model
published after it. Our best model achieves
state-of-the-art results on GLUE, RACE and
SQuAD. These results highlight the impor-
tance of previously overlooked design choices,
and raise questions about the source of re-
cently reported improvements. W e release our
models and code.
1
1 Introduction
Self-training methods such as ELMo (
Peters et al. ,
2018), GPT ( Radford et al. , 2018), BERT
(Devlin et al. , 2019), XLM ( Lample and Conneau ,
2019), and XLNet ( Y ang et al. , 2019) have
brought signiﬁcant performance gains, but it can
be challenging to determine which aspects of
the methods contribute the most. Training is
computationally expensive, limiting the amount
of tuning that can be done, and is often done with
private training data of varying sizes, limiting
our ability to measure the effects of the modeling
advances.
∗ Equal contribution.
1 Our models and code are available at:
https://github.com/pytorch/fairseq
W e present a replication study of BERT pre-
training ( Devlin et al. , 2019), which includes a
careful evaluation of the effects of hyperparmeter
tuning and training set size. W e ﬁnd that BERT
was signiﬁcantly undertrained and propose an im-
proved recipe for training BERT models, which
we call RoBERT a, that can match or exceed the
performance of all of the post-BERT methods.
Our modiﬁcations are simple, they include: (1)
training the model longer, with bigger batches,
over more data; (2) removing the next sentence
prediction objective; (3) training on longer se-
quences; and (4) dynamically changing the mask-
ing pattern applied to the training data. W e also
collect a large new dataset (CC-NEW S ) of compa-
rable size to other privately used datasets, to better
control for training set size effects.
When controlling for training data, our im-
proved training procedure improves upon the pub-
lished BERT results on both GLUE and SQuAD.
When trained for longer over additional data, our
model achieves a score of 88.5 on the public
GLUE leaderboard, matching the 88.4 reported
by
Y ang et al. (2019). Our model establishes a
new state-of-the-art on 4/9 of the GLUE tasks:
MNLI, QNLI, RTE and STS-B. W e also match
state-of-the-art results on SQuAD and RACE.
Overall, we re-establish that BERT’s masked lan-
guage model training objective is competitive
with other recently proposed training objectives
such as perturbed autoregressive language model-
ing (
Y ang et al. , 2019).2
In summary , the contributions of this paper
are: (1) W e present a set of important BERT de-
sign choices and training strategies and introduce
2 It is possible that these other methods could also improve
with more tuning. W e leave this exploration to future work.



Source: data\tc18_2309.07597v5\referenced_papers\[62]_2004.05986.pdf (Page 14):

Masking Type Data
Source
Training
Tokens #
Device Training
Steps
Batch
Size
Optimizer V ocabularyInit Ckpt
BERT-base WordPiecebase wiki 0.4B TPU Pod
v2
- - AdamW 21,128 Random
Init
BERT-wwm-ext-base WWM base wiki+ext 5.4B TPU v3 1M 384 LAMB ∼BERT ∼BERT
ALBERT-tiny WWM tiny CLUE
corpus
5B TPU Pod
v3
500k 4k LAMB ∼BERT Random
Init
ALBERT-xxlarge Span large CLUE
corpus
5B TPU Pod
v3
1M 8k AdamW ∼BERT Random
Init
ERNIE-base Knowledge
Masking
base wiki+ext 15B NVidia
v100
1M 8192 Adam 17964 Random
Init
XLNet-mid Sentence
Piece
mid wiki+ext 5.4B TPU v3 2M 32 Adam 32000 Random
Init
RoBERTa-large WWM large CLUE
corpus
5B TPU Pod 100k 8k AdamW ∼BERT Random
Init
RoBERTa-wwm-ext-base WWM base wiki+ext 5.4B TPU v3 1M 384 AdamW ∼BERT ∼BERT
RoBERTa-wwm-ext-large WWM large wiki+ext 5.4BTPU Pod
v3-32
2M 512 AdamW ∼BERT Random
Init
Table 6: Parameters for pre-training. ”BERT-base” is released by google (Devlin et al., 2019). “WWM”
stands for whole word masking. “ext” presents for extended data, different models may use different
extended data. “∼BERT” means similar to Google’s Chinese BERT.
Model Batch Size Max Length Epoch Learning Rate
AFQMC All* 16 128 3 2e-5
TNEWS All* 16 128 3 2e-5
IFLYTEK ALBERT-tiny 32 128 10 2e-5
RoBERT-large, RoBERTa-wwm-ext-large 24 128 3 2e-5
All* except above 32 128 3 2e-5
OCNLI BERT-base, RoBERTa-wwm-ext-large 32 128 3 2e-5
RoBERTa-wwm-ext, ERNIE 32 128 3 3e-5
ALBERT-tiny 32 128 4 5e-5
XLNET-mid 32 128 3 5e-5
CLUEWSC2020 ALBERT-tiny 8 128 50 1e-4
All* except ALBERT-tiny 8 128 50 2e-5
CSL RoBERTa-large 4 256 5 5e-6
All* except above 4 256 5 1e-5
CMRC* ALBERT-tiny 32 512 3 2e-4
RoBERTa-wwm-ext-large 32 512 2 2.5e-5
RoBERTa-large 32 256 2 3e-5
XLNET-mid, RoBERTa-wwm-ext-base 32 512 2 3e-5
All* except above 32 512 2 3e-5
CHID All* 24 64 3 2e-5
C3 All* 24 512 8 2e-5
Table 7: Parameters for ﬁne-tuning. CMRC* presents for CMRC dataset in 2018. All* means ALBERT-
tiny, BERT-base, BERT-wwm-ext-base, ERNIE-base, RoBERTa-large, XLNet-mid, RoBERTa-wwm-ext-
base and RoBERTa-wwm-ext-large namely. It should be noted that RoBERTa-large is pre-trained with
256 sequence length, which is shorter than 512 length pre-trained for others. So we individually limit
the length of RoBERTa-large to 256 for CMRC*, and use the striding text span to relieve this problem.
However, this drawback of RoBERTa-large may decrease performances of some datasets whose length
can not be effectively cut down, such as C3.



### Claim 20/38

#### Claim Text
In this work, single and small vacancy clusters migrate in 3D mode and vacancy clusters containing more than 5 vacancies are considered immobile, which is a common assumption in OKMC and cluster dynamics simulations .

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 15):

Preprint
(a) 182539: monotonic function
 (b) 1117099: active margin
Query
Relevant
ANCE Neg
BM25 Neg
Rand Neg (c) 1132213: yoga bow
Figure 7: t-SNE Plots for Losing Cases in Table 9.
margin” is a geographical terminology, not a ﬁnancial one (which we did not know ourselves and took some time
to ﬁgure out when conducting this case study). There are also some cases where the dense retrieved documents
make sense to us but were labeled irrelevant.
The t-SNE plots in Fig. 6 and Fig. 7 show many interesting patterns of the learned representation space. The
ANCE winning cases often correspond to clear separations of different document groups. For losing cases the
representation space is more mixed, or there is too few relevant documents which may cause the variances in
model performances. There are also many different interesting patterns in the ANCE-learned representation
space. We include the t-SNE plots for all 43 TREC DL Track queries in the supplementary material. More future
analyses of the learned patterns in the representation space may help provide more insights on dense retrieval.
16



Source: data\tc18_2309.07597v5\referenced_papers\[62]_2004.05986.pdf (Page 13):

TNEWS
sentence: 如果我的世界下架了，你会玩迷你世界吗？
sentence (en): If Minecraft is gone, will you play miniworld?
label: 116(news game)
iFLYTEK
sentence: 《钢铁英雄》是一款角色扮演类游戏。游戏拥有 ...... 带领他们逃出去。修复部分小错误，提升整
体稳定性。
sentence (en): ”Heroes of Steel” is a role-playing game. The game has ...... all four heroes are imprisoned and you
will lead them out. repair part small Errors to improve overall stability.
label: 22(Strategy)
CLUEWSC
text: 这时候放在床上枕头旁边的手机响了，我感到奇怪，因为欠费已被停机两个月，现在它突然响了。
text (en): At this moment, the cellphone on the bed next to the pillow rang. I feel this is quite strange because the
cellphone plan was terminated two months ago since I did not pay the bill. Now it was ringing all of a sudden.
label: true
AFQMC
sentence1: 本月花呗还不上怎么办 sentence2: 花呗超时怎么办
sentence1 (en): What to do if Ant Credit Pay is not available yet this month sentence2 (en): How to deal with Ant
Credit Pay overtime
label: 0(different)
CSL
abst: 不同阶段电子数据的操作都会留下表现各异的轨迹.从操作系统、计算机应用系统 ...... 分析审计电子数
据轨迹在计算机系统中表现形式,可以为审计人员提供有效的审计方法
keyword: [“计算机审计”, “数据轨迹”, “日志文件”]
abst (en): The operation of electronic data in different stages will leave different traces. From operating system,
computer application system ...... provide effective audit methods for auditors by analyzing the expression of audit
electronic data trace in computer system.
keyword (en): [“computer audit”, “data trace”, “log ﬁle”]
label: 0(false)
OCNLI
premise: 但是不光是中国,日本,整个东亚文化都有这个特点就是被权力影响很深 hypothesis: 有超过两个东
亚国家有这个特点
premise (en): But not only China and Japan, the entire East Asian culture has this feature, that is it is deeply inﬂuenced
by the power. hypothesis (en): More than two East Asian countries have this feature.
label: entailment
CMRC 2018
context: 萤火虫工作室是一家总部设在英国伦敦和康涅狄格州坎顿...... 目前，他们正在开发PC和Xbox360上
的次时代游戏。
question: 萤火虫工作室的总部设在哪里？ answer: 英国伦敦和康涅狄格州坎顿
context (en): Fireﬂy Studios is a video game developer based in London, UK and Canton, Connecticut, with a quality
department in Aberdeen, Scotland ...... Currently, they are developing next-generation games on PC and Xbox 360.
question (en): Where is Fireﬂy Studios headquartered? answer (en): London, UK and Canton, Connecticut
ChID
content: 中国青年报：篮协改革联赛切莫#idiom#......
candidates: [“急功近利”, “画蛇添足”, “本末倒置”(answer)]
content (en): China Youth Daily: Chinese Basketball Association should not #idiom# when reforming the league ......
candidates (en): [“seeking instant beneﬁt”, “to overdo it”, “take the branch for the root ”(answer)]
C3 document: 男：我们坐在第七排，应该能看清楚字幕吧? 女：肯定可以，对了，我们得把手机设成振动。
question: 他们最可能在哪儿?
candidates: [“图书馆”, “体育馆”,“电影院”(answer),“火车站”]
document (en): Man: Our seats are in the seventh row. We should be able to see the subtitles clearly, right? Woman:
Absolutely. By the way, we should set the phone to vibrate.
question (en): Where does the conversation most probably take place?
candidates (en): [“In a library”, “In a stadium”,“In a cinema ”(answer),“At a train station”]
Table 5: Development set examples from the tasks in CLUE. Bold text represents part of the example
format for each task. Chinese text is part of the model input, and the corresponding text in italics is
the English version translated from that. Underlined text is specially marked in the input. Text in a
monospaced font represents the expected model output.



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 12):

Preprint
0 100 2000.2
0.5
0.8
1.1
1.4Loss
NDCG@10
Loss
(a) 10k Batch; 4:4; 1e-5
0 100 200 (b) 20k Batch; 8:4; 1e-6
0 100 200 (c) 5k Batch; 4:8; 1e-6
0 100 200
0.4
0.5
0.6
NDCG@10 (d) 10k Batch; 4:4; 5e-6
Figure 5: Training loss and testing NDCG of ANCE (FirstP) on documents, with different ANN
index refreshing (e.g., per 10k Batch), Trainer:Inferencer GPU allocation, and learning rate (e.g.,
1e-5). X-axes is the training steps in thousands.
Table 7: Results of several different hyperparameter conﬁgurations. “Top K Neg” lists the top k ANN
retrieved candidates from which we sampled the ANCE negatives from.
Hyperparameter MARCO Dev PassageTREC DL Document
Learning rate Top K Neg Refresh (step) Retrieval MRR@10 Retrieval NDCG@10
Passage ANCE 1e-6 200 10k 0.33 –
1e-6 500 10k 0.31 –
2e-6 200 10k 0.29 –
2e-7 500 20k 0.303 –
2e-7 1000 20k 0.302 –
Document ANCE 1e-5 100 10k – 0.58
1e-6 100 20k – 0.59
1e-6 100 5k – 0.60
5e-6 200 10k – 0.614
1e-6 200 10k – 0.61
A.2 O VERLAP WITH SPARSE RETRIEVAL IN TREC 2019 DL T RACK
As a nature of TREC-style pooling evaluation, only those ranked in the top 10 by the 2019 TREC participating
systems were labeled. As a result, documents not in the pool and thus not labeled are all considered irrelevant,
even though there may be relevant ones among them. When reusing TREC style relevance labels, it is very
important to keep track of the “hole rate” on the evaluated systems, i.e., the fraction of the top K ranked results
without TREC labels (not in the pool). A larger hole rate shows that the evaluated methods are very different
from those systems that participated in the Track and contributed to the pool, thus the evaluation results are not
perfect. Note that the hole rate does not necessarily reﬂect the accuracy of the system, only the difference of it.
In TREC 2019 Deep Learning Track, all the participating systems are based on sparse retrieval. Dense retrieval
methods often differ considerably from sparse retrievals and in general will retrieve many new documents. This
is conﬁrmed in Table 6. All DR methods have very low overlap with the ofﬁcial BM25 in their top 100 retrieved
documents. At most, only 25% of documents retrieved by DR are also retrieved by BM25. This makes the hole
rate quite high and the recall metric not very informative. It also suggests that DR methods might beneﬁt more
in this year’s TREC 2020 Deep Learning Track if participants are contributing DR based systems.
The MS MARCO ranking labels were not constructed based on pooling the sparse retrieval results. They were
from Bing (Bajaj et al., 2016), which uses many signals beyond term overlap. This makes the recall metric in
MS MARCO more robust as it reﬂects how a single model can recover a complex online system.
A.3 I MPACT OF ASYNCHRONOUS GAP
Fig. 5 illustrates the behavior of asynchronous learning with different conﬁgurations. A large learning rate or a
low refreshing rate (Figure 5(a) and 5(b)) leads to ﬂuctuations as the async gap of the ANN index may drive
the representation learning to undesired local optima. Refreshing as often as every 5k Batches yields a smooth
convergence (Figure 5(c)), but requires twice as many GPU allocated to the Inferencer. A 1:1 GPUs allocation
of Trainer and Inference with appropriate learning rates is adequate to minimize the impact of async gap.
13



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 17):

Dataset Text Label
AmazonCounterfactualClassiﬁcation In person it looks as though it would have cost a lot more. counterfactual
AmazonPolarityClassiﬁcation an absolute masterpiece I am quite sure any of you actually taking the time to read this have played the game at least
once, and heard at least a few of the tracks here. And whether you were aware of it or not, Mitsuda’s music contributed
greatly to the...
positive
AmazonReviewsClassiﬁcation solo llega una unidad cuando te obligan a comprar dos Te obligan a comprar dos unidades y te llega solo una y no hay
forma de reclamar, una autentica estafa, no compreis!!
0
Banking77Classiﬁcation What currencies is an exchange rate calculated in? exchange_rate
EmotionClassiﬁcation i feel so inhibited in someone elses kitchen like im painting on someone elses picture sadness
ImdbClassiﬁcation When I ﬁrst saw a glimpse of this movie, I quickly noticed the actress who was playing the role of Lucille Ball. Rachel
York’s portrayal of Lucy is absolutely awful. Lucille Ball was an astounding comedian with incredible talent. To think
about a legend like Lucille Ball being portrayed the way she was in the movie is horrendous. I cannot believe...
negative
MassiveIntentClassiﬁcation réveille-moi à neuf heures du matin le vendredi alarm_set
MassiveScenarioClassiﬁcation tell me the artist of this song music
MTOPDomainClassiﬁcation Maricopa County weather forecast for this week weather
MTOPIntentClassiﬁcation what ingredients do is have left GET_INFO_RECIPES
ToxicConversationsClassiﬁcation The guy’s a damn cop, so what do you expect? toxic
TweetSentimentExtractionClassiﬁcation I really really like the song Love Story by Taylor Swift positive
Table 3: Classiﬁcation examples
Dataset Text Cluster
ArxivClusteringP2P Finite groups of rank two which do not involveQd(p). Let p >3 be a prime. We show that ifG is a ﬁnite group
with p-rank equal to 2, thenG involvesQd(p) if and only ifG p′-involvesQd(p). This allows us to use a version
of Glauberman’s ZJ-theorem to give a more direct construction of ﬁnite group actions on mod-p homotopy spheres.
We give an example to illustrate that the above conclusion does not hold forp ≤ 3.
math
ArxivClusteringS2S Vertical shift and simultaneous Diophantine approximation on polynomial curves math
BiorxivClusteringP2P Innate Immune sensing of Inﬂuenza A viral RNA through IFI16 promotes pyroptotic cell death Programmed cell death
pathways are triggered by various stresses or stimuli, including viral infections. The mechanism underlying the regula-
tion of these pathways upon Inﬂuenza A virus IA V infection is not well characterized. We report that a cytosolic DNA
sensor IFI16 is...
immunology
BiorxivClusteringS2S Association of CDH11 with ASD revealed by matched-gene co-expression analysis and mouse behavioral neuroscience
MedrxivClusteringP2P Temporal trends in the incidence of haemophagocytic lymphohistiocytosis: a nationwide cohort study from England
2003-2018. Haemophagocytic lymphohistiocytosis (HLH) is rare, results in high mortality and is increasingly being
diagnosed. Little is known about what is driving the apparent rise in the incidence of this disease. Using national linked
electronic health data from hospital admissions and death certiﬁcation cases of HLH that were diagnosed in England
between 1/1/2003 and 31/12/2018 were identiﬁed using a previously validated approach. We calculated incidence...
infectious diseases
MedrxivClusteringS2S Current and Lifetime Somatic Symptom Burden Among Transition-aged Young Adults on the Autism Spectrum psychiatry and clinical psychology
RedditClustering Could anyone tell me what breed my bicolor kitten is? r/cats
RedditClusteringP2P Headaches after working out? Hey guys! I’ve been diagnosed with adhd since I was seven. I just recently got rediag-
nosed (22f) and I’ve been out on a different medication, adderall I was normally taking vyvanse but because of cost and
no insurance adderall was more affordable. I’ve noticed that if I take adderall and workout...
r/ADHD
StackExchangeClustering Does this property characterize a space as Hausdorff? math.stackexchange.com
StackExchangeClusteringP2P Google play services error DEBUG: Application is pausing, which disconnects the RTMP client. I am having this issue
from past day with Google Play Services Unity. What happens is, when I install app directly ot device via Unity, the
Google Play Services work ﬁne but when I upload it as beta to play store console and install it via that then it starts to
give " DEBUG: Application is pausing, which disconnects the RTMP client" error. I have a proper SHA1 key.
unity
TwentyNewsgroupsClustering Commercial mining activities on the moon 14
Table 4: Clustering examples
Dataset Sentence 1 Sentence 2 Label
SprintDuplicateQuestions Franklin U722 USB modem signal strength How do I know if my Franklin U772 USB Modem has a
weak signal ?
1
TwitterSemEval2015 All the home alones watching 8 mile","All the home alones
watching 8 mile
The last rap battle in 8 Mile nevr gets old ahah 0
TwitterURLCorpus How the metaphors we use to describe discovery affect men
and women in the sciences
Light Bulbs or Seeds ? How Metaphors for Ideas Inﬂuence
Judgments About Genius
0
Table 5: Pair classiﬁcation examples. Labels are binary.



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 4):

Class. Clust. PairClass. Rerank. Retr. STS Summ. Avg.
Num. Datasets (→) 12 11 3 4 15 10 1 56
Self-supervised methods
Glove 57.29 27.73 70.92 43.29 21.62 61.85 28.87 41.97
Komninos 57.65 26.57 72.94 44.75 21.22 62.47 30.49 42.06
BERT 61.66 30.12 56.33 43.44 10.59 54.36 29.82 38.33
SimCSE-BERT-unsup 62.50 29.04 70.33 46.47 20.29 74.33 31.15 45.45
Supervised methods
SimCSE-BERT-sup 67.32 33.43 73.68 47.54 21.82 79.12 23.31 48.72
coCondenser-msmarco 64.71 37.64 81.74 51.84 32.96 76.47 29.50 52.35
Contriever 66.68 41.10 82.53 53.14 41.88 76.51 30.36 56.00
SPECTER 52.37 34.06 61.37 48.10 15.88 61.02 27.66 40.28
LaBSE 62.71 29.55 78.87 48.42 18.99 70.80 31.05 45.21
LASER2 53.65 15.28 68.86 41.44 7.93 55.32 26.80 33.63
MiniLM-L6 63.06 42.35 82.37 58.04 41.95 78.90 30.81 56.26
MiniLM-L12 63.21 41.81 82.41 58.44 42.69 79.80 27.90 56.53
MiniLM-L12-multilingual 64.30 37.14 78.45 53.62 32.45 78.92 30.67 52.44
MPNet 65.07 43.69 83.04 59.36 43.81 80.28 27.49 57.78
MPNet-multilingual 67.91 38.40 80.81 53.80 35.34 80.73 31.57 54.71
OpenAI Ada Similarity 70.44 37.52 76.86 49.02 18.36 78.60 26.94 49.52
SGPT-125M-nli 61.46 30.95 71.78 47.56 20.90 74.71 30.26 45.97
SGPT-5.8B-nli 70.14 36.98 77.03 52.33 32.34 80.53 30.38 53.74
SGPT-125M-msmarco 60.72 35.79 75.23 50.58 37.04 73.41 28.90 51.23
SGPT-1.3B-msmarco 66.52 39.92 79.58 54.00 44.49 75.74 25.44 56.11
SGPT-2.7B-msmarco 67.13 39.83 80.65 54.67 46.54 76.83 27.87 57.12
SGPT-5.8B-msmarco 68.13 40.35 82.00 56.56 50.25 78.10 24.75 58.81
SGPT-BLOOM-7.1B-msmarco66.19 38.93 81.90 55.65 48.21 77.74 24.99 57.44
GTR-Base 65.25 38.63 83.85 54.23 44.67 77.07 29.67 56.19
GTR-Large 67.14 41.60 85.33 55.36 47.42 78.19 29.50 58.28
GTR-XL 67.11 41.51 86.13 55.96 47.96 77.80 30.21 58.42
GTR-XXL 67.41 42.42 86.12 56.65 48.48 78.38 30.64 58.97
ST5-Base 69.81 40.21 85.17 53.09 33.63 81.14 31.39 55.27
ST5-Large 72.31 41.65 84.97 54.00 36.71 81.83 29.64 57.06
ST5-XL 72.84 42.34 86.06 54.71 38.47 81.66 29.91 57.87
ST5-XXL 73.42 43.71 85.06 56.43 42.24 82.63 30.08 59.51
Table 1: Average of the main metric (see Section 3.2) per task per model on MTEB English subsets.
Paragraph to paragraph (P2P) A paragraph is
compared with another paragraph. MTEB imposes
no limit on the input length, leaving it up to the
models to truncate if necessary. Several clustering
tasks are framed as both S2S and P2P tasks. The
former only compare titles, while the latter include
both title and content. For ArxivClustering, for
example, abstracts are concatenated to the title in
the P2P setting.
Sentence to paragraph (S2P) A few retrieval
datasets are mixed in a S2P setting. Here a query
is a single sentence, while documents are long
paragraphs consisting of multiple sentences.
Similarities across 56 MTEB datasets are vi-
sualized in Figure 2. Several datasets rely on
the same corpora, such as ClimateFEVER and
FEVER, resulting in a score of 1. Clusters of simi-
lar datasets can be seen among CQADupstack vari-
ations and STS datasets. S2S and P2P variations of
the same dataset tend to also be similar. Scientiﬁc
datasets, such as SciDocsRR, SciFact, ArxivClus-
tering, show high similarities among each other
even when coming from different tasks (Reranking,
Retrieval and Clustering in this case).
4 Results
4.1 Models
We evaluate on the test splits of all datasets except
for MSMARCO, where the dev split is used follow-
ing Thakur et al. (2021). We benchmark models
claiming state-of-the-art results on various embed-



### Claim 21/38

#### Claim Text
The δ-doped and (100)-implanted samples exhibit comparable fluorescence in the bulk, whereas the (111)-implanted one has fluorescence about four times smaller, which could be related to a less efficient NV yield by the implantation at this crystallographic orientation .By the δ-doped sample the saturation curves were measured before and after each annealing step.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 6):

Preprint
Table 4: OpenQA Test Scores in Single Task
Setting. ANCE+Reader switches the retrieve
of a system from DPR to ANCE and keeps the
same reading model, which is RAG-Token on
Natural Questions (NQ) and DPR Reader on
Trivia QA (TQA).
Model NQ TQA
T5-11B (Roberts et al., 2020) 34.5 -
T5-11B + SSM (Roberts et al., 2020)36.6 -
REALM (Guu et al., 2020) 40.4 -
DPR (Karpukhin et al., 2020) 41.5 56.8
DPR + BM25 (Karpukhin et al., 2020)39.0 57.0
RAG-Token (Lewis et al., 2020b)44.1 55.2
RAG-Sequence (Lewis et al., 2020b)44.5 56.1
ANCE + Reader 46.0 57.5
Table 5: Efﬁciency of ANCE Search and Training.
Operation Ofﬂine Online
BM25 Index Build 3h –
BM25 Retrieval – 37ms
BERT Rerank – 1.15s
Sparse IR Total (BM25 + BERT) – 1.42s
ANCE Inference
Encoding of Corpus/Per doc 10h/4.5ms –
Query Encoding – 2.6ms
ANN Retrieval (batched q) – 9ms
Dense Retrieval Total – 11.6ms
ANCE Training
Encoding of Corpus/Per doc 10h/4.5ms –
ANN Index Build 10s –
Neg Construction Per Batch 72ms –
Back Propagation Per Batch 19ms –
0 50 100 150 200
7
8
9
10
11
12
13
14
15
Positive
Negative
(a) ANCE FirstP (100%)
0 50 100 150 200
20
25
30
35 (b) NCE Neg (0%)
0 50 100 150 200
20
25
30
35
40 (c) Rand Neg (0%)
0 50 100 150 200
20
25
30
35 (d) BM25+Rand (7%)
Figure 3: The top DR scores for 10 random TREC DL testing queries. The x-axes are their ranking
order. The y-axes are their retrieval scores minus corpus average. All models are warmed up by BM25
Neg. The percentages are the overlaps between the testing and training negatives near convergence.
6.2 E MPIRICAL ANALYSES ON TRAINING CONVERGENCE
We ﬁrst show the long tail distribution of search relevance in dense retrieval. As plotted in Fig. 3,
there are a few instances per query with signiﬁcant higher retrieval scores, while the majority form
a long tail. In retrieval/ranking, the key challenge is to distinguish the relevant ones among those
highest scored ones; the rest is trivially irrelevant. We also empirically measure the probability of
local in-batch negatives including informative negatives (D−∗), by their overlap with top 100 highest
scored negatives. This probability, either using NCE Neg or Rand Neg, iszero, the same as our theory
assumes. In comparison, the overlap between BM25 Neg with top DR retrieved negatives is 15%,
while that of ANCE negatives starts at 63% and converges to 100% by design.
Then we empirically validate our theory that local negatives lead to lower loss, bounded gradient
norm, and thus slow convergence. The training loss and pre-clip gradient norms during DR training
are plotted in Fig. 4. As expected, the uninformative local negatives are trivial to separate, yielding
near-zero training loss, while ANCE global negatives are much harder and maintain a high training
loss. The same with our theoretical assumption, the gradient norms of local negatives are indeed
restricted closely to zero. In comparison, the gradient norms on ANCE global negatives are bigger
by orders of magnitude. This conﬁrms ANCE better approximates the oracle importance sampling
distribution p∗
d− ∝||∇θtl(d+,d−)||2 and improves learning convergence.
6.3 D ISCUSSIONS
We use BERT-Siamese and NLL loss to be consistent with recent research. We have experimented
with cosine similarity and BCE/hinge loss, where we observe even smaller gradient norms on local
negatives. But the retrieval accuracy is not much better. We include additional experiments in
Appendix. A.2 discusses the surprisingly small overlap (<25%) between dense retrieval results and
sparse retrieval results. DR is a fundamentally different approach and more studies are required to
understand its behavior. A.3 and A.4 study the asynchronous gaps and hyperparameters. A.5 includes
case studies that the irrelevant documents from ANCE are often still “semantically related” and very
different from those made by sparse retrieval.
7



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 23):

Model (→) Lexical Sparse Dense Late-Interaction Re-ranking
Dataset (↓) BM25 DeepCT SPARTA docT5query DPR ANCE TAS-B GenQ ColBERT BM25+CE
MS MARCO 0.658 0.752‡ 0.793‡ 0.819‡ 0.552 0.852‡ 0.884‡ 0.884‡ 0.865‡ 0.658‡
TREC-COVID0.498⋆ 0.347⋆ 0.409⋆ 0.541⋆ 0.212⋆ 0.457⋆ 0.387⋆ 0.456⋆ 0.464⋆ 0.498⋆
BioASQ 0.714 0.699 0.351 0.646 0.256 0.463 0.579 0.627 0.645 0.714
NFCorpus 0.250 0.235 0.243 0.253 0.208 0.232 0.280 0.280 0.254 0.250
NQ 0.760 0.636 0.787 0.832 0.880‡ 0.836 0.903 0.862 0.912 0.760
HotpotQA 0.740 0.731 0.651 0.709 0.591 0.578 0.728 0.673 0.748 0.740
FiQA-2018 0.539 0.489 0.446 0.598 0.342 0.581 0.593 0.618 0.603 0.539
Signal-1M (RT)0.370 0.299 0.270 0.351 0.162 0.239 0.304 0.281 0.283 0.370
TREC-NEWS0.422 0.316 0.262 0.439 0.215 0.398 0.418 0.412 0.367 0.422
Robust04 0.375 0.271 0.215 0.357 0.211 0.274 0.331 0.298 0.310 0.375
ArguAna 0.942 0.932 0.893 0.972 0.751 0.937 0.942 0.978 0.914 0.942
Touché-2020 0.538 0.406 0.381 0.557 0.301 0.458 0.431 0.451 0.439 0.538
CQADupStack0.606 0.545 0.521 0.638 0.403 0.579 0.622 0.654 0.624 0.606
Quora 0.973 0.954 0.896 0.982 0.470 0.987 0.986 0.988 0.989 0.973
DBPedia 0.398 0.372 0.411 0.365 0.349 0.319 0.499 0.431 0.461 0.398
SCIDOCS 0.356 0.314 0.297 0.360 0.219 0.269 0.335 0.332 0.344 0.356
FEVER 0.931 0.735 0.843 0.916 0.840 0.900 0.937 0.928 0.934 0.931
Climate-FEVER0.436 0.232 0.227 0.427 0.390 0.445 0.534 0.450 0.444 0.436
SciFact 0.908 0.893 0.863 0.914 0.727 0.816 0.891 0.893 0.878 0.908
Table 9: In-domain and zero-shot retrieval performance on BEIR datasets. Scores denote Recall@100. The
best retrieval performance on a given dataset is marked in bold, and the second best performance is underlined.
‡ indicates in-domain retrieval performance. ⋆ shows the capped Recall@100 score (Appendix G).
0 100 200 300 400 500
TREC-COVID
SBERT (Method)
Cosine-Sim.
Dot-Prod.
0 5 10 15 20 25
Signal-1M (RT)
SBERT (Method)
Cosine-Sim.
Dot-Prod.
0 100 200 300 400 500
FEVER
SBERT (Method)
Cosine-Sim.
Dot-Prod.
TREC-COVID Signal-1M (RT) FEVER
Cosine-Sim. Dot-Prod. Cosine-Sim. Dot-Prod. Cosine-Sim. Dot-Prod.
0.482 0.635 0.261 0.243 0.670 0.685
Table 10: Violin plots [22] of document lengths for the top-10 retrieved hits and nDCG@10 scores using a
distilbert-base-uncased model trained with either cosine similarity (blue, top) or dot product (orange, bottom) as
described in Appendix H.
24



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 2):

Preprint
instances, which have to be sampled in training:
θ∗= argminθ
∑
q
∑
d+∈D+
∑
d−∈ˆD−
l(f(q,d+),f(q,d−)). (3)
A natural choice is to sample negatives ˆD−from top documents retrieved by BM25. However, they
may bias the DR model to merely learn sparse retrieval and do not elevate DR models much beyond
BM25 (Luan et al., 2020). Another way is to sample negatives in local mini-batches, e.g., as in
contrastive learning (Oord et al., 2018; Chen et al., 2020a), however, these local negatives do not
signiﬁcantly outperform BM25 negatives (Karpukhin et al., 2020; Luan et al., 2020).
3 A NALYSES ON THE CONVERGENCE OF DENSE RETRIEVAL TRAINING
In this section, we provide theoretical analyses on the convergence of representation training in dense
retrieval. We ﬁrst show the connections between learning convergence and gradient norms, then the
bounded gradient norms by uninformative negatives, and ﬁnally, how in-batch local negatives are
ineffective under common conditions in dense retrieval.
Convergence Rate and Gradient Norms: Let l(d+,d−) = l(f(q,d+),f(q,d−) be the loss func-
tion on the training triple (q,d+,d−), PD− the negative sampling distribution for the given (q,d+),
and pd− the sampling probability of negative instance d−, a stochastic gradient decent (SGD) step
with importance sampling (Alain et al., 2015) is:
θt+1 = θt −η 1
Npd−
∇θtl(d+,d−), (4)
with θt the parameter at t-th step, θt+1 the one after, and N the total number of negatives. The scaling
factor 1
Npd−
is to make sure Eqn. 4 is an unbiased estimator of the full gradient.
Then we can characterize the converge rate of this SGD step as the movement to optimalθ∗. Following
derivations in variance reduction (Katharopoulos & Fleuret, 2018; Johnson & Guestrin, 2018), let
gd− = 1
Npd−
∇θtl(d+,d−) the weighted gradient, the convergence rate is:
E∆t = ||θt −θ∗||2 −EPD− (||θt+1 −θ∗||2) (5)
= ||θt||2 −2θT
t θ∗−EPD− (||θt −ηgd−||2) + 2θ∗TEPD− (θt −ηgd−) (6)
= −η2EPD− (||gd−||2) + 2ηθT
t EPD− (gd−) −2ηθ∗TEPD− (gd−) (7)
= 2ηEPD− (gd−)T(θt −θ∗) −η2EPD− (||gd−||2) (8)
= 2ηEPD− (gd−)T(θt −θ∗) −η2EPD− (gd−)TEPD− (gd−) −η2Tr(VPD− (gd−)). (9)
This shows we can obtain better convergence rate by sampling from a distributionPD− that minimizes
the variance of the gradient estimator,EPD− (||gd−||2), or Tr(VPD− (gd−)) as the estimator is unbiased.
There exists an optimal distribution that:
p∗
d− = argminpd− Tr(VPD− (gd−)) ∝||∇θtl(d+,d−)||2, (10)
which is to sample proportionally to per instance gradient norm. This is a well known result in
importance sampling (Alain et al., 2015; Johnson & Guestrin, 2018). It can be proved by applying
Jensen’s inequality on the gradient variance and then verifying that Eqn. 10 achieves the minimum.
We do not repeat this proof and refer to Johnson & Guestrin (2018) for exact derivations.
Intuitively, an negative instance with larger gradient norm is more likely to reduce the training loss
more, while those with diminishing gradients are not informative. Empirically, the correlation of
gradient norm and training convergence is also observed in BERT ﬁne-tuning (Mosbach et al., 2020).
Diminishing Gradients of Uninformative Negatives: The oracle distribution in Eqn. 10 is too
expensive to compute and the closed form of gradient norms can be complicated in deep neural
networks. Nevertheless, for MLP networks, Katharopoulos & Fleuret (2018) derives an upper bound
of the per sample gradient norm:
||∇θtl(d+,d−)||2 ≤Lρ||∇φLl(d+,d−)||2, (11)
3



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 13):

Preprint
Table 8: Queries in the TREC 2019 DL Track Document Ranking Tasks where ANCE performs
better than BM25. Snippets are manually extracted. The documents in the ﬁrst disagreed ranking
position are shown, where on all examples ANCE won. The NDCG@10 of ANCE and BM25 in the
corresponding query is listed.
ANCE BM25
Query: qid (104861): Cost of interior concrete ﬂooring
Title: Concrete network: Concrete Floor Cost Pinterest: Types of Flooring
DocNo: D293855 D2692315
Snippet: For a concrete ﬂoor with a basic ﬁnish,
you can expect to pay $2 to $12 per
square foot. . .
Know About Hardwood Flooring And
Its Types White Oak Floors Oak Floor-
ing Laminate Flooring In Bathroom . . .
Ranking Position: 1 1
TREC Label: 3 (Very Relevant) 0 (Irrelevant)
NDCG@10: 0.86 0.15
Query: qid (833860): What is the most popular food in Switzerland
Title: Wikipedia: Swiss cuisine Answers.com: Most popular traditional
food dishes of Mexico
DocNo: D1927155 D3192888
Snippet: Swiss cuisine bears witness to many re-
gional inﬂuences, . . . Switzerland was
historically a country of farmers, so tra-
ditional Swiss dishes tend not to be. . .
One of the most popular traditional Mex-
ican deserts is a spongy cake . . . (in
the related questions section) What is
the most popular food dish in Switzer-
land?. . .
Ranking Position: 1 1
TREC Label: 3 (Very Relevant) 0 (Irrelevant)
NDCG@10: 0.90 0.14
Query: qid (1106007): Deﬁne visceral
Title: V ocabulary.com: Visceral Quizlet.com: A&P EX3 autonomic 9-10
DocNo: D542828 D830758
Snippet: When something’s visceral, you feel it
in your guts. A visceral feeling is in-
tuitive — there might not be a rational
explanation, but you feel that you know
what’s best. . .
Acetylcholine A neurotransmitter liber-
ated by many peripheral nervous system
neurons and some central nervous sys-
tem neurons. . .
Ranking Position: 1 1
TREC Label: 3 (Very Relevant) 0 (Irrelevant)
NDCG@10: 0.80 0.14
A.4 H YPERPARAMETER STUDIES
We show the results of some hyperparameter conﬁgurations in Table 7. The cost of training with BERT makes it
difﬁcult to conduct a lot hyperparameter exploration. Often a failed conﬁguration leads to divergence early in
training. We barely explore other conﬁgurations due to the time-consuming nature of working with pretrained
language models. Our DR model architecture is kept consistent with recent parallel work and the learning
conﬁgurations in Table 7 are about all the explorations we did. Most of the hyperparameter choices are decided
solely using the training loss curve and otherwise by the loss in the MARCO Dev set. We found the training loss,
validation NDCG, and testing performance align well in our (limited) hyperparameter explorations.
A.5 C ASE STUDIES
In this section, we show Win/Loss case studies between ANCE and BM25. Among the 43 TREC 2019 DL Track
evaluation queries in the document task, ANCE outperforms BM25 on 29 queries, loses on 13 queries, and
ties on the rest 1 query. The winning examples are shown in Table 8 and the losing ones are in Table 9. Their
corresponding ANCE-learned (FirstP) representations are illustrated by t-SNE in Fig. 6 and Fig. 7.
In general, we found ANCE better captures the semantics in the documents and their relevance to the query. The
winning cases show the intrinsic limitations of sparse retrieval. For example, BM25 exact matches the “most
popular food” in the query “what is the most popular food in Switzerland” but using the document is about
Mexico. The term “Switzerland” only appears in the related question section of the web page.
14



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 3):

Preprint
q
𝑑!
𝐷!! "#
"
Trainer
Inferencer
q
𝑑!
𝐷!! "$
"
Checkpoint k-1
…
Checkpoint k
q
𝑑!
𝐷!! "$
"
q
𝑑!
𝐷!!
"
…
Checkpoint k+1
q
𝑑!
𝐷!! "#
"
…
Inferencing
Index & 
Search
Training 
Positives
ANCE 
Negatives
Index & 
Search
Figure 2: ANCE Asynchronous Training. The Trainer learns the representation using negatives from
the ANN index. The Inferencer uses a recent checkpoint to update the representation of documents in
the corpus and once ﬁnished, refreshes the ANN index with most up-to-date encodings.
where L is the number of layers, ρis composed by pre-activation weights and gradients in inter-
mediate layers, and ||∇φLl(d+,d−)||2 is the gradient w.r.t. the last layer. Intuitively, the inter-
mediate layers are more regulated by various normalization techniques; the main moving piece is
||∇φLl(d+,d−)||2 (Katharopoulos & Fleuret, 2018).
For common learning to rank loss functions, for example, BCE loss and pairwise hinge loss, we can
veriﬁed that (Katharopoulos & Fleuret, 2018):
l(d+,d−) →0 ⇒||∇φLl(d+,d−)||2 →0 ⇒||∇θtl(d+,d−)||2 →0. (12)
Intuitively, negative samples with near zero loss have near zero gradients and contribute little to
model convergence. The convergence of dense retrieval model training relies on the informativeness
of constructed negatives.
Inefﬁcacy of Local In-Batch Negatives: We argue that the in-batch local negatives are unlikely to
provide informative samples due to two common properties of text retrieval.
Let D−∗be the set of informative negatives that are hard to distinguish from D+, and bbe the batch
size, we have (1) b≪|C|, the batch size is far smaller than the corpus size; (2) |D−∗|≪| C|, that
only a few negatives are informative and the majority of corpus is trivially unrelated.
Both conditions are easy to verify empirically in dense retrieval benchmarks. The two together make
the probability that a random mini-batch includes meaningful negatives p= b|D−∗|
|C|2 close to zero.
Selecting negatives from local training batches is unlikely to provide optimal training signals for
dense retrieval.
4 A PPROXIMATE NEAREST NEIGHBOR NOISE CONTRASTIVE ESTIMATION
Our analyses show the importance, if not necessity, to construct negativesglobally from the corpus.
In this section, we propose Approximate nearest neighbor Negative Contrastive Estimation, (ANCE),
which selects negatives from the entire corpus using an asynchronously updated ANN index.
ANCE samples negatives from the top retrieved documents via the DR model from the ANN index:
θ∗= argminθ
∑
q
∑
d+∈D+
∑
d−∈D−
ANCE
l(f(q,d+),f(q,d−)), (13)
with D−
ANCE = ANNf(q,d) \D+ and ANNf(q,d) the top retrieved documents by f() from the ANN
index. By deﬁnition, D−
ANCE are the hardest negatives for the current DR model: D−
ANCE ≈D−∗.
In theory, these more informative negatives have higher training loss, higher upper bound on the
gradient norms, and will improve training convergence.
ANCE can be used to train any dense retrieval model. For simplicity, we use a simple set up in recent
research (Luan et al., 2020) with BERT Siamese/Dual Encoder (shared between qand d), dot product
similarity, and negative log likelihood (NLL) loss.
4



### Claim 22/38

#### Claim Text
Reference deals with a number of practicalities when applying the approach to multiple spherical particles suspended in fluid. 19 Modelling approaches and computational methods for particle-laden turbulent flows Figure 5.3: Left panel: Half-way bounce-back representation of a circular no-slip boundary on a square lattice.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[18]_2101.06983.pdf (Page 2):

• Computing partial derivatives ∂L
∂f (si) and
∂L
∂g(tj ) requires only encoded representations,
but not Θ or Λ.
These observations mean back propagation of
f(si) for data si can be run independently with
its own computation graph and activation if the
numerical value of the partial derivative ∂L
∂si
is
known. Meanwhile the derivation of ∂L
∂si
requires
only numerical values of two sets of representa-
tion vectors F = {f(s1),f(s2),..,f (s|S|)}and
G = {g(t1),g(t2),...,g (t|T|)}. A similar argu-
ment holds true for g, where we can use represen-
tation vectors to compute ∂L
∂tj
and back propagate
for each g(tj) independently. In the next section,
we will describe how to scale up batch size by pre-
computing these representation vectors.
3.3 Gradient Cache Technique
Given a large batch that does not ﬁt into the avail-
able GPU memory for training, we ﬁrst divide it
into a set of sub-batches each of which can ﬁt
into memory for gradient computation, denoted as
S = {ˆS1, ˆS2,..},T = {ˆT1, ˆT2,..}. The full-batch
gradient update is computed by the following steps.
Step1: Graph-less Forward Before gradient
computation, we ﬁrst run an extra encoder forward
pass for each batch instance to get its representa-
tion. Importantly, this forward pass runs without
constructing the computation graph. We collect
and store all representations computed.
Step2: Representation Gradient Computation
and Caching We then compute the contrastive
loss for the batch based on the representation from
Step1 and have a corresponding computation graph
constructed. Despite the mathematical derivation,
automatic differentiation system is used in actual
implementation, which automatically supports vari-
ations of contrastive loss. A backward pass is
then run to populate gradients for each represen-
tation. Note that the encoder is not included in
this gradient computation. Let ui = ∂L
∂f (si) and
vi = ∂L
∂g(ti) , we take these gradient tensors and
store them as a Representation Gradient Cache,
[u1,u2,.., v1,v2,..].
Step3: Sub-batch Gradient AccumulationWe
run encoder forward one sub-batch at a time to
compute representations and build the correspond-
ing computation graph. We take the sub-batch’s
representation gradients from the cache and run
back propagation through the encoder. Gradients
are accumulated for encoder parameters across all
sub-batches. Effectively for f we have,
∂L
∂Θ =
∑
ˆSj ∈S
∑
si∈ˆSj
∂L
∂f(si)
∂f(si)
∂Θ
=
∑
ˆSj ∈S
∑
si∈ˆSj
ui
∂f(si)
∂Θ
(8)
where the outer summation enumerates each sub-
batch and the entire internal summation corre-
sponds to one step of accumulation. Similarly, for
g, gradients accumulate based on,
∂L
∂Λ =
∑
ˆTj ∈T
∑
ti∈ ˆTj
vi
∂g(ti)
∂Λ (9)
Here we can see the equivalence with direct large
batch update by combining the two summations.
Step4: Optimization When all sub-batches are
processed, we can step the optimizer to update
model parameters as if the full batch is processed
in a single forward-backward pass.
Compared to directly updating with the full
batch, which requires memory linear to the number
of examples, our method ﬁxes the number of exam-
ples in each encoder gradient computation to be the
size of sub-batch and therefore requires constant
memory for encoder forward-backward pass. The
extra data pieces introduced by our method that re-
main persistent across steps are the representations
and their corresponding gradients with the former
turned into the latter after representation gradient
computation. Consequently, in a general case with
data from Sand T each represented with ddimen-
sion vectors, we only need to store (|S|d+ |T|d)
ﬂoating points in the cache on top of the computa-
tion graph. To remind our readers, this is several
orders smaller than million-size model parameters.
3.4 Multi-GPU Training
When training on multiple GPUs, we need to com-
pute the gradients with all examples across all
GPUs. This requires a single additional cross GPU
communication after Step1 when all representa-
tions are computed. We use an all-gather opera-
tion to make all representations available on all
GPUs. Denote Fn,Gn representations on n-th
GPU and a total of N device. Step2 runs with
gathered representations Fall = F1 ∪..∪FN and
Gall = G1 ∪..∪GN . While Fall and Gall are used



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 1):

Preprint
Query
Relevant
DR Neg
BM25 Neg
Rand Neg
Figure 1: T-SNE (Maaten & Hinton,
2008) representations of query, relevant
documents, negative training instances
from BM25 (BM25 Neg) or randomly
sampled (Rand Neg), and testing nega-
tives (DR Neg) in dense retrieval.
In this paper, we ﬁrst theoretically analyze the convergence
of dense retrieval training with negative sampling. Us-
ing the variance reduction framework (Alain et al., 2015;
Katharopoulos & Fleuret, 2018), we show that, under con-
ditions commonly met in dense retrieval, local in-batch
negatives lead to diminishing gradient norms, resulted in
high stochastic gradient variances and slow training con-
vergence — the local negative sampling is the bottleneck
of dense retrieval’s effectiveness.
Based on our analysis, we propose Approximate near-
est neighbor Negative Contrastive Estimation (ANCE),
a new contrastive representation learning mechanism for
dense retrieval. Instead of random or in-batch local neg-
atives, ANCE constructs global negatives using the being-
optimized DR model to retrieve from the entire corpus.
This fundamentally aligns the distribution of negative sam-
ples in training and of irrelevant documents to separate in
testing. From the variance reduction point of view, these ANCE negatives lift the upper bound of per
instance gradient norm, reduce the variance of the stochastic gradient estimation, and lead to faster
learning convergence.
We implement ANCE using an asynchronously updated ANN index of the corpus representation.
Similar to Guu et al. (2020), we maintain an Inferencer that parallelly computes the document
encodings with a recent checkpoint from the being optimized DR model, and refresh the ANN index
used for negative sampling once it ﬁnishes, to keep up with the model training. Our experiments
demonstrate the advantage of ANCE in three text retrieval scenarios: standard web search (Craswell
et al., 2020), OpenQA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), and in a commercial search
engine’s retrieval system. We also empirically validate our theory that the gradient norms on ANCE
sampled negatives are much bigger than local negatives and thus improve the convergence of dense
retrieval models. Our code and trained models are available at https://aka.ms/ance.
2 P RELIMINARIES
In this section, we discuss the preliminaries of dense retrieval and its representation learning.
Task Deﬁnition: Given a query qand a corpus C, the ﬁrst stage retrieval is to ﬁnd a set of documents
relevant to the query D+ = {d1,...,d i,...,d n}from C(|D+|≪| C|), which then serve as input to
later more complex models (Croft et al., 2010). Instead of using sparse term matches and inverted
index, Dense Retrieval calculates the retrieval score f() using similarities in a learned embedding
space (Lee et al., 2019; Luan et al., 2020; Karpukhin et al., 2020):
f(q,d) = sim(g(q; θ),g(d; θ)), (1)
where g() is the representation model that encodes the query or document to dense embeddings. The
encoder parameter θprovides the main capacity, often ﬁne-tuned from pretrained transformers, e.g.,
BERT (Lee et al., 2019). The similarity function (sim()) is often simply cosine or dot product, to
leverage efﬁcient ANN retrieval (Johnson et al., 2019; Guo et al., 2020).
Learning with Negative Sampling: The effectiveness of DR resides in learning a good representa-
tion space that maps query and relevant documents together, while separating irrelevant ones. The
learning of this representation often follows standard learning to rank (Liu, 2009): Given a query q, a
set of relevant document D+ and irrelevant ones D−, ﬁnd the best θ∗that:
θ∗= argminθ
∑
q
∑
d+∈D+
∑
d−∈D−
l(f(q,d+),f(q,d−)). (2)
The loss l() can be binary cross entropy (BCE), hinge loss, or negative log likelihood (NLL).
A unique challenge in dense retrieval, targeting ﬁrst stage retrieval, is that the irrelevant documents
to separate are from the entire corpus (D− = C \D+). This often leads to millions of negative
2



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 22):

Corpus Website (Link)
CORD-19 https://www.semanticscholar.org/cord19
NutritionFacts https://nutritionfacts.org
PubMed https://pubmed.ncbi.nlm.nih.gov
Signal-1M https://research.signal-ai.com/datasets/signal1m.html
TREC Washington Posthttps://ir.nist.gov/wapo/
TREC disks 4 and 5 https://trec.nist.gov/data/cd45/
Args.me https://zenodo.org/record/4139439/
DBPedia (2015-10) http://downloads.dbpedia.org/wiki-archive/Downloads2015-10.html
TREC-COVID (Annotated)https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid-beir.zip
Table 7: Corpus Name and Link used for datasets in BEIR .
Dataset Query Relevant-Document
MS MARCOwhat fruit is native to australia <Paragraph>Passiﬂora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned,white ﬂeshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, whileothers list the fruits as being bitter and inedible. assiﬂora herbertiana. A rare passion fruit native toAustralia...
TREC-COVIDwhat is the origin of COVID-19<Title>Origin of Novel Coronavirus (COVID-19): A Computational Biology Study using ArtiﬁcialIntelligence<Paragraph>Origin of the COVID-19 virus has been intensely debated in the community...
BioASQ What is the effect of HMGB2 loss on CTCFclustering <Title>HMGB2 Loss upon Senescence Entry Disrupts Genomic Organization and Induces CTCFClustering across Cell Types.<Paragraph>Processes like cellular senescence are characterized bycomplex events giving rise to heterogeneous cell populations. However, the early molecular eventsdriving this cascade remain elusive....
NFCorpus Titanium Dioxide & Inﬂammatory Bowel Dis-ease <Title>Titanium Dioxide Nanoparticles in Food and Personal Care Products<Paragraph>Titaniumdioxide is a common additive in many food, personal care, and other consumer products used by people,which after use can enter the sewage system, and subsequently enter the environment as treated efﬂuentdischarged to surface waters or biosolids applied to agricultural land, or incinerated wastes...
NQ when did they stop cigarette advertising on tele-vision? <Title>Tobacco advertising<Paragraph>The ﬁrst calls to restrict advertising came in 1962 from theRoyal College of Physicians, who highlighted the health problems and recommended stricter laws...
HotpotQA Stockely Webster has paintings hanging in whathome (that serves as the residence for the Mayorof New York)?
<Title>Stokely Webster<Paragraph>Stokely Webster (1912 – 2001) was best known as an Americanimpressionist painter who studied in Paris. His paintings can be found in the permanent collections ofmany museums, including the Metropolitan Museum of Art in New York, the National Museum...
FiQA-2018 What is the PEG ratio? How is the PEG ratiocalculated? How is the PEG ratio useful forstock investing?
<Paragraph>PEG is Price/Earnings to Growth. It is calculated as Price/Earnings/Annual EPS Growth.It represents how good a stock is to buy, factoring in growth of earnings, which P/E does not. Obviouslywhen PEG is lower, a stock is more undervalued, which means that it is a better buy, and more likely...
Signal-1M (RT)Genvoya, a Gentler Anti-HIV Cocktail, Okayedby EU Regulators <Paragraph>All people with #HIV should get anti-retroviral drugs: @WHO, by @kkelland via@Reuters_Health #AIDS #TasP
TREC-NEWSWebsites where children are prostituted are im-mune from prosecution. But why?<Title>Senate launches bill to remove immunity for websites hosting illegal content, spurred byBackpage.com<Paragraph>The legislation, along with a similar bill in the House, sets the stage for abattle between Congress and some of the Internet’s most powerful players, including Google and variousfree-speech advocates, who believe that Congress shouldn’t regulate Web content or try to force websitesto police themselves more rigorously...
Robust04 What were the causes for the Islamic Revolutionrelative to relations with the U.S.?<Paragraph>BFN [Editorial: "Sow the Wind and Reap the Whirlwind"] Yesterday marked the 14thanniversary of severing of diplomatic relations between the Islamic Republic and the United States ofAmerica. Several occasions arose in the last decade and a half for improving Irano-American relations...
Touché-2020Should the government allow illegal immigrantsto become citizens? <Title>America should support blanket amnesty for illegal immigrants.<Paragraph>Undocumentedworkers do not receive full Social Security beneﬁts because they are not United States citizens " norshould they be until they seek citizenship legally. Illegal immigrants are legally obligated to pay taxes...
CQADupStackCommand to display ﬁrst few and last few linesof a ﬁle <Title>Combing head and tail in a single call via pipe<Paragraph>On a regular basis, I am piping theoutput of some program to either ‘head‘ or ‘tail‘. Now, suppose that I want to see the ﬁrst AND last 10lines of piped output, such that I could do something like ./lotsofoutput | headtail...
Quora How long does it take to methamphetamine outof your blood? <Paragraph>How long does it take the body to get rid of methamphetamine?
DBPedia Paul Auster novels <Title>The New York Trilogy<Paragraph>The New York Trilogy is a series of novels by Paul Auster.Originally published sequentially as City of Glass (1985), Ghosts (1986) and The Locked Room (1986),it has since been collected into a single volume.
SCIDOCS CFD Analysis of Convective Heat Transfer Co-efﬁcient on External Surfaces of Buildings<Title>Application of CFD in building performance simulation for the outdoor environment: an overview<Paragraph>This paper provides an overview of the application of CFD in building performancesimulation for the outdoor environment, focused on four topics...
FEVER DodgeBall: A True Underdog Story is an Amer-ican movie from 2004 <Title>DodgeBall: A True Underdog Story<Paragraph>DodgeBall: A True Underdog Story is a2004 American sports comedy ﬁlm written and directed by Rawson Marshall Thurber and starring VinceVaughn and Ben Stiller. The ﬁlm follows friends who enter a dodgeball tournament...
Climate-FEVERSea level rise is now increasing faster than pre-dicted due to unexpectedly rapid ice melting.<Title>Sea level rise<Paragraph>A sea level rise is an increase in the volume of water in the world ’soceans, resulting in an increase in global mean sea level. The rise is usually attributed to global climatechange by thermal expansion of the water in the oceans and by melting of Ice sheets and glaciers...
Table 8: Examples of queries and relevant documents for all datasets included in BEIR . ( <Title>) and
(<Paragraph>) are used to distinguish the title separately from the paragraph within a document in the table
above. These tokens were not passed to the respective models.
23



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 15):

Preprint
(a) 182539: monotonic function
 (b) 1117099: active margin
Query
Relevant
ANCE Neg
BM25 Neg
Rand Neg (c) 1132213: yoga bow
Figure 7: t-SNE Plots for Losing Cases in Table 9.
margin” is a geographical terminology, not a ﬁnancial one (which we did not know ourselves and took some time
to ﬁgure out when conducting this case study). There are also some cases where the dense retrieved documents
make sense to us but were labeled irrelevant.
The t-SNE plots in Fig. 6 and Fig. 7 show many interesting patterns of the learned representation space. The
ANCE winning cases often correspond to clear separations of different document groups. For losing cases the
representation space is more mixed, or there is too few relevant documents which may cause the variances in
model performances. There are also many different interesting patterns in the ANCE-learned representation
space. We include the t-SNE plots for all 43 TREC DL Track queries in the supplementary material. More future
analyses of the learned patterns in the representation space may help provide more insights on dense retrieval.
16



Source: data\tc18_2309.07597v5\referenced_papers\[53]_2212.03533.pdf (Page 2):

consistency-based filter
share
……
1.3B unfiltered
270M
Encoder Encoder
query: text 1 passage: text 2
average pool
Eq Ep
CCPairs
average pool
Figure 1: Overview of our data curation pipeline and model architecture.
Harvesting semi-structured data sources Large-scale high-quality datasets like C4 [ 48] and
CCMatrix [ 51] are vital for the success of language model pre-training and machine translation. For
learning text embeddings, existing works either utilize small-scale human-annotated data such as NLI
[22] and MS-MARCO [ 8] or adopt heuristics such as random cropping [ 28] to obtain large-scale
but very noisy supervision signals.
Instead, we curate a text pair dataset CCPairs (Colossal Clean text Pairs) by harvesting heterogeneous
semi-structured data sources. Let ( q, p) denote a text pair consisting of a query q and a passage p.
Here we use “passage” to denote word sequences of arbitrary length, which can be a short sentence, a
paragraph, or a long document. Our dataset includes (post, comment) pairs from Reddit 3, (question,
upvoted answer) pairs from Stackexchange 4, (entity name + section title, passage) pairs from English
Wikipedia, (title, abstract) and citation pairs from Scientific papers [ 36], and (title, passage) pairs
from Common Crawl 5 web pages and various News sources.
We only include data sources that can be automatically mined, and some subsets are directly reused
from existing datasets. Simple heuristic rules are applied to filter data from Reddit and Common
Crawl. For example, we remove Reddit comments that are either too long ( > 4096 characters) or
receive score less than 1, and remove passages from web pages with high perplexity [ 60]. After
preliminary filtering, we end up with ∼ 1.3 billion text pairs, most of which come from Reddit and
Common Crawl. For more details and examples, please refer to Appendix A.
Consistency-based filter To further improve data quality and make training costs manageable, we
propose a consistency-based data filtering technique: a model is first trained on the 1.3B noisy text
pairs, and then used to rank each pair against a pool of 1 million random passages. A text pair is kept
only if it falls in the top-k ranked lists. In other words, the model’s prediction should be consistent
with the training labels. Here we set k = 2based on manual inspection of data quality. After this
step, we end up with ∼ 270M text pairs for contrastive pre-training.
The intuition for this technique comes from the memorization behaviors of neural networks [ 19]:
when trained on noisy datasets, neural networks tend to memorize the clean labels first and then
gradually overfit the noisy labels. Similar techniques [42, 15, 23] have been widely used for removing
dataset noises. It is also possible to apply this filter iteratively, we will leave it for future work.
4 Method
Our embeddings can be trained with only unlabeled text pairs from CCPairs with contrastive pre-
training. A second-stage fine-tuning on small, high-quality labeled datasets can be performed to
further boost the quality of the resulted embeddings. See Figure 1 for an overview.
3https://files.pushshift.io/reddit/
4https://archive.org/details/stackexchange
5https://commoncrawl.org/
3



### Claim 23/38

#### Claim Text
These reactors operate under extremely harsh conditions, characterized by high temperatures and intense radiation fields, necessitating structural materials with greater radiation tolerance than those used in current fission reactors to resist radiation-induced degradation .

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 22):

Corpus Website (Link)
CORD-19 https://www.semanticscholar.org/cord19
NutritionFacts https://nutritionfacts.org
PubMed https://pubmed.ncbi.nlm.nih.gov
Signal-1M https://research.signal-ai.com/datasets/signal1m.html
TREC Washington Posthttps://ir.nist.gov/wapo/
TREC disks 4 and 5 https://trec.nist.gov/data/cd45/
Args.me https://zenodo.org/record/4139439/
DBPedia (2015-10) http://downloads.dbpedia.org/wiki-archive/Downloads2015-10.html
TREC-COVID (Annotated)https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid-beir.zip
Table 7: Corpus Name and Link used for datasets in BEIR .
Dataset Query Relevant-Document
MS MARCOwhat fruit is native to australia <Paragraph>Passiﬂora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned,white ﬂeshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, whileothers list the fruits as being bitter and inedible. assiﬂora herbertiana. A rare passion fruit native toAustralia...
TREC-COVIDwhat is the origin of COVID-19<Title>Origin of Novel Coronavirus (COVID-19): A Computational Biology Study using ArtiﬁcialIntelligence<Paragraph>Origin of the COVID-19 virus has been intensely debated in the community...
BioASQ What is the effect of HMGB2 loss on CTCFclustering <Title>HMGB2 Loss upon Senescence Entry Disrupts Genomic Organization and Induces CTCFClustering across Cell Types.<Paragraph>Processes like cellular senescence are characterized bycomplex events giving rise to heterogeneous cell populations. However, the early molecular eventsdriving this cascade remain elusive....
NFCorpus Titanium Dioxide & Inﬂammatory Bowel Dis-ease <Title>Titanium Dioxide Nanoparticles in Food and Personal Care Products<Paragraph>Titaniumdioxide is a common additive in many food, personal care, and other consumer products used by people,which after use can enter the sewage system, and subsequently enter the environment as treated efﬂuentdischarged to surface waters or biosolids applied to agricultural land, or incinerated wastes...
NQ when did they stop cigarette advertising on tele-vision? <Title>Tobacco advertising<Paragraph>The ﬁrst calls to restrict advertising came in 1962 from theRoyal College of Physicians, who highlighted the health problems and recommended stricter laws...
HotpotQA Stockely Webster has paintings hanging in whathome (that serves as the residence for the Mayorof New York)?
<Title>Stokely Webster<Paragraph>Stokely Webster (1912 – 2001) was best known as an Americanimpressionist painter who studied in Paris. His paintings can be found in the permanent collections ofmany museums, including the Metropolitan Museum of Art in New York, the National Museum...
FiQA-2018 What is the PEG ratio? How is the PEG ratiocalculated? How is the PEG ratio useful forstock investing?
<Paragraph>PEG is Price/Earnings to Growth. It is calculated as Price/Earnings/Annual EPS Growth.It represents how good a stock is to buy, factoring in growth of earnings, which P/E does not. Obviouslywhen PEG is lower, a stock is more undervalued, which means that it is a better buy, and more likely...
Signal-1M (RT)Genvoya, a Gentler Anti-HIV Cocktail, Okayedby EU Regulators <Paragraph>All people with #HIV should get anti-retroviral drugs: @WHO, by @kkelland via@Reuters_Health #AIDS #TasP
TREC-NEWSWebsites where children are prostituted are im-mune from prosecution. But why?<Title>Senate launches bill to remove immunity for websites hosting illegal content, spurred byBackpage.com<Paragraph>The legislation, along with a similar bill in the House, sets the stage for abattle between Congress and some of the Internet’s most powerful players, including Google and variousfree-speech advocates, who believe that Congress shouldn’t regulate Web content or try to force websitesto police themselves more rigorously...
Robust04 What were the causes for the Islamic Revolutionrelative to relations with the U.S.?<Paragraph>BFN [Editorial: "Sow the Wind and Reap the Whirlwind"] Yesterday marked the 14thanniversary of severing of diplomatic relations between the Islamic Republic and the United States ofAmerica. Several occasions arose in the last decade and a half for improving Irano-American relations...
Touché-2020Should the government allow illegal immigrantsto become citizens? <Title>America should support blanket amnesty for illegal immigrants.<Paragraph>Undocumentedworkers do not receive full Social Security beneﬁts because they are not United States citizens " norshould they be until they seek citizenship legally. Illegal immigrants are legally obligated to pay taxes...
CQADupStackCommand to display ﬁrst few and last few linesof a ﬁle <Title>Combing head and tail in a single call via pipe<Paragraph>On a regular basis, I am piping theoutput of some program to either ‘head‘ or ‘tail‘. Now, suppose that I want to see the ﬁrst AND last 10lines of piped output, such that I could do something like ./lotsofoutput | headtail...
Quora How long does it take to methamphetamine outof your blood? <Paragraph>How long does it take the body to get rid of methamphetamine?
DBPedia Paul Auster novels <Title>The New York Trilogy<Paragraph>The New York Trilogy is a series of novels by Paul Auster.Originally published sequentially as City of Glass (1985), Ghosts (1986) and The Locked Room (1986),it has since been collected into a single volume.
SCIDOCS CFD Analysis of Convective Heat Transfer Co-efﬁcient on External Surfaces of Buildings<Title>Application of CFD in building performance simulation for the outdoor environment: an overview<Paragraph>This paper provides an overview of the application of CFD in building performancesimulation for the outdoor environment, focused on four topics...
FEVER DodgeBall: A True Underdog Story is an Amer-ican movie from 2004 <Title>DodgeBall: A True Underdog Story<Paragraph>DodgeBall: A True Underdog Story is a2004 American sports comedy ﬁlm written and directed by Rawson Marshall Thurber and starring VinceVaughn and Ben Stiller. The ﬁlm follows friends who enter a dodgeball tournament...
Climate-FEVERSea level rise is now increasing faster than pre-dicted due to unexpectedly rapid ice melting.<Title>Sea level rise<Paragraph>A sea level rise is an increase in the volume of water in the world ’soceans, resulting in an increase in global mean sea level. The rise is usually attributed to global climatechange by thermal expansion of the water in the oceans and by melting of Ice sheets and glaciers...
Table 8: Examples of queries and relevant documents for all datasets included in BEIR . ( <Title>) and
(<Paragraph>) are used to distinguish the title separately from the paragraph within a document in the table
above. These tokens were not passed to the respective models.
23



Source: data\tc18_2309.07597v5\referenced_papers\[52]_2207.02578.pdf (Page 12):

Retriever 1-2 Re-ranker Retriever distill
learning rate 2 ×10−5 3 ×10−5 3 ×10−5
PLM S IMLM ELECTRA base SIMLM
# of GPUs 4 8 4
warmup steps 1000 1000 1000
batch size 64 64 64
epoch 3 3 6
τ 0.02 n.a. 0.02
α n.a. n.a. 0.2
negatives depth 200 200 200
rerank depth n.a. 200 n.a.
query length 32 n.a. 32
passage length 144 192 † 144
# of negatives 15 63 23
Table 13: Hyper-parameters for supervised ﬁne-tuning on MS-MARCO passage ranking dataset. †: Max length
for the concatenation of the query and passage.
query is the keto diet good for kidney disease
BERTbase
Rank: 1, Relevant: 
Passage: The keto diet (also known as ketogenic diet, low carb diet and LCHF diet) is a low carbohydrate,
high fat diet. Maintaining this diet is a great tool for weight loss. More importantly though,
according to an increasing number of studies, it helps reduce risk factors for diabetes, heart diseases, stroke . . .
SIMLM
Rank: 1, Relevant: 
Passage: 4-Many kidney issues have either a hyperinsulinemic characteristic, an autoimmune characteristic,
and or a combination of autoimmunity or hyperinsulinism. A standard, low-ish carb paleo diet can ﬁx most of
these issues.5-For serious kidney damage a low-protein, ketogenic diet can be remarkably therapeutic.
query who announced the european recovery program?
BERTbase
Rank: 1, Relevant: 
Passage: 1 The CEEC submits its report estimating needs and the cost of the European Recovery Program
(ERP) over four years. 2 It provides for the establishment of the Organization for European
Economic Cooperation (OEEC) to coordinate the program from the European side. 3 February 1948.
SIMLM
Rank: 2, Relevant: 
Passage: Marshall Plan. Introduction. The Marshall Plan, also known as the European Recovery Program,
channeled over $13 billion to ﬁnance the economic recovery . . . The plan is named for Secretary of State
George C. Marshall, who announced it in a commencement speech at Harvard University on June 5, 1947.
query what is process control equipment
BERTbase
Rank: 1, Relevant: 
Passage: What is process control? Process control is an algorithm that is used in the during the manufacturing
process in the industries for the active changing process based on the output of process monitoring.
SIMLM
Rank: 1, Relevant: 
Passage: Process equipment is equipment used in chemical and materials processing, in facilities
like reﬁneries, chemical plants, and wastewater treatment plants. This equipment is usually designed with a
speciﬁc process or family of processes in mind and can be customized for a particular facility in some cases.
Table 14: Additional examples from dev set of MS-MARCO passage ranking dataset.



Source: data\tc18_2309.07597v5\referenced_papers\[20]_1711.05073.pdf (Page 6):

Systems Baidu Search Baidu Zhidao All
BLEU-4% Rouge-L% BLEU-4% Rouge-L% BLEU-4% Rouge-L%
Selected Paragraph 15.8 22.6 16.5 38.3 16.4 30.2
Match-LSTM 23.1 31.2 42.5 48.0 31.9 39.2
BiDAF 23.1 31.1 42.2 47.5 31.8 39.0
Human 55.1 54.4 57.1 60.7 56.1 57.4
Table 6: Performance of typical MRC systems on the DuReader.
BLEU-4% Rouge-L%
Gold Paragraph 31.7 61.3
Match-LSTM 46.3 52.4
BiDAF 46.3 51.8
Table 7: Model performance with gold paragraph.
The use of gold paragraphs could signiﬁcantly
boosts the overall performance.
get a vector representation for each position.
Implementation Details We randomly initial-
ize the word embeddings with a dimension of 300
and set the hidden vector size as 150 for all lay-
ers. We use the Adam algorithm (Kingma and Ba,
2014) to train both MRC models with an initial
learning rate of 0.001 and a batch size of 32.
4.2 Results and Analysis
We evaluate the reading comprehension task via
character-level BLEU-4 (Papineni et al., 2002) and
Rouge-L (Lin, 2004), which are widely used for
evaluating the quality of language generation. The
experimental results on test set are shown in Ta-
ble 6. For comparison, we also evaluate the Se-
lected Paragraph that has the largest overlap with
the question among all documents. We also assess
human performance by involving a new annotator
to annotate on the test data and treat his ﬁrst an-
swer as the prediction.
The results demonstrate that current reading
comprehension models can achieve an impressive
improvement compared with the selected para-
graph baseline, which approves the effectiveness
of these models. However, there is still a large per-
formance gap between these models and human.
An interesting discovery comes from the compar-
ison between results on Baidu Search and Baidu
Zhidao data. We ﬁnd that the reading comprehen-
sion models get much higher score on Zhidao data.
This shows that it is much harder for the models to
comprehend open-domain web articles than to ﬁnd
answers in passages from a question answering
community. In contrast, the performance of hu-
man beings on these two datasets shows little dif-
ference, which suggests that human’s reading skill
is more stable on different types of documents.
As described in Section 4.1, the most relevant
paragraph of each document is selected based on
its overlap with the corresponding question during
testing stage. To analyze the effect of paragraph
selection and obtain an upper bound of the base-
line MRC models, we re-evaluate our systems on
the gold paragraphs, each of which is selected if
it has the largest overlap with the human gener-
ated answers in a document. The experiment re-
sults have been shown in Table 7. Comparing Ta-
ble 7 with Table 6, we can see that the use of gold
paragraphs could signiﬁcantly boosts the overall
performance. Moreover, directly using the gold
paragraph can obtain a very high Rouge-L score.
It meets the exception, because each gold para-
graph is selected based on recall that is relevant to
Rouge-L. Though, we ﬁnd that the baseline mod-
els can get much better performance with respect
to BLEU, which means the models have learned to
select the answers. These results show that para-
graph selection is a crucial problem to solve in real
applications, while most current MRC datasets
suppose to ﬁnd the answer in a small paragraph
or passage. In contrast, DuReader provides the
full body text of each document to stimulate the
research in a real-world setting.
To gain more insight into the characteristics of
our dataset, we report the performance across dif-
ferent question types in Table 8. We can see
that both the models and human achieve relatively
good performance on description questions, while
YesNo questions seem to be the hardest to model.
We consider that description questions are usually
answered with long text on the same topic. This
is preferred by BLEU or Rouge. However, the
answers to YesNo questions are relatively short,
which could be a simple Yes or No in some cases.



Source: data\tc18_2309.07597v5\referenced_papers\[28]_2308.03281.pdf (Page 4):

Task Type Text Pair Format Query Doc
Web Page (title, body) Providence Real Estate | Providence Homes for SaleFounded by Roger Williams in 1636, Providence isrecognized as one of the country’s oldest cities. . .
Academic Paper(title, abstract) Polymer Quantum Mechanics and its Continuum LimitA rather non-standard quantum representation of thecanonical commutation relations of quantum mechanics. . .
Hyperlink (citation, reference)After the championship in 1996, the PGA of Americaraised its stake to 50% and announced that . . .Pebble Beach Golf Links The largest margin of victoryever in a major championship, surpassing the 13-shot . . .
Social Media (post, comment)Pretty sure any team with Lebron James will be a playoffcontender. Considering UNC would be in the East. . .I was being sarcastic and making fun of the East, buthonestly I was really in deep thought about this . . .
Knowledge Base(entity, description) Animation Animation is the process of creating the illusion of motionand shape change by means of the rapid display of . . .
Community QA (question, answer)How the human species evolved? A tough question as it overlaps science and theology. Sinceyou asked “how the human species evolved?” I’ll assume . . .
News (summary, content) Nepalese Opposition Welcomes Return of ParliamentNepal’s opposition alliance formally calls off weeks ofpro-democracy protests after King Gyenandra reinstates . . .
Code (text, code) SetMaxRecords sets the MaxRecords field’s value.func (s *DescribeSnapshotCopyGrantsInput) SetMaxRecords(v int64) *DescribeSnapshotCopyGrantsInput { s.MaxRecords
Table 2: Examples of mined (query, document) pairs in the pre-training data.
Consider a batch of positive text pair samples
B = {(q1, d1), (q2, d2), ...,(qn, dn)},
we use an improved contrastive loss which takes
the form
Licl = −1
n
nX
i=1
log es(qi,di)/τ
Z (5)
with the partition function being
Z =
X
j
es(qi,dj )/τ +
X
j̸=i
es(qi,qj )/τ
+
X
j
es(qj ,di)/τ +
X
j̸=i
es(dj ,di)/τ (6)
in which the first two terms are used for query to
document contrast, where as the last two terms are
used for the inverse. In this work, we use the cosine
similarity as the distance metric
s(q, d) = q · d
||q||2 · ||d||2
. (7)
The temperature τ is fixed to 0.01 in this work.
Training and Evaluation The training of our
embedding model consists of two stages. In the
first stage of contrastive pre-training with only in-
batch negatives, using a large batch size is crucial
to better model performance by reducing the gap
between training and inference with more nega-
tives included and providing a better approximation
to the underlying learning objective. To facilitate
this, we limit the maximum sequence length to 128
during pre-training and distribute the use of nega-
tives across all GPUs. Popular techniques such as
automatic mixed precision training (Micikevicius
et al., 2018) with fp16, deepspeed ZeRO (Rajb-
handari et al., 2020) stage 1 and gradient check-
pointing (Chen et al., 2016) are also jointly used to
reduce memory cost and scale up batch size to over
ten thousands. We run the pre-training for 50, 000
steps, which roughly corresponds to one epoch on
the whole pre-training data. We only tuned the
learning rate to ensure the convergence of larger
models. we employ the AdamW optimizer with
linear learning rate decay and a warm-up period
during the initial 5% of training steps. We con-
ducted experiments on three distinct model scales:
small, base, and large. These models were initial-
ized using the small-sized MiniLM (Wang et al.,
2020) model and the base and large models of the
BERT (Devlin et al., 2019) model. Further details
can be found in Table 3.
In the second stage of contrastive fine-tuning
with supervised data and hard negatives, a large
batch size is unnecessary since hard negatives can
already provide a reliable gradient estimation of
the learning objective (Xiong et al., 2021; Li et al.,
2023). Therefore, a global batch size of 128 and a
train group size of 16 are utilized, with one positive
example and the remaining being either hard nega-
tives or random negatives. Instead we increase the
max sequence length to 512 to better handle texts
with longer lengths. The learning rate is decreased
by a factor of ten during fine-tuning. The model
is fine-tuned on the collected dataset for a single
epoch. In-batch texts are also incorporated as nega-
tive candidates using the enhanced contrastive loss
described in Equation 5.
After training, we directly take the last check-
point for evaluation. We run model training on up
to 8 NVIDIA A100 GPUs with 80GB memory and
model evaluation on up to 8 NVIDIA Tesla V100
GPUs with 32GB memory. Models are trained with
mixed precision using fp16 and evaluated with half
precision fp16 as well.



Source: data\tc18_2309.07597v5\referenced_papers\[20]_1711.05073.pdf (Page 5):

77.10%
18.50%
4.40%
30.80%
17.90%
51.30%
0%
20%
40%
60%
80%
100%
0~3 4~10 10+
Answer proportion
The bins of edit distance between answers and 
source documents
Marco Du Reader
Figure 2: Span selection is unlikely to work well
for DuReader because many of the answers are
relatively far (in edit distance) from source doc-
uments (compared to MSMARCO).
rizing and paraphrasing the source documents to
generate an answer, instead of just copying words
from the source documents. Figure 2 compares
DuReader and MS-MARCO in terms of MED,
and suggests that span selection is unlikely to work
well for DuReader where many of the answers are
relatively far from source documents compared
to MSMARCO. Note that the MED of SQuAD,
NewsQA and TriviaQA should be zero.
The document length. In DuReader, ques-
tions tend to be short (4.8 words on average) com-
pared to answers (69.6 words), and answers tend
to be short compared to documents (396 words
on average). The documents in DuReader are 5x
longer than documents in MS-MARCO (Nguyen
et al., 2016). The difference is due to a design
decision to provide unabridged documents (as op-
posed to paragraphs). We believe unabridged doc-
uments may be helpful because there may be use-
ful clues throughout the document well beyond a
single paragraph or a few passages.
4 Experiments
In this section, we implement and evaluate the
baseline systems with two state-of-the-art mod-
els. Furthermore, with the rich annotations in
our dataset, we conduct comprehensive evalua-
tions from different perspectives.
4.1 Baseline Systems
As we discussed in previous section, DuReader
provides each question the full documents that
contain multi-paragraphs or multi-passages, while
previous work provides only a single para-
graph (Rajpurkar et al., 2016) or a few pas-
sages (Nguyen et al., 2016) to extract or generate
answers. The average length of each document is
much longer than previous ones (Nguyen et al.,
2016). If we directly apply the state-of-the-art
MRC models that was designed for answer span
selction, there will be efﬁciency issues. To im-
prove both the efﬁciency of training and testing,
our designed systems have two steps: (1) select
one most related paragraph from each document,
and (2) apply the state-of-the-art MRC models on
the selected paragraphs.
4.1.1 Paragraph Selection
In this paper, we apply simple strategies to select
the most relevant paragraph from each document.
In training stage, we select one paragraph from a
document as the most relevant one, if the para-
graph has the largest overlap with human gener-
ated answer. We select one most relevant para-
graph for each document. Then, MRC models de-
signed for answer span selection will be trained on
these selected paragraphs.
In testing stage, since we have no human gener-
ated answer, we select the most relevant paragraph
that has the largest overlap with the corresponding
question. Then, the trained MRC models designed
for answer span selection will be applied on the
these selected paragraphs.
4.1.2 Answer Span Selection
We implement two typical state-of-the-art models
designed for answer span selection as baselines.
Match-LSTM Match-LSTM is a widely used
MRC model and has been well explored in recent
studies (Wang and Jiang, 2017). To ﬁnd an answer
in a paragraph, it goes through the paragraph se-
quentially and dynamically aggregates the match-
ing of an attention-weighted question representa-
tion to each token of the paragraph. Finally, an an-
swer pointer layer is used to ﬁnd an answer span
in the paragraph.
BiDAF BiDAF is a promising MRC model, and
its improved version has achieved the best single
model performance on SQuAD dataset (Seo et al.,
2016). It uses both context-to-question attention
and question-to-context attention in order to high-
light the important parts in both question and con-
text. After that, the so-called attention ﬂow layer
is used to fuse all useful information in order to



### Claim 24/38

#### Claim Text
Thanks to the electron cooling effect [20–30] and tiny volume of a nanoabsorber, CEBs are suitable for space applications, since they can be operated in 3He sorption fridges and have record cosmic rays immunity .

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 22):

Corpus Website (Link)
CORD-19 https://www.semanticscholar.org/cord19
NutritionFacts https://nutritionfacts.org
PubMed https://pubmed.ncbi.nlm.nih.gov
Signal-1M https://research.signal-ai.com/datasets/signal1m.html
TREC Washington Posthttps://ir.nist.gov/wapo/
TREC disks 4 and 5 https://trec.nist.gov/data/cd45/
Args.me https://zenodo.org/record/4139439/
DBPedia (2015-10) http://downloads.dbpedia.org/wiki-archive/Downloads2015-10.html
TREC-COVID (Annotated)https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid-beir.zip
Table 7: Corpus Name and Link used for datasets in BEIR .
Dataset Query Relevant-Document
MS MARCOwhat fruit is native to australia <Paragraph>Passiﬂora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned,white ﬂeshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, whileothers list the fruits as being bitter and inedible. assiﬂora herbertiana. A rare passion fruit native toAustralia...
TREC-COVIDwhat is the origin of COVID-19<Title>Origin of Novel Coronavirus (COVID-19): A Computational Biology Study using ArtiﬁcialIntelligence<Paragraph>Origin of the COVID-19 virus has been intensely debated in the community...
BioASQ What is the effect of HMGB2 loss on CTCFclustering <Title>HMGB2 Loss upon Senescence Entry Disrupts Genomic Organization and Induces CTCFClustering across Cell Types.<Paragraph>Processes like cellular senescence are characterized bycomplex events giving rise to heterogeneous cell populations. However, the early molecular eventsdriving this cascade remain elusive....
NFCorpus Titanium Dioxide & Inﬂammatory Bowel Dis-ease <Title>Titanium Dioxide Nanoparticles in Food and Personal Care Products<Paragraph>Titaniumdioxide is a common additive in many food, personal care, and other consumer products used by people,which after use can enter the sewage system, and subsequently enter the environment as treated efﬂuentdischarged to surface waters or biosolids applied to agricultural land, or incinerated wastes...
NQ when did they stop cigarette advertising on tele-vision? <Title>Tobacco advertising<Paragraph>The ﬁrst calls to restrict advertising came in 1962 from theRoyal College of Physicians, who highlighted the health problems and recommended stricter laws...
HotpotQA Stockely Webster has paintings hanging in whathome (that serves as the residence for the Mayorof New York)?
<Title>Stokely Webster<Paragraph>Stokely Webster (1912 – 2001) was best known as an Americanimpressionist painter who studied in Paris. His paintings can be found in the permanent collections ofmany museums, including the Metropolitan Museum of Art in New York, the National Museum...
FiQA-2018 What is the PEG ratio? How is the PEG ratiocalculated? How is the PEG ratio useful forstock investing?
<Paragraph>PEG is Price/Earnings to Growth. It is calculated as Price/Earnings/Annual EPS Growth.It represents how good a stock is to buy, factoring in growth of earnings, which P/E does not. Obviouslywhen PEG is lower, a stock is more undervalued, which means that it is a better buy, and more likely...
Signal-1M (RT)Genvoya, a Gentler Anti-HIV Cocktail, Okayedby EU Regulators <Paragraph>All people with #HIV should get anti-retroviral drugs: @WHO, by @kkelland via@Reuters_Health #AIDS #TasP
TREC-NEWSWebsites where children are prostituted are im-mune from prosecution. But why?<Title>Senate launches bill to remove immunity for websites hosting illegal content, spurred byBackpage.com<Paragraph>The legislation, along with a similar bill in the House, sets the stage for abattle between Congress and some of the Internet’s most powerful players, including Google and variousfree-speech advocates, who believe that Congress shouldn’t regulate Web content or try to force websitesto police themselves more rigorously...
Robust04 What were the causes for the Islamic Revolutionrelative to relations with the U.S.?<Paragraph>BFN [Editorial: "Sow the Wind and Reap the Whirlwind"] Yesterday marked the 14thanniversary of severing of diplomatic relations between the Islamic Republic and the United States ofAmerica. Several occasions arose in the last decade and a half for improving Irano-American relations...
Touché-2020Should the government allow illegal immigrantsto become citizens? <Title>America should support blanket amnesty for illegal immigrants.<Paragraph>Undocumentedworkers do not receive full Social Security beneﬁts because they are not United States citizens " norshould they be until they seek citizenship legally. Illegal immigrants are legally obligated to pay taxes...
CQADupStackCommand to display ﬁrst few and last few linesof a ﬁle <Title>Combing head and tail in a single call via pipe<Paragraph>On a regular basis, I am piping theoutput of some program to either ‘head‘ or ‘tail‘. Now, suppose that I want to see the ﬁrst AND last 10lines of piped output, such that I could do something like ./lotsofoutput | headtail...
Quora How long does it take to methamphetamine outof your blood? <Paragraph>How long does it take the body to get rid of methamphetamine?
DBPedia Paul Auster novels <Title>The New York Trilogy<Paragraph>The New York Trilogy is a series of novels by Paul Auster.Originally published sequentially as City of Glass (1985), Ghosts (1986) and The Locked Room (1986),it has since been collected into a single volume.
SCIDOCS CFD Analysis of Convective Heat Transfer Co-efﬁcient on External Surfaces of Buildings<Title>Application of CFD in building performance simulation for the outdoor environment: an overview<Paragraph>This paper provides an overview of the application of CFD in building performancesimulation for the outdoor environment, focused on four topics...
FEVER DodgeBall: A True Underdog Story is an Amer-ican movie from 2004 <Title>DodgeBall: A True Underdog Story<Paragraph>DodgeBall: A True Underdog Story is a2004 American sports comedy ﬁlm written and directed by Rawson Marshall Thurber and starring VinceVaughn and Ben Stiller. The ﬁlm follows friends who enter a dodgeball tournament...
Climate-FEVERSea level rise is now increasing faster than pre-dicted due to unexpectedly rapid ice melting.<Title>Sea level rise<Paragraph>A sea level rise is an increase in the volume of water in the world ’soceans, resulting in an increase in global mean sea level. The rise is usually attributed to global climatechange by thermal expansion of the water in the oceans and by melting of Ice sheets and glaciers...
Table 8: Examples of queries and relevant documents for all datasets included in BEIR . ( <Title>) and
(<Paragraph>) are used to distinguish the title separately from the paragraph within a document in the table
above. These tokens were not passed to the respective models.
23



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 4):

TREC-COVID
BioASQ
NFCorpus
NQ
HotpotQA
Signal-1M (RT)
FiQA-2018
ArguAna
Touché-2020
CQADupStack
Quora
DBPedia
SCIDOCS
FEVER
SciFact
TREC-NEWS
Robust04
TREC-COVID
BioASQ
NFCorpus
NQ
HotpotQA
Signal-1M (RT)
FiQA-2018
ArguAna
Touché-2020
CQADupStack
Quora
DBPedia
SCIDOCS
FEVER
SciFact
TREC-NEWS
Robust04
0.51
0.39 0.44
0.24 0.22 0.19
0.16 0.16 0.13 0.46
0.14 0.13 0.11 0.34 0.25
0.18 0.17 0.15 0.28 0.17 0.26
0.21 0.19 0.17 0.34 0.22 0.26 0.33
0.2 0.19 0.17 0.36 0.21 0.29 0.41 0.44
0.18 0.18 0.14 0.24 0.16 0.2 0.3 0.22 0.29
0.18 0.17 0.15 0.33 0.22 0.3 0.36 0.29 0.37 0.29
0.16 0.15 0.13 0.44 0.89 0.24 0.17 0.21 0.2 0.15 0.22
0.32 0.29 0.23 0.26 0.16 0.16 0.22 0.24 0.23 0.26 0.22 0.16
0.18 0.18 0.15 0.52 0.8 0.27 0.2 0.24 0.24 0.18 0.25 0.78 0.19
0.44 0.53 0.41 0.19 0.13 0.11 0.15 0.18 0.17 0.16 0.15 0.13 0.26 0.15
0.18 0.16 0.15 0.42 0.29 0.4 0.32 0.34 0.37 0.23 0.32 0.28 0.2 0.32 0.14
0.23 0.21 0.18 0.46 0.31 0.34 0.35 0.38 0.35 0.23 0.31 0.3 0.24 0.35 0.18 0.45
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Figure 2: Domain overlap across each pairwise dataset in the BEIR benchmark. Heatmap (left) shows the
pairwise weighted jaccard similarity scores between BEIR datasets. 2D representation (right) using a force-
directed placement algorithm with NetworkX [20]. We color and mark datasets differently for different domains.
(Normalised Cumulative Discount Gain) for any top-k hits. One can use the BEIR benchmark for
evaluating existing models on new retrieval datasets and for evaluating new models on the included
datasets.
Datasets are often scattered online and are provided in various ﬁle-formats, making the evaluation of
models on various datasets difﬁcult. BEIR introduces a standard format (corpus, queries and qrels)
and converts existing datasets in this easy universal data format, allowing to evaluate faster on an
increasing number of datasets.
3.3 Evaluation Metric
Depending upon the nature and requirements of real-world applications, retrieval tasks can be either
be precision or recall focused. To obtain comparable results across models and datasets in BEIR , we
argue that it is important to leverage a single evaluation metric that can be computed comparably
across all tasks. Decision support metrics such as Precision and Recall which are both rank unaware
are not suitable. Binary rank-aware metrics such as MRR (Mean Reciprocal Rate) and MAP (Mean
Average Precision) fail to evaluate tasks with graded relevance judgements. We ﬁnd thatNormalised
Cumulative Discount Gain (nDCG@k) provides a good balance suitable for both tasks involving
binary and graded relevance judgements. We refer the reader to Wang et al. [71] for understanding
the theoretical advantages of the metric. For our experiments, we utilize the Python interface of the
ofﬁcial TREC evaluation tool [63] and compute nDCG@10 for all datasets.
4 Experimental Setup
We use BEIR to compare diverse, recent, state-of-the-art retrieval architectures with a focus on
transformer-based neural approaches. We evaluate on publicly available pre-trained checkpoints,
which we provide in Table 6. Due to the length limitations of transformer-based networks, we use
only the ﬁrst 512 word pieces within all documents in our experiments across all neural architectures.
We group the models based on their architecture: (i) lexical, (ii) sparse, (iii) dense, (iv) late-interaction,
and (v) re-ranking. Besides the included models, the BEIR benchmark is model agnostic and in future
different model conﬁgurations can be easily incorporated within the benchmark.
(i) Lexical Retrieval: (a) BM25 [55] is a commonly-used bag-of-words retrieval function based on
token-matching between two high-dimensional sparse vectors with TF-IDF token weights. We use
Anserini [36] with the default Lucene parameters (k=0.9 and b=0.4). We index the title (if available)
and passage as separate ﬁelds for documents. In our leaderboard, we also tested Elasticsearch BM25
and Anserini + RM3 expansion, but found Anserini BM25 to perform the best.
5



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 7):

BM25
+CE
docT5-
query
ColBERT TAS-B GenQ ANCE SPARTA DPR DeepCT
-20
-10
BM25
10
20 16
12
9 8 6 4 2 1 0
-2
-6
-9 -10 -12 -14 -16 -17 -18
Figure 3: Comparison of zero-shot neural retrieval per-
formances with BM25. Re-ranking based models, i.e.,
BM25+CE and sparse model: docT5query outperform
BM25 on more than half the BEIR evaluation datasets.
0 100 200 300 400 500
TREC-COVID [58]
TAS-B
ANCE
0 100 200 300 400 500
Touché-2020 [5]
TAS-B
ANCE
Figure 4: Distribution plots [22] for top-10 retrieved
document lengths (in words) using TAS-B (blue, top)
or ANCE (orange, bottom). TAS-B has a preference
towards shorter documents in BEIR .
7. Does domain adaptation help improve generalization of dense-retrievers? We evaluated
GenQ, which further ﬁne-tunes the TAS-B model on synthetic query data. It outperforms the TAS-B
model on specialized domains like scientiﬁc publications, ﬁnance or StackExchange. On broader and
more generic domains, like Wikipedia, it performs weaker than the original TAS-B model.
5.1 Efﬁciency: Retrieval Latency and Index Sizes
Models need to potentially compare a single query against millions of documents at inference, hence,
a high computational speed for retrieving results in real-time is desired. Besides speed, index sizes
are vital and are often stored entirely in memory. We randomly sample 1 million documents from
DBPedia [21] and evaluate latency. For dense models, we use exact search, while for ColBERT we
follow the original setup [32] and use approximate nearest neighbor search. Performances on CPU
were measured with an 8 core Intel Xeon Platinum 8168 CPU @ 2.70GHz and on GPU using a single
Nvidia Tesla V100, CUDA 11.0.
DBPedia [21] (1 Million)Retrieval Latency Index
Rank Model Dim. GPU CPU Size
(1) BM25+CE – 450ms 6100ms 0.4GB
(2) ColBERT 128 350ms – 20GB
(3) docT5query – – 30ms 0.4GB
(4) BM25 – – 20ms 0.4GB
(5) TAS-B 768 14ms 125ms 3GB
(6) GenQ 768 14ms 125ms 3GB
(7) ANCE 768 20ms 275ms 3GB
(8) SPARTA 2000 – 20ms 12GB
(9) DeepCT – – 25ms 0.4GB
(10) DPR 768 19ms 230ms 3GB
Table 3: Estimated average retrieval latency and
index sizes for a single query in DBPedia [ 21].
Ranked from best to worst on zero-shot BEIR .
Lower the latency or memory is desired.
Tradeoff between performance and retrieval la-
tency The best out-of-distribution generalization
performances by re-ranking top-100 BM25 docu-
ments and with late-interaction models come at the
cost of high latency (> 350 ms), being slowest at
inference. In contrast, dense retrievers are 20-30x
faster (< 20ms) compared to the re-ranking models
and follow a low-latency pattern. On CPU, the sparse
models dominate in terms of speed (20-25ms).
Tradeoff between performance and index sizes
Lexical, re-ranking and dense methods have the small-
est index sizes (< 3GB) to store 1M documents from
DBPedia. SPARTA requires the second largest index
to store a 30k dim sparse vector while ColBERT re-
quires the largest index as it stores multiple 128 dim
dense vectors for a single document. Index sizes are especially relevant when document sizes scale
higher: ColBERT requires ~900GB to store the BioASQ (~15M documents) index, whereas BM25
only requires 18GB.
6 Impact of Annotation Selection Bias
Creating a perfectly unbiased evaluation dataset for retrieval is inherently complex and is subject to
multiple biases induced by the: (i) annotation guidelines, (ii) annotation setup, and by the (iii) human
annotators. Further, it is impossible to manually annotate the relevance for all (query, document)-pairs.
Instead, existing retrieval methods are used to get a pool of candidate documents which are then
marked for their relevance. All other unseen documents are assumed to be irrelevant. This is a source
for selection bias [39]: A new retrieval system might retrieve vastly different results than the system
used for the annotation. These hits are automatically assumed to be irrelevant.
Many BEIR datasets are found to be subject to a lexical bias, i.e. a lexical based retrieval system like
TF-IDF or BM25 has been used to retrieve the candidates for annotation. For example, in BioASQ,
candidates have been retrieved for annotation via term-matching with boosting tags [61]. Creation of
Signal-1M (RT) involved retrieving tweets for a query with 7 out of these 8 techniques relying upon
8



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 8):

Model (→) BM25 DeepCT SPARTA docT5query DPR ANCE TAS-B ColBERT BM25+CE
Hole@10 (in %) 6.4% 19.4% 12.4% 2.8% 30.6% 14.4% 31.8% 12.4% 1.6%
nDCG@10 performances before and after manual annotation on TREC-COVID [65]
Original (w/ holes) 0.656 0.406 0.538 0.713 0.332 0.654 0.481 0.677 0.757
Annotated (w/o holes)0.668 0.472 0.624 0.714 0.445 0.735 0.555 0.735 0.760
Table 4: Hole@10 analysis on TREC-COVID. Annotated scores show improvement in performances after
removing holes@10 (documents in top-10 hits unseen by annotators) across each model.
lexical term-matching signals [59]. Such a lexical bias disfavours approaches that don’t rely on lexical
matching, like dense retrieval methods, as retrieved hits without lexical overlap are automatically
assumed to be irrelevant, even though the hits might be relevant for a query.
In order to study the impact of this particular type of bias, we conducted a study on the recent
TREC-COVID dataset. TREC-COVID used a pooling method [38, 40] to reduce the impact of the
aforementioned bias: The annotation set was constructed by using the search results from the various
systems participating in the challenge. Table 4 shows the Hole@10 rate [73] for the tested systems,
i.e., how many top-10 hits is each system retrieving that have not been seen by annotators.
The results reveal large differences between approaches: Lexical approaches like BM25 and
docT5query have a rather low Hole@10 value of 6.4% and 2.8%, indicating that the annotation pool
contained the top-hits from lexical retrieval systems. In contrast, dense retrieval systems like ANCE
and TAS-B have a much higher Hole@10 of 14.4% and 31.8%, indicating that a large fraction of hits
found by these systems have not been judged by annotators. Next, we manually added for all systems,
the missing annotation (or holes) following the original annotation guidelines. During annotation, we
were unaware of the system who retrieved the missing annotation to avoid a preference bias. In total,
we annotated 980 query-document pairs in TREC-COVID. We then re-computed nDCG@10 for all
systems with this additional annotations.
As shown in Table 4, we observe that lexical approaches improves only slightly, e.g. for docT5query
just from 0.713 to 0.714 after adding the missing relevance judgements. In contrast, for the dense
retrieval system ANCE, the performance improves from 0.654 (slightly below BM25) to 0.735, which
is 6.7 points above the BM25 performance. Similar improvements are noticed in ColBERT (5.8
points). Even though many systems contributed to the TREC-COVID annotation pool, the annotation
pool is still biased towards lexical approaches.
7 Conclusions and Future Work
In this work, we presented BEIR : a heterogeneous benchmark for information retrieval. We provided
a broader selection of target tasks ranging from narrow expert domains to open domain datasets. We
included nine different retrieval tasks spanning 18 diverse datasets.
By open-sourcing BEIR , with a standardized data format and easy-to-adapt code examples for many
different retrieval strategies, we take an important steps towards a uniﬁed benchmark to evaluate the
zero-shot capabilities of retrieval systems. It hopefully steers innovation towards more robust retrieval
systems and to new insights which retrieval architectures perform well across tasks and domains.
We studied the effectiveness of ten different retrieval models and demonstrate, that in-domain
performance cannot predict how well an approach will generalize in a zero-shot setup. Many
approaches that outperform BM25 on an in-domain evaluation, perform poorly on the BEIR datasets.
Cross-attentional re-ranking, late-interaction ColBERT, and the document expansion technique
docT5query performed overall well across the evaluated tasks.
Our study on annotation selection bias highlights the challenge of evaluating new models on existing
datasets: Even though TREC-COVID is based on the predictions from many systems, contributed by
a diverse set of teams, we found largely different Hole@10 rates for the tested systems, negatively
affecting non-lexical approaches. Better datasets, that use diverse pooling strategies, are needed for a
fair evaluation of retrieval approaches. By integrate a large number of diverse retrieval systems into
BEIR, creating such diverse pools becomes signiﬁcantly simpliﬁed.
9



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 3):

Split(→) Train Dev Test Avg. Word Lengths
Task (↓) Domain (↓) Dataset (↓) TitleRelevancy#Pairs#Query#Query #Corpus Avg. D / QQuery Document
Passage-RetrievalMisc. MS MARCO [45] Binary 532,761 —- 6,980 8,841,823 1.1 5.96 55.98
Bio-Medical Bio-MedicalTREC-COVID [65] 3-level —- —- 50 171,332 493.5 10.60 160.77Information Bio-MedicalNFCorpus [7]  3-level 110,575 324 323 3,633 38.2 3.30 232.26Retrieval (IR) Bio-MedicalBioASQ [61]  Binary 32,916 —- 500 14,914,602 4.7 8.05 202.61
Question WikipediaNQ [34]  Binary 132,803 —- 3,452 2,681,468 1.2 9.16 78.88Answering WikipediaHotpotQA [76]  Binary 170,0005,447 7,405 5,233,329 2.0 17.61 46.30(QA) Finance FiQA-2018 [44]  Binary 14,166 500 648 57,638 2.6 10.77 132.32
Tweet-RetrievalTwitter Signal-1M (RT) [59] 3-level —- —- 97 2,866,316 19.6 9.30 13.93
News News TREC-NEWS [58] 5-level —- —- 57 594,977 19.6 11.14 634.79Retrieval News Robust04 [64]  3-level —- —- 249 528,155 69.9 15.27 466.40
Argument Misc. ArguAna [67]  Binary —- —- 1,406 8,674 1.0 192.98 166.80Retrieval Misc. Touché-2020 [6] 3-level —- —- 49 382,545 19.0 6.55 292.37
Duplicate-QuestionStackEx. CQADupStack [25] Binary —- —- 13,145 457,199 1.4 8.59 129.09Retrieval Quora Quora  Binary —- 5,000 10,000 522,931 1.6 9.53 11.44
Entity-RetrievalWikipediaDBPedia [21]  3-level —- 67 400 4,635,922 38.2 5.39 49.68
Citation-PredictionScientiﬁc SCIDOCS [9]  Binary —- —- 1,000 25,657 4.9 9.38 176.19
WikipediaFEVER [60]  Binary 140,0856,666 6,666 5,416,568 1.2 8.13 84.76Fact CheckingWikipediaClimate-FEVER [14] Binary —- —- 1,535 5,416,593 3.0 20.13 84.76Scientiﬁc SciFact [68]  Binary 920 —- 300 5,183 1.1 12.37 213.63
Table 1: Statistics of datasets in BEIR benchmark. Few datasets contain documents without titles. Relevancy
indicates the query-document relation: binary (relevant, non-relevant) or graded into sub-levels. Avg. D/Q
indicates the average relevant documents per query.
datasets in depth. Examples for each dataset are listed in Table 8. We additionally provide dataset
licenses in Appendix E, and links to the datasets in Table 5.
Table 1 summarizes the statistics of the datasets provided in BEIR . A majority of datasets contain
binary relevancy judgements, i.e. relevant or non-relevant, and a few contain ﬁne-grained relevancy
judgements. Some datasets contain few relevant documents for a query (< 2), while other datasets
like TREC-COVID [65] can contain up to even 500 relevant documents for a query. Only 8 out of 19
datasets (including MS MARCO) have training data denoting the practical importance for zero-shot
retrieval benchmarking. All datasets except ArguAna [67] have short queries (either a single sentence
or 2-3 keywords). Figure 1 shows an overview of the tasks and datasets in the BEIR benchmark.
Information Retrieval (IR) is ubiquitous, there are lots of datasets available within each task and
further even more tasks with retrieval. However, it is not feasible to include all datasets within the
benchmark for evaluation. We tried to cover a balanced mixture of a wide range of tasks and datasets
and paid importance not to overweight a speciﬁc task like question-answering. Future datasets can
easily be integrated in BEIR , and existing models can be evaluated on any new dataset quickly. The
BEIR website will host an actively maintained leaderboard2 with all datasets and models.
3.1 Dataset and Diversity Analysis
The datasets present in BEIR are selected from diverse domains ranging from Wikipedia, scientiﬁc
publications, Twitter, news, to online user communities, and many more. To measure the diversity in
domains, we compute the domain overlap between the pairwise datasets using a pairwise weighted
Jaccard similarity [26] score on unigram word overlap between all dataset pairs. For more details
on the theoretical formulation of the similarity score, please refer to Appendix F. Figure 2 shows a
heatmap denoting the pairwise weighted jaccard scores and the clustered force-directed placement
diagram. Nodes (or datasets) close in this graph have a high word overlap, while nodes far away in
the graph have a low overlap. From Figure 2, we observe a rather low weighted Jaccard word overlap
across different domains, indicating that BEIR is a challenging benchmark where approaches must
generalize well to diverse out-of-distribution domains.
3.2 BEIR Software and Framework
The BEIR software3 provides an is an easy to use Python framework (pip install beir) for model
evaluation. It contains extensive wrappers to replicate experiments and evaluate models from well-
known repositories including Sentence-Transformers [53], Transformers [72], Anserini [74], DPR
[31], Elasticsearch, ColBERT [32], and Universal Sentence Encoder [75]. This makes the software
useful for both academia and industry. The software also provides you with all IR-based metrics
from Precision, Recall, MAP (Mean Average Precision), MRR (Mean Reciprocal Rate) to nDCG
2 BEIR Leaderboard: https://tinyurl.com/beir-leaderboard
3 BEIR Code & documentation: https://github.com/UKPLab/beir
4



### Claim 25/38

#### Claim Text
In particular, while linear curvature feedback was initially considered ideal for fitting Chlamydomonas data , the non-linear sliding feedback model proposed by , which incorporates the attachment and detachment of antagonistic molecular motors, provides a better fit.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[18]_2101.06983.pdf (Page 1):

Noise Contrastive Estimation (NCE) was later used
by Word2Vec (Mikolov et al., 2013) to learn word
embedding. Recent works use contrastive learning
to unsupervisedly pre-train (Lee et al., 2019; Chang
et al., 2020) as well as supervisedly train dense re-
triever (Karpukhin et al., 2020), where contrastive
loss is used to estimate retrieval probability over
the entire corpus. Inspired by SimCLR (Chen et al.,
2020), constrastive learning is used to learn better
sentence representation (Giorgi et al., 2020) and
pre-trained language model (Wu et al., 2020).
Deep Network Memory Reduction Many ex-
isting techniques deal with large and deep mod-
els. The gradient checkpoint method attempts to
emulate training deep networks by training shal-
lower layers and connecting them with gradient
checkpoints and re-computation (Chen et al., 2016).
Some methods also use reversible activation func-
tions, allowing internal activation in the network to
be recovered throughout back propagation (Gomez
et al., 2017; MacKay et al., 2018). However, their
effectiveness as part of contrastive encoders has
not been conﬁrmed. Recent work also attempts
to remove the redundancy in optimizer tracked pa-
rameters on each GPU (Rajbhandari et al., 2020).
Compared with the aforementioned methods, our
method is designed for scaling over the batch size
dimension for contrastive learning.
3 Methodologies
In this section, we formally introduce the notations
for contrastive loss and analyze the difﬁculties of
using it on limited hardware. We then show how
we can use a Gradient Cache technique to factor
the loss so that large batch gradient update can be
broken into several sub-updates.
3.1 Preliminaries
Under a general formulation, given two classes of
data S,T, we want to learn encoders f and gfor
each such that, given s∈S,t ∈T , encoded repre-
sentations f(s) and g(t) are close if related and far
apart if not related by some distance measurement.
For large Sand T and deep neural network based
f and g, direct training is not tractable, so a com-
mon approach is to use a contrastive loss: sample
anchors S ⊂S and targets T ⊂T as a training
batch, where each element si ∈S has a related
element tri ∈T as well as zero or more specially
sampled hard negatives. The rest of the random
samples in T will be used as in-batch negatives.
Deﬁne loss based on dot product as follows:
L= −1
|S|
∑
si∈S
log exp(f(si)⊤g(tri )/τ)∑
tj ∈T exp(f(si)⊺g(tj)/τ)
(1)
where each summation term depends on the entire
set T and requires ﬁtting all of them into memory.
We set temperature τ = 1 in the following dis-
cussion for simplicity as in general it only adds a
constant multiplier to the gradient.
3.2 Analysis of Computation
In this section, we give a mathematical analysis
of contrastive loss computation and its gradient.
We show that the back propagation process can be
divided into two parts, from loss to representation,
and from representation to encoder model. The
separation then enables us to devise a technique
that removes data dependency in encoder parameter
update. Suppose the function f is parameterized
with Θ and gis parameterized with Λ.
∂L
∂Θ =
∑
si∈S
∂L
∂f(si)
∂f(si)
∂Θ (2)
∂L
∂Λ =
∑
tj ∈T
∂L
∂g(tj)
∂g(tj)
∂Λ (3)
As an extra notation, denote normalized similarity,
pij = exp(f(si)⊺g(tj))∑
t∈T exp(f(si)⊺g(t)) (4)
We note that the summation term for a particularsi
or ti is a function of the batch, as,
∂L
∂f(si) = −1
|S|

g(tri ) −
∑
tj ∈T
pijg(tj)

, (5)
∂L
∂g(tj) = −1
|S|

ϵj −
∑
si∈S
pijf(si)

, (6)
where
ϵj =
{
f(sk) if ∃ks.t. rk = j
0 otherwise (7)
which prohibits the use of gradient accumulation.
We make two observations here:
• The partial derivative ∂f (si)
∂Θ depends only on
si and Θ while ∂g(tj )
∂Λ depends only on tj and
Λ; and



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 15):

Preprint
(a) 182539: monotonic function
 (b) 1117099: active margin
Query
Relevant
ANCE Neg
BM25 Neg
Rand Neg (c) 1132213: yoga bow
Figure 7: t-SNE Plots for Losing Cases in Table 9.
margin” is a geographical terminology, not a ﬁnancial one (which we did not know ourselves and took some time
to ﬁgure out when conducting this case study). There are also some cases where the dense retrieved documents
make sense to us but were labeled irrelevant.
The t-SNE plots in Fig. 6 and Fig. 7 show many interesting patterns of the learned representation space. The
ANCE winning cases often correspond to clear separations of different document groups. For losing cases the
representation space is more mixed, or there is too few relevant documents which may cause the variances in
model performances. There are also many different interesting patterns in the ANCE-learned representation
space. We include the t-SNE plots for all 43 TREC DL Track queries in the supplementary material. More future
analyses of the learned patterns in the representation space may help provide more insights on dense retrieval.
16



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 20):

is quite low and unintuitive. To avoid this we cap the recall score (R_cap@k) at k for datasets if the
number of relevant documents for a query greater than k. It is deﬁned as:
R_cap@k= 1
|Q|
|Q|∑
i=1
|maxk(Ai) ∩A⋆
i |
min(k,|A⋆
i |)
where the only difference lies within the denominator where we compute the minimum of k and |A⋆
i |,
instead of |A⋆
i |present in the original recall.
H Document Length Preference for Dense Retrieval System
As we show in Figure 4, TAS-B prefers retrieval of shorter documents, and in comparison, ANCE
retrieves longer documents. The difference is especially extreme for the TREC-COVID dataset:
TAS-B retrieves lots of top hit documents containing only a title and an empty abstract, while ANCE
retrieves top hit documents with a non-empty abstract.
Identifying the source for this contrasting behaviour is difﬁcult, as TAS-B and ANCE use different
models (DistilBERT vs. RoBERTa-base), a different loss function (InfoNCE [62] vs. Margin-MSE
[24] with in-batch negatives), and different hard negative mining strategies. Hence, we decided to
harmonize the training setup and to alter the training by just one aspect: The similarity function.
Dense models require a similarity function to retrieve relevant documents for a given query within
an embedding space. This similarity function is also used during training dense models with the
InfoNCE [62] loss:
Lq = −log exp(τ ·sim(q,d+))∑n
i=0 exp(τ ·sim(q,di))
using nin-batch negatives for each query qand a scaling factor τ. where d+ denotes the relevant
(positive) document for queryq. Commonly used similarity functions (sim(q,d)) are cosine-similarity
or dot-product.
We trained two distilbert-base-uncased models with an identical training setup on MS MARCO
(identical training parameters) and only changed the similarity function from cosine-similarity to
dot-product. As shown in Table 10, we observe signiﬁcant performance differences for some BEIR
datasets. For TREC-COVID, the dot-product model achieves the biggest improvement with 15.3
points, while for a majority on other datasets, it performs worse than the cosine-similarity model.
We observe that these (nearly) identical models retrieve documents with vastly different lengths
as shown in the violin plots in Table 10. For all datasets, we ﬁnd the cosine-similarity model to
prefer shorter documents over longer ones. This is especially severe for TREC-COVID: a large
fraction of the scientiﬁc papers (approx. 42k out of 171k) consist only of publication titles without an
abstract. The cosine-similarity model prefers retrieving these documents. In contrast, the dot-product
model primarily retrieves longer documents, i.e., publications with an abstract. Cosine-similarity
uses vectors of unit length, thereby having no notion of the encoded text length. In contrast, for
dot-product, longer documents result in vectors with higher magnitudes which can yield higher
similarity scores for a query.
Further, as we observe in Figure 5, relevance judgement scores are not uniformly distributed over
document lengths: for some datasets, longer documents are annotated with higher relevancy scores,
while in others, shorter documents are. This can be either due to the annotation process, e.g., the
candidate selection method prefers short or long documents, or due to the task itself, where shorter
or longer documents could be more relevant to the user information need. Hence, it can be more
advantageous to train a model with either cosine-similarity or dot-product depending upon the nature
and needs of the speciﬁc task.
21



Source: data\tc18_2309.07597v5\referenced_papers\[52]_2207.02578.pdf (Page 3):

distillationBM25 negatives mined negatives
Retriever 1 Retriever 2
Data
Re-ranker
mined negatives
Retrieverdistill
Pre-trained model
initializeinitialize initialize
Figure 2: Illustration of our supervised ﬁne-tuning pipeline. Note that we only use SimLM to initialize the
biencoder-based retrievers. For cross-encoder based re-ranker, we use off-the-shelf pre-trained models such as
ELECTRAbase.
supervised ﬁne-tuning pipeline. In contrast to
previous approaches, our proposed pipeline is
relatively straightforward and does not require
joint training (Ren et al., 2021b) or re-building
index periodically (Xiong et al., 2021). Each
stage takes the outputs from the previous stage
as inputs and can be trained in a standalone fashion.
Retriever1 Given a labeled query-passage pair
(q+,d+), we take the last-layer [CLS] vector of
the pre-trained encoder as their representations
(hq+,hd+). Both the in-batch negatives and BM25
hard negatives are used to compute the contrastive
loss Lcont:
−log φ(q+,d+)
φ(q+,d+) +
∑
ni∈N
(φ(q+,ni) +φ(d+,ni))
(3)
Where N denotes all the negatives, and φ(q,d) is a
function to compute the matching score between
query q and passage d. In this paper, we use
temperature-scaled cosine similarity function:
φ(q,d) =exp(1
τ cos(hq,hd)). τ is a temperature
hyper-parameter and set to a constant 0.02 in our
experiments.
Retriever2 It is trained in the same way as
Retriever1 except that the hard negatives are mined
based on a well-trained Retriever1 checkpoint.
Re-ranker is a cross-encoder that re-ranks the top-
kresults of Retriever2. It takes the concatenation
of query qand passage das input and outputs a real-
valued score θ(q,d). Given a labeled positive pair
(q+,d+) and n−1 hard negative passages randomly
sampled from top-kpredictions of Retriever2, we
adopt a listwise loss to train the re-ranker:
−log exp(θ(q+,d+))
exp(θ(q+,d+)) +∑n−1
i=1 exp(θ(q+,d−
i ))
(4)
The cross-encoder architecture can model the
full interaction between the query and the passage,
making it suitable to be a teacher model for
knowledge distillation.
Retrieverdistill Although cross-encoder based re-
ranker is powerful, it is not scalable enough for
ﬁrst-stage retrieval. To combine the scalability of
biencoder and the effectiveness of cross-encoder,
we can train a biencoder-based retriever by dis-
tilling the knowledge from the re-ranker. The re-
ranker from the previous stage is employed to com-
pute scores for both positive pairs and mined nega-
tives from Retriever2. These scores are then used
as training data for knowledge distillation. With
n−1 mined hard negatives, we use KL (Kullback-
Leibler) divergence Lkl as the loss function for
distilling the soft labels:
Lkl =
n∑
i=1
pi
ranker log pi
ranker
piret
(5)
where pranker and pret are normalized probabili-
ties from the re-ranker teacher and Retriever distill
student. For training with the hard labels, we
use the contrastive loss Lcont as deﬁned in Equa-
tion 3. The ﬁnal loss is their linear interpolation:
L= Lkl + αLcont.
Our pre-trained SimLM model is used to ini-
tialize all three biencoder-based retrievers but not
the cross-encoder re-ranker. Since our pre-training



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 1):

Preprint
Query
Relevant
DR Neg
BM25 Neg
Rand Neg
Figure 1: T-SNE (Maaten & Hinton,
2008) representations of query, relevant
documents, negative training instances
from BM25 (BM25 Neg) or randomly
sampled (Rand Neg), and testing nega-
tives (DR Neg) in dense retrieval.
In this paper, we ﬁrst theoretically analyze the convergence
of dense retrieval training with negative sampling. Us-
ing the variance reduction framework (Alain et al., 2015;
Katharopoulos & Fleuret, 2018), we show that, under con-
ditions commonly met in dense retrieval, local in-batch
negatives lead to diminishing gradient norms, resulted in
high stochastic gradient variances and slow training con-
vergence — the local negative sampling is the bottleneck
of dense retrieval’s effectiveness.
Based on our analysis, we propose Approximate near-
est neighbor Negative Contrastive Estimation (ANCE),
a new contrastive representation learning mechanism for
dense retrieval. Instead of random or in-batch local neg-
atives, ANCE constructs global negatives using the being-
optimized DR model to retrieve from the entire corpus.
This fundamentally aligns the distribution of negative sam-
ples in training and of irrelevant documents to separate in
testing. From the variance reduction point of view, these ANCE negatives lift the upper bound of per
instance gradient norm, reduce the variance of the stochastic gradient estimation, and lead to faster
learning convergence.
We implement ANCE using an asynchronously updated ANN index of the corpus representation.
Similar to Guu et al. (2020), we maintain an Inferencer that parallelly computes the document
encodings with a recent checkpoint from the being optimized DR model, and refresh the ANN index
used for negative sampling once it ﬁnishes, to keep up with the model training. Our experiments
demonstrate the advantage of ANCE in three text retrieval scenarios: standard web search (Craswell
et al., 2020), OpenQA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), and in a commercial search
engine’s retrieval system. We also empirically validate our theory that the gradient norms on ANCE
sampled negatives are much bigger than local negatives and thus improve the convergence of dense
retrieval models. Our code and trained models are available at https://aka.ms/ance.
2 P RELIMINARIES
In this section, we discuss the preliminaries of dense retrieval and its representation learning.
Task Deﬁnition: Given a query qand a corpus C, the ﬁrst stage retrieval is to ﬁnd a set of documents
relevant to the query D+ = {d1,...,d i,...,d n}from C(|D+|≪| C|), which then serve as input to
later more complex models (Croft et al., 2010). Instead of using sparse term matches and inverted
index, Dense Retrieval calculates the retrieval score f() using similarities in a learned embedding
space (Lee et al., 2019; Luan et al., 2020; Karpukhin et al., 2020):
f(q,d) = sim(g(q; θ),g(d; θ)), (1)
where g() is the representation model that encodes the query or document to dense embeddings. The
encoder parameter θprovides the main capacity, often ﬁne-tuned from pretrained transformers, e.g.,
BERT (Lee et al., 2019). The similarity function (sim()) is often simply cosine or dot product, to
leverage efﬁcient ANN retrieval (Johnson et al., 2019; Guo et al., 2020).
Learning with Negative Sampling: The effectiveness of DR resides in learning a good representa-
tion space that maps query and relevant documents together, while separating irrelevant ones. The
learning of this representation often follows standard learning to rank (Liu, 2009): Given a query q, a
set of relevant document D+ and irrelevant ones D−, ﬁnd the best θ∗that:
θ∗= argminθ
∑
q
∑
d+∈D+
∑
d−∈D−
l(f(q,d+),f(q,d−)). (2)
The loss l() can be binary cross entropy (BCE), hinge loss, or negative log likelihood (NLL).
A unique challenge in dense retrieval, targeting ﬁrst stage retrieval, is that the irrelevant documents
to separate are from the entire corpus (D− = C \D+). This often leads to millions of negative
2



### Claim 26/38

#### Claim Text
Focusing on the spreading dynamics on the simplicial susceptibleinfected-recovered (s-SIR) model, Palafox-Castillo et al. defined a stochastic model to study variations beyond contagion processes on simplicial networks.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 19):

FEVER [60] The Fact Extraction and VERiﬁcation dataset is collected to facilitate the automatic
fact checking. We utilize the original paper splits as queries Q and retrieve evidences from the
pre-processed Wikipedia Abstracts (June 2017 dump) as our corpus T.
Climate-FEVER [14] is a dataset for veriﬁcation of real-world climate claims. We include the
original dataset claims as queries Q and retrieve evidences from the same FEVER Wiki corpusT.
We manually included few Wikipedia articles (25) missing from our corpus, but present within our
relevance judgements.
SciFact [68] veriﬁes scientiﬁc claims using evidence from the research literature containing scientiﬁc
paper abstracts. We use the original publicly available dev split from the task containing 300 queries
as our test queries Q, and include all documents from the original dataset as our corpus T.
E Dataset Licenses
The authors of 4 out of the 19 datasets in the BEIR benchmark (NFCorpus, FiQA-2018, Quora,
Climate-Fever) do not report the dataset license in the paper or a repository; We overview the rest:
• MSMARCO: Provided under “MIT License” for non-commercial research purposes.
• FEVER, NQ, DBPedia, Signal-1M: All provided under CC BY-SA 3.0 license.
• TREC-NEWS, Robust04, BioASQ: Data collection archives are under Copyright.
• ArguAna, Touché-2020: Provided under CC BY 4.0 license.
• CQADupStack: Provided under Apache License 2.0 license.
• SciFact: Provided under the CC BY-NC 2.0 license.
• SCIDOCS: Provided under the GNU General Public License v3.0 license.
• HotpotQA: Provided under the CC BY-SA 4.0 license.
• TREC-COVID: Provided under the “Dataset License Agreement”.
F Weighted Jaccard Similarity
The weighted Jaccard similarity J(S,T) [26] is intuitively calculated as the unique word overlap for
all words present in both the datasets. More formally, the normalized frequency for an unique word k
in a dataset is calculated as the frequency of word kdivided over the sum of frequencies of all words
in the dataset.
Sk is the normalized frequency of word kin the source dataset S and Tk for the target dataset T
respectively. The weighted Jaccard similarity between Sand T is deﬁned as:
J(S,T) =
∑
k min(Sk,Tk)∑
k max(Sk,Tk)
where the sum is over all unique words kpresent in datasets Sand T.
G Capped Recall@k Score
Recall at k is calculated as the fraction of the relevant documents that are successfully retrieved
within the top kextracted documents. More formally, the R@kscore is calculated as:
R@k= 1
|Q|
|Q|∑
i=1
|maxk(Ai) ∩A⋆
i |
|A⋆
i |
where Qis the set of queries, A⋆
i is the set of relevant documents for the ith query, and Ai is a scored
list of documents provided by the model, from which top kare extracted.
However measuring recall can be counterintuitive, if a high number of relevant documents (>k) are
present within a dataset. For example, consider a hypothetical dataset with 500 relevant documents
for a query. Retrieving all relevant documents would produce a maximum R@100 score = 0.2, which
20



Source: data\tc18_2309.07597v5\referenced_papers\[52]_2207.02578.pdf (Page 2):

encoder
shallow decoder
You never [MASK] what you’re going to [MASK] in life .
You never know what you’re going to get in life .
random mask
bottleneck Lenc : Replaced Language Model Loss
generator
[CLS] You never tell what you’re going to learn in life .
You never expect what you’re going to be in life .
random sampling
Ldec : Replaced Language Model Loss
𝑥𝑒𝑛𝑐
𝑥𝑑𝑒𝑐
𝑥
Figure 1: Pre-training architecture of SimLM. Replaced tokens (underlined) are randomly sampled from the gen-
erator distribution.
much information as possible. RetroMAE (Liu
and Shao, 2022) is a concurrent work at the time
of writing that combines a bottleneck architecture
and the masked auto-encoding objective.
3 SimLM
3.1 Pre-training
For pre-training, we assume there is a collection of
passages C = {xi}|C|
i=1, where x denotes a single
passage. Since our motivation is to have a general
pre-training method, we do not assume access to
any query or human-labeled data.
The overall pre-training architecture is shown
in Figure 1. Given a text sequence x, its tokens
are randomly replaced with probability pby two
sequential operations: random masking with prob-
ability p denoted as x′ = Mask(x,p), and then
sampling from an ELECTRA-style generator gde-
noted as Sample(g,x′). Due to the randomness of
sampling, a replaced token can be the same as the
original one. The above operations are performed
twice with potentially different replace probabili-
ties penc and pdec to get the encoder input xenc and
decoder input xdec.
xenc = Sample(g, Mask(x, penc))
xdec = Sample(g, Mask(x, pdec)) (1)
We also make sure that any replaced token inxenc
is also replaced in xdec to increase the difﬁculty of
the pre-training task.
The encoder is a deep multi-layer Transformer
that can be initialized with pre-trained models like
BERT (Devlin et al., 2019). It takes xenc as in-
put and outputs the last layer [CLS] vector hcls
as a representation bottleneck. The decoder is a
2-layer shallow Transformer with a language mod-
eling head and takes xdec and hcls as inputs. Unlike
the decoder component in autoregressive sequence-
to-sequence models, the self-attention in our de-
coder is bi-directional. The pre-training task is
replaced language modeling for both the encoder
and decoder, which predicts the tokens before re-
placement at all positions. The loss function is the
token-level cross-entropy. The encoder loss Lenc is
shown as follows:
min Lenc = −1
|x|
|x|∑
i=1
log p(x[i] |xenc) (2)
Similarly for the decoder loss Ldec. The ﬁnal pre-
training loss is their simple sum: Lpt = Lenc +Ldec.
We do not ﬁne-tune the parameters of the generator
as our preliminary experiments do not show any
performance gain.
It is often reasonable to assume access to the tar-
get retrieval corpus before seeing any query. There-
fore, we directly pre-train on the target corpus sim-
ilar to coCondenser (Gao and Callan, 2022). After
the pre-training ﬁnishes, we throw away the de-
coder and only keep the encoder for supervised
ﬁne-tuning.
Since the decoder has very limited modeling
capacity, it needs to rely on the representation bot-
tleneck to perform well on the pre-training task.
For the encoder, it should learn to compress all the
semantic information and pass it to the decoder
through the bottleneck.
3.2 Fine-tuning
Compared to training text classiﬁcation or gen-
eration models, training state-of-the-art dense
retrieval models requires a relatively compli-
cated procedure. In Figure 2, we show our



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 20):

is quite low and unintuitive. To avoid this we cap the recall score (R_cap@k) at k for datasets if the
number of relevant documents for a query greater than k. It is deﬁned as:
R_cap@k= 1
|Q|
|Q|∑
i=1
|maxk(Ai) ∩A⋆
i |
min(k,|A⋆
i |)
where the only difference lies within the denominator where we compute the minimum of k and |A⋆
i |,
instead of |A⋆
i |present in the original recall.
H Document Length Preference for Dense Retrieval System
As we show in Figure 4, TAS-B prefers retrieval of shorter documents, and in comparison, ANCE
retrieves longer documents. The difference is especially extreme for the TREC-COVID dataset:
TAS-B retrieves lots of top hit documents containing only a title and an empty abstract, while ANCE
retrieves top hit documents with a non-empty abstract.
Identifying the source for this contrasting behaviour is difﬁcult, as TAS-B and ANCE use different
models (DistilBERT vs. RoBERTa-base), a different loss function (InfoNCE [62] vs. Margin-MSE
[24] with in-batch negatives), and different hard negative mining strategies. Hence, we decided to
harmonize the training setup and to alter the training by just one aspect: The similarity function.
Dense models require a similarity function to retrieve relevant documents for a given query within
an embedding space. This similarity function is also used during training dense models with the
InfoNCE [62] loss:
Lq = −log exp(τ ·sim(q,d+))∑n
i=0 exp(τ ·sim(q,di))
using nin-batch negatives for each query qand a scaling factor τ. where d+ denotes the relevant
(positive) document for queryq. Commonly used similarity functions (sim(q,d)) are cosine-similarity
or dot-product.
We trained two distilbert-base-uncased models with an identical training setup on MS MARCO
(identical training parameters) and only changed the similarity function from cosine-similarity to
dot-product. As shown in Table 10, we observe signiﬁcant performance differences for some BEIR
datasets. For TREC-COVID, the dot-product model achieves the biggest improvement with 15.3
points, while for a majority on other datasets, it performs worse than the cosine-similarity model.
We observe that these (nearly) identical models retrieve documents with vastly different lengths
as shown in the violin plots in Table 10. For all datasets, we ﬁnd the cosine-similarity model to
prefer shorter documents over longer ones. This is especially severe for TREC-COVID: a large
fraction of the scientiﬁc papers (approx. 42k out of 171k) consist only of publication titles without an
abstract. The cosine-similarity model prefers retrieving these documents. In contrast, the dot-product
model primarily retrieves longer documents, i.e., publications with an abstract. Cosine-similarity
uses vectors of unit length, thereby having no notion of the encoded text length. In contrast, for
dot-product, longer documents result in vectors with higher magnitudes which can yield higher
similarity scores for a query.
Further, as we observe in Figure 5, relevance judgement scores are not uniformly distributed over
document lengths: for some datasets, longer documents are annotated with higher relevancy scores,
while in others, shorter documents are. This can be either due to the annotation process, e.g., the
candidate selection method prefers short or long documents, or due to the task itself, where shorter
or longer documents could be more relevant to the user information need. Hence, it can be more
advantageous to train a model with either cosine-similarity or dot-product depending upon the nature
and needs of the speciﬁc task.
21



Source: data\tc18_2309.07597v5\referenced_papers\[52]_2207.02578.pdf (Page 0):

SIMLM: Pre-training with Representation Bottleneck for
Dense Passage Retrieval
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao
Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei
Microsoft Corporation
{wangliang,nanya,xiaolhu,binxjia,yang.linjun,djiang,ranganm,fuwei}@microsoft.com
Abstract
In this paper, we propose S IMLM ( Similarity
matching with Language Model pre-training),
a simple yet effective pre-training method for
dense passage retrieval. It employs a simple
bottleneck architecture that learns to compress
the passage information into a dense vector
through self-supervised pre-training. We use a
replaced language modeling objective, which
is inspired by ELECTRA (Clark et al., 2020),
to improve the sample efﬁciency and reduce
the mismatch of the input distribution between
pre-training and ﬁne-tuning. S IMLM only re-
quires access to an unlabeled corpus and is
more broadly applicable when there are no
labeled data or queries. We conduct experi-
ments on several large-scale passage retrieval
datasets and show substantial improvements
over strong baselines under various settings.
Remarkably, SIMLM even outperforms multi-
vector approaches such as ColBERTv2 (San-
thanam et al., 2021) which incurs signiﬁcantly
more storage cost. Our code and model check-
points are available at https://github.com/
microsoft/unilm/tree/master/simlm.
1 Introduction
Passage retrieval is an important component in ap-
plications like ad-hoc information retrieval, open-
domain question answering (Karpukhin et al.,
2020), retrieval-augmented generation (Lewis
et al., 2020) and fact veriﬁcation (Thorne et al.,
2018). Sparse retrieval methods such as BM25
were the dominant approach for several decades,
and still play a vital role nowadays. With the emer-
gence of large-scale pre-trained language models
(PLM) (Devlin et al., 2019), increasing attention is
being paid to neural dense retrieval methods (Yates
et al., 2021). Dense retrieval methods map both
queries and passages into a low-dimensional vector
space, where the relevance between the queries and
passages are measured by the dot product or cosine
similarity between their respective vectors.
PLM MS-MARCO GLUE
BERT 33.7 80.5
RoBERTa 33.1 88.1
ELECTRA 31.9 89.4
Table 1: Inconsistent performance trends between dif-
ferent models on retrieval task and NLU tasks. We re-
port MRR@10 on the dev set of MS-MARCO passage
ranking dataset and test set results on GLUE bench-
mark. Details are available in the Appendix A.
Like other NLP tasks, dense retrieval beneﬁts
greatly from a strong general-purpose pre-trained
language model. However, general-purpose pre-
training does not solve all the problems. As shown
in Table 1, improved pre-training techniques that
are veriﬁed by benchmarks like GLUE (Wang
et al., 2019) do not result in consistent performance
gain for retrieval tasks. Similar observations are
also made by Lu et al. (2021). We hypothesize
that, to perform robust retrieval, the [CLS] vector
used for computing matching scores should encode
all the essential information in the passage. The
next-sentence prediction (NSP) task in BERT intro-
duces some supervision signals for the [CLS] token,
while RoBERTa (Liu et al., 2019) and ELECTRA
do not have such sequence-level tasks.
In this paper, we propose SimLM to pre-train a
representation bottleneck with replaced language
modeling objective. SimLM consists of a deep en-
coder and a shallow decoder connected with a rep-
resentation bottleneck, which is the [CLS] vector
in our implementation. Given a randomly masked
text segment, we ﬁrst employ a generator to sample
replaced tokens for masked positions, then use both
the deep encoder and shallow decoder to predict
the original tokens at all positions. Since the de-
coder only has limited modeling capacity, it must
rely on the representation bottleneck to perform
well on this pre-training task. As a result, the en-
coder will learn to compress important semantic
information into the bottleneck, which would help
arXiv:2207.02578v2  [cs.IR]  12 May 2023



Source: data\tc18_2309.07597v5\referenced_papers\[46]_1908.10084.pdf (Page 3):

Model STS12 STS13 STS14 STS15 STS16 STSb SICK-R Avg.
Avg. GloVe embeddings 55.14 70.66 59.73 68.25 63.66 58.02 53.76 61.32
Avg. BERT embeddings 38.78 57.98 57.98 63.15 61.06 46.35 58.40 54.81
BERT CLS-vector 20.16 30.01 20.09 36.88 38.08 16.50 42.63 29.19
InferSent - Glove 52.86 66.75 62.15 72.77 66.87 68.03 65.65 65.01
Universal Sentence Encoder 64.49 67.80 64.61 76.83 73.18 74.92 76.69 71.22
SBERT-NLI-base 70.97 76.53 73.19 79.09 74.30 77.03 72.91 74.89
SBERT-NLI-large 72.27 78.46 74.90 80.99 76.25 79.23 73.75 76.55
SRoBERTa-NLI-base 71.54 72.49 70.80 78.74 73.69 77.77 74.46 74.21
SRoBERTa-NLI-large 74.53 77.00 73.18 81.85 76.82 79.10 74.29 76.68
Table 1: Spearman rank correlation ρbetween the cosine similarity of sentence representations and the gold labels
for various Textual Similarity (STS) tasks. Performance is reported by convention as ρ×100. STS12-STS16:
SemEval 2012-2016, STSb: STSbenchmark, SICK-R: SICK relatedness dataset.
(Williams et al., 2018) dataset. The SNLI is a col-
lection of 570,000 sentence pairs annotated with
the labels contradiction, eintailment, and neu-
tral. MultiNLI contains 430,000 sentence pairs
and covers a range of genres of spoken and written
text. We ﬁne-tune SBERT with a 3-way softmax-
classiﬁer objective function for one epoch. We
used a batch-size of 16, Adam optimizer with
learning rate 2e−5, and a linear learning rate
warm-up over 10% of the training data. Our de-
fault pooling strategy is MEAN.
4 Evaluation - Semantic Textual
Similarity
We evaluate the performance of SBERT for com-
mon Semantic Textual Similarity (STS) tasks.
State-of-the-art methods often learn a (complex)
regression function that maps sentence embed-
dings to a similarity score. However, these regres-
sion functions work pair-wise and due to the com-
binatorial explosion those are often not scalable if
the collection of sentences reaches a certain size.
Instead, we always use cosine-similarity to com-
pare the similarity between two sentence embed-
dings. We ran our experiments also with nega-
tive Manhatten and negative Euclidean distances
as similarity measures, but the results for all ap-
proaches remained roughly the same.
4.1 Unsupervised STS
We evaluate the performance of SBERT for STS
without using any STS speciﬁc training data. We
use the STS tasks 2012 - 2016 (Agirre et al., 2012,
2013, 2014, 2015, 2016), the STS benchmark (Cer
et al., 2017), and the SICK-Relatedness dataset
(Marelli et al., 2014). These datasets provide la-
bels between 0 and 5 on the semantic relatedness
of sentence pairs. We showed in (Reimers et al.,
2016) that Pearson correlation is badly suited for
STS. Instead, we compute the Spearman’s rank
correlation between the cosine-similarity of the
sentence embeddings and the gold labels. The
setup for the other sentence embedding methods
is equivalent, the similarity is computed by cosine-
similarity. The results are depicted in Table 1.
The results shows that directly using the output
of BERT leads to rather poor performances. Av-
eraging the BERT embeddings achieves an aver-
age correlation of only 54.81, and using the CLS-
token output only achieves an average correlation
of 29.19. Both are worse than computing average
GloVe embeddings.
Using the described siamese network structure
and ﬁne-tuning mechanism substantially improves
the correlation, outperforming both InferSent and
Universal Sentence Encoder substantially. The
only dataset where SBERT performs worse than
Universal Sentence Encoder is SICK-R. Universal
Sentence Encoder was trained on various datasets,
including news, question-answer pages and dis-
cussion forums, which appears to be more suitable
to the data of SICK-R. In contrast, SBERT was
pre-trained only on Wikipedia (via BERT) and on
NLI data.
While RoBERTa was able to improve the per-
formance for several supervised tasks, we only
observe minor difference between SBERT and
SRoBERTa for generating sentence embeddings.
4.2 Supervised STS
The STS benchmark (STSb) (Cer et al., 2017) pro-
vides is a popular dataset to evaluate supervised
STS systems. The data includes 8,628 sentence
pairs from the three categoriescaptions, news, and
forums. It is divided into train (5,749), dev (1,500)
and test (1,379). BERT set a new state-of-the-art
performance on this dataset by passing both sen-
tences to the network and using a simple regres-



### Claim 27/38

#### Claim Text
Interestingly, the equation first derived there has recently been rederived in using a molecular mechanics approach.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 22):

Corpus Website (Link)
CORD-19 https://www.semanticscholar.org/cord19
NutritionFacts https://nutritionfacts.org
PubMed https://pubmed.ncbi.nlm.nih.gov
Signal-1M https://research.signal-ai.com/datasets/signal1m.html
TREC Washington Posthttps://ir.nist.gov/wapo/
TREC disks 4 and 5 https://trec.nist.gov/data/cd45/
Args.me https://zenodo.org/record/4139439/
DBPedia (2015-10) http://downloads.dbpedia.org/wiki-archive/Downloads2015-10.html
TREC-COVID (Annotated)https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid-beir.zip
Table 7: Corpus Name and Link used for datasets in BEIR .
Dataset Query Relevant-Document
MS MARCOwhat fruit is native to australia <Paragraph>Passiﬂora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned,white ﬂeshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, whileothers list the fruits as being bitter and inedible. assiﬂora herbertiana. A rare passion fruit native toAustralia...
TREC-COVIDwhat is the origin of COVID-19<Title>Origin of Novel Coronavirus (COVID-19): A Computational Biology Study using ArtiﬁcialIntelligence<Paragraph>Origin of the COVID-19 virus has been intensely debated in the community...
BioASQ What is the effect of HMGB2 loss on CTCFclustering <Title>HMGB2 Loss upon Senescence Entry Disrupts Genomic Organization and Induces CTCFClustering across Cell Types.<Paragraph>Processes like cellular senescence are characterized bycomplex events giving rise to heterogeneous cell populations. However, the early molecular eventsdriving this cascade remain elusive....
NFCorpus Titanium Dioxide & Inﬂammatory Bowel Dis-ease <Title>Titanium Dioxide Nanoparticles in Food and Personal Care Products<Paragraph>Titaniumdioxide is a common additive in many food, personal care, and other consumer products used by people,which after use can enter the sewage system, and subsequently enter the environment as treated efﬂuentdischarged to surface waters or biosolids applied to agricultural land, or incinerated wastes...
NQ when did they stop cigarette advertising on tele-vision? <Title>Tobacco advertising<Paragraph>The ﬁrst calls to restrict advertising came in 1962 from theRoyal College of Physicians, who highlighted the health problems and recommended stricter laws...
HotpotQA Stockely Webster has paintings hanging in whathome (that serves as the residence for the Mayorof New York)?
<Title>Stokely Webster<Paragraph>Stokely Webster (1912 – 2001) was best known as an Americanimpressionist painter who studied in Paris. His paintings can be found in the permanent collections ofmany museums, including the Metropolitan Museum of Art in New York, the National Museum...
FiQA-2018 What is the PEG ratio? How is the PEG ratiocalculated? How is the PEG ratio useful forstock investing?
<Paragraph>PEG is Price/Earnings to Growth. It is calculated as Price/Earnings/Annual EPS Growth.It represents how good a stock is to buy, factoring in growth of earnings, which P/E does not. Obviouslywhen PEG is lower, a stock is more undervalued, which means that it is a better buy, and more likely...
Signal-1M (RT)Genvoya, a Gentler Anti-HIV Cocktail, Okayedby EU Regulators <Paragraph>All people with #HIV should get anti-retroviral drugs: @WHO, by @kkelland via@Reuters_Health #AIDS #TasP
TREC-NEWSWebsites where children are prostituted are im-mune from prosecution. But why?<Title>Senate launches bill to remove immunity for websites hosting illegal content, spurred byBackpage.com<Paragraph>The legislation, along with a similar bill in the House, sets the stage for abattle between Congress and some of the Internet’s most powerful players, including Google and variousfree-speech advocates, who believe that Congress shouldn’t regulate Web content or try to force websitesto police themselves more rigorously...
Robust04 What were the causes for the Islamic Revolutionrelative to relations with the U.S.?<Paragraph>BFN [Editorial: "Sow the Wind and Reap the Whirlwind"] Yesterday marked the 14thanniversary of severing of diplomatic relations between the Islamic Republic and the United States ofAmerica. Several occasions arose in the last decade and a half for improving Irano-American relations...
Touché-2020Should the government allow illegal immigrantsto become citizens? <Title>America should support blanket amnesty for illegal immigrants.<Paragraph>Undocumentedworkers do not receive full Social Security beneﬁts because they are not United States citizens " norshould they be until they seek citizenship legally. Illegal immigrants are legally obligated to pay taxes...
CQADupStackCommand to display ﬁrst few and last few linesof a ﬁle <Title>Combing head and tail in a single call via pipe<Paragraph>On a regular basis, I am piping theoutput of some program to either ‘head‘ or ‘tail‘. Now, suppose that I want to see the ﬁrst AND last 10lines of piped output, such that I could do something like ./lotsofoutput | headtail...
Quora How long does it take to methamphetamine outof your blood? <Paragraph>How long does it take the body to get rid of methamphetamine?
DBPedia Paul Auster novels <Title>The New York Trilogy<Paragraph>The New York Trilogy is a series of novels by Paul Auster.Originally published sequentially as City of Glass (1985), Ghosts (1986) and The Locked Room (1986),it has since been collected into a single volume.
SCIDOCS CFD Analysis of Convective Heat Transfer Co-efﬁcient on External Surfaces of Buildings<Title>Application of CFD in building performance simulation for the outdoor environment: an overview<Paragraph>This paper provides an overview of the application of CFD in building performancesimulation for the outdoor environment, focused on four topics...
FEVER DodgeBall: A True Underdog Story is an Amer-ican movie from 2004 <Title>DodgeBall: A True Underdog Story<Paragraph>DodgeBall: A True Underdog Story is a2004 American sports comedy ﬁlm written and directed by Rawson Marshall Thurber and starring VinceVaughn and Ben Stiller. The ﬁlm follows friends who enter a dodgeball tournament...
Climate-FEVERSea level rise is now increasing faster than pre-dicted due to unexpectedly rapid ice melting.<Title>Sea level rise<Paragraph>A sea level rise is an increase in the volume of water in the world ’soceans, resulting in an increase in global mean sea level. The rise is usually attributed to global climatechange by thermal expansion of the water in the oceans and by melting of Ice sheets and glaciers...
Table 8: Examples of queries and relevant documents for all datasets included in BEIR . ( <Title>) and
(<Paragraph>) are used to distinguish the title separately from the paragraph within a document in the table
above. These tokens were not passed to the respective models.
23



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 15):

Preprint
(a) 182539: monotonic function
 (b) 1117099: active margin
Query
Relevant
ANCE Neg
BM25 Neg
Rand Neg (c) 1132213: yoga bow
Figure 7: t-SNE Plots for Losing Cases in Table 9.
margin” is a geographical terminology, not a ﬁnancial one (which we did not know ourselves and took some time
to ﬁgure out when conducting this case study). There are also some cases where the dense retrieved documents
make sense to us but were labeled irrelevant.
The t-SNE plots in Fig. 6 and Fig. 7 show many interesting patterns of the learned representation space. The
ANCE winning cases often correspond to clear separations of different document groups. For losing cases the
representation space is more mixed, or there is too few relevant documents which may cause the variances in
model performances. There are also many different interesting patterns in the ANCE-learned representation
space. We include the t-SNE plots for all 43 TREC DL Track queries in the supplementary material. More future
analyses of the learned patterns in the representation space may help provide more insights on dense retrieval.
16



Source: data\tc18_2309.07597v5\referenced_papers\[32]_2202.08904.pdf (Page 13):

A Additional results
Ranking ( →) Re-rank Top 0 Re-rank Top 10 Re-rank Top 100
Model ( →) [41] SGPT-CE SGPT-CE
Dataset ( ↓) BM25 125M 1.3B Bound 125M 1.3B Bound
MS MARCO 0.228 0.237 0.245 0.383 0.232 0.267 0.664
TREC-COVID 0.688 0.695 0.694 0.750 0.693 0.735 0.988
BioASQ 0.488 0.507 0.514 0.588 0.511 0.528 0.798
NFCorpus 0.306 0.314 0.316 0.364 0.298 0.327 0.513
NQ 0.326 0.336 0.354 0.514 0.319 0.367 0.788
HotpotQA 0.602 0.633 0.645 0.690 0.658 0.688 0.808
FiQA-2018 0.254 0.281 0.297 0.363 0.280 0.340 0.595
Signal-1M (RT) 0.330 0.339 0.343 0.390 0.307 0.322 0.619
TREC-NEWS 0.405 0.400 0.402 0.492 0.393 0.443 0.831
Robust04 0.425 0.419 0.434 0.508 0.382 0.427 0.854
ArguAna 0.472 0.394 0.383 0.754 0.315 0.299 0.952
Touché-2020 0.347 0.340 0.335 0.467 0.268 0.261 0.881
CQADupStack 0.326 0.348 0.360 0.426 0.357 0.394 0.637
Quora 0.808 0.794 0.809 0.914 0.764 0.791 0.982
DBPedia 0.320 0.328 0.336 0.397 0.329 0.356 0.651
SCIDOCS 0.165 0.173 0.177 0.245 0.171 0.185 0.465
FEVER 0.649 0.735 0.718 0.824 0.762 0.729 0.931
Climate-FEVER 0.186 0.194 0.191 0.281 0.179 0.167 0.474
SciFact 0.611 0.626 0.645 0.729 0.621 0.652 0.825
Average 0.428 0.436 0.442 0.539 0.423 0.445 0.755
Table 7: Additional SGPT Cross-Encoder scores on BEIR. Bounds are the maximum achievable score,
given the ﬁrst-stage BM25 results. We report additional Max Re-rank=10 scores using OpenAI’s
search endpoint: TREC-COVID: 0.545 (Ada), 0.539 (Davinci); SciFact: 0.670 (Ada), 0.658 (Davinci).
Scores are nDCG@10. Average scores do not include MS MARCO.
14



Source: data\tc18_2309.07597v5\referenced_papers\[28]_2308.03281.pdf (Page 7):

Params Class. Clust. Pair. Rerank Retr. STS Summ. Avg
# of datasets → 12 11 3 4 15 10 1 56
Unsupervised models
Glove 120M 57.3 27.7 70.9 43.3 21.6 61.9 28.9 42.0
BERT 110M 61.7 30.1 56.3 43.4 10.6 54.4 29.8 38.3
SimCSE 110M 62.5 29.0 70.3 46.5 20.3 74.3 31.2 45.5
E5small 30M 67.0 41.7 78.2 53.1 40.8 68.8 25.2 54.2
E5base 110M 67.9 43.4 79.2 53.5 42.9 69.5 24.3 55.5
E5large 330M 69.0 44.3 80.3 54.4 44.2 69.9 24.8 56.4
GTEsmall 30M 71.0 44.9 82.4 57.5 43.4 77.2 30.4 58.5
GTEbase 110M 71.5 46.0 83.3 58.4 44.2 76.5 29.5 59.0
GTElarge 330M 71.8 46.4 83.3 58.8 44.6 76.3 30.1 59.3
Supervised models
SimCSE 110M 67.3 33.4 73.7 47.5 21.8 79.1 23.3 48.7
Contriever 110M 66.7 41.1 82.5 53.1 41.9 76.5 30.4 56.0
GTRlarge 330M 67.1 41.6 85.3 55.4 47.4 78.2 29.5 58.3
Sentence-T5large 330M 72.3 41.7 85.0 54.0 36.7 81.8 29.6 57.1
E5small 30M 71.7 39.5 85.1 54.5 46.0 80.9 31.4 58.9
E5base 110M 72.6 42.1 85.1 55.7 48.7 81.0 31.0 60.4
E5large 330M 73.1 43.3 85.9 56.5 50.0 82.1 31.0 61.4
InstructORbase 110M 72.6 42.1 85.1 55.7 48.8 81.0 31.0 60.4
InstructORlarge 330M 73.9 45.3 85.9 57.5 47.6 83.2 31.8 61.6
OpenAIada-001 n.a. 70.4 37.5 76.9 49.0 18.4 78.6 26.9 49.5
OpenAIada-002 n.a. 70.9 45.9 84.9 56.3 49.3 81.0 30.8 61.0
GTEsmall 30M 72.3 44.9 83.5 57.7 49.5 82.1 30.4 61.4
GTEbase 110M 73.0 46.1 84.3 58.6 51.2 82.3 30.7 62.4
GTElarge 330M 73.3 46.8 85.0 59.1 52.2 83.4 31.7 63.1
Larger models
InstructORxl 1.5B 73.1 44.7 86.6 57.3 49.3 83.1 32.3 61.8
GTRxxl 4.5B 67.4 42.4 86.1 56.7 48.5 78.4 30.6 59.0
Sentence-T5xxl 4.5B 73.4 43.7 85.1 56.4 42.2 82.6 30.1 59.5
Table 6: Results on the MTEB (Muennighoff et al., 2023) (56 datasets in English subset). Compared models include
SimCSE (Gao et al., 2021), Sentence-T5 (Ni et al., 2022a), GTR (Ni et al., 2022b), Contriever (Izacard et al., 2022a),
OpenAI text embedding API (Neelakantan et al., 2022), E5 (Wang et al., 2022b) and InstructOR (Su et al., 2023).
Exact parameter amount of OpenAI ada model is not available, but is suspected to be ∼300M, comparable to the
BERT large size model.
by a large margin despite using a modest model
size. GTE small is comparable to E5 large while be-
ing 10× smaller. GTE large establishes new state-
of-the-art performance on the MTEB benchmark,
outperforming the multi-task instruction-finetuned
embedding model, InstructOR large, by 1.5 points
on average.
4.4 Code Search
Programming languages can be regarded as a dis-
tinct form of text. To assess the effectiveness of our
approach in code search, we conduct a comparative
analysis with other code-based language models,
such as CodeBERT (Guo et al., 2021) and Graph-
CodeBERT (Guo et al., 2021). We also compare
our approach with a more recent code language
model called UniXcoder (Guo et al., 2022), which
aims to integrate various pre-training tasks into a
unified model. CodeRetriever (Li et al., 2022) is ini-
tialized from GraphCodeBERT and pre-trained on
large-scale multi-modal code-text pairs mined and
cleaned by heuristics. It is important to note that
while the baseline models are individually trained
and evaluated for each programming language, our
model is directly evaluated across all the languages.
In line with recent work (Guo et al., 2021, 2022;



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 20):

is quite low and unintuitive. To avoid this we cap the recall score (R_cap@k) at k for datasets if the
number of relevant documents for a query greater than k. It is deﬁned as:
R_cap@k= 1
|Q|
|Q|∑
i=1
|maxk(Ai) ∩A⋆
i |
min(k,|A⋆
i |)
where the only difference lies within the denominator where we compute the minimum of k and |A⋆
i |,
instead of |A⋆
i |present in the original recall.
H Document Length Preference for Dense Retrieval System
As we show in Figure 4, TAS-B prefers retrieval of shorter documents, and in comparison, ANCE
retrieves longer documents. The difference is especially extreme for the TREC-COVID dataset:
TAS-B retrieves lots of top hit documents containing only a title and an empty abstract, while ANCE
retrieves top hit documents with a non-empty abstract.
Identifying the source for this contrasting behaviour is difﬁcult, as TAS-B and ANCE use different
models (DistilBERT vs. RoBERTa-base), a different loss function (InfoNCE [62] vs. Margin-MSE
[24] with in-batch negatives), and different hard negative mining strategies. Hence, we decided to
harmonize the training setup and to alter the training by just one aspect: The similarity function.
Dense models require a similarity function to retrieve relevant documents for a given query within
an embedding space. This similarity function is also used during training dense models with the
InfoNCE [62] loss:
Lq = −log exp(τ ·sim(q,d+))∑n
i=0 exp(τ ·sim(q,di))
using nin-batch negatives for each query qand a scaling factor τ. where d+ denotes the relevant
(positive) document for queryq. Commonly used similarity functions (sim(q,d)) are cosine-similarity
or dot-product.
We trained two distilbert-base-uncased models with an identical training setup on MS MARCO
(identical training parameters) and only changed the similarity function from cosine-similarity to
dot-product. As shown in Table 10, we observe signiﬁcant performance differences for some BEIR
datasets. For TREC-COVID, the dot-product model achieves the biggest improvement with 15.3
points, while for a majority on other datasets, it performs worse than the cosine-similarity model.
We observe that these (nearly) identical models retrieve documents with vastly different lengths
as shown in the violin plots in Table 10. For all datasets, we ﬁnd the cosine-similarity model to
prefer shorter documents over longer ones. This is especially severe for TREC-COVID: a large
fraction of the scientiﬁc papers (approx. 42k out of 171k) consist only of publication titles without an
abstract. The cosine-similarity model prefers retrieving these documents. In contrast, the dot-product
model primarily retrieves longer documents, i.e., publications with an abstract. Cosine-similarity
uses vectors of unit length, thereby having no notion of the encoded text length. In contrast, for
dot-product, longer documents result in vectors with higher magnitudes which can yield higher
similarity scores for a query.
Further, as we observe in Figure 5, relevance judgement scores are not uniformly distributed over
document lengths: for some datasets, longer documents are annotated with higher relevancy scores,
while in others, shorter documents are. This can be either due to the annotation process, e.g., the
candidate selection method prefers short or long documents, or due to the task itself, where shorter
or longer documents could be more relevant to the user information need. Hence, it can be more
advantageous to train a model with either cosine-similarity or dot-product depending upon the nature
and needs of the speciﬁc task.
21



### Claim 28/38

#### Claim Text
Although no unique measure of “evenness” of a spatial distribution in three spatial dimensions exists, both iterative approaches as well as explicit constructions on the sphere are available for practical purposes, as discussed e.g. in and .

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 20):

is quite low and unintuitive. To avoid this we cap the recall score (R_cap@k) at k for datasets if the
number of relevant documents for a query greater than k. It is deﬁned as:
R_cap@k= 1
|Q|
|Q|∑
i=1
|maxk(Ai) ∩A⋆
i |
min(k,|A⋆
i |)
where the only difference lies within the denominator where we compute the minimum of k and |A⋆
i |,
instead of |A⋆
i |present in the original recall.
H Document Length Preference for Dense Retrieval System
As we show in Figure 4, TAS-B prefers retrieval of shorter documents, and in comparison, ANCE
retrieves longer documents. The difference is especially extreme for the TREC-COVID dataset:
TAS-B retrieves lots of top hit documents containing only a title and an empty abstract, while ANCE
retrieves top hit documents with a non-empty abstract.
Identifying the source for this contrasting behaviour is difﬁcult, as TAS-B and ANCE use different
models (DistilBERT vs. RoBERTa-base), a different loss function (InfoNCE [62] vs. Margin-MSE
[24] with in-batch negatives), and different hard negative mining strategies. Hence, we decided to
harmonize the training setup and to alter the training by just one aspect: The similarity function.
Dense models require a similarity function to retrieve relevant documents for a given query within
an embedding space. This similarity function is also used during training dense models with the
InfoNCE [62] loss:
Lq = −log exp(τ ·sim(q,d+))∑n
i=0 exp(τ ·sim(q,di))
using nin-batch negatives for each query qand a scaling factor τ. where d+ denotes the relevant
(positive) document for queryq. Commonly used similarity functions (sim(q,d)) are cosine-similarity
or dot-product.
We trained two distilbert-base-uncased models with an identical training setup on MS MARCO
(identical training parameters) and only changed the similarity function from cosine-similarity to
dot-product. As shown in Table 10, we observe signiﬁcant performance differences for some BEIR
datasets. For TREC-COVID, the dot-product model achieves the biggest improvement with 15.3
points, while for a majority on other datasets, it performs worse than the cosine-similarity model.
We observe that these (nearly) identical models retrieve documents with vastly different lengths
as shown in the violin plots in Table 10. For all datasets, we ﬁnd the cosine-similarity model to
prefer shorter documents over longer ones. This is especially severe for TREC-COVID: a large
fraction of the scientiﬁc papers (approx. 42k out of 171k) consist only of publication titles without an
abstract. The cosine-similarity model prefers retrieving these documents. In contrast, the dot-product
model primarily retrieves longer documents, i.e., publications with an abstract. Cosine-similarity
uses vectors of unit length, thereby having no notion of the encoded text length. In contrast, for
dot-product, longer documents result in vectors with higher magnitudes which can yield higher
similarity scores for a query.
Further, as we observe in Figure 5, relevance judgement scores are not uniformly distributed over
document lengths: for some datasets, longer documents are annotated with higher relevancy scores,
while in others, shorter documents are. This can be either due to the annotation process, e.g., the
candidate selection method prefers short or long documents, or due to the task itself, where shorter
or longer documents could be more relevant to the user information need. Hence, it can be more
advantageous to train a model with either cosine-similarity or dot-product depending upon the nature
and needs of the speciﬁc task.
21



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 15):

Preprint
(a) 182539: monotonic function
 (b) 1117099: active margin
Query
Relevant
ANCE Neg
BM25 Neg
Rand Neg (c) 1132213: yoga bow
Figure 7: t-SNE Plots for Losing Cases in Table 9.
margin” is a geographical terminology, not a ﬁnancial one (which we did not know ourselves and took some time
to ﬁgure out when conducting this case study). There are also some cases where the dense retrieved documents
make sense to us but were labeled irrelevant.
The t-SNE plots in Fig. 6 and Fig. 7 show many interesting patterns of the learned representation space. The
ANCE winning cases often correspond to clear separations of different document groups. For losing cases the
representation space is more mixed, or there is too few relevant documents which may cause the variances in
model performances. There are also many different interesting patterns in the ANCE-learned representation
space. We include the t-SNE plots for all 43 TREC DL Track queries in the supplementary material. More future
analyses of the learned patterns in the representation space may help provide more insights on dense retrieval.
16



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 2):

Preprint
instances, which have to be sampled in training:
θ∗= argminθ
∑
q
∑
d+∈D+
∑
d−∈ˆD−
l(f(q,d+),f(q,d−)). (3)
A natural choice is to sample negatives ˆD−from top documents retrieved by BM25. However, they
may bias the DR model to merely learn sparse retrieval and do not elevate DR models much beyond
BM25 (Luan et al., 2020). Another way is to sample negatives in local mini-batches, e.g., as in
contrastive learning (Oord et al., 2018; Chen et al., 2020a), however, these local negatives do not
signiﬁcantly outperform BM25 negatives (Karpukhin et al., 2020; Luan et al., 2020).
3 A NALYSES ON THE CONVERGENCE OF DENSE RETRIEVAL TRAINING
In this section, we provide theoretical analyses on the convergence of representation training in dense
retrieval. We ﬁrst show the connections between learning convergence and gradient norms, then the
bounded gradient norms by uninformative negatives, and ﬁnally, how in-batch local negatives are
ineffective under common conditions in dense retrieval.
Convergence Rate and Gradient Norms: Let l(d+,d−) = l(f(q,d+),f(q,d−) be the loss func-
tion on the training triple (q,d+,d−), PD− the negative sampling distribution for the given (q,d+),
and pd− the sampling probability of negative instance d−, a stochastic gradient decent (SGD) step
with importance sampling (Alain et al., 2015) is:
θt+1 = θt −η 1
Npd−
∇θtl(d+,d−), (4)
with θt the parameter at t-th step, θt+1 the one after, and N the total number of negatives. The scaling
factor 1
Npd−
is to make sure Eqn. 4 is an unbiased estimator of the full gradient.
Then we can characterize the converge rate of this SGD step as the movement to optimalθ∗. Following
derivations in variance reduction (Katharopoulos & Fleuret, 2018; Johnson & Guestrin, 2018), let
gd− = 1
Npd−
∇θtl(d+,d−) the weighted gradient, the convergence rate is:
E∆t = ||θt −θ∗||2 −EPD− (||θt+1 −θ∗||2) (5)
= ||θt||2 −2θT
t θ∗−EPD− (||θt −ηgd−||2) + 2θ∗TEPD− (θt −ηgd−) (6)
= −η2EPD− (||gd−||2) + 2ηθT
t EPD− (gd−) −2ηθ∗TEPD− (gd−) (7)
= 2ηEPD− (gd−)T(θt −θ∗) −η2EPD− (||gd−||2) (8)
= 2ηEPD− (gd−)T(θt −θ∗) −η2EPD− (gd−)TEPD− (gd−) −η2Tr(VPD− (gd−)). (9)
This shows we can obtain better convergence rate by sampling from a distributionPD− that minimizes
the variance of the gradient estimator,EPD− (||gd−||2), or Tr(VPD− (gd−)) as the estimator is unbiased.
There exists an optimal distribution that:
p∗
d− = argminpd− Tr(VPD− (gd−)) ∝||∇θtl(d+,d−)||2, (10)
which is to sample proportionally to per instance gradient norm. This is a well known result in
importance sampling (Alain et al., 2015; Johnson & Guestrin, 2018). It can be proved by applying
Jensen’s inequality on the gradient variance and then verifying that Eqn. 10 achieves the minimum.
We do not repeat this proof and refer to Johnson & Guestrin (2018) for exact derivations.
Intuitively, an negative instance with larger gradient norm is more likely to reduce the training loss
more, while those with diminishing gradients are not informative. Empirically, the correlation of
gradient norm and training convergence is also observed in BERT ﬁne-tuning (Mosbach et al., 2020).
Diminishing Gradients of Uninformative Negatives: The oracle distribution in Eqn. 10 is too
expensive to compute and the closed form of gradient norms can be complicated in deep neural
networks. Nevertheless, for MLP networks, Katharopoulos & Fleuret (2018) derives an upper bound
of the per sample gradient norm:
||∇θtl(d+,d−)||2 ≤Lρ||∇φLl(d+,d−)||2, (11)
3



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 4):

Class. Clust. PairClass. Rerank. Retr. STS Summ. Avg.
Num. Datasets (→) 12 11 3 4 15 10 1 56
Self-supervised methods
Glove 57.29 27.73 70.92 43.29 21.62 61.85 28.87 41.97
Komninos 57.65 26.57 72.94 44.75 21.22 62.47 30.49 42.06
BERT 61.66 30.12 56.33 43.44 10.59 54.36 29.82 38.33
SimCSE-BERT-unsup 62.50 29.04 70.33 46.47 20.29 74.33 31.15 45.45
Supervised methods
SimCSE-BERT-sup 67.32 33.43 73.68 47.54 21.82 79.12 23.31 48.72
coCondenser-msmarco 64.71 37.64 81.74 51.84 32.96 76.47 29.50 52.35
Contriever 66.68 41.10 82.53 53.14 41.88 76.51 30.36 56.00
SPECTER 52.37 34.06 61.37 48.10 15.88 61.02 27.66 40.28
LaBSE 62.71 29.55 78.87 48.42 18.99 70.80 31.05 45.21
LASER2 53.65 15.28 68.86 41.44 7.93 55.32 26.80 33.63
MiniLM-L6 63.06 42.35 82.37 58.04 41.95 78.90 30.81 56.26
MiniLM-L12 63.21 41.81 82.41 58.44 42.69 79.80 27.90 56.53
MiniLM-L12-multilingual 64.30 37.14 78.45 53.62 32.45 78.92 30.67 52.44
MPNet 65.07 43.69 83.04 59.36 43.81 80.28 27.49 57.78
MPNet-multilingual 67.91 38.40 80.81 53.80 35.34 80.73 31.57 54.71
OpenAI Ada Similarity 70.44 37.52 76.86 49.02 18.36 78.60 26.94 49.52
SGPT-125M-nli 61.46 30.95 71.78 47.56 20.90 74.71 30.26 45.97
SGPT-5.8B-nli 70.14 36.98 77.03 52.33 32.34 80.53 30.38 53.74
SGPT-125M-msmarco 60.72 35.79 75.23 50.58 37.04 73.41 28.90 51.23
SGPT-1.3B-msmarco 66.52 39.92 79.58 54.00 44.49 75.74 25.44 56.11
SGPT-2.7B-msmarco 67.13 39.83 80.65 54.67 46.54 76.83 27.87 57.12
SGPT-5.8B-msmarco 68.13 40.35 82.00 56.56 50.25 78.10 24.75 58.81
SGPT-BLOOM-7.1B-msmarco66.19 38.93 81.90 55.65 48.21 77.74 24.99 57.44
GTR-Base 65.25 38.63 83.85 54.23 44.67 77.07 29.67 56.19
GTR-Large 67.14 41.60 85.33 55.36 47.42 78.19 29.50 58.28
GTR-XL 67.11 41.51 86.13 55.96 47.96 77.80 30.21 58.42
GTR-XXL 67.41 42.42 86.12 56.65 48.48 78.38 30.64 58.97
ST5-Base 69.81 40.21 85.17 53.09 33.63 81.14 31.39 55.27
ST5-Large 72.31 41.65 84.97 54.00 36.71 81.83 29.64 57.06
ST5-XL 72.84 42.34 86.06 54.71 38.47 81.66 29.91 57.87
ST5-XXL 73.42 43.71 85.06 56.43 42.24 82.63 30.08 59.51
Table 1: Average of the main metric (see Section 3.2) per task per model on MTEB English subsets.
Paragraph to paragraph (P2P) A paragraph is
compared with another paragraph. MTEB imposes
no limit on the input length, leaving it up to the
models to truncate if necessary. Several clustering
tasks are framed as both S2S and P2P tasks. The
former only compare titles, while the latter include
both title and content. For ArxivClustering, for
example, abstracts are concatenated to the title in
the P2P setting.
Sentence to paragraph (S2P) A few retrieval
datasets are mixed in a S2P setting. Here a query
is a single sentence, while documents are long
paragraphs consisting of multiple sentences.
Similarities across 56 MTEB datasets are vi-
sualized in Figure 2. Several datasets rely on
the same corpora, such as ClimateFEVER and
FEVER, resulting in a score of 1. Clusters of simi-
lar datasets can be seen among CQADupstack vari-
ations and STS datasets. S2S and P2P variations of
the same dataset tend to also be similar. Scientiﬁc
datasets, such as SciDocsRR, SciFact, ArxivClus-
tering, show high similarities among each other
even when coming from different tasks (Reranking,
Retrieval and Clustering in this case).
4 Results
4.1 Models
We evaluate on the test splits of all datasets except
for MSMARCO, where the dev split is used follow-
ing Thakur et al. (2021). We benchmark models
claiming state-of-the-art results on various embed-



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 1):

Preprint
Query
Relevant
DR Neg
BM25 Neg
Rand Neg
Figure 1: T-SNE (Maaten & Hinton,
2008) representations of query, relevant
documents, negative training instances
from BM25 (BM25 Neg) or randomly
sampled (Rand Neg), and testing nega-
tives (DR Neg) in dense retrieval.
In this paper, we ﬁrst theoretically analyze the convergence
of dense retrieval training with negative sampling. Us-
ing the variance reduction framework (Alain et al., 2015;
Katharopoulos & Fleuret, 2018), we show that, under con-
ditions commonly met in dense retrieval, local in-batch
negatives lead to diminishing gradient norms, resulted in
high stochastic gradient variances and slow training con-
vergence — the local negative sampling is the bottleneck
of dense retrieval’s effectiveness.
Based on our analysis, we propose Approximate near-
est neighbor Negative Contrastive Estimation (ANCE),
a new contrastive representation learning mechanism for
dense retrieval. Instead of random or in-batch local neg-
atives, ANCE constructs global negatives using the being-
optimized DR model to retrieve from the entire corpus.
This fundamentally aligns the distribution of negative sam-
ples in training and of irrelevant documents to separate in
testing. From the variance reduction point of view, these ANCE negatives lift the upper bound of per
instance gradient norm, reduce the variance of the stochastic gradient estimation, and lead to faster
learning convergence.
We implement ANCE using an asynchronously updated ANN index of the corpus representation.
Similar to Guu et al. (2020), we maintain an Inferencer that parallelly computes the document
encodings with a recent checkpoint from the being optimized DR model, and refresh the ANN index
used for negative sampling once it ﬁnishes, to keep up with the model training. Our experiments
demonstrate the advantage of ANCE in three text retrieval scenarios: standard web search (Craswell
et al., 2020), OpenQA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), and in a commercial search
engine’s retrieval system. We also empirically validate our theory that the gradient norms on ANCE
sampled negatives are much bigger than local negatives and thus improve the convergence of dense
retrieval models. Our code and trained models are available at https://aka.ms/ance.
2 P RELIMINARIES
In this section, we discuss the preliminaries of dense retrieval and its representation learning.
Task Deﬁnition: Given a query qand a corpus C, the ﬁrst stage retrieval is to ﬁnd a set of documents
relevant to the query D+ = {d1,...,d i,...,d n}from C(|D+|≪| C|), which then serve as input to
later more complex models (Croft et al., 2010). Instead of using sparse term matches and inverted
index, Dense Retrieval calculates the retrieval score f() using similarities in a learned embedding
space (Lee et al., 2019; Luan et al., 2020; Karpukhin et al., 2020):
f(q,d) = sim(g(q; θ),g(d; θ)), (1)
where g() is the representation model that encodes the query or document to dense embeddings. The
encoder parameter θprovides the main capacity, often ﬁne-tuned from pretrained transformers, e.g.,
BERT (Lee et al., 2019). The similarity function (sim()) is often simply cosine or dot product, to
leverage efﬁcient ANN retrieval (Johnson et al., 2019; Guo et al., 2020).
Learning with Negative Sampling: The effectiveness of DR resides in learning a good representa-
tion space that maps query and relevant documents together, while separating irrelevant ones. The
learning of this representation often follows standard learning to rank (Liu, 2009): Given a query q, a
set of relevant document D+ and irrelevant ones D−, ﬁnd the best θ∗that:
θ∗= argminθ
∑
q
∑
d+∈D+
∑
d−∈D−
l(f(q,d+),f(q,d−)). (2)
The loss l() can be binary cross entropy (BCE), hinge loss, or negative log likelihood (NLL).
A unique challenge in dense retrieval, targeting ﬁrst stage retrieval, is that the irrelevant documents
to separate are from the entire corpus (D− = C \D+). This often leads to millions of negative
2



### Claim 29/38

#### Claim Text
Fortunately, the 500 mm focal length lens results look more similar to the expected profiles and the 750 mm results are almost in agreement with literature .

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 20):

is quite low and unintuitive. To avoid this we cap the recall score (R_cap@k) at k for datasets if the
number of relevant documents for a query greater than k. It is deﬁned as:
R_cap@k= 1
|Q|
|Q|∑
i=1
|maxk(Ai) ∩A⋆
i |
min(k,|A⋆
i |)
where the only difference lies within the denominator where we compute the minimum of k and |A⋆
i |,
instead of |A⋆
i |present in the original recall.
H Document Length Preference for Dense Retrieval System
As we show in Figure 4, TAS-B prefers retrieval of shorter documents, and in comparison, ANCE
retrieves longer documents. The difference is especially extreme for the TREC-COVID dataset:
TAS-B retrieves lots of top hit documents containing only a title and an empty abstract, while ANCE
retrieves top hit documents with a non-empty abstract.
Identifying the source for this contrasting behaviour is difﬁcult, as TAS-B and ANCE use different
models (DistilBERT vs. RoBERTa-base), a different loss function (InfoNCE [62] vs. Margin-MSE
[24] with in-batch negatives), and different hard negative mining strategies. Hence, we decided to
harmonize the training setup and to alter the training by just one aspect: The similarity function.
Dense models require a similarity function to retrieve relevant documents for a given query within
an embedding space. This similarity function is also used during training dense models with the
InfoNCE [62] loss:
Lq = −log exp(τ ·sim(q,d+))∑n
i=0 exp(τ ·sim(q,di))
using nin-batch negatives for each query qand a scaling factor τ. where d+ denotes the relevant
(positive) document for queryq. Commonly used similarity functions (sim(q,d)) are cosine-similarity
or dot-product.
We trained two distilbert-base-uncased models with an identical training setup on MS MARCO
(identical training parameters) and only changed the similarity function from cosine-similarity to
dot-product. As shown in Table 10, we observe signiﬁcant performance differences for some BEIR
datasets. For TREC-COVID, the dot-product model achieves the biggest improvement with 15.3
points, while for a majority on other datasets, it performs worse than the cosine-similarity model.
We observe that these (nearly) identical models retrieve documents with vastly different lengths
as shown in the violin plots in Table 10. For all datasets, we ﬁnd the cosine-similarity model to
prefer shorter documents over longer ones. This is especially severe for TREC-COVID: a large
fraction of the scientiﬁc papers (approx. 42k out of 171k) consist only of publication titles without an
abstract. The cosine-similarity model prefers retrieving these documents. In contrast, the dot-product
model primarily retrieves longer documents, i.e., publications with an abstract. Cosine-similarity
uses vectors of unit length, thereby having no notion of the encoded text length. In contrast, for
dot-product, longer documents result in vectors with higher magnitudes which can yield higher
similarity scores for a query.
Further, as we observe in Figure 5, relevance judgement scores are not uniformly distributed over
document lengths: for some datasets, longer documents are annotated with higher relevancy scores,
while in others, shorter documents are. This can be either due to the annotation process, e.g., the
candidate selection method prefers short or long documents, or due to the task itself, where shorter
or longer documents could be more relevant to the user information need. Hence, it can be more
advantageous to train a model with either cosine-similarity or dot-product depending upon the nature
and needs of the speciﬁc task.
21



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 23):

Dataset LanguageKomninos LASER2 LaBSE MiniLM-L12-multilingual MPNet-multilingual SGPT-BLOOM-7.1B-msmarco
STS17 ko-ko 2.54 70.52 71.32 77.03 83.41 66.89
STS17 ar-ar 13.78 67.47 69.07 79.16 79.10 76.42
STS17 en-ar 9.08 65.05 74.51 81.22 80.85 78.07
STS17 en-de -3.11 66.66 73.85 84.22 83.28 59.10
STS17 en-tr -0.45 70.05 72.07 76.74 74.90 11.80
STS17 es-en -8.18 55.30 65.71 84.44 86.11 78.22
STS17 es-es 48.23 79.67 80.83 85.56 85.14 86.00
STS17 fr-en 5.81 70.82 76.98 76.59 81.17 80.46
STS17 it-en 3.64 70.98 76.99 82.35 84.24 51.58
STS17 nl-en -0.44 68.12 75.22 81.71 82.51 45.85
STS22 de 33.04 25.69 48.58 44.64 46.70 30.05
STS22 es 48.53 54.92 63.18 56.56 59.91 65.41
STS22 pl 12.47 18.34 39.30 33.74 33.65 31.13
STS22 tr 47.38 36.97 58.15 53.39 56.30 47.14
STS22 ar 32.42 42.57 57.67 46.2 52.19 58.67
STS22 ru 19.44 39.24 57.49 57.08 58.74 43.36
STS22 zh 4.78 49.41 63.02 58.75 61.75 66.78
STS22 fr 49.43 58.61 77.95 70.55 74.30 80.38
STS22 de-en 28.65 32.35 50.14 52.65 50.81 51.16
STS22 es-en 26.97 54.34 71.86 67.33 70.26 75.06
STS22 it 57.77 60.31 72.22 55.22 60.65 65.65
STS22 pl-en 45.55 53.63 69.41 69.02 73.07 53.31
STS22 zh-en 14.05 46.19 64.02 65.71 67.96 68.45
STS22 es-it 41.10 42.21 69.69 47.67 53.70 65.50
STS22 de-fr 14.77 37.41 53.28 51.73 62.34 53.28
STS22 de-pl 11.21 15.67 58.69 44.22 40.53 43.05
STS22 fr-pl 39.44 39.44 61.98 50.71 84.52 28.17
Average mix 22.14 51.55 65.67 64.23 67.71 57.81
Table 14: Multilingual STS Results. Scores are Spearman correlations of cosine similarities.



Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 6):

GTR-FT GTR-PT GTR
Fine-tuning   
NDCG@10 on MS Marco
Base 0.400 0.258 0.420
Large 0.415 0.262 0.430
XL 0.418 0.259 0.439
XXL 0.422 0.252 0.442
Zero-shot average NDCG@10 w/o MS Marco
Base 0.387 0.295 0.416
Large 0.412 0.315 0.445
XL 0.433 0.315 0.453
XXL 0.430 0.332 0.458
Table 5: Comparisons (NDCG@10) of the models
trained with and without pre-training and ﬁne-tuning.
Notably, the GTR-FT XL model already achieves an
average zero-shot NDCG@10 of 0.433, which outper-
forms the previous best dual encoder model TAS-B
(NDCG@10=0.415).
6.1 Effect of scaling up for different training
stages
The ﬁrst ablation study aims to investigate how
scaling up effects dual encoder pre-training and
ﬁne-tuning. Results are listed in table 5.
For ﬁne-tuning only models, scaling up beneﬁts
both in-domain and out-of-domain performance.
For pre-training only models, the improvement on
in-domain performance is not obvious; meanwhile
for out-of-domain tasks, scaling up also improves
the generalization. Finally with both pre-training
and ﬁne-tuning, GTR models consistently improve
over GTR-FT models of all sizes. This shows the
power of combining scaling up and a generic pre-
training stage.
6.2 Importance of the ﬁne-tuning dataset
In table 5, we compare GTR and GTR-PT on the
BEIR benchmark to understand the importance of
ﬁne-tuning on MS Marco. The table shows that
there is a clear gap between GTR models before
and after ﬁne-tuning. The result shows the neces-
sity of leveraging a high quality dataset (e.g. search
data) to ﬁne-tune the dual encoders.
In table 6, we compare ﬁne-tuning GTR on NQ
instead of MS Marco. Compared to MS Marco,
NQ only covers Wikipedia documents and is much
smaller in size, which allows us to investigate the
performance of GTR when ﬁne-tuned on a less
generalizable dataset. In addition, ﬁne-tuning on
NQ can give us a fair comparison with DPR.
As shown in table 6, the GTR-base model ﬁne-
Model Fine-tuning dataset Zero-shot aver-
age NDCG@10
DPR NQ 0.237
GTR-Base NQ 0.360
GTR-Large NQ 0.379
GTR-XL NQ 0.407
GTR-Large MS Marco 0.445
GTR-XL MS Marco 0.453
Table 6: Comparisons of GTR models ﬁne-tuned on
MS Marco and NQ. We report the zero-shot average
NDCG@10. Scaling up improves model performance
both on NQ and MS Marco.
tuned on NQ outperforms the original DPR model,
which uses a BERT-Base model as the encoder
backbone. This demonstrates the effectiveness of
our pre-training on the Web dataset as well as the
hard negatives introduced from Lu et al. (2021)
for NQ. Fine-tuning on NQ leads to inferior per-
formance compared to ﬁne-tuning on MS Marco,
which is consistent with prior work (Thakur et al.,
2021). However, importantly, scaling up GTR size
improves zero-shot performance on BEIR when
ﬁne-tuning on NQ. This shows that the beneﬁt of
scaling up holds for different ﬁne-tuning datasets.
Furthermore, when scaling from Large to XL, we
observe a larger gain when ﬁne-tuning with NQ
than with MS Marco, indicating that scaling up
helps more when using weaker ﬁne-tuning data.
6.3 Document length vs model capacity
Previously, BEIR has shown that models trained
with cosine similarity prefer short documents while
those trained with dot-product prefer long docu-
ments (Thakur et al., 2021). We investigate whether
scaling up affect this observation. Speciﬁcally, we
compute the median lengths (in words) of the top-
10 retrieved documents for all queries. Results are
shown in ﬁg. 5.
Though all GTR models are trained using co-
sine similarity, we found that scaling up the model
size has inﬂuence over the lengths of retrieved
documents. We observe an increasing trend of
document length for DB-Pedia, Fever, HotpotQA,
Signal-1M, Trec-News, and Web-Touche2020 with
scaling up. In particular, for Web-Touche2020, the
lengths of the retrieved documents grow drastically
as the models scale up: The largest GTR-XXL
retrieves documents that are on average twice as
long compared with the smallest GTR-Base. This
plays in our favor since Thakur et al. (2021) show
that the majority of relevant documents in Web-



Source: data\tc18_2309.07597v5\referenced_papers\[39]_2108.08877.pdf (Page 6):

Model Fine-tune data MR CR SUBJ MPQA SST TREC MRPC Avg
ST5-Enc mean (Large) N/A 89.13 92.69 97.06 90.70 92.92 93.60 73.74 89.98
ST5-Enc mean (3B) N/A 90.35 92.77 97.43 90.15 93.85 95.60 72.70 90.41
ST5-Enc mean (11B) N/A 91.15 93.33 97.55 90.20 94.07 94.40 74.26 90.71
SBERT-NLI Large♣ NLI+MNLI 84.88 90.07 94.52 90.33 90.66 87.40 75.94 87.69
SimCSE-RoBERTa Large♣ NLI 88.12 92.37 95.11 90.49 92.75 91.80 76.64 89.61
ST5-Enc mean (Large) NLI 88.82 93.43 95.73 91.75 93.08 94.00 76.35 90.45
ST5-EncDec ﬁrst (Large) NLI 87.63 92.85 94.32 91.37 91.98 93.00 76.99 89.73
ST5-Enc mean (3B) NLI 89.92 93.27 96.19 91.54 94.18 94.20 76.87 90.88
ST5-EncDec ﬁrst (3B) NLI 87.83 92.85 94.75 91.01 93.14 93.60 78.26 90.21
ST5-Enc mean (11B) NLI 90.13 93.85 96.02 91.39 93.96 95.20 76.99 91.08
ST5-EncDec ﬁrst (11B) NLI 90.00 93.94 95.01 91.53 93.85 92.20 76.70 90.46
ST5-Enc mean (Large) CommQA+NLI 88.89 93.46 95.38 91.50 94.23 96.20 77.10 90.97
ST5-Enc mean (3B) CommQA+NLI 89.94 94.09 95.85 91.58 94.84 96.20 77.86 91.48
ST5-Enc mean (11B) CommQA+NLI 90.83 94.44 96.33 91.68 94.84 95.40 77.91 91.63
Model Fine-tune data STS12 STS13 STS14 STS15 STS16 STSb SICK-R Avg
ST5-Enc mean (Large) N/A 28.01 52.60 41.35 61.28 63.58 56.31 59.48 51.80
ST5-Enc mean (3B) N/A 24.89 51.49 41.09 61.37 64.51 52.57 59.99 50.85
ST5-Enc mean (11B) N/A 34.97 60.19 47.59 66.40 70.62 62.83 63.57 58.02
SBERT-NLI Large♣ NLI+MNLI 72.27 78.46 74.90 80.99 76.25 79.23 73.75 76.55
SimCSE-RoBERTa Large♣ NLI 77.46 87.27 82.36 86.66 83.93 86.70 81.95 83.76
ST5-Enc mean (Large) NLI 76.52 85.75 81.01 87.13 83.26 85.45 79.85 82.71
ST5-EncDec ﬁrst (Large) NLI 79.15 87.42 83.61 87.64 83.92 86.35 80.64 84.11
ST5-Enc mean (3B) NLI 77.13 86.73 82.53 87.36 84.51 85.71 81.39 83.62
ST5-EncDec ﬁrst (3B) NLI 79.24 87.80 83.95 87.75 84.60 86.62 80.91 84.41
ST5-Enc mean (11B) NLI 77.42 87.50 82.51 87.47 84.88 85.61 80.77 83.74
ST5-EncDec ﬁrst (11B) NLI 80.11 88.78 84.33 88.36 85.55 86.82 80.60 84.94
ST5-Enc mean (Large) CommQA+NLI 79.10 87.32 83.17 88.27 84.36 86.73 79.84 84.11
ST5-Enc mean (3B) CommQA+NLI 79.02 88.80 84.33 88.89 85.31 86.25 79.51 84.59
ST5-Enc mean (11B) CommQA+NLI 80.10 88.75 84.70 88.86 85.17 86.77 80.39 84.96
Table 5: Comparisons of models’ performance on SentEval benchmark when scaling up model size. ♣ results are
from (Gao et al., 2021). The ﬁrst set of results are for the transfer task; the second set are for the similarity task.
7.2 Improving the ST5 Fine-tuning
As shown in table 5, we ﬁnd that scaling up model
capacity leads to consistently better performance
on all downstream tasks. For the ST5 11B model,
the encoder-only model achieves an average score
of 91.08 for transfer tasks which is better than 90.45
from the ST5 Large model; while the encoder-
decoder model pushes the STS score to 84.94 that
also outperforms the ST5 Large model. This in-
spires us to explore even larger model sizes to
achieve better sentence embedding quality.
For STS tasks, we observe that the gain from in-
creasing model size from 3B to 11B is smaller than
that from Large to 3B. This might be due to the fact
that the embedding sizes are ﬁxed for all models
in our experiments. One potential exploration is
to increase the sentence embedding size for larger
models to fully leverage the model capacity.
We further compute the alignment loss and uni-
formity loss as deﬁned in Wang and Isola (2020) to
measure the quality of the sentence embeddings:
Lalign = − E
v,v+∼ppos
∥f(v) −f(v+)∥ (3)
Luniform = log E
v,wi.i.d∼pdata
e−2∥f(v)−f(w)∥, (4)
where ppos is all positive data and pdata is the data
distribution. Lalign denotes the expected distance
between embeddings of the positive pairs of data,
while Luniform indicates how uniformly the em-
beddings are distributed. For both losses, lower
numbers indicate better performance. As shown in
ﬁg. 4, when models scale up, both the encoder and
encoder-decoder models decrease the uniformity
loss with only a slight increase in alignment loss.
We seek to investigate whether the effects of
larger model size and more training data are addi-
tive for better sentence embeddings. As shown in
the last two rows of table 5, when scaling up to
Large and 3B parameters, ST5 further improves on
downstream tasks by training on the Community
QA dataset in addition to NLI.



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 19):

Glove
Komninos
BERT
SimCSE-BERT-unsup
SimCSE-BERT-sup
coCondenser-msmarco
Contriever
SPECTER
LaBSE
LASER2
MiniLM-L6
MiniLM-L12
MiniLM-L12-multilingual
MPNet
MPNet-multilingual
SGPT-125M-nli
SGPT-5.8B-nli
SGPT-125M-msmarco
SGPT-1.3B-msmarco
SGPT-2.7B-msmarco
SGPT-5.8B-msmarco
SGPT-BLOOM-7.1B-msmarco
GTR-Base
GTR-Large
GTR-XL
GTR-XXL
ST5-Base
ST5-Large
ST5-XL
ST5-XXL
Glove
Komninos
BERT
SimCSE-BERT-unsup
SimCSE-BERT-sup
coCondenser-msmarco
Contriever
SPECTER
LaBSE
LASER2
MiniLM-L6
MiniLM-L12
MiniLM-L12-multilingual
MPNet
MPNet-multilingual
SGPT-125M-nli
SGPT-5.8B-nli
SGPT-125M-msmarco
SGPT-1.3B-msmarco
SGPT-2.7B-msmarco
SGPT-5.8B-msmarco
SGPT-BLOOM-7.1B-msmarco
GTR-Base
GTR-Large
GTR-XL
GTR-XXL
ST5-Base
ST5-Large
ST5-XL
ST5-XXL
98
90 88
97 95 93
95 94 92 99
95 93 85 95 94
90 89 79 91 90 98
93 92 89 92 91 90 84
96 94 92 98 97 95 91 94
94 94 91 97 96 90 85 90 97
91 90 78 91 90 97 97 88 92 87
90 88 77 91 90 97 97 87 91 86 100
95 93 85 96 96 98 96 91 95 92 97 97
89 88 78 90 90 96 96 87 90 86 99 99 96
94 93 85 97 96 98 97 89 95 93 97 97 99 95
97 96 91 99 98 95 91 94 97 96 92 91 97 91 96
96 95 89 98 98 97 94 91 96 94 94 94 97 93 98 99
92 90 78 91 89 96 96 87 89 85 95 96 96 94 95 93 95
89 87 75 88 87 96 97 83 87 82 96 96 94 94 95 89 94 99
87 86 73 87 85 95 97 81 85 80 95 96 94 94 94 88 93 98 100
84 82 69 83 81 92 95 78 81 77 93 94 91 92 91 84 90 97 99 99
84 83 69 83 81 92 94 78 82 78 94 95 91 93 91 84 90 97 98 99 100
87 85 74 88 87 96 98 79 87 83 96 97 95 95 95 88 92 96 97 97 96 95
85 83 72 87 86 95 98 76 84 80 95 96 93 95 94 86 90 94 97 97 95 94 100
85 83 72 86 85 95 97 76 84 79 95 96 93 95 94 85 90 94 97 97 95 95 100 100
84 82 71 85 85 94 97 75 84 79 95 96 92 95 93 84 89 93 96 96 94 94 99 100 100
94 92 87 97 97 96 93 88 95 93 94 93 96 94 97 97 97 91 90 89 85 85 92 91 91 91
92 90 85 95 96 94 93 86 93 91 93 93 95 94 96 95 97 90 90 89 86 86 92 92 92 92 99
92 90 84 94 95 94 93 85 92 90 93 93 94 94 95 94 96 91 91 90 87 87 93 93 93 92 99 100
90 88 81 92 93 94 94 83 90 87 93 94 94 95 95 92 96 92 93 92 90 90 95 95 95 94 97 99 99
 70
75
80
85
90
95
100
(a) Model correlation based on all results
Class. Clust. PairClass. Rerank. Retr. STS Summ.
Class.Clust.PairClass.Rerank.Retr.STSSumm.
68
72 81
58 95 83
57 85 87 90
79 75 85 78 69
3 -4 8 -8 -16 0
0
20
40
60
80
100 (b) Task correlation based on average task results
Figure 6: Pearson correlations across model and task results. Left: Size variants of the same architecture show
high correlations. Right: Performance on clustering and reranking correlates strongest, while summarization and
classiﬁcation show weaker correlation with other tasks.
Model Public Checkpoint
Glove https://huggingface.co/sentence-transformers/average_word_embeddings_glove.6B.300d
Komninos https://huggingface.co/sentence-transformers/average_word_embeddings_komninos
BERT https://huggingface.co/bert-base-uncased
SimCSE-BERT-unsup https://huggingface.co/princeton-nlp/unsup-simcse-bert-base-uncased
SimCSE-BERT-sup https://huggingface.co/princeton-nlp/sup-simcse-bert-base-uncased
coCondenser-msmarco https://huggingface.co/sentence-transformers/msmarco-bert-co-condensor
Contriever https://huggingface.co/nthakur/contriever-base-msmarco
SPECTER https://huggingface.co/sentence-transformers/allenai-specter
LaBSE https://huggingface.co/sentence-transformers/LaBSE
LASER2 https://github.com/facebookresearch/LASER
MiniLM-L6 https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
MiniLM-L12 https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2
MiniLM-L12-multilingual https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
MPNet https://huggingface.co/sentence-transformers/all-mpnet-base-v2
MPNet-multilingual https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2
MiniLM-L12-multilingual https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
SGPT-125M-nli https://huggingface.co/Muennighoff/SGPT-125M-weightedmean-nli-bitfit
SGPT-5.8B-nli https://huggingface.co/Muennighoff/SGPT-5.8B-weightedmean-nli-bitfit
SGPT-125M-msmarco https://huggingface.co/Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit
SGPT-1.3B-msmarco https://huggingface.co/Muennighoff/SGPT-1.3B-weightedmean-msmarco-specb-bitfit
SGPT-2.7B-msmarco https://huggingface.co/Muennighoff/SGPT-2.7B-weightedmean-msmarco-specb-bitfit
SGPT-5.8B-msmarco https://huggingface.co/Muennighoff/SGPT-5.8B-weightedmean-msmarco-specb-bitfit
SGPT-BLOOM-7.1B-msmarco https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco
SGPT-BLOOM-1.7B-nli https://huggingface.co/bigscience-data/sgpt-bloom-1b7-nli
GTR-Base https://huggingface.co/sentence-transformers/gtr-t5-base
GTR-Large https://huggingface.co/sentence-transformers/gtr-t5-large
GTR-XL https://huggingface.co/sentence-transformers/gtr-t5-xl
GTR-XXL https://huggingface.co/sentence-transformers/gtr-t5-xxl
ST5-Base https://huggingface.co/sentence-transformers/sentence-t5-base
ST5-Large https://huggingface.co/sentence-transformers/sentence-t5-large
ST5-XL https://huggingface.co/sentence-transformers/sentence-t5-xl
ST5-XXL https://huggingface.co/sentence-transformers/sentence-t5-xxl
Table 10: Publicly available model links used for evaluation



### Claim 30/38

#### Claim Text
This photon in turn can ionize an oxygen molecule somewhere else, creating an additional electron : N∗ 2 → N2 + hν O2 + hν → O+ 2 + e, (3) where hν represents the photon energy.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[15]_1810.04805.pdf (Page 3):

Input/Output Representations To make BERT
handle a variety of down-stream tasks, our input
representation is able to unambiguously represent
both a single sentence and a pair of sentences
(e.g., ⟨Question, Answer ⟩) in one token sequence.
Throughout this work, a “sentence” can be an arbi-
trary span of contiguous text, rather than an actual
linguistic sentence. A “sequence” refers to the in-
put token sequence to BERT, which may be a sin-
gle sentence or two sentences packed together.
We use WordPiece embeddings (Wu et al.,
2016) with a 30,000 token vocabulary. The ﬁrst
token of every sequence is always a special clas-
siﬁcation token ( [CLS]). The ﬁnal hidden state
corresponding to this token is used as the ag-
gregate sequence representation for classiﬁcation
tasks. Sentence pairs are packed together into a
single sequence. We differentiate the sentences in
two ways. First, we separate them with a special
token ([SEP]). Second, we add a learned embed-
ding to every token indicating whether it belongs
to sentence A or sentence B. As shown in Figure 1,
we denote input embedding as E, the ﬁnal hidden
vector of the special [CLS] token as C ∈RH,
and the ﬁnal hidden vector for the ith input token
as Ti ∈RH.
For a given token, its input representation is
constructed by summing the corresponding token,
segment, and position embeddings. A visualiza-
tion of this construction can be seen in Figure 2.
3.1 Pre-training BERT
Unlike Peters et al. (2018a) and Radford et al.
(2018), we do not use traditional left-to-right or
right-to-left language models to pre-train BERT.
Instead, we pre-train BERT using two unsuper-
vised tasks, described in this section. This step
is presented in the left part of Figure 1.
Task #1: Masked LM Intuitively, it is reason-
able to believe that a deep bidirectional model is
strictly more powerful than either a left-to-right
model or the shallow concatenation of a left-to-
right and a right-to-left model. Unfortunately,
standard conditional language models can only be
trained left-to-right or right-to-left, since bidirec-
tional conditioning would allow each word to in-
directly “see itself”, and the model could trivially
predict the target word in a multi-layered context.
former is often referred to as a “Transformer encoder” while
the left-context-only version is referred to as a “Transformer
decoder” since it can be used for text generation.
In order to train a deep bidirectional representa-
tion, we simply mask some percentage of the input
tokens at random, and then predict those masked
tokens. We refer to this procedure as a “masked
LM” (MLM), although it is often referred to as a
Cloze task in the literature (Taylor, 1953). In this
case, the ﬁnal hidden vectors corresponding to the
mask tokens are fed into an output softmax over
the vocabulary, as in a standard LM. In all of our
experiments, we mask 15% of all WordPiece to-
kens in each sequence at random. In contrast to
denoising auto-encoders (Vincent et al., 2008), we
only predict the masked words rather than recon-
structing the entire input.
Although this allows us to obtain a bidirec-
tional pre-trained model, a downside is that we
are creating a mismatch between pre-training and
ﬁne-tuning, since the [MASK] token does not ap-
pear during ﬁne-tuning. To mitigate this, we do
not always replace “masked” words with the ac-
tual [MASK] token. The training data generator
chooses 15% of the token positions at random for
prediction. If the i-th token is chosen, we replace
the i-th token with (1) the [MASK] token 80% of
the time (2) a random token 10% of the time (3)
the unchanged i-th token 10% of the time. Then,
Ti will be used to predict the original token with
cross entropy loss. We compare variations of this
procedure in Appendix C.2.
Task #2: Next Sentence Prediction (NSP)
Many important downstream tasks such as Ques-
tion Answering (QA) and Natural Language Infer-
ence (NLI) are based on understanding the rela-
tionship between two sentences, which is not di-
rectly captured by language modeling. In order
to train a model that understands sentence rela-
tionships, we pre-train for a binarized next sen-
tence prediction task that can be trivially gener-
ated from any monolingual corpus. Speciﬁcally,
when choosing the sentencesA and B for each pre-
training example, 50% of the time B is the actual
next sentence that follows A (labeled as IsNext),
and 50% of the time it is a random sentence from
the corpus (labeled as NotNext). As we show
in Figure 1, C is used for next sentence predic-
tion (NSP). 5 Despite its simplicity, we demon-
strate in Section 5.1 that pre-training towards this
task is very beneﬁcial to both QA and NLI. 6
5The ﬁnal model achieves 97%-98% accuracy on NSP.
6The vector C is not a meaningful sentence representation
without ﬁne-tuning, since it was trained with NSP.



Source: data\tc18_2309.07597v5\referenced_papers\[62]_2004.05986.pdf (Page 13):

TNEWS
sentence: 如果我的世界下架了，你会玩迷你世界吗？
sentence (en): If Minecraft is gone, will you play miniworld?
label: 116(news game)
iFLYTEK
sentence: 《钢铁英雄》是一款角色扮演类游戏。游戏拥有 ...... 带领他们逃出去。修复部分小错误，提升整
体稳定性。
sentence (en): ”Heroes of Steel” is a role-playing game. The game has ...... all four heroes are imprisoned and you
will lead them out. repair part small Errors to improve overall stability.
label: 22(Strategy)
CLUEWSC
text: 这时候放在床上枕头旁边的手机响了，我感到奇怪，因为欠费已被停机两个月，现在它突然响了。
text (en): At this moment, the cellphone on the bed next to the pillow rang. I feel this is quite strange because the
cellphone plan was terminated two months ago since I did not pay the bill. Now it was ringing all of a sudden.
label: true
AFQMC
sentence1: 本月花呗还不上怎么办 sentence2: 花呗超时怎么办
sentence1 (en): What to do if Ant Credit Pay is not available yet this month sentence2 (en): How to deal with Ant
Credit Pay overtime
label: 0(different)
CSL
abst: 不同阶段电子数据的操作都会留下表现各异的轨迹.从操作系统、计算机应用系统 ...... 分析审计电子数
据轨迹在计算机系统中表现形式,可以为审计人员提供有效的审计方法
keyword: [“计算机审计”, “数据轨迹”, “日志文件”]
abst (en): The operation of electronic data in different stages will leave different traces. From operating system,
computer application system ...... provide effective audit methods for auditors by analyzing the expression of audit
electronic data trace in computer system.
keyword (en): [“computer audit”, “data trace”, “log ﬁle”]
label: 0(false)
OCNLI
premise: 但是不光是中国,日本,整个东亚文化都有这个特点就是被权力影响很深 hypothesis: 有超过两个东
亚国家有这个特点
premise (en): But not only China and Japan, the entire East Asian culture has this feature, that is it is deeply inﬂuenced
by the power. hypothesis (en): More than two East Asian countries have this feature.
label: entailment
CMRC 2018
context: 萤火虫工作室是一家总部设在英国伦敦和康涅狄格州坎顿...... 目前，他们正在开发PC和Xbox360上
的次时代游戏。
question: 萤火虫工作室的总部设在哪里？ answer: 英国伦敦和康涅狄格州坎顿
context (en): Fireﬂy Studios is a video game developer based in London, UK and Canton, Connecticut, with a quality
department in Aberdeen, Scotland ...... Currently, they are developing next-generation games on PC and Xbox 360.
question (en): Where is Fireﬂy Studios headquartered? answer (en): London, UK and Canton, Connecticut
ChID
content: 中国青年报：篮协改革联赛切莫#idiom#......
candidates: [“急功近利”, “画蛇添足”, “本末倒置”(answer)]
content (en): China Youth Daily: Chinese Basketball Association should not #idiom# when reforming the league ......
candidates (en): [“seeking instant beneﬁt”, “to overdo it”, “take the branch for the root ”(answer)]
C3 document: 男：我们坐在第七排，应该能看清楚字幕吧? 女：肯定可以，对了，我们得把手机设成振动。
question: 他们最可能在哪儿?
candidates: [“图书馆”, “体育馆”,“电影院”(answer),“火车站”]
document (en): Man: Our seats are in the seventh row. We should be able to see the subtitles clearly, right? Woman:
Absolutely. By the way, we should set the phone to vibrate.
question (en): Where does the conversation most probably take place?
candidates (en): [“In a library”, “In a stadium”,“In a cinema ”(answer),“At a train station”]
Table 5: Development set examples from the tasks in CLUE. Bold text represents part of the example
format for each task. Chinese text is part of the model input, and the corresponding text in italics is
the English version translated from that. Underlined text is specially marked in the input. Text in a
monospaced font represents the expected model output.



Source: data\tc18_2309.07597v5\referenced_papers\[25]_2004.04906.pdf (Page 12):

Question Passage received by BM25 Passage retrieved by DPR
What is the body of water
between England and Ireland?
Title:British Cycling Title: Irish Sea
. . .England is not recognised as a region by the UCI, and
there is no English cycling team outside the Commonwealth
Games. For those occasions, British Cycling selects and sup-
ports the England team. Cycling is represented on the Isle
of Man by the Isle of Man Cycling Association. Cycling in
Northern Ireland is organised under Cycling Ulster, part of
the all-Ireland governing body Cycling Ireland. Until 2006,
a rival governing body existed, . . .
. . . Annual trafﬁc between Great Britain andIreland amounts
to over 12 million passengers and of traded goods. The Irish
Sea is connected to the North Atlantic at both its northern
and southern ends. To the north, the connection is through
the North Channel between Scotland and Northern Ireland
and the Malin Sea. The southern end is linked to the Atlantic
through the St George’s Channel between Ireland and Pem-
brokeshire, and the Celtic Sea. . . .
Who plays Thoros of Myr in
Game of Thrones?
Title: No One (Game of Thrones) Title: P˚al Sverre Hagen
. . . He may be ”no one,” but there’s still enough of a person
left in him to respect, and admire who this girl is and what
she’s become. Arya ﬁnally tells us something that we’ve kind
of known all along, that she’s not no one, she’s Arya Stark
of Winterfell.” ”No One” saw the reintroduction of Richard
Dormer and Paul Kaye, who portrayed Beric Dondarrion and
Thoros of Myr, respectively, in the third season, . . .
P˚al Sverre Valheim Hagen (born 6 November 1980) is a Nor-
wegian stage and screen actor. He appeared in the Norwe-
gian ﬁlm ”Max Manus” and played Thor Heyerdahl in the
Oscar-nominated 2012 ﬁlm ”Kon-Tiki”. Pl Hagen was born
in Stavanger, Norway, the son of Roar Hagen, a Norwegian
cartoonist who has long been associated with Norway´s largest
daily, ”VG”. He lived in Jtten, a neighborhood in the city of
Stavanger in south-western Norway. . . .
Table 7: Examples of passages returned from BM25 and DPR. Correct answers are written inblue and the content
words in the question are written in bold.
from the top 5 positive and top 30 negative passages
(from the set of top 100 passages retrieved from
the same question). The question encoder’s initial
state is taken from a DPR model previously trained
on the NQ dataset. The reader’s initial state is a
BERT-base model. In terms of the end-to-end QA
results, our joint-training scheme does not provide
better results compared to the usual retriever/reader
training pipeline, resulting in the same 39.8 exact
match score on NQ dev as in our regular reader
model training.



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 3):

Preprint
q
𝑑!
𝐷!! "#
"
Trainer
Inferencer
q
𝑑!
𝐷!! "$
"
Checkpoint k-1
…
Checkpoint k
q
𝑑!
𝐷!! "$
"
q
𝑑!
𝐷!!
"
…
Checkpoint k+1
q
𝑑!
𝐷!! "#
"
…
Inferencing
Index & 
Search
Training 
Positives
ANCE 
Negatives
Index & 
Search
Figure 2: ANCE Asynchronous Training. The Trainer learns the representation using negatives from
the ANN index. The Inferencer uses a recent checkpoint to update the representation of documents in
the corpus and once ﬁnished, refreshes the ANN index with most up-to-date encodings.
where L is the number of layers, ρis composed by pre-activation weights and gradients in inter-
mediate layers, and ||∇φLl(d+,d−)||2 is the gradient w.r.t. the last layer. Intuitively, the inter-
mediate layers are more regulated by various normalization techniques; the main moving piece is
||∇φLl(d+,d−)||2 (Katharopoulos & Fleuret, 2018).
For common learning to rank loss functions, for example, BCE loss and pairwise hinge loss, we can
veriﬁed that (Katharopoulos & Fleuret, 2018):
l(d+,d−) →0 ⇒||∇φLl(d+,d−)||2 →0 ⇒||∇θtl(d+,d−)||2 →0. (12)
Intuitively, negative samples with near zero loss have near zero gradients and contribute little to
model convergence. The convergence of dense retrieval model training relies on the informativeness
of constructed negatives.
Inefﬁcacy of Local In-Batch Negatives: We argue that the in-batch local negatives are unlikely to
provide informative samples due to two common properties of text retrieval.
Let D−∗be the set of informative negatives that are hard to distinguish from D+, and bbe the batch
size, we have (1) b≪|C|, the batch size is far smaller than the corpus size; (2) |D−∗|≪| C|, that
only a few negatives are informative and the majority of corpus is trivially unrelated.
Both conditions are easy to verify empirically in dense retrieval benchmarks. The two together make
the probability that a random mini-batch includes meaningful negatives p= b|D−∗|
|C|2 close to zero.
Selecting negatives from local training batches is unlikely to provide optimal training signals for
dense retrieval.
4 A PPROXIMATE NEAREST NEIGHBOR NOISE CONTRASTIVE ESTIMATION
Our analyses show the importance, if not necessity, to construct negativesglobally from the corpus.
In this section, we propose Approximate nearest neighbor Negative Contrastive Estimation, (ANCE),
which selects negatives from the entire corpus using an asynchronously updated ANN index.
ANCE samples negatives from the top retrieved documents via the DR model from the ANN index:
θ∗= argminθ
∑
q
∑
d+∈D+
∑
d−∈D−
ANCE
l(f(q,d+),f(q,d−)), (13)
with D−
ANCE = ANNf(q,d) \D+ and ANNf(q,d) the top retrieved documents by f() from the ANN
index. By deﬁnition, D−
ANCE are the hardest negatives for the current DR model: D−
ANCE ≈D−∗.
In theory, these more informative negatives have higher training loss, higher upper bound on the
gradient norms, and will improve training convergence.
ANCE can be used to train any dense retrieval model. For simplicity, we use a simple set up in recent
research (Luan et al., 2020) with BERT Siamese/Dual Encoder (shared between qand d), dot product
similarity, and negative log likelihood (NLL) loss.
4



Source: data\tc18_2309.07597v5\referenced_papers\[25]_2004.04906.pdf (Page 2):

At run-time, DPR applies a different encoderEQ(·)
that maps the input question to a d-dimensional
vector, and retrieves kpassages of which vectors
are the closest to the question vector. We deﬁne
the similarity between the question and the passage
using the dot product of their vectors:
sim(q,p) =EQ(q)⊺EP (p). (1)
Although more expressive model forms for measur-
ing the similarity between a question and a passage
do exist, such as networks consisting of multiple
layers of cross attentions, the similarity function
needs to be decomposable so that the represen-
tations of the collection of passages can be pre-
computed. Most decomposable similarity functions
are some transformations of Euclidean distance
(L2). For instance, cosine is equivalent to inner
product for unit vectors and the Mahalanobis dis-
tance is equivalent to L2 distance in a transformed
space. Inner product search has been widely used
and studied, as well as its connection to cosine
similarity and L2 distance (Mussmann and Ermon,
2016; Ram and Gray, 2012). As our ablation study
ﬁnds other similarity functions perform compara-
bly (Section 5.2; Appendix B), we thus choose
the simpler inner product function and improve the
dense passage retriever by learning better encoders.
Encoders Although in principle the question and
passage encoders can be implemented by any neu-
ral networks, in this work we use two independent
BERT (Devlin et al., 2019) networks (base, un-
cased) and take the representation at the [CLS]
token as the output, so d= 768.
Inference During inference time, we apply the
passage encoder EP to all the passages and index
them using FAISS (Johnson et al., 2017) ofﬂine.
FAISS is an extremely efﬁcient, open-source li-
brary for similarity search and clustering of dense
vectors, which can easily be applied to billions of
vectors. Given a question qat run-time, we derive
its embedding vq = EQ(q) and retrieve the top k
passages with embeddings closest to vq.
3.2 Training
Training the encoders so that the dot-product sim-
ilarity (Eq. (1)) becomes a good ranking function
for retrieval is essentially a metric learning prob-
lem (Kulis, 2013). The goal is to create a vector
space such that relevant pairs of questions and pas-
sages will have smaller distance (i.e., higher simi-
larity) than the irrelevant ones, by learning a better
embedding function.
Let D = {⟨qi,p+
i ,p−
i,1,··· ,p−
i,n⟩}m
i=1 be the
training data that consists of m instances. Each
instance contains one question qi and one relevant
(positive) passage p+
i , along with nirrelevant (neg-
ative) passages p−
i,j. We optimize the loss function
as the negative log likelihood of the positive pas-
sage:
L(qi,p+
i ,p−
i,1,··· ,p−
i,n) (2)
= −log esim(qi,p+
i )
esim(qi,p+
i ) + ∑n
j=1 esim(qi,p−
i,j) .
Positive and negative passages For retrieval
problems, it is often the case that positive examples
are available explicitly, while negative examples
need to be selected from an extremely large pool.
For instance, passages relevant to a question may
be given in a QA dataset, or can be found using the
answer. All other passages in the collection, while
not speciﬁed explicitly, can be viewed as irrelevant
by default. In practice, how to select negative ex-
amples is often overlooked but could be decisive
for learning a high-quality encoder. We consider
three different types of negatives: (1) Random: any
random passage from the corpus; (2) BM25: top
passages returned by BM25 which don’t contain
the answer but match most question tokens; (3)
Gold: positive passages paired with other questions
which appear in the training set. We will discuss the
impact of different types of negative passages and
training schemes in Section 5.2. Our best model
uses gold passages from the same mini-batch and
one BM25 negative passage. In particular, re-using
gold passages from the same batch as negatives
can make the computation efﬁcient while achiev-
ing great performance. We discuss this approach
below.
In-batch negatives Assume that we have B
questions in a mini-batch and each one is asso-
ciated with a relevant passage. Let Q and P be the
(B×d) matrix of question and passage embeddings
in a batch of size B. S = QPT is a (B×B) ma-
trix of similarity scores, where each row of which
corresponds to a question, paired with Bpassages.
In this way, we reuse computation and effectively
train on B2 (qi, pj) question/passage pairs in each
batch. Any (qi, pj) pair is a positive example when
i= j, and negative otherwise. This creates Btrain-
ing instances in each batch, where there are B−1



### Claim 31/38

#### Claim Text
We observe a typical known behavior in flagella : by 15 increasing the parameter Sp, the traveling wave velocity increases.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[15]_1810.04805.pdf (Page 7):

Dev Set
Tasks MNLI-m QNLI MRPC SST-2 SQuAD
(Acc) (Acc) (Acc) (Acc) (F1)
BERTBASE 84.4 88.4 86.7 92.7 88.5
No NSP 83.9 84.9 86.5 92.6 87.9
LTR & No NSP 82.1 84.3 77.5 92.1 77.8
+ BiLSTM 82.1 84.1 75.7 91.6 84.9
Table 5: Ablation over the pre-training tasks using the
BERTBASE architecture. “No NSP” is trained without
the next sentence prediction task. “LTR & No NSP” is
trained as a left-to-right LM without the next sentence
prediction, like OpenAI GPT. “+ BiLSTM” adds a ran-
domly initialized BiLSTM on top of the “LTR + No
NSP” model during ﬁne-tuning.
ablation studies can be found in Appendix C.
5.1 Effect of Pre-training Tasks
We demonstrate the importance of the deep bidi-
rectionality of BERT by evaluating two pre-
training objectives using exactly the same pre-
training data, ﬁne-tuning scheme, and hyperpa-
rameters as BERTBASE :
No NSP: A bidirectional model which is trained
using the “masked LM” (MLM) but without the
“next sentence prediction” (NSP) task.
LTR & No NSP: A left-context-only model which
is trained using a standard Left-to-Right (LTR)
LM, rather than an MLM. The left-only constraint
was also applied at ﬁne-tuning, because removing
it introduced a pre-train/ﬁne-tune mismatch that
degraded downstream performance. Additionally,
this model was pre-trained without the NSP task.
This is directly comparable to OpenAI GPT, but
using our larger training dataset, our input repre-
sentation, and our ﬁne-tuning scheme.
We ﬁrst examine the impact brought by the NSP
task. In Table 5, we show that removing NSP
hurts performance signiﬁcantly on QNLI, MNLI,
and SQuAD 1.1. Next, we evaluate the impact
of training bidirectional representations by com-
paring “No NSP” to “LTR & No NSP”. The LTR
model performs worse than the MLM model on all
tasks, with large drops on MRPC and SQuAD.
For SQuAD it is intuitively clear that a LTR
model will perform poorly at token predictions,
since the token-level hidden states have no right-
side context. In order to make a good faith at-
tempt at strengthening the LTR system, we added
a randomly initialized BiLSTM on top. This does
signiﬁcantly improve results on SQuAD, but the
results are still far worse than those of the pre-
trained bidirectional models. The BiLSTM hurts
performance on the GLUE tasks.
We recognize that it would also be possible to
train separate LTR and RTL models and represent
each token as the concatenation of the two mod-
els, as ELMo does. However: (a) this is twice as
expensive as a single bidirectional model; (b) this
is non-intuitive for tasks like QA, since the RTL
model would not be able to condition the answer
on the question; (c) this it is strictly less powerful
than a deep bidirectional model, since it can use
both left and right context at every layer.
5.2 Effect of Model Size
In this section, we explore the effect of model size
on ﬁne-tuning task accuracy. We trained a number
of BERT models with a differing number of layers,
hidden units, and attention heads, while otherwise
using the same hyperparameters and training pro-
cedure as described previously.
Results on selected GLUE tasks are shown in
Table 6. In this table, we report the average Dev
Set accuracy from 5 random restarts of ﬁne-tuning.
We can see that larger models lead to a strict ac-
curacy improvement across all four datasets, even
for MRPC which only has 3,600 labeled train-
ing examples, and is substantially different from
the pre-training tasks. It is also perhaps surpris-
ing that we are able to achieve such signiﬁcant
improvements on top of models which are al-
ready quite large relative to the existing literature.
For example, the largest Transformer explored in
Vaswani et al. (2017) is (L=6, H=1024, A=16)
with 100M parameters for the encoder, and the
largest Transformer we have found in the literature
is (L=64, H=512, A=2) with 235M parameters
(Al-Rfou et al., 2018). By contrast, BERT BASE
contains 110M parameters and BERT LARGE con-
tains 340M parameters.
It has long been known that increasing the
model size will lead to continual improvements
on large-scale tasks such as machine translation
and language modeling, which is demonstrated
by the LM perplexity of held-out training data
shown in Table 6. However, we believe that
this is the ﬁrst work to demonstrate convinc-
ingly that scaling to extreme model sizes also
leads to large improvements on very small scale
tasks, provided that the model has been sufﬁ-
ciently pre-trained. Peters et al. (2018b) presented



Source: data\tc18_2309.07597v5\referenced_papers\[39]_2108.08877.pdf (Page 6):

Model Fine-tune data MR CR SUBJ MPQA SST TREC MRPC Avg
ST5-Enc mean (Large) N/A 89.13 92.69 97.06 90.70 92.92 93.60 73.74 89.98
ST5-Enc mean (3B) N/A 90.35 92.77 97.43 90.15 93.85 95.60 72.70 90.41
ST5-Enc mean (11B) N/A 91.15 93.33 97.55 90.20 94.07 94.40 74.26 90.71
SBERT-NLI Large♣ NLI+MNLI 84.88 90.07 94.52 90.33 90.66 87.40 75.94 87.69
SimCSE-RoBERTa Large♣ NLI 88.12 92.37 95.11 90.49 92.75 91.80 76.64 89.61
ST5-Enc mean (Large) NLI 88.82 93.43 95.73 91.75 93.08 94.00 76.35 90.45
ST5-EncDec ﬁrst (Large) NLI 87.63 92.85 94.32 91.37 91.98 93.00 76.99 89.73
ST5-Enc mean (3B) NLI 89.92 93.27 96.19 91.54 94.18 94.20 76.87 90.88
ST5-EncDec ﬁrst (3B) NLI 87.83 92.85 94.75 91.01 93.14 93.60 78.26 90.21
ST5-Enc mean (11B) NLI 90.13 93.85 96.02 91.39 93.96 95.20 76.99 91.08
ST5-EncDec ﬁrst (11B) NLI 90.00 93.94 95.01 91.53 93.85 92.20 76.70 90.46
ST5-Enc mean (Large) CommQA+NLI 88.89 93.46 95.38 91.50 94.23 96.20 77.10 90.97
ST5-Enc mean (3B) CommQA+NLI 89.94 94.09 95.85 91.58 94.84 96.20 77.86 91.48
ST5-Enc mean (11B) CommQA+NLI 90.83 94.44 96.33 91.68 94.84 95.40 77.91 91.63
Model Fine-tune data STS12 STS13 STS14 STS15 STS16 STSb SICK-R Avg
ST5-Enc mean (Large) N/A 28.01 52.60 41.35 61.28 63.58 56.31 59.48 51.80
ST5-Enc mean (3B) N/A 24.89 51.49 41.09 61.37 64.51 52.57 59.99 50.85
ST5-Enc mean (11B) N/A 34.97 60.19 47.59 66.40 70.62 62.83 63.57 58.02
SBERT-NLI Large♣ NLI+MNLI 72.27 78.46 74.90 80.99 76.25 79.23 73.75 76.55
SimCSE-RoBERTa Large♣ NLI 77.46 87.27 82.36 86.66 83.93 86.70 81.95 83.76
ST5-Enc mean (Large) NLI 76.52 85.75 81.01 87.13 83.26 85.45 79.85 82.71
ST5-EncDec ﬁrst (Large) NLI 79.15 87.42 83.61 87.64 83.92 86.35 80.64 84.11
ST5-Enc mean (3B) NLI 77.13 86.73 82.53 87.36 84.51 85.71 81.39 83.62
ST5-EncDec ﬁrst (3B) NLI 79.24 87.80 83.95 87.75 84.60 86.62 80.91 84.41
ST5-Enc mean (11B) NLI 77.42 87.50 82.51 87.47 84.88 85.61 80.77 83.74
ST5-EncDec ﬁrst (11B) NLI 80.11 88.78 84.33 88.36 85.55 86.82 80.60 84.94
ST5-Enc mean (Large) CommQA+NLI 79.10 87.32 83.17 88.27 84.36 86.73 79.84 84.11
ST5-Enc mean (3B) CommQA+NLI 79.02 88.80 84.33 88.89 85.31 86.25 79.51 84.59
ST5-Enc mean (11B) CommQA+NLI 80.10 88.75 84.70 88.86 85.17 86.77 80.39 84.96
Table 5: Comparisons of models’ performance on SentEval benchmark when scaling up model size. ♣ results are
from (Gao et al., 2021). The ﬁrst set of results are for the transfer task; the second set are for the similarity task.
7.2 Improving the ST5 Fine-tuning
As shown in table 5, we ﬁnd that scaling up model
capacity leads to consistently better performance
on all downstream tasks. For the ST5 11B model,
the encoder-only model achieves an average score
of 91.08 for transfer tasks which is better than 90.45
from the ST5 Large model; while the encoder-
decoder model pushes the STS score to 84.94 that
also outperforms the ST5 Large model. This in-
spires us to explore even larger model sizes to
achieve better sentence embedding quality.
For STS tasks, we observe that the gain from in-
creasing model size from 3B to 11B is smaller than
that from Large to 3B. This might be due to the fact
that the embedding sizes are ﬁxed for all models
in our experiments. One potential exploration is
to increase the sentence embedding size for larger
models to fully leverage the model capacity.
We further compute the alignment loss and uni-
formity loss as deﬁned in Wang and Isola (2020) to
measure the quality of the sentence embeddings:
Lalign = − E
v,v+∼ppos
∥f(v) −f(v+)∥ (3)
Luniform = log E
v,wi.i.d∼pdata
e−2∥f(v)−f(w)∥, (4)
where ppos is all positive data and pdata is the data
distribution. Lalign denotes the expected distance
between embeddings of the positive pairs of data,
while Luniform indicates how uniformly the em-
beddings are distributed. For both losses, lower
numbers indicate better performance. As shown in
ﬁg. 4, when models scale up, both the encoder and
encoder-decoder models decrease the uniformity
loss with only a slight increase in alignment loss.
We seek to investigate whether the effects of
larger model size and more training data are addi-
tive for better sentence embeddings. As shown in
the last two rows of table 5, when scaling up to
Large and 3B parameters, ST5 further improves on
downstream tasks by training on the Community
QA dataset in addition to NLI.



Source: data\tc18_2309.07597v5\referenced_papers\[29]_1907.11692.pdf (Page 5):

bsz steps lr ppl MNLI-m SST -2
256 1M 1e-4 3.99 84.7 92.7
2K 125K 7e-4 3.68 85.2 92.9
8K 31K 1e-3 3.77 84.6 92.8
T able 3: Perplexity on held-out training data ( ppl) and
development set accuracy for base models trained over
BO O KCO RP U S and W IK IP E D IA with varying batch
sizes ( bsz). W e tune the learning rate ( lr) for each set-
ting. Models make the same number of passes over the
data (epochs) and have the same computational cost.
task performance of BERTBA S E as we increase the
batch size, controlling for the number of passes
through the training data. W e observe that train-
ing with large batches improves perplexity for the
masked language modeling objective, as well as
end-task accuracy . Large batches are also easier to
parallelize via distributed data parallel training,
8
and in later experiments we train with batches of
8K sequences.
Notably
Y ou et al. (2019) train BERT with even
larger batche sizes, up to 32K sequences. W e leave
further exploration of the limits of large batch
training to future work.
4.4 T ext Encoding
Byte-Pair Encoding (BPE) (
Sennrich et al. , 2016)
is a hybrid between character- and word-level rep-
resentations that allows handling the large vocab-
ularies common in natural language corpora. In-
stead of full words, BPE relies on subwords units,
which are extracted by performing statistical anal-
ysis of the training corpus.
BPE vocabulary sizes typically range from
10K-100K subword units. However, unicode char-
acters can account for a sizeable portion of this
vocabulary when modeling large and diverse cor-
pora, such as the ones considered in this work.
Radford et al. (2019) introduce a clever imple-
mentation of BPE that uses bytes instead of uni-
code characters as the base subword units. Using
bytes makes it possible to learn a subword vocab-
ulary of a modest size (50K units) that can still en-
code any input text without introducing any “un-
known” tokens.
8 Large batch training can improve training efﬁciency even
without large scale parallel hardware through gradient ac-
cumulation, whereby gradients from multiple mini-batches
are accumulated locally before each optimization step. Thi s
functionality is supported natively in FA I R S E Q (Ott et al. ,
2019).
The original BERT implementa-
tion ( Devlin et al. , 2019) uses a character-level
BPE vocabulary of size 30K, which is learned
after preprocessing the input with heuristic tok-
enization rules. Following
Radford et al. (2019),
we instead consider training BERT with a larger
byte-level BPE vocabulary containing 50K sub-
word units, without any additional preprocessing
or tokenization of the input. This adds approxi-
mately 15M and 20M additional parameters for
BERTBA S E and BERT L A R G E, respectively .
Early experiments revealed only slight dif-
ferences between these encodings, with the
Radford et al. (2019) BPE achieving slightly
worse end-task performance on some tasks. Nev-
ertheless, we believe the advantages of a univer-
sal encoding scheme outweighs the minor degre-
dation in performance and use this encoding in
the remainder of our experiments. A more de-
tailed comparison of these encodings is left to fu-
ture work.
5 RoBERT a
In the previous section we propose modiﬁcations
to the BERT pretraining procedure that improve
end-task performance. W e now aggregate these
improvements and evaluate their combined im-
pact. W e call this conﬁgurationRoBERT a for
R
obustly optimized BERT approach. Speciﬁ-
cally , RoBERT a is trained with dynamic mask-
ing (Section
4.1), FU LL -SEN TE N C E S without NSP
loss (Section 4.2), large mini-batches (Section 4.3)
and a larger byte-level BPE (Section 4.4).
Additionally , we investigate two other impor-
tant factors that have been under-emphasized in
previous work: (1) the data used for pretraining,
and (2) the number of training passes through the
data. For example, the recently proposed XLNet
architecture (
Y ang et al. , 2019) is pretrained us-
ing nearly 10 times more data than the original
BERT (
Devlin et al. , 2019). It is also trained with
a batch size eight times larger for half as many op-
timization steps, thus seeing four times as many
sequences in pretraining compared to BERT .
T o help disentangle the importance of these fac-
tors from other modeling choices (e.g., the pre-
training objective), we begin by training RoBERT a
following the BERTL A R G E architecture ( L = 24,
H = 1024, A = 16, 355M parameters). W e
pretrain for 100K steps over a comparable B O O K-
CO R PU S plus W IK IPED IA dataset as was used in



Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 6):

GTR-FT GTR-PT GTR
Fine-tuning   
NDCG@10 on MS Marco
Base 0.400 0.258 0.420
Large 0.415 0.262 0.430
XL 0.418 0.259 0.439
XXL 0.422 0.252 0.442
Zero-shot average NDCG@10 w/o MS Marco
Base 0.387 0.295 0.416
Large 0.412 0.315 0.445
XL 0.433 0.315 0.453
XXL 0.430 0.332 0.458
Table 5: Comparisons (NDCG@10) of the models
trained with and without pre-training and ﬁne-tuning.
Notably, the GTR-FT XL model already achieves an
average zero-shot NDCG@10 of 0.433, which outper-
forms the previous best dual encoder model TAS-B
(NDCG@10=0.415).
6.1 Effect of scaling up for different training
stages
The ﬁrst ablation study aims to investigate how
scaling up effects dual encoder pre-training and
ﬁne-tuning. Results are listed in table 5.
For ﬁne-tuning only models, scaling up beneﬁts
both in-domain and out-of-domain performance.
For pre-training only models, the improvement on
in-domain performance is not obvious; meanwhile
for out-of-domain tasks, scaling up also improves
the generalization. Finally with both pre-training
and ﬁne-tuning, GTR models consistently improve
over GTR-FT models of all sizes. This shows the
power of combining scaling up and a generic pre-
training stage.
6.2 Importance of the ﬁne-tuning dataset
In table 5, we compare GTR and GTR-PT on the
BEIR benchmark to understand the importance of
ﬁne-tuning on MS Marco. The table shows that
there is a clear gap between GTR models before
and after ﬁne-tuning. The result shows the neces-
sity of leveraging a high quality dataset (e.g. search
data) to ﬁne-tune the dual encoders.
In table 6, we compare ﬁne-tuning GTR on NQ
instead of MS Marco. Compared to MS Marco,
NQ only covers Wikipedia documents and is much
smaller in size, which allows us to investigate the
performance of GTR when ﬁne-tuned on a less
generalizable dataset. In addition, ﬁne-tuning on
NQ can give us a fair comparison with DPR.
As shown in table 6, the GTR-base model ﬁne-
Model Fine-tuning dataset Zero-shot aver-
age NDCG@10
DPR NQ 0.237
GTR-Base NQ 0.360
GTR-Large NQ 0.379
GTR-XL NQ 0.407
GTR-Large MS Marco 0.445
GTR-XL MS Marco 0.453
Table 6: Comparisons of GTR models ﬁne-tuned on
MS Marco and NQ. We report the zero-shot average
NDCG@10. Scaling up improves model performance
both on NQ and MS Marco.
tuned on NQ outperforms the original DPR model,
which uses a BERT-Base model as the encoder
backbone. This demonstrates the effectiveness of
our pre-training on the Web dataset as well as the
hard negatives introduced from Lu et al. (2021)
for NQ. Fine-tuning on NQ leads to inferior per-
formance compared to ﬁne-tuning on MS Marco,
which is consistent with prior work (Thakur et al.,
2021). However, importantly, scaling up GTR size
improves zero-shot performance on BEIR when
ﬁne-tuning on NQ. This shows that the beneﬁt of
scaling up holds for different ﬁne-tuning datasets.
Furthermore, when scaling from Large to XL, we
observe a larger gain when ﬁne-tuning with NQ
than with MS Marco, indicating that scaling up
helps more when using weaker ﬁne-tuning data.
6.3 Document length vs model capacity
Previously, BEIR has shown that models trained
with cosine similarity prefer short documents while
those trained with dot-product prefer long docu-
ments (Thakur et al., 2021). We investigate whether
scaling up affect this observation. Speciﬁcally, we
compute the median lengths (in words) of the top-
10 retrieved documents for all queries. Results are
shown in ﬁg. 5.
Though all GTR models are trained using co-
sine similarity, we found that scaling up the model
size has inﬂuence over the lengths of retrieved
documents. We observe an increasing trend of
document length for DB-Pedia, Fever, HotpotQA,
Signal-1M, Trec-News, and Web-Touche2020 with
scaling up. In particular, for Web-Touche2020, the
lengths of the retrieved documents grow drastically
as the models scale up: The largest GTR-XXL
retrieves documents that are on average twice as
long compared with the smallest GTR-Base. This
plays in our favor since Thakur et al. (2021) show
that the majority of relevant documents in Web-



Source: data\tc18_2309.07597v5\referenced_papers\[40]_2112.07899.pdf (Page 5):

0.420.430.440.44
0.23
MS MARCO
0.540.56
0.58
0.5
0.66
Trec-Covid
0.27
0.320.320.32
0.46
BioASQ
0.31
0.33
0.340.34
0.32
NFCorpus
0.5
0.550.560.57
0.33
NQ
0.54
0.580.590.6 0.6
HotpotQA
0.35
0.420.44
0.47
0.24
FiQA-2018
0.260.260.270.27
0.33
Signal-1M
0.340.340.350.35
0.4
Trec-News
0.44
0.470.48
0.51
0.41
Robust04
0.510.520.530.54
0.32
ArguAna
0.2 0.220.23
0.26
0.37
Touché-2020
0.880.890.890.89
0.79
Quora
0.35
0.390.4 0.41
0.31
DBPedia-entity
0.15
0.160.160.160.16
SCIDOCS
BaseLarge
XLXXLBM25
0.66
0.710.72
0.740.75
Fever
BaseLarge
XLXXLBM25
0.24
0.260.270.27
0.21
Climate-Fever
BaseLarge
XLXXLBM25
0.6
0.640.64
0.660.66
SciFact
BaseLarge
XLXXLBM25
0.36
0.380.390.4
0.3
CQADupStack
BaseLarge
XLXXLBM25
0.42
0.440.450.46
0.41
Avg
Figure 4: Comparison with BM25 on NDCG@10. The
GTR-Base model outperforms BM25 on 9 datasets and
the larger GTR models continue to improve on these 9
tasks. The GTR-XXL model catches up or surpasses
BM25 on the other 5 datasets and only under-performs
on 5 of the remaining tasks.
model already outperforms the previous best dense
retrieval model TAS-B as well as the best sparse
model DocT5Query. Scaling up to GTR-XXL
leads to another jump in retrieval performance.
Similar improvements are found on Recall@100
as shown in the Appendix’s table 8. On average,
the scaling up process demonstrates an encourag-
ing ascending trend that eventually outperforms all
baseline methods on all evaluation metrics. This
conﬁrms that scaling up is a valid path towards
generalizability.
Previously, dual encoders failed to match the
performance of BM25 for tasks that require better
lexical matching capabilities. Thus, we wanted to
investigate what kind of tasks can get improved
by scaling up the model size. Figure 4 presents a
detailed comparison of all sizes of GTR models
against the BM25 baseline.
For tasks like NQ where dual encoders have been
previously shown to be more effective than BM25,
increasing the model size continues to advance the
performance of dual encoders. This suggests scal-
ing up can further boost the head start of dense
models over sparse models on these datasets.
For tasks like BioASQ and NFCorpus, where
dual encoders previously struggled to match the
performance of BM25 for inherent reasons, we dis-
covered that scaling up consistently improves the
retrieval performance. In particular, for NFCor-
pus, our Base model under-performs BM25 but the
GTR-FT GTR
Ratio of data Large XL Large XL XXL
NDCG@10 on MS Marco
10% 0.402 0.397 0.428 0.426 -
100% 0.415 0.418 0.430 0.439 0.430
Zero-shot average NDCG@10 w/o MS Marco
10% 0.413 0.418 0.452 0.462 0.465
100% 0.412 0.433 0.445 0.453 0.458
Table 4: Comparisons of NDCG@10 for GTR models
trained with different amount of ﬁne-tuning data. With
only 10% of the MS Marco data, both GTR-FT and
GTR large and XL models achieve slightly worse in-
domain performance; meanwhile they obtain compara-
ble or even superior out-of-domain performance than
using the complete MS Marco data.
XL model outperforms BM25 by 5.5% (0.343 vs.
0.325). This exciting ﬁnding veriﬁes our assump-
tion that scaling up can further exploit the powerful
semantic matching capabilities of the dual encoder
models and enable them to ultimately outperform
BM25.
5.3 Data efﬁciency for large retrievers
To better understand the data efﬁciency for large
dual encoders, we trained models using different
portions of the MS Marco dataset during ﬁne-
tuning. In particular, we sampled a subset of the
training data by keeping only 10% of the training
queries as well as their relevant (positive) passages
and irrelevant (hard negative) passages.
As shown in table 4, using 10% of training data
reduces the in-domain performance of the GTR
models on MS Marco. For the GTR-FT (ﬁne-
tuning only) models, using 10% of the data leads
to a mixed result of out-of-domain performance.
On the other hand, for full GTR models, using
10% of the MS Marco dataset is sufﬁcient for ﬁne-
tuning. In particular, the GTR-Large, XL and XXL
models achieve comparable or even better OOD
performance than ﬁne-tuning on the complete MS
Marco dataset. This might suggest that GTR mod-
els have the beneﬁt of data efﬁciency and could use
less training data for domain adaptation.
6 Ablation Study and Analysis
In this section we present ablations and analysis to
further understand the effects of scaling up, the im-
pact of ﬁne-tuning and pre-training, and the trends
of the GTR model on different experimental condi-
tions.



### Claim 32/38

#### Claim Text
In 2004, Resch and Steinberg incorporated the idea of weak measurement in a more general scenario, a joint quantum measurement .

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 20):

is quite low and unintuitive. To avoid this we cap the recall score (R_cap@k) at k for datasets if the
number of relevant documents for a query greater than k. It is deﬁned as:
R_cap@k= 1
|Q|
|Q|∑
i=1
|maxk(Ai) ∩A⋆
i |
min(k,|A⋆
i |)
where the only difference lies within the denominator where we compute the minimum of k and |A⋆
i |,
instead of |A⋆
i |present in the original recall.
H Document Length Preference for Dense Retrieval System
As we show in Figure 4, TAS-B prefers retrieval of shorter documents, and in comparison, ANCE
retrieves longer documents. The difference is especially extreme for the TREC-COVID dataset:
TAS-B retrieves lots of top hit documents containing only a title and an empty abstract, while ANCE
retrieves top hit documents with a non-empty abstract.
Identifying the source for this contrasting behaviour is difﬁcult, as TAS-B and ANCE use different
models (DistilBERT vs. RoBERTa-base), a different loss function (InfoNCE [62] vs. Margin-MSE
[24] with in-batch negatives), and different hard negative mining strategies. Hence, we decided to
harmonize the training setup and to alter the training by just one aspect: The similarity function.
Dense models require a similarity function to retrieve relevant documents for a given query within
an embedding space. This similarity function is also used during training dense models with the
InfoNCE [62] loss:
Lq = −log exp(τ ·sim(q,d+))∑n
i=0 exp(τ ·sim(q,di))
using nin-batch negatives for each query qand a scaling factor τ. where d+ denotes the relevant
(positive) document for queryq. Commonly used similarity functions (sim(q,d)) are cosine-similarity
or dot-product.
We trained two distilbert-base-uncased models with an identical training setup on MS MARCO
(identical training parameters) and only changed the similarity function from cosine-similarity to
dot-product. As shown in Table 10, we observe signiﬁcant performance differences for some BEIR
datasets. For TREC-COVID, the dot-product model achieves the biggest improvement with 15.3
points, while for a majority on other datasets, it performs worse than the cosine-similarity model.
We observe that these (nearly) identical models retrieve documents with vastly different lengths
as shown in the violin plots in Table 10. For all datasets, we ﬁnd the cosine-similarity model to
prefer shorter documents over longer ones. This is especially severe for TREC-COVID: a large
fraction of the scientiﬁc papers (approx. 42k out of 171k) consist only of publication titles without an
abstract. The cosine-similarity model prefers retrieving these documents. In contrast, the dot-product
model primarily retrieves longer documents, i.e., publications with an abstract. Cosine-similarity
uses vectors of unit length, thereby having no notion of the encoded text length. In contrast, for
dot-product, longer documents result in vectors with higher magnitudes which can yield higher
similarity scores for a query.
Further, as we observe in Figure 5, relevance judgement scores are not uniformly distributed over
document lengths: for some datasets, longer documents are annotated with higher relevancy scores,
while in others, shorter documents are. This can be either due to the annotation process, e.g., the
candidate selection method prefers short or long documents, or due to the task itself, where shorter
or longer documents could be more relevant to the user information need. Hence, it can be more
advantageous to train a model with either cosine-similarity or dot-product depending upon the nature
and needs of the speciﬁc task.
21



Source: data\tc18_2309.07597v5\referenced_papers\[25]_2004.04906.pdf (Page 11):

A Distant Supervision
When training our ﬁnal DPR model using Natural
Questions, we use the passages in our collection
that best match the gold context as the positive
passages. As some QA datasets contain only the
question and answer pairs, it is thus interesting
to see when using the passages that contain the
answers as positives (i.e., the distant supervision
setting), whether there is a signiﬁcant performance
degradation. Using the question and answer to-
gether as the query, we run Lucene-BM25 and pick
the top passage that contains the answer as the pos-
itive passage. Table 5 shows the performance of
DPR when trained using the original setting and
the distant supervision setting.
B Alternative Similarity Functions &
Triplet Loss
In addition to dot product (DP) and negative log-
likelihood based on softmax (NLL), we also exper-
iment with Euclidean distance (L2) and the triplet
loss. We negate L2 similarity scores before ap-
plying softmax and change signs of question-to-
positive and question-to-negative similarities when
applying the triplet loss on dot product scores. The
margin value of the triplet loss is set to 1. Ta-
ble 6 summarizes the results. All these additional
experiments are conducted using the same hyper-
parameters tuned for the baseline (DP, NLL).
Note that the retrieval accuracy for our “baseline”
settings reported in Table 5 (Gold) and Table 6
(DP, NLL) is slightly better than those reported in
Table 3. This is due to a better hyper-parameter
setting used in these analysis experiments, which
is documented in our code release.
C Qualitative Analysis
Although DPR performs better than BM25 in gen-
eral, the retrieved passages of these two retrievers
actually differ qualitatively. Methods like BM25
are sensitive to highly selective keywords and
phrases, but cannot capture lexical variations or se-
mantic relationships well. In contrast, DPR excels
at semantic representation, but might lack sufﬁcient
capacity to represent salient phrases which appear
rarely. Table 7 illustrates this phenomenon with
two examples. In the ﬁrst example, the top scor-
ing passage from BM25 is irrelevant, even though
keywords such as England and Ireland appear mul-
tiple times. In comparison, DPR is able to return
Top-1 Top-5 Top-20 Top-100
Gold 44.9 66.8 78.1 85.0
Dist. Sup. 43.9 65.3 77.1 84.4
Table 5: Retrieval accuracy on the development set of
Natural Questions, trained on passages that match the
gold context (Gold) or the top BM25 passage that con-
tains the answer (Dist. Sup.).
Sim Loss Retrieval Accuracy
Top-1 Top-5 Top-20 Top-100
DP NLL 44.9 66.8 78.1 85.0
Triplet 41.6 65.0 77.2 84.5
L2 NLL 43.5 64.7 76.1 83.1
Triplet 42.2 66.0 78.1 84.9
Table 6: Retrieval Top-kaccuracy on the development
set of Natural Questions using different similarity and
loss functions.
the correct answer, presumably by matching “body
of water” with semantic neighbors such as sea and
channel, even though no lexical overlap exists. The
second example is one where BM25 does better.
The salient phrase “Thoros of Myr” is critical, and
DPR is unable to capture it.
D Joint Training of Retriever and
Reader
We ﬁx the passage encoder in our joint-training
scheme while allowing only the question encoder
to receive backpropagation signal from the com-
bined (retriever + reader) loss function. This allows
us to leverage the HNSW-based FAISS index for
efﬁcient low-latency retrieving, without reindexing
the passages during model updates. Our loss func-
tion largely follows ORQA’s approach, which uses
log probabilities of positive passages selected from
the retriever model, and correct spans and passages
selected from the reader model. Since the passage
encoder is ﬁxed, we could use larger amount of
retrieved passages when calculating the retriever
loss. Speciﬁcally, we get top 100 passages for each
question in a mini-batch and use the method similar
to in-batch negative training: all retrieved passages’
vectors participate in the loss calculation for all
questions in a batch. Our training batch size is set
to 16, which effectively gives 1,600 passages per
question to calculate retriever loss. The reader still
uses 24 passages per question, which are selected



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 1):

Fact Checking
Citation-Prediction
W iki 
FEVER
QUERY
DOCS
Natural Claim
Wikipedia Articles
W iki 
Climate-FEVER
QUERY
DOCS
Climate-based Claim
Wikipedia Articles
SciFact
QUERY
DOCS
Scientific claim
PubMed ArticlesScientific 
SCIDOCS
QUERY
DOCS
Article Title
PubMed ArticlesScientific 
Dup. Question Retrieval
Quora 
Quora
QUERY
DOCS
StackEx. 
CQADupStack
QUERY
DOCS
Argument Retrieval
Misc. 
QUERY
DOCS
Misc. 
ArguAna
QUERY
DOCS
Tóuche-2020
Query Title
Query Title + Body
Query Title
Quora Questions
Argument
Idebate Arguments
Args.me Arguments
News Retrieval
TREC-NEWS
QUERY
DOCS News ArticlesNews Tweet Retrieval
Signal-1M
QUERY
DOCS
News Headline
Twitter TweetsT witter 
Question-Answering
W iki 
NQ
QUERY
DOCS
W iki 
HotpotQA
QUERY
DOCS
FiQA-2018
QUERY
DOCSFinance 
Bio-Medical IR
QUERY
DOCS
Scientific 
BioASQ
QUERY
DOCS
NFCorpus
QUERY
DOCSScientific 
Entity Retrieval
DBPedia
QUERY
DOCS
Entity-based Query
DBPedia ArticlesW iki 
TREC-COVID
Scientific 
Wikipedia Articles
Wikipedia Articles
Natural Query
Multi-Hop Query
CORD-19 Articles
PubMed Articles
PubMed Articles
COVID-19 Query
Nutrition Facts
Bio-Medical Query
Financial Query
Investment Articles
Controversial Query
9 Tasks
18 Datasets
News Headline
Robust04
QUERY
DOCSNews 
 News Articles
News Query
Figure 1: An overview of the diverse tasks and datasets in BEIR benchmark.
So far, it is unclear how well existing trained neural models will perform for other text domains or
textual retrieval tasks. Even more important, it is unclear how well different approaches, like sparse
embeddings vs. dense embeddings, generalize to out-of-distribution data.
In this work, we present a novel robust and heterogeneous benchmark called BEIR (Benchmarking
IR), comprising of 18 retrieval datasets for comparison and evaluation of model generalization. Prior
retrieval benchmarks [19, 50] have issues of a comparatively narrow evaluation focusing either only
on a single task, like question-answering, or on a certain domain. In BEIR , we focus on Diversity, we
include nine different retrieval tasks: Fact checking, citation prediction, duplicate question retrieval,
argument retrieval, news retrieval, question answering, tweet retrieval, bio-medical IR, and entity
retrieval. Further, we include datasets from diverse text domains, datasets that cover broad topics (like
Wikipedia) and specialized topics (like COVID-19 publications), different text types (news articles vs.
Tweets), datasets of various sizes (3.6k - 15M documents), and datasets with different query lengths
(average query length between 3 and 192 words) and document lengths (average document length
between 11 and 635 words).
We use BEIR to evaluate ten diverse retrieval methods from ﬁve broad architectures: lexical, sparse,
dense, late interaction, and re-ranking. From our analysis, we ﬁnd that no single approach consistently
outperforms other approaches on all datasets. Further, we notice that the in-domain performance of a
model does not correlate well with its generalization capabilities: models ﬁne-tuned with identical
training data might generalize differently. In terms of efﬁciency, we ﬁnd a trade-off between the
performances and the computational cost: computationally expensive models, like re-ranking models
and late interaction model perform the best. More efﬁcient approaches e.g. based on dense or sparse
embeddings can substantially underperform traditional lexical models like BM25. Overall, BM25
remains a strong baseline for zero-shot text retrieval.
Finally, we notice that there can be a strong lexical bias present in datasets included within the
benchmark, likely as lexical models are pre-dominantly used during the annotation or creation of
datasets. This can give an unfair disadvantage to non-lexical approaches. We analyze this for the
TREC-COVID [65] dataset: We manually annotate the missing relevance judgements for the tested
systems and see a signiﬁcant performance improvement for non-lexical approaches. Hence, future
work requires better unbiased datasets that allow a fair comparison for all types of retrieval systems.
With BEIR , we take an important step towards a single and uniﬁed benchmark to evaluate the zero-shot
capabilities of retrieval systems. It allows to study when and why certain approaches perform well,
and hopefully steers innovation to more robust retrieval systems. We release BEIR and an integration
of diverse retrieval systems and datasets in a well-documented, easy to use and extensible open-source
package. BEIR is model-agnostic, welcomes methods of all kinds, and also allows easy integration of
new tasks and datasets. More details are available at https://github.com/UKPLab/beir.
2 Related Work and Background
To our knowledge, BEIR is the ﬁrst broad, zero-shot information retrieval benchmark. Existing works
[19, 50] do not evaluate retrieval in a zero-shot setting in depth, they either focus over a single task,
small corpora or on a certain domain. This setting hinders for investigation of model generalization
across diverse set of domains and task types. MultiReQA [19] consists of eight Question-Answering
(QA) datasets and evaluates sentence-level answer retrieval given a question. It only tests a single
task and ﬁve out of eight datasets are from Wikipedia. Further, MultiReQA evaluates retrieval over
rather small corpora: six out of eight tasks have less than 100k candidate sentences, which beneﬁts
dense retrieval over lexical as previously shown [54]. KILT [50] consists of ﬁve knowledge-intensive
2



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 0):

BEIR: A Heterogeneous Benchmark for Zero-shot
Evaluation of Information Retrieval Models
Nandan Thakur, Nils Reimers, Andreas Rücklé∗, Abhishek Srivastava, Iryna Gurevych
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universität Darmstadt
www.ukp.tu-darmstadt.de
Abstract
Existing neural information retrieval (IR) models have often been studied in ho-
mogeneous and narrow settings, which has considerably limited insights into their
out-of-distribution (OOD) generalization capabilities. To address this, and to facili-
tate researchers to broadly evaluate the effectiveness of their models, we introduce
Benchmarking-IR (BEIR ), a robust and heterogeneous evaluation benchmark for
information retrieval. We leverage a careful selection of 18 publicly available
datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-
art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking
architectures on the BEIR benchmark. Our results show BM25 is a robust baseline
and re-ranking and late-interaction based models on average achieve the best zero-
shot performances, however, at high computational costs. In contrast, dense and
sparse-retrieval models are computationally more efﬁcient but often underperform
other approaches, highlighting the considerable room for improvement in their
generalization capabilities. We hope this framework allows us to better evaluate
and understand existing retrieval systems, and contributes to accelerating progress
towards better robust and generalizable systems in the future. BEIR is publicly
available at https://github.com/UKPLab/beir.
1 Introduction
Major natural language processing (NLP) problems rely on a practical and efﬁcient retrieval com-
ponent as a ﬁrst step to ﬁnd relevant information. Challenging problems include open-domain
question-answering [8], claim-veriﬁcation [60], duplicate question detection [78], and many more.
Traditionally, retrieval has been dominated by lexical approaches like TF-IDF or BM25 [55]. How-
ever, these approaches suffer from lexical gap [5] and are able to only retrieve documents containing
keywords present within the query. Further, lexical approaches treat queries and documents as
bag-of-words by not taking word ordering into consideration.
Recently, deep learning and in particular pre-trained Transformer models like BERT [ 12] have
become popular in information retrieval [37]. These neural retrieval systems can be used in many
fundamentally different ways to improve retrieval performance. We provide an brief overview of the
systems in Section 2.1. Many prior work train neural retrieval systems on large datasets like Natural
Questions (NQ) [34] (133k training examples) or MS MARCO [45] (533k training examples), which
both focus on passage retrieval given a question or short keyword-based query. In most prior work,
approaches are afterward evaluated on the same dataset, where signiﬁcant performance gains over
lexical approaches like BM25 are demonstrated [15, 31, 46].
However, creating a large training corpus is often time-consuming and expensive and hence many
retrieval systems are applied in a zero-shot setup, with no available training data to train the system.
∗Contributions made prior to joining Amazon.
Preprint. Under review.
arXiv:2104.08663v4  [cs.IR]  21 Oct 2021



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 23):

Model (→) Lexical Sparse Dense Late-Interaction Re-ranking
Dataset (↓) BM25 DeepCT SPARTA docT5query DPR ANCE TAS-B GenQ ColBERT BM25+CE
MS MARCO 0.658 0.752‡ 0.793‡ 0.819‡ 0.552 0.852‡ 0.884‡ 0.884‡ 0.865‡ 0.658‡
TREC-COVID0.498⋆ 0.347⋆ 0.409⋆ 0.541⋆ 0.212⋆ 0.457⋆ 0.387⋆ 0.456⋆ 0.464⋆ 0.498⋆
BioASQ 0.714 0.699 0.351 0.646 0.256 0.463 0.579 0.627 0.645 0.714
NFCorpus 0.250 0.235 0.243 0.253 0.208 0.232 0.280 0.280 0.254 0.250
NQ 0.760 0.636 0.787 0.832 0.880‡ 0.836 0.903 0.862 0.912 0.760
HotpotQA 0.740 0.731 0.651 0.709 0.591 0.578 0.728 0.673 0.748 0.740
FiQA-2018 0.539 0.489 0.446 0.598 0.342 0.581 0.593 0.618 0.603 0.539
Signal-1M (RT)0.370 0.299 0.270 0.351 0.162 0.239 0.304 0.281 0.283 0.370
TREC-NEWS0.422 0.316 0.262 0.439 0.215 0.398 0.418 0.412 0.367 0.422
Robust04 0.375 0.271 0.215 0.357 0.211 0.274 0.331 0.298 0.310 0.375
ArguAna 0.942 0.932 0.893 0.972 0.751 0.937 0.942 0.978 0.914 0.942
Touché-2020 0.538 0.406 0.381 0.557 0.301 0.458 0.431 0.451 0.439 0.538
CQADupStack0.606 0.545 0.521 0.638 0.403 0.579 0.622 0.654 0.624 0.606
Quora 0.973 0.954 0.896 0.982 0.470 0.987 0.986 0.988 0.989 0.973
DBPedia 0.398 0.372 0.411 0.365 0.349 0.319 0.499 0.431 0.461 0.398
SCIDOCS 0.356 0.314 0.297 0.360 0.219 0.269 0.335 0.332 0.344 0.356
FEVER 0.931 0.735 0.843 0.916 0.840 0.900 0.937 0.928 0.934 0.931
Climate-FEVER0.436 0.232 0.227 0.427 0.390 0.445 0.534 0.450 0.444 0.436
SciFact 0.908 0.893 0.863 0.914 0.727 0.816 0.891 0.893 0.878 0.908
Table 9: In-domain and zero-shot retrieval performance on BEIR datasets. Scores denote Recall@100. The
best retrieval performance on a given dataset is marked in bold, and the second best performance is underlined.
‡ indicates in-domain retrieval performance. ⋆ shows the capped Recall@100 score (Appendix G).
0 100 200 300 400 500
TREC-COVID
SBERT (Method)
Cosine-Sim.
Dot-Prod.
0 5 10 15 20 25
Signal-1M (RT)
SBERT (Method)
Cosine-Sim.
Dot-Prod.
0 100 200 300 400 500
FEVER
SBERT (Method)
Cosine-Sim.
Dot-Prod.
TREC-COVID Signal-1M (RT) FEVER
Cosine-Sim. Dot-Prod. Cosine-Sim. Dot-Prod. Cosine-Sim. Dot-Prod.
0.482 0.635 0.261 0.243 0.670 0.685
Table 10: Violin plots [22] of document lengths for the top-10 retrieved hits and nDCG@10 scores using a
distilbert-base-uncased model trained with either cosine similarity (blue, top) or dot product (orange, bottom) as
described in Appendix H.
24



### Claim 33/38

#### Claim Text
Lipid rafts and membrane heterogeneity: Interactions of particles with lipid rafts in cell membranes have been shown to lead to complex diffusive behaviors ;.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 22):

Corpus Website (Link)
CORD-19 https://www.semanticscholar.org/cord19
NutritionFacts https://nutritionfacts.org
PubMed https://pubmed.ncbi.nlm.nih.gov
Signal-1M https://research.signal-ai.com/datasets/signal1m.html
TREC Washington Posthttps://ir.nist.gov/wapo/
TREC disks 4 and 5 https://trec.nist.gov/data/cd45/
Args.me https://zenodo.org/record/4139439/
DBPedia (2015-10) http://downloads.dbpedia.org/wiki-archive/Downloads2015-10.html
TREC-COVID (Annotated)https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid-beir.zip
Table 7: Corpus Name and Link used for datasets in BEIR .
Dataset Query Relevant-Document
MS MARCOwhat fruit is native to australia <Paragraph>Passiﬂora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned,white ﬂeshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, whileothers list the fruits as being bitter and inedible. assiﬂora herbertiana. A rare passion fruit native toAustralia...
TREC-COVIDwhat is the origin of COVID-19<Title>Origin of Novel Coronavirus (COVID-19): A Computational Biology Study using ArtiﬁcialIntelligence<Paragraph>Origin of the COVID-19 virus has been intensely debated in the community...
BioASQ What is the effect of HMGB2 loss on CTCFclustering <Title>HMGB2 Loss upon Senescence Entry Disrupts Genomic Organization and Induces CTCFClustering across Cell Types.<Paragraph>Processes like cellular senescence are characterized bycomplex events giving rise to heterogeneous cell populations. However, the early molecular eventsdriving this cascade remain elusive....
NFCorpus Titanium Dioxide & Inﬂammatory Bowel Dis-ease <Title>Titanium Dioxide Nanoparticles in Food and Personal Care Products<Paragraph>Titaniumdioxide is a common additive in many food, personal care, and other consumer products used by people,which after use can enter the sewage system, and subsequently enter the environment as treated efﬂuentdischarged to surface waters or biosolids applied to agricultural land, or incinerated wastes...
NQ when did they stop cigarette advertising on tele-vision? <Title>Tobacco advertising<Paragraph>The ﬁrst calls to restrict advertising came in 1962 from theRoyal College of Physicians, who highlighted the health problems and recommended stricter laws...
HotpotQA Stockely Webster has paintings hanging in whathome (that serves as the residence for the Mayorof New York)?
<Title>Stokely Webster<Paragraph>Stokely Webster (1912 – 2001) was best known as an Americanimpressionist painter who studied in Paris. His paintings can be found in the permanent collections ofmany museums, including the Metropolitan Museum of Art in New York, the National Museum...
FiQA-2018 What is the PEG ratio? How is the PEG ratiocalculated? How is the PEG ratio useful forstock investing?
<Paragraph>PEG is Price/Earnings to Growth. It is calculated as Price/Earnings/Annual EPS Growth.It represents how good a stock is to buy, factoring in growth of earnings, which P/E does not. Obviouslywhen PEG is lower, a stock is more undervalued, which means that it is a better buy, and more likely...
Signal-1M (RT)Genvoya, a Gentler Anti-HIV Cocktail, Okayedby EU Regulators <Paragraph>All people with #HIV should get anti-retroviral drugs: @WHO, by @kkelland via@Reuters_Health #AIDS #TasP
TREC-NEWSWebsites where children are prostituted are im-mune from prosecution. But why?<Title>Senate launches bill to remove immunity for websites hosting illegal content, spurred byBackpage.com<Paragraph>The legislation, along with a similar bill in the House, sets the stage for abattle between Congress and some of the Internet’s most powerful players, including Google and variousfree-speech advocates, who believe that Congress shouldn’t regulate Web content or try to force websitesto police themselves more rigorously...
Robust04 What were the causes for the Islamic Revolutionrelative to relations with the U.S.?<Paragraph>BFN [Editorial: "Sow the Wind and Reap the Whirlwind"] Yesterday marked the 14thanniversary of severing of diplomatic relations between the Islamic Republic and the United States ofAmerica. Several occasions arose in the last decade and a half for improving Irano-American relations...
Touché-2020Should the government allow illegal immigrantsto become citizens? <Title>America should support blanket amnesty for illegal immigrants.<Paragraph>Undocumentedworkers do not receive full Social Security beneﬁts because they are not United States citizens " norshould they be until they seek citizenship legally. Illegal immigrants are legally obligated to pay taxes...
CQADupStackCommand to display ﬁrst few and last few linesof a ﬁle <Title>Combing head and tail in a single call via pipe<Paragraph>On a regular basis, I am piping theoutput of some program to either ‘head‘ or ‘tail‘. Now, suppose that I want to see the ﬁrst AND last 10lines of piped output, such that I could do something like ./lotsofoutput | headtail...
Quora How long does it take to methamphetamine outof your blood? <Paragraph>How long does it take the body to get rid of methamphetamine?
DBPedia Paul Auster novels <Title>The New York Trilogy<Paragraph>The New York Trilogy is a series of novels by Paul Auster.Originally published sequentially as City of Glass (1985), Ghosts (1986) and The Locked Room (1986),it has since been collected into a single volume.
SCIDOCS CFD Analysis of Convective Heat Transfer Co-efﬁcient on External Surfaces of Buildings<Title>Application of CFD in building performance simulation for the outdoor environment: an overview<Paragraph>This paper provides an overview of the application of CFD in building performancesimulation for the outdoor environment, focused on four topics...
FEVER DodgeBall: A True Underdog Story is an Amer-ican movie from 2004 <Title>DodgeBall: A True Underdog Story<Paragraph>DodgeBall: A True Underdog Story is a2004 American sports comedy ﬁlm written and directed by Rawson Marshall Thurber and starring VinceVaughn and Ben Stiller. The ﬁlm follows friends who enter a dodgeball tournament...
Climate-FEVERSea level rise is now increasing faster than pre-dicted due to unexpectedly rapid ice melting.<Title>Sea level rise<Paragraph>A sea level rise is an increase in the volume of water in the world ’soceans, resulting in an increase in global mean sea level. The rise is usually attributed to global climatechange by thermal expansion of the water in the oceans and by melting of Ice sheets and glaciers...
Table 8: Examples of queries and relevant documents for all datasets included in BEIR . ( <Title>) and
(<Paragraph>) are used to distinguish the title separately from the paragraph within a document in the table
above. These tokens were not passed to the respective models.
23



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 20):

is quite low and unintuitive. To avoid this we cap the recall score (R_cap@k) at k for datasets if the
number of relevant documents for a query greater than k. It is deﬁned as:
R_cap@k= 1
|Q|
|Q|∑
i=1
|maxk(Ai) ∩A⋆
i |
min(k,|A⋆
i |)
where the only difference lies within the denominator where we compute the minimum of k and |A⋆
i |,
instead of |A⋆
i |present in the original recall.
H Document Length Preference for Dense Retrieval System
As we show in Figure 4, TAS-B prefers retrieval of shorter documents, and in comparison, ANCE
retrieves longer documents. The difference is especially extreme for the TREC-COVID dataset:
TAS-B retrieves lots of top hit documents containing only a title and an empty abstract, while ANCE
retrieves top hit documents with a non-empty abstract.
Identifying the source for this contrasting behaviour is difﬁcult, as TAS-B and ANCE use different
models (DistilBERT vs. RoBERTa-base), a different loss function (InfoNCE [62] vs. Margin-MSE
[24] with in-batch negatives), and different hard negative mining strategies. Hence, we decided to
harmonize the training setup and to alter the training by just one aspect: The similarity function.
Dense models require a similarity function to retrieve relevant documents for a given query within
an embedding space. This similarity function is also used during training dense models with the
InfoNCE [62] loss:
Lq = −log exp(τ ·sim(q,d+))∑n
i=0 exp(τ ·sim(q,di))
using nin-batch negatives for each query qand a scaling factor τ. where d+ denotes the relevant
(positive) document for queryq. Commonly used similarity functions (sim(q,d)) are cosine-similarity
or dot-product.
We trained two distilbert-base-uncased models with an identical training setup on MS MARCO
(identical training parameters) and only changed the similarity function from cosine-similarity to
dot-product. As shown in Table 10, we observe signiﬁcant performance differences for some BEIR
datasets. For TREC-COVID, the dot-product model achieves the biggest improvement with 15.3
points, while for a majority on other datasets, it performs worse than the cosine-similarity model.
We observe that these (nearly) identical models retrieve documents with vastly different lengths
as shown in the violin plots in Table 10. For all datasets, we ﬁnd the cosine-similarity model to
prefer shorter documents over longer ones. This is especially severe for TREC-COVID: a large
fraction of the scientiﬁc papers (approx. 42k out of 171k) consist only of publication titles without an
abstract. The cosine-similarity model prefers retrieving these documents. In contrast, the dot-product
model primarily retrieves longer documents, i.e., publications with an abstract. Cosine-similarity
uses vectors of unit length, thereby having no notion of the encoded text length. In contrast, for
dot-product, longer documents result in vectors with higher magnitudes which can yield higher
similarity scores for a query.
Further, as we observe in Figure 5, relevance judgement scores are not uniformly distributed over
document lengths: for some datasets, longer documents are annotated with higher relevancy scores,
while in others, shorter documents are. This can be either due to the annotation process, e.g., the
candidate selection method prefers short or long documents, or due to the task itself, where shorter
or longer documents could be more relevant to the user information need. Hence, it can be more
advantageous to train a model with either cosine-similarity or dot-product depending upon the nature
and needs of the speciﬁc task.
21



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 15):

Preprint
(a) 182539: monotonic function
 (b) 1117099: active margin
Query
Relevant
ANCE Neg
BM25 Neg
Rand Neg (c) 1132213: yoga bow
Figure 7: t-SNE Plots for Losing Cases in Table 9.
margin” is a geographical terminology, not a ﬁnancial one (which we did not know ourselves and took some time
to ﬁgure out when conducting this case study). There are also some cases where the dense retrieved documents
make sense to us but were labeled irrelevant.
The t-SNE plots in Fig. 6 and Fig. 7 show many interesting patterns of the learned representation space. The
ANCE winning cases often correspond to clear separations of different document groups. For losing cases the
representation space is more mixed, or there is too few relevant documents which may cause the variances in
model performances. There are also many different interesting patterns in the ANCE-learned representation
space. We include the t-SNE plots for all 43 TREC DL Track queries in the supplementary material. More future
analyses of the learned patterns in the representation space may help provide more insights on dense retrieval.
16



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 6):

Model (→) Lexical Sparse Dense Late-Interaction Re-ranking
Dataset (↓) BM25 DeepCT SPARTA docT5query DPR ANCE TAS-B GenQ ColBERT BM25+CE
MS MARCO 0.228 0.296‡ 0.351‡ 0.338‡ 0.177 0.388‡ 0.408‡ 0.408‡ 0.401‡ 0.413‡
TREC-COVID 0.656 0.406 0.538 0.713 0.332 0.654 0.481 0.619 0.677 0.757
BioASQ 0.465 0.407 0.351 0.431 0.127 0.306 0.383 0.398 0.474 0.523
NFCorpus 0.325 0.283 0.301 0.328 0.189 0.237 0.319 0.319 0.305 0.350
NQ 0.329 0.188 0.398 0.399 0.474‡ 0.446 0.463 0.358 0.524 0.533
HotpotQA 0.603 0.503 0.492 0.580 0.391 0.456 0.584 0.534 0.593 0.707
FiQA-2018 0.236 0.191 0.198 0.291 0.112 0.295 0.300 0.308 0.317 0.347
Signal-1M (RT)0.330 0.269 0.252 0.307 0.155 0.249 0.289 0.281 0.274 0.338
TREC-NEWS 0.398 0.220 0.258 0.420 0.161 0.382 0.377 0.396 0.393 0.431
Robust04 0.408 0.287 0.276 0.437 0.252 0.392 0.427 0.362 0.391 0.475
ArguAna 0.315 0.309 0.279 0.349 0.175 0.415 0.429 0.493 0.233 0.311
Touché-2020 0.367 0.156 0.175 0.347 0.131 0.240 0.162 0.182 0.202 0.271
CQADupStack 0.299 0.268 0.257 0.325 0.153 0.296 0.314 0.347 0.350 0.370
Quora 0.789 0.691 0.630 0.802 0.248 0.852 0.835 0.830 0.854 0.825
DBPedia 0.313 0.177 0.314 0.331 0.263 0.281 0.384 0.328 0.392 0.409
SCIDOCS 0.158 0.124 0.126 0.162 0.077 0.122 0.149 0.143 0.145 0.166
FEVER 0.753 0.353 0.596 0.714 0.562 0.669 0.700 0.669 0.771 0.819
Climate-FEVER0.213 0.066 0.082 0.201 0.148 0.198 0.228 0.175 0.184 0.253
SciFact 0.665 0.630 0.582 0.675 0.318 0.507 0.643 0.644 0.671 0.688
Avg. Performance vs. BM25- 27.9% - 20.3% + 1.6% - 47.7% - 7.4% - 2.8% - 3.6% + 2.5% + 11%
Table 2: In-domain and zero-shot performances on BEIR benchmark. All scores denote nDCG@10. The best
score on a given dataset is marked in bold, and the second best is underlined. Corresponding Recall@100
performances can be found in Table 9. ‡ indicates the in-domain performances.
perform well in-domain on MS MARCO, they completely fail to generalize well by under performing
BM25 on nearly all datasets. In contrast, document expansion based docT5query is able to add new
relevant keywords to a document and performs strong on the BEIR datasets. It outperforms BM25 on
11/18 datasets while providing a competitive performance on the remaining datasets.
3. Dense retrieval models with issues for out-of-distribution data. Dense retrieval models (esp.
ANCE and TAS-B), that map queries and documents independently to vector spaces, perform
strongly on certain datasets, while on many other datasets perform signiﬁcantly worse than BM25.
For example, dense retrievers are observed to underperform on datasets with a large domain shift
compared from what they have been trained on, like in BioASQ, or task-shifts like in Touché-2020.
DPR, the only non-MSMARCO trained dataset overall performs the worst in generalization on the
benchmark.
4. Re-ranking and Late-Interaction models generalize well to out-of-distribution data. The
cross-attentional re-ranking model (BM25+CE) performs the best and is able to outperform BM25
on almost all (16/18) datasets. It only fails on ArguAna and Touché-2020, two retrieval tasks that
are extremely different to the MS MARCO training dataset. The late-interaction model ColBERT
computes token embeddings independently for the query and document, and scores (query, document)-
pairs by a cross-attentional like MaxSim operation. It performs a bit weaker than the cross-attentional
re-ranking model, but is still able to outperform BM25 on 9/18 datasets. It appears that cross-attention
and cross-attentional like operations are important for a good out-of-distribution generalization.
5. Strong training losses for dense retrieval leads to better out-of-distribution performances.
TAS-B provides the best zero-shot generalization performance among its dense counterparts. It
outperforms ANCE on 14/18 and DPR on 17/18 datasets respectively. We speculate that the reason lies
in a strong training setup in combination of both in-domain batch negatives and Margin-MSE losses
for the TAS-B model. This training loss function (with strong ensemble teachers in a Knowledge
Distillation setup) shows strong generalization performances.
6. TAS-B model prefers to retrieve documents with shorter lengths. TAS-B underperforms
ANCE on two datasets: TREC-COVID by 17.3 points and Touché-2020 by 7.8 points. We observed
that these models retrieve documents with vastly different lengths as shown in Figure 4. On TREC-
COVID, TAS-B retrieves documents with a median length of mere 10 words versus ANCE with 160
words. Similarly on Touché-2020, 14 words vs. 89 words with TAS-B and ANCE respectively. As
discussed in Appendix H, this preference for shorter or longer documents is due to the used loss
function.
7



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 7):

deu-engmal-engnob-engspa-engepo-engtur-engtel-engpol-engvie-enghrv-engron-enghin-engglg-engsqi-engces-engest-enghun-engslk-englit-engfin-engafr-engtha-engnld-engslv-engtgl-engmon-englvs-engdan-engswe-engzsm-engcat-engjpn-engina-engell-engcmn-engkat-engeus-engbel-engaze-engbos-engfra-engisl-engpes-engbul-engnno-engsrp-engpor-enghye-engukr-enggle-engrus-engind-engmkd-engurd-engita-engmar-enguig-engcym-engxho-engheb-engamh-engkor-engast-engwuu-engyue-engido-engfry-engtam-engara-engyid-engben-engkaz-engfao-engtat-enggla-engile-engswh-enguzb-engkur-englat-engjav-engcbk-engnds-engkhm-engarz-engtuk-engnov-engawa-englfn-enghsb-engoci-engdsb-engpms-engceb-engmax-engwar-engswg-engang-engtzl-engcsb-enggsw-engarq-engorv-engcha-engmhr-engbre-engkzj-engdtp-engpam-engcor-engber-engkab-eng
0.0
0.2
0.4
0.6
0.8
1.0
F1 score
LaBSE
LASER2
MiniLM-L12-multilingual
MPNet-multilingual
SGPT-BLOOM-7.1B-msmarco
(a) Bitext Mining on Tatoeba
en hi
zh-CN
pt id es th it fr ru de fa sv vi
zh-TW
nl ms da pl tr sq el ro hu sl ko fi ja nb ml lv he ur bn ar te af ta hy my az mn is kn tl jv sw ka kmam cy zh
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Accuracy
(b) Multilingual Classiﬁcation
ko fr es en ar it zh ru tr de pl
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8Cos. Sim. Spearman Corr.
fr-en en-aren-de it-en es-en nl-en pl-en zh-en en-tr es-it fr-pl de-fr de-ende-pl
 (c) Multi- and Crosslingual STS
Figure 5: MTEB multilingual performance. Bitext mining is dominated by LaBSE, while classiﬁcation and STS
results are mixed. SGPT-BLOOM-7B1-msmarco tends to perform well on the languages BLOOM has been pre-
trained on, such as Chinese, French and Portuguese.
Pair Classiﬁcation GTR-XL and GTR-XXL
have the strongest performance. Pair classiﬁca-
tion is closest to STS in its framing, yet models
rank signiﬁcantly differently on the two tasks. This
highlights the importance of benchmarking on a
diverse set of tasks to avoid blindly reusing a model
for a different task.
Reranking MPNet and MiniLM models perform
strongly on reranking tasks. On SciDocsRR (Co-
han et al., 2020a) they perform far better than big-
ger models, which is likely due to parts of Sci-
DocsRR being included in their training data. Our
scale of experiments and that of model pre-training
make controlling for data contamination challeng-
ing. Thus, we ignore overlap of MTEB datasets
with model training datasets in MTEB scores. As
long as enough datasets are averaged, we believe
these effects to be insigniﬁcant.
Retrieval SGPT-5.8B-msmarco is the best em-
bedding model on the BEIR subset in MTEB
as well as on the full BEIR benchmark (Thakur
et al., 2021; Muennighoff, 2022). The even larger
7.1B SGPT model making use of BLOOM (Scao
et al., 2022) performs signiﬁcantly weaker, which
is likely due to the multilinguality of BLOOM.
Models geared towards STS (SimCSE, ST5, SGPT-
nli) perform badly on retrieval tasks. Retrieval
tasks are unique in that there are two distinct types
of texts: Queries and documents (“asymmetric”),
while other tasks only have a single type of text
(“symmetric”). On the QuoraRetrieval dataset,
which has been shown to be largely symmetric
(Muennighoff, 2022), the playing ﬁeld is more
even with SGPT-5.8B-nli outperforming SGPT-
5.8B-msmarco, see Table 11.
STS & Summarization Retrieval models (GTR,
SGPT-msmarco) perform badly on STS, while ST5-
XXL has the highest performance. This highlights
the bifurcation of the ﬁeld into separate embedding
models for retrieval (asymmetric) and similarity
(symmetric) use cases (Muennighoff, 2022).
4.3 Efﬁciency
We investigate the latency-performance trade-off
of models in Figure 4. The graph allows for signiﬁ-
cant elimination of model candidates in the model
selection process. It brings model selection down
to three clusters:
Maximum speed Word Embedding models offer
maximum speed with Glove taking the lead on both
performance and speed, thus making the choice
simple in this case.
Maximum performance If latency is less impor-
tant than performance, the left-hand side of the
graph offers a cluster of highly performant, but
slow models. Depending on the task at hand, GTR-
XXL, ST5-XXL or SGPT-5.8B may be the right
choice, see Section 4.2. SGPT-5.8B comes with
the additional caveat of its high-dimensional em-
beddings requiring more storage.
Speed and performance The ﬁne-tuned MPNet
and MiniLM models lead the middle cluster mak-
ing the choice easy.



### Claim 34/38

#### Claim Text
Characteristics of the used plasma source can be found in .

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[52]_2207.02578.pdf (Page 11):

Hang Zhang, Yeyun Gong, Yelong Shen, Jiancheng
Lv, Nan Duan, and Weizhu Chen. 2021. Adversar-
ial retriever-ranker for dense text retrieval. ArXiv
preprint, abs/2110.03611.
A Details on Table 1
The numbers for the GLUE benchmark are from
the ofﬁcial leaderboard 2. Note that the leader-
board submission from BERT does not use ensem-
ble, so the comparison is not entirely fair. However,
this does not change our conclusion that BERT gen-
erally performs worse than RoBERTa and ELEC-
TRA on NLP tasks. For the MS-MARCO dataset,
we ﬁne-tune all the pre-trained models with BM25
hard negatives only. For BERT and RoBERTa, we
use the same hyperparameters as discussed in Sec-
tion 4.1. For ELECTRA, we train for 6 epochs
with a peak learning rate 4 ×10−5 since it con-
verges much slower.
B Implementation Details
MS-MARCO Wikipedia
# of passages 8.8M 21M
PLM BERT base BERTbase
batch size 2048 2048
text length 144 144
learning rate 3 ×10−4 3 ×10−4
warmup steps 4000 4000
train steps 80k 200k
encoder replace rate 30% 30%
decoder replace rate 50% 50%
Table 11: Hyper-parameters for pre-training. The
Wikipedia corpus comes from DPR (Karpukhin et al.,
2020) instead of the original one used for BERT pre-
training.
The hyper-parameters for our proposed pre-
training and ﬁne-tuning are listed in Table 11 and
13, respectively. For supervised ﬁne-tuning, One
shared encoder is used to encode both the query
and passages. We start with the ofﬁcial BM25 hard
negatives in the ﬁrst training round and then change
to mined hard negatives. During inference, given
a query, we use brute force search to rank all the
passages for a fair comparison with previous works.
The generator is initialized with the released one
by ELECTRA authors 3, and its parameters are
2 https://gluebenchmark.com/leaderboard
3https://huggingface.co/google/
electra-base-generator
frozen during pre-training. All the reported results
are based on a single run, we ﬁnd that the numbers
are quite stable with different random seeds.
For ﬁne-tuning on the NQ dataset, we reuse most
hyper-parameters values from MS-MARCO train-
ing. A few exceptions are listed below. We ﬁne-
tune for 20ksteps with learning rate 5×10−6. The
maximum length for passage is 192. The mined
hard negatives come from top-100 predictions that
do not contain any correct answer.
C Variants of Generators
In the ELECTRA pre-training, the generator plays
a critical role. Using either a too strong or too weak
generator hurts the learnability and generalization
of the discriminator.
generator MRR@10 R@1k
frozen generator 38.0 98.3
joint train 38.0 98.4
joint train w/ random init 37.8 98.4
Table 12: Variants of generators for SimLM pre-
training. Performances are reported on the dev set of
MS-MARCO with BM25 negatives only.
We also tried several variants of generators. In
Table 12, “frozen generator” keeps the genera-
tor parameters unchanged during our pre-training,
“joint train” also ﬁne-tunes the generator parame-
ters, and “joint train w/ random init” uses randomly
initialized generator parameters. We do not ob-
serve any signiﬁcant performance difference be-
tween these variants. In our experiments, we sim-
ply use the “frozen generator” as it has a faster
training speed.



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 22):

Corpus Website (Link)
CORD-19 https://www.semanticscholar.org/cord19
NutritionFacts https://nutritionfacts.org
PubMed https://pubmed.ncbi.nlm.nih.gov
Signal-1M https://research.signal-ai.com/datasets/signal1m.html
TREC Washington Posthttps://ir.nist.gov/wapo/
TREC disks 4 and 5 https://trec.nist.gov/data/cd45/
Args.me https://zenodo.org/record/4139439/
DBPedia (2015-10) http://downloads.dbpedia.org/wiki-archive/Downloads2015-10.html
TREC-COVID (Annotated)https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid-beir.zip
Table 7: Corpus Name and Link used for datasets in BEIR .
Dataset Query Relevant-Document
MS MARCOwhat fruit is native to australia <Paragraph>Passiﬂora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned,white ﬂeshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, whileothers list the fruits as being bitter and inedible. assiﬂora herbertiana. A rare passion fruit native toAustralia...
TREC-COVIDwhat is the origin of COVID-19<Title>Origin of Novel Coronavirus (COVID-19): A Computational Biology Study using ArtiﬁcialIntelligence<Paragraph>Origin of the COVID-19 virus has been intensely debated in the community...
BioASQ What is the effect of HMGB2 loss on CTCFclustering <Title>HMGB2 Loss upon Senescence Entry Disrupts Genomic Organization and Induces CTCFClustering across Cell Types.<Paragraph>Processes like cellular senescence are characterized bycomplex events giving rise to heterogeneous cell populations. However, the early molecular eventsdriving this cascade remain elusive....
NFCorpus Titanium Dioxide & Inﬂammatory Bowel Dis-ease <Title>Titanium Dioxide Nanoparticles in Food and Personal Care Products<Paragraph>Titaniumdioxide is a common additive in many food, personal care, and other consumer products used by people,which after use can enter the sewage system, and subsequently enter the environment as treated efﬂuentdischarged to surface waters or biosolids applied to agricultural land, or incinerated wastes...
NQ when did they stop cigarette advertising on tele-vision? <Title>Tobacco advertising<Paragraph>The ﬁrst calls to restrict advertising came in 1962 from theRoyal College of Physicians, who highlighted the health problems and recommended stricter laws...
HotpotQA Stockely Webster has paintings hanging in whathome (that serves as the residence for the Mayorof New York)?
<Title>Stokely Webster<Paragraph>Stokely Webster (1912 – 2001) was best known as an Americanimpressionist painter who studied in Paris. His paintings can be found in the permanent collections ofmany museums, including the Metropolitan Museum of Art in New York, the National Museum...
FiQA-2018 What is the PEG ratio? How is the PEG ratiocalculated? How is the PEG ratio useful forstock investing?
<Paragraph>PEG is Price/Earnings to Growth. It is calculated as Price/Earnings/Annual EPS Growth.It represents how good a stock is to buy, factoring in growth of earnings, which P/E does not. Obviouslywhen PEG is lower, a stock is more undervalued, which means that it is a better buy, and more likely...
Signal-1M (RT)Genvoya, a Gentler Anti-HIV Cocktail, Okayedby EU Regulators <Paragraph>All people with #HIV should get anti-retroviral drugs: @WHO, by @kkelland via@Reuters_Health #AIDS #TasP
TREC-NEWSWebsites where children are prostituted are im-mune from prosecution. But why?<Title>Senate launches bill to remove immunity for websites hosting illegal content, spurred byBackpage.com<Paragraph>The legislation, along with a similar bill in the House, sets the stage for abattle between Congress and some of the Internet’s most powerful players, including Google and variousfree-speech advocates, who believe that Congress shouldn’t regulate Web content or try to force websitesto police themselves more rigorously...
Robust04 What were the causes for the Islamic Revolutionrelative to relations with the U.S.?<Paragraph>BFN [Editorial: "Sow the Wind and Reap the Whirlwind"] Yesterday marked the 14thanniversary of severing of diplomatic relations between the Islamic Republic and the United States ofAmerica. Several occasions arose in the last decade and a half for improving Irano-American relations...
Touché-2020Should the government allow illegal immigrantsto become citizens? <Title>America should support blanket amnesty for illegal immigrants.<Paragraph>Undocumentedworkers do not receive full Social Security beneﬁts because they are not United States citizens " norshould they be until they seek citizenship legally. Illegal immigrants are legally obligated to pay taxes...
CQADupStackCommand to display ﬁrst few and last few linesof a ﬁle <Title>Combing head and tail in a single call via pipe<Paragraph>On a regular basis, I am piping theoutput of some program to either ‘head‘ or ‘tail‘. Now, suppose that I want to see the ﬁrst AND last 10lines of piped output, such that I could do something like ./lotsofoutput | headtail...
Quora How long does it take to methamphetamine outof your blood? <Paragraph>How long does it take the body to get rid of methamphetamine?
DBPedia Paul Auster novels <Title>The New York Trilogy<Paragraph>The New York Trilogy is a series of novels by Paul Auster.Originally published sequentially as City of Glass (1985), Ghosts (1986) and The Locked Room (1986),it has since been collected into a single volume.
SCIDOCS CFD Analysis of Convective Heat Transfer Co-efﬁcient on External Surfaces of Buildings<Title>Application of CFD in building performance simulation for the outdoor environment: an overview<Paragraph>This paper provides an overview of the application of CFD in building performancesimulation for the outdoor environment, focused on four topics...
FEVER DodgeBall: A True Underdog Story is an Amer-ican movie from 2004 <Title>DodgeBall: A True Underdog Story<Paragraph>DodgeBall: A True Underdog Story is a2004 American sports comedy ﬁlm written and directed by Rawson Marshall Thurber and starring VinceVaughn and Ben Stiller. The ﬁlm follows friends who enter a dodgeball tournament...
Climate-FEVERSea level rise is now increasing faster than pre-dicted due to unexpectedly rapid ice melting.<Title>Sea level rise<Paragraph>A sea level rise is an increase in the volume of water in the world ’soceans, resulting in an increase in global mean sea level. The rise is usually attributed to global climatechange by thermal expansion of the water in the oceans and by melting of Ice sheets and glaciers...
Table 8: Examples of queries and relevant documents for all datasets included in BEIR . ( <Title>) and
(<Paragraph>) are used to distinguish the title separately from the paragraph within a document in the table
above. These tokens were not passed to the respective models.
23



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 5):

0.1B 1B 2B 4B
Model Parameters (Billions)
0.62
0.64
0.66
0.68
0.70
0.72
0.74Average Performance (accuracy)
Classification
0.1B 1B 2B 4B
Model Parameters (Billions)
0.36
0.37
0.38
0.39
0.40
0.41
0.42
0.43
0.44Average Performance (v_measure)
Clustering
0.1B 1B 2B 4B
Model Parameters (Billions)
0.76
0.78
0.80
0.82
0.84
0.86Average Performance (ap)
PairClassification
0.1B 1B 2B 4B
Model Parameters (Billions)
0.51
0.52
0.53
0.54
0.55
0.56Average Performance (map)
Reranking
0.1B 1B 2B 4B
Model Parameters (Billions)
0.350
0.375
0.400
0.425
0.450
0.475
0.500Average Performance (nDCG@10)
Retrieval
0.1B 1B 2B 4B
Model Parameters (Billions)
0.74
0.76
0.78
0.80
0.82Average Performance (cos. sim. spearman corr.)
STS
GTR ST5 SGPT
Figure 3: MTEB performance scales with model
size. The smallest SGPT variant underperforms similar-
sized GTR and ST5 variants. This may be due to the
bias-only ﬁne-tuning SGPT employs, which catches
up with full ﬁne-tuning only as model size and thus
the number of bias parameters increases (Muennighoff,
2022).
ding tasks leading to a high representation of trans-
formers (Vaswani et al., 2017). We group models
into self-supervised and supervised methods.
Self-supervised methods (a) Transformer-
based BERT (Devlin et al., 2018) is trained using
self-supervised mask and sentence prediction tasks.
By taking the mean across the sequence length
(mean-pooling) the model can directly be used
to produce text embeddings. SimCSE-Unsup
(Gao et al., 2021b) uses BERT as a foundation
and performs additional self-supervised training.
(b) Non-transformer : Komninos (Komninos
and Manandhar, 2016) and Glove (Pennington
et al., 2014) are two word embedding models
that directly map words to vectors. Hence, their
embeddings lack context awareness, but provide
signiﬁcant speed-ups.
Supervised methods The original transformer
model (Vaswani et al., 2017) consists of an encoder
and decoder network. Subsequent transformers
often train only encoders like BERT (Devlin et al.,
2018) or decoders like GPT (Radford et al., 2019).
(a) Transformer encoder methods coCon-
denser (Gao and Callan, 2021), Contriever (Izac-
ard et al., 2021), LaBSE (Feng et al., 2020) and
SimCSE-BERT-sup (Gao et al., 2021b) are based
on the pre-trained BERT model (Devlin et al.,
2018). coCondenser and Contriever add a self-
supervised stage prior to supervised ﬁne-tuning
for a total of three training stages. LaBSE uses
BERT to perform additional pre-training on par-
allel data to produce a competitive bitext mining
model. SPECTER (Cohan et al., 2020a) relies on
the pre-trained SciBERT (Beltagy et al., 2019) vari-
ant instead and ﬁne-tunes on citation graphs. GTR
(Ni et al., 2021b) and ST5 (Ni et al., 2021a) are
based on the encoder part of the T5 model (Raf-
fel et al., 2020) and only differ in their ﬁne-tuning
datasets. After additional self-supervised training,
ST5 does contrastive ﬁne-tuning on NLI (Ni et al.,
2021a; Gao et al., 2021b) being geared towards
STS tasks. Meanwhile, GTR ﬁne-tunes on MS-
MARCO and focuses on retrieval tasks. MPNet
and MiniLM correspond to ﬁne-tuned embedding
models (Reimers and Gurevych, 2019) of the pre-
trained MPNet (Song et al., 2020) and MiniLM
(Wang et al., 2020) models using diverse datasets
to target any embedding use case.
(b) Transformer decoder methods SGPT Bi-
Encoders (Muennighoff, 2022) perform contrastive
ﬁne-tuning of <0.1% of pre-trained parameters us-
ing weighted-mean pooling. Similar to ST5 and
GTR, SGPT-nli models are geared towards STS,
while SGPT-msmarco models towards retrieval.
SGPT-msmarco models embed queries and doc-
uments for retrieval with different special tokens
to help the model distinguish their role. For non-
retrieval tasks, we use its query representations.
We benchmark publicly available SGPT models
based on GPT-NeoX (Andonian et al., 2021), GPT-
J (Wang and Komatsuzaki, 2021) and BLOOM
(Scao et al., 2022). Alternatively, cpt-text (Nee-
lakantan et al., 2022) passes pre-trained GPT de-
coders through a two-stage process using last token
pooling to provide embeddings from decoders. We
benchmark their models via the OpenAI Embed-
dings API4.
(c) Non-transformer LASER (Heffernan et al.,
2022) is the only context aware non-transformer
model we benchmark, relying on an LSTM
4https://beta.openai.com/docs/guides/
embeddings



Source: data\tc18_2309.07597v5\referenced_papers\[32]_2202.08904.pdf (Page 7):

Dataset (→) AskU CQA TwitterP SciDocs Avg Quora STS-B
Method (↓) TURL PIT Avg Cite CC CR CV Avg
OOD Unsupervised
BM25♦ 53.4 13.3 71.9 70.5 71.2 58.9 61.3 67.3 66.9 63.6 50.4 80.8†
OOD Unsupervised + OOD Supervised (NLI)
SBERT-base-nli-v2-bs64 52.8 11.6 75.5 71.5 73.5 68.0 70.6 71.1 73.5 70.8 52.2 78.9 83.9
SBERT-base-nli-v2-bf-bs64 53.8 11.7 76.6 72.9 74.8 67.5 70.6 70.8 73.0 70.5 52.7 78.8 81.8
SGPT-0.1B-weightedmean-nli-bs64 54.9 11.2 72.7 66.0 69.3 66.2 68.9 68.9 71.7 68.9 51.1 79.5 81.0
SGPT-0.1B-weightedmean-nli-bf-bs64 54.9 10.8 72.3 65.3 68.8 64.7 67.4 68.0 70.8 67.8 50.6 77.5 78.6
SGPT-0.1B-weightedmean-nli-bf-bs1024 55.7 11.1 72.8 66.5 69.6 65.1 67.8 68.6 70.5 68.0 51.1 79.0 79.5
SGPT-1.3B-weightedmean-nli-bf-bs1024 56.0 13.5 75.4 70.7 73.1 70.1 72.9 73.2 75.0 72.8 53.8 82.3 83.9
SGPT-2.7B-weightedmean-nli-bf-bs1024 57.5 14.0 75.8 71.0 73.4 72.3 75.4 74.7 76.5 74.7 54.9 82.6 84.7
SGPT-5.8B-weightedmean-nli-bf-bs1024 57.1 16.0 76.5 76.0 76.3 75.0 78.2 77.1 78.3 77.2 56.6 84.7 85.7
OOD Unsupervised + OOD Unsupervised + OOD Supervised (NLI, [27])
Ada Similarity (Mar 2022) 55.9 13.6 76.6 76.2 76.4 66.7 71.0 71.3 73.7 70.7 54.1 82.2
Curie Similarity (Mar 2022) 57.3 15.8 76.3 78.9 77.6 66.3 71.7 71.3 73.0 70.6 55.3 83.1
Davinci Similarity (June 2022) 55.9 76.3 78.0 77.1
Table 5: Results on USEB, Quora and STS-B. Metrics are average precision for USEB, nDCG@10
for Quora and Spearman correlation for STS-B. bf=BitFit. bs=Batch Size. OOD=Out-of-domain,
to contrast these numbers from in-domain numbers in [49]. However, fragments may be in-domain
due to the large pre-training data of the transformer models. SGPT-0.1B-weightedmean-nli performs
2% worse than SBERT-base-nli-v2 on USEB, but improves on Quora by 1%. Note that there is still a
size difference of 14% between the two models. ♦: Results from [49] except when marked with †.
CQADupstack and SciDocs differ from the same-name datasets in BEIR.
batch size of 1024. In Appendix §A, we provide results using a lower batch size and additional
ablations. Results for Ada and Curie were obtained by querying the OpenAI Similarity Embeddings
API in March 2022. They correspond to the cpt-text similarity models from [27] and we provide their
parameters in Table 2.
4.2 Asymmetric Search
4.2.1 Method
If not otherwise speciﬁed, we follow the same setup as in §4.1.1. For asymmetric search, we train
on MS-MARCO [28]. We limit the model sequence length to 300 tokens during both training and
inference. We follow concurrent work [27] and add enclosing brackets to help the model distinguish
between query and document. We embed the tokens of query q in two brackets as [q0−n]. For
documents, we use curly brackets: {d0−n}. We add the token ids of the brackets to the already
tokenized text to avoid the tokens intermingling. We refer to these special brackets as specb.
4.2.2 Results
Table 6 benchmarksSGPT-BE-5.8B(SGPT-5.8B-weightedmean-msmarco-specb-bitﬁt) on BEIR [44]
with: (a) BM25 [41], a non-semantic fast baseline (b) SGPT-CE-6.1B from §3 (c) BM25+CE [44],
the current overall state-of-the-art on BEIR (d) TAS-B [17], the original Bi-Encoder state-of-the-art
on BEIR (e) Contriever [20], a similar training scheme as [ 27] but using an encoder transformer
(f ) GTR-XXL [29], the current Bi-Encoder state-of-the-art on BEIR with 4.8 billion parameters
using the BERT-like encoder transformer of T5 [38] (g) cpt-text, a GPT-like decoder transformer
architecture concurrently proposed in [27]. Corresponding parameter estimates are in Table 2.
SGPT-5.8B achieves the best average nDCG@10 both on the BEIR subset selected in [27] and on the
full BEIR benchmark. It outperforms the roughly same-sized cpt-text-L and the 30x larger cpt-text-XL
by 8.1% and 4.2%, respectively. Yet, cpt-text models have gone through an additional unsupervised
training stage [27] and are fully trained. SGPT-BE-5.8B ﬁne-tunes just 700K parameters, 0.0004%
of the parameters ﬁne-tuned for cpt-text-XL [ 27]. See Table 2 for sizes. We suspect much of
the difference to come from the cpt-text model’s inferior last token pooling as shown in Figure
3. Further, we suspect that the beneﬁts of the additional unsupervised contrastive pre-training
stage diminish when followed by supervised contrastive ﬁne-tuning. SGPT-BE-5.8B improves on
the overall state-of-the-art, a Cross-Encoder, by 3%. It improves on the previously best sentence
embeddings (Bi-Encoder) on BEIR, GTR-XXL, by 7%. However, these improvements come at a
signiﬁcant cost. GTR-XXL has 20% fewer parameters and its embeddings have 768 dimensions.
SGPT-BE-5.8B produces embeddings with 4096 dimensions, hence requiring about 5x more storage.
It took the model six days on one Nvidia A100 GPU to encode the entire BioASQ corpus with 15M
8



Source: data\tc18_2309.07597v5\referenced_papers\[32]_2202.08904.pdf (Page 14):

Model ( → ) [41] SGPT-BE
Dataset ( ↓ ) BM25 125M 1.3B 2.7B
MS MARCO 0.228 0.279 0.361 0.388
TREC-COVID 0.688 0.738 0.785 0.807
BioASQ 0.488 0.272 0.347 0.384
NFCorpus 0.306 0.228 0.321 0.339
NQ 0.326 0.297 0.430 0.467
HotpotQA 0.602 0.409 0.499 0.528
FiQA-2018 0.254 0.211 0.300 0.333
Signal-1M (RT) 0.330 0.236 0.250 0.249
TREC-NEWS 0.405 0.319 0.424 0.438
Robust04 0.425 0.313 0.421 0.449
ArguAna 0.472 0.455 0.497 0.505
Touché-2020 0.347 0.230 0.245 0.235
CQADupStack 0.326 0.249 0.320 0.349
Quora 0.808 0.730 0.853 0.856
DBPedia 0.320 0.227 0.315 0.347
SCIDOCS 0.165 0.121 0.161 0.165
FEVER 0.649 0.605 0.682 0.728
Climate-FEVER 0.186 0.218 0.266 0.272
SciFact 0.611 0.569 0.683 0.702
Sub-Average 0.477 0.420 0.495 0.514
Average 0.428 0.357 0.433 0.453
Table 8: Additional SGPT Bi-Encoder scores on BEIR. Scores are nDCG@10. Average scores do
not include MS MARCO.
15



### Claim 35/38

#### Claim Text
For 15N2O, 22 of the lines belonging to the v1+v3 band reported here are not included in GEISA .

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 15):

Preprint
(a) 182539: monotonic function
 (b) 1117099: active margin
Query
Relevant
ANCE Neg
BM25 Neg
Rand Neg (c) 1132213: yoga bow
Figure 7: t-SNE Plots for Losing Cases in Table 9.
margin” is a geographical terminology, not a ﬁnancial one (which we did not know ourselves and took some time
to ﬁgure out when conducting this case study). There are also some cases where the dense retrieved documents
make sense to us but were labeled irrelevant.
The t-SNE plots in Fig. 6 and Fig. 7 show many interesting patterns of the learned representation space. The
ANCE winning cases often correspond to clear separations of different document groups. For losing cases the
representation space is more mixed, or there is too few relevant documents which may cause the variances in
model performances. There are also many different interesting patterns in the ANCE-learned representation
space. We include the t-SNE plots for all 43 TREC DL Track queries in the supplementary material. More future
analyses of the learned patterns in the representation space may help provide more insights on dense retrieval.
16



Source: data\tc18_2309.07597v5\referenced_papers\[35]_2210.07316.pdf (Page 23):

Dataset LanguageKomninos LASER2 LaBSE MiniLM-L12-multilingual MPNet-multilingual SGPT-BLOOM-7.1B-msmarco
STS17 ko-ko 2.54 70.52 71.32 77.03 83.41 66.89
STS17 ar-ar 13.78 67.47 69.07 79.16 79.10 76.42
STS17 en-ar 9.08 65.05 74.51 81.22 80.85 78.07
STS17 en-de -3.11 66.66 73.85 84.22 83.28 59.10
STS17 en-tr -0.45 70.05 72.07 76.74 74.90 11.80
STS17 es-en -8.18 55.30 65.71 84.44 86.11 78.22
STS17 es-es 48.23 79.67 80.83 85.56 85.14 86.00
STS17 fr-en 5.81 70.82 76.98 76.59 81.17 80.46
STS17 it-en 3.64 70.98 76.99 82.35 84.24 51.58
STS17 nl-en -0.44 68.12 75.22 81.71 82.51 45.85
STS22 de 33.04 25.69 48.58 44.64 46.70 30.05
STS22 es 48.53 54.92 63.18 56.56 59.91 65.41
STS22 pl 12.47 18.34 39.30 33.74 33.65 31.13
STS22 tr 47.38 36.97 58.15 53.39 56.30 47.14
STS22 ar 32.42 42.57 57.67 46.2 52.19 58.67
STS22 ru 19.44 39.24 57.49 57.08 58.74 43.36
STS22 zh 4.78 49.41 63.02 58.75 61.75 66.78
STS22 fr 49.43 58.61 77.95 70.55 74.30 80.38
STS22 de-en 28.65 32.35 50.14 52.65 50.81 51.16
STS22 es-en 26.97 54.34 71.86 67.33 70.26 75.06
STS22 it 57.77 60.31 72.22 55.22 60.65 65.65
STS22 pl-en 45.55 53.63 69.41 69.02 73.07 53.31
STS22 zh-en 14.05 46.19 64.02 65.71 67.96 68.45
STS22 es-it 41.10 42.21 69.69 47.67 53.70 65.50
STS22 de-fr 14.77 37.41 53.28 51.73 62.34 53.28
STS22 de-pl 11.21 15.67 58.69 44.22 40.53 43.05
STS22 fr-pl 39.44 39.44 61.98 50.71 84.52 28.17
Average mix 22.14 51.55 65.67 64.23 67.71 57.81
Table 14: Multilingual STS Results. Scores are Spearman correlations of cosine similarities.



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 22):

Corpus Website (Link)
CORD-19 https://www.semanticscholar.org/cord19
NutritionFacts https://nutritionfacts.org
PubMed https://pubmed.ncbi.nlm.nih.gov
Signal-1M https://research.signal-ai.com/datasets/signal1m.html
TREC Washington Posthttps://ir.nist.gov/wapo/
TREC disks 4 and 5 https://trec.nist.gov/data/cd45/
Args.me https://zenodo.org/record/4139439/
DBPedia (2015-10) http://downloads.dbpedia.org/wiki-archive/Downloads2015-10.html
TREC-COVID (Annotated)https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid-beir.zip
Table 7: Corpus Name and Link used for datasets in BEIR .
Dataset Query Relevant-Document
MS MARCOwhat fruit is native to australia <Paragraph>Passiﬂora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned,white ﬂeshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, whileothers list the fruits as being bitter and inedible. assiﬂora herbertiana. A rare passion fruit native toAustralia...
TREC-COVIDwhat is the origin of COVID-19<Title>Origin of Novel Coronavirus (COVID-19): A Computational Biology Study using ArtiﬁcialIntelligence<Paragraph>Origin of the COVID-19 virus has been intensely debated in the community...
BioASQ What is the effect of HMGB2 loss on CTCFclustering <Title>HMGB2 Loss upon Senescence Entry Disrupts Genomic Organization and Induces CTCFClustering across Cell Types.<Paragraph>Processes like cellular senescence are characterized bycomplex events giving rise to heterogeneous cell populations. However, the early molecular eventsdriving this cascade remain elusive....
NFCorpus Titanium Dioxide & Inﬂammatory Bowel Dis-ease <Title>Titanium Dioxide Nanoparticles in Food and Personal Care Products<Paragraph>Titaniumdioxide is a common additive in many food, personal care, and other consumer products used by people,which after use can enter the sewage system, and subsequently enter the environment as treated efﬂuentdischarged to surface waters or biosolids applied to agricultural land, or incinerated wastes...
NQ when did they stop cigarette advertising on tele-vision? <Title>Tobacco advertising<Paragraph>The ﬁrst calls to restrict advertising came in 1962 from theRoyal College of Physicians, who highlighted the health problems and recommended stricter laws...
HotpotQA Stockely Webster has paintings hanging in whathome (that serves as the residence for the Mayorof New York)?
<Title>Stokely Webster<Paragraph>Stokely Webster (1912 – 2001) was best known as an Americanimpressionist painter who studied in Paris. His paintings can be found in the permanent collections ofmany museums, including the Metropolitan Museum of Art in New York, the National Museum...
FiQA-2018 What is the PEG ratio? How is the PEG ratiocalculated? How is the PEG ratio useful forstock investing?
<Paragraph>PEG is Price/Earnings to Growth. It is calculated as Price/Earnings/Annual EPS Growth.It represents how good a stock is to buy, factoring in growth of earnings, which P/E does not. Obviouslywhen PEG is lower, a stock is more undervalued, which means that it is a better buy, and more likely...
Signal-1M (RT)Genvoya, a Gentler Anti-HIV Cocktail, Okayedby EU Regulators <Paragraph>All people with #HIV should get anti-retroviral drugs: @WHO, by @kkelland via@Reuters_Health #AIDS #TasP
TREC-NEWSWebsites where children are prostituted are im-mune from prosecution. But why?<Title>Senate launches bill to remove immunity for websites hosting illegal content, spurred byBackpage.com<Paragraph>The legislation, along with a similar bill in the House, sets the stage for abattle between Congress and some of the Internet’s most powerful players, including Google and variousfree-speech advocates, who believe that Congress shouldn’t regulate Web content or try to force websitesto police themselves more rigorously...
Robust04 What were the causes for the Islamic Revolutionrelative to relations with the U.S.?<Paragraph>BFN [Editorial: "Sow the Wind and Reap the Whirlwind"] Yesterday marked the 14thanniversary of severing of diplomatic relations between the Islamic Republic and the United States ofAmerica. Several occasions arose in the last decade and a half for improving Irano-American relations...
Touché-2020Should the government allow illegal immigrantsto become citizens? <Title>America should support blanket amnesty for illegal immigrants.<Paragraph>Undocumentedworkers do not receive full Social Security beneﬁts because they are not United States citizens " norshould they be until they seek citizenship legally. Illegal immigrants are legally obligated to pay taxes...
CQADupStackCommand to display ﬁrst few and last few linesof a ﬁle <Title>Combing head and tail in a single call via pipe<Paragraph>On a regular basis, I am piping theoutput of some program to either ‘head‘ or ‘tail‘. Now, suppose that I want to see the ﬁrst AND last 10lines of piped output, such that I could do something like ./lotsofoutput | headtail...
Quora How long does it take to methamphetamine outof your blood? <Paragraph>How long does it take the body to get rid of methamphetamine?
DBPedia Paul Auster novels <Title>The New York Trilogy<Paragraph>The New York Trilogy is a series of novels by Paul Auster.Originally published sequentially as City of Glass (1985), Ghosts (1986) and The Locked Room (1986),it has since been collected into a single volume.
SCIDOCS CFD Analysis of Convective Heat Transfer Co-efﬁcient on External Surfaces of Buildings<Title>Application of CFD in building performance simulation for the outdoor environment: an overview<Paragraph>This paper provides an overview of the application of CFD in building performancesimulation for the outdoor environment, focused on four topics...
FEVER DodgeBall: A True Underdog Story is an Amer-ican movie from 2004 <Title>DodgeBall: A True Underdog Story<Paragraph>DodgeBall: A True Underdog Story is a2004 American sports comedy ﬁlm written and directed by Rawson Marshall Thurber and starring VinceVaughn and Ben Stiller. The ﬁlm follows friends who enter a dodgeball tournament...
Climate-FEVERSea level rise is now increasing faster than pre-dicted due to unexpectedly rapid ice melting.<Title>Sea level rise<Paragraph>A sea level rise is an increase in the volume of water in the world ’soceans, resulting in an increase in global mean sea level. The rise is usually attributed to global climatechange by thermal expansion of the water in the oceans and by melting of Ice sheets and glaciers...
Table 8: Examples of queries and relevant documents for all datasets included in BEIR . ( <Title>) and
(<Paragraph>) are used to distinguish the title separately from the paragraph within a document in the table
above. These tokens were not passed to the respective models.
23



Source: data\tc18_2309.07597v5\referenced_papers\[32]_2202.08904.pdf (Page 13):

A Additional results
Ranking ( →) Re-rank Top 0 Re-rank Top 10 Re-rank Top 100
Model ( →) [41] SGPT-CE SGPT-CE
Dataset ( ↓) BM25 125M 1.3B Bound 125M 1.3B Bound
MS MARCO 0.228 0.237 0.245 0.383 0.232 0.267 0.664
TREC-COVID 0.688 0.695 0.694 0.750 0.693 0.735 0.988
BioASQ 0.488 0.507 0.514 0.588 0.511 0.528 0.798
NFCorpus 0.306 0.314 0.316 0.364 0.298 0.327 0.513
NQ 0.326 0.336 0.354 0.514 0.319 0.367 0.788
HotpotQA 0.602 0.633 0.645 0.690 0.658 0.688 0.808
FiQA-2018 0.254 0.281 0.297 0.363 0.280 0.340 0.595
Signal-1M (RT) 0.330 0.339 0.343 0.390 0.307 0.322 0.619
TREC-NEWS 0.405 0.400 0.402 0.492 0.393 0.443 0.831
Robust04 0.425 0.419 0.434 0.508 0.382 0.427 0.854
ArguAna 0.472 0.394 0.383 0.754 0.315 0.299 0.952
Touché-2020 0.347 0.340 0.335 0.467 0.268 0.261 0.881
CQADupStack 0.326 0.348 0.360 0.426 0.357 0.394 0.637
Quora 0.808 0.794 0.809 0.914 0.764 0.791 0.982
DBPedia 0.320 0.328 0.336 0.397 0.329 0.356 0.651
SCIDOCS 0.165 0.173 0.177 0.245 0.171 0.185 0.465
FEVER 0.649 0.735 0.718 0.824 0.762 0.729 0.931
Climate-FEVER 0.186 0.194 0.191 0.281 0.179 0.167 0.474
SciFact 0.611 0.626 0.645 0.729 0.621 0.652 0.825
Average 0.428 0.436 0.442 0.539 0.423 0.445 0.755
Table 7: Additional SGPT Cross-Encoder scores on BEIR. Bounds are the maximum achievable score,
given the ﬁrst-stage BM25 results. We report additional Max Re-rank=10 scores using OpenAI’s
search endpoint: TREC-COVID: 0.545 (Ada), 0.539 (Davinci); SciFact: 0.670 (Ada), 0.658 (Davinci).
Scores are nDCG@10. Average scores do not include MS MARCO.
14



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 14):

Preprint
(a) 104861: interior ﬂooring cost.
 (b) 833860: popular Swiss food
Query
Relevant
ANCE Neg
BM25 Neg
Rand Neg (c) 1106007: deﬁne visceral
Figure 6: t-SNE Plots for Winning Cases in Table 8.
Table 9: Queries in the TREC 2019 DL Track Document Ranking Tasks where ANCE performs
worse than BM25. Snippets are manually extracted. The documents in the ﬁrst position where BM25
wins are shown. The NDCG@10 of ANCE and BM25 in the corresponding query is listed. Typos in
the query are from the real web search queries in TREC.
ANCE BM25
Query: qid (182539): Example of monotonic function
Title: Wikipedia: Monotonic function Explain Extended: Things SQL needs:
sargability of monotonic functions
DocNo: D510209 D175960
Snippet: In mathematics, a monotonic function
(or monotone function) is a function be-
tween ordered sets that preserves or re-
verses the given order... For example, if
y=g(x) is strictly monotonic on the range
[a,b] . . .
I’m going to write a series of articles
about the things SQL needs to work
faster and more efﬁcienly. . .
Ranking Position: 1 1
TREC Label: 0 (Irrelevant) 2 (Relevant)
NDCG@10: 0.25 0.61
Query: qid (1117099): What is a active margin
Title: Wikipedia: Margin (ﬁnance) Yahoo Answer: What is the difference
between passive and active continental
margins
DocNo: D166625 D2907204
Snippet: In ﬁnance, margin is collateral that the
holder of a ﬁnancial instrument . . .
An active continental margin is found on
the leading edge of the continent where
. . .
Ranking Position: 2 2
TREC Label: 0 (Irrelevant) 3 (Very Relevant)
NDCG@10: 0.44 0.74
Query: qid (1132213): How long to hold bow in yoga
Title: Yahoo Answer: How long should you
hold a yoga pose for
yogaoutlet.com: How to do bow pose in
yoga
DocNo: D3043610 D3378723
Snippet: so i’ve been doing yoga for a few weeks
now and already notice that my ﬂexi-
ablity has increased drastically. . . . That
depends on the posture itself . . .
Bow Pose is an intermediate yoga back-
bend that deeply opens the chest and the
front of the body. . . Hold for up to 30
seconds . . .
Ranking Position: 3 3
TREC Label: 0 (Irrelevant) 3 (Very Relevant)
NDCG@10: 0.66 0.74
The losing cases in Table 9 are also quite interesting. Many times we found that it is not that DR fails completely
and retrieves documents not related to the query’s information needs at all, which was a big concern when we
started research in DR. The errors ANCE made include retrieving documents that are related just not exactly
relevant to the query, for example, “yoga pose” for “bow in yoga”. In other cases, ANCE retrieved wrong
documents due to the lack of the domain knowledge: the pretrained language model may not know “active
15



### Claim 36/38

#### Claim Text
The incompressibility constraint is therefore a much stronger constraint than the CFL stability condition |u|∆t/∆ < 1 that usually applies to explicit schemes.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[18]_2101.06983.pdf (Page 1):

Noise Contrastive Estimation (NCE) was later used
by Word2Vec (Mikolov et al., 2013) to learn word
embedding. Recent works use contrastive learning
to unsupervisedly pre-train (Lee et al., 2019; Chang
et al., 2020) as well as supervisedly train dense re-
triever (Karpukhin et al., 2020), where contrastive
loss is used to estimate retrieval probability over
the entire corpus. Inspired by SimCLR (Chen et al.,
2020), constrastive learning is used to learn better
sentence representation (Giorgi et al., 2020) and
pre-trained language model (Wu et al., 2020).
Deep Network Memory Reduction Many ex-
isting techniques deal with large and deep mod-
els. The gradient checkpoint method attempts to
emulate training deep networks by training shal-
lower layers and connecting them with gradient
checkpoints and re-computation (Chen et al., 2016).
Some methods also use reversible activation func-
tions, allowing internal activation in the network to
be recovered throughout back propagation (Gomez
et al., 2017; MacKay et al., 2018). However, their
effectiveness as part of contrastive encoders has
not been conﬁrmed. Recent work also attempts
to remove the redundancy in optimizer tracked pa-
rameters on each GPU (Rajbhandari et al., 2020).
Compared with the aforementioned methods, our
method is designed for scaling over the batch size
dimension for contrastive learning.
3 Methodologies
In this section, we formally introduce the notations
for contrastive loss and analyze the difﬁculties of
using it on limited hardware. We then show how
we can use a Gradient Cache technique to factor
the loss so that large batch gradient update can be
broken into several sub-updates.
3.1 Preliminaries
Under a general formulation, given two classes of
data S,T, we want to learn encoders f and gfor
each such that, given s∈S,t ∈T , encoded repre-
sentations f(s) and g(t) are close if related and far
apart if not related by some distance measurement.
For large Sand T and deep neural network based
f and g, direct training is not tractable, so a com-
mon approach is to use a contrastive loss: sample
anchors S ⊂S and targets T ⊂T as a training
batch, where each element si ∈S has a related
element tri ∈T as well as zero or more specially
sampled hard negatives. The rest of the random
samples in T will be used as in-batch negatives.
Deﬁne loss based on dot product as follows:
L= −1
|S|
∑
si∈S
log exp(f(si)⊤g(tri )/τ)∑
tj ∈T exp(f(si)⊺g(tj)/τ)
(1)
where each summation term depends on the entire
set T and requires ﬁtting all of them into memory.
We set temperature τ = 1 in the following dis-
cussion for simplicity as in general it only adds a
constant multiplier to the gradient.
3.2 Analysis of Computation
In this section, we give a mathematical analysis
of contrastive loss computation and its gradient.
We show that the back propagation process can be
divided into two parts, from loss to representation,
and from representation to encoder model. The
separation then enables us to devise a technique
that removes data dependency in encoder parameter
update. Suppose the function f is parameterized
with Θ and gis parameterized with Λ.
∂L
∂Θ =
∑
si∈S
∂L
∂f(si)
∂f(si)
∂Θ (2)
∂L
∂Λ =
∑
tj ∈T
∂L
∂g(tj)
∂g(tj)
∂Λ (3)
As an extra notation, denote normalized similarity,
pij = exp(f(si)⊺g(tj))∑
t∈T exp(f(si)⊺g(t)) (4)
We note that the summation term for a particularsi
or ti is a function of the batch, as,
∂L
∂f(si) = −1
|S|

g(tri ) −
∑
tj ∈T
pijg(tj)

, (5)
∂L
∂g(tj) = −1
|S|

ϵj −
∑
si∈S
pijf(si)

, (6)
where
ϵj =
{
f(sk) if ∃ks.t. rk = j
0 otherwise (7)
which prohibits the use of gradient accumulation.
We make two observations here:
• The partial derivative ∂f (si)
∂Θ depends only on
si and Θ while ∂g(tj )
∂Λ depends only on tj and
Λ; and



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 2):

Preprint
instances, which have to be sampled in training:
θ∗= argminθ
∑
q
∑
d+∈D+
∑
d−∈ˆD−
l(f(q,d+),f(q,d−)). (3)
A natural choice is to sample negatives ˆD−from top documents retrieved by BM25. However, they
may bias the DR model to merely learn sparse retrieval and do not elevate DR models much beyond
BM25 (Luan et al., 2020). Another way is to sample negatives in local mini-batches, e.g., as in
contrastive learning (Oord et al., 2018; Chen et al., 2020a), however, these local negatives do not
signiﬁcantly outperform BM25 negatives (Karpukhin et al., 2020; Luan et al., 2020).
3 A NALYSES ON THE CONVERGENCE OF DENSE RETRIEVAL TRAINING
In this section, we provide theoretical analyses on the convergence of representation training in dense
retrieval. We ﬁrst show the connections between learning convergence and gradient norms, then the
bounded gradient norms by uninformative negatives, and ﬁnally, how in-batch local negatives are
ineffective under common conditions in dense retrieval.
Convergence Rate and Gradient Norms: Let l(d+,d−) = l(f(q,d+),f(q,d−) be the loss func-
tion on the training triple (q,d+,d−), PD− the negative sampling distribution for the given (q,d+),
and pd− the sampling probability of negative instance d−, a stochastic gradient decent (SGD) step
with importance sampling (Alain et al., 2015) is:
θt+1 = θt −η 1
Npd−
∇θtl(d+,d−), (4)
with θt the parameter at t-th step, θt+1 the one after, and N the total number of negatives. The scaling
factor 1
Npd−
is to make sure Eqn. 4 is an unbiased estimator of the full gradient.
Then we can characterize the converge rate of this SGD step as the movement to optimalθ∗. Following
derivations in variance reduction (Katharopoulos & Fleuret, 2018; Johnson & Guestrin, 2018), let
gd− = 1
Npd−
∇θtl(d+,d−) the weighted gradient, the convergence rate is:
E∆t = ||θt −θ∗||2 −EPD− (||θt+1 −θ∗||2) (5)
= ||θt||2 −2θT
t θ∗−EPD− (||θt −ηgd−||2) + 2θ∗TEPD− (θt −ηgd−) (6)
= −η2EPD− (||gd−||2) + 2ηθT
t EPD− (gd−) −2ηθ∗TEPD− (gd−) (7)
= 2ηEPD− (gd−)T(θt −θ∗) −η2EPD− (||gd−||2) (8)
= 2ηEPD− (gd−)T(θt −θ∗) −η2EPD− (gd−)TEPD− (gd−) −η2Tr(VPD− (gd−)). (9)
This shows we can obtain better convergence rate by sampling from a distributionPD− that minimizes
the variance of the gradient estimator,EPD− (||gd−||2), or Tr(VPD− (gd−)) as the estimator is unbiased.
There exists an optimal distribution that:
p∗
d− = argminpd− Tr(VPD− (gd−)) ∝||∇θtl(d+,d−)||2, (10)
which is to sample proportionally to per instance gradient norm. This is a well known result in
importance sampling (Alain et al., 2015; Johnson & Guestrin, 2018). It can be proved by applying
Jensen’s inequality on the gradient variance and then verifying that Eqn. 10 achieves the minimum.
We do not repeat this proof and refer to Johnson & Guestrin (2018) for exact derivations.
Intuitively, an negative instance with larger gradient norm is more likely to reduce the training loss
more, while those with diminishing gradients are not informative. Empirically, the correlation of
gradient norm and training convergence is also observed in BERT ﬁne-tuning (Mosbach et al., 2020).
Diminishing Gradients of Uninformative Negatives: The oracle distribution in Eqn. 10 is too
expensive to compute and the closed form of gradient norms can be complicated in deep neural
networks. Nevertheless, for MLP networks, Katharopoulos & Fleuret (2018) derives an upper bound
of the per sample gradient norm:
||∇θtl(d+,d−)||2 ≤Lρ||∇φLl(d+,d−)||2, (11)
3



Source: data\tc18_2309.07597v5\referenced_papers\[18]_2101.06983.pdf (Page 4):

twice the amount of time than accumulation3.
5 Extend to Deep Distance Function
Previous discussion assumes a simple parameter-
less dot product similarity. In general it can also be
deep distance function Φ richly parameterized by
Ω, formally,
dij = d(si,tj) = Φ(f(si),g(tj)) (10)
This can still scale by introducing an extraDistance
Gradient Cache. In the ﬁrst forward we collect
all representations as well as all distances. We
compute loss with dijs and back propagate to get
wij = ∂L
∂dij
, and store them in Distance Gradient
Cache, [w00,w01,..,w 10,..]. We can then update
Ω in a sub-batch manner,
∂L
∂Ω =
∑
ˆS∈S
∑
ˆT∈T
∑
si∈ˆS
∑
tj ∈ ˆT
wij
∂Φ(f(si),g(tj))
∂Ω
(11)
Additionally, we simultaneously compute with the
constructed computation graph ∂dij
∂f (si) and ∂dij
∂g(tj )
and accumulate across batches,
ui = ∂L
∂f(si) =
∑
j
wij
∂dij
∂f(si) (12)
and,
vj = ∂L
∂g(tj) =
∑
i
wij
∂dij
∂g(tj) (13)
with which we can build up the Representation
Gradient Cache. When all representations’ gra-
dients are computed and stored, encoder gradi-
ent can be computed with Step3 described in sub-
section 3.3. In philosophy this method links up
two caches. Note this covers early interaction
f(s) = s,g(t) = tas a special case.
6 Conclusion
In this paper, we introduce a gradient cache tech-
nique that breaks GPU memory limitations for
large batch contrastive learning. We propose to con-
struct a representation gradient cache that removes
in-batch data dependency in encoder optimization.
Our method produces the exact same gradient up-
date as training with a large batch. We show the
3We used the gradient checkpoint implemented in Hug-
gingface transformers package
method is efﬁcient and capable of preserving accu-
racy on resource-limited hardware. We believe a
critical contribution of our work is providing a large
population in the NLP community with access to
batch-wise contrastive learning. While many previ-
ous works come from people with industry-grade
hardware, researchers with limited hardware can
now use our technique to reproduce state-of-the-art
models and further advance the research without
being constrained by available GPU memory.
Acknowledgments
The authors would like to thank Zhuyun Dai and
Chenyan Xiong for comments on the paper, and
the anonymous reviewers for their reviews.



Source: data\tc18_2309.07597v5\referenced_papers\[18]_2101.06983.pdf (Page 0):

Scaling Deep Contrastive Learning Batch Size
under Memory Limited Setup
Luyu Gao1, Yunyi Zhang2, Jiawei Han2, Jamie Callan1
1 Language Technologies Institute, Carnegie Mellon University
2 Department of Computer Science, University of Illinois Urbana-Champaign
1{luyug, callan}@cs.cmu.edu 2{yzhan238, hanj}@illionis.edu
Abstract
Contrastive learning has been applied suc-
cessfully to learn vector representations of
text. Previous research demonstrated that
learning high-quality representations beneﬁts
from batch-wise contrastive loss with a large
number of negatives. In practice, the technique
of in-batch negative is used, where for each ex-
ample in a batch, other batch examples’ pos-
itives will be taken as its negatives, avoiding
encoding extra negatives. This, however, still
conditions each example’s loss on all batch
examples and requires ﬁtting the entire large
batch into GPU memory. This paper intro-
duces a gradient caching technique that decou-
ples backpropagation between contrastive loss
and the encoder, removing encoder backward
pass data dependency along the batch dimen-
sion. As a result, gradients can be computed
for one subset of the batch at a time, leading to
almost constant memory usage. 1
1 Introduction
Contrastive learning learns to encode data into an
embedding space such that related data points have
closer representations and unrelated ones have fur-
ther apart ones. Recent works in NLP adopt deep
neural nets as encoders and use unsupervised con-
trastive learning on sentence representation (Giorgi
et al., 2020), text retrieval (Lee et al., 2019),
and language model pre-training tasks (Wu et al.,
2020). Supervised contrastive learning (Khosla
et al., 2020) has also been shown effective in train-
ing dense retrievers (Karpukhin et al., 2020; Qu
et al., 2020). These works typically use batch-wise
contrastive loss, sharing target texts as in-batch
negatives. With such a technique, previous works
have empirically shown that larger batches help
learn better representations. However, computing
loss and updating model parameters with respect
1Our code is at github.com/luyug/GradCache.
to a big batch require encoding all batch data and
storing all activation, so batch size is limited by to-
tal available GPU memory. This limits application
and research of contrastive learning methods under
memory limited setup, e.g. academia. For example,
Lee et al. (2019) pre-train a BERT (Devlin et al.,
2019) passage encoder with a batch size of 4096
while a high-end commercial GPU RTX 2080ti can
only ﬁt a batch of 8. The gradient accumulation
technique, splitting a large batch into chunks and
summing gradients across several backwards, can-
not emulate a large batch as each smaller chunk
has fewer in-batch negatives.
In this paper, we present a simple technique
that thresholds peak memory usage for contrastive
learning to almost constant regardless of the batch
size. For deep contrastive learning, the memory
bottlenecks are at the deep neural network based
encoder. We observe that we can separate the back-
propagation process of contrastive loss into two
parts, from loss to representation, and from repre-
sentation to model parameter, with the latter being
independent across batch examples given the for-
mer, detailed in subsection 3.2. We then show in
subsection 3.3 that by separately pre-computing
the representations’ gradient and store them in a
cache, we can break the update of the encoder into
multiple sub-updates that can ﬁt into the GPU mem-
ory. This pre-computation of gradients allows our
method to produce the exact samegradient update
as training with large batch. Experiments show that
with about 20% increase in runtime, our technique
enables a single consumer-grade GPU to reproduce
the state-of-the-art large batch trained models that
used to require multiple professional GPUs.
2 Related Work
Contrastive Learning First introduced for prob-
ablistic language modeling (Mnih and Teh, 2012),
arXiv:2101.06983v2  [cs.LG]  14 Jun 2021



Source: data\tc18_2309.07597v5\referenced_papers\[18]_2101.06983.pdf (Page 2):

• Computing partial derivatives ∂L
∂f (si) and
∂L
∂g(tj ) requires only encoded representations,
but not Θ or Λ.
These observations mean back propagation of
f(si) for data si can be run independently with
its own computation graph and activation if the
numerical value of the partial derivative ∂L
∂si
is
known. Meanwhile the derivation of ∂L
∂si
requires
only numerical values of two sets of representa-
tion vectors F = {f(s1),f(s2),..,f (s|S|)}and
G = {g(t1),g(t2),...,g (t|T|)}. A similar argu-
ment holds true for g, where we can use represen-
tation vectors to compute ∂L
∂tj
and back propagate
for each g(tj) independently. In the next section,
we will describe how to scale up batch size by pre-
computing these representation vectors.
3.3 Gradient Cache Technique
Given a large batch that does not ﬁt into the avail-
able GPU memory for training, we ﬁrst divide it
into a set of sub-batches each of which can ﬁt
into memory for gradient computation, denoted as
S = {ˆS1, ˆS2,..},T = {ˆT1, ˆT2,..}. The full-batch
gradient update is computed by the following steps.
Step1: Graph-less Forward Before gradient
computation, we ﬁrst run an extra encoder forward
pass for each batch instance to get its representa-
tion. Importantly, this forward pass runs without
constructing the computation graph. We collect
and store all representations computed.
Step2: Representation Gradient Computation
and Caching We then compute the contrastive
loss for the batch based on the representation from
Step1 and have a corresponding computation graph
constructed. Despite the mathematical derivation,
automatic differentiation system is used in actual
implementation, which automatically supports vari-
ations of contrastive loss. A backward pass is
then run to populate gradients for each represen-
tation. Note that the encoder is not included in
this gradient computation. Let ui = ∂L
∂f (si) and
vi = ∂L
∂g(ti) , we take these gradient tensors and
store them as a Representation Gradient Cache,
[u1,u2,.., v1,v2,..].
Step3: Sub-batch Gradient AccumulationWe
run encoder forward one sub-batch at a time to
compute representations and build the correspond-
ing computation graph. We take the sub-batch’s
representation gradients from the cache and run
back propagation through the encoder. Gradients
are accumulated for encoder parameters across all
sub-batches. Effectively for f we have,
∂L
∂Θ =
∑
ˆSj ∈S
∑
si∈ˆSj
∂L
∂f(si)
∂f(si)
∂Θ
=
∑
ˆSj ∈S
∑
si∈ˆSj
ui
∂f(si)
∂Θ
(8)
where the outer summation enumerates each sub-
batch and the entire internal summation corre-
sponds to one step of accumulation. Similarly, for
g, gradients accumulate based on,
∂L
∂Λ =
∑
ˆTj ∈T
∑
ti∈ ˆTj
vi
∂g(ti)
∂Λ (9)
Here we can see the equivalence with direct large
batch update by combining the two summations.
Step4: Optimization When all sub-batches are
processed, we can step the optimizer to update
model parameters as if the full batch is processed
in a single forward-backward pass.
Compared to directly updating with the full
batch, which requires memory linear to the number
of examples, our method ﬁxes the number of exam-
ples in each encoder gradient computation to be the
size of sub-batch and therefore requires constant
memory for encoder forward-backward pass. The
extra data pieces introduced by our method that re-
main persistent across steps are the representations
and their corresponding gradients with the former
turned into the latter after representation gradient
computation. Consequently, in a general case with
data from Sand T each represented with ddimen-
sion vectors, we only need to store (|S|d+ |T|d)
ﬂoating points in the cache on top of the computa-
tion graph. To remind our readers, this is several
orders smaller than million-size model parameters.
3.4 Multi-GPU Training
When training on multiple GPUs, we need to com-
pute the gradients with all examples across all
GPUs. This requires a single additional cross GPU
communication after Step1 when all representa-
tions are computed. We use an all-gather opera-
tion to make all representations available on all
GPUs. Denote Fn,Gn representations on n-th
GPU and a total of N device. Step2 runs with
gathered representations Fall = F1 ∪..∪FN and
Gall = G1 ∪..∪GN . While Fall and Gall are used



### Claim 37/38

#### Claim Text
Considering that awareness is an important factor influencing vaccination, Kabir et al. proposed a framework for vaccine uptake with the unaware-aware (UA) information propagation.

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[25]_2004.04906.pdf (Page 0):

Dense Passage Retrieval for Open-Domain Question Answering
Vladimir Karpukhin∗, Barlas O˘guz∗, Sewon Min†, Patrick Lewis,
Ledell Wu, Sergey Edunov, Danqi Chen‡, Wen-tau Yih
Facebook AI †University of Washington ‡Princeton University
{vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com
sewon@cs.washington.edu
danqic@cs.princeton.edu
Abstract
Open-domain question answering relies on ef-
ﬁcient passage retrieval to select candidate
contexts, where traditional sparse vector space
models, such as TF-IDF or BM25, are the de
facto method. In this work, we show that
retrieval can be practically implemented us-
ing dense representations alone, where em-
beddings are learned from a small number
of questions and passages by a simple dual-
encoder framework. When evaluated on a
wide range of open-domain QA datasets, our
dense retriever outperforms a strong Lucene-
BM25 system greatly by 9%-19% absolute in
terms of top-20 passage retrieval accuracy, and
helps our end-to-end QA system establish new
state-of-the-art on multiple open-domain QA
benchmarks.1
1 Introduction
Open-domain question answering (QA) (V oorhees,
1999) is a task that answers factoid questions us-
ing a large collection of documents. While early
QA systems are often complicated and consist of
multiple components (Ferrucci (2012); Moldovan
et al. (2003), inter alia), the advances of reading
comprehension models suggest a much simpliﬁed
two-stage framework: (1) a context retriever ﬁrst
selects a small subset of passages where some
of them contain the answer to the question, and
then (2) a machine reader can thoroughly exam-
ine the retrieved contexts and identify the correct
answer (Chen et al., 2017). Although reducing
open-domain QA to machine reading is a very rea-
sonable strategy, a huge performance degradation
is often observed in practice2, indicating the needs
of improving retrieval.
∗Equal contribution
1The code and trained models have been released at
https://github.com/facebookresearch/DPR.
2For instance, the exact match score on SQuAD v1.1 drops
from above 80% to less than 40% (Yang et al., 2019a).
Retrieval in open-domain QA is usually imple-
mented using TF-IDF or BM25 (Robertson and
Zaragoza, 2009), which matches keywords efﬁ-
ciently with an inverted index and can be seen
as representing the question and context in high-
dimensional, sparse vectors (with weighting). Con-
versely, the dense, latent semantic encoding is com-
plementary to sparse representations by design. For
example, synonyms or paraphrases that consist of
completely different tokens may still be mapped to
vectors close to each other. Consider the question
“Who is the bad guy in lord of the rings?”, which can
be answered from the context “Sala Baker is best
known for portraying the villain Sauron in the Lord
of the Rings trilogy. ”A term-based system would
have difﬁculty retrieving such a context, while
a dense retrieval system would be able to better
match “bad guy” with “villain” and fetch the cor-
rect context. Dense encodings are also learnable
by adjusting the embedding functions, which pro-
vides additional ﬂexibility to have a task-speciﬁc
representation. With special in-memory data struc-
tures and indexing schemes, retrieval can be done
efﬁciently using maximum inner product search
(MIPS) algorithms (e.g., Shrivastava and Li (2014);
Guo et al. (2016)).
However, it is generally believed that learn-
ing a good dense vector representation needs a
large number of labeled pairs of question and con-
texts. Dense retrieval methods have thus never
be shown to outperform TF-IDF/BM25 for open-
domain QA before ORQA (Lee et al., 2019), which
proposes a sophisticated inverse cloze task (ICT)
objective, predicting the blocks that contain the
masked sentence, for additional pretraining. The
question encoder and the reader model are then ﬁne-
tuned using pairs of questions and answers jointly.
Although ORQA successfully demonstrates that
dense retrieval can outperform BM25, setting new
state-of-the-art results on multiple open-domain
arXiv:2004.04906v3  [cs.CL]  30 Sep 2020



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 0):

BEIR: A Heterogeneous Benchmark for Zero-shot
Evaluation of Information Retrieval Models
Nandan Thakur, Nils Reimers, Andreas Rücklé∗, Abhishek Srivastava, Iryna Gurevych
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universität Darmstadt
www.ukp.tu-darmstadt.de
Abstract
Existing neural information retrieval (IR) models have often been studied in ho-
mogeneous and narrow settings, which has considerably limited insights into their
out-of-distribution (OOD) generalization capabilities. To address this, and to facili-
tate researchers to broadly evaluate the effectiveness of their models, we introduce
Benchmarking-IR (BEIR ), a robust and heterogeneous evaluation benchmark for
information retrieval. We leverage a careful selection of 18 publicly available
datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-
art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking
architectures on the BEIR benchmark. Our results show BM25 is a robust baseline
and re-ranking and late-interaction based models on average achieve the best zero-
shot performances, however, at high computational costs. In contrast, dense and
sparse-retrieval models are computationally more efﬁcient but often underperform
other approaches, highlighting the considerable room for improvement in their
generalization capabilities. We hope this framework allows us to better evaluate
and understand existing retrieval systems, and contributes to accelerating progress
towards better robust and generalizable systems in the future. BEIR is publicly
available at https://github.com/UKPLab/beir.
1 Introduction
Major natural language processing (NLP) problems rely on a practical and efﬁcient retrieval com-
ponent as a ﬁrst step to ﬁnd relevant information. Challenging problems include open-domain
question-answering [8], claim-veriﬁcation [60], duplicate question detection [78], and many more.
Traditionally, retrieval has been dominated by lexical approaches like TF-IDF or BM25 [55]. How-
ever, these approaches suffer from lexical gap [5] and are able to only retrieve documents containing
keywords present within the query. Further, lexical approaches treat queries and documents as
bag-of-words by not taking word ordering into consideration.
Recently, deep learning and in particular pre-trained Transformer models like BERT [ 12] have
become popular in information retrieval [37]. These neural retrieval systems can be used in many
fundamentally different ways to improve retrieval performance. We provide an brief overview of the
systems in Section 2.1. Many prior work train neural retrieval systems on large datasets like Natural
Questions (NQ) [34] (133k training examples) or MS MARCO [45] (533k training examples), which
both focus on passage retrieval given a question or short keyword-based query. In most prior work,
approaches are afterward evaluated on the same dataset, where signiﬁcant performance gains over
lexical approaches like BM25 are demonstrated [15, 31, 46].
However, creating a large training corpus is often time-consuming and expensive and hence many
retrieval systems are applied in a zero-shot setup, with no available training data to train the system.
∗Contributions made prior to joining Amazon.
Preprint. Under review.
arXiv:2104.08663v4  [cs.IR]  21 Oct 2021



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 19):

FEVER [60] The Fact Extraction and VERiﬁcation dataset is collected to facilitate the automatic
fact checking. We utilize the original paper splits as queries Q and retrieve evidences from the
pre-processed Wikipedia Abstracts (June 2017 dump) as our corpus T.
Climate-FEVER [14] is a dataset for veriﬁcation of real-world climate claims. We include the
original dataset claims as queries Q and retrieve evidences from the same FEVER Wiki corpusT.
We manually included few Wikipedia articles (25) missing from our corpus, but present within our
relevance judgements.
SciFact [68] veriﬁes scientiﬁc claims using evidence from the research literature containing scientiﬁc
paper abstracts. We use the original publicly available dev split from the task containing 300 queries
as our test queries Q, and include all documents from the original dataset as our corpus T.
E Dataset Licenses
The authors of 4 out of the 19 datasets in the BEIR benchmark (NFCorpus, FiQA-2018, Quora,
Climate-Fever) do not report the dataset license in the paper or a repository; We overview the rest:
• MSMARCO: Provided under “MIT License” for non-commercial research purposes.
• FEVER, NQ, DBPedia, Signal-1M: All provided under CC BY-SA 3.0 license.
• TREC-NEWS, Robust04, BioASQ: Data collection archives are under Copyright.
• ArguAna, Touché-2020: Provided under CC BY 4.0 license.
• CQADupStack: Provided under Apache License 2.0 license.
• SciFact: Provided under the CC BY-NC 2.0 license.
• SCIDOCS: Provided under the GNU General Public License v3.0 license.
• HotpotQA: Provided under the CC BY-SA 4.0 license.
• TREC-COVID: Provided under the “Dataset License Agreement”.
F Weighted Jaccard Similarity
The weighted Jaccard similarity J(S,T) [26] is intuitively calculated as the unique word overlap for
all words present in both the datasets. More formally, the normalized frequency for an unique word k
in a dataset is calculated as the frequency of word kdivided over the sum of frequencies of all words
in the dataset.
Sk is the normalized frequency of word kin the source dataset S and Tk for the target dataset T
respectively. The weighted Jaccard similarity between Sand T is deﬁned as:
J(S,T) =
∑
k min(Sk,Tk)∑
k max(Sk,Tk)
where the sum is over all unique words kpresent in datasets Sand T.
G Capped Recall@k Score
Recall at k is calculated as the fraction of the relevant documents that are successfully retrieved
within the top kextracted documents. More formally, the R@kscore is calculated as:
R@k= 1
|Q|
|Q|∑
i=1
|maxk(Ai) ∩A⋆
i |
|A⋆
i |
where Qis the set of queries, A⋆
i is the set of relevant documents for the ith query, and Ai is a scored
list of documents provided by the model, from which top kare extracted.
However measuring recall can be counterintuitive, if a high number of relevant documents (>k) are
present within a dataset. For example, consider a hypothetical dataset with 500 relevant documents
for a query. Retrieving all relevant documents would produce a maximum R@100 score = 0.2, which
20



Source: data\tc18_2309.07597v5\referenced_papers\[25]_2004.04906.pdf (Page 8):

been used as the standard method applied broadly
to various QA tasks (e.g., Chen et al., 2017; Yang
et al., 2019a,b; Nie et al., 2019; Min et al., 2019a;
Wolfson et al., 2020). Augmenting text-based re-
trieval with external structured information, such
as knowledge graph and Wikipedia hyperlinks, has
also been explored recently (Min et al., 2019b; Asai
et al., 2020).
The use of dense vector representations for re-
trieval has a long history since Latent Semantic
Analysis (Deerwester et al., 1990). Using labeled
pairs of queries and documents, discriminatively
trained dense encoders have become popular re-
cently (Yih et al., 2011; Huang et al., 2013; Gillick
et al., 2019), with applications to cross-lingual
document retrieval, ad relevance prediction, Web
search and entity retrieval. Such approaches com-
plement the sparse vector methods as they can po-
tentially give high similarity scores to semantically
relevant text pairs, even without exact token match-
ing. The dense representation alone, however, is
typically inferior to the sparse one. While not the
focus of this work, dense representations from pre-
trained models, along with cross-attention mecha-
nisms, have also been shown effective in passage
or dialogue re-ranking tasks (Nogueira and Cho,
2019; Humeau et al., 2020). Finally, a concurrent
work (Khattab and Zaharia, 2020) demonstrates
the feasibility of full dense retrieval in IR tasks.
Instead of employing the dual-encoder framework,
they introduced a late-interaction operator on top
of the BERT encoders.
Dense retrieval for open-domain QA has been
explored by Das et al. (2019), who propose to re-
trieve relevant passages iteratively using reformu-
lated question vectors. As an alternative approach
that skips passage retrieval, Seo et al. (2019) pro-
pose to encode candidate answer phrases as vectors
and directly retrieve the answers to the input ques-
tions efﬁciently. Using additional pretraining with
the objective that matches surrogates of questions
and relevant passages, Lee et al. (2019) jointly train
the question encoder and reader. Their approach
outperforms the BM25 plus reader paradigm on
multiple open-domain QA datasets in QA accuracy,
and is further extended by REALM (Guu et al.,
2020), which includes tuning the passage encoder
asynchronously by re-indexing the passages dur-
ing training. The pretraining objective has also
recently been improved by Xiong et al. (2020b).
In contrast, our model provides a simple and yet
effective solution that shows stronger empirical per-
formance, without relying on additional pretraining
or complex joint training schemes.
DPR has also been used as an important mod-
ule in very recent work. For instance, extending
the idea of leveraging hard negatives, Xiong et al.
(2020a) use the retrieval model trained in the pre-
vious iteration to discover new negatives and con-
struct a different set of examples in each training
iteration. Starting from our trained DPR model,
they show that the retrieval performance can be
further improved. Recent work (Izacard and Grave,
2020; Lewis et al., 2020b) have also shown that
DPR can be combined with generation models
such as BART (Lewis et al., 2020a) and T5 (Raf-
fel et al., 2019), achieving good performance on
open-domain QA and other knowledge-intensive
tasks.
8 Conclusion
In this work, we demonstrated that dense retrieval
can outperform and potentially replace the tradi-
tional sparse retrieval component in open-domain
question answering. While a simple dual-encoder
approach can be made to work surprisingly well,
we showed that there are some critical ingredients
to training a dense retriever successfully. Moreover,
our empirical analysis and ablation studies indicate
that more complex model frameworks or similarity
functions do not necessarily provide additional val-
ues. As a result of improved retrieval performance,
we obtained new state-of-the-art results on multiple
open-domain question answering benchmarks.
Acknowledgments
We thank the anonymous reviewers for their helpful
comments and suggestions.
References
Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,
Richard Socher, and Caiming Xiong. 2020. Learn-
ing to retrieve reasoning paths over Wikipedia graph
for question answering. In International Conference
on Learning Representations (ICLR).
Petr Baudi ˇs and Jan ˇSediv`y. 2015. Modeling of the
question answering task in the yodaqa system. In In-
ternational Conference of the Cross-Language Eval-
uation Forum for European Languages, pages 222–
228. Springer.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 1):

Fact Checking
Citation-Prediction
W iki 
FEVER
QUERY
DOCS
Natural Claim
Wikipedia Articles
W iki 
Climate-FEVER
QUERY
DOCS
Climate-based Claim
Wikipedia Articles
SciFact
QUERY
DOCS
Scientific claim
PubMed ArticlesScientific 
SCIDOCS
QUERY
DOCS
Article Title
PubMed ArticlesScientific 
Dup. Question Retrieval
Quora 
Quora
QUERY
DOCS
StackEx. 
CQADupStack
QUERY
DOCS
Argument Retrieval
Misc. 
QUERY
DOCS
Misc. 
ArguAna
QUERY
DOCS
Tóuche-2020
Query Title
Query Title + Body
Query Title
Quora Questions
Argument
Idebate Arguments
Args.me Arguments
News Retrieval
TREC-NEWS
QUERY
DOCS News ArticlesNews Tweet Retrieval
Signal-1M
QUERY
DOCS
News Headline
Twitter TweetsT witter 
Question-Answering
W iki 
NQ
QUERY
DOCS
W iki 
HotpotQA
QUERY
DOCS
FiQA-2018
QUERY
DOCSFinance 
Bio-Medical IR
QUERY
DOCS
Scientific 
BioASQ
QUERY
DOCS
NFCorpus
QUERY
DOCSScientific 
Entity Retrieval
DBPedia
QUERY
DOCS
Entity-based Query
DBPedia ArticlesW iki 
TREC-COVID
Scientific 
Wikipedia Articles
Wikipedia Articles
Natural Query
Multi-Hop Query
CORD-19 Articles
PubMed Articles
PubMed Articles
COVID-19 Query
Nutrition Facts
Bio-Medical Query
Financial Query
Investment Articles
Controversial Query
9 Tasks
18 Datasets
News Headline
Robust04
QUERY
DOCSNews 
 News Articles
News Query
Figure 1: An overview of the diverse tasks and datasets in BEIR benchmark.
So far, it is unclear how well existing trained neural models will perform for other text domains or
textual retrieval tasks. Even more important, it is unclear how well different approaches, like sparse
embeddings vs. dense embeddings, generalize to out-of-distribution data.
In this work, we present a novel robust and heterogeneous benchmark called BEIR (Benchmarking
IR), comprising of 18 retrieval datasets for comparison and evaluation of model generalization. Prior
retrieval benchmarks [19, 50] have issues of a comparatively narrow evaluation focusing either only
on a single task, like question-answering, or on a certain domain. In BEIR , we focus on Diversity, we
include nine different retrieval tasks: Fact checking, citation prediction, duplicate question retrieval,
argument retrieval, news retrieval, question answering, tweet retrieval, bio-medical IR, and entity
retrieval. Further, we include datasets from diverse text domains, datasets that cover broad topics (like
Wikipedia) and specialized topics (like COVID-19 publications), different text types (news articles vs.
Tweets), datasets of various sizes (3.6k - 15M documents), and datasets with different query lengths
(average query length between 3 and 192 words) and document lengths (average document length
between 11 and 635 words).
We use BEIR to evaluate ten diverse retrieval methods from ﬁve broad architectures: lexical, sparse,
dense, late interaction, and re-ranking. From our analysis, we ﬁnd that no single approach consistently
outperforms other approaches on all datasets. Further, we notice that the in-domain performance of a
model does not correlate well with its generalization capabilities: models ﬁne-tuned with identical
training data might generalize differently. In terms of efﬁciency, we ﬁnd a trade-off between the
performances and the computational cost: computationally expensive models, like re-ranking models
and late interaction model perform the best. More efﬁcient approaches e.g. based on dense or sparse
embeddings can substantially underperform traditional lexical models like BM25. Overall, BM25
remains a strong baseline for zero-shot text retrieval.
Finally, we notice that there can be a strong lexical bias present in datasets included within the
benchmark, likely as lexical models are pre-dominantly used during the annotation or creation of
datasets. This can give an unfair disadvantage to non-lexical approaches. We analyze this for the
TREC-COVID [65] dataset: We manually annotate the missing relevance judgements for the tested
systems and see a signiﬁcant performance improvement for non-lexical approaches. Hence, future
work requires better unbiased datasets that allow a fair comparison for all types of retrieval systems.
With BEIR , we take an important step towards a single and uniﬁed benchmark to evaluate the zero-shot
capabilities of retrieval systems. It allows to study when and why certain approaches perform well,
and hopefully steers innovation to more robust retrieval systems. We release BEIR and an integration
of diverse retrieval systems and datasets in a well-documented, easy to use and extensible open-source
package. BEIR is model-agnostic, welcomes methods of all kinds, and also allows easy integration of
new tasks and datasets. More details are available at https://github.com/UKPLab/beir.
2 Related Work and Background
To our knowledge, BEIR is the ﬁrst broad, zero-shot information retrieval benchmark. Existing works
[19, 50] do not evaluate retrieval in a zero-shot setting in depth, they either focus over a single task,
small corpora or on a certain domain. This setting hinders for investigation of model generalization
across diverse set of domains and task types. MultiReQA [19] consists of eight Question-Answering
(QA) datasets and evaluates sentence-level answer retrieval given a question. It only tests a single
task and ﬁve out of eight datasets are from Wikipedia. Further, MultiReQA evaluates retrieval over
rather small corpora: six out of eight tasks have less than 100k candidate sentences, which beneﬁts
dense retrieval over lexical as previously shown [54]. KILT [50] consists of ﬁve knowledge-intensive
2



### Claim 38/38

#### Claim Text
Notice that, in the frequency domain, equation (20) becomes the same as the non linear relation used in . 3.3 The chemoEH model We present the third and last model, the chemoEH model , based on the qualitative description made in .

#### Retrieved Documents
Source: data\tc18_2309.07597v5\referenced_papers\[18]_2101.06983.pdf (Page 1):

Noise Contrastive Estimation (NCE) was later used
by Word2Vec (Mikolov et al., 2013) to learn word
embedding. Recent works use contrastive learning
to unsupervisedly pre-train (Lee et al., 2019; Chang
et al., 2020) as well as supervisedly train dense re-
triever (Karpukhin et al., 2020), where contrastive
loss is used to estimate retrieval probability over
the entire corpus. Inspired by SimCLR (Chen et al.,
2020), constrastive learning is used to learn better
sentence representation (Giorgi et al., 2020) and
pre-trained language model (Wu et al., 2020).
Deep Network Memory Reduction Many ex-
isting techniques deal with large and deep mod-
els. The gradient checkpoint method attempts to
emulate training deep networks by training shal-
lower layers and connecting them with gradient
checkpoints and re-computation (Chen et al., 2016).
Some methods also use reversible activation func-
tions, allowing internal activation in the network to
be recovered throughout back propagation (Gomez
et al., 2017; MacKay et al., 2018). However, their
effectiveness as part of contrastive encoders has
not been conﬁrmed. Recent work also attempts
to remove the redundancy in optimizer tracked pa-
rameters on each GPU (Rajbhandari et al., 2020).
Compared with the aforementioned methods, our
method is designed for scaling over the batch size
dimension for contrastive learning.
3 Methodologies
In this section, we formally introduce the notations
for contrastive loss and analyze the difﬁculties of
using it on limited hardware. We then show how
we can use a Gradient Cache technique to factor
the loss so that large batch gradient update can be
broken into several sub-updates.
3.1 Preliminaries
Under a general formulation, given two classes of
data S,T, we want to learn encoders f and gfor
each such that, given s∈S,t ∈T , encoded repre-
sentations f(s) and g(t) are close if related and far
apart if not related by some distance measurement.
For large Sand T and deep neural network based
f and g, direct training is not tractable, so a com-
mon approach is to use a contrastive loss: sample
anchors S ⊂S and targets T ⊂T as a training
batch, where each element si ∈S has a related
element tri ∈T as well as zero or more specially
sampled hard negatives. The rest of the random
samples in T will be used as in-batch negatives.
Deﬁne loss based on dot product as follows:
L= −1
|S|
∑
si∈S
log exp(f(si)⊤g(tri )/τ)∑
tj ∈T exp(f(si)⊺g(tj)/τ)
(1)
where each summation term depends on the entire
set T and requires ﬁtting all of them into memory.
We set temperature τ = 1 in the following dis-
cussion for simplicity as in general it only adds a
constant multiplier to the gradient.
3.2 Analysis of Computation
In this section, we give a mathematical analysis
of contrastive loss computation and its gradient.
We show that the back propagation process can be
divided into two parts, from loss to representation,
and from representation to encoder model. The
separation then enables us to devise a technique
that removes data dependency in encoder parameter
update. Suppose the function f is parameterized
with Θ and gis parameterized with Λ.
∂L
∂Θ =
∑
si∈S
∂L
∂f(si)
∂f(si)
∂Θ (2)
∂L
∂Λ =
∑
tj ∈T
∂L
∂g(tj)
∂g(tj)
∂Λ (3)
As an extra notation, denote normalized similarity,
pij = exp(f(si)⊺g(tj))∑
t∈T exp(f(si)⊺g(t)) (4)
We note that the summation term for a particularsi
or ti is a function of the batch, as,
∂L
∂f(si) = −1
|S|

g(tri ) −
∑
tj ∈T
pijg(tj)

, (5)
∂L
∂g(tj) = −1
|S|

ϵj −
∑
si∈S
pijf(si)

, (6)
where
ϵj =
{
f(sk) if ∃ks.t. rk = j
0 otherwise (7)
which prohibits the use of gradient accumulation.
We make two observations here:
• The partial derivative ∂f (si)
∂Θ depends only on
si and Θ while ∂g(tj )
∂Λ depends only on tj and
Λ; and



Source: data\tc18_2309.07597v5\referenced_papers\[51]_2104.08663.pdf (Page 22):

Corpus Website (Link)
CORD-19 https://www.semanticscholar.org/cord19
NutritionFacts https://nutritionfacts.org
PubMed https://pubmed.ncbi.nlm.nih.gov
Signal-1M https://research.signal-ai.com/datasets/signal1m.html
TREC Washington Posthttps://ir.nist.gov/wapo/
TREC disks 4 and 5 https://trec.nist.gov/data/cd45/
Args.me https://zenodo.org/record/4139439/
DBPedia (2015-10) http://downloads.dbpedia.org/wiki-archive/Downloads2015-10.html
TREC-COVID (Annotated)https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid-beir.zip
Table 7: Corpus Name and Link used for datasets in BEIR .
Dataset Query Relevant-Document
MS MARCOwhat fruit is native to australia <Paragraph>Passiﬂora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned,white ﬂeshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, whileothers list the fruits as being bitter and inedible. assiﬂora herbertiana. A rare passion fruit native toAustralia...
TREC-COVIDwhat is the origin of COVID-19<Title>Origin of Novel Coronavirus (COVID-19): A Computational Biology Study using ArtiﬁcialIntelligence<Paragraph>Origin of the COVID-19 virus has been intensely debated in the community...
BioASQ What is the effect of HMGB2 loss on CTCFclustering <Title>HMGB2 Loss upon Senescence Entry Disrupts Genomic Organization and Induces CTCFClustering across Cell Types.<Paragraph>Processes like cellular senescence are characterized bycomplex events giving rise to heterogeneous cell populations. However, the early molecular eventsdriving this cascade remain elusive....
NFCorpus Titanium Dioxide & Inﬂammatory Bowel Dis-ease <Title>Titanium Dioxide Nanoparticles in Food and Personal Care Products<Paragraph>Titaniumdioxide is a common additive in many food, personal care, and other consumer products used by people,which after use can enter the sewage system, and subsequently enter the environment as treated efﬂuentdischarged to surface waters or biosolids applied to agricultural land, or incinerated wastes...
NQ when did they stop cigarette advertising on tele-vision? <Title>Tobacco advertising<Paragraph>The ﬁrst calls to restrict advertising came in 1962 from theRoyal College of Physicians, who highlighted the health problems and recommended stricter laws...
HotpotQA Stockely Webster has paintings hanging in whathome (that serves as the residence for the Mayorof New York)?
<Title>Stokely Webster<Paragraph>Stokely Webster (1912 – 2001) was best known as an Americanimpressionist painter who studied in Paris. His paintings can be found in the permanent collections ofmany museums, including the Metropolitan Museum of Art in New York, the National Museum...
FiQA-2018 What is the PEG ratio? How is the PEG ratiocalculated? How is the PEG ratio useful forstock investing?
<Paragraph>PEG is Price/Earnings to Growth. It is calculated as Price/Earnings/Annual EPS Growth.It represents how good a stock is to buy, factoring in growth of earnings, which P/E does not. Obviouslywhen PEG is lower, a stock is more undervalued, which means that it is a better buy, and more likely...
Signal-1M (RT)Genvoya, a Gentler Anti-HIV Cocktail, Okayedby EU Regulators <Paragraph>All people with #HIV should get anti-retroviral drugs: @WHO, by @kkelland via@Reuters_Health #AIDS #TasP
TREC-NEWSWebsites where children are prostituted are im-mune from prosecution. But why?<Title>Senate launches bill to remove immunity for websites hosting illegal content, spurred byBackpage.com<Paragraph>The legislation, along with a similar bill in the House, sets the stage for abattle between Congress and some of the Internet’s most powerful players, including Google and variousfree-speech advocates, who believe that Congress shouldn’t regulate Web content or try to force websitesto police themselves more rigorously...
Robust04 What were the causes for the Islamic Revolutionrelative to relations with the U.S.?<Paragraph>BFN [Editorial: "Sow the Wind and Reap the Whirlwind"] Yesterday marked the 14thanniversary of severing of diplomatic relations between the Islamic Republic and the United States ofAmerica. Several occasions arose in the last decade and a half for improving Irano-American relations...
Touché-2020Should the government allow illegal immigrantsto become citizens? <Title>America should support blanket amnesty for illegal immigrants.<Paragraph>Undocumentedworkers do not receive full Social Security beneﬁts because they are not United States citizens " norshould they be until they seek citizenship legally. Illegal immigrants are legally obligated to pay taxes...
CQADupStackCommand to display ﬁrst few and last few linesof a ﬁle <Title>Combing head and tail in a single call via pipe<Paragraph>On a regular basis, I am piping theoutput of some program to either ‘head‘ or ‘tail‘. Now, suppose that I want to see the ﬁrst AND last 10lines of piped output, such that I could do something like ./lotsofoutput | headtail...
Quora How long does it take to methamphetamine outof your blood? <Paragraph>How long does it take the body to get rid of methamphetamine?
DBPedia Paul Auster novels <Title>The New York Trilogy<Paragraph>The New York Trilogy is a series of novels by Paul Auster.Originally published sequentially as City of Glass (1985), Ghosts (1986) and The Locked Room (1986),it has since been collected into a single volume.
SCIDOCS CFD Analysis of Convective Heat Transfer Co-efﬁcient on External Surfaces of Buildings<Title>Application of CFD in building performance simulation for the outdoor environment: an overview<Paragraph>This paper provides an overview of the application of CFD in building performancesimulation for the outdoor environment, focused on four topics...
FEVER DodgeBall: A True Underdog Story is an Amer-ican movie from 2004 <Title>DodgeBall: A True Underdog Story<Paragraph>DodgeBall: A True Underdog Story is a2004 American sports comedy ﬁlm written and directed by Rawson Marshall Thurber and starring VinceVaughn and Ben Stiller. The ﬁlm follows friends who enter a dodgeball tournament...
Climate-FEVERSea level rise is now increasing faster than pre-dicted due to unexpectedly rapid ice melting.<Title>Sea level rise<Paragraph>A sea level rise is an increase in the volume of water in the world ’soceans, resulting in an increase in global mean sea level. The rise is usually attributed to global climatechange by thermal expansion of the water in the oceans and by melting of Ice sheets and glaciers...
Table 8: Examples of queries and relevant documents for all datasets included in BEIR . ( <Title>) and
(<Paragraph>) are used to distinguish the title separately from the paragraph within a document in the table
above. These tokens were not passed to the respective models.
23



Source: data\tc18_2309.07597v5\referenced_papers\[61]_2007.00808.pdf (Page 3):

Preprint
q
𝑑!
𝐷!! "#
"
Trainer
Inferencer
q
𝑑!
𝐷!! "$
"
Checkpoint k-1
…
Checkpoint k
q
𝑑!
𝐷!! "$
"
q
𝑑!
𝐷!!
"
…
Checkpoint k+1
q
𝑑!
𝐷!! "#
"
…
Inferencing
Index & 
Search
Training 
Positives
ANCE 
Negatives
Index & 
Search
Figure 2: ANCE Asynchronous Training. The Trainer learns the representation using negatives from
the ANN index. The Inferencer uses a recent checkpoint to update the representation of documents in
the corpus and once ﬁnished, refreshes the ANN index with most up-to-date encodings.
where L is the number of layers, ρis composed by pre-activation weights and gradients in inter-
mediate layers, and ||∇φLl(d+,d−)||2 is the gradient w.r.t. the last layer. Intuitively, the inter-
mediate layers are more regulated by various normalization techniques; the main moving piece is
||∇φLl(d+,d−)||2 (Katharopoulos & Fleuret, 2018).
For common learning to rank loss functions, for example, BCE loss and pairwise hinge loss, we can
veriﬁed that (Katharopoulos & Fleuret, 2018):
l(d+,d−) →0 ⇒||∇φLl(d+,d−)||2 →0 ⇒||∇θtl(d+,d−)||2 →0. (12)
Intuitively, negative samples with near zero loss have near zero gradients and contribute little to
model convergence. The convergence of dense retrieval model training relies on the informativeness
of constructed negatives.
Inefﬁcacy of Local In-Batch Negatives: We argue that the in-batch local negatives are unlikely to
provide informative samples due to two common properties of text retrieval.
Let D−∗be the set of informative negatives that are hard to distinguish from D+, and bbe the batch
size, we have (1) b≪|C|, the batch size is far smaller than the corpus size; (2) |D−∗|≪| C|, that
only a few negatives are informative and the majority of corpus is trivially unrelated.
Both conditions are easy to verify empirically in dense retrieval benchmarks. The two together make
the probability that a random mini-batch includes meaningful negatives p= b|D−∗|
|C|2 close to zero.
Selecting negatives from local training batches is unlikely to provide optimal training signals for
dense retrieval.
4 A PPROXIMATE NEAREST NEIGHBOR NOISE CONTRASTIVE ESTIMATION
Our analyses show the importance, if not necessity, to construct negativesglobally from the corpus.
In this section, we propose Approximate nearest neighbor Negative Contrastive Estimation, (ANCE),
which selects negatives from the entire corpus using an asynchronously updated ANN index.
ANCE samples negatives from the top retrieved documents via the DR model from the ANN index:
θ∗= argminθ
∑
q
∑
d+∈D+
∑
d−∈D−
ANCE
l(f(q,d+),f(q,d−)), (13)
with D−
ANCE = ANNf(q,d) \D+ and ANNf(q,d) the top retrieved documents by f() from the ANN
index. By deﬁnition, D−
ANCE are the hardest negatives for the current DR model: D−
ANCE ≈D−∗.
In theory, these more informative negatives have higher training loss, higher upper bound on the
gradient norms, and will improve training convergence.
ANCE can be used to train any dense retrieval model. For simplicity, we use a simple set up in recent
research (Luan et al., 2020) with BERT Siamese/Dual Encoder (shared between qand d), dot product
similarity, and negative log likelihood (NLL) loss.
4



Source: data\tc18_2309.07597v5\referenced_papers\[22]_2112.09118.pdf (Page 18):

Published in Transactions on Machine Learning Research (08/2022)
Table 11: Unsupervised retrieval. Performance of unsupervised methods on the BEIR datasets. We
report the capped recall@100 on Trec-COVID following the original BEIR setup. For SimCSE we report
results of the model using RoBERTa large. REALM uses annotated entity recognition data for training. On
Trec-COVID we report the capped Recall@100, see Thakur et al. (2021) for more details.
Model (→ ) BM25 BERT SimCSE REALM Contriever
Dataset (↓) Recall@100
MS MARCO 65.8 3.5 33.6 52.6 67.2
Trec-COVID 49.8 10.6 26.8 8.1 17.2
NFCorpus 25.0 6.7 18.2 23.0 29.4
NQ 76.0 14.3 42.9 58.1 77.1
HotpotQA 74.0 15.8 42.7 56.1 70.4
FiQA-2018 53.9 6.9 41.0 28.0 56.2
ArguAna 94.2 59.1 95.2 73.1 90.1
Tóuche-2020 53.8 3.0 18.6 11.5 22.5
CQADupStack 60.6 11.0 48.9 35.5 61.4
Quora 97.3 74.6 97.9 92.7 98.7
DBPedia 39.8 7.1 21.5 33.0 45.3
SCIDOCS 35.6 11.3 23.0 23.1 36.0
Fever 93.1 13.6 50.8 82.6 93.6
Climate-fever 43.6 12.8 44.8 42.3 44.1
SciFact 90.8 35.2 75.3 83.8 92.6
Avg. 63.6 19.0 45.4 46.9 60.1
Best on 3 0 2 0 10
NDCG@10
MS MARCO 22.8 0.6 8.8 15.2 20.6
Trec-COVID 65.6 16.6 38.6 20.1 27.4
NFCorpus 32.5 2.5 14.0 24.1 31.7
NQ 32.9 2.7 12.6 15.2 25.4
HotpotQA 60.3 4.9 23.3 40.5 48.1
FiQA-2018 23.6 1.4 14.8 9.7 24.5
ArguAna 31.5 23.1 45.6 22.8 37.9
Tóuche-2020 36.7 3.4 11.6 7.3 19.3
CQADupStack 29.9 2.5 20.2 13.5 28.4
Quora 78.9 3.9 81.5 71.6 83.5
DBPedia 31.3 3.9 13.7 22.7 29.2
SCIDOCS 15.8 2.7 7.4 9.0 14.9
FEVER 75.3 4.9 20.1 42.9 68.2
Climate-fever 21.3 4.1 17.6 14.3 15.5
SciFact 66.5 9.8 38.5 47.1 64.9
Avg. 41.7 8.7 24.6 25.1 36.0
Best on 12 0 1 0 2
B.3 Curse of multilinguality
We tried to pre-train models on diﬀerent sets of languages. We generally observed performance deterioration
when scaling to more languages similarly to what has been observed for general multilingual masked language
models Conneau et al. (2019). In Table 15 we report results on Mr. TyDi with a model pre-trained on the
11 languages of Mr. TyDi versus the model used in the rest of the paper which has been pre-trained on 29
languages including the 11 languages of Mr. TyDi as detailed in Table 12. We also report performance of these
models after training on MS MARCO, eventually followed by further ﬁne-tuning on Mr. TyDi. It appears that
the performance of the unsupervised model and the performance after ﬁne-tuning on MS MARCO are better
for the model pre-trained only on 11 languages. The diﬀerence is mitigated after ﬁne-tuning on Mr. TyDi.
19



Source: data\tc18_2309.07597v5\referenced_papers\[28]_2308.03281.pdf (Page 2):

2022), have demonstrated that constructing posi-
tive pairs through random passage cropping yields
superior results compared to the ICT task. Building
upon the ideas presented in (Chang et al., 2020),
some researchers have also put forth methods for
constructing higher-quality positive pairs using the
web link topology for retriever pre-training (Zhou
et al., 2022), a technique that proves effective in
zero-shot scenarios. Furthermore, in the field of
dense retrieval, significant research is dedicated to
enhancing the text representation capabilities of
pre-trained language models through the design of
auxiliary pre-training tasks (Gao and Callan, 2021;
Xiao et al., 2022; Gao and Callan, 2022; Wang
et al., 2022a; Long et al., 2022b; Li et al., 2023).
The previous two lines of research can be gen-
eralized as learning a vector representation for a
piece of text and distinguished by the type of down-
stream tasks. Recently, several studies have ex-
plored the construction of unified text representa-
tion models through large-scale contrastive learn-
ing and prompt-based learning (Neelakantan et al.,
2022; Wang et al., 2022b; Su et al., 2023). Ad-
ditionally, some research efforts have focused on
constructing evaluation datasets to better assess
the stability of text representation models across
different tasks and domains. BEIR (Benchmark-
ing IR) (Thakur et al., 2021) collects a substantial
number of retrieval tasks from various domains to
evaluate the robustness of dense retriever models in
zero-shot scenarios. Meanwhile, MTEB (Massive
Text Embedding Benchmark) (Muennighoff et al.,
2023) benchmarks over 56 datasets spanning seven
categories, providing a comprehensive evaluation
of text embedding models.
This study aims to develop a general text em-
bedding model through a multi-stage training ap-
proach. In the initial stage of unsupervised con-
trastive learning, we generate weak supervised cor-
relation text pairs using publicly available data
from various sources. Unlike previous study (Wang
et al., 2022b), we exclusively utilized open-source
data and did not employ any filtering or cleaning
methods. Pre-training on a large-scale text pairs
can effectively improve the domain generalization
of text representation models and bridge the gap
between the MLM training objective and the con-
trastive learning objective of representation models,
making the language model more suitable for text
representation tasks. In the supervised fine-tuning
stage, the mixture of training data in our approach
is more varied to further enhance the model’s ver-
satility. Moreover, our model does not incorpo-
rate task-specific prompts, which enhances repro-
ducibility and ease of use.
3 Approach
The training process of our model consists of two
stages: unsupervised pre-training and supervised
fine-tuning. Both stages employ the learning ob-
jective of contrastive learning. Firstly, we will in-
troduce the basic framework of the model. Subse-
quently, we will discuss the sources and construc-
tion methods of the training data in the two stages.
Finally, we will present some special optimization
strategies used to enhance the model’s performance
during the training process.
3.1 Model Architecture
The backbone of our embedding model is a deep
Transformer encoder (Vaswani et al., 2017) which
can be initialized with pre-trained language models
such as BERT (Devlin et al., 2019). Our model
follows the vanilla dual-encoder architecture with
mean pooling on top of the contextualized token
representations produced by the language model.
Formally, given a piece of textx = (x1, . . . , xn)
consisting of n tokens, an embedding modelE con-
vert the text into a low-dimensional dense vector
x = E(x) ∈ Rd. To implement E, we first employ
a language model to get the deep contextualized
token representations
h = LM(x) ∈ Rn×d. (1)
Then we apply a lightweight mean pooling
across the first dimension to get the text representa-
tion,
x = 1
n
nX
i=1
hi ∈ Rd (2)
The text representations are learned through the
contrastive objective, distinguishing semantic rele-
vant text pairs from irrelevant ones. Such training
procedure requires positive and negative pairs, tak-
ing the format of (q, d+, d−). For a query q, a rel-
evant document d+, a set of irrelevant documents
D− = {d−
1 , . . . , d−
n }, one popular contrastive ob-
jective is the InfoNCE loss (van den Oord et al.,
2018),
Lcl = −log es(q,d+)/τ
es(q,d+)/τ +
nP
i=1
es(q,d−
i )/τ
, (3)



## Processing Completed
Finished at: 2025-01-25 10:04:08
