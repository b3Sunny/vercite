[
  {
    "original_claim": "Thus, the collected data must be properly cleaned before being utilized for the training of embeddings [53]. ‚Ä¢Training.",
    "context_before": [
      "Finally, the augmentation of scale and diversity will probably introduce noise."
    ],
    "context_after": [
      "The training of general-purpose text embeddings depends on two critical elements: a well-suited backbone encoder and an appropriate training recipe."
    ],
    "references": [
      "53"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "BEIR [51] provides a collection of 18 to evaluate the embedding‚Äôs general performances on different retrieval tasks, e.g., question answering and fact-checking.",
    "context_before": [
      "Another pre-requisite condition is the establishment of proper benchmarks, where all needed capabilities of text embeddings can be comprehensively evaluated."
    ],
    "context_after": [
      "Later, MTEB proposes a more holistic evaluation of embeddings and extends BEIR."
    ],
    "references": [
      "51"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Later, MTEB [35] proposes a more holistic evaluation of embeddings and extends BEIR.",
    "context_before": [
      "BEIR provides a collection of 18 to evaluate the embedding‚Äôs general performances on different retrieval tasks, e.g., question answering and fact-checking."
    ],
    "context_after": [
      "It integrates 56 datasets, where all important capabilities of text embeddings, like retrieval, ranking, clustering, etc., can be jointly evaluated."
    ],
    "references": [
      "35"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In recent years, continual progresses have been achieved in this field, such as Contriever [22], E5 [53], GTR [40], and OpenAI Text Embedding [37].",
    "context_before": [
      "Altogether, the development of general-purpose text embedding needs to be made on top of a mixture of driving forces, from data, and encoder models, to training methods and benchmarking."
    ],
    "context_after": [
      "Nevertheless, most of these models are dedicated to the English-centric scenarios."
    ],
    "references": [
      "53",
      "37",
      "40",
      "22"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In recent years, there has been a continual effort in this field, where a series of well-known works are proposed, like Contriever [22], GTR [40], sentence-T5 [39], Sentence-Transformer [46], E5 [52], OpenAI text embedding [37], etc.",
    "context_before": [
      "Compared with the conventional task-specific methods, the general text embedding needs to be extensively applicable in different scenarios."
    ],
    "context_after": [
      "Although it remains an open problem, recent studies highlight the following important factors. ‚Ä¢Firstly, the training data is desired to be large-scale and diversified, from which the embedding model can learn to recognize different kinds of semantic relationships ."
    ],
    "references": [
      "46",
      "22",
      "52",
      "40",
      "37",
      "39"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Unlike previous task-specific evaluations, like MSMARCO [ 38], SentEval [14], it is needed to substantially augment the benchmarks so as to evaluate the embedding‚Äôs performance for a wide variety of tasks.",
    "context_before": [
      "In C-Pack, these operations are integrated, optimized, and pipelined, which significantly facilitates people‚Äôs reproduction and continual fine-tuning of BGE. ‚Ä¢Aside from the above factors, it is also critical to establish proper benchmarks to evaluate the generality of text embeddings."
    ],
    "context_after": [
      "One representative work is made by BEIR , where the embeddings can be evaluated across different retrieval tasks."
    ],
    "references": [
      "14"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "It is later extended by MTEB [35], where all major aspects of text embeddings can be comprehensively evaluated.",
    "context_before": [
      "One representative work is made by BEIR , where the embeddings can be evaluated across different retrieval tasks."
    ],
    "context_after": [
      "However, no such works were done for the Chinese community before."
    ],
    "references": [
      "35"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In the past few years, the community has put forward many datasets for text representation and language understanding tasks in Chinese, such as CMNLI [62], DuReader [20], T2Ranking [60].",
    "context_before": [
      "Then, we discuss the training recipe, which enables us to train the state-of-the-art models for general Chinese embedding based on the offered resources. 3.1 Benchmark: C-MTEB C-MTEB is established for the comprehensive evaluation of the generality of Chinese embeddings (Figure 2)."
    ],
    "context_after": [
      "However, these datasets are independently curated, lacking a fair and shared ground to comprehensively evaluate the general capability of text embeddings."
    ],
    "references": [
      "62",
      "20",
      "60"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In this work, we use the setting from BEIR [51], using NDCG@10 as the main metric. ‚Ä¢Re-ranking.",
    "context_before": [
      "The retrieval quality can be measured by ranking and recall metrics at different cut-offs."
    ],
    "context_after": [
      "The re-ranking task is presented with test queries and their candidate documents (1 positive plus N negative documents)."
    ],
    "references": [
      "51"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "The classification task re-uses the logistic regression classifier from MTEB [35], where the average precision is used as the main metric. ‚Ä¢Pair-classification.",
    "context_before": [
      "Following the original setting in Sentence-BERT [ 46], 6https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB Figure 3: The evaluation pipeline of C-MTEB. the Spearman‚Äôs correlation is computed with the given label, whose result is used as the main metric. ‚Ä¢Classification."
    ],
    "context_after": [
      "This task deals with a pair of input sentences, whose relationship is presented by a binarized label."
    ],
    "references": [
      "35"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Following the original setting in MTEB [35], it uses the mini-batch k-means method for the evaluation, with batch size equal to 32 and k equal to the number of labels within the mini-batch.",
    "context_before": [
      "The clustering task is to group sentences into meaningful clusters."
    ],
    "context_after": [
      "The V-measure score is used as the main metric."
    ],
    "references": [
      "35"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Our models are based on the BERT-like architecture [15], which go through three-stage of training (to be discussed in the next section).",
    "context_before": [
      "In our work, we provide a comprehensive class of welltrained embedding models for the community."
    ],
    "context_after": [
      "There are three available scales: large (with 326M parameters), base (with 102M parameters), and small (with 24M parameters)."
    ],
    "references": [
      "15"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Instead of mining hard negative samples on purpose, we purely rely on in-batch negative samples [25] and resort to a big batch size (as large as 19,200) to improve the discriminativeness of the embedding. ‚Ä¢Task-specific fine-tuning.",
    "context_before": [
      "One critical factor of contrastive learning is the negative samples."
    ],
    "context_after": [
      "The embedding model is further fine-tuned with C-MTP (labeled)."
    ],
    "references": [
      "25"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "The hard negative sample is mined from the task‚Äôs original corpus, following the ANN-style sampling strategy in [61]. 4 EXPERIMENTS In this section, we conduct experimental studies for the exploration of following problems.",
    "context_before": [
      "On the other hand, the negative sampling is updated: in addition to the in-batch negative samples, one hard negative sample ùëû‚Ä≤is mined for each text pair (ùëù, ùëû)."
    ],
    "context_after": [
      "P."
    ],
    "references": [
      "61"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "We consider the following popular Chinese text embedding models as the baselines for our experiments: Text2Vec-Chinese10 base and large; Luotuo11; M3E12 base and large; multilingual E5 [53] and OpenAI text embedding ada.",
    "context_before": [
      "The exploration of the impacts introduced by the training recipe."
    ],
    "context_after": [
      "The main metric presented in Section 3.1 is reported for each evaluation task in C-MTEB. 4.1 General Evaluation We extensively evaluate BGE against popular Chinese text embeddings on C-MTEB as shown in Table 2.14, where we can make the following observations."
    ],
    "references": [
      "53"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "At the time of public release, our English BGE models achieved the state-of-the-art performance on the English MTEB benchmark [35] across its 56 datasets.",
    "context_before": [
      "It was the first time that such comprehensive training data was made publicly available."
    ],
    "context_after": [
      "Although there were many strong competitors in the English community, such as E5 , SGPT , GTE , GTR , and OpenAI Ada-002 , we were able to notably advance the prior SOTA by an absolute 1.1 points in total average, which further verify the effectiveness of our data curation and training method. 4.2 Detailed Analysis We investigate the detailed impact of C-MTP and our training recipe."
    ],
    "references": [
      "35"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Although there were many strong competitors in the English community, such as E5 [53], SGPT [32], GTE [28], GTR [40], and OpenAI Ada-002 [37], we were able to notably advance the prior SOTA by an absolute 1.1 points in total average, which further verify the effectiveness of our data curation and training method. 4.2 Detailed Analysis We investigate the detailed impact of C-MTP and our training recipe.",
    "context_before": [
      "At the time of public release, our English BGE models achieved the state-of-the-art performance on the English MTEB benchmark across its 56 datasets."
    ],
    "context_after": [
      "The corresponding experiment results are presented in Table 3 and Table 4, respectively."
    ],
    "references": [
      "28",
      "40",
      "32",
      "53",
      "37"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In our implementation, we use a compound strategy of gradient checkpointing and cross-device embedding sharing [18], which results in a maximum batch size of 19,.",
    "context_before": [
      "Given our dependency on in-batch negative samples, the batch size needs to be expanded as much as possible."
    ],
    "context_after": [
      "By making a parallel comparison between bz: 256, 2028, 19,200, we observe consistent improvement in embedding quality with the expansion of batch size (noted as bz)."
    ],
    "references": [
      "18"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "One more characteristic is that we use a specifically pre-trained text encoder to train BGE, rather than using common choices, like BERT [15] and RoBERTa [29].",
    "context_before": [
      "This indicates that using instructions may substantially contribute to the quality of task-specific fine-tuning."
    ],
    "context_after": [
      "To explore its impact, we replace the pre-trained text encoder with the widely used Chinese-RoBERTa15, noted as ‚ÄúBGE w.o. pre-train‚Äù."
    ],
    "references": [
      "15",
      "29"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  }
]