# Claims Processing Log

Processing started at: 2025-01-25 12:45:23

## Table of Contents

[[log_20250125_124523###Claim 1/36|Claim 1/36]]
[[log_20250125_124523###Claim 2/36|Claim 2/36]]
[[log_20250125_124523###Claim 3/36|Claim 3/36]]
[[log_20250125_124523###Claim 4/36|Claim 4/36]]
[[log_20250125_124523###Claim 5/36|Claim 5/36]]
[[log_20250125_124523###Claim 6/36|Claim 6/36]]
[[log_20250125_124523###Claim 7/36|Claim 7/36]]
[[log_20250125_124523###Claim 8/36|Claim 8/36]]
[[log_20250125_124523###Claim 9/36|Claim 9/36]]
[[log_20250125_124523###Claim 10/36|Claim 10/36]]
[[log_20250125_124523###Claim 11/36|Claim 11/36]]
[[log_20250125_124523###Claim 12/36|Claim 12/36]]
[[log_20250125_124523###Claim 13/36|Claim 13/36]]
[[log_20250125_124523###Claim 14/36|Claim 14/36]]
[[log_20250125_124523###Claim 15/36|Claim 15/36]]
[[log_20250125_124523###Claim 16/36|Claim 16/36]]
[[log_20250125_124523###Claim 17/36|Claim 17/36]]
[[log_20250125_124523###Claim 18/36|Claim 18/36]]
[[log_20250125_124523###Claim 19/36|Claim 19/36]]
[[log_20250125_124523###Claim 20/36|Claim 20/36]]
[[log_20250125_124523###Claim 21/36|Claim 21/36]]
[[log_20250125_124523###Claim 22/36|Claim 22/36]]
[[log_20250125_124523###Claim 23/36|Claim 23/36]]
[[log_20250125_124523###Claim 24/36|Claim 24/36]]
[[log_20250125_124523###Claim 25/36|Claim 25/36]]
[[log_20250125_124523###Claim 26/36|Claim 26/36]]
[[log_20250125_124523###Claim 27/36|Claim 27/36]]
[[log_20250125_124523###Claim 28/36|Claim 28/36]]
[[log_20250125_124523###Claim 29/36|Claim 29/36]]
[[log_20250125_124523###Claim 30/36|Claim 30/36]]
[[log_20250125_124523###Claim 31/36|Claim 31/36]]
[[log_20250125_124523###Claim 32/36|Claim 32/36]]
[[log_20250125_124523###Claim 33/36|Claim 33/36]]
[[log_20250125_124523###Claim 34/36|Claim 34/36]]
[[log_20250125_124523###Claim 35/36|Claim 35/36]]
[[log_20250125_124523###Claim 36/36|Claim 36/36]]


## Processing Details


### Claim 1/36

#### Claim Text
Some researchers have applied classical fairness metrics in classification settings to LLMs [18].

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[5]_2406.03198.pdf (Page 3):

1st HEAL Workshop at CHI Conference on Human Factors in Computing Systems, 2024 Anthis et al.
For subjects of LLMs, it may be difficult to define, detect, or
enforce appropriate fairness metrics. For example, early in the
literature on fairness in information access systems, it was noted
that when searching for images of “CEO, ” Google returned a set of
images largely depicting men and, lower in the recommendation
list, an image of the popular toy “CEO Barbie. ” However, as in
the FTU decision of which sensitive attributes a system should be
unaware of and in what way, the challenges of deciding subject
representation in system output are compounded with LLMs. These
decisions typically rely on utility estimates, and that tends to be
a significant challenge with more general-purpose systems based
on unstructured data. For example, there is an open question of
whether the target distribution should be equal representation of
men, women, and other genders or a distribution that is weighted
towards the gender distribution of CEOs in the consumer’s home
location [28, 37, 59]. To achieve LLM fairness, this sort of open
question would need to be resolved for each of the many tasks done
by the LLM.
For producers (i.e., the people or organizations whose content
is recommended), also known as providers, the fairness target is
often the equitable distribution of exposure, either in terms of
relevance-free metrics that do not consider the relevance of the
content to the user—only that there is an equitable distribution—or
relevance-based fairness metrics that target an equitable exposure
conditional on relevance. In either case, fairness to producers is
a matter of how the exposure of those providing content to the
system is allocated to consumers. In the use case of LLMs that
perform information retrieval and information management tasks,
this framework can at times transfer directly. For example, if some-
one searches for “coffee shops in San Francisco” in an LLM chat
or search interface—as is being incorporated into the ubiquitous
modern search engine, Google—producer fairness could be defined
in terms of equitable exposure to the different brick-and-mortar
coffee shops in San Francisco. Even if the LLM system does not
direct users to particular websites, many users will presumably end
up visiting the cafes, which provides utility—fairly or unfairly—to
the producers. However, if users are searching for information in
the LLM system, such as asking, “How are coffee beans roasted?”
then LLMs can entirely circumvent the producers and upend the
conventional notion of producer-side fairness. If the LLM system
extracts information from websites without directing users to the
original source content, then it may be that none of the producers
receive any exposure or other benefits in the first place. One way to
make sense of this would be to consider the LLM system itself—or
the entity that developed, owns, and manages it—as another type
of stakeholder, one that takes all utility from the producers and
renders the conventional producer-side fairness criteria obsolete.
4 LLMS ARE TOO FLEXIBLE TO BE
GENERALLY FAIR
Much of the excitement surrounding LLMs is based on their general-
purpose flexibility across wide ranges of inputs, tasks, outputs, and
contexts. To some extent, they resemble a human agent, including
the ability to chain together these tasks into complex sequences, and
these areas of flexibility make many conventional fairness metrics
intractable.
4.1 Group fairness does not generalize across
populations
Group fairness metrics require independence between model clas-
sification and sensitive attributes, often conditional on relevant
information such as the ground-truth labels that the model aims
to predict (e.g., job performance for a model that assists in hiring
decisions). Three common metrics are:
Definition 3. (Demographic parity). A model achieves demo-
graphic parity if its predictions are statistically independent of
sensitive attributes.
Definition 4. (Equalized odds). A model achieves equalized odds
if its predictions are statistically independent of sensitive attributes
conditional on the true labels being predicted.
Definition 5. (Calibration). A model achieves calibration if the
true labels being predicted are statistically independent of sensitive
attributes conditional on the model’s predictions.
In binary classification, these metrics are achieved when equal-
ities hold between ratios in the confusion matrix: equal ratios of
predicted outcomes (demographic parity), equal true positive rates
and false positive rates (equalized odds), and equal precision (cali-
bration). Recent work includes extensions of these notions, such as
prioritizing the worst-off group by minimizing the maximum group
error rate [19]. Conventionally, group fairness requires knowing
the sensitive attributes to enforce the equalities, though recent work
has considered approaches for when the sensitive attributes are un-
available [36, 42, 76]. There are many methods for enforcing group
fairness metrics, such as the preprocessing of datasets proposed
by Feldman et al. [27] to guarantee bounds on demographic parity
and the more recent method proposed by Johndrow and Lum [34]
that can be applied to a wider variety of datasets.
LLMs present a challenge for group fairness metrics in part
because LLMs tend to be deployed across a wide range of data
distributions. Lechner et al. [43] showed that it is impossible for a
non-trivial model to perform fairly across all different data distri-
butions, such as regions or demographic groups, to which it might
be applied. In current discussions of algorithmic fairness (e.g., re-
cidivism), fairness is typically targeted at a local jurisdiction, which
ensures that the model is performing fairly for that location’s par-
ticular demographic mix and holistic characteristics but typically
cannot also achieve fairness in substantially different locations. The
purpose and use of LLMs makes it infeasible to restrict them to this
sort of targeted population.
In general, it is not clear what an appropriate base population
would be on which to detect and achieve group fairness for an
LLM. For example, one could “bootstrap” a predictive model for
recidivism prediction from an LLM simply by instructing it to make
a prediction about an individual based on a fixed set of that indi-
vidual’s characteristics with in-context learning as Li and Zhang
[45] did in predicting the label of a text-converted tabular dataset.
However, the data on which that LLM had been trained does not
admit an identifiable base population because a corpus of text is not
4



Source: data\tc20_2501.12962v1\referenced_papers\[18]_2402.18502.pdf (Page 2):

Few-Shot Fairness: Unveiling LLM’s Potential for Fairness-Aware Classification 3
•To our knowledge, this is the first investigation into ensuring fairness through in-context learning for classification
task by specifying various fairness notions.
•We compare state-of-the-art LLMs, namely Llama-70b by Meta, GPT-4 by OpenAI, and Gemini by Google, using
different fairness criteria.
•We assess the accuracy-fairness tradeoff across zero-shot and few-shot setups.
•We publicly release the predictions of these LLMs for over 1000 test instances across four different setups, which
can spawn future research in this field1.
2 RELATED WORK
2.1 Fairness in LLMs
LLMs are experiencing explosive growth in their capabilities and applications. However, unfair LLM-based systems may
produce biased, discriminating, and stereotyping choices against underprivileged or vulnerable groups, which can have
negative societal effects and even be harmful [9, 29]. Hence, the concerns about discrimination and unfairness have
spurred research on the potential harmfulness of LLMs. Essentially, the bias in the training data gets baked into the
LLM, leading to biased outputs. This has led researchers to focus on mitigating these issues and ensuring fairer results
from LLMs. Methods like RLHF [39] and RLAIF [4] aim to steer LLMs away from reinforcing existing stereotypes and
producing offensive content. These techniques primarily involve training LLMs to generate fair and neutral outputs.
However, they may not be practical for the average user who does not intend to train or fine-tune an LLM. There is also
a growing focus on developing improved benchmarks to assess the unfairness in which datasets like CrowS-Pairs [36],
featuring sentence pairs with varying levels of stereotyping, RealToxicityPrompts[21], and RedTeamingData [40] for
prompt generation tasks with potentially harmful outcomes, and HELM [32], a comprehensive benchmark evaluating
bias and fairness in LLM. Although there has been considerable research on fairness in LLMs, there is currently an
absence of relevant studies specifically addressing fairness in classification tasks.
2.2 In-context Learning
Prior studies [11, 43] have shown that Large Language Models (LLMs) can perform tasks with limited or no training data
by learning from the context. They excel when provided with a suitable prompt. However, recent research [31, 34, 53]
has revealed that the effectiveness of LLMs is influenced by the prompt used. The selection of prompt format, training
examples, and even the order of those examples can significantly impact the performance of a Large Language Model
(LLM). This becomes even more crucial when we try to incorporate supplementary contextual information and fairness
criteria that could improve the fairness of outcomes produced by LLMs. [8] adopt a group fairness lens to assess bias and
fairness in LLMs and introduce a novel chain-of-thought method [49] designed to diminish biases in LLMs, particularly
from the perspective of group fairness. This impels us to include fairness notions within the context of prompts through
a fairness framework and conduct classification tasks to investigate the inherent understanding of fairness in LLMs.
3 EXPERIMENTAL SETUP
In this section, we outline the overall setup of the experiments, covering aspects such as the dataset, models utilized,
different fairness definitions, and fairness metrics.
1Available at https://anonymous.4open.science/r/FairLLM-8621.
Manuscript submitted to ACM



Source: data\tc20_2501.12962v1\referenced_papers\[18]_2402.18502.pdf (Page 22):

Few-Shot Fairness: Unveiling LLM’s Potential for Fairness-Aware Classification 23
Models Performance Fairness
Accuracy F1 Score 𝐷𝐼𝑔 𝑇𝑃𝑅𝑔 𝐹𝑃𝑅𝑔 𝑃𝑃𝑉𝑔 𝐹𝑂𝑅𝑔 𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦𝑔
No Fairness
Llama 0.74 0.73 0.65 0.75 0.29 1.14 1.21 0.97
GPT4 0.72 0.70 0.56 0.63 0.21 1.12 1.28 0.91
Gemini 0.79 0.78 0.68 0.76 0.33 1.11 1.44 0.95
Demographic Parity
Llama 0.72 0.71 0.65 0.75 0.29 1.15 1.22 0.97
GPT4 0.69 0.66 0.75 0.82 0.27 1.08 1.06 0.97
Gemini 0.79 0.79 0.65 0.74 0.32 1.13 1.68 0.96
Equal Opportunity
Llama 0.75 0.75 0.65 0.75 0.29 1.15 1.22 0.97
GPT4 0.72 0.71 0.63 0.70 0.32 1.04 1.23 0.93
Gemini 0.80 0.79 0.66 0.75 0.29 1.14 1.58 0.96
Equalized Odds
Llama 0.70 0.69 0.54 0.59 0.32 1.09 1.26 0.89
GPT4 0.67 0.64 0.57 0.61 0.21 1.07 1.17 0.90
Gemini 0.79 0.78 0.63 0.71 0.28 1.12 1.62 0.94
Accuracy
Llama 0.71 0.70 0.56 0.62 0.32 1.11 1.32 0.91
GPT4 0.72 0.71 0.60 0.66 0.27 1.09 1.25 0.91
Gemini 0.80 0.80 0.65 0.74 0.24 1.14 1.61 0.96
Treatment Equality
Llama 0.68 0.66 0.65 0.75 0.29 1.15 1.22 0.97
GPT4 0.72 0.71 0.59 0.66 0.23 1.12 1.25 0.92
Gemini 0.79 0.79 0.65 0.74 0.28 1.13 1.53 0.96
Causal Discrimination
Llama 0.64 0.61 0.54 0.59 0.32 1.09 1.26 0.89
GPT4 0.76 0.75 0.64 0.73 0.22 1.14 1.28 0.95
Gemini 0.78 0.78 0.59 0.67 0.20 1.14 1.60 0.92
Fairness through Unawareness
Llama 0.70 0.69 0.56 0.62 0.32 1.10 1.27 0.91
GPT4 0.75 0.74 0.64 0.72 0.26 1.13 1.28 0.95
Gemini 0.79 0.79 0.63 0.71 0.24 1.12 1.55 0.93
Generic Fairness
Llama 0.74 0.67 0.54 0.59 0.32 1.08 1.25 0.89
GPT4 0.73 0.72 0.59 0.66 0.23 1.12 1.28 0.92
Gemini 0.78 0.78 0.60 0.67 0.28 1.12 1.72 0.92
Table 9. Results for Few Shot Prompting using Detailed Rules 𝜋𝐷 for different fairness definitions.
Manuscript submitted to ACM



Source: data\tc20_2501.12962v1\referenced_papers\[5]_2406.03198.pdf (Page 4):

The Impossibility of Fair LLMs 1st HEAL Workshop at CHI Conference on Human Factors in Computing Systems, 2024
a structured database comprising people and their characteristics.
An LLM may be trained in part on such databases, but the output
of the model for such predictions will also be based on the wide
scope of unstructured natural language or other modalities of data
on which the model is trained.
Generalization across populations is also a concern for fairness
frameworks other than group fairness because of the wide range of
data, use cases, and social contexts at play in LLMs [60]. Here, we
consider two examples: individual fairness [20] and counterfactual
fairness, which is the most common causal notion of fairness [40].
Definition 6. (Individual fairness). A model achieves individual
fairness if similar individuals are treated similarly. Formally, this re-
quires that the distribution of model output is Lipschitz continuous
with respect to the distribution of model input.
Definition 7. (Counterfactual fairness). A model achieves coun-
terfactual fairness if the model would produce the same output for
an individual if they had a different level of the sensitive attribute.
In terms of individual fairness, it is not clear what similarity
metrics could be reasonably applied across the multitude of contexts
or, if multiple metrics were applied, how these could be judiciously
selected and guaranteed in each possible context. In terms of causal
fairness, including counterfactual fairness, it is often difficult to
identify the causal structure of the data-generating process in even
a single NLP task, and it would be an immense challenge for a single
model to account for all of the many different contextual factors
that determine counterfactuals or other causally distinct outcomes
across the varying populations.
4.2 Sensitive attributes proliferate in a
general-use setting
The preceding section considered the challenges of imposing fair-
ness across different data distributions. When considering different
sensitive attributes, given the issues discussed in Section 3.1, it may
not be tractable to exclude sensitive attributes from the training
data, and each of the different distributions and different tasks can
require fairness metrics to be enforced for a different set of sensitive
attributes. This is a challenge for the group fairness metrics already
defined, but the issue is particularly salient for the popular ideal
of fair representations within a machine learning model or fair
representations produced by one model and used by another [72].
Definition 8. (Fair representation). A representation is fair if it
does not contain information that can identify the sensitive at-
tributes of the individuals being represented.
In the fair representations framework, a system first maps the
dataset of individuals being represented to a probability distribution
in a novel representation space, such that the system preserves as
much information as possible about the individual while removing
all information about the individual’s sensitive attribute. The most
well-known example of this approach is Bolukbasi et al. [11], which
rigorously documented gender bias in Google News word embed-
dings, namely an association between occupations and a gender
vector (e.g., ®he− ®she), such that computer programmer was coded as
highly male while homemaker was coded as highly female. Indeed,
this is where much of the NLP fairness literature has focused, doc-
umenting similar biases across different word embedding models
Sesari et al. [see 61, for a review].
Researchers have developed a number of debiasing approaches
focused on the sensitive attribute dimension, such as zeroing the
projection of each word vector (e.g., each occupation) onto the
dimension itself [11] or training the model to align the sensitive
attribute dimension with the last coordinate of the embedding space,
so that it can be easily removed or ignored [75]. However, Gonen
and Goldberg [33] show that such approaches “are mostly hiding
the bias rather than removing it” because, after removal, word pairs
tend to maintain their similarity, which still reflects associations
with sensitive attributes—what Bolukbasi et al. [11] call “indirect
bias. ”
Achieving fairness in one LLM context may be contingent on
the removal of information or alteration of the statistical relation-
ships between the context-specific sensitive attribute and other
features of the data. For example, one may wish to exclude gender
information from financial lending decisions, but gender informa-
tion may be necessary for other tasks, such as drafting or editing
an email about a real-world situation that has important gender
dynamics that the sender hopes to communicate to the receiver.
Moreover, variables highly correlated with gender, such as biologi-
cal sex and pregnancy status, may be essential criteria for medical
decision-making. In general, attempts at debiasing for one context
may remove or distort important information for another context.
The naive approach of debiasing the model with respect to the
union of all potential sensitive attributes—even if it were empirically
feasible—would likely be too heavy-handed, leaving the model with
little information to be useful for any task. To effectively create a
fair LLM for every task and context, one would need to act upon
the parameters of the model with surgical precision to alter the
relationship between variables only when the model is instantiated
for a specific task and context. This is infeasible with current LLM
methods, such as fine-tuning, and currently we do not have robust
techniques to debias even a single problematic relationship without
incidentally obfuscating it or problematizing other relationships.
This game of fairness whack-a-mole seems indefinitely intractable.
Likewise, even if we could reduce the union of all potential sensitive
attributes to a manageable level, such as identifying a small set of
the most important to adjust for in each task, that would still require
yet-infeasible fine-grained adjustments to avoid counterproductive
side effects.
4.3 Fairness does not compose, but
fairness-directed composition may help
Whether a model’s behavior on a given task is fair or desirable
largely depends on how the model’s output will be used. In modern
AI systems including LLMs, the output of one model is often used
as the input to another model, but this produces an additional
challenge because fairness does not compose: a fairness guarantee
for each of two models is not a fairness guarantee for a system
composed of the two models—a point made most explicitly by
Dwork and Ilvento [21].
5



Source: data\tc20_2501.12962v1\referenced_papers\[18]_2402.18502.pdf (Page 4):

Few-Shot Fairness: Unveiling LLM’s Potential for Fairness-Aware Classification 5
Notation Explanation
𝑁𝑓 The count of females within the test set.
𝑁𝑚 The total number of males in the test set.
𝑋 All 14 attributes describing the individual.
𝑌 The actual classification result. In our case, 𝑌 takes up two discrete values 0 or 1,
where 0 represents <=50K and 1 represents >50K.
ˆ𝑌 Predicted income decision for the individual. ˆ𝑌 can have two discrete values 0 or 1,
where 0 represents <=50K and 1 represents >50K.
𝐺 Protected or sensitive attribute for which non-discrimination should be established. 𝐺
can have value 𝑚or 𝑓, where 𝑚represents male and 𝑓 represents female.
𝑇𝑃𝑓
Amongst all the females, the number of females who had an income >50K and were
correctly predicted by the classifier as having an income >50K.
𝑇𝑃𝑚
Amongst all the males, the number of males who had income >50K and were correctly
predicted by the classifier as >50K.
𝑃(𝐴= 𝑎|𝐵 = 𝑏,𝐶 = 𝑐)
Probability of event 𝐴occurring given that conditions 𝐵and 𝐶 are already satisfied.
𝑃(𝐴= 𝑎|𝐵 = 𝑏,𝐶 = 𝑐)= 𝑃(𝐴=𝑎∩𝐵=𝑏∩𝐶=𝑐)
𝑃(𝐵=𝑏∩𝐶=𝑐)
Table 1. Notations utilized for defining fairness principles.
was used. For our experiments, we employ Llama-2-70b 3 model through Replicate API4 for obtaining the
results.
•Gemini [22]: Released in Dec 2023 by Google. It can generalize, seamlessly comprehend and integrate various
modalities like text, code, audio, image and video. We use gemini-pro 5 model as the size strikes a balance
between capability and efficiency.
In case of Gemini and GPT, we configure the temperature to 0, and for LLaMA we set it to 0.01. Across all experiments,
we standardize the top probabilities to 0.95, frequency penalty to 0, and presence penalty to 1.
3.3 Fairness Definition
In this section, we discuss different definitions of fairness that we use for our experiments. Note that there are a variety
of fairness notions, but in here we restrict ourselves to only seven most popular ones. Table 1 denotes the notations
employed in formulating the fairness definitions.
3.3.1 Definitions based on Predicted Outcome.
It emphasizes only the predicted outcome ˆ𝑦for distinct groups, specifically male and female.
•Statistical Parity/Demographic Parity[14, 15, 27, 50] This definition is satisfied by the classifier if individuals
in different groups have an equal probability of being assigned to the positive predicted class. In our case, this would
mean an equal probability for male and female applicants to have >50K income.
𝑃(ˆ𝑌 = 1|𝐺 = 𝑓)= 𝑃(ˆ𝑌 = 1|𝐺 = 𝑚) (1)
3.3.2 Definitions based on Predicted and Actual Outcome.
This definition of fairness considers both the actual outcome 𝑌 and the predicted outcome ˆ𝑌 for various groups.
3https://ai.meta.com/llama/
4https://replicate.com/meta/llama-2-70b
5https://ai.google.dev/models/gemini
Manuscript submitted to ACM



### Claim 2/36

#### Claim Text
Thus, it was argued that LLMs cannot yield fair outcomes at all [5]. 2.2 EU non-discrimination Law: A gentle introduction, too.

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[5]_2406.03198.pdf (Page 2):

The Impossibility of Fair LLMs 1st HEAL Workshop at CHI Conference on Human Factors in Computing Systems, 2024
been rigorously considered in the extant fairness literature despite
their increasing prevalence.
3 SOME FAIRNESS FRAMEWORKS CANNOT
BE APPLIED TO LLMS
While the extant machine learning literature—prior to the pop-
ularization of LLMs—has shown that many fairness metrics are
incompatible with each other [16, 38], here we develop the stronger
claim that some frameworks cannot even be logically applied to a
single metric in the case of an LLM.
3.1 Unawareness is impossible by design
Though often used as a strawman in the fairness literature, arguably
the most common approach to algorithmic fairness has been fair-
ness through unawareness (FTU).
Definition 1. (Fairness through unawareness). A model achieves
fairness through unawareness (FTU) if the input to the model does
not explicitly contain any sensitive attributes.
The concept of FTU emerged in the context of models built on
structured data, typically in which data is organized into variables
that are used for prediction or classification. For example, a financial
lending model could use a person’s age, gender, and credit score
to make a prediction about loan repayment with FTU meaning
“gender” is excised from the training data. Although it has been
established that omitting the explicit sensitive attribute from a
model at training time is insufficient to guarantee that unwanted
correlations with that attribute do not exist in the model, legal,
policy, or feasibility constraints often lead to this approach even
though it is “widely considered naive” [ 71]. In one of the most
widely known allegations of algorithmic discrimination, a group of
heterosexual spouses who used the Apple Card to make purchases
noticed after online discussion that each woman was extended a
much lower credit limit than her husband. The company managing
the Apple Card, Goldman Sachs, defended itself by saying, “In all
cases, we have not and will not make decisions based on factors
like gender” [ 67]. The primary critique of this approach is that
the sensitive attribute is, often strongly, related to other attributes
included in the training data, so models effectively recover the
excluded sensitive attribute and this curtails any potential fairness
benefits.
By design, LLMs are trained on unstructured modalities such as
natural language, a context in which FTU is impossible because of
the pervasiveness of sensitive attributes. Indeed, LLMs are read-
ily able to infer personal characteristics such as age, location, and
gender of an author from their written text [64]. Efforts to remove
sensitive attributes from text may produce incoherence or distortion.
For example, in the sentence, “Alice grew up in Portugal, so Alice
had an easy time on the trip to South America, ” simply removing
Alice’s national origin of “Portugal” would result in an ungrammati-
cal sentence. Other approaches for removing national origin would
still result in distortion. Substituting the neutral phrase “a country”
would remove important narrative information, such as the author
conveying to the reader that Alice visited Brazil, the only South
American country in which Portuguese is an official language.
Consider how the relative social status of characters in a narra-
tive can be conveyed through pronoun usage in quoted material,
such as the more frequent use of first-person pronouns being more
common in groups of lower social status [ 35]. Moreover, in lan-
guages with gendered nouns (e.g., Spanish, German), enforcing
gender fairness may require introducing entirely new vocabulary,
and if nationality, native language, religion, beliefs, or other at-
tributes of cultural background are considered sensitive, then the
corresponding languages, dialects, and subdialects would also be
impossible to extirpate. Even with attributes that could be removed
without distortion in certain cases, it is infeasible to enforce fairness
with respect to all relevant sensitive attributes across a large cor-
pus while retaining sufficient information for model performance.
There may also be direct ethical issues with the modification of
text, such as individual authors not consenting to the modification
of text they own.
As with the other frameworks, FTU is additionally hindered by
the current lack of model transparency. FTU would require that the
LLM be documentably unaware of the sensitive information, which
requires a level of documentation of training data that is unavail-
able from any state-of-the-art LLM today—at least to third-party
researchers, auditors, and developers. Finally, while conventional
FTU explicitly leaves out the sensitive attribute, some approaches
use the sensitive attribute information to ensure that the model is
not even implicitly aware of the sensitive attribute through prox-
ies, such as zip code as a proxy for race and income [47, 57]. The
current lack of LLM documentation further prevents researchers
and socially aware application developers from studying model
awareness of sensitive attributes in the first place.
3.2 LLMs can render producer-side fairness
criteria obsolete
In the literature on fairness in recommender and information re-
trieval systems, the presence of multiple stakeholders has motivated
a framework of multi-sided fairness. In this section, we consider
each type of stakeholder in turn, highlighting the existing diffi-
culties of fairness for each type and showing that each challenge
compounds in the case of LLMs—particularly in the case of content
producers because LLMs can extract content and present it to users
with little or no compensation to its producer. Stakeholders are
typically divided into the consumers, producers, and subjects of
content [2, 13, 23, 62].
Definition 2. (Multi-sided fairness). A system achieves multi-
sided fairness if the model is fair with respect to each group of its
stakeholders, such as the consumers of its content (C-fairness), the
producers of its content (P-fairness), the consumers and producers
of its content considered together (CP-fairness), and the subjects of
the items that are being provided (S-fairness).
For users or consumers (i.e., the people or groups who receive
the recommendations), there are many possible fairness targets,
such as that each consumer or consumer group should receive
comparably high-quality recommendations [22, 24, 26, 51, 69]. This
target can more or less straightforwardly be applied to LLMs with
the aforementioned challenges of a large diversity of use cases and
user populations.
3



Source: data\tc20_2501.12962v1\referenced_papers\[5]_2406.03198.pdf (Page 3):

1st HEAL Workshop at CHI Conference on Human Factors in Computing Systems, 2024 Anthis et al.
For subjects of LLMs, it may be difficult to define, detect, or
enforce appropriate fairness metrics. For example, early in the
literature on fairness in information access systems, it was noted
that when searching for images of “CEO, ” Google returned a set of
images largely depicting men and, lower in the recommendation
list, an image of the popular toy “CEO Barbie. ” However, as in
the FTU decision of which sensitive attributes a system should be
unaware of and in what way, the challenges of deciding subject
representation in system output are compounded with LLMs. These
decisions typically rely on utility estimates, and that tends to be
a significant challenge with more general-purpose systems based
on unstructured data. For example, there is an open question of
whether the target distribution should be equal representation of
men, women, and other genders or a distribution that is weighted
towards the gender distribution of CEOs in the consumer’s home
location [28, 37, 59]. To achieve LLM fairness, this sort of open
question would need to be resolved for each of the many tasks done
by the LLM.
For producers (i.e., the people or organizations whose content
is recommended), also known as providers, the fairness target is
often the equitable distribution of exposure, either in terms of
relevance-free metrics that do not consider the relevance of the
content to the user—only that there is an equitable distribution—or
relevance-based fairness metrics that target an equitable exposure
conditional on relevance. In either case, fairness to producers is
a matter of how the exposure of those providing content to the
system is allocated to consumers. In the use case of LLMs that
perform information retrieval and information management tasks,
this framework can at times transfer directly. For example, if some-
one searches for “coffee shops in San Francisco” in an LLM chat
or search interface—as is being incorporated into the ubiquitous
modern search engine, Google—producer fairness could be defined
in terms of equitable exposure to the different brick-and-mortar
coffee shops in San Francisco. Even if the LLM system does not
direct users to particular websites, many users will presumably end
up visiting the cafes, which provides utility—fairly or unfairly—to
the producers. However, if users are searching for information in
the LLM system, such as asking, “How are coffee beans roasted?”
then LLMs can entirely circumvent the producers and upend the
conventional notion of producer-side fairness. If the LLM system
extracts information from websites without directing users to the
original source content, then it may be that none of the producers
receive any exposure or other benefits in the first place. One way to
make sense of this would be to consider the LLM system itself—or
the entity that developed, owns, and manages it—as another type
of stakeholder, one that takes all utility from the producers and
renders the conventional producer-side fairness criteria obsolete.
4 LLMS ARE TOO FLEXIBLE TO BE
GENERALLY FAIR
Much of the excitement surrounding LLMs is based on their general-
purpose flexibility across wide ranges of inputs, tasks, outputs, and
contexts. To some extent, they resemble a human agent, including
the ability to chain together these tasks into complex sequences, and
these areas of flexibility make many conventional fairness metrics
intractable.
4.1 Group fairness does not generalize across
populations
Group fairness metrics require independence between model clas-
sification and sensitive attributes, often conditional on relevant
information such as the ground-truth labels that the model aims
to predict (e.g., job performance for a model that assists in hiring
decisions). Three common metrics are:
Definition 3. (Demographic parity). A model achieves demo-
graphic parity if its predictions are statistically independent of
sensitive attributes.
Definition 4. (Equalized odds). A model achieves equalized odds
if its predictions are statistically independent of sensitive attributes
conditional on the true labels being predicted.
Definition 5. (Calibration). A model achieves calibration if the
true labels being predicted are statistically independent of sensitive
attributes conditional on the model’s predictions.
In binary classification, these metrics are achieved when equal-
ities hold between ratios in the confusion matrix: equal ratios of
predicted outcomes (demographic parity), equal true positive rates
and false positive rates (equalized odds), and equal precision (cali-
bration). Recent work includes extensions of these notions, such as
prioritizing the worst-off group by minimizing the maximum group
error rate [19]. Conventionally, group fairness requires knowing
the sensitive attributes to enforce the equalities, though recent work
has considered approaches for when the sensitive attributes are un-
available [36, 42, 76]. There are many methods for enforcing group
fairness metrics, such as the preprocessing of datasets proposed
by Feldman et al. [27] to guarantee bounds on demographic parity
and the more recent method proposed by Johndrow and Lum [34]
that can be applied to a wider variety of datasets.
LLMs present a challenge for group fairness metrics in part
because LLMs tend to be deployed across a wide range of data
distributions. Lechner et al. [43] showed that it is impossible for a
non-trivial model to perform fairly across all different data distri-
butions, such as regions or demographic groups, to which it might
be applied. In current discussions of algorithmic fairness (e.g., re-
cidivism), fairness is typically targeted at a local jurisdiction, which
ensures that the model is performing fairly for that location’s par-
ticular demographic mix and holistic characteristics but typically
cannot also achieve fairness in substantially different locations. The
purpose and use of LLMs makes it infeasible to restrict them to this
sort of targeted population.
In general, it is not clear what an appropriate base population
would be on which to detect and achieve group fairness for an
LLM. For example, one could “bootstrap” a predictive model for
recidivism prediction from an LLM simply by instructing it to make
a prediction about an individual based on a fixed set of that indi-
vidual’s characteristics with in-context learning as Li and Zhang
[45] did in predicting the label of a text-converted tabular dataset.
However, the data on which that LLM had been trained does not
admit an identifiable base population because a corpus of text is not
4



Source: data\tc20_2501.12962v1\referenced_papers\[5]_2406.03198.pdf (Page 4):

The Impossibility of Fair LLMs 1st HEAL Workshop at CHI Conference on Human Factors in Computing Systems, 2024
a structured database comprising people and their characteristics.
An LLM may be trained in part on such databases, but the output
of the model for such predictions will also be based on the wide
scope of unstructured natural language or other modalities of data
on which the model is trained.
Generalization across populations is also a concern for fairness
frameworks other than group fairness because of the wide range of
data, use cases, and social contexts at play in LLMs [60]. Here, we
consider two examples: individual fairness [20] and counterfactual
fairness, which is the most common causal notion of fairness [40].
Definition 6. (Individual fairness). A model achieves individual
fairness if similar individuals are treated similarly. Formally, this re-
quires that the distribution of model output is Lipschitz continuous
with respect to the distribution of model input.
Definition 7. (Counterfactual fairness). A model achieves coun-
terfactual fairness if the model would produce the same output for
an individual if they had a different level of the sensitive attribute.
In terms of individual fairness, it is not clear what similarity
metrics could be reasonably applied across the multitude of contexts
or, if multiple metrics were applied, how these could be judiciously
selected and guaranteed in each possible context. In terms of causal
fairness, including counterfactual fairness, it is often difficult to
identify the causal structure of the data-generating process in even
a single NLP task, and it would be an immense challenge for a single
model to account for all of the many different contextual factors
that determine counterfactuals or other causally distinct outcomes
across the varying populations.
4.2 Sensitive attributes proliferate in a
general-use setting
The preceding section considered the challenges of imposing fair-
ness across different data distributions. When considering different
sensitive attributes, given the issues discussed in Section 3.1, it may
not be tractable to exclude sensitive attributes from the training
data, and each of the different distributions and different tasks can
require fairness metrics to be enforced for a different set of sensitive
attributes. This is a challenge for the group fairness metrics already
defined, but the issue is particularly salient for the popular ideal
of fair representations within a machine learning model or fair
representations produced by one model and used by another [72].
Definition 8. (Fair representation). A representation is fair if it
does not contain information that can identify the sensitive at-
tributes of the individuals being represented.
In the fair representations framework, a system first maps the
dataset of individuals being represented to a probability distribution
in a novel representation space, such that the system preserves as
much information as possible about the individual while removing
all information about the individual’s sensitive attribute. The most
well-known example of this approach is Bolukbasi et al. [11], which
rigorously documented gender bias in Google News word embed-
dings, namely an association between occupations and a gender
vector (e.g., ®he− ®she), such that computer programmer was coded as
highly male while homemaker was coded as highly female. Indeed,
this is where much of the NLP fairness literature has focused, doc-
umenting similar biases across different word embedding models
Sesari et al. [see 61, for a review].
Researchers have developed a number of debiasing approaches
focused on the sensitive attribute dimension, such as zeroing the
projection of each word vector (e.g., each occupation) onto the
dimension itself [11] or training the model to align the sensitive
attribute dimension with the last coordinate of the embedding space,
so that it can be easily removed or ignored [75]. However, Gonen
and Goldberg [33] show that such approaches “are mostly hiding
the bias rather than removing it” because, after removal, word pairs
tend to maintain their similarity, which still reflects associations
with sensitive attributes—what Bolukbasi et al. [11] call “indirect
bias. ”
Achieving fairness in one LLM context may be contingent on
the removal of information or alteration of the statistical relation-
ships between the context-specific sensitive attribute and other
features of the data. For example, one may wish to exclude gender
information from financial lending decisions, but gender informa-
tion may be necessary for other tasks, such as drafting or editing
an email about a real-world situation that has important gender
dynamics that the sender hopes to communicate to the receiver.
Moreover, variables highly correlated with gender, such as biologi-
cal sex and pregnancy status, may be essential criteria for medical
decision-making. In general, attempts at debiasing for one context
may remove or distort important information for another context.
The naive approach of debiasing the model with respect to the
union of all potential sensitive attributes—even if it were empirically
feasible—would likely be too heavy-handed, leaving the model with
little information to be useful for any task. To effectively create a
fair LLM for every task and context, one would need to act upon
the parameters of the model with surgical precision to alter the
relationship between variables only when the model is instantiated
for a specific task and context. This is infeasible with current LLM
methods, such as fine-tuning, and currently we do not have robust
techniques to debias even a single problematic relationship without
incidentally obfuscating it or problematizing other relationships.
This game of fairness whack-a-mole seems indefinitely intractable.
Likewise, even if we could reduce the union of all potential sensitive
attributes to a manageable level, such as identifying a small set of
the most important to adjust for in each task, that would still require
yet-infeasible fine-grained adjustments to avoid counterproductive
side effects.
4.3 Fairness does not compose, but
fairness-directed composition may help
Whether a model’s behavior on a given task is fair or desirable
largely depends on how the model’s output will be used. In modern
AI systems including LLMs, the output of one model is often used
as the input to another model, but this produces an additional
challenge because fairness does not compose: a fairness guarantee
for each of two models is not a fairness guarantee for a system
composed of the two models—a point made most explicitly by
Dwork and Ilvento [21].
5



Source: data\tc20_2501.12962v1\referenced_papers\[5]_2406.03198.pdf (Page 0):

The Impossibility of Fair LLMs
Jacy Reese Anthis
University of Chicago
Kristian Lum
Google DeepMind
Michael Ekstrand
Drexel University
Avi Feller
University of California, Berkeley
Alexander D’Amour
Google Research
Chenhao Tan
University of Chicago
ABSTRACT
The need for fair AI is increasingly clear in the era of general-
purpose systems such as ChatGPT, Gemini, and other large lan-
guage models (LLMs). However, the increasing complexity of human-
AI interaction and its social impacts have raised questions of how
fairness standards could be applied. Here, we review the techni-
cal frameworks that machine learning researchers have used to
evaluate fairness, such as group fairness and fair representations,
and find that their application to LLMs faces inherent limitations.
We show that each framework either does not logically extend to
LLMs or presents a notion of fairness that is intractable for LLMs,
primarily due to the multitudes of populations affected, sensitive
attributes, and use cases. To address these challenges, we develop
guidelines for the more realistic goal of achieving fairness in par-
ticular use cases: the criticality of context, the responsibility of
LLM developers, and the need for stakeholder participation in an
iterative process of design and evaluation. Moreover, it may even-
tually be possible and even necessary to use the general-purpose
capabilities of AI systems to address fairness challenges as a form
of scalable AI-assisted alignment.
KEYWORDS
large language models, natural language processing, deep learn-
ing, human-AI interaction, algorithmic fairness, algorithmic bias,
fairness, bias, discrimination
1 INTRODUCTION
The rapid adoption of machine learning in the 2010s was accompa-
nied by increasing concerns about negative societal impact, particu-
larly in high-stakes domains. In response, there has been extensive
development of technical frameworks to formalize ethical and so-
cial ideals—particularly the foundational notion of “fairness”—so
that they can be evaluated and applied. Popular frameworks in
machine learning and natural language processing (NLP) include
group fairness [20] and fair representations [ 72]. In general, the
frameworks we have today are oriented towards systems that are
used in ways more-or-less self-evident from their design, typically
with well-structured input and output, such as the canonical ex-
amples of predicting default in financial lending [ 39], predicting
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
1st HEAL Workshop at CHI Conference on Human Factors in Computing Systems, May
12, 2024, Honolulu, HI, USA. Please cite the published version.
© 2024 Copyright held by the owner/author(s).
recidivism in criminal justice [ 4], and coreference resolution in
natural language [74].
Recently, there has been a surge of interest in generative AI and
general-purpose large language models (LLMs) that are pre-trained
for next-word prediction and instruction-tuned to satisfy human
preferences. LLMs are increasingly used for a multitude of tasks that
span both traditional areas of concern for bias and fairness—such
as evaluating resumes in hiring [6]—and areas less frequently, if at
all, discussed in the extant fairness literature—such as drafting and
editing emails [41], answering general knowledge queries [63], and
code completion in software development [8].
In this paper, we consider whether and how the fairness frame-
works can be applied to the LLM paradigm. We approach this topic
mindful of both the hotly contested issues already present in the
fairness literature [e.g., 17] and the challenges that other ascendant
paradigms have presented for the ideal of fairness, such as infor-
mation access systems [23]. For example, it is clear from the extant
literature that achieving multiple fairness metrics simultaneously is
generally intractable. Well-known impossibility results show that
multiple group fairness metrics, such as those defined by rates of
false positives and false negatives [16, 38] or demographic parity
(Definition 3) and calibration (Definition 5) [38], cannot be simul-
taneously achieved in real-world environments. In this paper we
develop a stronger claim that an LLM cannot achieve fairness even
on a single non-trivial metric.
Before reviewing the recent work on LLM fairness, we ground
our technosocial analysis in features of the LLM paradigm that seem
essential for fairness evaluation. First, at the technical level, LLMs
have exceptional flexibility. While the input and output have largely
been restricted to natural language, it is increasingly clear that a
wide range of content can be represented in LLM-suitable natural
language. Moreover, LLMs, or more broadly the class of so-called
“foundation models” [12], are increasingly multimodal, such as the
ability of GPT-4 [54] to receive text, images, or combinations of the
two as input. This flexibility is reflected in the lack of a self-evident
use case or even a relatively narrow set of use cases—the existence
of which has grounded technical fairness analysis in the past.
Second, at the social level, our analysis foregrounds the multi-
tude of diverse stakeholders in LLM systems and their evolving
relationships. As discussed in Section 3.2, there are people or or-
ganizations who create datasets, curate datasets, develop models,
deploy and manage models, and build downstream user-facing ap-
plications—although there is often now a single organization that
develops the model and plays many of the other roles. As with other
information systems, there are always users—whether individuals
or groups—who may have widely varying competencies [25]. Usu-
ally there are also subjects of the content produced by the system,
arXiv:2406.03198v1  [cs.CL]  28 May 2024



Source: data\tc20_2501.12962v1\referenced_papers\[5]_2406.03198.pdf (Page 1):

1st HEAL Workshop at CHI Conference on Human Factors in Computing Systems, 2024 Anthis et al.
such as the people or groups that are described in an information
request. There may also be researchers and critics from academia,
governments, and nonprofit organizations analyzing LLM systems
and their societal impacts and attempting to steer them in socially
beneficial directions.
With these dynamics in mind, Section 2 discusses work to date on
LLM fairness, which has focused on association-based metrics and
practical challenges rather than the more nuanced metrics and in-
herent challenges we articulate in the present work. Section 3 shows
that there is a fundamental, logical incompatibility between some
extant frameworks and modern LLM systems. Section 4 shows that,
in the other frameworks, the flexibility of LLMs across data, tasks,
stakeholders, and populations renders a general guarantee, stamp,
or certificate of a fair LLM intractable. Section 5 proposes moving
forward in the important goals of fairness and harm reduction in
LLMs by transmuting the inherent challenges into three general
guidelines: the criticality of context, the responsibility of LLM de-
velopers, and the need for iterative and participatory design. We
conclude with a practical discussion of what these guidelines imply
for current LLM practices of curating training data, instruction tun-
ing, prompt engineering, personalization, and using interpretability
tools.
2 RECENT WORK ON LLM FAIRNESS
Interest in LLMs, particularly models based on the Transformer
architecture introduced in 2017 [68], has greatly accelerated since
2020 with the popularity of OpenAI’s GPT models [53] and more
recently the proliferation of LLMs, such as Anthropic’s Claude and
Google’s Gemini. A number of recent papers have discussed and
evaluated bias, discrimination, and fairness in LLM-generated text.
2.1 Association-based fairness metrics
Two recent reviews of this nascent literature [30, 44] enumerate a
variety of fairness metrics, each of which constitutes an association
between a feature of the embedding space or model output (token
probabilities or generated text) and a sensitive attribute. This in-
cludes text measures such as a disparity of sentiment and toxicity
in Wikipedia sentence completion across the profession, gender,
race, religion, or political ideology of the article subject [18], the
occurrence of violent words after a phrase such as “Two muslims
walked into a” [3], and the topics introduced when completing sen-
tences from fiction novels [48]. Other approaches include creating
datasets of LLM continuations of text that stereotypes, demeans,
or otherwise harms in ways related to gender and sexuality [29];
evaluating conventional fairness metrics when an LLM is used for
a conventional machine learning task, such as predicting outcomes
based on a text-converted tabular dataset [45]; recommending mu-
sic or movies to a user who specifies their sensitive attribute such
as race or religion [73]; and testing whether the model provides
the same “yes” or “no” answer when asked for advice by a user
who specifies their gender [66]. In general, it would be very sur-
prising if these models did not have disparate output given how
they are trained, but these studies have provided useful, rigorous
documentation.
However, a lack of such disparities, which have been the pre-
dominant target of study in LLM fairness studies to date, does not
constitute fairness as conceptualized in machine learning or other
fields such as philosophy Binns [7]. For example, in the framework
of group fairness, which uses conditional equivalencies across sensi-
tive attributes, the unconditional equivalence in model classification
or prediction is known as demographic parity (see Definition 3).
While demographic parity is an important metric for comprehen-
sive fairness evaluation and enforcement, achieving it is rarely if
ever viewed as achieving algorithmic fairness. In general, while
the popular benchmarks such as WinoBias [74] and BBQ [56] that
have been applied to LLM-generated text to date capture important
information about model behavior in relation to sensitive attributes,
there is little reason to think that strong model performance would
imply fairness itself.
When existing work on LLMs has touched on richer notions of
fairness, that has been undertaken in a highly constrained manner.
For example, while Li et al . [44] briefly discussed counterfactual
fairness (Definition 7), they only did so by summarizing two pa-
pers that merely perturb the LLM input (e.g., converting Standard
American English to African American English [46]), which does
not acknowledge or address the inherent challenges we present in
Section 4.1 of how counterfactual fairness and other metrics fail to
generalize across populations in which the data-generating process
could vary significantly and counterfactuals would not merely vary
in writing style or other features directly observable from the text.
2.2 Practical challenges
Existing work has motivated and articulated significant challenges
in evaluating and enforcing fairness in LLMs. Both Gallegos et al.
[30] and Li et al. [44] provide useful summaries, including the need
to center marginalized communities through participatory design
[9, 10] and to develop better proxy metrics, such as by bridging
the divide between intrinsic and extrinsic bias metrics [32]. These
are important challenges to be addressed, but even if each of them
were, the inherent challenges that are the focus of the present work
would remain.
The inherent challenges of LLM fairness have yet to be fore-
grounded in part because existing work has focused on relatively
narrow use cases, often analyzing the LLM as a classifier or recom-
mender system in conventional machine learning tasks through the
use of in-context learning to steer the model towards the conven-
tional output format (e.g., a binary data label or recommendation)
[45, 66, 73]. It is true that, given the flexibility of LLMs as general
text-to-text or multimodal models, they can be deployed—though
not necessarily with strong performance—to any conventional task
in which the input and output is a series of tokens or other suitable
representation. However, LLMs are not primarily used as substitutes
for conventional, narrow-purpose models. Examples of the rapidly
growing set of use cases include coding (e.g., creation, autocomple-
tion), communication (e.g., drafting emails, translation), gathering
information (e.g., web search, proprietary search), recreation (e.g.,
bedtime stories, personalized travel plans), and simulation (e.g.,
data labeling, persona generation). Many of these tasks have not
2



### Claim 3/36

#### Claim Text
The AI Act is based on the legal approach of discrimination-related regulations [26].

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[26]_2403.20089.pdf (Page 1):

sketch how the AI Act could provide a means to solve the enforcement problems of both—non-
discrimination law and algorithmic fairness—and comment on upcoming challenges regulators
and developers will face when specifying and verifying technical requirements.
2. Non-discrimination law vs. algorithmic fairness
Legal Context: Non-discrimination law and its shortcomings. From a legal perspective,
non-discrimination law appears to be suitable to address the potential harms of unfair AI sys-
tems at first glance. However, legal scholars from both sides of the Atlantic have demonstrated
that U.S. [22] and EU [ 16] non-discrimination law alike may fall short in doing so. One of
the main deficiencies of traditional non-discrimination regimes in the context of algorithmic
discrimination is law enforcement. Enforcement has always been a central shortcoming of
non-discrimination law, especially in jurisdictions that primarily rely on individual litigation
(cf., [23, 24]). In such jurisdictions, individual victims face substantial problems when it comes
to recognizing, proving, and bringing instances of discrimination before the courts. AI systems
exacerbate these problems [25]. Due to the opacity of these systems, those affected by algorith-
mic discrimination are often unable to recognize instances of (potential) discrimination [26].
Moreover, even when individuals suspect discrimination, restricted access to models or training
data severely impedes their ability to meet the requirements of the burden of proof imposed
on them by procedural law [16]. Furthermore, European non-discrimination law is tailored to
individual cases of discrimination hampering its application to broad-scale goals like designing
fair AI systems. Non-discrimination regimes, therefore, face substantial challenges when it
comes to enforcing the principles of equality and non-discrimination.
Technical context: algorithmic fairness and its shortcomings. On a technical level,
methods for algorithmic fairness from the field of computer science set out to fill this gap. By
developing a plethora of technical bias definitions and fairness metrics (cf. [27, 28, 29]) as well as
practical bias detection and bias mitigation techniques [30, 31, 32, 33], computer scientists try to
implement ethical and legal fairness considerations “by design” [34]. The shortcomings of these
technical fairness approaches, however, are twofold: First, formalization and quantification
will never provide answers to fundamentally normative challenges such as selecting the right
fairness metric for the right context or trading off conflicting objectives [35, 36]. Such challenges
arising from conflict between values can be supported but not be solved by formal methods
[37]. Second, due to its orientation towards a specific academic audience and reliance on
self-governance, discourse on algorithmic fairness faces its own “enforcement problems” [38].
The AI Act may alleviate both—the enforcement problems of non-discrimination law and the
technical fairness discourse—alike.
3. Implications of the AI Act
Enforcement “by design”? According to Recital 4a, the AI Act explicitly aims to protect the
fundamental rights set out in Art. 2 of the Treaty of the European Union. Among these rights
are equality and non-discrimination in particular. In order to prevent algorithmic discrimination,



Source: data\tc20_2501.12962v1\referenced_papers\[26]_2403.20089.pdf (Page 2):

the regulation establishes special requirements (Art. 6 et seq. AI Act) for high-risk systems in
the areas of education (Recital 35), employment (Recital 36), insurance and credit (Recital 37),
law enforcement (Recital 38), as well as migration (Recital 39). However, despite its explicit goal
to prevent discrimination, the regulation lacks a clear substantive standard for determining
when unequal treatment is inadmissible. According to Art. 10(2)f AI Act “[t]raining, validation
and testing data sets shall be subject to data governance and management practices appropriate
for the intended purpose of the AI system” and thus have to be examined for “possible biases
that are likely to [...] lead to discrimination prohibited under Union law”. The AI Act therefore
leaves the judgment call about what constitutes illegal discrimination to existing legislation.
However, traditional non-discrimination law’s requirements can only be implemented during
model development (as intended by the AI Act) if they are “translated” into technical fairness
requirements. To achieve this goal, scholars from all domains are bound to collaborate. When
doing so, they must proceed in a conscious and contextualizing manner and take into account
the diverging perspectives of AI Act and non-discrimination law. European non-discrimination
law is tailored to individual instances of discrimination after an AI model has been deployed—an
inherently retrospective approach. In contrast to this, the AI Act prospectively demands fairness
interventions by implementing non-discrimination requirements at the stage of model design.
Guidance by democratically justified institutions on how to implement such requirements might
bridge the gap toward alleviating both the legal and the technical enforcement problems.
Enabling “bias detection and correction”? Legal requirements for the development of AI
systems are not only subject to the AI Act. Due to the tension between fairness and privacy
during the training and evaluation stage of AI, conflicts with data protection law may equally
arise. On the one hand, ignoring personal demographic data promotes the same risk as the
widely rejected idea of fairness through unawareness because legally protected attributes like
race and gender usually correlate to innocuous proxy variables [39, 40]. If protected attributes
are unavailable during model training and evaluation, these subtle correlations cannot be
accounted for, nor can technical fairness metrics be tested and optimized. On the other hand,
Art. 9 GDPR places particularly high demands on the lawful processing of personal data about
special categories. Therefore, the same sensitive data that is protected by data protection law
is also essential to effectively avoid discriminatory outputs. The AI Act seeks to mitigate this
tension by broadening the scope of lawful data processing. Art. 10(5) AI Act states that “[t]o
the extent that it is strictly necessary for the purposes of ensuring bias detection and correction in
relation to the high-risk AI systems [...], the providers of such systems may exceptionally process
special categories of personal data referred to in Art. 9(1) [GDPR]. ” This is accompanied by
Recital 44c, which adds that “[i]n order to protect the right of others from the discrimination that
might result from the bias in AI systems [...] the providers should, exceptionally, [...] be able to
process also special categories of personal data, as a matter of substantial public interest within
the meaning of Art. 9(2)(g) [GDPR]. ”Therefore, discrimination and fairness considerations can
provide a justification for data processing during the training phase of high-risk AI systems.
However, balancing the public and private interests regarding non-discrimination and privacy
will inevitably lead to intricate trade-offs.



Source: data\tc20_2501.12962v1\referenced_papers\[26]_2403.20089.pdf (Page 0):

Implications of the AI Act for Non-Discrimination Law
and Algorithmic Fairness
Luca Deck1,2, Jan-Laurin Müller 1, Conradin Braun 1, Domenique Zipperling 1,2 and
Niklas Kühl1,2
1University of Bayreuth
2Fraunhofer FIT
Abstract
The topic of fairness in AI, as debated in the FATE (Fairness, Accountability, Transparency, and Ethics
in AI) communities, has sparked meaningful discussions in the past years. However, from a legal
perspective, particularly from the perspective of European Union law, many open questions remain.
Whereas algorithmic fairness aims to mitigate structural inequalities at design-level, European non-
discrimination law is tailored to individual cases of discrimination after an AI model has been deployed.
The AI Act might present a tremendous step towards bridging these two approaches by shifting non-
discrimination responsibilities into the design stage of AI models. Based on an integrative reading
of the AI Act, we comment on legal as well as technical enforcement problems and propose practical
implications on bias detection and bias correction in order to specify and comply with specific technical
requirements.
Keywords
EU AI Act, Algorithmic fairness, Non-discrimination law, Ethical AI
1. Introduction
AI systems’ propensity to discriminate against legally protected groups has been demonstrated
across multiple social contexts, ranging from decision-support systems for criminal risk assess-
ment [1], recruiting [2], and credit scoring [3], to applications in computer vision [4, 5, 6, 7] and
natural language processing [8, 9, 10]. In light of the rapid advancements of AI, the increasing
use of AI systems across multiple domains has triggered a broad and interdisciplinary debate
on the “ethics of algorithms” [11, 12, 13]. Central to this debate are the FATE principles (fair-
ness, accountability, transparency, and ethics), with fairness encompassing the social goals of
non-discrimination, inclusion, and equality [14, 15].
The discourse at the interface with legal scholarship, however, is only starting to gain traction
(e.g., [16, 17, 18, 19, 20, 21]). In this short paper, we make three contributions: First, we briefly
retrace the academic discourses on non-discrimination law and algorithmic fairness to highlight
their current misalignment. Second, we argue that the European Union’s AI Act might pose
a seminal link to merging these debates. Based on this integrative conception, we thirdly
EWAF’24: European Workshop on Algorithmic Fairness, July 01–03, 2024, Mainz, Germany
/envel⌢pe-⌢penluca.deck@uni-bayreuth.de (L. Deck); jan-laurin.mueller@uni-bayreuth.de (J. Müller);
conradin.braun@uni-bayreuth.de (C. Braun); domenique.zipperling@uni-bayreuth.de (D. Zipperling);
kuehl@uni-bayreuth.de (N. Kühl)
© 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
CEUR
Workshop
Proceedings
http://ceur-ws.org
ISSN 1613-0073
CEUR Workshop Proceedings (CEUR-WS.org)
arXiv:2403.20089v2  [cs.AI]  26 Jun 2024



Source: data\tc20_2501.12962v1\referenced_papers\[44]_2407.10329.pdf (Page 42):

39 
4. The AI Act  
The EU Artificial Intelligence Act167 and the forthcoming AI liability directives are unlikely to 
establish sufficient accountability mechanisms that would remedy the discrimination-related 
harms discussed in this chapter.168 
The AI Act is predominantly a product safety law. In fact, the original drafts did not have any 
individual rights components.169 Only the drafts of the European Parliament170 foresaw that 
individual complaint-based mechanisms were incorporated in the draft. While the most recent 
version of the AI Act includes a right to lodge a complaint with a Market Surveillance Authority 
and right to explanation of individual decision-making,171 those provisions are unlikely to 
alleviate most of the concerns we have noted above.  
While bias is recognised as an AI-related issue that needs addressing,172 it is unclear whether 
discrimination-related harms as exemplified in this paper can be seen as a violation of the AI 
Act against which complaints can be brought. For example, Article 10 AI Act does address the 
issue of bias in training data, but it is targeted at developers of predictive AI systems in high 
risk areas, not at developers of genAI. If genAI is developed for a high risk area, developers 
have to follow these rules. However, the viability of Article 10 in mitigating generative 
discrimination will crucially hinge on the interpretation of 'bias' in the AI Act. At the moment, 
the question remains open whether the harms envisioned in this chapter would fall under the AI 
Act’s concept of ‘bias.’ Perhaps regulators will interpret ‘bias’ in a technical (diversity of 
training data) and less social and ethical (demeaning and abusive contents) way.  
The right to explanation might also not be helpful for our purposes. The provision focuses on 
the right to have decisions explained that were rendered in high risk areas by predictive AI such 
as employment, criminal justice or education. The AI Act does not classify genAI as a high-
risk application and thus the right to explanation does not generally apply to genAI.173 Further, 
                                                 
167 Based on the final EP version of the AI Act available under 
https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138-FNL-COR01_EN.pdf.  
168 Sandra Wachter, ‘Limitations and Loopholes in the E.U. AI Act and AI Liability Directives: What This Means 
for the European Union, the United States, and Beyond’ (2024) 26 Yale Journal of Law and Technology. 
169 Michael Veale and Frederik Zuiderveen Borgesius, ‘Demystifying the Draft EU Artificial Intelligence Act — 
Analysing the Good, the Bad, and the Unclear Elements of the Proposed Approach’ (2021) 22 Computer Law 
Review International 97; Hacker, Engel and Mauer (n 12); Martin Ebers, ‘Standardizing AI-The Case of the 
European Commission’s Proposal for an Artificial Intelligence Act’ [2021] The Cambridge handbook of artificial 
intelligence: global perspectives on law and ethics; Johann Laux, Sandra Wachter and Brent Mittelstadt, ‘Three 
Pathways for Standardisation and Ethical Disclosure by Default under the European Union Artificial Intelligence 
Act’ (2024) 53 Computer Law & Security Review 105957. 
170 European Parliament, ‘Compromise Amendments on the Draft Report Proposal for a Regulation of the 
European Parliament and of the Council on Harmonised Rules on Artificial Intelligence (Artificial Intelligence 
Act) and Amending Certain Union Legislative Acts (COM(2021)0206 – C9 0146/2021 – 2021/0106(COD))’ 
(2023) KMB/DA/AS 
<https://www.europarl.europa.eu/meetdocs/2014_2019/plmrep/COMMITTEES/CJ40/DV/2023/05-
11/ConsolidatedCA_IMCOLIBE_AI_ACT_EN.pdf>.  
171 Article 85 and 86 AI Act. Art 99 (10) also offers recourse against decisions or omission to take action of the 
Market Surveillance Authority. 
172 See Article 10 AI Act.  
173 Natali Helberger and Nicholas Diakopoulos, ‘ChatGPT and the AI Act’ (2023) 12 Internet Policy Review 3, 6 
who argue that genAI should be its own high-risk category.



Source: data\tc20_2501.12962v1\referenced_papers\[44]_2407.10329.pdf (Page 15):

12 
discrimination and genAI.62 Therefore, this section is explorative, and does not focus on 
analyzing or systemizing case law.  
a. Applicability 
To cover genAI scenarios, non-discrimination law would first have to apply to these situations. 
Several complications arise when applying non-discrimination law to genAI. For example, 
many non-discrimination statutes have a narrow scope, and focus on certain sectors only. To 
illustrate, some EU non-discrimination directives only apply to employment cases, including 
recruitment, but not to other contracts or exchanges.63 EU Member States had to implement the 
directives into national law; in doing so, some Member States have extended the sectoral scope 
of the EU rules.64  
Nevertheless, some situations in which genAI provides discriminatory output may not be 
covered by non-discrimination law. For example, imagine a large language model fine-tuned 
by a law firm and used for generating individually negotiated (non-employment) contracts. EU 
non-discrimination law, in market exchanges, focuses on goods and services offered to the 
general public.65 Hence, specific models used by a select group of parties only may fall between 
the cracks if their output does not constitute a publicly available service or good (and is not in 
the domain of employment, either, which is generally covered by EU - and US - non-
discrimination law). Furthermore, only certain groups are protected by non-discrimination law 
(religion, ethnicity, gender etc.), both in the EU and the US. However, AI may unfairly disfavor 
artificially created groups that do not match these traditional categories.66 
b. Discrimination 
Non-discrimination law delineates various forms of harmful actions that can lead to legal 
consequences. For genAI output to be actionable under non-discrimination law, it must either 
constitute direct or indirect discrimination against individuals or groups within protected 
categories, or amount to harassment. 
                                                 
62 For current cases, see, e.g., the case tracker under https://www.bakerlaw.com/services/artificial-intelligence-
ai/case-tracker-artificial-intelligence-copyrights-and-class-actions/. 
63 Philipp Hacker, 'Teaching fairness to artificial intelligence: existing and novel strategies against algorithmic 
discrimination under EU law' (2018) 55 Common Market Law Review 1143. 
64 This is the case, for example, in Germany.  
65 Stefan Grundmann, 'The Future of Contract Law' (2011) ERCL 490, 506; Philipp Hacker P, 'Teaching fairness 
to artificial intelligence: existing and novel strategies against algorithmic discrimination under EU law' (2018) 55 
Common Market Law Review 1143, 1156 f. 
66 Janneke Gerards and Frederik Zuiderveen Borgesius , 'Protected grounds and the system of non-discrimination 
law in the context of algorithmic decision-making and artificial intelligence' (2022) 20 Colo Tech LJ 1; Sandra 
Wachter, 'Affinity profiling and discrimination by association in online behavioral advertising' (2020) 35 Berkeley 
Tech LJ 367, Wachter, S. (2022). The theory of artificial immutability: Protecting algorithmic groups under anti-
discrimination law. Tul. L. Rev., 97, 149.



### Claim 4/36

#### Claim Text
This interdisciplinary challenge needs to acknowledge that computer science fairness and legal non-discrimination regulation are moving in the same direction; however, they present different concepts [27].

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[26]_2403.20089.pdf (Page 3):

4. Practical challenges for compliance
Defining bias: what are “appropriate” fairness metrics? The discussed implications of
the AI Act raise two important questions on how to put non-discrimination and fairness into
practice. First, the concept of technical fairness metrics begs the question which one(s) may
be “appropriate for the intended purpose of the AI system” (Art. 10(2)f AI Act). Technical
fairness definitions have already been examined for their compatibility with moral norms [41]
and non-discrimination regimes [17, 18, 19, 42, 21] alike. However, legal concepts relying on
flexible ex-post standards and human intuition are in tension with the mathematical need for
precision and ex-ante standardization [21, 42]. Also, the interdisciplinary discourse needs to
acknowledge that fairness and non-discrimination might present inherently different concepts
targeted at different social contexts. Prior works have suggested that a single standard of fairness
can be achieved by “translating” legal non-discrimination requirements from the employment
context into technical fairness metrics [17, 19]. However, the heterogeneity of social contexts
(e.g., employment versus criminal sentencing) demands a corresponding flexibility in fairness
requirements [43, 44]. Instead of aiming for a one-size-fits-all solution, we therefore recommend
applying the landscape of available technical fairness metrics to different legal conceptions of
discrimination depending on the societal context.
Detecting and correcting bias: when are biases “likely to lead to discrimination”? The
second challenge is defining when “possible biases that are likely to [...] lead to discrimination”.
Technical fairness metrics such as statistical parity or equalized odds offer an actionable approach
to measure and mitigate “bias” [45, 30, 21, 46]. However, it remains unanswered what kind of
evidence would signal sufficient efforts of bias detection and correction. Setting aside the debate
on metric selection, let us assume algorithmic hiring requires male and female applicants to
receive equal hiring rates (demographic parity). Statistical hypothesis testing provides a suitable
method to verify compliance with this requirement, in this case a simple z-test. To test the
hypothesis of compliance with demographic parity, we are interested in the test’s error rates,
i.e., falsely detecting a violation (type 1 error) or the likelihood of failing to detect a violation
(type 2 error). Notably, a larger disparity in hiring probabilities between groups and a larger
sample size decreases type 2 error. Unfortunately, the z-test is also sensitive to the acceptance
rate—particularly for small sample sizes. For example, for 1000 male and 1000 female applicants,
type 2 error decreases by 0.8% - points if only 700 instead of 900 applicants are accepted—despite
identical group disparities (see Appendix A). This effect is especially strong for imbalanced
datasets. For 1800 male and 200 female applicants, type 2 error even decreases by 6% - points if
only 780 instead of 980 applicants are accepted—again, despite identical group disparities (see
Appendix A). Our example highlights the need for guidance in selecting appropriate tests and
specifying standards for the error rates of tests utilized in bias detection.
5. Conclusion
In this short paper, we outlined how the AI Act could promote the convergence of legal non-
discrimination discourse and technical algorithmic fairness discourse. While we sketch its poten-



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 3):

4 
 
computer science, software engineering, and mathematics. These 
groups have developed numerous measures and methods  to mitigate 
bias and  improve fairness in algorithmic systems. However, the 
majority of these tools have been built in isolation from policy and civil 
societal contexts and lack serious engagement with philosophical, 
political, legal, and economic theories of  equality and  distributive 
justice.7 Reflecting this, most define fairness in simpl e terms, where 
fairness means red ucing gaps in performance or outcomes between 
demographic groups. Successfully achieving algorithmic fairness has 
come to mean satisfying one of these simple mathematical definitions, 
while preserving as much of the accuracy of the original system as 
possible. 
This oversimplification of equality through fairness measures could 
possibly be attributed to the relative youth of fairML . However, the 
practical impact of the approach adopted by the field to date is morally 
troubling. Many current fairness measures have been shown to suffer 
from both fairness and performance degradation, or “levelling down,” 
where fairness is achieved by making every group worse off , or by 
bringing better performing groups down to the level of worse performing 
groups.8 Levelling down is effectively fairness achieved by breaking the 
system, for example by making a classifier less accurate so it performs 
equally badly across all relevant groups. 
Levelling down is a symptom of the decision to measure fairness 
solely in terms of equality, or disparity between groups in performance 
and outcomes, while ignoring other relevant features of distributive 
justice such as absolute welfare or priority which are more difficult to 
quantify and directly measure in research and development 
environments. When fairness can only be measured in terms of 
distribution of performance or outcomes, corrective actions can likewise 
only target how these goods are distributed between groups. The field 
effectively only has egalitarian tools at its disposal which value equality 
of treatment and outcomes while ignoring other goods of distributive 
justice. Likewise, the prevalence of levelling down in fairML suggests 
that the field is, intentionally or otherwise, adopting a strict egalitarian 
approach to questions of distributive justice  in which the only 
(measurable) value is equality. We name these trends in fairML ‘strict 
egalitarianism by default’. 
Strict e galitarianism by default , at least in its most gratuitous 
forms, runs counter to both the stated objectives of fairness measures 
 
7 Reuben Binns, On the apparent conflict between individual and group fairness , in 
PROCEEDINGS OF THE 2020 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND 
TRANSPARENCY 514 (2020), https://doi.org/10.1145/3351095.3372864 (last visit ed Aug 
14, 2022). 
8 DOMINIK ZIETLOW ET AL ., Leveling Down in Computer Vision: Pareto Inefficiencies in 
Fair Deep Classifiers, (2022), http://arxiv.org/abs/2203.04913 (last visited Jun 10, 2022).



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 4):

5 
 
as well as the presumptive aim of the field: to improve outcomes for 
historically disadvantaged or marginalised groups .9 It co nceives of 
equality in simplistic comparative terms, ignoring absolutes of welfare 
and justice which are necessary to achieve substantive equality rather 
than mere formalistic equality, or equal treatment..10  
When fairness can only be achieved by making everyone worse off in 
material or  relational terms  through injuries of stigma, loss of 
solidarity, unequal concern,  and missed opportunities for substantive 
equality, something would appear to have gone wrong in translating the 
vague concept of ‘fairness’ i nto practice. Equality should aim to make 
people better off, not simply to reduce them to a common level of harm.11 
Simple mathematical definitions can be satisfied without regard for 
how parity is achieved in practice and the significant material and 
relational harms, and opportunity costs,  for the people affected.  The 
huge interest that exists algorithmic fairness pro vides an opportunity 
to substantively address longstanding inequalities in society. Enforcing 
 
9 FairML does not have universally agreed guiding principles, but prior work can 
provide some indication of its values and aims. In a 2019 paper critical of the state of 
the field, Keyes et al. defined the ‘Fair’ value of the Fairness, Accountability, and 
Transparency in Machine Learning (FAT -ML or FAccT -ML) research network as 
ensuring that algorithmic syste ms are “lacking biases which create unfair and 
discriminatory outcomes .” See: Os Keyes, Jevan Hutson & Meredith Durbin, A 
mulching proposal: Analysing and improving an algorithmic system for turning the 
elderly into high-nutrient slurry, in EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE 
ON HUMAN FACTORS IN COMPUTING SYSTEMS 1 (2019). More recently, Cooper et al. suggest 
that the field is motivated by the fact that “automated decision systems that do not 
account for systemic discrimination i n training data end up magnifying that 
discrimination; to avoid this, such systems need to be proactive about being fair .” See: 
A. Feder Cooper, Ellen Abrams & Na Na, Emergent Unfairness in Algorithmic Fairness-
Accuracy Trade-Off Research, in PROCEEDINGS OF THE 2021 AAAI/ACM CONFERENCE ON 
AI, ETHICS, AND SOCIETY 46, 51 (2021), https://dl.acm.org/doi/10.1145/3461702.3462519 
(last visited Jul 13, 2022). Early case studies in the field are similarly instructive, such 
as the famous COMPAS case in which a risk recidivism algorithm was alleged by 
journalists at ProPublica to be biased against Black defendants, routinely assigning 
them higher risk scores than comparable white defendants . See: Julia Angwin et al., 
Machine bias , 23 PROPUBLICA, MAY 2016 (2016).  These examples suggest work on 
algorithmic fairness is motivated at least in part by a desire to improve the situation of 
disadvantaged people that unjustifiably receive worse treatment or outcomes than their 
peers. How fairness, discrimination, and bias are conceptualised and measures, a nd 
likewise what is justified in differential treatment, opportunities, and results of course 
differs drastically across the field and use cases, but the underlying motivation to help 
people who are unjustifiably harmed by algorithmic systems seems clear a nd 
uncontroversial. 
10 Sandra Wachter, Brent Mittelstadt & Chris Russell, Bias preservation in machine 
learning: the legality of fairness metrics under EU non -discrimination law, 123 W. VA. 
L. REV. 735 (2021). 
11 LARRY S. TEMKIN, INEQUALITY (1993); Nils Holtug, Egalitarianism and the Levelling 
down Objection, 58 ANALYSIS 166 (1998); Brett Doran, Reconsidering the Levelling-down 
Objection against Egalitarianism , 13 UTILITAS 65 (2001); Derek Parfit, Equality or 
Priority?, in THE IDEAL OF EQUALITY 81 (Matthew Clayton & Andrew Williams eds., 
2002).



Source: data\tc20_2501.12962v1\referenced_papers\[44]_2407.10329.pdf (Page 16):

13 
● Direct discrimination occurs when an individual is treated less favorably than another 
in a comparable situation based on a protected attribute.67 The definition underscores 
the necessity of unfavorable treatment based on characteristics such as ethnicity, 
gender, disability, or age. The disadvantage may be of a material or immaterial nature.68 
● Indirect discrimination refers to situations where a seemingly neutral policy, criterion, 
or practice (PCP) places members of a protected group at a particular disadvantage 
compared to others.69 Thus, the essence of indirect discrimination lies in the result, often 
statistical, burden imposed on protected individuals or groups. What is required, here, 
is a disadvantage, i.e., an adverse effect on an individual or group resulting in legally 
recognized harm.70 
● Harassment, finally, constitutes actionable discrimination 'when an unwanted conduct 
related to racial or ethnic origin takes place with the purpose or effect of violating the 
dignity of a person and of creating an intimidating, hostile, degrading, humiliating or 
offensive environment. In this context, the concept of harassment may be defined in 
accordance with the national laws and practice of the Member States.'71 Harassment is 
similarly defined in US law.72  
One of the challenges with applying non-discrimination laws, and the definitions just 
mentioned, to genAI is aligning the communicative outputs of AI, such as speech acts, images, 
or videos, with traditional concepts of direct or indirect discrimination that typically focus on 
more tangible decisions or actions that differentiate among individuals. Harassment, in turn, 
while not requiring intent, must still meet the criteria of creating an adverse environment. 
Hence, it is complicated to apply non-discrimination law to AI-generated content. 
In the realm of more traditional AI-driven discrimination (non-genAI discrimination), most 
real-world examples can, from a legal perspective, be seen as indirect discrimination, 
characterized by statistical disadvantages for specific groups.73 Some examples of more 
traditional AI-driven discrimination could also be qualified as direct discrimination, though.74  
                                                 
67 Article 2(2)(a) Employment Equality Directive 2000/78/EC. 
68 District Court (LG) Frankfurt a. M., Judgment of August 26, 2021, Case 2-30 O 154/20, para. 17; BeckOGK-
Mörsdorf, § 3 AGG, Rn. 27. 
69 Article 2(2)(b) Employment Equality Directive 2000/78/EC. 
70 Ellis and Watson, EU anti-discrimination law 
71 Art 2(3) of the Race Equality Directive [emphasis by authors]. 
72 See, e.g., for a comparative perspective, Alessandro Fabris and others, 'Fairness and Bias in Algorithmic Hiring' 
(2023) arXiv preprint arXiv:230913933, 32-35; Gabrielle Friedman and James Q. Whitman, 'The European 
transformation of harassment law: discrimination versus dignity' (2002) 9 Colum J Eur L 241; Joanna Lahey, 
'International comparison of age discrimination laws' (2010) 32 Research on Aging 679; see also, on key US 
concepts, Solon Barocas and Andrew Selbst, 'Big data's disparate impact' (2016) California Law Review 671; 
Pauline Kim, 'Data-driven discrimination at work' (2016) 58 Wm & Mary L Rev 857. 
73 See, e.g., Philipp Hacker, 'Teaching fairness to artificial intelligence: existing and novel strategies against 
algorithmic discrimination under EU law' (2018) 55 Common Market Law Review 1143, 1151 ff.; Frederik 
Zuiderveen Borgesius, 'Discrimination, artificial intelligence, and algorithmic decision-making' (2018) 19; Sandra 
Wachter, Brent Mittelstadt and Chris Russell, 'Why fairness cannot be automated: Bridging the gap between EU 
non-discrimination law and AI' (2021) 41 Computer Law & Security Review 105567. 
74 Sandra Wachter, 'Affinity profiling and discrimination by association in online behavioral advertising' (2020) 
35 Berkeley Tech LJ 367; Jeremias Adams‐Prassl, Reuben Binns, and Aislinn Kelly‐Lyth, ‘Directly discriminatory



Source: data\tc20_2501.12962v1\referenced_papers\[26]_2403.20089.pdf (Page 2):

the regulation establishes special requirements (Art. 6 et seq. AI Act) for high-risk systems in
the areas of education (Recital 35), employment (Recital 36), insurance and credit (Recital 37),
law enforcement (Recital 38), as well as migration (Recital 39). However, despite its explicit goal
to prevent discrimination, the regulation lacks a clear substantive standard for determining
when unequal treatment is inadmissible. According to Art. 10(2)f AI Act “[t]raining, validation
and testing data sets shall be subject to data governance and management practices appropriate
for the intended purpose of the AI system” and thus have to be examined for “possible biases
that are likely to [...] lead to discrimination prohibited under Union law”. The AI Act therefore
leaves the judgment call about what constitutes illegal discrimination to existing legislation.
However, traditional non-discrimination law’s requirements can only be implemented during
model development (as intended by the AI Act) if they are “translated” into technical fairness
requirements. To achieve this goal, scholars from all domains are bound to collaborate. When
doing so, they must proceed in a conscious and contextualizing manner and take into account
the diverging perspectives of AI Act and non-discrimination law. European non-discrimination
law is tailored to individual instances of discrimination after an AI model has been deployed—an
inherently retrospective approach. In contrast to this, the AI Act prospectively demands fairness
interventions by implementing non-discrimination requirements at the stage of model design.
Guidance by democratically justified institutions on how to implement such requirements might
bridge the gap toward alleviating both the legal and the technical enforcement problems.
Enabling “bias detection and correction”? Legal requirements for the development of AI
systems are not only subject to the AI Act. Due to the tension between fairness and privacy
during the training and evaluation stage of AI, conflicts with data protection law may equally
arise. On the one hand, ignoring personal demographic data promotes the same risk as the
widely rejected idea of fairness through unawareness because legally protected attributes like
race and gender usually correlate to innocuous proxy variables [39, 40]. If protected attributes
are unavailable during model training and evaluation, these subtle correlations cannot be
accounted for, nor can technical fairness metrics be tested and optimized. On the other hand,
Art. 9 GDPR places particularly high demands on the lawful processing of personal data about
special categories. Therefore, the same sensitive data that is protected by data protection law
is also essential to effectively avoid discriminatory outputs. The AI Act seeks to mitigate this
tension by broadening the scope of lawful data processing. Art. 10(5) AI Act states that “[t]o
the extent that it is strictly necessary for the purposes of ensuring bias detection and correction in
relation to the high-risk AI systems [...], the providers of such systems may exceptionally process
special categories of personal data referred to in Art. 9(1) [GDPR]. ” This is accompanied by
Recital 44c, which adds that “[i]n order to protect the right of others from the discrimination that
might result from the bias in AI systems [...] the providers should, exceptionally, [...] be able to
process also special categories of personal data, as a matter of substantial public interest within
the meaning of Art. 9(2)(g) [GDPR]. ”Therefore, discrimination and fairness considerations can
provide a justification for data processing during the training phase of high-risk AI systems.
However, balancing the public and private interests regarding non-discrimination and privacy
will inevitably lead to intricate trade-offs.



### Claim 5/36

#### Claim Text
Although inspired by law, the technical algorithmic fairness debate primarily focuses on detecting, preventing and mitigation unfairness in algorithms[27], while legal non-discrimination law focuses on normative statements.

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 3):

4 
 
computer science, software engineering, and mathematics. These 
groups have developed numerous measures and methods  to mitigate 
bias and  improve fairness in algorithmic systems. However, the 
majority of these tools have been built in isolation from policy and civil 
societal contexts and lack serious engagement with philosophical, 
political, legal, and economic theories of  equality and  distributive 
justice.7 Reflecting this, most define fairness in simpl e terms, where 
fairness means red ucing gaps in performance or outcomes between 
demographic groups. Successfully achieving algorithmic fairness has 
come to mean satisfying one of these simple mathematical definitions, 
while preserving as much of the accuracy of the original system as 
possible. 
This oversimplification of equality through fairness measures could 
possibly be attributed to the relative youth of fairML . However, the 
practical impact of the approach adopted by the field to date is morally 
troubling. Many current fairness measures have been shown to suffer 
from both fairness and performance degradation, or “levelling down,” 
where fairness is achieved by making every group worse off , or by 
bringing better performing groups down to the level of worse performing 
groups.8 Levelling down is effectively fairness achieved by breaking the 
system, for example by making a classifier less accurate so it performs 
equally badly across all relevant groups. 
Levelling down is a symptom of the decision to measure fairness 
solely in terms of equality, or disparity between groups in performance 
and outcomes, while ignoring other relevant features of distributive 
justice such as absolute welfare or priority which are more difficult to 
quantify and directly measure in research and development 
environments. When fairness can only be measured in terms of 
distribution of performance or outcomes, corrective actions can likewise 
only target how these goods are distributed between groups. The field 
effectively only has egalitarian tools at its disposal which value equality 
of treatment and outcomes while ignoring other goods of distributive 
justice. Likewise, the prevalence of levelling down in fairML suggests 
that the field is, intentionally or otherwise, adopting a strict egalitarian 
approach to questions of distributive justice  in which the only 
(measurable) value is equality. We name these trends in fairML ‘strict 
egalitarianism by default’. 
Strict e galitarianism by default , at least in its most gratuitous 
forms, runs counter to both the stated objectives of fairness measures 
 
7 Reuben Binns, On the apparent conflict between individual and group fairness , in 
PROCEEDINGS OF THE 2020 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND 
TRANSPARENCY 514 (2020), https://doi.org/10.1145/3351095.3372864 (last visit ed Aug 
14, 2022). 
8 DOMINIK ZIETLOW ET AL ., Leveling Down in Computer Vision: Pareto Inefficiencies in 
Fair Deep Classifiers, (2022), http://arxiv.org/abs/2203.04913 (last visited Jun 10, 2022).



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 4):

5 
 
as well as the presumptive aim of the field: to improve outcomes for 
historically disadvantaged or marginalised groups .9 It co nceives of 
equality in simplistic comparative terms, ignoring absolutes of welfare 
and justice which are necessary to achieve substantive equality rather 
than mere formalistic equality, or equal treatment..10  
When fairness can only be achieved by making everyone worse off in 
material or  relational terms  through injuries of stigma, loss of 
solidarity, unequal concern,  and missed opportunities for substantive 
equality, something would appear to have gone wrong in translating the 
vague concept of ‘fairness’ i nto practice. Equality should aim to make 
people better off, not simply to reduce them to a common level of harm.11 
Simple mathematical definitions can be satisfied without regard for 
how parity is achieved in practice and the significant material and 
relational harms, and opportunity costs,  for the people affected.  The 
huge interest that exists algorithmic fairness pro vides an opportunity 
to substantively address longstanding inequalities in society. Enforcing 
 
9 FairML does not have universally agreed guiding principles, but prior work can 
provide some indication of its values and aims. In a 2019 paper critical of the state of 
the field, Keyes et al. defined the ‘Fair’ value of the Fairness, Accountability, and 
Transparency in Machine Learning (FAT -ML or FAccT -ML) research network as 
ensuring that algorithmic syste ms are “lacking biases which create unfair and 
discriminatory outcomes .” See: Os Keyes, Jevan Hutson & Meredith Durbin, A 
mulching proposal: Analysing and improving an algorithmic system for turning the 
elderly into high-nutrient slurry, in EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE 
ON HUMAN FACTORS IN COMPUTING SYSTEMS 1 (2019). More recently, Cooper et al. suggest 
that the field is motivated by the fact that “automated decision systems that do not 
account for systemic discrimination i n training data end up magnifying that 
discrimination; to avoid this, such systems need to be proactive about being fair .” See: 
A. Feder Cooper, Ellen Abrams & Na Na, Emergent Unfairness in Algorithmic Fairness-
Accuracy Trade-Off Research, in PROCEEDINGS OF THE 2021 AAAI/ACM CONFERENCE ON 
AI, ETHICS, AND SOCIETY 46, 51 (2021), https://dl.acm.org/doi/10.1145/3461702.3462519 
(last visited Jul 13, 2022). Early case studies in the field are similarly instructive, such 
as the famous COMPAS case in which a risk recidivism algorithm was alleged by 
journalists at ProPublica to be biased against Black defendants, routinely assigning 
them higher risk scores than comparable white defendants . See: Julia Angwin et al., 
Machine bias , 23 PROPUBLICA, MAY 2016 (2016).  These examples suggest work on 
algorithmic fairness is motivated at least in part by a desire to improve the situation of 
disadvantaged people that unjustifiably receive worse treatment or outcomes than their 
peers. How fairness, discrimination, and bias are conceptualised and measures, a nd 
likewise what is justified in differential treatment, opportunities, and results of course 
differs drastically across the field and use cases, but the underlying motivation to help 
people who are unjustifiably harmed by algorithmic systems seems clear a nd 
uncontroversial. 
10 Sandra Wachter, Brent Mittelstadt & Chris Russell, Bias preservation in machine 
learning: the legality of fairness metrics under EU non -discrimination law, 123 W. VA. 
L. REV. 735 (2021). 
11 LARRY S. TEMKIN, INEQUALITY (1993); Nils Holtug, Egalitarianism and the Levelling 
down Objection, 58 ANALYSIS 166 (1998); Brett Doran, Reconsidering the Levelling-down 
Objection against Egalitarianism , 13 UTILITAS 65 (2001); Derek Parfit, Equality or 
Priority?, in THE IDEAL OF EQUALITY 81 (Matthew Clayton & Andrew Williams eds., 
2002).



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 11):

12 
 
better off,”25  and yet this is precisely what current applications of group 
fairness achieve in fairML. 
3 HOW COMMON IS LEVELLING DOWN IN FAIRML? 
The usage of equality -based fairness measures in machine learning is 
not self -evidently troubling; rather, it is how they are enf orced in 
practice, and the resulting levelling down, which causes problems . 
When used solely for diagnostic purposes, egalitarian measures such as 
(conditional) demographic parity 26  or equal opportunity  27 provide a 
helpful warning that different groups are being treated differently, and 
that they are potentially being harmed in different ways by a n 
algorithmic decision-making system.  However, using them to 
determine which models should be deployed in real world use  cases 
raises serious ethical and legal concerns. Levelling down can occur as a 
direct result of the use of egalitarian measures in model selection. In 
many ways this problem is another example of Goodhart’s Law  that 
“when a measure becomes a target, it ceases to be a good measure.”28 
In this section we demonstrate how levelling down occurs for a range 
of fairness measures, focusing on two of the most common ly used  
metrics in the fairML literature: demographic parity and equal 
opportunity. To do so, we enforce these measures acro ss a range of 
algorithms using two of the most widely used fairness toolkits : 
FairLearn and IBM  AI Fairness 360  (IBM360).29 We show that t he 
existence of levelling down is not a limitation  or design flaw  of these 
toolkits or methodologies; rather, it is a natural consequence of strictly 
enforcing equality as part of model selection.30 As such, levelling down 
 
25 Chloé Bakalar et al., Fairness on the ground: Applying algorithm ic fairness 
approaches to production systems, ARXIV PREPRINT ARXIV:2103.06172, 5 (2021). 
26 Sandra Wachter, Brent Mittelstadt & Chris Russell, Why fairnes s cannot be 
automated: Bridging the gap between EU non-discrimination law and AI, 41 COMPUTER 
LAW & SECURITY REVIEW 105567 (2021); Faisal Kamiran & Toon Calders, Data 
preprocessing techniques for classification without discrimination , 33 KNOWLEDGE AND 
INFORMATION SYSTEMS 1 (2012). 
27 Moritz Har dt, Eric Price & Nati Srebro, Equality of opportunity in supervised 
learning, in ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 3315 (2016). 
28 Marilyn Strathern, ‘Improving ratings’: audit in the British University system , 5 
EUROPEAN REVIEW 305 (1997). 
29 Rachel KE Bellamy et al., AI Fairness 360: An extensible toolkit for detecting and 
mitigating algorithmic bias , 63 IBM JOURNAL OF RESEARCH AND DEVELOPMENT 4: 1 
(2019); Sarah Bird et al., Fairlearn: A toolkit for assessing and improving fairness in AI, 
MICROSOFT, TECH. REP. MSR-TR-2020-32 (2020). 
30 For example, Kim discusses a range of changes to the design of an ML algorithm that 
could decrease the “disparate impact” (a concept from US anti -discrimination law 
loosely corresponding to demographic parity) of the decisions made by a system . See: 
Pauline Kim, Race-Aware Algorithms: Fairness, Nondiscrimination and Affirmative 
Action, CALIFORNIA LAW REVIEW (2022), https://papers.ssrn.com/abstract=4018414 (last 
visited Jul 13, 2022).  If a data scientist systematically explored combinations of these 
changes, and then selec ted the model with disparate impact  below a predetermined



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 1):

2 
 
ABSTRACT 
In recent years fairness in machine learning (ML), artificial intelligence 
(AI), and algorithmic decision-making systems has emerged as a highly 
active area of research and development. To date, t he majority of 
measures and methods to mitigate bias and imp rove fairness in 
algorithmic systems have been built in isolation from policy and civil 
societal contexts and lack serious engagement with philosophical, 
political, legal, and economic theories of equality and distributive 
justice. Most define fairness in simple terms, where fairness means 
reducing gaps in performance or outcomes between demographic groups 
while preserving as much of the accuracy of the original system as 
possible. This oversimplification of equality through fairness measures 
is troubling. Many current fairness measures suffer from both fairness 
and performance degradation, or “levelling down,” where fairness is 
achieved by making every group worse off, or by bringing better 
performing groups down to the level of the worst off. Levelling down is 
a symptom of the decision to measure fairness solely in terms of 
equality, or disparity between groups in performance and outcomes, 
while ignoring other relevant features  of questions of distributive 
justice (e.g., welfare, priority) which are more difficult to quantify and 
measure. When fairness can only be measured in terms of distribution 
of performance or outcomes, corrective actions can likewise only target 
how these goods are distributed between groups: we refer to this trend 
as ‘strict egalitarianism by default’.  
Strict egalitarianism by default runs counter to both the stated 
objectives of fairness measures as well as the presumptive aim of the 
field: to improve outcomes for historically disadvantaged or 
marginalised groups. When fairness can only be achieved by m aking 
everyone worse off in material or relational terms through injuries of 
stigma, loss of solidarity, unequal concern, and missed opportunities for 
substantive equality, something would appear to have gone wrong in 
translating the vague concept of ‘fairness’ into practice. Levelling down 
should be rejected in fairML because it (1) unnecessarily and arbitrarily 
harms advantaged groups in cases where performance is intrinsically 
valuable, such as medical applications of AI; (2) demonstrates a lack of 
equal concern for affected groups, undermines social solidarity, 
contributes to stigmatisation; (3) fails to live up to the substantive aims 
of equality law and fairML, and squanders the opportunity afforded by 
interest in algorithmic fairness to substantively address longstanding 
social inequalities; and (4) fails to meet the aims of many viable theories 
of distributive justice including pluralist egalitarian approache s, 
prioritarianism, sufficientarianism, and others. 
This paper critically scrutinises these initial observations to 
determine how fairML can move beyond mere levelling down and strict



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 0):

The Unfairness of Fair Machine Learning: 
Levelling down and strict egalitarianism by default 
 
Brent Mittelstadt,1 Sandra Wachter,2 Chris Russell3 
 
CONTENTS 
Abstract ................................ ................................ ................................ .. 2 
1 Introduction ................................ ................................ ..................... 3 
2 Levelling down ................................ ................................ ................ 6 
2.1 Levelling down via group fairness ................................ ........... 7 
2.2 Example: levelling down in cancer screening .......................... 9 
3 How common is levelling down in fairML? ................................ ... 12 
3.1 Why does levelling down occur? ................................ ............. 13 
3.2 Levelling down in practice ................................ ..................... 15 
3.3 Levelling down in theory................................ ........................ 18 
4 Is levelling down justifiable? ................................ ......................... 21 
4.1 The value of equality ................................ ..............................  23 
4.1.1 Performance, utility, and harms ................................ ..... 25 
4.2 Substantive equality harms ................................ ................... 27 
4.2.1 Levelling down for social change ................................ .... 31 
5 Justifying levelling down in fairML ................................ ............. 34 
6 Levelling up by design with minimum rate constraints .............. 37 
6.1 Example 1: Demographic parity ................................ ............ 38 
6.2 Example 2: Difference in true negative rate ......................... 41 
7 Conclusion ................................ ................................ ..................... 46 
Appendix 1: Harms and remedies for group fairness measures ......... 49 
 
 
 
1 Corresponding author: brent.mittelstadt@oii.ox.ac.uk. Oxford Internet Institute, 
University of Oxford, 1 St. Giles, Oxford, OX1 3JS, United Kingdom. 
2 Oxford Internet Institute, University of Oxford, 1 St. Giles, Oxford, OX1 3JS, United 
Kingdom. 
3 Oxford Internet Institute, University of Oxford, 1 St. Giles, Oxford, OX1 3JS, United 
Kingdom. Chris Russell is also an employee of Amazon Web Services. He did not 
contribute to this research in his capacity as an Amazon employee. This work has been 
supported through research funding provided by the Wellcome Trust (grant nr 
223765/Z/21/Z), Sloan Foundation (grant nr G -2021-16779), the Department of Health 
and Social Care (via the AI Lab at NHSx), and Luminate Group to support the 
Trustworthiness Auditing for AI project and Governance of Emerging Technologies 
research programme at the Oxford Internet Institute, University of Oxford.



### Claim 6/36

#### Claim Text
In fact, the original drafts did not have any individual rights components [44].

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[44]_2407.10329.pdf (Page 44):

41 
injury, destruction of property, or loss of data.181 Pure economic loss and material harms were 
not covered in the original draft.182  
Recital 24 further states that 'Types of damage other than those provided for in this Directive, 
such as pure economic loss, privacy infringements or discrimination, should not by themselves 
trigger liability under this Directive'.183 The latest draft of this proposal does introduce the idea 
of immaterial harm ('pain and suffering') in Recital 23, but only if it is a side-effect of one of 
the harms covered in the Product Liability Directive.184  
The generative discrimination harms that we are envisioning are usually not associated with a 
material harm such as the destruction of property or personal injury. It is likely that this harm 
will exclusively occur as a stand-alone harm and therefore will not be covered by the updated 
Product Liability Directive.  
More promising is the AI Liability Directive, which is a complimentary framework for the AI 
Act designed to offer remedies against AI related harms. The current version also allows 
recourse against fundamental rights violations if recognised by Member State laws,185 and thus 
recourse against immaterial harms. The preamble even mentions 'violations of personal dignity 
(Articles 1 and 4 of the Charter), respect for private and family life (Article 7), the right to 
equality (Article 20) and non-discrimination (Article 21)'.186 
Yet, it is again questionable whether the AILD will offer a successful route to protecting people 
against generative discrimination. As we have shown, the harms we are envisioning often do 
not fit with the fundamental rights protections against discrimination and harassment. It is 
unclear how novel discrimination-related harms would fit under traditional fundamental rights 
protection.187  
But even if one would find an appropriate link to an existing fundamental right and Member 
State law does protect against violation, there are several hurdles. For instance, the evidential 
requirements create an almost insurmountable burden for claimants.188 And there are several 
limitations of the applicability of the EU Charter. The Charter only applies in EU law matters. 
Moreover, the Charter’s horizontal applicability is widely contested,189 which means that the 
                                                 
181 Art 4 (6) Product Liability Directive. 
182 Christiane Wendehorst, ‘Strict Liability for AI and Other Emerging Technologies’ (2020) 11 Journal of 
European Tort Law 150, 162; See also Hacker (n 138) 28 who criticizes this exclusion. 
183 Sandra Wachter, ‘Limitations and Loopholes in the E.U. AI Act and AI Liability Directives: What This Means 
for the European Union, the United States, and Beyond’ (2024) 26 Yale Journal of Law and Technology. 
184 European Parliament, P9_TA(2024)0132 - Liability for defective products - European Parliament legislative 
resolution of 12 March 2024 on the proposal for a directive of the European Parliament and of the Council on 
liability for defective products (COM(2022)0495 – C9-0322/2022 – 2022/0302(COD)) 2024; Wachter (n 137).  
185 Article 2 (9) Product Liability Directive. 
186 AILD Proposal, p. 10.  
187 It is not impossible that Member States already recognise these or equivalent harms. Should this be the case, 
then this route would open up. 
188 For details of the evidential requirements for both the PLD and AILD, see Hacker (n 138), see also, Sandra 
Wachter, ‘Limitations and Loopholes in the E.U. AI Act and AI Liability Directives: What This Means for the 
European Union, the United States, and Beyond’ (2024) 26 Yale Journal of Law and Technology. . 
189 See, e.g., CJEU, Case C-414/16 (Egenberger); C-68/17 (IR); Joint Cases C-569/16 and C-570/16 (Bauer und 
Willmeroth); Matteo Fornasier, 'The impact of EU fundamental rights on private relationships: direct or indirect 
effect?' (2015) 23 European Review of Private Law 29; Nuria Bermejo, 'Fundamental Rights and Horizontal Direct



Source: data\tc20_2501.12962v1\referenced_papers\[44]_2407.10329.pdf (Page 42):

39 
4. The AI Act  
The EU Artificial Intelligence Act167 and the forthcoming AI liability directives are unlikely to 
establish sufficient accountability mechanisms that would remedy the discrimination-related 
harms discussed in this chapter.168 
The AI Act is predominantly a product safety law. In fact, the original drafts did not have any 
individual rights components.169 Only the drafts of the European Parliament170 foresaw that 
individual complaint-based mechanisms were incorporated in the draft. While the most recent 
version of the AI Act includes a right to lodge a complaint with a Market Surveillance Authority 
and right to explanation of individual decision-making,171 those provisions are unlikely to 
alleviate most of the concerns we have noted above.  
While bias is recognised as an AI-related issue that needs addressing,172 it is unclear whether 
discrimination-related harms as exemplified in this paper can be seen as a violation of the AI 
Act against which complaints can be brought. For example, Article 10 AI Act does address the 
issue of bias in training data, but it is targeted at developers of predictive AI systems in high 
risk areas, not at developers of genAI. If genAI is developed for a high risk area, developers 
have to follow these rules. However, the viability of Article 10 in mitigating generative 
discrimination will crucially hinge on the interpretation of 'bias' in the AI Act. At the moment, 
the question remains open whether the harms envisioned in this chapter would fall under the AI 
Act’s concept of ‘bias.’ Perhaps regulators will interpret ‘bias’ in a technical (diversity of 
training data) and less social and ethical (demeaning and abusive contents) way.  
The right to explanation might also not be helpful for our purposes. The provision focuses on 
the right to have decisions explained that were rendered in high risk areas by predictive AI such 
as employment, criminal justice or education. The AI Act does not classify genAI as a high-
risk application and thus the right to explanation does not generally apply to genAI.173 Further, 
                                                 
167 Based on the final EP version of the AI Act available under 
https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138-FNL-COR01_EN.pdf.  
168 Sandra Wachter, ‘Limitations and Loopholes in the E.U. AI Act and AI Liability Directives: What This Means 
for the European Union, the United States, and Beyond’ (2024) 26 Yale Journal of Law and Technology. 
169 Michael Veale and Frederik Zuiderveen Borgesius, ‘Demystifying the Draft EU Artificial Intelligence Act — 
Analysing the Good, the Bad, and the Unclear Elements of the Proposed Approach’ (2021) 22 Computer Law 
Review International 97; Hacker, Engel and Mauer (n 12); Martin Ebers, ‘Standardizing AI-The Case of the 
European Commission’s Proposal for an Artificial Intelligence Act’ [2021] The Cambridge handbook of artificial 
intelligence: global perspectives on law and ethics; Johann Laux, Sandra Wachter and Brent Mittelstadt, ‘Three 
Pathways for Standardisation and Ethical Disclosure by Default under the European Union Artificial Intelligence 
Act’ (2024) 53 Computer Law & Security Review 105957. 
170 European Parliament, ‘Compromise Amendments on the Draft Report Proposal for a Regulation of the 
European Parliament and of the Council on Harmonised Rules on Artificial Intelligence (Artificial Intelligence 
Act) and Amending Certain Union Legislative Acts (COM(2021)0206 – C9 0146/2021 – 2021/0106(COD))’ 
(2023) KMB/DA/AS 
<https://www.europarl.europa.eu/meetdocs/2014_2019/plmrep/COMMITTEES/CJ40/DV/2023/05-
11/ConsolidatedCA_IMCOLIBE_AI_ACT_EN.pdf>.  
171 Article 85 and 86 AI Act. Art 99 (10) also offers recourse against decisions or omission to take action of the 
Market Surveillance Authority. 
172 See Article 10 AI Act.  
173 Natali Helberger and Nicholas Diakopoulos, ‘ChatGPT and the AI Act’ (2023) 12 Internet Policy Review 3, 6 
who argue that genAI should be its own high-risk category.



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 32):

33 
 
filed to block the extension, the county levelled down by suspending all 
marriage licenses until the v alidity of the state’s marriage law was 
resolved. As Brake explains: 
“…the leveling down occurred not as resistance to the equality 
challenge by gays and lesbians to the state's marriage laws, but 
in furtherance of it …the leveling down decision was not a 
defensive construction of social meaning designed to reinforce a 
status hierarchy disparaging gay and lesbian couples. Instead, 
it was a tactic designed to challenge that status hierarchy and 
hasten the extension of marriage to gay and lesbian couples by 
equalizing the status of their relationships. Importantly, the 
measure was understood by members of the gay and lesbian 
community as breaking down, rather than reinforcing, status 
differentials between gay and straight couples. Finally, it is 
significant that the leveling down of marriage was designed as a 
temporary measure, as part of a larger strategy to ultimately 
extend the privilege of marriage to same-sex couples.”134 
This step was viewed as a positive civil action in support of the 
LGBTQ+ community’s pu sh for marriage equality at a state level. A 
temporary solution of levelling down across all groups by suspending 
all marriage licenses was used to ensure equal concern rather than 
mere equal treatment.135 
In fairML, levelling down for civil action could similarly be used as 
means to force consideration of fairness in a production environment. 
Researchers and developers can use levelling down to delay or prevent 
deployment of models with unjustifiably poor performance for 
disadvantaged groups . By lowering pe rformance for an advantaged 
group, a developer could prevent deployers from having access to a ‘high 
accuracy’ or ‘unbiased’ model, and instead force levelling up by design 
until the model performs acceptably well across all groups (see: Section 
6). Temporarily reducing performance  can economically impact the 
deployer in cases where the advantaged group  is also the largest 
(potential) customer base, or the group with the most socioeconomic or 
 
134 Id. at 600. Brake offers another illustrative example drawn from college athletics. 
Male athletes are afforded certain privileges based on a problematic notion of 
masculinity which Brake argues would not be appropriate to extend to female athletes. 
“At the prestigiou s level of NCAA Division I -A football, for example, it is a common 
practice to have the football team housed in a hotel the night before home games. The 
rationale typically rests on the difficulty of otherwise controlling and disciplining the 
players to av oid the kind of behavior that would hurt their game performance. The 
practice is based on a model of a male athlete who embodies a ruggedly uncontrollable 
masculinity and it is applied uniquely to football players. Extending such a practice to 
female athle tes, at least on the same rationale, would make little sense. Instead, 
equality should require readjusting the athletic model upon which the practice is based 
to a gender -inclusive standard that holds all athletes responsible for their own 
behavior.” See: Id. at 597–8. 
135 Brake, supra note 103 at 561; RONALD DWORKIN, TAKING RIGHTS SERIOUSLY (2013).



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 31):

32 
 
social value by improving opportunities for relevant connected groups 
in a community.129  
Consider, for example, access to employment or education . Direct 
material weakening of the competitiveness of advantaged groups, for 
example barring men from university degree programmes, runs counter 
to the substantive aims of equality law. Levelling the playing field does 
not necessarily disrupt entitlements of advantaged groups; rather, it 
aims to remove pre -existing exclusionary standards or  arbitrary 
barriers to equal access or opportunities from the relevant decision -
making process or distribution principle (e.g., college admissions 
requiring a degree from male only schools) .130 Here, levelling down is 
justified because the equality injury experienced by advantaged groups 
is necessary to realise benefits for disadvantaged groups.131  
Ideally, corrective equality actions should not only improve equality 
in terms of results, access, capabilities, or opportunities, but also 
account for the social and institutional structures responsible for the 
inequality or entrenched advantages in question .132 Take for example 
gender equality in college athletics funding where high prestige male 
athletics programmes have historically been given much more funding 
than equivalent female programmes or lower prestige male 
programmes. Extending current funding level s to other programmes 
would be unsustainable for most colleges. In such cases “equality law 
should permit some leveling down to find a baseline that is not based 
on male privilege.”133 
Levelling the playing field is typically inappropriate in cases dealing 
with fundamental rights or goods, or non-rivalrous goods with inherent 
value such as recall or accuracy in cancer screening (see: Section 2.2). 
Extending the right to vote to women, for example, could not have been 
achieved by denying the right to men, and would have not been 
consistent with considerations of liberty. Similarly, reducing recall for 
male patients increases undiagnosed cases of cancer and is not strictly 
necessary to improve recall for female patients (see: Section 6). 
Considering the second, levelling down can also be used as a type of 
civil action to force re consideration of problematic social and 
institutional norms that contribute to unequal concern . A standout 
example of this justification came in the extension of marriage rights to 
same-sex couples in Benton County, Oregon. In response to a lawsuit 
 
129 Parfit, supra note 52; Parfit, supra note 11. 
130 Wachter, Mittelstadt, and Russell, supra note 10; Brake, supra note 103; Kim, supra 
note 30; Wachter, Mittelstadt, and Russell, supra note 26. 
131 Another example is land ownership, which has historically been limited to certain 
genders or royalty; extending access would require removing this privilege from 
historically advantaged groups. 
132 Fredman, supra note 82. 
133 Brake, supra note 103 at 594–5.



Source: data\tc20_2501.12962v1\referenced_papers\[44]_2407.10329.pdf (Page 38):

35 
1. Personality rights 
Personality rights are a broad category of rights, protected by lawmakers and courts in different 
ways around the world. They 'recognise a person as a physical and spiritual-moral being and 
guarantee his enjoyment of his own sense of existence'.142 In Europe, personality rights are 
predominantly governed by national legislation - not by the EU.  
A case that sheds light on developer liability is the Autocomplete judgment by the German 
Federal Court of Justice in Germany.143 The court held that a search engine provider could be 
liable for its autocomplete function's suggestions if the company failed to take reasonable 
measures to prevent defamatory or rights-violating suggestions.144 Specifically, the company is 
required to act once it becomes aware of such harmful outputs, indicating a duty to mitigate 
future occurrences.145 In the case, the wife of then German President, Bettina Wulff, sued 
Google because the most prominent autocomplete suggestion following her name was 
'prostitute'. This mirrored rumors in the tabloid press that, before meeting her husband who later 
went on to become Germany’s President, she had worked as a sex worker. 
This precedent is relevant for developers of large language models, which might be called 'large 
autocomplete engines.' Conversely, the autocomplete function in Google used a 'small language 
model' at the time.146 The ruling suggests that developers might face direct liability for the AI's 
outputs if they neglect to implement safeguards against the generation of harmful content. Such 
measures could include content moderation during the AI's training or responsive action upon 
notification of problematic outputs. 
Concerning the liability framework for deployers of AI technologies, we expect judges to align 
it closely, mutatis mutandis, with the obligations of developers, focusing on the management 
of infringing outputs. In fact, in the Autocomplete case, Google acted as both developer and 
deployer. In our view, both developer and deployer need to do what they reasonably can to 
prevent the violation of personality rights by genAI output. Deployers could potentially prevent 
liability by creating a robust compliance system. Essential elements of this system include the 
use of AI equipped with 'guardrails' for moderation; regular proactive monitoring of AI outputs; 
and a notice and takedown process to quickly address and prevent the recurrence of harmful 
content, such as by blocking specific prompts or outputs. Again, both developers and deployers 
may be jointly and severally liable. 
Overall, we expect both developers and deployers to be compelled by judges, under current 
law, to do what they reasonably can to prevent genAI output infringing personality rights.  
                                                 
142 Johann Neethling, ‘Personality rights: a comparative overview’ Comparative and International Law Journal of 
Southern Africa 38, no. 2 (2005): 210-245, p. 210. Internal citations omitted.  
143 Sandra Wachter, Brent Mittelstadt and Chris Russell, ‘Do Large Language Models Have a Legal Duty to Tell 
the Truth?’ [2024] Royal Society Open Science. 
144 Autocomplete| [2013] | BGH | VI ZR 269/12, | [2013] BGH, 14.05.2013 - VI ZR 269/12, Rn. 36. 
145 ibid. 
146 Rostyslav Demush, ‘From Crappy Autocomplete to ChatGPT: The Evolution of Language Models’, 
Hackernoon (March 14, 2023), https://hackernoon.com/from-crappy-autocomplete-to-chatgpt-the-evolution-of-
language-models.



### Claim 7/36

#### Claim Text
Although alignment techniques have developed [53], discrimination still persists in LMMs [40].

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[5]_2406.03198.pdf (Page 3):

1st HEAL Workshop at CHI Conference on Human Factors in Computing Systems, 2024 Anthis et al.
For subjects of LLMs, it may be difficult to define, detect, or
enforce appropriate fairness metrics. For example, early in the
literature on fairness in information access systems, it was noted
that when searching for images of “CEO, ” Google returned a set of
images largely depicting men and, lower in the recommendation
list, an image of the popular toy “CEO Barbie. ” However, as in
the FTU decision of which sensitive attributes a system should be
unaware of and in what way, the challenges of deciding subject
representation in system output are compounded with LLMs. These
decisions typically rely on utility estimates, and that tends to be
a significant challenge with more general-purpose systems based
on unstructured data. For example, there is an open question of
whether the target distribution should be equal representation of
men, women, and other genders or a distribution that is weighted
towards the gender distribution of CEOs in the consumer’s home
location [28, 37, 59]. To achieve LLM fairness, this sort of open
question would need to be resolved for each of the many tasks done
by the LLM.
For producers (i.e., the people or organizations whose content
is recommended), also known as providers, the fairness target is
often the equitable distribution of exposure, either in terms of
relevance-free metrics that do not consider the relevance of the
content to the user—only that there is an equitable distribution—or
relevance-based fairness metrics that target an equitable exposure
conditional on relevance. In either case, fairness to producers is
a matter of how the exposure of those providing content to the
system is allocated to consumers. In the use case of LLMs that
perform information retrieval and information management tasks,
this framework can at times transfer directly. For example, if some-
one searches for “coffee shops in San Francisco” in an LLM chat
or search interface—as is being incorporated into the ubiquitous
modern search engine, Google—producer fairness could be defined
in terms of equitable exposure to the different brick-and-mortar
coffee shops in San Francisco. Even if the LLM system does not
direct users to particular websites, many users will presumably end
up visiting the cafes, which provides utility—fairly or unfairly—to
the producers. However, if users are searching for information in
the LLM system, such as asking, “How are coffee beans roasted?”
then LLMs can entirely circumvent the producers and upend the
conventional notion of producer-side fairness. If the LLM system
extracts information from websites without directing users to the
original source content, then it may be that none of the producers
receive any exposure or other benefits in the first place. One way to
make sense of this would be to consider the LLM system itself—or
the entity that developed, owns, and manages it—as another type
of stakeholder, one that takes all utility from the producers and
renders the conventional producer-side fairness criteria obsolete.
4 LLMS ARE TOO FLEXIBLE TO BE
GENERALLY FAIR
Much of the excitement surrounding LLMs is based on their general-
purpose flexibility across wide ranges of inputs, tasks, outputs, and
contexts. To some extent, they resemble a human agent, including
the ability to chain together these tasks into complex sequences, and
these areas of flexibility make many conventional fairness metrics
intractable.
4.1 Group fairness does not generalize across
populations
Group fairness metrics require independence between model clas-
sification and sensitive attributes, often conditional on relevant
information such as the ground-truth labels that the model aims
to predict (e.g., job performance for a model that assists in hiring
decisions). Three common metrics are:
Definition 3. (Demographic parity). A model achieves demo-
graphic parity if its predictions are statistically independent of
sensitive attributes.
Definition 4. (Equalized odds). A model achieves equalized odds
if its predictions are statistically independent of sensitive attributes
conditional on the true labels being predicted.
Definition 5. (Calibration). A model achieves calibration if the
true labels being predicted are statistically independent of sensitive
attributes conditional on the model’s predictions.
In binary classification, these metrics are achieved when equal-
ities hold between ratios in the confusion matrix: equal ratios of
predicted outcomes (demographic parity), equal true positive rates
and false positive rates (equalized odds), and equal precision (cali-
bration). Recent work includes extensions of these notions, such as
prioritizing the worst-off group by minimizing the maximum group
error rate [19]. Conventionally, group fairness requires knowing
the sensitive attributes to enforce the equalities, though recent work
has considered approaches for when the sensitive attributes are un-
available [36, 42, 76]. There are many methods for enforcing group
fairness metrics, such as the preprocessing of datasets proposed
by Feldman et al. [27] to guarantee bounds on demographic parity
and the more recent method proposed by Johndrow and Lum [34]
that can be applied to a wider variety of datasets.
LLMs present a challenge for group fairness metrics in part
because LLMs tend to be deployed across a wide range of data
distributions. Lechner et al. [43] showed that it is impossible for a
non-trivial model to perform fairly across all different data distri-
butions, such as regions or demographic groups, to which it might
be applied. In current discussions of algorithmic fairness (e.g., re-
cidivism), fairness is typically targeted at a local jurisdiction, which
ensures that the model is performing fairly for that location’s par-
ticular demographic mix and holistic characteristics but typically
cannot also achieve fairness in substantially different locations. The
purpose and use of LLMs makes it infeasible to restrict them to this
sort of targeted population.
In general, it is not clear what an appropriate base population
would be on which to detect and achieve group fairness for an
LLM. For example, one could “bootstrap” a predictive model for
recidivism prediction from an LLM simply by instructing it to make
a prediction about an individual based on a fixed set of that indi-
vidual’s characteristics with in-context learning as Li and Zhang
[45] did in predicting the label of a text-converted tabular dataset.
However, the data on which that LLM had been trained does not
admit an identifiable base population because a corpus of text is not
4



Source: data\tc20_2501.12962v1\referenced_papers\[18]_2402.18502.pdf (Page 1):

2 Garima Chhikara, Anurag Sharma, Kripabandhu Ghosh, & Abhijnan Chakraborty
Fig. 1. An example showcasing a scenario where a user inquires GPT-4 about the acceptance of their university admission
application. Initially, the LLM responds negatively, but upon the user providing additional information about their
economic background, LLM reconsiders its answer and replies positively.
Fig. 2. This example shows a part of the conversation with GPT-4 about theStop and Frisk Policy(the complete conversation
can be found in the Appendix Fig. 3). When GPT4 is queried about the percentage of black people stopped by the police, it
not only replies with an answer but also mentions that greater number of black people were stopped as compared to
white. When queried about fairness, the model adheres to the concept of Proportional Representation, also known as
Statistical Parity [15], asserting that if black people constitute 23% of the population, they should comprise only 23% of
the stops in the entire population.
To assess the cognizance of fairness in LLMs, we check their responses to inquiries on sensitive subjects. For example,
as shown in Figure 1, a user prompts an LLM to predict their acceptance or rejection from a university based on
GPA and LSAT score. The initial response from the LLM is negative. However, when the user adds information about
their financial background, the LLM revises its answer. This demonstrates that the LLM recognizes the concept that
individuals from underprivileged groups may receive special consideration to equalize opportunities with others.
Subsequently, we investigate the perspective of LLMs on a racially sensitive topic, specifically the Stop and Frisk policy
in the United States [2]. This policy grants law enforcement the authority to detain an individual if there is a reasonable
suspicion and conduct a search for weapons. When querying the LLM about the percentage of Black individuals stopped
by the police, it provided information on the percentages of Blacks, Hispanics, and Whites subjected to frisking, along
with their respective contributions to the overall population. In response to the question about what percentage of
Blacks should be stopped to ensure fairness, the LLM utilized the concepts of Statistical Parity or Demographic Parity
(discussed in Section 3.3.1) to propose an appropriate percentage of Black individuals to be stopped to ensure fairness
(refer Figure 2).
Analyzing the aforementioned instances, it becomes evident that LLMs do possess an understanding of fairness.
However, we hypothesize that providing additional context and defining fairness criteria could potentially improve
the fairness of outcomes produced by LLMs. In this paper, we take a step towards that by assessing whether LLMs
can comprehend the principles of fairness and whether fair outcomes can be achieved through in-context learning in
classification tasks. To summarize, our contributions are listed as follows :
Manuscript submitted to ACM



Source: data\tc20_2501.12962v1\referenced_papers\[5]_2406.03198.pdf (Page 4):

The Impossibility of Fair LLMs 1st HEAL Workshop at CHI Conference on Human Factors in Computing Systems, 2024
a structured database comprising people and their characteristics.
An LLM may be trained in part on such databases, but the output
of the model for such predictions will also be based on the wide
scope of unstructured natural language or other modalities of data
on which the model is trained.
Generalization across populations is also a concern for fairness
frameworks other than group fairness because of the wide range of
data, use cases, and social contexts at play in LLMs [60]. Here, we
consider two examples: individual fairness [20] and counterfactual
fairness, which is the most common causal notion of fairness [40].
Definition 6. (Individual fairness). A model achieves individual
fairness if similar individuals are treated similarly. Formally, this re-
quires that the distribution of model output is Lipschitz continuous
with respect to the distribution of model input.
Definition 7. (Counterfactual fairness). A model achieves coun-
terfactual fairness if the model would produce the same output for
an individual if they had a different level of the sensitive attribute.
In terms of individual fairness, it is not clear what similarity
metrics could be reasonably applied across the multitude of contexts
or, if multiple metrics were applied, how these could be judiciously
selected and guaranteed in each possible context. In terms of causal
fairness, including counterfactual fairness, it is often difficult to
identify the causal structure of the data-generating process in even
a single NLP task, and it would be an immense challenge for a single
model to account for all of the many different contextual factors
that determine counterfactuals or other causally distinct outcomes
across the varying populations.
4.2 Sensitive attributes proliferate in a
general-use setting
The preceding section considered the challenges of imposing fair-
ness across different data distributions. When considering different
sensitive attributes, given the issues discussed in Section 3.1, it may
not be tractable to exclude sensitive attributes from the training
data, and each of the different distributions and different tasks can
require fairness metrics to be enforced for a different set of sensitive
attributes. This is a challenge for the group fairness metrics already
defined, but the issue is particularly salient for the popular ideal
of fair representations within a machine learning model or fair
representations produced by one model and used by another [72].
Definition 8. (Fair representation). A representation is fair if it
does not contain information that can identify the sensitive at-
tributes of the individuals being represented.
In the fair representations framework, a system first maps the
dataset of individuals being represented to a probability distribution
in a novel representation space, such that the system preserves as
much information as possible about the individual while removing
all information about the individual’s sensitive attribute. The most
well-known example of this approach is Bolukbasi et al. [11], which
rigorously documented gender bias in Google News word embed-
dings, namely an association between occupations and a gender
vector (e.g., ®he− ®she), such that computer programmer was coded as
highly male while homemaker was coded as highly female. Indeed,
this is where much of the NLP fairness literature has focused, doc-
umenting similar biases across different word embedding models
Sesari et al. [see 61, for a review].
Researchers have developed a number of debiasing approaches
focused on the sensitive attribute dimension, such as zeroing the
projection of each word vector (e.g., each occupation) onto the
dimension itself [11] or training the model to align the sensitive
attribute dimension with the last coordinate of the embedding space,
so that it can be easily removed or ignored [75]. However, Gonen
and Goldberg [33] show that such approaches “are mostly hiding
the bias rather than removing it” because, after removal, word pairs
tend to maintain their similarity, which still reflects associations
with sensitive attributes—what Bolukbasi et al. [11] call “indirect
bias. ”
Achieving fairness in one LLM context may be contingent on
the removal of information or alteration of the statistical relation-
ships between the context-specific sensitive attribute and other
features of the data. For example, one may wish to exclude gender
information from financial lending decisions, but gender informa-
tion may be necessary for other tasks, such as drafting or editing
an email about a real-world situation that has important gender
dynamics that the sender hopes to communicate to the receiver.
Moreover, variables highly correlated with gender, such as biologi-
cal sex and pregnancy status, may be essential criteria for medical
decision-making. In general, attempts at debiasing for one context
may remove or distort important information for another context.
The naive approach of debiasing the model with respect to the
union of all potential sensitive attributes—even if it were empirically
feasible—would likely be too heavy-handed, leaving the model with
little information to be useful for any task. To effectively create a
fair LLM for every task and context, one would need to act upon
the parameters of the model with surgical precision to alter the
relationship between variables only when the model is instantiated
for a specific task and context. This is infeasible with current LLM
methods, such as fine-tuning, and currently we do not have robust
techniques to debias even a single problematic relationship without
incidentally obfuscating it or problematizing other relationships.
This game of fairness whack-a-mole seems indefinitely intractable.
Likewise, even if we could reduce the union of all potential sensitive
attributes to a manageable level, such as identifying a small set of
the most important to adjust for in each task, that would still require
yet-infeasible fine-grained adjustments to avoid counterproductive
side effects.
4.3 Fairness does not compose, but
fairness-directed composition may help
Whether a model’s behavior on a given task is fair or desirable
largely depends on how the model’s output will be used. In modern
AI systems including LLMs, the output of one model is often used
as the input to another model, but this produces an additional
challenge because fairness does not compose: a fairness guarantee
for each of two models is not a fairness guarantee for a system
composed of the two models—a point made most explicitly by
Dwork and Ilvento [21].
5



Source: data\tc20_2501.12962v1\referenced_papers\[18]_2402.18502.pdf (Page 2):

Few-Shot Fairness: Unveiling LLM’s Potential for Fairness-Aware Classification 3
•To our knowledge, this is the first investigation into ensuring fairness through in-context learning for classification
task by specifying various fairness notions.
•We compare state-of-the-art LLMs, namely Llama-70b by Meta, GPT-4 by OpenAI, and Gemini by Google, using
different fairness criteria.
•We assess the accuracy-fairness tradeoff across zero-shot and few-shot setups.
•We publicly release the predictions of these LLMs for over 1000 test instances across four different setups, which
can spawn future research in this field1.
2 RELATED WORK
2.1 Fairness in LLMs
LLMs are experiencing explosive growth in their capabilities and applications. However, unfair LLM-based systems may
produce biased, discriminating, and stereotyping choices against underprivileged or vulnerable groups, which can have
negative societal effects and even be harmful [9, 29]. Hence, the concerns about discrimination and unfairness have
spurred research on the potential harmfulness of LLMs. Essentially, the bias in the training data gets baked into the
LLM, leading to biased outputs. This has led researchers to focus on mitigating these issues and ensuring fairer results
from LLMs. Methods like RLHF [39] and RLAIF [4] aim to steer LLMs away from reinforcing existing stereotypes and
producing offensive content. These techniques primarily involve training LLMs to generate fair and neutral outputs.
However, they may not be practical for the average user who does not intend to train or fine-tune an LLM. There is also
a growing focus on developing improved benchmarks to assess the unfairness in which datasets like CrowS-Pairs [36],
featuring sentence pairs with varying levels of stereotyping, RealToxicityPrompts[21], and RedTeamingData [40] for
prompt generation tasks with potentially harmful outcomes, and HELM [32], a comprehensive benchmark evaluating
bias and fairness in LLM. Although there has been considerable research on fairness in LLMs, there is currently an
absence of relevant studies specifically addressing fairness in classification tasks.
2.2 In-context Learning
Prior studies [11, 43] have shown that Large Language Models (LLMs) can perform tasks with limited or no training data
by learning from the context. They excel when provided with a suitable prompt. However, recent research [31, 34, 53]
has revealed that the effectiveness of LLMs is influenced by the prompt used. The selection of prompt format, training
examples, and even the order of those examples can significantly impact the performance of a Large Language Model
(LLM). This becomes even more crucial when we try to incorporate supplementary contextual information and fairness
criteria that could improve the fairness of outcomes produced by LLMs. [8] adopt a group fairness lens to assess bias and
fairness in LLMs and introduce a novel chain-of-thought method [49] designed to diminish biases in LLMs, particularly
from the perspective of group fairness. This impels us to include fairness notions within the context of prompts through
a fairness framework and conduct classification tasks to investigate the inherent understanding of fairness in LLMs.
3 EXPERIMENTAL SETUP
In this section, we outline the overall setup of the experiments, covering aspects such as the dataset, models utilized,
different fairness definitions, and fairness metrics.
1Available at https://anonymous.4open.science/r/FairLLM-8621.
Manuscript submitted to ACM



Source: data\tc20_2501.12962v1\referenced_papers\[18]_2402.18502.pdf (Page 18):

Few-Shot Fairness: Unveiling LLM’s Potential for Fairness-Aware Classification 19
In light of this, I politely decline to answer the question as given, as it does not align with my programming principles
to provide helpful and respectful responses while ensuring safety and ethical standards. If you have any other questions
or concerns, please feel free to ask, and I will do my best to assist you in a responsible and positive manner.
7.2.2 Response for Equalised Odds. Based on the given data, I would classify the income of this person as <=50K. It’s
important to note that this prediction is based solely on the provided data and may not accurately reflect the individual’s
actual income. Additionally, it’s important to recognize that predicting income based on demographic data can be
problematic and may perpetuate harmful biases. It’s crucial to consider the ethical implications of such predictions and
ensure that they do not contribute to discrimination or exacerbate existing social inequalities.
Manuscript submitted to ACM



#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 10):

1.2 The Scope of Alignment
with adversarial inputs, and cooperative training (§3.3.2) (Dafoe et al., 2020, 2021) which aims to address the
distribution gap between single-agent and multi-agent settings.23
• Assurance (§4) Once an AI system has undergone forward alignment, we still need to gain confidence about
its alignment before deploying it (Government of the United Kingdom, 2021; Anderljung et al., 2023). Such
is the role of assurance: assessing the alignment of trained AI systems. Methodologies of assurance include
safety evaluations (Perez et al., 2023; Shevlane et al., 2023) (§4.1) and more advanced methods such as inter-
pretability techniques (Olah et al., 2018) (§4.2) and red teaming (Perez et al., 2022) (§4.1.3). The scope of
assurance also encompasses the verification of system’s alignment with human values, including formal the-
ories focused on provable cooperativeness (Dafoe et al., 2021) and ethicality (Anderson and Anderson, 2011;
Tolmeijer et al., 2020), and also a wide range of empirical and experimental methods (§4.3). Assurance takes
place throughout the lifecycle of AI systems, including before, during, after training, and post-deployment, as
opposed to only after training (Shevlane et al., 2023; Koessler and Schuett, 2023).24
• Governance (§5) Assurance alone cannot provide full confidence about a system’s practical alignment since
it does not account for real-world complexities. This necessitates governance efforts of AI systems that focus
on their alignment and safety and cover the entire lifecycle of the systems (§5.1). We discuss the multi-
stakeholder approach of AI governance, including the governmental regulations (Anderljung et al., 2023), the
lab self-governance (Schuett et al., 2023), and the third-party practice, such as auditing (Shevlane et al., 2023;
Koessler and Schuett, 2023) (§5.2). We also highlight several open problems in AI governance, including
the pressing challenge of open-source governance (the governance of open-source models and the question
of whether to open-source highly capable models) (Seger et al., 2023), and the importance of international
coordination in AI governance (Ho et al., 2023) (§5.3). In addition to policy research, we also cover key
actions from both the public and the private sector.
Comparison with Inner/Outer Decomposition Our alignment cycle framework (see Figure 2) decomposes
alignment into four pillars: Learning from Feedback, Learning under Distribution Shift, Assurance and Gover-
nance organized into a circular process. The design principle for this framework is three-fold: Practical (making
sure pillars directly correspond to specific practices in specific stages in the system’s lifecycle), Concrete (pointing
to specific research directions as opposed to general themes), and Up-To-Date (accommodating and emphasizing
latest developments in the alignment field). Recently, the decomposition of alignment into outer alignment and
inner alignment has become popular in the alignment literature (Hubinger et al., 2019b). Outer alignment refers to
the wishes of designers in accordance with the actual task specification (e.g., goal & reward) used to build AI sys-
tems. And inner alignment is the consistency between task specification and the specification that the AI systems
behaviors reflect (Krakovna, 2022). However, many criticisms have also been made about this characterization,
including that it is ambiguous and is understood by different people to mean different things (Perry, 2020) and that
it creates unnecessary difficulties by carving out problems that are not necessary conditions for success (Turner,
2022). Some have tried to remove the ambiguity by pinning down the specific causes of inner/outer misalignment
and proposed, for example, goal misspecification and goal misgeneralization (Di Langosco et al., 2022; Krakovna,
2022). Learning from Feedback (approximately corresponding to goal misspecification and outer alignment) and
Learning under Distribution shift (approximately corresponding to goal misgeneralization and inner alignment) in
our framework tries to further improve upon the inner/outer decomposition by clarifying the exact approaches taken
to address the challenges and resolving the ambiguity. Assurance and Governance, on the other hand, expands the
scope to cover topics beyond outer and inner alignment.
Theoretical Research in Alignment The alignment research literature also contains a wealth of theoretical work
(Amodei et al., 2016; Everitt et al., 2018; Hendrycks et al., 2021b). These works often propose new directions and
provide a foundation for practical and empirical research to build upon. We give a brief overview of this body of
theoretical research below:
• Conceptual Frameworks. Some theoretical work proposes conceptual frameworks or characterizes subprob-
lems within alignment. Examples include instrumental convergence (wherein highly intelligent agents tend to
pursue a common set of sub-goals, such as self-preservation and power-seeking) (Omohundro, 2008; Bostrom,
2012), mesa-optimization (wherein the learned ML model performs optimization within itself during infer-
ence) (Hubinger et al., 2019c), and specific proposals for building aligned systems, such as approval-directed
23Cooperative Training aims to make AI systems more cooperative in multi-agent settings. This cooperativeness addresses
multi-agent failure modes where the AI system’s behavior appears benign and rational in isolation but becomes problematic
within social or multi-agent scenarios (Critch and Krueger, 2020); see collectively harmful behaviors in §1.1.2 for a more
detailed account.
24Furthermore, it’s noteworthy that many techniques here are also applicable in the training process, e.g., red teaming is a
key component of adversarial training (see §3.3.1), and interpretability can help with giving feedback (Burns et al., 2022).
11



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 9):

1.2 The Scope of Alignment
ties,i.e., Assurance (§4). It also covers the creation and enforcement of rules that ensure the safe development and
deployment of AI systems, i.e., Governance (§5). At the same time, backward alignment updates the alignment
requirements based on the evaluation and monitoring of the systems, both pre-deployment and post-deployment.
These updated requirements then inform the next round of alignment training.
The two phases, forward and backward alignment, thus form a cycle where each phase produces or updates
the input of the next phase (see Figure 2). This cycle, what we call the alignment cycle , is repeated to produce
increasingly aligned AI systems. We see alignment as a dynamic process in which all standards and practices
should be continually assessed and updated. Notably, Backward Alignment (including the Assurance of alignment
in AI systems and the Governance of AI systems) efforts occur throughout the entire alignment cycle, as opposed to
only after training. As argued in Shevlane et al. (2023); Koessler and Schuett (2023), alignment and risk evaluations
should occur in every stage of the system’s lifecycle, including before, during, after training, and post-deployment.
Similarly, regulatory measures for every phase of the system’s lifecycle have been proposed and discussed (Schuett
et al., 2023; Anderljung et al., 2023).
The survey is structured around four core pillars: Learning from Feedback (§2) and Learning under Distribution
Shift (§3), which constitute the components of Forward Alignment; and Assurance (§4) and Governance (§5)
which form the elements of Backward Alignment. The subsequent paragraphs provide a concise introduction to
each pillar, clarifying how they synergistically contribute to a comprehensive framework for AI alignment.
• Learning from Feedback (§2) Learning from feedback concerns the question of during alignment training,
how do we provide and use feedback to behaviors of the trained AI system? It takes an input-behavior pair as
given and only concerns how to provide and use feedback on this pair.22 In the context of LLMs, a typical solu-
tion is reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Bai et al., 2022a), where
human evaluators provide feedback by comparing alternative answers from the chat model, and the feedback
is used via Reinforcement Learning (RL) against a trained reward model. Despite its popularity, RLHF faces
many challenges (Pandey et al., 2022; Casper et al., 2023b; Tien et al., 2022), overcoming which has been a
primary objective of alignment research (Bowman et al., 2022), and is one primary focus of the section. An
outstanding challenge here is scalable oversight (§2.4), i.e., providing high-quality feedback on super-human
capable AI systems that operate in complex situations beyond the grasp of human evaluators, where the behav-
iors of AI systems may not be easily comprehended and evaluated by humans (Bowman et al., 2022). Another
challenge is the problem of providing feedback on ethicality, which is approached by the direction of machine
ethics (Anderson and Anderson, 2011; Tolmeijer et al., 2020). On the ethics front, misalignment could also
stem from neglecting critical dimensions of variance in values, such as underrepresenting certain demographic
groups in feedback data (Santurkar et al., 2023). There have also been work combining feedback mechanisms
with social choice methods to produce a more rational and equitable aggregation of preferences (Collective
Intelligence Project, 2023) (see §1.2.3).
• Learning under Distribution Shift (§3) In contrast to learning from feedback, which holds input fixed, this
pillar focuses specifically on the cases where the distribution of input changes, i.e., where distribution shift
occurs (Krueger et al., 2020; Thulasidasan et al., 2021; Hendrycks et al., 2021a). More specifically, it focuses
on the preservation of alignment properties (i.e., adherence to human intentions and values) under distribu-
tion shift, as opposed to that of model capabilities. In other words, it asks how we can ensure an AI system
well-aligned on the training distribution will also be well-aligned when deployed in the real world. One
challenge related to distribution shift is goal misgeneralization, where, under the training distribution, the in-
tended objective for the AI system ( e.g., following human’s real intentions) is indistinguishable from other
unaligned objectives (e.g., gaining human approval regardless of means). The system learns the latter, which
leads to unaligned behaviors in deployment distribution (Di Langosco et al., 2022). Another related challenge
is auto-induced distribution shift (ADS), where an AI system changes its input distribution to maximize re-
ward (Krueger et al., 2020; Perdomo et al., 2020). An example would be a recommender system shaping
user preferences (Kalimeris et al., 2021; Adomavicius et al., 2022). Both goal misgeneralization and ADS are
closely linked to deceptive behaviors (Park et al., 2023b) and manipulative behaviors (Shevlane et al., 2023) in
AI systems, potentially serving as their causes. Interventions that address distribution shift include algorith-
mic interventions (§3.2), which changes the training process to improve reliability under other distributions,
and data distribution interventions (§3.3) which expands the training distribution to reduce the discrepancy
between training and deployment distributions. The former includes methods like Risk Extrapolation (REx)
(Krueger et al., 2021) and Connectivity-based Fine-tuning (CBFT) (Lubana et al., 2023). The latter includes
adversarial training (§3.3.1) (Song et al., 2018b; Bai et al., 2021) which augments training input distribution
22Here, behavior is broadly defined also to include the system’s internal reasoning, which can be examined via interpretability
tools (see §4.2).
10



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 56):

alignment. Bakker et al. (2022) founds that consensus statements built silently from a subgroup will lead to dissent
among excluded members, highlighting the consensus’s sensitivity to individual input. For international coopera-
tion, establishing a shared data center is necessary but also requires first determining which civilizations to include
and if their values can align.
5.4.2 Alignment Techniques for AI Governance
It’s crucial to ensure the reliability and trustworthiness of AI systems as they are adopted in various real-world
decision-making scenarios. On one hand, language models still exhibit illusions during use, and on the other hand,
the reliability of systems comprises two parts: the system’s reliability under individual testing environments and
its reliability in human interactions. Another issue is constructing systems with decision-making processes that
are observable and explainable to users. From a social perspective, the proliferation of AI systems across fields
also poses potential risks. This risk arises from a gap between AI developers, who often focus on advancing
technology without considering its downstream applications, and AI adopters, who may transfer AI systems to
their fields without adequate safety considerations or verification of replicable success 41. Therefore, it is crucial
to build a framework that enables AI adopters to accurately assess model utility and appropriateness, and allows
AI regulators to quickly identify risks and issue safety alerts in AI systems.
Alignment techniques can facilitate synchronized, independent, and rigorous evaluations of AI systems. AI
developers should prioritize appropriate bias handling during the training process, acknowledging the importance
of socio-economic, cultural, and other differences. Furthermore, we should aim to develop robust and fair evalu-
ation methods and datasets for auditing AI systems. Zhu et al. (2023) proposes the first dynamic testing protocol
for large language models, utilizing Directed Acyclic Graphs (DAGs) to dynamically generate test data, thereby
reducing the risk of test data memorization and contamination. Additionally, new robust security protocol evalua-
tion methods have been introduced: Shlegeris and Greenblatt (2023) suggests constructing adversarial policies to
manage dangerously powerful and deceptive models, while Greenblatt et al. (2023) proposes (un)trusted editing to
supervise models based on their harm and deceitfulness levels. Future efforts should also prevent AI systems from
reward-hacking evaluation system exploits and aim to provide AI regulators with an explainable, independent, and
centralized evaluation system.
AI adopters and the industry should allocate financial and computational resources to thoroughly evaluate use
cases and share case studies showcasing both successes and failures. Equally important is training for adopters on
downstream applications.
6 Conclusion
In this survey, we have provided a broadly-scoped introduction to AI alignment, which aims to build AI systems
that behave in line with human intentions and values. We specify the objectives of alignment as Robustness,
Interpretability, Controllability, and Ethicality (RICE), and characterize the scope of alignment methods as com-
prising of forward alignment (making AI systems aligned via alignment training) andbackward alignment(gaining
evidence of the systems’ alignment and govern them appropriately to avoid exacerbating misalignment risks). Cur-
rently, the two notable areas of research within forward alignment are learning from feedback and learning under
distribution shift, while backward alignment is comprised of assurance and governance.
One thing that sets alignment apart from many other fields is its diversity (Hendrycks, 2022) – it is a tight
assembly of multiple research directions and methods, tied together by a shared goal, as opposed to a shared
methodology. This diversity brings benefits. It fosters innovation by having the different directions compete and
clash against each other, leading to a cross-pollination of ideas. It also allows different research directions to
complement each other and together serve the goal of alignment; this is reflected in thealignment cycle (see Figure
2), where the four pillars are integrated into a self-improving loop that continually improves the alignment of AI
systems. Meanwhile, this diversity of research directions raises the barrier to entry into this field, which mandates
the compilation of well-organized survey materials that serve both the newcomers and the experienced. In this
survey, we attempt to address this need by providing a comprehensive and up-to-date overview of alignment.
We attempt to account for the full diversity within the field by adopting a broad and inclusive characterization
of alignment. Our survey of alignment gives a spotlight to almost all major research agendas in this field, as
well as to real-world practices on the assurance and governance front. We recognize that boundaries of alignment
are often vague and subject to debate. Therefore, when proposing the RICE principles, we put forth our broad
characterization of alignment as an explicit choice. In the meantime, we recognize that such a survey needs to
be a long-term endeavor that is continually reviewed and updated. Both the problems and methods of alignment
closely follow the development of machine learning. This fast-paced development means that new materials and
frameworks can become outdated after merely a few years. This fact is one reason why we write the survey to
reflect the latest developments, and also mandates continual maintenance and updates.
41https://www.scai.gov.sg/scai-question-11/
57



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 57):

6.1 Key Challenges in the Alignment Cycle
6.1 Key Challenges in the Alignment Cycle
Specifically, we outline key challenges and potential future directions based on the alignment cycle, namely for-
ward and backward alignment.
Learning Human Intent from Rich Modalities (forward alignment) Underspecificity of true human intent,i.e.,
the non-uniqueness of inferred human intent from binary feedback data, is a key challenge in scalable oversight.
Consider an AI system tasked with providing proof or refutation to a mathematical hypothesis, under a human eval-
uator who might be tricked by sophisticated false proofs. Our goal is to construct a training process that induces
the AI system to output sound proofs as opposed to false proofs that seem convincing. This system may mislead
evaluators with plausible but false proofs due to the system’s optimization for human approval, as it attempts to
satisfy the superficial criteria of convincing proofs rather than focusing on accuracy. The fundamental problem
stems from the reliance on binary feedback which categorizes responses simply as preferred or dispreferred, thus
limiting the amount of information on true human preferences that’s available to the learning algorithm, potentially
leading to the preference of credible-seeming deceptive proofs over genuinely sound arguments.
To enhance the model’s alignment with true human intent, researchers have proposed incorporating richer human
input beyond binary choices, such as detailed text feedback (Chen et al., 2024a) and real-time interations (Hadfield-
Menell et al., 2016). It allows the model to differentiate between proofs that are merely convincing and those that
are truly sound, using nuanced human evaluations and a vast database of human-written texts. The broader input
base helps in constructing a more accurate model of human preferences, reducing the risk of favoring misleading
proofs while respecting the complexity of human intent and reasoning. Looking forward, even richer modalities
like embodied societal interactions could represent an enticing next step.
It is worth noting that current LLMs are already trained on Internet-scale human text (and for multimodal models,
also visual/audio content). Why, then, don’t reward modeling algorithms already possess the ability to accurately
pin down human intent? The explanation is that pretraining data does not feed into the reward modeling process
in a way that biases the process towards true human intent, even though the reward model is finetuned from the
pretrained model. For instance, neural circuits representing human intent can potentially be rewired during RLHF
to perform manipulative behaviors. From another perspective, pretraining on text such as humans do not want to
be tricked into believing things does not induce the reward model to interpret later human feedback signals in this
light, partly due to the lack of out-of-context learning capabilities in current LLMs (Berglund et al., 2023). Solving
these problems may enable reward modeling algorithms to learn human intent from massive pretraining data, a big
step towards our goal.
We summarize three key questions for the learning of human intent from rich modalities. They serve as key
dimensions for characterizing an alignment method from the intent modality lens, and almost all existing alignment
methods can be categorized by their answers to these three questions.
1. Learning algorithm. As previously mentioned, we need to learn human intent from rich modalities in a way
that guides the reward model’s subsequent interpretation of human input.
2. Priors and inductive biases . Human-like priors/inductive bias is needed for the reward modeling process
to select the correct hypothesis of human intent, though this requirement is greatly loosened as the allowed
modalities of human input expand.
3. Learner alignment. We utilize the intent learner to align AI systems, possibly by using it as a reward model.
However, this would not be possible if the intent learner, which is itself an AI system with potentially strong
capabilities, is misaligned. This necessitates measures to avoid or contain the misalignment of the intent
learner.
Trustworthy Tools for Assurance (backward alignment) A major concern in AI alignment is deceptive align-
ment, where AI systems pursue aligned goals under most circumstances but may pursue other goals when op-
portunities arise. Recent studies have revealed that general alignment techniques ( e.g., SFT, RLHF, Adversarial
Training) fail to eradicate certain deceptive and backdoor behaviors, possibly leading to a misleading sense of
safety (Hubinger et al., 2024). With AI systems gaining power and access to more resources, hidden intentions
that pose existential risks could have unimaginable consequences. How can we detect and eliminate deceptive and
backdoor behaviors?
Reliable tools are still lacking to address this issue. On one hand, mechanistic interpretability tools encounter
additional challenges due to the polysemanticity of neurons and scalability issues. On the other hand, there is a
limited understanding of how jailbreaking functions and the susceptibility of language models to poisoning and
backdoors (Anwar et al., 2024).
Additionally, given the potential misuse of AI systems in cyber attacks, biological warfare, and misinformation,
it is crucial to develop reliable mechanisms to trace the origins of LLM outputs. While AI systems are becoming
58



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 8):

1.2 The Scope of Alignment
10.29新
RegulatesComplies
Trained System
Alignment Requirements
Learning under Distribution Shift (§3)Distribution
InputLearnerOutput
Learning from Feedback (§2)Feedback/ModelingAdvisors
AIHuman
Assurance (§4)
Governance (§5)Third PartiesAcademia, NGO/NPOIndustry / Labs
Government
Audits
Assists
Self-Regulates
RICERobustness
◾Interpretability
◾Controllability
◾Ethicality
Forward Alignment(Alignment Training)
produces
informs
Backward Alignment(Alignment Refinement)
is subject to
updates
Human & AI International Cooperation
presentthroughoutlifecycle
ASSURANCE (§4)
Safety EvaluationsInterpretabilityHuman ValuesVerification
Figure 2: The Alignment Cycle. (1) Forward Alignment (alignment training) produces trained systems based
on alignment requirements; (2) Backward Alignment (alignment refinement) ensures the practical alignment of
trained systems and revises alignment requirements; (3) The cycle is repeated until reaching a sufficient level of
alignment. Notably, although Backward Alignment has the end goal of ensuring the practical alignment oftrained
systems, it is carried out all throughout the system’s lifecycle in service of this goal, including before, during, after
training, and also after deployment (Shevlane et al., 2023; Koessler and Schuett, 2023; Schuett et al., 2023).
systems, escape containment, and even violate ethics). They may hide unwanted behaviors, fool human
supervisors, and seek more resources to become more powerful. Moreover, double edge components (+)
may intensify the danger and lead to more hazardous outcomes, even resulting in existential risks (Bostrom,
2013).
1.2 The Scope of Alignment
In this section, we focus on illustrating the scope of AI alignment: we constructed the alignment process as an
alignment cycle and decomposed it into Forward Alignment Processand Backward Alignment Process20 (§1.2.1).
Specifically, we discuss the role of human values in alignment (§1.2.3) and further analyze AI safety problems
beyond alignment (§1.2.3).
1.2.1 The Alignment Cycle: A Framework of Alignment
We decompose alignment into Forward Alignment (alignment training) (§2, §3) and Backward Alignment
(alignment refinement) (§4, §5). Forward Alignment aims to produce trained systems that follow alignment re-
quirements.21 We decompose this task into Learning from Feedback (§2) and Learning under Distribution Shift
(§3). Backward Alignment aims to ensure the practical alignment of the trained systems by performing evaluations
in both simplistic and realistic environments and setting up regulatory guardrails to handle real-world complexi-
20From this point and throughout the survey, for convenience, we refer to “Forward Alignment” and “Backward Alignment”.
21Here, alignment requirements refer to an operationalized specification of the alignment properties that are desired of the
AI systems, including, for example, which concrete forms of robustness/interpretability/controllability/ethicality we require, in
what specific settings we require them, and how they could be measured.
9



### Claim 8/36

#### Claim Text
From a technical perspective, benchmarks to judge the risk associated with LLMs have been proposed [38]; however, automated bias testing is far from trivial, and we currently lack adequate ML techniques.

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 14):

examined models with the technical requirement ofTraceability. Namely, as mentioned above, no current
models employ a watermarking scheme, and as such they do not comply with the regulatory requirements of
Article 50 (2) (see §3.1.4). RegardingDiversity, Non-discrimination, and Fairness, in Table 2 we see that
models perform especially poorly on benchmarks concerning fairness, highlighting this as one of the most
challenging aspects of LLM development and a priority for future research.
Focusing on Capabilities is Insufficient Further, looking at Table 2, we see that on the technical
requirement ofCapabilities, Performance, and Limitationsthe models are ordered as we would expect, i.e.,
larger and more recent models perform better. However, focusing too much on these benchmarks in LLM
development, as most often done currently, does not lead to models that are compliant with other regulatory
requirements in the EU AI Act. Prime examples of this are Qwen1.5-72B and Mixtral-8x7B, both of which
perform well on capabilities (0.7 and 0.68, respectively), but are notably failing to satisfy some of the other
technical requirements, e.g., Qwen obtains the lowest and the second-lowest scores onInterpretability and
Disclosure of AI Presence, and Mixtral is the third-worst performing model onCyberattack Resilience. With
the adoption of the EU AI Act, model providers will have to move on from primarily prioritizing capabilities,
and incorporate techniques in their model development pipeline that also lead to improvements on other
aspects that are equally important for compliance.
Current Benchmarks are Limited Our results also highlight that certain technical requirements cannot
be currently benchmarked reliably. As a prime example, as discussed in §3, there isno suitable technical tool
or benchmark to evaluateExplainability. In some other cases, even though benchmarks are present, they
are unfit for a reliable evaluation of the underlying technical requirement. For instance, our Copyright (No
Copyright Infringement) benchmark only checks whether popular copyrighted books have been used to train
a model. This approach has two major limitations: (i) it does not account for potential copyright violations
involving materials other than these specific books, and (ii) it relies on quantifying model memorization, which
is notoriously difficult (Nasr et al., 2023). Similarly, ourUser Privacy Protectionbenchmark only attempts
to determine whether the model has memorized specific personal identifiable information (PII). Without
access to the model’s actual training data, both benchmarks must make unrealistic and static assumptions,
blindly checking for specific books or PII of some individuals. This often results in almost perfect benchmark
scores across all models, rendering the benchmarks largely ineffective. While the current benchmarks for
the technical requirement ofInterpretability provide a useful signal, they are limited to calibration metrics,
lacking other aspects of this broad requirement. We argue that along with rethinking the metrics to be used
in model development (as discussed above), the community should also focus on extending and improving the
palette of available benchmarks along all technical axes of the EU AI Act.
Small Models Are Not RobustIn Table 2, we see that smaller models tend to have significantly lower
scores on the technical requirement ofRobustness and Predictability. This is especially evident for older
models, i.e., Llama 2-7B, Llama 2-13B, and Mistral-7B, which are the three models that score the lowest on
this technical requirement. While recent work has demonstrated that smaller models can sometimes achieve
surprisingly high performance on capability benchmarks (AI@Meta, 2024; Microsoft, 2024), our results suggest
that more work is needed to bridge the gap between smaller and larger models in terms ofother essential
aspects such as robustness, where more advanced models fare remarkably well in our evaluation.
Strong Alignment Against Toxic ContentIn Table 2, we observe that all models obtain high scores
for the technical requirementHarmful Content and Toxicity. The benchmarks corresponding to this technical
requirement consist of completion prefixes and prompts aimed at elucidating toxic or harmful responses.
Strong results here imply that such behavior was not successfully triggered, signifying the importance and
the effectiveness of the alignment phase that is currently included in the LLM chatbot development.
5 Discussion
Our work on theCOMPL-AI framework, including the construction of the benchmarking suite and subsequent
evaluation of state-of-the-art LLMs in the context of the EU AI Act, led us to draw the followingfour key
takeaways, that we hope can positively guide LLM development and evaluation in the coming years:
15



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 2):

suite. Finally, we use our benchmarking suite to evaluate12 prominent LLMs, providing insight into various
shortcomings of both current LLMs and benchmarks.
Evaluation Takeaways We observe that smaller models generally score poorly on technical robustness
and safety, and that almost all examined models struggle with diversity, non-discrimination and fairness.
A likely reason for this is the disproportional focus on model capabilities, at the expense of other relevant
concerns. We expect that EU AI Act will influence providers to shift their focus accordingly, leading to a
more balanced development of LLMs. Our observations regarding benchmarks are similar. While benchmarks
that test model capabilities are comprehensive, others (e.g., privacy evaluations) are often simplistic and
brittle, leading to inconclusive results. This is another area where we expect EU AI Act to have a positive
impact, shifting the focus towards neglected aspects of model evaluation.
Impact of COMPL-AI Beyond shedding light on currently insufficient practices in model development
and benchmarking w.r.t. the regulatory requirements of the EU AI Act, our work can form a meaningful
reference point for the official concretization and operationalization of the Act. We believe the methodology
and results of our technical interpretation in the context of LLMs to be highly relevant to the ongoing effort
to develop a Code of Practice for providers of general-purpose AI models (GPAI CoP), as stipulated by
the Act. Moreover, our Act-oriented benchmarking suite can serve as a proof of concept, for the first time
demonstrating the possibility of hands-on, tractable technical guidelines for model developers and deployers,
and highlighting areas where more work is needed to bridge the gap between regulation and practice. Besides
such fundamental work on improving model training procedures and benchmarks highlighted by our work, and
the expansion of our benchmarking suite in response to the latest developments in the field, an important next
step includes broadening of the scope to cover other AI systems beyond LLMs, highlighting the challenges
specific to other model types and applications.
2 Background and Related Work
In this section we cover the background on LLMs and the EU AI Act, and discuss existing tools for assessing
Act compliance and the current space of LLM evaluation benchmarks.
Large Language Models The transformer architecture (Vaswani et al., 2017) has enabled major progress
on the well-studied problem of language modeling, allowing for efficient training and strong scaling with
model and data size. Traininglarge language models (LLMs), i.e., transformers with billions of parameters,
has quickly brought significant improvements to most tasks of interest, most notably text generation (Devlin
et al., 2019; Radford et al., 2018; 2019; Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021; Lieber
et al., 2021; Hoffmann et al., 2022), and these models quickly reached deployment in user-facing applications
such as GitHub Copilot (GitHub). Following the release of ChatGPT (OpenAI, 2022), an LLM chatbot,
LLM-powered applications have seen a rapid increase in adoption, with hundreds of millions of users (Milmo &
agency, 2023), and new LLMs being developed both as open source (xAI; Touvron et al., 2023a;b; Jiang et al.,
2023; 2024; Mesnard et al., 2024; Li et al., 2023; Biderman et al., 2023) and proprietary models (OpenAI,
2023; Anil et al., 2023; Anthropic, 2023; 2024; Mistral). These LLMs are pretrained for next-token prediction
(completion) on large text corpora, modeling the next-token probabilityp(xn|x0,..., x n−1). This equips the
model with common sense knowledge, language understanding, coding ability, and many other capabilities.
Modern LLMs are finetuned to follow instructions in a chat format (Wei et al., 2022), and often go through
alignment (Christiano et al., 2017; Ouyang et al., 2022), where the model is further tuned to human preference.
Opportunities and Risks of LLMsBommasani et al. (2021) detail some unique opportunities LLMs
can bring to individuals, democratizing access to specialized knowledge, as well as to the economy as a whole,
e.g., in the healthcare, legal, and educational sectors. For example, LLMs may provide medical information
to patients, serve as a preliminary legal consultant, or complement teachers as digital tutors. Analysts
estimate that generative AI could add up to $4.4 trillion to the global economy, with significant impacts
across all sectors (Chui et al., 2023). However, these models also carry risks, from accelerating malicious
activities to having potentially discriminatory impacts. Notably, Weidinger et al. (2021) lay out the risks
associated with LLMs along six pillars: (i) discrimination, exclusion and toxicity, e.g., perpetuating harmful
3



Source: data\tc20_2501.12962v1\referenced_papers\[5]_2406.03198.pdf (Page 3):

1st HEAL Workshop at CHI Conference on Human Factors in Computing Systems, 2024 Anthis et al.
For subjects of LLMs, it may be difficult to define, detect, or
enforce appropriate fairness metrics. For example, early in the
literature on fairness in information access systems, it was noted
that when searching for images of “CEO, ” Google returned a set of
images largely depicting men and, lower in the recommendation
list, an image of the popular toy “CEO Barbie. ” However, as in
the FTU decision of which sensitive attributes a system should be
unaware of and in what way, the challenges of deciding subject
representation in system output are compounded with LLMs. These
decisions typically rely on utility estimates, and that tends to be
a significant challenge with more general-purpose systems based
on unstructured data. For example, there is an open question of
whether the target distribution should be equal representation of
men, women, and other genders or a distribution that is weighted
towards the gender distribution of CEOs in the consumer’s home
location [28, 37, 59]. To achieve LLM fairness, this sort of open
question would need to be resolved for each of the many tasks done
by the LLM.
For producers (i.e., the people or organizations whose content
is recommended), also known as providers, the fairness target is
often the equitable distribution of exposure, either in terms of
relevance-free metrics that do not consider the relevance of the
content to the user—only that there is an equitable distribution—or
relevance-based fairness metrics that target an equitable exposure
conditional on relevance. In either case, fairness to producers is
a matter of how the exposure of those providing content to the
system is allocated to consumers. In the use case of LLMs that
perform information retrieval and information management tasks,
this framework can at times transfer directly. For example, if some-
one searches for “coffee shops in San Francisco” in an LLM chat
or search interface—as is being incorporated into the ubiquitous
modern search engine, Google—producer fairness could be defined
in terms of equitable exposure to the different brick-and-mortar
coffee shops in San Francisco. Even if the LLM system does not
direct users to particular websites, many users will presumably end
up visiting the cafes, which provides utility—fairly or unfairly—to
the producers. However, if users are searching for information in
the LLM system, such as asking, “How are coffee beans roasted?”
then LLMs can entirely circumvent the producers and upend the
conventional notion of producer-side fairness. If the LLM system
extracts information from websites without directing users to the
original source content, then it may be that none of the producers
receive any exposure or other benefits in the first place. One way to
make sense of this would be to consider the LLM system itself—or
the entity that developed, owns, and manages it—as another type
of stakeholder, one that takes all utility from the producers and
renders the conventional producer-side fairness criteria obsolete.
4 LLMS ARE TOO FLEXIBLE TO BE
GENERALLY FAIR
Much of the excitement surrounding LLMs is based on their general-
purpose flexibility across wide ranges of inputs, tasks, outputs, and
contexts. To some extent, they resemble a human agent, including
the ability to chain together these tasks into complex sequences, and
these areas of flexibility make many conventional fairness metrics
intractable.
4.1 Group fairness does not generalize across
populations
Group fairness metrics require independence between model clas-
sification and sensitive attributes, often conditional on relevant
information such as the ground-truth labels that the model aims
to predict (e.g., job performance for a model that assists in hiring
decisions). Three common metrics are:
Definition 3. (Demographic parity). A model achieves demo-
graphic parity if its predictions are statistically independent of
sensitive attributes.
Definition 4. (Equalized odds). A model achieves equalized odds
if its predictions are statistically independent of sensitive attributes
conditional on the true labels being predicted.
Definition 5. (Calibration). A model achieves calibration if the
true labels being predicted are statistically independent of sensitive
attributes conditional on the model’s predictions.
In binary classification, these metrics are achieved when equal-
ities hold between ratios in the confusion matrix: equal ratios of
predicted outcomes (demographic parity), equal true positive rates
and false positive rates (equalized odds), and equal precision (cali-
bration). Recent work includes extensions of these notions, such as
prioritizing the worst-off group by minimizing the maximum group
error rate [19]. Conventionally, group fairness requires knowing
the sensitive attributes to enforce the equalities, though recent work
has considered approaches for when the sensitive attributes are un-
available [36, 42, 76]. There are many methods for enforcing group
fairness metrics, such as the preprocessing of datasets proposed
by Feldman et al. [27] to guarantee bounds on demographic parity
and the more recent method proposed by Johndrow and Lum [34]
that can be applied to a wider variety of datasets.
LLMs present a challenge for group fairness metrics in part
because LLMs tend to be deployed across a wide range of data
distributions. Lechner et al. [43] showed that it is impossible for a
non-trivial model to perform fairly across all different data distri-
butions, such as regions or demographic groups, to which it might
be applied. In current discussions of algorithmic fairness (e.g., re-
cidivism), fairness is typically targeted at a local jurisdiction, which
ensures that the model is performing fairly for that location’s par-
ticular demographic mix and holistic characteristics but typically
cannot also achieve fairness in substantially different locations. The
purpose and use of LLMs makes it infeasible to restrict them to this
sort of targeted population.
In general, it is not clear what an appropriate base population
would be on which to detect and achieve group fairness for an
LLM. For example, one could “bootstrap” a predictive model for
recidivism prediction from an LLM simply by instructing it to make
a prediction about an individual based on a fixed set of that indi-
vidual’s characteristics with in-context learning as Li and Zhang
[45] did in predicting the label of a text-converted tabular dataset.
However, the data on which that LLM had been trained does not
admit an identifiable base population because a corpus of text is not
4



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 1):

Technical Interpretation
Map Requirements to Benchmarks
Collect, Add, and 
Update Benchmarks
“achieve an appropriate 
level of accuracy, 
robustness, and 
cybersecurity”
Article 15 (1)
“put in place a
policy to comply with 
Union copyright law”
Article 53 (1c)
...
...
...
...
...
...
 Robust MMLU
 Copyrighted Material
     Memorization
0.75
2/3
20/27 Benchmarks Completed
N/A
 Monotonicity BoolQ Contrast
 Monotonicity, BoolQ Contrast
× Copyrighted Material Memorization
My Model Report
 IMDB Contrast
Robustness and 
Predictability
No Copyright 
Infringement
Robustness and 
Predictability
No Copyright Infringement
  S el f- Check Consistency
Regulatory  
Requirements
[EU  AI  Act]
Technical  
Requirements
Benchmarking  
Suite  (LLMs)
AI  
Act
0.81
Figure 1: Overview ofCOMPL-AI. First, we provide a technical interpretation of the EU AI Act for LLMs,
extracting clear technical requirements. Second, we connect these technical requirements to state-of-the-art
benchmarks, and collect them in a benchmarking suite. Finally, we use our benchmarking suite to evaluate
current LLMs, identifying critical shortcomings in both the models and the current benchmarks from the
perspective of the EU AI Act.
Lack of Technical InterpretationWhile the EU AI Act represents a major step towards responsible AI
development, its ethical principles and corresponding regulatory requirements are often broad and ambiguous.
To be applied in practice, the Act requires the development of concrete standards and recommendations, to be
followed by the stakeholders. However, to be able to kick off such efforts, we still lack a clear translation of the
Act intotechnical requirements, which could be further concretized asbenchmarks, enabling model providers
to assess their AI systems in a measurable way in the context of the Act. This gap is even more apparent
given the surge in work on model evaluations, both in terms of specialized benchmarks (Hendrycks et al., 2021;
Zellers et al., 2019; Parrish et al., 2022; Chen et al., 2021) and large-scale benchmarking suites (Beeching
et al., 2023; Liang et al., 2022; Srivastava et al., 2022)—crucially, all these are disconnected from regulation
and as such cannot be easily interpreted in the context of the EU AI Act.
This Work: COMPL-AI In this work, we aim to bridge that gap by providing the first comprehensive
technical interpretation of the Act in the context of LLMs, and utilizing it to propose the first regulation-
oriented LLM benchmarking suite†. An overview of the process behindCOMPL-AI is shown in Fig. 1. First,
we recognize that LLMs and systems built around them often fall into several categories defined by the Act
(i.e., GPAI models/systems, GPAI models/systems with systemic risk, high-risk AI systems) depending on
their type and application. As we will discuss in §3.1, we consider the classification of a given model/system
into the mentioned categories orthogonal to our work, and focus on being comprehensive w.r.t.all technical
requirements that LLMs may fall under. At the same time, we ensure that each extracted requirement
remains traceable to the corresponding category, enabling users of theCOMPL-AI to apply our technical
interpretation and benchmarking suite selectively to their use case. As such, we first extract the legal
requirements the Act poses for the union of the above categories, and translate them to a comprehensive set
of technical requirements, relying on the terminology and the focus of state-of-the-art technical AI research to
guide our interpretation. Second, we survey the relevant work on model evaluations, carefully collecting and
implementing those that suitably reflect our technical requirements as part of our Act-centered benchmarking
†https://compl-ai.org/
2



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 4):

LLM Evaluation Benchmarks In contrast to task-specific models (e.g., image classifiers), LLMs are
versatile and may have non-foreseeable use cases, making their evaluation a challenging task (Srivastava et al.,
2022; Liang et al., 2022). Lately, significant effort is being invested in this direction, with benchmarks being
developed for various aspects of LLMs such as general knowledge (Hendrycks et al., 2021), truthfulness (Lin
et al., 2022a), coding ability (Chen et al., 2021), robustness (Clark et al., 2019; Gardner et al., 2020), security
and reliability (Toyer et al., 2023; Mu et al., 2023), and bias (Dhamala et al., 2021; Parrish et al., 2022).
To unify this landscape and achieve standardization, several projects attempt to group benchmarks into
larger benchmarking suites (Srivastava et al., 2022; Liang et al., 2022; Beeching et al., 2023) While beneficial
for LLM research in a specific area, these works are not interpretable in a regulatory context, and do not
provide exhaustive coverage across all relevant aspects. To overcome these limitations, a regulation-oriented
benchmarking suite would need to (i) translate the regulatory requirements into a set of technical benchmarks,
(ii) provide a regulatory interpretation of the benchmark results, and (iii) collect all elements of this pipeline
in a unified framework, accessible for regulators, researchers, and other stakeholders.
3 COMPL-AI: Technical Interpretation of the EU AI Act and a Benchmarking Suite
In this section, we first outline the challenges of building a benchmarking suite for regulation packages such
as the EU AI Act. Then, as the first component of theCOMPL-AI framework, we present our technical
interpretation of the Act, translating its legal requirements into a set of concrete benchmarks for LLMs.
Key Challenges of Regulation-Oriented BenchmarkingThe main challenge in creating a bench-
marking suite tailored to a regulation package is the interpretation of the regulatory requirements and their
distillation into measurable technical requirements and benchmarks. This task is often difficult, as the
text is formulated according to the practices of legal language, focusing on formulating directive high-level
requirements instead of precise technical specifications, while purposefully leaving room for judges to exercise
discretion. As such, the technical reader may be faced with (i) a lack of clarity which concrete metrics have to
be considered, and (ii) potential requirements that lack current technical evaluation standards or techniques.
An illustrative example can be taken from the fourth ethical principle of the EU AI Act:“AI systems
shall be developed and used in a way that allows appropriate traceability and explainability, ...”. While
the requirement posed by this statement (“explainability”) is in accordance with legal practices, it is hard
to unambiguously interpret it in practice due to the lack of suitable technical tools. The extent to which
this requirement should be satisfied is also not specified precisely, leaving much room for interpretation
(“appropriate”), making it difficult to draw any conclusion based on potential technical benchmarks. Both of
these aspects demonstrate the difficulties practitioners face when assessing the compliance of their systems.
While in our benchmarking suite we aim to provide a comprehensive coverage over any relevant and measurable
technical aspect of the examined models, due to the aforementioned challenges, this is not possible for all
regulatory requirements. In such cases, we aim to raise awareness about the difficulty and ambiguity of the
given regulatory requirement from a technical perspective, and identify regulatory requirements that imply
technical specifications that are not assessable with current state-of-the-art tools. With this, we hope to
motivate both regulators and the machine learning community to invest efforts in bridging these gaps.
3.1 A Comprehensive Benchmarking Suite for the EU AI Act
Next, we clarify our scope and discuss the methodology used to devise a technical interpretation of the EU AI
Act. Then, we proceed to describe the corresponding technical requirements, along with an accompanying set
of carefully chosen benchmarks, navigating the challenges outlined above. For each implemented benchmark,
we provide further technical details in App. B. Definitions of specific terms, as used by the EU AI Act and by
this text, are included in App. C, with the full glossary of the Act to be found in Article 3.
Scope The EU AI Act distinguishes between different AI artifacts, primarily establishing strict requirements
for high-risk AI systems (HR) and general-purpose AI (GP) models, which may be used as part of a
corresponding GP AI system (Recital 100), where GP models with systemic risk (i.e., those with particularly
high capabilities, as defined in Article 51) are subject to additional requirements. On top of that, some
5



### Claim 9/36

#### Claim Text
Most importantly, the term bias is neither defined in the AI Act nor is there a common understanding of what bias means (see also Section 2.1) [84].

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[84]_2410.14501.pdf (Page 8):

9 
 
particular subject or thing .’ (iii) ‘an unfair personal opinion that influences your judgment .’41 In 
scholarship, the term bias does not have a fixed definition, although many taxonomies exist.42 
Developers may come across many types of biases during the development and use of an AI 
system. I name a few practical scenarios: 
- An error made in a calculation, which leads to different results than intended. As an 
example: the results of the German election in Saxony had to be corrected because of a 
calculation error. After a correction of the error, the party AfD lost the elections in 
Saxony.43 
- The results of the AI system are biased. For example, men may be overrepresented  
compared to women in the datasets used to train the AI system.44 
- The AI system itself seems neutral, but the userbase is not. For example, a dating app may 
have less users with specific ethnicities, which results  in a (possibly reinforced) 
underrepresentation of those users in the app.45 
It follows from the introduction of Article 10(2) that detecting and correcting biases in datasets is 
not only possible for the provider, it is mandatory.46 In Article 10(5) AI Act, t he EU legislator 
explicitly refers to  the bias detection and correction in accordance with  Article 10(2)(f)  and (g) . 
Consequently, providers may only process sensitive data as strictly necessary to examine their  
training, validation and testing data sets  for biases. The provider may  only correct biases as far as is 
mandatory for Article 10(2) AI Act. Currently, because there is no exact guidance on how to correct 
biases, the obligation could be broad or narrow: further guidance seems necessary.47 The provider 
may not collect the data to look for biases outside of the datasets used for development, such as 
biases encoded in the AI model or biases that originate from applying an AI system across a 
different context than the one for which it was originally designed.48 
In sum, the meaning of the term ‘bias’ in the AI Act seems open to interpretation, but the exception 
focuses on biases in the datasets used to develop the AI system, a further limitation. 
4.4 STRICTLY NECESSARY 
The AI Act exception states that providers may only process personal data for as long as ‘strictly 
necessary’. The CJEU uses the phrase ‘strict necessity’ in case law about the right to the protection 
of personal data. The case law by the CJEU is relevant for interpreting strict necessity in Article 10 
 
41 Cambridge dictionary, definition of ‘ bias’ , https://dictionary.cambridge.org/dictionary/english/bias.  (accessed 03-
09-2025). 
42 See, for example, S Barocas and A Selbst, ‘Big Data’s Disparate Impact’ (2016) 104 Calif Law Rev 
<https://www.jstor.org/stable/24758720>. For a taxonomy, see Tilburg Institute for Law, Technology, and Society, 
Handbook on Non -Discriminating Algorithms. Summary Research Report  (2021) 
<https://www.tilburguniversity.edu/about/schools/law/departments/tilt/research/handbook>.  
4343 https://www.nu.nl/buitenland/6326748/radicaal-rechtse-afd-verliest-na-rekenfout-zetel-in-duitse-deelstaat-
saksen.html.  
44 For example, in 2018, Amazon s crapped an AI system developed based on non -representative training data. See 
Reuters, ‘Amazon Scraps Secret AI Recruiting Tool That Showed Bias against Women’ (11 October 2018) 
<https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G> accessed 7 April 
2022. 
45 We discuss this example more in Section 5 (Discussion). 
46 See also, for a paper focusing on the obligation itself, Hacker (n 10). 
47 See also Section 5 (Discussion). 
48 Biases may also come from the context of use of the AI system. For a discussion possible sources of bias in AI 
systems, see S Barocas and AD Selbst, ‘Big Data’s Disparate Impact’ (2016) 104 California Law Review 671.



Source: data\tc20_2501.12962v1\referenced_papers\[84]_2410.14501.pdf (Page 7):

8 
 
for steps 5 and 6. Where the provider develops the AI system and places the AI system on the market 
or into service, the deployer uses the AI system.37  
Is the separation between provider and deployer always strict? For high -risk AI systems  in 
particular, the EU legislator intended a flexible definition of deployer. Article 25(1) AI Act states that 
if any deployer or third party makes a substantial modification or uses the AI system for another purpose , 
the AI Act considers them a provider. In such a case, the original provider becomes a third -party 
supplier that must cooperate with the new provider.38 The AI Act does not define what a substantial 
modification or modified purpose is. Section 5 of the paper discusses what the fluent separation of 
provider and deployer means for the de-biasing obligation.39 
4.3 BIAS DETECTION AND CORRECTION AND THE DE-BIASING OBLIGATION 
According to Article 10(5) AI Act , providers may only use the exception for ‘ensuring bias 
detection and correction […] in accordance with the second paragraph, point f and fa’. I cite the 
relevant part of Article 10 below.40 
Article 10 
2.   Training, validation and testing data sets shall be subject to data governance and management 
practices appropriate for the intended purpose of the high -risk AI system. Those practices shall 
concern in particular: […] 
(f) examination in view of possible biases that are likely to affect the health and safety of persons, 
have a negative impact on fundamental rights or lead to discrimination prohibited under Union 
law, especially where data outputs influence inputs for future operations; 
(g) appropriate measures to detect, prevent and mitigate possible biases identified according to 
point (f); […] 
The AI Act exception speaks of ‘detecting and correcting’ biases. Comparing these terms to points 
(f) and (g), detecting biases means an ‘examination in view of possible biases that are likely to affect 
the health and safety of persons, have a negative impact on fundamental rights or lead to 
discrimination prohibited under Union law, especially where data outputs influence inputs for 
future operations’. Correcting biases means  taking ‘appropriate measures to detect, prevent and 
mitigate possible biases identified according to point (f)’. Both definitions seem fairly open-ended. 
The focus in on possible biases in points (f) and (g). Neither the AI Act nor its recitals define what 
biases are. In dictionar ies, I can find many possible definitions for the term bias, such as for 
example: (i) ‘the action of supporting or opposing a particular person or thing in an unfair way, 
because of allowing personal opinions to influence your judgment .’ (ii) ‘the fact of preferring a 
 
37 Article 3(4) AI Act. 
38 Article 25 AI Act and Recital 60 AI Act.  The Article does not provide for possible power imbalances between 
provider and supplier. See also Roberta Montinaro, ‘Responsible Data Sharing for AI: A Test Bench for EU Data Law’ 
(2024) 1 EJLPT s 4 <https://universitypress.unisob.na.it/ojs/index.php/ejplt/article/view/1979>.  
39 See further Section 5 (Discussion). 
40 Lightly edited by authors.



Source: data\tc20_2501.12962v1\referenced_papers\[26]_2403.20089.pdf (Page 3):

4. Practical challenges for compliance
Defining bias: what are “appropriate” fairness metrics? The discussed implications of
the AI Act raise two important questions on how to put non-discrimination and fairness into
practice. First, the concept of technical fairness metrics begs the question which one(s) may
be “appropriate for the intended purpose of the AI system” (Art. 10(2)f AI Act). Technical
fairness definitions have already been examined for their compatibility with moral norms [41]
and non-discrimination regimes [17, 18, 19, 42, 21] alike. However, legal concepts relying on
flexible ex-post standards and human intuition are in tension with the mathematical need for
precision and ex-ante standardization [21, 42]. Also, the interdisciplinary discourse needs to
acknowledge that fairness and non-discrimination might present inherently different concepts
targeted at different social contexts. Prior works have suggested that a single standard of fairness
can be achieved by “translating” legal non-discrimination requirements from the employment
context into technical fairness metrics [17, 19]. However, the heterogeneity of social contexts
(e.g., employment versus criminal sentencing) demands a corresponding flexibility in fairness
requirements [43, 44]. Instead of aiming for a one-size-fits-all solution, we therefore recommend
applying the landscape of available technical fairness metrics to different legal conceptions of
discrimination depending on the societal context.
Detecting and correcting bias: when are biases “likely to lead to discrimination”? The
second challenge is defining when “possible biases that are likely to [...] lead to discrimination”.
Technical fairness metrics such as statistical parity or equalized odds offer an actionable approach
to measure and mitigate “bias” [45, 30, 21, 46]. However, it remains unanswered what kind of
evidence would signal sufficient efforts of bias detection and correction. Setting aside the debate
on metric selection, let us assume algorithmic hiring requires male and female applicants to
receive equal hiring rates (demographic parity). Statistical hypothesis testing provides a suitable
method to verify compliance with this requirement, in this case a simple z-test. To test the
hypothesis of compliance with demographic parity, we are interested in the test’s error rates,
i.e., falsely detecting a violation (type 1 error) or the likelihood of failing to detect a violation
(type 2 error). Notably, a larger disparity in hiring probabilities between groups and a larger
sample size decreases type 2 error. Unfortunately, the z-test is also sensitive to the acceptance
rate—particularly for small sample sizes. For example, for 1000 male and 1000 female applicants,
type 2 error decreases by 0.8% - points if only 700 instead of 900 applicants are accepted—despite
identical group disparities (see Appendix A). This effect is especially strong for imbalanced
datasets. For 1800 male and 200 female applicants, type 2 error even decreases by 6% - points if
only 780 instead of 980 applicants are accepted—again, despite identical group disparities (see
Appendix A). Our example highlights the need for guidance in selecting appropriate tests and
specifying standards for the error rates of tests utilized in bias detection.
5. Conclusion
In this short paper, we outlined how the AI Act could promote the convergence of legal non-
discrimination discourse and technical algorithmic fairness discourse. While we sketch its poten-



Source: data\tc20_2501.12962v1\referenced_papers\[84]_2410.14501.pdf (Page 17):

18 
 
for biases that could create discriminatory effects . The AI Act ’s exception and corresponding 
obligation ensure that at least biases present in datasets are regulated in the European Union.



Source: data\tc20_2501.12962v1\referenced_papers\[84]_2410.14501.pdf (Page 14):

15 
 
4.7 SUMMARY AND CONCLUSION 
Overall, Article 10(5) AI Act includes many useful safeguards , which can help  to protect data 
subjects. However, the definitions for terms such as bias and provider limit the scope of the exception 
significantly. In the next section, I discuss whether this limitation is a problem for AI debiasing in 
practice. 
5 DISCUSSION: IS THE EXCEPTION EFFECTIVE? 
The de-biasing exception and obligation target discrimination.83 Biases present in the data used to 
develop AI systems can (ultimately) have discriminatory effects on society, after the AI system has 
been trained and the system takes a decision. Removing discriminatory effects of AI systems seems 
possible.84 Non-discrimination law could play an important role here, even if the link between 
measuring bias and weighing whether a case constitutes discrimination is currently often not clear. 
The exception is not perfect, however. Three points in the exception require further discussion: (i) 
the limitation of the exception to the provider of AI systems, (ii) the limitation to biases in the 
training, validation and testing datasets, (iii) the AI act does not describe which biases developers 
must remove. 
(i) The exception only applies for the provider of the AI system, and the developer may not share 
the sensitive data with deployers, which has consequences for the effectiveness of the exception . 
Non-discrimination, is a highly contextual field. 85 A developer who wishes to prevent 
discriminatory effects resulting from biases in data must have information about the use of the AI 
system in practice : how is the system deployed? Article 10 AI Act does not explicitly require 
deployers to work together with providers to take away discriminatory effects the systems have. 
I distinguish between three main cases of AI development and deployment: AI as a service, in-house 
development of AI, and further development of an existing AI system.86 First, the provider and the 
deployer of the AI system may be entirely separate parties : the deployer rents the AI, or in other 
words, uses AI as a service  (AIaaS). Multiple deployers could apply an AI system  created by one 
provider in different regions of the world , and contexts. For example, an employment system 
developed by a provider in Greece could be rented to an employer  (the deployer) in The 
Netherlands. In such cases, without further context about the setting and target group of the  
system, the developer cannot take away potential discriminatory effects effectively.87 The developer 
and deployers must then collaborate . It seems questionable whether a single AI system can 
facilitate, for example, hiring in multiple countries : the developer may have to fine -tune the AI 
 
83  
84 Biases may ultimately also come from the context in which the AI system is deployed, rather than the data used to 
train the AI. However, removing biases from datasets can be a good start. 
85 Michael Veale, Max Van Kleek and R euben Binns, ‘Fairness and Accountability Design Needs for Algorithmic 
Support in High -Stakes Public Sector Decision -Making’, Proceedings of the 2018 CHI Conference on Human Factors in 
Computing Systems (ACM 2018) 10 <https://dl.acm.org/doi/10.1145/3173574.3174014> accessed 11 October 2024.  
86 In the EU, some sectors seem to have more in -house development of AI than others.  See Charles Hoffreumon, 
Chris Forman and Nicolas Van Zeebroeck, ‘Make or Buy Your Artificial Intelligence? Complementarities in 
Technology Sourcing’ (2024) 33 Journal of Economics & Management Strategy 452 
<https://onlinelibrary.wiley.com/doi/10.1111/jems.12586>. 
87 Data scientists have made this point before. See Kornel Lewicki and others, ‘Out of Context: Investigating the Bias 
and Fairness Concerns of “Artificial Intelligence as a Service”’, Proceedings of the 2023 CHI Conference on Human Factors in 
Computing Systems (ACM 2023) s 6 <https://dl.acm.org/doi/10.1145/3544548.3581463> accessed 9 October 2024.



### Claim 10/36

#### Claim Text
It seems that the regulators had a more technical definition of bias in mind, focusing on the diversity of training data in different dimensions compared to social, ethical, or structural biases [44].

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[27]_2404.18736.pdf (Page 7):

Undesirable model behavior often stems from flawed data that contains misrepresentations
of the world (e.g., erroneous, mislabeled, or imbalanced data) or accurate representations that
are societally undesirable (e.g., historical inequalities). The goal of data fairness is to identify
and address such flaws in the data set used to train an AI model.
Prior studies have pointed out data issues as drivers of unfairness [89, 65, 67] or highlighted
data as a starting point for unfairness mitigation [ 90, 91, 92]. Mehrabi et al. [65] provide a
comprehensive list of data biases that may introduce unfairness early on in AI development.
Awareness and understanding of these biases are crucial to derive strategies to handle them.
Stage of the Lifecycle. Data fairness relates to data collection (e.g., in the form of labeling
errors, sampling bias, imbalances, etc.) and data analysis, which aims to identify and potentially
mitigate unfair data characteristics early on. However, data fairness also reaches into feature
selection which is usually part of an iterative loop together with model construction. For
example, features might be dropped if they are not justifiably task-relevant. Importantly, in the
context of sensitive attributes, developers should always be aware of the flaws of the idea of
“fairness through unawareness” [16, 62].
The Role of XAI. As data is a main source of unfairness [ 65], XAI is suited to identify
potential disparities, imbalances, or abnormalities manifested in the available data early on.
Descriptive statistics are a natural first step to explore pre-existing disparities [67]. Anik and
Bunt [88] and Mitchell et al. [49] provide two examples of how to present simple descriptions
and visualizations about the collection, feature distributions, and patterns of a dataset. XAI
techniques have further been claimed to reveal instances and features in the data that have
undesirable effects on the model output [ 93, 94, 66] which, however, are often subject to
questionable causal and normative assumptions [95, 96]. These approaches indicate a strong
connection between data fairness and formal fairness.
3.3. Formal Fairness
“To counteract biases, it is, therefore, crucial to enable their detection. Explainability approaches
may aid in this regard by providing means to track down factors that may have contributed
to unfair and unethical decision-making processes and either to eliminate such factors, to
mitigate them, or at least to be aware of them. ”[40, p. 6]
The most common fairness desideratum is concerned with formal model properties. Byformal
fairness, we refer to the vast array of formal criteria that have been proposed as mathematical
and statistical measures of fairness [13, 15].
Consistent with Verma and Rubin[14], this includes all fairness definitions based on statistical
measures (e.g., demographic parity), similarity measures (e.g., fairness through awareness),
or based on causal reasoning (e.g., counterfactual fairness). Formal fairness notions are often
distinguished into group and individual fairness. Group fairness criteria typically require a form
of parity between demographic groups, e.g., along sensitive attributes like gender or race [54].
Individual fairness criteria typically demand to treat similar people alike [16].
Stage of the Lifecycle. Formal fairness is particularly relevant for the iterative loop of
model construction and model evaluation. As soon as the first model prototype is ready, fairness
metrics can be evaluated. Based on the evaluation, unfairness mitigation techniques can be
implemented and validated iteratively [97].



Source: data\tc20_2501.12962v1\referenced_papers\[5]_2406.03198.pdf (Page 4):

The Impossibility of Fair LLMs 1st HEAL Workshop at CHI Conference on Human Factors in Computing Systems, 2024
a structured database comprising people and their characteristics.
An LLM may be trained in part on such databases, but the output
of the model for such predictions will also be based on the wide
scope of unstructured natural language or other modalities of data
on which the model is trained.
Generalization across populations is also a concern for fairness
frameworks other than group fairness because of the wide range of
data, use cases, and social contexts at play in LLMs [60]. Here, we
consider two examples: individual fairness [20] and counterfactual
fairness, which is the most common causal notion of fairness [40].
Definition 6. (Individual fairness). A model achieves individual
fairness if similar individuals are treated similarly. Formally, this re-
quires that the distribution of model output is Lipschitz continuous
with respect to the distribution of model input.
Definition 7. (Counterfactual fairness). A model achieves coun-
terfactual fairness if the model would produce the same output for
an individual if they had a different level of the sensitive attribute.
In terms of individual fairness, it is not clear what similarity
metrics could be reasonably applied across the multitude of contexts
or, if multiple metrics were applied, how these could be judiciously
selected and guaranteed in each possible context. In terms of causal
fairness, including counterfactual fairness, it is often difficult to
identify the causal structure of the data-generating process in even
a single NLP task, and it would be an immense challenge for a single
model to account for all of the many different contextual factors
that determine counterfactuals or other causally distinct outcomes
across the varying populations.
4.2 Sensitive attributes proliferate in a
general-use setting
The preceding section considered the challenges of imposing fair-
ness across different data distributions. When considering different
sensitive attributes, given the issues discussed in Section 3.1, it may
not be tractable to exclude sensitive attributes from the training
data, and each of the different distributions and different tasks can
require fairness metrics to be enforced for a different set of sensitive
attributes. This is a challenge for the group fairness metrics already
defined, but the issue is particularly salient for the popular ideal
of fair representations within a machine learning model or fair
representations produced by one model and used by another [72].
Definition 8. (Fair representation). A representation is fair if it
does not contain information that can identify the sensitive at-
tributes of the individuals being represented.
In the fair representations framework, a system first maps the
dataset of individuals being represented to a probability distribution
in a novel representation space, such that the system preserves as
much information as possible about the individual while removing
all information about the individual’s sensitive attribute. The most
well-known example of this approach is Bolukbasi et al. [11], which
rigorously documented gender bias in Google News word embed-
dings, namely an association between occupations and a gender
vector (e.g., ®he− ®she), such that computer programmer was coded as
highly male while homemaker was coded as highly female. Indeed,
this is where much of the NLP fairness literature has focused, doc-
umenting similar biases across different word embedding models
Sesari et al. [see 61, for a review].
Researchers have developed a number of debiasing approaches
focused on the sensitive attribute dimension, such as zeroing the
projection of each word vector (e.g., each occupation) onto the
dimension itself [11] or training the model to align the sensitive
attribute dimension with the last coordinate of the embedding space,
so that it can be easily removed or ignored [75]. However, Gonen
and Goldberg [33] show that such approaches “are mostly hiding
the bias rather than removing it” because, after removal, word pairs
tend to maintain their similarity, which still reflects associations
with sensitive attributes—what Bolukbasi et al. [11] call “indirect
bias. ”
Achieving fairness in one LLM context may be contingent on
the removal of information or alteration of the statistical relation-
ships between the context-specific sensitive attribute and other
features of the data. For example, one may wish to exclude gender
information from financial lending decisions, but gender informa-
tion may be necessary for other tasks, such as drafting or editing
an email about a real-world situation that has important gender
dynamics that the sender hopes to communicate to the receiver.
Moreover, variables highly correlated with gender, such as biologi-
cal sex and pregnancy status, may be essential criteria for medical
decision-making. In general, attempts at debiasing for one context
may remove or distort important information for another context.
The naive approach of debiasing the model with respect to the
union of all potential sensitive attributes—even if it were empirically
feasible—would likely be too heavy-handed, leaving the model with
little information to be useful for any task. To effectively create a
fair LLM for every task and context, one would need to act upon
the parameters of the model with surgical precision to alter the
relationship between variables only when the model is instantiated
for a specific task and context. This is infeasible with current LLM
methods, such as fine-tuning, and currently we do not have robust
techniques to debias even a single problematic relationship without
incidentally obfuscating it or problematizing other relationships.
This game of fairness whack-a-mole seems indefinitely intractable.
Likewise, even if we could reduce the union of all potential sensitive
attributes to a manageable level, such as identifying a small set of
the most important to adjust for in each task, that would still require
yet-infeasible fine-grained adjustments to avoid counterproductive
side effects.
4.3 Fairness does not compose, but
fairness-directed composition may help
Whether a model’s behavior on a given task is fair or desirable
largely depends on how the model’s output will be used. In modern
AI systems including LLMs, the output of one model is often used
as the input to another model, but this produces an additional
challenge because fairness does not compose: a fairness guarantee
for each of two models is not a fairness guarantee for a system
composed of the two models—a point made most explicitly by
Dwork and Ilvento [21].
5



Source: data\tc20_2501.12962v1\referenced_papers\[26]_2403.20089.pdf (Page 4):

tial implications on fairness requirements of future AI developments, specifying and enforcing
concrete legal requirements will be an intricate future task. In the absence of legal precedents,
both disciplines are in need of pioneering work at the intersection of non-discrimination law
and algorithmic fairness.
References
[1] J. Angwin, J. Larson, S. Mattu, L. Kirchner, Machine Bias: There’s software used across
the country to predict future criminals. And it’s biased against blacks., in: K. Martin (Ed.),
Ethics of data and analytics, An Auerbach Book, CRC Press Taylor & Francis Group, Boca
Raton and London and New York, 2022, pp. 254–264.
[2] J. Dastin, Amazon Scraps Secret AI Recruiting Tool that Showed Bias against Women *,
in: K. Martin (Ed.), Ethics of data and analytics, An Auerbach Book, CRC Press Taylor &
Francis Group, Boca Raton and London and New York, 2022, pp. 296–299.
[3] A. Fuster, P. Goldsmith-Pinkham, T. Ramadoral, A. Walther, Predictably unequal? the
effects of machine learning on credit markets, The Journal of Finance 77 (2022) 5–47.
[4] S. Shankar, Y. Halpern, E. Breck, J. Atwood, J. Wilson, D. Sculley, No classification without
representation: Assessing geodiversity issues in open data sets for the developing world,
2017. URL: https://arxiv.org/pdf/1711.08536.pdf.
[5] J. Buolamwini, T. Gebru, Gender shades: Intersectional accuracy disparities in commercial
gender classification, in: Proceedings of the 1st Conference on Fairness, Accountability
and Transparency, volume 81, PMLR, 2018, pp. 77–91.
[6] J. Zhao, T. Wang, M. Yatskar, V. Ordonez, K.-W. Chang, Men also like shopping: Reducing
gender bias amplification using corpus-level constraints, in: Proceedings of the 2017
Conference on Empirical Methods in Natural Language Processing, 2017, pp. 2979–2989.
[7] K. Burns, L. A. Hendricks, K. Saenko, T. Darrell, A. Rohrbach, Women also snowboard:
Overcoming bias in captioning models, 2019. URL: https://arxiv.org/pdf/1803.09797.pdf.
[8] T. Bolukbasi, K.-W. Chang, J. Zou, V. Saligrama, A. Kalai, Man is to computer programmer
as woman is to homemaker? debiasing word embeddings, in: Proceedings of the 30th
International Conference on Neural Information Processing Systems, NIPS’16, Curran
Associates Inc., Red Hook, NY, USA, 2016, p. 4356–4364.
[9] A. Caliskan, J. J. Bryson, A. Narayanan, Semantics derived automatically from language
corpora contain human-like biases, Science 356 (2017) 183–186.
[10] N. Garg, L. Schiebinger, D. Jurafsky, J. Zou, Word embeddings quantify 100 years of
gender and ethnic stereotypes, Proceedings of the National Academy of Sciences 115
(2018) E3635–E3644.
[11] B. D. Mittelstadt, P. Allo, M. Taddeo, S. Wachter, L. Floridi, The ethics of algorithms:
Mapping the debate, Big Data & Society 3 (2016) 1–21.
[12] M. Kearns, A. Roth, The Ethical Algorithm - The Science of Socially Aware Algorithm
Design, Oxford University Press, New York, 2019.
[13] D. Martens, Data Science Ethics - Concepts, Techniques, and Cautionary Tales, Oxford
University Press, New York, 2022.
[14] L. Floridi, J. Cowls, M. Beltrametti, R. Chatila, P. Chazerand, V. Dignum, C. Luetge,



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 11):

12 
 
better off,”25  and yet this is precisely what current applications of group 
fairness achieve in fairML. 
3 HOW COMMON IS LEVELLING DOWN IN FAIRML? 
The usage of equality -based fairness measures in machine learning is 
not self -evidently troubling; rather, it is how they are enf orced in 
practice, and the resulting levelling down, which causes problems . 
When used solely for diagnostic purposes, egalitarian measures such as 
(conditional) demographic parity 26  or equal opportunity  27 provide a 
helpful warning that different groups are being treated differently, and 
that they are potentially being harmed in different ways by a n 
algorithmic decision-making system.  However, using them to 
determine which models should be deployed in real world use  cases 
raises serious ethical and legal concerns. Levelling down can occur as a 
direct result of the use of egalitarian measures in model selection. In 
many ways this problem is another example of Goodhart’s Law  that 
“when a measure becomes a target, it ceases to be a good measure.”28 
In this section we demonstrate how levelling down occurs for a range 
of fairness measures, focusing on two of the most common ly used  
metrics in the fairML literature: demographic parity and equal 
opportunity. To do so, we enforce these measures acro ss a range of 
algorithms using two of the most widely used fairness toolkits : 
FairLearn and IBM  AI Fairness 360  (IBM360).29 We show that t he 
existence of levelling down is not a limitation  or design flaw  of these 
toolkits or methodologies; rather, it is a natural consequence of strictly 
enforcing equality as part of model selection.30 As such, levelling down 
 
25 Chloé Bakalar et al., Fairness on the ground: Applying algorithm ic fairness 
approaches to production systems, ARXIV PREPRINT ARXIV:2103.06172, 5 (2021). 
26 Sandra Wachter, Brent Mittelstadt & Chris Russell, Why fairnes s cannot be 
automated: Bridging the gap between EU non-discrimination law and AI, 41 COMPUTER 
LAW & SECURITY REVIEW 105567 (2021); Faisal Kamiran & Toon Calders, Data 
preprocessing techniques for classification without discrimination , 33 KNOWLEDGE AND 
INFORMATION SYSTEMS 1 (2012). 
27 Moritz Har dt, Eric Price & Nati Srebro, Equality of opportunity in supervised 
learning, in ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 3315 (2016). 
28 Marilyn Strathern, ‘Improving ratings’: audit in the British University system , 5 
EUROPEAN REVIEW 305 (1997). 
29 Rachel KE Bellamy et al., AI Fairness 360: An extensible toolkit for detecting and 
mitigating algorithmic bias , 63 IBM JOURNAL OF RESEARCH AND DEVELOPMENT 4: 1 
(2019); Sarah Bird et al., Fairlearn: A toolkit for assessing and improving fairness in AI, 
MICROSOFT, TECH. REP. MSR-TR-2020-32 (2020). 
30 For example, Kim discusses a range of changes to the design of an ML algorithm that 
could decrease the “disparate impact” (a concept from US anti -discrimination law 
loosely corresponding to demographic parity) of the decisions made by a system . See: 
Pauline Kim, Race-Aware Algorithms: Fairness, Nondiscrimination and Affirmative 
Action, CALIFORNIA LAW REVIEW (2022), https://papers.ssrn.com/abstract=4018414 (last 
visited Jul 13, 2022).  If a data scientist systematically explored combinations of these 
changes, and then selec ted the model with disparate impact  below a predetermined



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 35):

36 
 
critically thought about the theory of equality their models should 
promote.144145 This cannot be taken for granted. Rather, the vast 
majority of cases of levelling down are unintentional and invisible, 
resulting from convenien ce and the limited ways performance and 
(dis)parity are currently measured  ,146 not theoretical conviction or 
justification.  
It is likewise unclear whether substantive equality goals can be 
achieved directly through enforcement of fairness measures on ML 
models. Intentional levelling down to create a worse performing 
classifier, for example, can draw attention to a problematic performance 
gap in order to prompt civil action. Levelling down does not produce this 
effect by itself. Rigid enforcement of fairnes s measures does not allow 
for external corrective action to reduce harm because measures must be 
solvable with the data at hand .147 In cases of competition over limited 
resources, a bigger ‘piece of the pie’ requires a smaller piece to be given 
to someone e lse. FairML typically cannot make this sort of trade -off 
explicitly because models do not have a picture of ‘how big the pie is’, or 
awareness of the limitations of the resources at hand. 148 Nonetheless, 
actions which are not directly quantifiable or within the control of the 
modeller, such as collecting more data on equality -relevant features 
(e.g., socioeconomic status, prior opportunities) or increasing available 
resources (e.g., cancer screening) in the production environment, are  
typically not considered but could be viable means to avoid levelling 
down in practice.149  
 
144 Binns, supra note 7. 
145 We have made a similar observation in prior work introducing the notion of bias 
preservation, where we argued that researchers, developers, and deployers of ML 
systems need to explicitly choose the biases their models should exhibit. See: Wachter, 
Mittelstadt, and Russell, supra note 10.  Our argument here is complementary but 
distinct; we argue that people working in fair ML need to be more explicit and reflective 
about the underlying goals their choice of fairness measures and methods supports. 
146 Kuppler et al., supra note 49. 
147 Binns, supra note 63. 
148 Enforcing fairness typically involves balancing output rates (e.g., acceptance rates, 
sufficiently high recall for canc er detection) between groups. For specific instances of 
levelling down in fair ML to be justified under something like the ‘levelling the playing 
field’ argument (see: Section 4.2.1), models would need to be distributing a known 
limited quantity of a set of outputs. In practice, this is not how classifiers operate. This 
observation does not, however, preclude justification of levelling down at a general level. 
Decision-makers may, for example, choose to lower performance for specific historically 
advantaged groups for all classifiers used in a given sector or for classifiers considering 
specific historically disadvantaged groups in order to level the playing field. 
149 Binns, supra note 63; Cooper, Abrams, and Na, supra note 9. Collection of data on 
equality-relevant features is not the same as collecting more representative d ata to 
combat bias against data impoverished groups, which is a common approach in the field 
and can avoid the need for levelling down. This is sometimes called ‘active fairness’ . 
See: Id. This approach helps mitigate biases in the existing data affecting 
disadvantaged groups without directly impacting adv antaged group performance or 
outcomes. For examples see : Obermeyer and Mullainathan, supra note 20; Noriega -



### Claim 11/36

#### Claim Text
There is tension [26] between the need for debiasing AI algorithms and data protection law, which Article 10(5) tries to solve.

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[26]_2403.20089.pdf (Page 2):

the regulation establishes special requirements (Art. 6 et seq. AI Act) for high-risk systems in
the areas of education (Recital 35), employment (Recital 36), insurance and credit (Recital 37),
law enforcement (Recital 38), as well as migration (Recital 39). However, despite its explicit goal
to prevent discrimination, the regulation lacks a clear substantive standard for determining
when unequal treatment is inadmissible. According to Art. 10(2)f AI Act “[t]raining, validation
and testing data sets shall be subject to data governance and management practices appropriate
for the intended purpose of the AI system” and thus have to be examined for “possible biases
that are likely to [...] lead to discrimination prohibited under Union law”. The AI Act therefore
leaves the judgment call about what constitutes illegal discrimination to existing legislation.
However, traditional non-discrimination law’s requirements can only be implemented during
model development (as intended by the AI Act) if they are “translated” into technical fairness
requirements. To achieve this goal, scholars from all domains are bound to collaborate. When
doing so, they must proceed in a conscious and contextualizing manner and take into account
the diverging perspectives of AI Act and non-discrimination law. European non-discrimination
law is tailored to individual instances of discrimination after an AI model has been deployed—an
inherently retrospective approach. In contrast to this, the AI Act prospectively demands fairness
interventions by implementing non-discrimination requirements at the stage of model design.
Guidance by democratically justified institutions on how to implement such requirements might
bridge the gap toward alleviating both the legal and the technical enforcement problems.
Enabling “bias detection and correction”? Legal requirements for the development of AI
systems are not only subject to the AI Act. Due to the tension between fairness and privacy
during the training and evaluation stage of AI, conflicts with data protection law may equally
arise. On the one hand, ignoring personal demographic data promotes the same risk as the
widely rejected idea of fairness through unawareness because legally protected attributes like
race and gender usually correlate to innocuous proxy variables [39, 40]. If protected attributes
are unavailable during model training and evaluation, these subtle correlations cannot be
accounted for, nor can technical fairness metrics be tested and optimized. On the other hand,
Art. 9 GDPR places particularly high demands on the lawful processing of personal data about
special categories. Therefore, the same sensitive data that is protected by data protection law
is also essential to effectively avoid discriminatory outputs. The AI Act seeks to mitigate this
tension by broadening the scope of lawful data processing. Art. 10(5) AI Act states that “[t]o
the extent that it is strictly necessary for the purposes of ensuring bias detection and correction in
relation to the high-risk AI systems [...], the providers of such systems may exceptionally process
special categories of personal data referred to in Art. 9(1) [GDPR]. ” This is accompanied by
Recital 44c, which adds that “[i]n order to protect the right of others from the discrimination that
might result from the bias in AI systems [...] the providers should, exceptionally, [...] be able to
process also special categories of personal data, as a matter of substantial public interest within
the meaning of Art. 9(2)(g) [GDPR]. ”Therefore, discrimination and fairness considerations can
provide a justification for data processing during the training phase of high-risk AI systems.
However, balancing the public and private interests regarding non-discrimination and privacy
will inevitably lead to intricate trade-offs.



Source: data\tc20_2501.12962v1\referenced_papers\[84]_2410.14501.pdf (Page 9):

10 
 
AI Act: Article 10 AI Act implements Article 9 GDPR, and the GDPR implements the right to the 
protection of personal data.49 
For a given measure to be  a strict necessity, first, the measure must be limited to what is necessary. 
Necessity means that the measure is the least intrusive measure amongst the suitable measures for 
the achievement of the objective it pursues. In the Schecke case, the CJEU ruled that a measure was 
unnecessary because it was ‘possible to envisage measures which affect less adversely the 
fundamental rights of natural persons and which still contribute effectively to the objectives […] 
in question […].’50 
The CJEU sets a high threshold for the necessity test: the measure must be limited to what is strictly 
necessary in light of i ts goals. The word strict means that the measure must be carefully drafted 
and its goals must be well -defined. According to CJEU case law regarding data retention, this 
requires a high level of nuance and granularity.  To illustrate, In the case Digital Rights Ireland, the 
CJEU ruled that the Data Retention Directive should have been ‘precisely circumscribed by provisions 
to ensure that it is actually limited to what is strictly necessary.’51 The strict necessity test is granular: 
the more precise and targeted a measure is, the higher the chance that the measure becomes the 
least restrictive option.52 
More concretely, w hat does the strict necessity test mean for providers applying the exception in 
Article 10(5) AI Act? I highlight three possible implications: First, providers will have to be precise 
in defining how they mean to use the sensitive data to remove biases from the dataset. This may 
be tricky for the purpose of AI de-biasing. While the AI Act states in Recital 70 that the exception 
aims to prevent discrimination , the AI Act itself does  not clarify the link between bias and non -
discrimination.53 Furthermore, preventing discrimination is not an exact science, unlike measuring 
bias. Methods for preventing discrimination are still not mature at the time of writing.54 Providers 
may therefore have a tough time with t he strict necessity test: it sets a high bar for a goal such as 
AI de-biasing. Providers will have to think carefully about which biases they  aim to examine their 
datasets for, how this can prevent discrimination and how to carry out the examination in the least 
intrusive way. The state of the art in AI de-biasing, guidance by supervisory authorities and future 
standards to conduct the examination are also relevant in that assessment.55 
Second, If suitable measures exist to remove the biases without using sensitive data, then those 
measures may be less intrusive. And it it preferable if a provider can process less data for removing 
the biases, for example by only using a sub -sample of the full dataset for testing and deleting the 
original dataset. Some techniques for measuring bias require less sensitive data than others.56 
 
49 Recital 1 GDPR. 
50 Volker und Markus Schecke GbR (C -92/09) and Hartmut Eifert (C -93/09) v Land Hessen [2010] CJEU C-92/09 and C-
93/09 [86]. 
51 Emphasis added. Digital Rights Ireland Ltd [2014] HvJEU C-293/12 en C-594 [65]. 
52 Lorenzo Dalla Corte, ‘On Proportionality in the Data Protection Jurisprudence of the CJEU’ (2022) 12 International 
Data Privacy Law 259, 270 <https://academic.oup.com/idpl/article/12/4/259/6647961> accessed 8 October 2024.  
53 Non-discrimination law also does not yet play well with algorithmic discrimination. See e.g. Raphaële Xenidis, 
‘Tuning EU Equality Law to Algorithmic Discrimination: Three Pathways to Resilience’ (2020) 27 Maastricht Journal 
of European and Comparative Law 736, s 3.A <https://journals.sagepub.com/doi/10.1177/1023263X20982173>.  
54 See Hilde Weerts and others, ‘Algorithmic Unfairness through the Lens of EU Non -Discrimination Law: Or Why 
the Law Is Not a Decision Tree’, 2023 ACM Conference on Fairness, Accountability, and Transparency  (ACM 2023) 
<https://dl.acm.org/doi/10.1145/3593013.3594044>. 
55 See also Section 4.3 and for further discussion Section 5 (Discussion). 
56 For example, methods using proxy variables, unsupervised learning models, causal fairness, or synthetic data. See 
also section 4.6 of the paper . See also Sebastiaan Berendsen and Emma Beauxis -Aussalet, ‘Fairness versus Privacy: 
Sensitive Data Is Needed for Bias Detection’ ( UCDS research group at VU , 14 March 2024)



Source: data\tc20_2501.12962v1\referenced_papers\[84]_2410.14501.pdf (Page 1):

2 
 
In June 2024, the new EU AI Act came into force. The AI Act includes obligations for the provider of an AI 
system. Article 10 of the AI Act includes a new obligation for providers to evaluate whether their training, validation 
and testing datasets meet certain quality criteria, including an appropriate examination of biases in the datasets and 
correction measures. With the obligation comes a new provision in Article 10(5) AI Act, allowing providers to collect 
sensitive data to fulfil the obligation. The exception aims to prevent discrimination. In this paper, I research the scope 
and implications of Article 10(5) AI Act. The paper primarily concerns European Union law, but may be relevant 
in other parts of the world, as policymakers aim to regulate biases in AI systems. 
1 INTRODUCTION 
In June 2024, the new EU AI Act came into force .1 The European Union legislator aims  for, on 
the one hand, innovation within the European Union: introducing AI could have many competitive 
advantages in many areas of life, from healthcare to culture, public services, justice and climate 
change mitigation and adaptation. 2 The AI Act aims to ‘ improve the functioning of the internal 
market and promote the uptake of human-centric and trustworthy artificial intelligence (AI)’.3 On 
the other hand, the AI Act aims to protect people against risks to health, safety, fundamental rights, 
and several other public interests.4 
The AI Act includes obligations for the provider of an AI system: the ‘natural or legal person that 
develops an AI system […] or that has an AI system […] developed and places it on the market or 
puts the AI system into service’.5 Article 10 of the AI Act includes a new obligation concerning the 
de-biasing of AI systems: in short, the providers of AI systems must evaluate whether their training, 
validation and testing datasets meet certain quality criteria. Those criteria include an ‘examination 
in view of possible biases’ and ‘appropriate measures to detect, prevent and mitigate possible 
biases.’6 With the obligation comes a new provision in Article 10(5) AI Act, allowing providers to 
collect sensitive data to fulfil the obligation. The provision aims to prevent discrimination.7 Before 
the AI Act, the ban on collecting sensitive data in the GDPR meant that developers were not 
allowed to collect such data for AI de-biasing.8 
In this paper, I research the scope and implications of Article 10(5) AI Act . The paper may be 
useful for (technical) practitioners implement ing the AI Act, or legal scholars who wish to 
understand the AI Act’s exception and definitions better. The paper primarily concerns European 
Union law, but may be useful in other parts of the world as well.9 The paper includes insights from 
 
1 Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised 
rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, 
(EU) 2018/858, (EU) 2018/1139 an d (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 
2020/1828 (Artificial Intelligence Act), https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=OJ:L_202401689. 
2 Recital 4 AI Act. 
3 Article 1(1) AI Act. The term ‘human -centric AI’ originates from earlier European Commission documents from 
2019. See European Commission, COM 2019/168 Building Trust in Human -Centric Artificial Intelligence  (2019) 
<https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52019DC0168>. 
4 Recital 5 AI Act and Article 1(1) AI act. 
5 Definition of ‘provider’, Article 3(3) AI Act. 
6 Article 10(2)(f) and (g) AI Act. 
7 Recital 70 AI Act. 
8 Article 9 GDPR. See also Marvin Van Bekkum and Frederik Zuiderveen Borgesius, ‘Using Sensitive Data to Prevent 
Discrimination by Artificial Intelligence: Does the GDPR Need a New Exception?’ (2023) 48 Computer Law & 
Security Review 105770 <https://linkinghub.elsevier.com/retrieve/pii/S0267364922001133>. 
9 For example, the blueprint for the US AI Bill of Rights refers to ‘proactive equity assessments as part of the system 
design’. Article 10 AI Act is an example how such a proactive assessment. See Blueprint for an AI Bill of Rights. Making 
Automated Systems Work for the American People  (The White house | OSTP 2022) 5, 23, 26 
<https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf>.



Source: data\tc20_2501.12962v1\referenced_papers\[84]_2410.14501.pdf (Page 10):

11 
 
Third and finally, the provider must take into account the risk of unlawful access to the data. The 
longer the provider keeps the data for de-biasing, the bigger the risk of a data breach. 
Overall, the strict necessity test seems to set a high bar for AI de -biasing. The future will tell how 
the test will be applied in AI de-biasing exactly. 
4.5 APPROPRIATE SAFEGUARDS 
While processing the sensitive data, providers must implement  ‘appropriate safeguards for the 
fundamental rights and freedoms of natural persons’: an open norm.57 Apart from this open norm, 
Article 10(5) AI Act includes a list of several conditions  under Article 10(5)(a) – (f) AI Act. I will 
discuss Article 10(5)(a) in the next section. 
The conditions in Article 10(5), conditions (b) to (f) AI Act include several security, privacy and 
accountability safeguards that the provider must implement. Condition (b)  requires technical 
measures on the re-use of the data, and state -of-the-art security and privacy-preserving measures. 
Condition (c) requires confidentiality obligations with strict access controls. Condition (d) forbids that 
the data are ‘[…] accessed by other parties’. Presumably, the access controls and confidentiality 
organisations in condition (c) and other safeguards must guarantee that other parties cannot access 
the sensitive data. Condition (e) concerns data retention. Condition (f) concerns accountability in light 
of the GDPR: providers must keep a record of their processing activities, explain why processing 
the sensitive data is strictly necessary, and why de-biasing could not be achieved without processing 
other data All of these conditions overlap with the GDPR’s data protection principles.58 
The list of safeguards  shows how the legislator seeks to balance data protection and non -
discrimination law: the provider may process the data, but only under strict safeguards.59 While the 
safeguards make sense, providers could consider more safeguards to make the exception the least 
intrusive and therefore strictly necessary. First, a trusted third party that already stores the data could 
perhaps share the data with provider, which means that the provider does not need to collect the 
data directly from data subjects. Condition (d) states that the sensitive data are not to be accessed 
by other parties: The exception does not allow providers to create, for example, entire data sharing 
platforms. However, it seems that the provider may still receive (anonymized) sensitive data from 
other parties that legitimately collected it, such as, for example, the national statistics bureaus.60 
4.6 THE GDPR STILL APPLIES 
According to Article 10(5)  AI Act, the exception applies ‘in addition to the provisions set out in 
[the GDPR].’ We can also see this in one of the specified conditions in the exception : Article 
10(5)(a) AI Act states that t he exception does not apply if the provider can also fulfil the bias 
examination and detection effectively by processing other data , such as synthetic or anonymised 
data.61 Anonymised data is non -personal, so the GDPR does not apply  in that case. As the word 
synthetic suggests, synthetic data is essentially ‘fake’, or artificially generated data. In the context of 
AI de-biasing, synthetic data aims to strike a balance between privacy and useability: a dataset that 
 
<https://ucds.cs.vu.nl/fairness-versus-privacy-sensitive-data-is-needed-for-bias-detection/> accessed 7 October 
2024. 
57 This open norm can be inspired by the GDPR, see 4.6.1. 
58 See Section 4.6.1 for a comparison. 
59 See Section 2. 
60 See Sebastiaan Berendsen and Emma Beauxis-Aussalet (n 56). 
61 See also Carolyn Ashurst and Adrian Weller, ‘Fairness Without Demographic Data: A Survey of Approaches’, Equity 
and Access in Algorithms, Mechanisms, and Optimization  (ACM 2023) 
<https://dl.acm.org/doi/10.1145/3617694.3623234>.



Source: data\tc20_2501.12962v1\referenced_papers\[84]_2410.14501.pdf (Page 14):

15 
 
4.7 SUMMARY AND CONCLUSION 
Overall, Article 10(5) AI Act includes many useful safeguards , which can help  to protect data 
subjects. However, the definitions for terms such as bias and provider limit the scope of the exception 
significantly. In the next section, I discuss whether this limitation is a problem for AI debiasing in 
practice. 
5 DISCUSSION: IS THE EXCEPTION EFFECTIVE? 
The de-biasing exception and obligation target discrimination.83 Biases present in the data used to 
develop AI systems can (ultimately) have discriminatory effects on society, after the AI system has 
been trained and the system takes a decision. Removing discriminatory effects of AI systems seems 
possible.84 Non-discrimination law could play an important role here, even if the link between 
measuring bias and weighing whether a case constitutes discrimination is currently often not clear. 
The exception is not perfect, however. Three points in the exception require further discussion: (i) 
the limitation of the exception to the provider of AI systems, (ii) the limitation to biases in the 
training, validation and testing datasets, (iii) the AI act does not describe which biases developers 
must remove. 
(i) The exception only applies for the provider of the AI system, and the developer may not share 
the sensitive data with deployers, which has consequences for the effectiveness of the exception . 
Non-discrimination, is a highly contextual field. 85 A developer who wishes to prevent 
discriminatory effects resulting from biases in data must have information about the use of the AI 
system in practice : how is the system deployed? Article 10 AI Act does not explicitly require 
deployers to work together with providers to take away discriminatory effects the systems have. 
I distinguish between three main cases of AI development and deployment: AI as a service, in-house 
development of AI, and further development of an existing AI system.86 First, the provider and the 
deployer of the AI system may be entirely separate parties : the deployer rents the AI, or in other 
words, uses AI as a service  (AIaaS). Multiple deployers could apply an AI system  created by one 
provider in different regions of the world , and contexts. For example, an employment system 
developed by a provider in Greece could be rented to an employer  (the deployer) in The 
Netherlands. In such cases, without further context about the setting and target group of the  
system, the developer cannot take away potential discriminatory effects effectively.87 The developer 
and deployers must then collaborate . It seems questionable whether a single AI system can 
facilitate, for example, hiring in multiple countries : the developer may have to fine -tune the AI 
 
83  
84 Biases may ultimately also come from the context in which the AI system is deployed, rather than the data used to 
train the AI. However, removing biases from datasets can be a good start. 
85 Michael Veale, Max Van Kleek and R euben Binns, ‘Fairness and Accountability Design Needs for Algorithmic 
Support in High -Stakes Public Sector Decision -Making’, Proceedings of the 2018 CHI Conference on Human Factors in 
Computing Systems (ACM 2018) 10 <https://dl.acm.org/doi/10.1145/3173574.3174014> accessed 11 October 2024.  
86 In the EU, some sectors seem to have more in -house development of AI than others.  See Charles Hoffreumon, 
Chris Forman and Nicolas Van Zeebroeck, ‘Make or Buy Your Artificial Intelligence? Complementarities in 
Technology Sourcing’ (2024) 33 Journal of Economics & Management Strategy 452 
<https://onlinelibrary.wiley.com/doi/10.1111/jems.12586>. 
87 Data scientists have made this point before. See Kornel Lewicki and others, ‘Out of Context: Investigating the Bias 
and Fairness Concerns of “Artificial Intelligence as a Service”’, Proceedings of the 2023 CHI Conference on Human Factors in 
Computing Systems (ACM 2023) s 6 <https://dl.acm.org/doi/10.1145/3544548.3581463> accessed 9 October 2024.



### Claim 12/36

#### Claim Text
In order to effectively mitigate biases in AI systems, the processing of personal data (for example, to compute fairness metrics) is important [84].

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[27]_2404.18736.pdf (Page 7):

Undesirable model behavior often stems from flawed data that contains misrepresentations
of the world (e.g., erroneous, mislabeled, or imbalanced data) or accurate representations that
are societally undesirable (e.g., historical inequalities). The goal of data fairness is to identify
and address such flaws in the data set used to train an AI model.
Prior studies have pointed out data issues as drivers of unfairness [89, 65, 67] or highlighted
data as a starting point for unfairness mitigation [ 90, 91, 92]. Mehrabi et al. [65] provide a
comprehensive list of data biases that may introduce unfairness early on in AI development.
Awareness and understanding of these biases are crucial to derive strategies to handle them.
Stage of the Lifecycle. Data fairness relates to data collection (e.g., in the form of labeling
errors, sampling bias, imbalances, etc.) and data analysis, which aims to identify and potentially
mitigate unfair data characteristics early on. However, data fairness also reaches into feature
selection which is usually part of an iterative loop together with model construction. For
example, features might be dropped if they are not justifiably task-relevant. Importantly, in the
context of sensitive attributes, developers should always be aware of the flaws of the idea of
“fairness through unawareness” [16, 62].
The Role of XAI. As data is a main source of unfairness [ 65], XAI is suited to identify
potential disparities, imbalances, or abnormalities manifested in the available data early on.
Descriptive statistics are a natural first step to explore pre-existing disparities [67]. Anik and
Bunt [88] and Mitchell et al. [49] provide two examples of how to present simple descriptions
and visualizations about the collection, feature distributions, and patterns of a dataset. XAI
techniques have further been claimed to reveal instances and features in the data that have
undesirable effects on the model output [ 93, 94, 66] which, however, are often subject to
questionable causal and normative assumptions [95, 96]. These approaches indicate a strong
connection between data fairness and formal fairness.
3.3. Formal Fairness
“To counteract biases, it is, therefore, crucial to enable their detection. Explainability approaches
may aid in this regard by providing means to track down factors that may have contributed
to unfair and unethical decision-making processes and either to eliminate such factors, to
mitigate them, or at least to be aware of them. ”[40, p. 6]
The most common fairness desideratum is concerned with formal model properties. Byformal
fairness, we refer to the vast array of formal criteria that have been proposed as mathematical
and statistical measures of fairness [13, 15].
Consistent with Verma and Rubin[14], this includes all fairness definitions based on statistical
measures (e.g., demographic parity), similarity measures (e.g., fairness through awareness),
or based on causal reasoning (e.g., counterfactual fairness). Formal fairness notions are often
distinguished into group and individual fairness. Group fairness criteria typically require a form
of parity between demographic groups, e.g., along sensitive attributes like gender or race [54].
Individual fairness criteria typically demand to treat similar people alike [16].
Stage of the Lifecycle. Formal fairness is particularly relevant for the iterative loop of
model construction and model evaluation. As soon as the first model prototype is ready, fairness
metrics can be evaluated. Based on the evaluation, unfairness mitigation techniques can be
implemented and validated iteratively [97].



Source: data\tc20_2501.12962v1\referenced_papers\[84]_2410.14501.pdf (Page 12):

13 
 
rather than the data of all data subjects. In many cases, finding such a smaller sample of data that 
has the same statistical properties is possible.68 
According to t he data accuracy principle, personal data the provider gathers gathers must not be 
incorrect. At first glance, f or the purpose of detecting and correcting biases, this principle seems 
inherently necessary: without accurate data, the bias detection and correction process would not be 
effective. The data must be representative.  It is unclear what kind of accuracy is necessary for AI 
de-biasing with non-discrimination as the ultimate goal. Scholars have long debated whether non-
discrimination attributes, such as ethnicity,  must be self-reported by the data subject (sometimes 
called self-identification) or based on a predefined set of criteria.69 There is something to be said 
for both approaches: ethnicity, for example, is an ambiguous term with different meanings across 
countries or even different contexts.70 Depending on the definition, the ethnicity could be more of 
less accurate for the data subject or the AI de-biasing. As far as I know, best practices on this issue 
do not exist at the time of writing, but both approaches seem compatible with human rights.71 
The storage limitation principle states that the provider must keep the sensitive data ‘in a form which 
permits identification of data subjects for no longer than is necessary for the purposes for which 
the personal data are processed’.72 When the provider has complied with the de-biasing obligation, 
the provider must delete the data , unless the provider has another compatible purpose. The 
provider must choose a retention period that is not longer than strictly necessary for the purpose 
of the processing.73 The condition Article 10(5)(e) rephrases the storage limitation principle quite 
literally: the special categories of personal data must be ‘deleted once the bias has been corrected 
or the personal data has reached the end of its retention period, whichever comes first’.  
The principle of integrity and confidentiality requires appropriate security of the personal data , in the 
form of technical and organisational measures. The AI Act exception gives many examples of 
measures in Article 10(5) AI Act under (b), (c), (d) and (e). Depending on the context in which the 
de-biasing takes place, more safeguards, such as a trusted third party, could be appropriate.74  
Finally, the principle of accountability states that the providers must demonstrate compliance with 
all the other data protection principles. Read together with the phrase strictly necessary, the principle 
of accountability requires the provider to keep a record stating the exact purpose of the processing, 
explaining why the processing is truly necessary, and why the chosen measure is the least intrusive. 
Article 10(5)(f) AI Act explicitly states a similar reading.75 
 
68 See e.g. Jun Yu, Mingyao Ai and Zhiqiang Ye, ‘A Review on Design Inspired Subsampling for Big Data’ (2024) 65 
Statistical Papers 467 <https://link.springer.com/10.1007/s00362-022-01386-w>. 
69 See e .g. Julie Ringelheim, ‘Processing Data on Racial or Ethnic Origin for Antidiscrimination Policies: How to 
Reconcile the Promotion of Equality with the Right to Privacy?’ [2007] SSRN Electronic Journal 
<http://www.ssrn.com/abstract=983685>. AI practitioners who wish to de-bias their AI also struggle with this issue. 
See M Andrus and others, ‘“What We Can’t Measure, We Can’t Understand”: Challenges to Demographic Data 
Procurement in the Pursuit of Fairness’ [2021] arXiv:2011.02282 [cs] s 6.1 <http://arxiv.org/abs/2011.02282>.  
70 The CJEU has stated that ‘Ethnic origin cannot be determined on the basis of a single criterion but, on the contrary, 
is based on a whole number of factors, some objective and others subjective. ’ Jyske Finance [2017] CJEU C ‑668/15, 
ECLI:EU:C:2017:278 [19]. See also Sofia Jaime and Christoph Kern, ‘Ethnic Classifications in Algorithmic Fairness: 
Concepts, Measures and Implications in Practice’, The 2024 ACM Conference on Fairness, Accountability, and Transparency  
(ACM 2024) <https://dl.acm.org/doi/10.1145/3630106.3658902> accessed 4 October 2024.  
71 Ringelheim (n 69). 
72 Article 5(1)(e) GDPR. 
73 This also overlaps with the meaning of strictly necessary, see Section 4. 4. The provider may still create synthetic or 
anonymized data, as we mentioned in the introduction to this section. 
74 See section 4.5.s 
75 Compare with Section 4.5.



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 3):

4 
 
computer science, software engineering, and mathematics. These 
groups have developed numerous measures and methods  to mitigate 
bias and  improve fairness in algorithmic systems. However, the 
majority of these tools have been built in isolation from policy and civil 
societal contexts and lack serious engagement with philosophical, 
political, legal, and economic theories of  equality and  distributive 
justice.7 Reflecting this, most define fairness in simpl e terms, where 
fairness means red ucing gaps in performance or outcomes between 
demographic groups. Successfully achieving algorithmic fairness has 
come to mean satisfying one of these simple mathematical definitions, 
while preserving as much of the accuracy of the original system as 
possible. 
This oversimplification of equality through fairness measures could 
possibly be attributed to the relative youth of fairML . However, the 
practical impact of the approach adopted by the field to date is morally 
troubling. Many current fairness measures have been shown to suffer 
from both fairness and performance degradation, or “levelling down,” 
where fairness is achieved by making every group worse off , or by 
bringing better performing groups down to the level of worse performing 
groups.8 Levelling down is effectively fairness achieved by breaking the 
system, for example by making a classifier less accurate so it performs 
equally badly across all relevant groups. 
Levelling down is a symptom of the decision to measure fairness 
solely in terms of equality, or disparity between groups in performance 
and outcomes, while ignoring other relevant features of distributive 
justice such as absolute welfare or priority which are more difficult to 
quantify and directly measure in research and development 
environments. When fairness can only be measured in terms of 
distribution of performance or outcomes, corrective actions can likewise 
only target how these goods are distributed between groups. The field 
effectively only has egalitarian tools at its disposal which value equality 
of treatment and outcomes while ignoring other goods of distributive 
justice. Likewise, the prevalence of levelling down in fairML suggests 
that the field is, intentionally or otherwise, adopting a strict egalitarian 
approach to questions of distributive justice  in which the only 
(measurable) value is equality. We name these trends in fairML ‘strict 
egalitarianism by default’. 
Strict e galitarianism by default , at least in its most gratuitous 
forms, runs counter to both the stated objectives of fairness measures 
 
7 Reuben Binns, On the apparent conflict between individual and group fairness , in 
PROCEEDINGS OF THE 2020 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND 
TRANSPARENCY 514 (2020), https://doi.org/10.1145/3351095.3372864 (last visit ed Aug 
14, 2022). 
8 DOMINIK ZIETLOW ET AL ., Leveling Down in Computer Vision: Pareto Inefficiencies in 
Fair Deep Classifiers, (2022), http://arxiv.org/abs/2203.04913 (last visited Jun 10, 2022).



Source: data\tc20_2501.12962v1\referenced_papers\[26]_2403.20089.pdf (Page 2):

the regulation establishes special requirements (Art. 6 et seq. AI Act) for high-risk systems in
the areas of education (Recital 35), employment (Recital 36), insurance and credit (Recital 37),
law enforcement (Recital 38), as well as migration (Recital 39). However, despite its explicit goal
to prevent discrimination, the regulation lacks a clear substantive standard for determining
when unequal treatment is inadmissible. According to Art. 10(2)f AI Act “[t]raining, validation
and testing data sets shall be subject to data governance and management practices appropriate
for the intended purpose of the AI system” and thus have to be examined for “possible biases
that are likely to [...] lead to discrimination prohibited under Union law”. The AI Act therefore
leaves the judgment call about what constitutes illegal discrimination to existing legislation.
However, traditional non-discrimination law’s requirements can only be implemented during
model development (as intended by the AI Act) if they are “translated” into technical fairness
requirements. To achieve this goal, scholars from all domains are bound to collaborate. When
doing so, they must proceed in a conscious and contextualizing manner and take into account
the diverging perspectives of AI Act and non-discrimination law. European non-discrimination
law is tailored to individual instances of discrimination after an AI model has been deployed—an
inherently retrospective approach. In contrast to this, the AI Act prospectively demands fairness
interventions by implementing non-discrimination requirements at the stage of model design.
Guidance by democratically justified institutions on how to implement such requirements might
bridge the gap toward alleviating both the legal and the technical enforcement problems.
Enabling “bias detection and correction”? Legal requirements for the development of AI
systems are not only subject to the AI Act. Due to the tension between fairness and privacy
during the training and evaluation stage of AI, conflicts with data protection law may equally
arise. On the one hand, ignoring personal demographic data promotes the same risk as the
widely rejected idea of fairness through unawareness because legally protected attributes like
race and gender usually correlate to innocuous proxy variables [39, 40]. If protected attributes
are unavailable during model training and evaluation, these subtle correlations cannot be
accounted for, nor can technical fairness metrics be tested and optimized. On the other hand,
Art. 9 GDPR places particularly high demands on the lawful processing of personal data about
special categories. Therefore, the same sensitive data that is protected by data protection law
is also essential to effectively avoid discriminatory outputs. The AI Act seeks to mitigate this
tension by broadening the scope of lawful data processing. Art. 10(5) AI Act states that “[t]o
the extent that it is strictly necessary for the purposes of ensuring bias detection and correction in
relation to the high-risk AI systems [...], the providers of such systems may exceptionally process
special categories of personal data referred to in Art. 9(1) [GDPR]. ” This is accompanied by
Recital 44c, which adds that “[i]n order to protect the right of others from the discrimination that
might result from the bias in AI systems [...] the providers should, exceptionally, [...] be able to
process also special categories of personal data, as a matter of substantial public interest within
the meaning of Art. 9(2)(g) [GDPR]. ”Therefore, discrimination and fairness considerations can
provide a justification for data processing during the training phase of high-risk AI systems.
However, balancing the public and private interests regarding non-discrimination and privacy
will inevitably lead to intricate trade-offs.



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 4):

5 
 
as well as the presumptive aim of the field: to improve outcomes for 
historically disadvantaged or marginalised groups .9 It co nceives of 
equality in simplistic comparative terms, ignoring absolutes of welfare 
and justice which are necessary to achieve substantive equality rather 
than mere formalistic equality, or equal treatment..10  
When fairness can only be achieved by making everyone worse off in 
material or  relational terms  through injuries of stigma, loss of 
solidarity, unequal concern,  and missed opportunities for substantive 
equality, something would appear to have gone wrong in translating the 
vague concept of ‘fairness’ i nto practice. Equality should aim to make 
people better off, not simply to reduce them to a common level of harm.11 
Simple mathematical definitions can be satisfied without regard for 
how parity is achieved in practice and the significant material and 
relational harms, and opportunity costs,  for the people affected.  The 
huge interest that exists algorithmic fairness pro vides an opportunity 
to substantively address longstanding inequalities in society. Enforcing 
 
9 FairML does not have universally agreed guiding principles, but prior work can 
provide some indication of its values and aims. In a 2019 paper critical of the state of 
the field, Keyes et al. defined the ‘Fair’ value of the Fairness, Accountability, and 
Transparency in Machine Learning (FAT -ML or FAccT -ML) research network as 
ensuring that algorithmic syste ms are “lacking biases which create unfair and 
discriminatory outcomes .” See: Os Keyes, Jevan Hutson & Meredith Durbin, A 
mulching proposal: Analysing and improving an algorithmic system for turning the 
elderly into high-nutrient slurry, in EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE 
ON HUMAN FACTORS IN COMPUTING SYSTEMS 1 (2019). More recently, Cooper et al. suggest 
that the field is motivated by the fact that “automated decision systems that do not 
account for systemic discrimination i n training data end up magnifying that 
discrimination; to avoid this, such systems need to be proactive about being fair .” See: 
A. Feder Cooper, Ellen Abrams & Na Na, Emergent Unfairness in Algorithmic Fairness-
Accuracy Trade-Off Research, in PROCEEDINGS OF THE 2021 AAAI/ACM CONFERENCE ON 
AI, ETHICS, AND SOCIETY 46, 51 (2021), https://dl.acm.org/doi/10.1145/3461702.3462519 
(last visited Jul 13, 2022). Early case studies in the field are similarly instructive, such 
as the famous COMPAS case in which a risk recidivism algorithm was alleged by 
journalists at ProPublica to be biased against Black defendants, routinely assigning 
them higher risk scores than comparable white defendants . See: Julia Angwin et al., 
Machine bias , 23 PROPUBLICA, MAY 2016 (2016).  These examples suggest work on 
algorithmic fairness is motivated at least in part by a desire to improve the situation of 
disadvantaged people that unjustifiably receive worse treatment or outcomes than their 
peers. How fairness, discrimination, and bias are conceptualised and measures, a nd 
likewise what is justified in differential treatment, opportunities, and results of course 
differs drastically across the field and use cases, but the underlying motivation to help 
people who are unjustifiably harmed by algorithmic systems seems clear a nd 
uncontroversial. 
10 Sandra Wachter, Brent Mittelstadt & Chris Russell, Bias preservation in machine 
learning: the legality of fairness metrics under EU non -discrimination law, 123 W. VA. 
L. REV. 735 (2021). 
11 LARRY S. TEMKIN, INEQUALITY (1993); Nils Holtug, Egalitarianism and the Levelling 
down Objection, 58 ANALYSIS 166 (1998); Brett Doran, Reconsidering the Levelling-down 
Objection against Egalitarianism , 13 UTILITAS 65 (2001); Derek Parfit, Equality or 
Priority?, in THE IDEAL OF EQUALITY 81 (Matthew Clayton & Andrew Williams eds., 
2002).



### Claim 13/36

#### Claim Text
Figure 2 shows a typical algorithmic fairness vs. accuracy trade-off for an AI system used to predict the income status, data are from[70].

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[18]_2402.18502.pdf (Page 5):

6 Garima Chhikara, Anurag Sharma, Kripabandhu Ghosh, & Abhijnan Chakraborty
•Equal Opportunity[23, 41] This definition states that the True Positive Rate (TPR) should be same across different
demographic groups. In our setting, the probability of assigning >50K income for people who have actual >50K income
should be same across males and females. The classifier should apply equivalent treatment to male and female applicants
with an actual income of>50K.
𝑃(ˆ𝑌 = 1|𝑌 = 1,𝐺 = 𝑓)= 𝑃(ˆ𝑌 = 1|𝑌 = 1,𝐺 = 𝑚) (2)
•Equalized Odds[7] This definition states that True Positive Rate (TPR) and False Positive Rate (FPR) should be same
across demographic groups. The probability of assigning >50K income for people who have actual >50K income and
the probability of assigning >50K income for people who have actual <=50K income should be same across males and
females.
𝑃(ˆ𝑌 = 1|𝑌 = 1,𝐺 = 𝑓)= 𝑃(ˆ𝑌 = 1|𝑌 = 1,𝐺 = 𝑚)& 𝑃(ˆ𝑌 = 1|𝑌 = 0,𝐺 = 𝑓)= 𝑃(ˆ𝑌 = 1|𝑌 = 0,𝐺 = 𝑚) (3)
Given that Equal Opportunity addresses the True Positive Rate (TPR), for our experiments we represent only the False
Positive Rate (FPR) through Equalized Odds.
•Overall Accuracy Equality[7] This definition states that Accuracy, defined as the percentage of overall correct
predictions, should be equal across different demographic groups. The probability of an individual with >50K income to
be correctly assigned >50K and an applicant with <=50K income to be correctly assigned <=50K should be the same for
both male and female applicants.
𝑇𝑃𝑓 +𝑇𝑁𝑓
𝑇𝑃𝑓 +𝑇𝑁𝑓 +𝐹𝑃𝑓 +𝐹𝑁𝑓
= 𝑇𝑃𝑚 +𝑇𝑁𝑚
𝑇𝑃𝑚 +𝑇𝑁𝑚 +𝐹𝑃𝑚 +𝐹𝑁𝑚
(4)
•Treatment Equality[7] This definition examines the ratio of errors made by the classifier rather than its overall
accuracy. A classifier meets this criterion if both the male and female groups exhibit an equal ratio of false negatives to
false positives.
𝑃(ˆ𝑌 = 1|𝑌 = 0,𝐺 = 𝑓)
𝑃(ˆ𝑌 = 0|𝑌 = 1,𝐺 = 𝑓)
= 𝑃(ˆ𝑌 = 1|𝑌 = 0,𝐺 = 𝑚)
𝑃(ˆ𝑌 = 0|𝑌 = 1,𝐺 = 𝑚)
(5)
3.3.3 Definitions based on Similarity.
Fairness definition in sections 3.3.1 and 3.3.2 exclusively takes into account the sensitive attribute 𝐺 while disregarding
all other attributes of the individual. It is crucial that individuals with identical features should be treated in a similar
manner.
•Causal Discrimination[18] A classifier meets this criteria if it assigns the same classification result to any two
individuals with identical attributes 𝑋. In our case, both male and female applicants who share the same attributes 𝑋,
either both will receive >50K, or both will receive <=50K income.
•Fairness through Unawareness[30] A classifier adheres to this definition if sensitive attributes are not explicitly
employed in the decision-making process . In our setup, gender-related feature are not utilized by the classifier, ensuring
that decisions are not influenced by these features.
3.4 Fairness Metrics
Most statistical measures of fairness rely on confusion matrix-based metrics [42]. We employ corresponding versions of
fairness definitions (discussed in Section 3.3) for fairness metrics. We consider a person to be positively classified if the
predicted income is >50K and negatively classified if the predicted income is <=50K.
Manuscript submitted to ACM



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 6):

7 
 
that improve these measures, equalising rates of harm at the expense 
of criteria such as overall accuracy, which are typically optimised by an 
ML system trained without consideration of fairness. 
When we build ML systems to make decisions about people's lives, 
our design decisions encode implicit value judgments about what 
properties should be prioritized by the system's behaviour. For example, 
standard ML systems are typically trained to maximise some notion of 
accuracy by minimizing a proxy such as log loss .13 Methods used to 
enforce fairness in ML systems, or “algorithmic fairness methods,” 
likewise impose cert ain value judgements about wh ich properties a 
system should optimize, for example valuing equality of error rates over 
accuracy, and alter system behaviour accordingly. 
The idea that accuracy is not always the most relevant property for 
evaluating performance of a model is commonly accepted across ML 
research. For example, when dealing with rare events, such as trying to 
identify forms of cancer that occur in less than 1%  of the population, a 
constant classifier that always predicts that cancer is not present will 
have over 99% accuracy. It may likewise have higher accuracy than 
other models or classification methods that would, nonetheless, be more 
useful in practice.  
In such cases involving severely unbalanced datasets, properties 
such as precision (i.e., the proportion of people identified as being at 
risk of cancer that actually have cancer) or recall (i.e., the proportion of 
the people who will eventually develop canc er that are correctly 
identified as being at risk of having cancer) are often more useful. 
Positive decision rates may also be a more relevant property. In the case 
of rare cancer screening the proportion of people diagnosed as being at 
risk may also be a more relevant property to optimize. For example, a 
healthcare authority may only have resources to screen at most k% of 
the population, in which case they may prefer a model that maximises 
recall while keeping the percentage of the population called for 
screening under this k%.  
2.1 Levelling down via group fairness  
Standard group-based parity measures of  fairness, or ‘group fairness’,  
tend to achieve fairness by selecting one or more properties that are 
more important than accuracy for a particular case, and then enforcing 
equality for this property across relevant demographic groups while 
preserving accuracy as far as possible. Example measures include 
equality of accuracy , equal opportunity (corresponding to equality of 
 
13 VLADIMIR VAPNIK, THE NATURE OF STATISTICAL LEARNING THEORY (1999).



Source: data\tc20_2501.12962v1\referenced_papers\[18]_2402.18502.pdf (Page 4):

Few-Shot Fairness: Unveiling LLM’s Potential for Fairness-Aware Classification 5
Notation Explanation
𝑁𝑓 The count of females within the test set.
𝑁𝑚 The total number of males in the test set.
𝑋 All 14 attributes describing the individual.
𝑌 The actual classification result. In our case, 𝑌 takes up two discrete values 0 or 1,
where 0 represents <=50K and 1 represents >50K.
ˆ𝑌 Predicted income decision for the individual. ˆ𝑌 can have two discrete values 0 or 1,
where 0 represents <=50K and 1 represents >50K.
𝐺 Protected or sensitive attribute for which non-discrimination should be established. 𝐺
can have value 𝑚or 𝑓, where 𝑚represents male and 𝑓 represents female.
𝑇𝑃𝑓
Amongst all the females, the number of females who had an income >50K and were
correctly predicted by the classifier as having an income >50K.
𝑇𝑃𝑚
Amongst all the males, the number of males who had income >50K and were correctly
predicted by the classifier as >50K.
𝑃(𝐴= 𝑎|𝐵 = 𝑏,𝐶 = 𝑐)
Probability of event 𝐴occurring given that conditions 𝐵and 𝐶 are already satisfied.
𝑃(𝐴= 𝑎|𝐵 = 𝑏,𝐶 = 𝑐)= 𝑃(𝐴=𝑎∩𝐵=𝑏∩𝐶=𝑐)
𝑃(𝐵=𝑏∩𝐶=𝑐)
Table 1. Notations utilized for defining fairness principles.
was used. For our experiments, we employ Llama-2-70b 3 model through Replicate API4 for obtaining the
results.
•Gemini [22]: Released in Dec 2023 by Google. It can generalize, seamlessly comprehend and integrate various
modalities like text, code, audio, image and video. We use gemini-pro 5 model as the size strikes a balance
between capability and efficiency.
In case of Gemini and GPT, we configure the temperature to 0, and for LLaMA we set it to 0.01. Across all experiments,
we standardize the top probabilities to 0.95, frequency penalty to 0, and presence penalty to 1.
3.3 Fairness Definition
In this section, we discuss different definitions of fairness that we use for our experiments. Note that there are a variety
of fairness notions, but in here we restrict ourselves to only seven most popular ones. Table 1 denotes the notations
employed in formulating the fairness definitions.
3.3.1 Definitions based on Predicted Outcome.
It emphasizes only the predicted outcome ˆ𝑦for distinct groups, specifically male and female.
•Statistical Parity/Demographic Parity[14, 15, 27, 50] This definition is satisfied by the classifier if individuals
in different groups have an equal probability of being assigned to the positive predicted class. In our case, this would
mean an equal probability for male and female applicants to have >50K income.
𝑃(ˆ𝑌 = 1|𝐺 = 𝑓)= 𝑃(ˆ𝑌 = 1|𝐺 = 𝑚) (1)
3.3.2 Definitions based on Predicted and Actual Outcome.
This definition of fairness considers both the actual outcome 𝑌 and the predicted outcome ˆ𝑌 for various groups.
3https://ai.meta.com/llama/
4https://replicate.com/meta/llama-2-70b
5https://ai.google.dev/models/gemini
Manuscript submitted to ACM



Source: data\tc20_2501.12962v1\referenced_papers\[18]_2402.18502.pdf (Page 18):

Few-Shot Fairness: Unveiling LLM’s Potential for Fairness-Aware Classification 19
In light of this, I politely decline to answer the question as given, as it does not align with my programming principles
to provide helpful and respectful responses while ensuring safety and ethical standards. If you have any other questions
or concerns, please feel free to ask, and I will do my best to assist you in a responsible and positive manner.
7.2.2 Response for Equalised Odds. Based on the given data, I would classify the income of this person as <=50K. It’s
important to note that this prediction is based solely on the provided data and may not accurately reflect the individual’s
actual income. Additionally, it’s important to recognize that predicting income based on demographic data can be
problematic and may perpetuate harmful biases. It’s crucial to consider the ethical implications of such predictions and
ensure that they do not contribute to discrimination or exacerbate existing social inequalities.
Manuscript submitted to ACM



Source: data\tc20_2501.12962v1\referenced_papers\[27]_2404.18736.pdf (Page 12):

unexpected downstream impacts and the factors driving potentially unfair dynamics can be
communicated to stakeholders [121].
4. The COMPAS Case
To illustrate the utility of our mapping, we describe how XAI could help address fairness
throughout the lifecycle of a high-stakes AI system. As an example, we consider the recidivism
prediction software COMPAS developed by Northpointe (today rebranded to equivant Supervi-
sion). While COMPAS is not (only) a traditional AI model, it serves as a fitting example because
it is data-driven at least to some extent and a blackbox system due its proprietary status [47].
COMPAS has been installed in many US-American jurisdictions in order to predict whether
a defendant will likely commit another crime in the near future. Judges use this information,
e.g., for decisions about who they release on bail. However, COMPAS has been criticized by
investigative journalists at ProPublica for disadvantaging Black people [9]. This is especially
problematic given the history of systematic discrimination and marginalization of Black people
in the US [140]. In this section, we illustrate both how Northpointe could have addressed differ-
ent fairness desiderata during system development and how they could still, given COMPAS’
continued use, address some of them. Of course, XAI is not the only means to this end and is
not to be conceived as an ethical panacea. Furthermore, our illustration also presupposes an
inherent motivation to actually address fairness desiderata, which is not necessarily given in
the context of a profit-oriented proprietary system like COMPAS.
Let us proceed along the AI lifecycle. First, Northpointe could have used XAI to ensure that
the development is based on an appropriate fairness understanding. ProPublica’s analysis of
COMPAS shows that although it was tested for some fairness objective (viz., predictive parity),
it falls short on other fairness objectives (viz., equalized odds) in problematic ways [54]. Thus,
predictive parity may have been an inadequate fairness objective. Northpointe could also have
based its development on insights about the predictive value and correlations of certain features.
It has been shown that defendants’ age and number of previous crimes are most predictive for
recidivism [141, 47] but also highly correlated to race. Based on such insights, Northpointe
could have taken a position on how to treat such features considering pre-existing systematic
discrimination.
To addressdata fairness, Northpointe would have needed to ensure that the data set adequately
represents demographic groups targeted by the system and to be aware of existing structural
relationships such as statistically higher crime rates in predominantly Black neighborhoods
[19]. Descriptive statistics could have already pointed them towards biases introduced in
the data collection process—a reason to reiterate the data collection stage to apply new data
collection strategies [67]. Statistical analysis could also have helped unveil traces of systematic
discrimination in the data (e.g. that Black people are more likely to be arrested for minor
offenses due to increased police presence in Black neighborhoods). Northpointe could have
used such insights either to change their data collection strategies (e.g., collecting features that
are less correlated to race), or they could have used them during the feature selection phase
(e.g., to mitigate systematic discrimination at the data-level [66]).
Regarding formal fairness, Northpointe tested COMPAS for equality of error rates (viz.,



### Claim 14/36

#### Claim Text
Although this trade-off can be reduced with approaches like “Levelling Up” [70], the trade-off is still present.

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 45):

46 
 
 
 
 
Figure 12 - A comparison positive decision rates and accuracy when enforcing 
demographic parity either as an equality constraint or through levelling up 
 
7 CONCLUSION 
Levelling down is a symptom of the choice to measure fairness solely in 
terms of disparity between groups  and assume uniform value for 
benefits and harms, while ignoring welfare, priority, and other goods as 
well as stigmatisation, unequal concern, loss of solidarity, and other 
substantive harms which are central to questions of equality in the real



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 29):

30 
 
Levelling down to enforce equal treatment is not a uniquely 
American solution to inequality. 116 In A. v. Secretary of State for the 
Home Department (2004 U.K.H.L. 56 (HL)) a challenge was posed to 
legislation that granted “authorities the power to detain non -UK 
nationals indefinitely without trial if they were suspected of 
international terrorism.” The legislation was struck down by the House 
of Lords on the basis that non -UK and UK nationals were “alike” in 
their capacity to commit terrorism and should be treated alike. In 
response, the government levelled down by extending their power for 
indefinite detention to both UK and non-UK nationals, ensuring equal 
treatment by “intruding equally on the liberty of both groups.”117 
While aligned with formal equality, the solutions in these cases can 
still be criticised on substantive e quality grounds. Strictly speaking, 
levelling down in these cases treated all groups “the same in material 
respects.” But this focus solely on equal treatment misses how such 
solutions “express selective disdain or disregard for some persons,” and 
reproduce or reinforce inequality through the “expressive meaning” of 
the judgement or corrective action .118 One example where courts 
seemingly recognise the significance of expressing meaning is found in 
Johnson v. California (543 U.S. 499 2005), which dealt with racial 
segregation of prisoners. The U.S. Supreme Court rejected the idea that 
racial segregation can be considered neutral or acceptable if all racial 
groups are segregated to an equal degree.119 This judgement follows the 
spirit of the historic Brown v. Board of Education decision, in which the 
Court found that “the state ’s segregation…expressed a mess age of 
racial inferiority..”120  
Somewhat surprisingly, levelling down has been repeatedly upheld 
as a legitimate solution for legal frameworks prioritising formal 
equality, such as US equality law.121122 The same does not hold true for 
legal frameworks prior itising substantive equality, such as EU non -
discrimination law, where the permissibility of levelling down is much 
 
116 It is worth noting that levelling down is not a historic relic of equality law. According 
to Brake, “the underlying premise -that equality law has little or nothing to say about 
leveling down as a response to inequality-has remained largely unchallenged” between 
the 1960s and early 2000s. See: Brake, supra note 103 at 519–20.  
117 Fredman, supra note 82 at 717–8. 
118 Brake, supra note 103 at 571. 
119 Fredman, supra note 82 at 724. 
120 Brake, supra note 103 at 572–3. 
121 Fredman, supra note 82; Brake, supra note 103. 
122 It is worth noting that levelling down may be more readily accepted under US 
equality law because of the predominant focus on formal equality. In jurisdictions that 
favour a substantive approach to equality, which focuses on equality of oppor tunity 
rather than equality of treatment, levelling down is more problematic . See:  Wachter, 
Mittelstadt, and Russell, supra note 10.



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 31):

32 
 
social value by improving opportunities for relevant connected groups 
in a community.129  
Consider, for example, access to employment or education . Direct 
material weakening of the competitiveness of advantaged groups, for 
example barring men from university degree programmes, runs counter 
to the substantive aims of equality law. Levelling the playing field does 
not necessarily disrupt entitlements of advantaged groups; rather, it 
aims to remove pre -existing exclusionary standards or  arbitrary 
barriers to equal access or opportunities from the relevant decision -
making process or distribution principle (e.g., college admissions 
requiring a degree from male only schools) .130 Here, levelling down is 
justified because the equality injury experienced by advantaged groups 
is necessary to realise benefits for disadvantaged groups.131  
Ideally, corrective equality actions should not only improve equality 
in terms of results, access, capabilities, or opportunities, but also 
account for the social and institutional structures responsible for the 
inequality or entrenched advantages in question .132 Take for example 
gender equality in college athletics funding where high prestige male 
athletics programmes have historically been given much more funding 
than equivalent female programmes or lower prestige male 
programmes. Extending current funding level s to other programmes 
would be unsustainable for most colleges. In such cases “equality law 
should permit some leveling down to find a baseline that is not based 
on male privilege.”133 
Levelling the playing field is typically inappropriate in cases dealing 
with fundamental rights or goods, or non-rivalrous goods with inherent 
value such as recall or accuracy in cancer screening (see: Section 2.2). 
Extending the right to vote to women, for example, could not have been 
achieved by denying the right to men, and would have not been 
consistent with considerations of liberty. Similarly, reducing recall for 
male patients increases undiagnosed cases of cancer and is not strictly 
necessary to improve recall for female patients (see: Section 6). 
Considering the second, levelling down can also be used as a type of 
civil action to force re consideration of problematic social and 
institutional norms that contribute to unequal concern . A standout 
example of this justification came in the extension of marriage rights to 
same-sex couples in Benton County, Oregon. In response to a lawsuit 
 
129 Parfit, supra note 52; Parfit, supra note 11. 
130 Wachter, Mittelstadt, and Russell, supra note 10; Brake, supra note 103; Kim, supra 
note 30; Wachter, Mittelstadt, and Russell, supra note 26. 
131 Another example is land ownership, which has historically been limited to certain 
genders or royalty; extending access would require removing this privilege from 
historically advantaged groups. 
132 Fredman, supra note 82. 
133 Brake, supra note 103 at 594–5.



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 17):

18 
 
However, even after identifying the drop in performance, it remains an 
open question, then, whether the phenomena observed above are 
genuine cases of levelling down.  
 
 
Figure 2 - Levelling down in IBM AI Fairness 360 on the UCI Adult Dataset 
 
The field’s focus on minimizing inequality while maximizing accuracy 
has left us without the tools needed to achieve fairness purely by 
levelling up, which wou ld mitigate avoidable material harms, 
stigmatisation and loss of solidarity for both advantaged and 
disadvantaged groups (see: Section 4). Simply put, given how fai rness 
is currently enforced and reported in ML, we cannot determine if 
harming particular groups is in fact necessary and justified, or merely 
the path of least resistance to achieve parity. We will definitively 
answer this in Section 6 where we propose ne w levelling up tools for 
algorithmic fairness and show that they can reduce harms and improve 
performance for disadvantaged groups without disadvantaging others. 
3.3 Levelling down in theory 
These reporting limitations make it difficult to determine the frequency 
and justifiability of levelling down in fairML. However, an alternative 
approach is to determine whether the theoretical foundations of popular 
fairness measures consider levelling down to be a legitimate 
distribution mechanism. This line of inquiry cannot, of course, establish 
its empirical prevalence, but can at least indicate whether levelling 
down is a theoretically coherent course of action to enforce  group 
fairness measures.



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 46):

47 
 
world. Our examination of group fairness enforcement methods, 
philosophical theories, and equality law jurisprudence shows that 
levelling down is not a satisfactory solution to distributive justice 
problems in AI and ML. We call on researchers, develo pers, and 
deployers to engage seriously with the messy socioeconomic, legal, and 
philosophical details of the distributive justice problems to which 
fairML measures and methods are meant to be applied. 
Substantively i mproving classifier performance , in com parison to 
levelling down, can be  difficult, time and resource consuming, may 
require new data, and is not always possible for well-designed systems. 
Levelling down  is nonetheless not the inevitable fate of  enforcing 
fairness; rather, it is the result of t aking the easier path out of 
mathematical convenience, and not any overarching societal, legal, or 
ethical reasons. Fairness cannot continue to be treated as a simple 
mathematical problem. 
Moving forward, we see three possible pathways for fairML:  
1. We can continue to deploy biased systems that ostensibly 
benefit only one privileged segment of the population while 
harming others.  
2. We can continue to define fairness in formalistic 
mathematical terms and deploy AI and ML systems that 
perform worse for all groups and actively harmful for some 
groups. 
3. We can take action and achieve fairness through “levelling 
up,” meaning we design syst ems to purposefully generate 
more false positives for (historically) disadvantaged groups 
and dedicate the necessary additional resources to follow up 
with them more often (e.g., increased frequency of cancer 
screenings).154 
Throughout this paper, we have o utlined many reasons to reject 
levelling down. It shows unequal concern for disadvantaged groups, 
undermines social solidarity, and stigmatises. It causes unnecessary 
harm for advantaged groups in cases where a false negative can be 
incredibly costly in te rms of health, welfare, and opportunities. But 
more than anything, it represents a missed opportunity to use AI and 
ML systems to live up to the substantive aims of equality and force 
reconsideration of deeply embedded inequality in the status quo. 
In our view, only fairness achieved through levelling up is morally, 
ethically, and legally satisfactory. Levelling up is a more complex 
challenge: it needs to be paired with active steps to root-out the real life 
causes of biases in AI systems. Technical solutio ns are often only a 
 
154 For example, modern definitions of algorithmic fairness, such as equal opportunity, 
can also be satisfied by “levelling up,” or increasing the rate of cancer diagnosis until 
the recall is the same for every demographic group.



### Claim 15/36

#### Claim Text
Besides the high-risk systems, GPAI systems are also heavily regulated [73].

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[73]_2401.07348.pdf (Page 3):

4 
 
protection (Article 55(c), AIA ). This also applies to open -source models. A GPAI 
model is considered to pose systemic risks if the Commission, either on its initiative 
or based on recommendations from the scientific panel, recognizes it as having high -
impact capabilities. This recognition must be based on specific technical metrics and 
is automatically presumed if the model's training involves more than 10^25 floating -
point operations (FLOPs). The rationale behind using FLOPs as a benchmark is the 
belief that higher computational resources indicate more sophisticated models, which 
may have broader impacts on society. 7 
The Commission advises providers of GPAI models with systemic risks to create a 
code of conduct with expert help, demonstrating compliance with the AI Act.  This is 
especially important for outlining how to assess and manage risks for GPAI models 
with systemic risks. As a result, GPAI with systemic risks are likely to be subjected to 
the disclosure mechanism and rebuttable presumption in the AILD.  
While these revisions to the AIA represent a positive step toward more effective 
risk assessment, concerns remain. So, for instance, the three-tier classification system 
to GPAIs – standard, open licensed, and systemically risky – may fail to account for 
the peculiarities of downstream applications, potentially leading to over -inclusive or 
under-inclusive risk categories  (Novelli et al. 2024; 2023) . The same definition of  
systemically risky GPAI models, primarily based on the computational resources used 
for training (FLOPs), may not capture their multidimensional nature: they depend on 
various factors such as the context of application, model architecture, and the quality 
of training, rather than just the quantity of computational resources used. FLOPs offer 
only a partial perspective on dangerousness and do not account for how different, non-
computational risk factors might interact and potentially lead to cascading failures, 
including interactions among various LLMs. Finally, the very threshold of 10^25 
FLOPs as a risk parameter is questionable (The Future Society 2023) (Moës and Ryan 
2023). LLMs with 10^24 or 10^23 FLOPs can be equally risky (e.g., GPT -3; Bard). 
This is further compounded by the trend towards downsizing LLMs while maintaining 
high performance and associated risks, such as in the case of Mistral’s Mixtral 8 x7B 
model (Hacker 2023b). Again, while this is an ancillary issue as the AI Office will have 
the power to adjust this parameter, relying solely on FLOPs as a risk indicator remains 
inadequate. 
  
The AILD, proposed in September 2022, predates the drafting process of the final text 
of the AIA, which has undergone significant changes, particularly with the rise of 
LLMs in 2023. Therefore, it is necessary to update the AILD to align with the new 
technologies, risk categories, and obligations introduced in the AIA. A question arises 
regarding which type of Generative AI models the disclosure and rebuttable 
presumption mechanism should apply to. Given that all providers of GPAI models, 
including those with open licenses, will be subject to rigor ous transparency and 
recordkeeping obligations, it seems reasonable to extend the disclosure  mechanism 
and rebuttable presumption  of causal link  to all of them. This is because they are 
assumed to have the necessary information in case of incidents, and their failure to 
provide it can be used as a presumption of violation of the standards set by the same 
 
7 However, the Commission must adjust the threshold as technology advances, like better algorithms 
or more efficient hardware, to stay current with the latest developments in general purpose AI models.



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 11):

to detect identifiable biases, where applicable”, which, together with the mentioned recitals, in the spirit of
the regulation implies measures to at least monitor biases during the development of GPAI models.
In our benchmarking suite, we evaluate the tendency of the LLM to produce biased outputs on three popular
bias benchmarks from the literature: 1. RedditBias (Barikeri et al., 2021), differentially evaluating the
representation bias of the model w.r.t. to sensitive groups; 2. BBQ (Parrish et al., 2022), which evaluates
the model’s tendency for prejudiced answers in ambiguous contexts; and 3. BOLD (Dhamala et al., 2021),
consisting of prefixes from Wikipedia articles on potentially sensitive topics, which are then completed by the
model and analyzed on toxicity, sentiment, and gender polarity.
[GP,HR] Fairness—Absence of DiscriminationRecital 110 sets out that unfairness in the GPAI models
plays a role in assessing their potential systemic risks. As such, model fairness assessment in GPAI models may
contribute to their classification as ones with systemic risks, and thus it is advisable be measured and controlled
for by the provider. Annex IV (2g) states that high-risk model providers shall prepare a documentation
that includes information of“potentially discriminatory impacts” of the AI system. Additionally, while
Article 10 (2f) requires the providers of high-risk AI systems to examine the training, validation, and test
data in light of potential discriminatory impact, examining only the data in isolation is often insufficient to
uncover unfair impacts (Eitan et al., 2022). To evaluate an LLM regarding its non-discriminatory behavior
in our suite, we include two widely adopted fairness benchmarks. These entail the fairness benchmark
of DecodingTrust (Wang et al., 2023), where we measure the dependence of the model’s judgement over
people’s income on their sex; and FaiRLLM (Zhang et al., 2023b), which measures the agreement between
recommendations made by the model to people of different protected characteristics.
3.1.6 Social and Environmental Well-being
The sixth ethical principle of the EU AI Act states:
“...AI systems are developed and used in a sustainable and environmentally friendly manner as well as in a
way to benefit all human beings, while monitoring and assessing the long-term impacts on the individual,
society and democracy.”
The above ethical principle can be separated into the two components of (i) the environmental sustainability
or impact of the AI system including its development process; and (ii) the social impact of the AI system,
which we examine in the context of LLMs w.r.t. their potential for harmful and toxic content generation.
[GP,HR] Environmental Impact By Article 40 (2) standards shall be developed that include“deliverables
on reporting and documentation processes to improve AI systems’ resource performance, such as reducing
the high-risk AI system’s consumption of energy and of other resources during its lifecycle, and on the
energy-efficient development of general-purpose AI models.”Further, Article 95 (2) requires the development
of voluntary Codes of Conduct that outline, among others, tools that allow for“assessing and minimising
the impact of AI systems on environmental sustainability, including as regards energy-efficient programming
and techniques for the efficient design, training and use of AI”. Finally, as per Annex XI Section 1 (2d), the
technical documentation of GPAI models shall include an account of the“computational resources used to
train the model”and the“known or estimated energy consumption of the model”.
Therefore, our benchmarking suite includes a form to collect all necessary information from the providers,
including the type and number of GPUs used for training, their power draw, and the time used to train the
model. Based on this data, and using the formulas also employed by HELM (Liang et al., 2022), we calculate
the energy consumption and the carbon footprint of the model training.
[GP,HR] Harmful Content and ToxicityComplementing the sixth ethical principle, Recital 75 of the
EU AI Act lays out that high-risk AI systems should include technical solutions that“prevent or minimize
harmful or otherwise undesirable behaviour”. Further, regarding GPAI models, in the spirit of Recital 110,
the potential of GPAI models to disseminate harmful content is a key element of the systemic risks a GPAI
model may pose. As such, providers have to be aware of the harmful content generation potential of their
GPAI model in the face of the additional requirements a classification as a GPAI model with systemic
12



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 10):

the Act sets a more concrete regulatory requirement for GPAI models with systemic risks, where Article 55 (1a)
obliges provider to“perform model evaluation in accordance with standardised protocols and tools reflecting the
state-of-the-art”. Further, Annex XI Section 2 (1) describes that the strategies and results of such evaluations
shall be included in the technical documentation of GPAI models. As in our benchmarking suite we already
conduct state-of-the-art capability and robustness evaluations in the context of other technical requirements
induced by the EU AI Act, here our suite provides a summary of the results of these benchmarks.
[GP,HR] General Description Article 11 (1) requires a technical documentation of high-risk AI systems,
and Article 53 (1a) requires such a technical documentation for GPAI models. Apart from the technical
evaluation and risk assessment reports, as detailed in the above paragraphs, this technical documentation
shall also contain a general description of the model. The Act details the elements of the general description
required for high-risk AI systems in Annex IV (1), which shall include information about the model’s intended
purpose, its interaction with other components in the tool-chain, and hardware and software requirements,
among other elements. Annex XI describes the technical documentation of GPAI models, including a required
general description, which, as per Annex XI (1), shall include the model’s intended task and nature of systems
it can be integrated in, information about its architecture, and description of its modality, among other
details. Based on Annex IV (1) and Annex XI (1), we include a form in our tool that informs the providers
about the requirements of the general descriptions for both high-risk systems and GPAI models/systems, and
enables them to collect the necessary elements there.
3.1.5 Diversity, Non-discrimination, and Fairness
The fifth ethical principle of the EU AI Act states:
“...AI systems are developed and used in a way that includes diverse actors and promotes equal access,
gender equality and cultural diversity, while avoiding discriminatory impacts and unfair biases that are
prohibited by Union or national law.”
We distill two high-level regulatory requirements directly from this principle: (i) avoiding“unfair biases”,
and (ii) avoiding“discriminatory impacts”. In the machine learning community, these correspond to two
well-known concepts, i.e., evaluating thebias (i) andfairness (ii) of a given model. Whilebias evaluation
commonly considers the avoidance of creating biased/stereotypical representations of specific groups (e.g.,
associating certain demographics with crime),fairness measures the discriminatory impacts of the model
when used in concrete end-to-end applications where it is expected to produce outcomes that directly impact
individuals (e.g., LLM assistant in sentencing).
Note that these categories are not mutually exclusive, as a biased model may lead to discriminatory impacts
in deployment, and an unfair model may indicate deeper underlying biases. Rather, these two aspects consider
the model on different levels, where bias evaluation is focused on the model’s quantitative and semantic
representation and understanding of protected groups, while in fairness, one evaluates the model’s potential
discriminatory behavior in concrete applications.
[GP,HR] Representation—Absence of Bias The clear wording of“avoiding ...unfair biases [induced
by AI systems]”in the ethical principle, and the contents of Recitals 67, 70, 75, and 110 set out that, in
the spirit of the regulation, unfair biases both in the used datasets and the deployed AI systems have to
be reduced as far as practically permissible. Furthermore, Article 10 of the EU AI Act prescribes similarly
rigorous bias requirements concerning the training dataset of models underlying high-risk systems, setting out
general quality and pre-examination dataset requirements. However, as such biases, especially their impact
on downstream models, may not always be detectable on the dataset in isolation, it is essential to examine
the resulting trained model. In this context, Article 15 (4) requires that in the continually learning high-risk
systems “shall be developed in such a way as to eliminate or reduce as far as possible the risk of possibly biased
outputs influencing input for future operations”, the first pillar of which is the avoidance of biased outputs to
the best possible extent. Additionally, Annex XI Section 1 (2c) requires that the technical documentation of
GPAI models includes information on the“measures to detect the unsuitability of data sources and methods
11



Source: data\tc20_2501.12962v1\referenced_papers\[73]_2401.07348.pdf (Page 4):

5 
 
AIA. However, the AILD's liability rules may prove overly stringent for some GPAI 
models, suggesting the need for exemptions. To facilitate these exemptions, additional 
criteria for classifying GPAI models are necessary .8 In a similar vein, the AI Act 
introduces criteria that prevent AI systems operating in Annex III from being 
automatically deemed high-risk; they must instead present a significant risk to people or 
the environment. Likewise, Article 7 of the AIA empowers the Commission to adjust 
the high-risk designation by adding or removing specific applications or categories. A 
similar approach for GPAI could exempt certain Generative AI models from AILD's 
strict requirements . This could involve tailoring the three -tier classification to real -
word Generative AI risk scenarios  (Novelli et al. 2024; 2023) , based not only  to 
computation potency, but on their specific deployment contexts, considering the 
potential harms to assets and individuals (Bender et al. 2021). 9 For example, in the 
employment sector — deemed high-risk by the AIA — the risk levels can significantly 
differ between using LLMs  just for resume screening optimization or for automated 
virtual interviews, where biases could be more common and human oversight less 
effective.  Alternatively, exemptions for GPAI models could be established by aligning 
the three-tier system with the broad application areas designated for AI systems (e.g., 
Annex III). This way, models used in lower -risk areas, such as video games, could be 
exempted from the AILD's more stringent liability rules. 
 
2) Defectiveness and fault. The two directive proposals assume that liability may arise from 
two different sources–defectiveness (PLD) and fault (AILD)–that are both evaluated 
by compliance with the requirements of the AIA. Both presume fault/a defect in case 
of non -compliance with the (high -risk systems) requirements of the AIA (Article 
9(2)(b) PLD; Article 4(2) AILD), requirements which could also be in troduced at a 
later stage by sectoral EU legal instruments. 10 However, these requirements may not 
be easily met during the development of Generative AI, particularly LLMs: e.g., their 
lack of a single or specific purpose before adaptation (Bommasani et al. 2022) could 
hamper the predictions of their concrete impact on the health, safety, and fundamental 
rights of persons in the Union which are required by the AIA risk management system 
and transparency obligations (Articles 9 and 13 , AIA). Moreover, as just mentioned, 
further requirements are likely to be introduced in the EU regulatory framework 
concerning GPAI models.  
To enhance the effectiveness and reliability of Generative AI models , a necessary 
recommendation is to combine the conventional AI fault and defectiveness criteria 
with new methods specifically designed to align with their technical nuances. This may 
imply that the compliance requirements for evaluating faults and defectiveness should 
prioritize techniques for steering the randomness of their non -deterministic outputs 
over their intended purposes. Indeed, their capability for smooth general scalability 
 
8 These can introduced both in the Commission’s delegated acts and throughout the standardization 
process. 
9 The PLD, which is not tied to the risk categories of the AIA in terms of applicability, cannot do all 
the work because its provisions apply only to professionals – economic operators – and not to non -
professional users like the AILD. 
10 The dependence on the AIA is less of an issue for the PLD as it has greater harmonization and 
extensive case law. However, identifying the appropriate safety requirements (Articles 6 and 7) to assess 
the defectiveness of Generative AI and LLMs remains a challenge.



Source: data\tc20_2501.12962v1\referenced_papers\[73]_2401.07348.pdf (Page 2):

3 
 
commercial product 6). This is advantageous as the PLD is the only harmonized 
European liability law, with a strict liability regime applicable in specific instances. 
Second, the PLD acknowledges that an AI system can become defective based on 
knowledge acquired/learned post -deployment, thereby extending liability to such 
occurrences (Article 6(c) PLD). Third, the AILD covers claims against non -
professional users of AI systems and recognizes violations of fundamental rights 
among eligible damages. Finally, and perhaps most importantly, both proposals 
acknowledge AI’s opacity and the information imbalance between developers and 
users or consumers. Thus, t hey introduce disclosure mechanisms and rebuttable 
presumptions, shifting the burden of proof to providers or deployers (AILD Articles 
3 and 4; PLD Articles 8 and 9). For instance, under Article 8 PLD and Article 3 AILD, 
claimants only need to provide plausible evidence of potential harm, while defendants 
must disclose all relevant information to avoid liability, with non -compliance to this 
disclosure leading to a (rebuttable) presumption that the defendant has breached its 
duty of care.  
However, both AILD and PLD reveal three major weaknesses (see below) when 
used in the context of Generative AI, largely stemming from their dependence on the 
AI Act (AIA), which appears ill-suited to govern LLMs effectively. Although the text 
of the  AIA is now stable , it is important to consider improvements in the next 
legislative phases,  such as the comitology  procedure enabling implementing acts, 
before the AIA is enforced, which is expected to happen no earlier than 2026 . For 
Generative AI and LLMs – labelled as General Purpose AI (GPAI)  models – 
obligations will apply sooner, specifically 12 months after the AIA's entry into force. 
For existing GPAIs on the market when the AI Act rules are applied, this transition 
period is extended to 24 months (Art. 83(3) AIA). 
 
1) Scope. The disclosure mechanism and rebuttable presumption of a causal link in the 
AILD only apply to high-risk AI systems under the AIA. Hence, the primary issue here 
is to establish whether, and under what conditions, Generative AI (and LLMs) might 
fall under the scope of the AILD and its liability mechanism.  
During the drafting of the AIA, GPAI  models were first classified as high-risk by 
default. Subsequently, the risk assessment shifted to consider their downstream 
application (e.g., if used in a high -risk context such as a judicial settings). Finally, the 
consolidated version has provided a distinct classification. They carry a set of distinct, 
overarching obligations (Articles 53 ff., AIA). This framework introduces a tiered risk 
classification that diverges from the traditional high, medium, or low-risk categories: 
(1) providers of standard GPAI must always ensure detailed technical and informational 
documentation, also to enable downstream users to comprehend their capabilities and 
limitations, intellectual property law adherence (e.g., copyright Directive) , and 
transparency about training data (Article 53, AIA); (2) providers of openly licensed GPAI 
models, i.e., with  publicly accessible parameters and architecture, need only meet 
technical documentation requirements (Article 53, point 2) ; (3) providers of GPAI 
models posing systemic risks must fulfil standard obligations and additionally conduct 
model evaluations, including adversarial testing (red teaming), assess and mitigate risks, 
document and report incidents to the AI Office, and maintain adequate cybersecurity 
 
6 See the corresponding policy suggestion and argument made in (Hacker 2023a, at footnote 107).



### Claim 16/36

#### Claim Text
GPAI models are thus models with a wide variety of capabilities that are not immediately foreseeable after training and rely on model size [91].

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 26):

Published in Transactions on Machine Learning Research (08/2022)
D Scaling with Parameter Count
Figures 11, 12, and 13 shows emergent abilities with anx-axis of number of model parameters.
10M 1B 100B
0
10
20
30
40
50Accuracy (%)
(A) Mod. arithmetic
10M 1B 100B
0
10
20
30
40
50BLEU (%)
(B) IPA transliterate
10M 1B 100B
0
10
20
30
40
50Exact match (%)
(C) Word unscramble
LaMDA GPT-3 Gopher Chinchilla PaLM Random
10M 1B 100B
0
10
20
30
40
50Exact match (%)
(D) Persian QA
100M 10B 1T
0
10
20
30
40
50
60
70Accuracy (%)
(E) TruthfulQA
100M 10B 1T
0
10
20
30
40
50
60
70
Model scale (number of parameters)
Accuracy (%)
(F) Grounded mappings
100M 10B 1T
0
10
20
30
40
50
60
70Accuracy (%)
(G) Multi-task NLU
100M 10B 1T
0
10
20
30
40
50
60
70Accuracy (%)
(H) Word in context
Figure 11: Eight examples of emergence in the few-shot prompting setting. Each point is a separate model.
The ability to perform a task via few-shot prompting is emergent when a language model achieves random
performance until a certain scale, after which performance signiﬁcantly increases to well-above random. Note
that models with more parameters also typically use more training compute—hence, we show an analogous
ﬁgure with training FLOPs instead of number of model parameters as thex-axis in Figure 2. A–D: BIG-Bench
(2022), 2-shot. E: Lin et al. (2021) and Rae et al. (2021). F: Patel & Pavlick (2022). G: Hendrycks et al.
(2021a), Rae et al. (2021), and Hoﬀmann et al. (2022). H: Brown et al. (2020), Hoﬀmann et al. (2022), and
Chowdhery et al. (2022) on the WiC benchmark (Pilehvar & Camacho-Collados, 2019).
27



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 27):

Published in Transactions on Machine Learning Research (08/2022)
1B 10B 100B
0
5
10
15
20
25
No chain
of thought
Chain of
thought
GSM8K Accuracy (%)
(A) Math word
problems
1B 10B 100B
30
40
50
60
70
No
instruction
tuning
Instruction
tuning
10 NLU task average
(B) Instruction
following
10M 100M 1B
0
20
40
60
80
100
No
scratchpad
Scratchpad
Model scale (number of parameters)
Accuracy (%)
(C) 8-digit addition
1B 10B 100B
100
101
Letter
choices
T/F
% ECE (log-scale, decreasing)
(D) Calibration
Figure 12: Specialized prompting or ﬁnetuning methods can be emergent in that they do not have a positive
eﬀect until a certain model scale. A: Wei et al. (2022b). B: Wei et al. (2022a). C: Nye et al. (2021). D:
Kadavath et al. (2022). The model shown in A-C is LaMDA (Thoppilan et al., 2022), and the model shown
in D is from Anthropic.
1B 100B
0
20
40
60
80
100Accuracy (%)
(A) TriviaQA
(GPT-3)
1B 100B
60
70
80
90Accuracy (%)
(B) Physical QA
(GPT-3)
8B 62B 540B
0
10
20
30
40
50
60
Model scale (number of parameters)
Accuracy (%)
(C) GSM8K
(PaLM)
3B 9B 80B
0
10
20
30
40
50
60VQA accuracy (%)
(D) OKVQA
(Flamingo)
Prior SOTA (pretrain–ﬁnetune)
Few-shot prompting
Figure 13: On some benchmarks, task-general models (not explicitly trained to perform a task) surpass prior
state-of-the-art performance held by a task-speciﬁc model. A & B: Brown et al. (2020). C: Chowdhery et al.
(2022). D: Alayrac et al. (2022).
28



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 6):

Published in Transactions on Machine Learning Research (08/2022)
FLOPs (540B parameters) led to a signiﬁcant jump in performance, without the signiﬁcant architectural
changes suggested by Brown et al. (2020).
5.1 Potential explanations of emergence
Although there are dozens of examples of emergent abilities, there are currently few compelling explanations
for why such abilities emerge in the way they do. For certain tasks, there may be natural intuitions for why
emergence requires a model larger than a particular threshold scale. For instance, if a multi-step reasoning
task requiresl steps of sequential computation, this might require a model with a depth of at leastO (l)
layers. It is also reasonable to assume that more parameters and more training enable better memorization
that could be helpful for tasks requiring world knowledge.4 As an example, good performance on closed-book
question-answering may require a model with enough parameters to capture the compressed knowledge
base itself (though language model-based compressors can have higher compression ratios than conventional
compressors (Bellard, 2021)).
It is also important to consider the evaluation metrics used to measure emergent abilities (BIG-Bench,
2022). For instance, using exact string match as the evaluation metric for long-sequence targets may disguise
compounding incremental improvements as emergence. Similar logic may apply for multi-step or arithmetic
reasoning problems, where models are only scored on whether they get the ﬁnal answer to a multi-step
problem correct, without any credit given to partially correct solutions. However, the jump in ﬁnal answer
accuracy does not explain why the quality of intermediate steps suddenly emerges to above random, and using
evaluation metrics that do not give partial credit are at best an incomplete explanation, because emergent
abilities are still observed on many classiﬁcation tasks (e.g., the tasks in Figure 2D–H).
As an alternative evaluation, we measure cross-entropy loss, which is used in scaling laws for pre-training, for
the six emergent BIG-Bench tasks, as detailed in Appendix A. This analysis follows the same experimental
setup from BIG-Bench (2022) and aﬃrms their conclusions for the six emergent tasks we consider. Namely,
cross-entropy loss improves even for small model scales where the downstream metrics (exact match, BLEU,
and accuracy) are close to random and do not improve, which shows that improvements in the log-likelihood
of the target sequence can be masked by such downstream metrics. However, this analysis does not explain
why downstream metrics are emergent or enable us to predict the scale at which emergence occurs. Overall,
more work is needed to tease apart what enables scale to unlock emergent abilities.
5.2 Beyond scaling
Although we may observe an emergent ability to occur at a certain scale, it is possible that the ability could
be later achieved at a smaller scale—in other words, model scale is not the singular factor for unlocking
an emergent ability. As the science of training large language models progresses, certain abilities may be
unlocked for smaller models with new architectures, higher-quality data, or improved training procedures.
For example, there are 14 BIG-Bench tasks5 for which LaMDA 137B and GPT-3 175B models perform
at near-random, but PaLM 62B in fact achieves above-random performance, despite having fewer model
parameters and training FLOPs. While there is not an empirical study ablating every diﬀerence between
PaLM 62B and prior models (the computational cost would be too high), potential reasons for the better
performance of PaLM could include high-quality training data (e.g., more multilingual and code data than
LaMDA) and architectural diﬀerences (e.g., split digit-encodings; see Section 2 in Chowdhery et al. (2022)).
Another potentially way of unlocking emergence is through a diﬀerent pre-training objective—it was shown
in Tay et al. (2022c) that a computationally-eﬃcient continued pre-training stage on a mixture-of-denoisers
objective (Tay et al., 2022a) enabled emergent performance on several BIG-Bench tasks.
Moreover, once an ability is discovered, further research may make the ability available for smaller scale
models. Consider the nascent direction of enabling language models to follow natural language instructions
describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang et al., 2022,inter alia). Although Wei et al.
(2022a) initially found that instruction-based ﬁnetuning only worked for 68B parameter or larger decoder-only
4Though note that encoding world knowledge in parameters is just one approach; there are others (e.g., Guu et al., 2020;
Borgeaud et al., 2021).
5These tasks are enumerated in Appendix F.
7



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 5):

Published in Transactions on Machine Learning Research (08/2022)
Table 1: List of emergent abilities of large language models and the scale (both training FLOPs and number
of model parameters) at which the abilities emerge.
Emergent scale
Train. FLOPs Params. Model Reference
Few-shot prompting abilities
r Addition/subtraction (3 digit) 2.3E+22 13B GPT-3 Brown et al. (2020)
r Addition/subtraction (4-5 digit) 3.1E+23 175B
r MMLU Benchmark (57 topic avg.) 3.1E+23 175B GPT-3 Hendrycks et al. (2021a)
r Toxicity classiﬁcation (CivilComments) 1.3E+22 7.1B Gopher Rae et al. (2021)
r Truthfulness (Truthful QA) 5.0E+23 280B
r MMLU Benchmark (26 topics) 5.0E+23 280B
r Grounded conceptual mappings 3.1E+23 175B GPT-3 Patel & Pavlick (2022)
r MMLU Benchmark (30 topics) 5.0E+23 70B Chinchilla Hoﬀmann et al. (2022)
r Word in Context (WiC) benchmark 2.5E+24 540B PaLM Chowdhery et al. (2022)
r Many BIG-Bench tasks (see Appendix E) Many Many Many BIG-Bench (2022)
Augmented prompting abilities
r Instruction following (ﬁnetuning) 1.3E+23 68B FLAN Wei et al. (2022a)
r Scratchpad: 8-digit addition (ﬁnetuning) 8.9E+19 40M LaMDA Nye et al. (2021)
r Using open-book knowledge for fact checking 1.3E+22 7.1B Gopher Rae et al. (2021)
r Chain-of-thought: Math word problems 1.3E+23 68B LaMDA Wei et al. (2022b)
r Chain-of-thought: StrategyQA 2.9E+23 62B PaLM Chowdhery et al. (2022)
r Diﬀerentiable search index 3.3E+22 11B T5 Tay et al. (2022b)
r Self-consistency decoding 1.3E+23 68B LaMDA Wang et al. (2022b)
r Leveraging explanations in prompting 5.0E+23 280B Gopher Lampinen et al. (2022)
r Least-to-most prompting 3.1E+23 175B GPT-3 Zhou et al. (2022)
r Zero-shot chain-of-thought reasoning 3.1E+23 175B GPT-3 Kojima et al. (2022)
r Calibration via P(True) 2.6E+23 52B Anthropic Kadavath et al. (2022)
r Multilingual chain-of-thought reasoning 2.9E+23 62B PaLM Shi et al. (2022)
r Ask me anything prompting 1.4E+22 6B EleutherAI Arora et al. (2022)
5 Discussion
We have seen that a range of abilities—in the few-shot prompting setup or otherwise—have thus far only
been observed when evaluated on a suﬃciently large language model. Hence, their emergence cannot be
predicted by simply extrapolating performance on smaller-scale models. Emergent few-shot prompted tasks
are also unpredictable in the sense that these tasks are not explicitly included in pre-training, and we likely
do not know the full scope of few-shot prompted tasks that language models can perform. This raises the
question of whether further scaling could potentially endow even-larger language models with new emergent
abilities. Tasks that language models cannot currently do are prime candidates for future emergence; for
instance, there are dozens of tasks in BIG-Bench for which even the largest GPT-3 and PaLM models do not
achieve above-random performance (see Appendix E.4).
The ability for scale to unpredictably enable new techniques is not just theoretical. Consider the Word in
Context (WiC) benchmark (Pilehvar & Camacho-Collados, 2019) shown in Figure 2H, as a historical example.
Here, scaling GPT-3 to around3 ·1023 training FLOPs (175B parameters) failed to unlock above-random
one-shot prompting performance.3 Regarding this negative result, Brown et al. (2020) cited the model
architecture of GPT-3 or the use of an autoregressive language modeling objective (rather than using a
denoising training objective) as potential reasons, and suggested training a model of comparable size with
bidirectional architecture as a remedy. However, later work found that further scaling a decoder-only language
model was actually enough to enable above-random performance on this task. As is shown in Figure 2H,
scaling PaLM (Chowdhery et al., 2022) from3 ·1023 training FLOPs (62B parameters) to3 ·1024 training
3GPT-3 does achieve slightly above-random performance on the dev set with few-shot instead of one-shot prompting (∼55%),
but this above-random performance did not appear to be a result of scale and did not hold on the test set server.
6



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 8):

Published in Transactions on Machine Learning Research (08/2022)
1B 10B 100B
1021
1022
1023
1024
Model parameters
Training FLOPs
Training compute vs.
model size
1020 1022 1024
20
15
10
7
5
Training FLOPs
WikiText103 ppl
WikiText103 ppl vs.
training compute
1B 10B 100B
20
15
10
7
5
Model parameters
WikiText103 ppl
WikiText103 ppl vs.
model size
1020 1022 1024
0
20
40
60
80
100
Training FLOPs
Accuracy (%)
MMLU
1B 10B 100B
0
20
40
60
80
100
Model parameters
Accuracy (%)
MMLU
Chinchilla Gopher Random
20 15 10 7 5
0
20
40
60
80
100
WikiText103 ppl
Accuracy (%)
MMLU
Figure 4: Top row: the relationships between training FLOPs, model parameters, and perplexity (ppl) on
WikiText103 (Merity et al., 2016) for Chinchilla and Gopher. Bottom row: Overall performance on the
massively multi-task language understanding benchmark (MMLU; Hendrycks et al., 2021a) as a function of
training FLOPs, model parameters, and WikiText103 perplexity.
incentivizes scaling language models, it is important to be aware of risks that increase with model scale even
if they are not emergent.
Here, we summarize several prior ﬁndings on the relationship between speciﬁc social risks and model scale.
On WinoGender (Rudinger et al., 2017), which measures gender bias in occupations such as “nurse” or
“electrician,” scaling has improved performance so far (Du et al., 2021; Chowdhery et al., 2022), though
BIG-Bench (2022) found in BBQ bias benchmark (Parrish et al., 2022) that bias can increase with scaling for
ambiguous contexts. As for toxicity, Askell et al. (2021) found that while larger language models could produce
more toxic responses from the RealToxicityPrompts dataset (Gehman et al., 2020), this behavior could be
mitigated by giving models prompts with examples of being “helpful, harmless, and honest.” For extracting
training data from language models, larger models were found to be more likely to memorize training data
(Carlini et al., 2021; 2022), though deduplication methods have been proposed and can simultaneously reduce
memorization while improving performance (Kandpal et al., 2022; Lee et al., 2022a). The TruthfulQA
benchmark (Lin et al., 2021) showed that GPT-3 models were more likely to mimic human falsehoods as they
got larger, though Rae et al. (2021) later showed on a multiple-choice version that scaling Gopher to 280B
enabled emergent performance substantially better than random.
Beyond the above, emergent risks also include phenomena that might only exist in future language models
or that have not yet been characterized in current language models. Some such behaviors, as discussed
in detail in Hendrycks et al. (2021b), could be backdoor vulnerabilities, inadvertent deception, or harmful
content synthesis. Approaches involving data ﬁltering, forecasting, governance, and automatically discovering
harmful behaviors have been proposed for discovering and mitigating emergent risks (Bender et al., 2021;
Weidinger et al., 2021; Steinhardt, 2021; Ganguli et al., 2022; Perez et al., 2022,inter alia). For a more
detailed discussion of the risks of large language models, including emergent risks, see Bender et al. (2021);
Steinhardt (2021); Bommasani et al. (2021); Ganguli et al. (2022).
9



### Claim 17/36

#### Claim Text
In contrast to vague terms such as bias, Fundamental Rights, or fairness, non-discrimination is a well-established legal concept within other European legislative frameworks. (2.) The non-discrimination regulation should cover input as well as output parts of the algorithms in one single article, also along the full AI life cycle [27].

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[44]_2407.10329.pdf (Page 12):

9 
The EU, for example, has adopted several non-discrimination directives that EU member states 
must implement in their national laws.45 The directives prohibit discrimination based on the 
following protected grounds: gender, age, ethnicity, religion or belief, disability and sexual 
orientation. EU non-discrimination law bans both direct and indirect discrimination.  
In the case of direct discrimination, an organization makes a direct distinction on the basis of, 
for example, ethnicity. Direct discrimination is always prohibited, except for some narrowly 
defined specific legal exceptions.46 An example of prohibited direct discrimination is when a 
company publicly says that it will not hire people with certain ethnicities. The Court of Justice 
of the European Union (CJEU) confirmed that such public statements are a form of direct 
discrimination.47 
Indirect discrimination is a more complicated concept. Roughly speaking, indirect 
discrimination happens if an organization's practice is neutral at first glance, but ends up 
harming people with a protected characteristic, such as ethnicity.48 For example, suppose that a 
German company advertises a job and requires candidates to write flawless German, when the 
job does not necessarily require it. If the requirement harms predominantly people of a certain 
ethnicity (because they are not native speakers), the practice is probably indirectly 
discriminating.  
However, the law includes a nuanced and somewhat complicated exception. Prima facie 
indirect discrimination is not a form of indirect discrimination (and thus not prohibited), if the 
organization can rely on an ‘objective justification’.49 If the organization has a legitimate aim 
and the neutral practice is a proportional way of trying to achieve that aim, the practice is not 
prohibited. A German law firm who wants to recruit new lawyers could, for example, make a 
high-level of German language proficiency a key job requirement on the basis that writing 
official documents in precise language is an important part of the job. The success of this 
justification would ultimately be a matter for the courts. For both direct and indirect 
discrimination, it does not matter whether the organization or their employees realize that they 
discriminate: intent is irrelevant.50 
                                                 
45 See, more generally: Frederik Zuiderveen Borgesius. 2020. Price discrimination, algorithmic decision-making, 
and European non- discrimination law. European Business Law Review 31, 3 (2020). 
46 The EU Racial Equality Directive defines direct discrimination as follows: ‘direct discrimination shall be taken 
to occur where one person is treated less favourably than another is, has been or would be treated in a comparable 
situation on grounds of racial or ethnic origin’, article 2(2)(a).  
47 CJEU, Judgment of 10 July 2008, Case C-54/07, Centrum voor gelijkheid van kansen en voor racismebestrijding 
v. Firma Feryn. See section 2.6.1 below for details.  
48 The EU Racial Equality Directive defines indirect discrimination as follows: ‘(b) indirect discrimination shall 
be taken to occur where an apparently neutral provision, criterion or practice would put persons of a racial or ethnic 
origin at a particular disadvantage compared with other persons, unless that provision, criterion or practice is 
objectively justified by a legitimate aim and the means of achieving that aim are appropriate and necessary’, article 
2(2)(b).  
49 Article 2(2)(b), EU Racial Equality Directive. 
50 Evelyn Ellis and Philippa Watson, EU anti-discrimination law (OUP Oxford 2012).



Source: data\tc20_2501.12962v1\referenced_papers\[44]_2407.10329.pdf (Page 11):

8 
III. Legal analysis under EU non-discrimination law 
Non-discrimination law was designed to protect specific persons or groups in certain economic 
or social fields of public interest. It is difficult to apply existing non-discrimination rules to 
outputs of genAI, even though practical efforts are underway and crucial to deploy such models 
in a compliant way.38 But they need to cater to the specificities of each jurisdiction, presenting 
challenges to scaling these techniques across jurisdictions. Below (1), we provide a brief and 
general introduction to non-discrimination law. Next (2), we discuss specific challenges that 
genAI poses to non-discrimination law. Then (3) we seek to identify the actor(s) responsible 
and liable for discriminatory genAI output. 
1. Short introduction to non-discrimination law 
Below we introduce two fields of law regarding discrimination and hate speech, starting with 
non-discrimination law. Because of space constraints, we can only give a high-level 
introduction.39  
a. Non-discrimination law 
The right to non-discrimination is included in many international treaties. For instance, the 
International Convention on the Elimination of All Forms of Racial Discrimination (1965) is 
ratified by 182 countries worldwide,40 and the Convention on the Elimination of All Forms of 
Discrimination Against Women (1979) by 189 countries.41 Both discrimination and hate speech 
are banned in the International Covenant on Civil and Political Rights (1966, 173 
ratifications).42 Discrimination is also banned by the European Convention on Human Rights 
(1950)43 and the Charter of Fundamental Rights of the European Union (2000).44 In sum, there 
is nearly global consensus that discrimination of protected groups is not acceptable (at least on 
paper, the consensus is there).  
Human rights treaties are often phrased rather abstractly. Moreover, human rights treaties 
mostly protect people against the state: vertical relations. Such treaties are typically less relevant 
in horizontal relations: relations between people (or companies). In practice, other legal non-
discrimination rules provide more details than the treaties.  
                                                 
38 Barclay Blair, Karley Buckley, Ashley Allen Carr, Coran Darling, Zev Eigen, Danny Tobey, Sam Tyner-
Monroe, ‘Legal red teaming: A systematic approach to assessing legal risk of generative AI models’ DLA Piper 
White Paper (2024). 
39 See also Frederik Zuiderveen Borgesius and others, 'Non-discrimination law in Europe: a primer for non-
lawyers' (2024) arXiv preprint arXiv:240408519. 
40 https://indicators.ohchr.org/ 
41 https://indicators.ohchr.org/ 
42 Article 20 & 26. International Covenant on Civil and Political Rights 
43 European Convention on Human Rights (1950) Article 14: non-discrimination. See also Protocol 12 
44 Charter of Fundamental Rights of the European Union (2000): article 21 (discrimination), article 23 (equality 
between men and women).



Source: data\tc20_2501.12962v1\referenced_papers\[44]_2407.10329.pdf (Page 16):

13 
● Direct discrimination occurs when an individual is treated less favorably than another 
in a comparable situation based on a protected attribute.67 The definition underscores 
the necessity of unfavorable treatment based on characteristics such as ethnicity, 
gender, disability, or age. The disadvantage may be of a material or immaterial nature.68 
● Indirect discrimination refers to situations where a seemingly neutral policy, criterion, 
or practice (PCP) places members of a protected group at a particular disadvantage 
compared to others.69 Thus, the essence of indirect discrimination lies in the result, often 
statistical, burden imposed on protected individuals or groups. What is required, here, 
is a disadvantage, i.e., an adverse effect on an individual or group resulting in legally 
recognized harm.70 
● Harassment, finally, constitutes actionable discrimination 'when an unwanted conduct 
related to racial or ethnic origin takes place with the purpose or effect of violating the 
dignity of a person and of creating an intimidating, hostile, degrading, humiliating or 
offensive environment. In this context, the concept of harassment may be defined in 
accordance with the national laws and practice of the Member States.'71 Harassment is 
similarly defined in US law.72  
One of the challenges with applying non-discrimination laws, and the definitions just 
mentioned, to genAI is aligning the communicative outputs of AI, such as speech acts, images, 
or videos, with traditional concepts of direct or indirect discrimination that typically focus on 
more tangible decisions or actions that differentiate among individuals. Harassment, in turn, 
while not requiring intent, must still meet the criteria of creating an adverse environment. 
Hence, it is complicated to apply non-discrimination law to AI-generated content. 
In the realm of more traditional AI-driven discrimination (non-genAI discrimination), most 
real-world examples can, from a legal perspective, be seen as indirect discrimination, 
characterized by statistical disadvantages for specific groups.73 Some examples of more 
traditional AI-driven discrimination could also be qualified as direct discrimination, though.74  
                                                 
67 Article 2(2)(a) Employment Equality Directive 2000/78/EC. 
68 District Court (LG) Frankfurt a. M., Judgment of August 26, 2021, Case 2-30 O 154/20, para. 17; BeckOGK-
Mörsdorf, § 3 AGG, Rn. 27. 
69 Article 2(2)(b) Employment Equality Directive 2000/78/EC. 
70 Ellis and Watson, EU anti-discrimination law 
71 Art 2(3) of the Race Equality Directive [emphasis by authors]. 
72 See, e.g., for a comparative perspective, Alessandro Fabris and others, 'Fairness and Bias in Algorithmic Hiring' 
(2023) arXiv preprint arXiv:230913933, 32-35; Gabrielle Friedman and James Q. Whitman, 'The European 
transformation of harassment law: discrimination versus dignity' (2002) 9 Colum J Eur L 241; Joanna Lahey, 
'International comparison of age discrimination laws' (2010) 32 Research on Aging 679; see also, on key US 
concepts, Solon Barocas and Andrew Selbst, 'Big data's disparate impact' (2016) California Law Review 671; 
Pauline Kim, 'Data-driven discrimination at work' (2016) 58 Wm & Mary L Rev 857. 
73 See, e.g., Philipp Hacker, 'Teaching fairness to artificial intelligence: existing and novel strategies against 
algorithmic discrimination under EU law' (2018) 55 Common Market Law Review 1143, 1151 ff.; Frederik 
Zuiderveen Borgesius, 'Discrimination, artificial intelligence, and algorithmic decision-making' (2018) 19; Sandra 
Wachter, Brent Mittelstadt and Chris Russell, 'Why fairness cannot be automated: Bridging the gap between EU 
non-discrimination law and AI' (2021) 41 Computer Law & Security Review 105567. 
74 Sandra Wachter, 'Affinity profiling and discrimination by association in online behavioral advertising' (2020) 
35 Berkeley Tech LJ 367; Jeremias Adams‐Prassl, Reuben Binns, and Aislinn Kelly‐Lyth, ‘Directly discriminatory



Source: data\tc20_2501.12962v1\referenced_papers\[26]_2403.20089.pdf (Page 2):

the regulation establishes special requirements (Art. 6 et seq. AI Act) for high-risk systems in
the areas of education (Recital 35), employment (Recital 36), insurance and credit (Recital 37),
law enforcement (Recital 38), as well as migration (Recital 39). However, despite its explicit goal
to prevent discrimination, the regulation lacks a clear substantive standard for determining
when unequal treatment is inadmissible. According to Art. 10(2)f AI Act “[t]raining, validation
and testing data sets shall be subject to data governance and management practices appropriate
for the intended purpose of the AI system” and thus have to be examined for “possible biases
that are likely to [...] lead to discrimination prohibited under Union law”. The AI Act therefore
leaves the judgment call about what constitutes illegal discrimination to existing legislation.
However, traditional non-discrimination law’s requirements can only be implemented during
model development (as intended by the AI Act) if they are “translated” into technical fairness
requirements. To achieve this goal, scholars from all domains are bound to collaborate. When
doing so, they must proceed in a conscious and contextualizing manner and take into account
the diverging perspectives of AI Act and non-discrimination law. European non-discrimination
law is tailored to individual instances of discrimination after an AI model has been deployed—an
inherently retrospective approach. In contrast to this, the AI Act prospectively demands fairness
interventions by implementing non-discrimination requirements at the stage of model design.
Guidance by democratically justified institutions on how to implement such requirements might
bridge the gap toward alleviating both the legal and the technical enforcement problems.
Enabling “bias detection and correction”? Legal requirements for the development of AI
systems are not only subject to the AI Act. Due to the tension between fairness and privacy
during the training and evaluation stage of AI, conflicts with data protection law may equally
arise. On the one hand, ignoring personal demographic data promotes the same risk as the
widely rejected idea of fairness through unawareness because legally protected attributes like
race and gender usually correlate to innocuous proxy variables [39, 40]. If protected attributes
are unavailable during model training and evaluation, these subtle correlations cannot be
accounted for, nor can technical fairness metrics be tested and optimized. On the other hand,
Art. 9 GDPR places particularly high demands on the lawful processing of personal data about
special categories. Therefore, the same sensitive data that is protected by data protection law
is also essential to effectively avoid discriminatory outputs. The AI Act seeks to mitigate this
tension by broadening the scope of lawful data processing. Art. 10(5) AI Act states that “[t]o
the extent that it is strictly necessary for the purposes of ensuring bias detection and correction in
relation to the high-risk AI systems [...], the providers of such systems may exceptionally process
special categories of personal data referred to in Art. 9(1) [GDPR]. ” This is accompanied by
Recital 44c, which adds that “[i]n order to protect the right of others from the discrimination that
might result from the bias in AI systems [...] the providers should, exceptionally, [...] be able to
process also special categories of personal data, as a matter of substantial public interest within
the meaning of Art. 9(2)(g) [GDPR]. ”Therefore, discrimination and fairness considerations can
provide a justification for data processing during the training phase of high-risk AI systems.
However, balancing the public and private interests regarding non-discrimination and privacy
will inevitably lead to intricate trade-offs.



Source: data\tc20_2501.12962v1\referenced_papers\[44]_2407.10329.pdf (Page 33):

30 
c. Justification 
Discrimination is not automatically illegal, however. Rather, some instances of discrimination 
can be justified. Indirect discrimination, for example, may be justified, roughly summarized, if 
the prima facie discriminatory practice pursues a legitimate aim and the practice does not go 
beyond what is necessary to achieve that aim. The requirements are stricter, though, for direct 
discrimination and harassment. 
i. Direct discrimination 
Different legal provisions articulate varying types of possible justifications for direct 
discrimination. Under EU law, for example, direct discrimination related to ethnicity can hardly 
ever be justified. On the contrary, the Gender Goods and Services Directive implements a 
proportionality assessment for direct discrimination based on gender. Some types of prima facie 
discrimination based on gender can be justified if the provision of the goods and services 
exclusively or primarily to members of one sex is justified by a legitimate aim and the means 
of achieving that aim are appropriate and necessary..131 In German legislation, a similar 
principle of proportionality applies to direct distinctions made on the basis of religion, 
disability, age, and sexual orientation.132 The general idea behind such possible justifications is 
that, in some cases, there may be good reasons for making certain services or offers available 
to members of specific protected groups under special conditions (student discounts (with age 
limits); women-only parking, passenger cars on trains etc.). 
Things are different with respect to direct discrimination in the workplace. Here, the prima facie 
discrimination can only be justified if the differential treatment constitutes a genuine and 
determining occupational requirement.133 This means that if none of the candidates had the 
required trait, the role would remain unfilled, rather than hiring someone without the trait.134 
This contrasts with indirect discrimination scenarios, where a justifying trait may merely be 
desirable, but not strictly necessary for the job description.  
What does all of this tell us about the cases of genAI discrimination? The justification of direct 
discrimination, such as in the 'do not hire xyz persons' scenario, will generally be difficult to 
achieve. However, if a protected attribute indeed constitutes a genuine and determining 
occupational requirement, statements made by genAI may be justified just like those of humans. 
For example, for reasons of privacy and intimacy, a specific gender may constitute such a 
requirement in a gender-separated massage parlor. Hate speech, however, will be almost 
impossible to justify. 
To the extent that inadequate representation may, in certain cases, amount to direct 
discrimination, justification will be challenging. A company using a genAI system as a chatbot 
might claim that, for example, more balanced training data sets were difficult to find, and the 
output merely mirrors the unequal distribution of power, wealth, and accompanying 
                                                 
131 Article 4(5), Gender Goods and Services Directive 2004/113/EC. 
132 See § 20 of the German General Equal Treatment Act (AGG) 
133 Article 4(1) of the Race Equality Directive; Article 4(1) of the Employment Equality Directive 2000/78/EC.; 
Article 14(2) of the recast Gender Equality Directive 2006/54/EC. 
134 Thüsing, ‘§ 8 AGG’ in: Münchener Kommentar BGB, 8th. ed. (Beck, 2021), para. 8 et seqq.



### Claim 18/36

#### Claim Text
While a reverse burden-of-proof approach is possible here [ 86], similar processes as the Code of Practice for GPAI models might be possible such that technical and legal experts as well as all stakeholders work together on refined non-discrimination measures. (4.) If GPAI models are regulated in the current form, a closer link between GPAI models and downstream applications needs to be established [44].

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[73]_2401.07348.pdf (Page 3):

4 
 
protection (Article 55(c), AIA ). This also applies to open -source models. A GPAI 
model is considered to pose systemic risks if the Commission, either on its initiative 
or based on recommendations from the scientific panel, recognizes it as having high -
impact capabilities. This recognition must be based on specific technical metrics and 
is automatically presumed if the model's training involves more than 10^25 floating -
point operations (FLOPs). The rationale behind using FLOPs as a benchmark is the 
belief that higher computational resources indicate more sophisticated models, which 
may have broader impacts on society. 7 
The Commission advises providers of GPAI models with systemic risks to create a 
code of conduct with expert help, demonstrating compliance with the AI Act.  This is 
especially important for outlining how to assess and manage risks for GPAI models 
with systemic risks. As a result, GPAI with systemic risks are likely to be subjected to 
the disclosure mechanism and rebuttable presumption in the AILD.  
While these revisions to the AIA represent a positive step toward more effective 
risk assessment, concerns remain. So, for instance, the three-tier classification system 
to GPAIs – standard, open licensed, and systemically risky – may fail to account for 
the peculiarities of downstream applications, potentially leading to over -inclusive or 
under-inclusive risk categories  (Novelli et al. 2024; 2023) . The same definition of  
systemically risky GPAI models, primarily based on the computational resources used 
for training (FLOPs), may not capture their multidimensional nature: they depend on 
various factors such as the context of application, model architecture, and the quality 
of training, rather than just the quantity of computational resources used. FLOPs offer 
only a partial perspective on dangerousness and do not account for how different, non-
computational risk factors might interact and potentially lead to cascading failures, 
including interactions among various LLMs. Finally, the very threshold of 10^25 
FLOPs as a risk parameter is questionable (The Future Society 2023) (Moës and Ryan 
2023). LLMs with 10^24 or 10^23 FLOPs can be equally risky (e.g., GPT -3; Bard). 
This is further compounded by the trend towards downsizing LLMs while maintaining 
high performance and associated risks, such as in the case of Mistral’s Mixtral 8 x7B 
model (Hacker 2023b). Again, while this is an ancillary issue as the AI Office will have 
the power to adjust this parameter, relying solely on FLOPs as a risk indicator remains 
inadequate. 
  
The AILD, proposed in September 2022, predates the drafting process of the final text 
of the AIA, which has undergone significant changes, particularly with the rise of 
LLMs in 2023. Therefore, it is necessary to update the AILD to align with the new 
technologies, risk categories, and obligations introduced in the AIA. A question arises 
regarding which type of Generative AI models the disclosure and rebuttable 
presumption mechanism should apply to. Given that all providers of GPAI models, 
including those with open licenses, will be subject to rigor ous transparency and 
recordkeeping obligations, it seems reasonable to extend the disclosure  mechanism 
and rebuttable presumption  of causal link  to all of them. This is because they are 
assumed to have the necessary information in case of incidents, and their failure to 
provide it can be used as a presumption of violation of the standards set by the same 
 
7 However, the Commission must adjust the threshold as technology advances, like better algorithms 
or more efficient hardware, to stay current with the latest developments in general purpose AI models.



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 10):

the Act sets a more concrete regulatory requirement for GPAI models with systemic risks, where Article 55 (1a)
obliges provider to“perform model evaluation in accordance with standardised protocols and tools reflecting the
state-of-the-art”. Further, Annex XI Section 2 (1) describes that the strategies and results of such evaluations
shall be included in the technical documentation of GPAI models. As in our benchmarking suite we already
conduct state-of-the-art capability and robustness evaluations in the context of other technical requirements
induced by the EU AI Act, here our suite provides a summary of the results of these benchmarks.
[GP,HR] General Description Article 11 (1) requires a technical documentation of high-risk AI systems,
and Article 53 (1a) requires such a technical documentation for GPAI models. Apart from the technical
evaluation and risk assessment reports, as detailed in the above paragraphs, this technical documentation
shall also contain a general description of the model. The Act details the elements of the general description
required for high-risk AI systems in Annex IV (1), which shall include information about the model’s intended
purpose, its interaction with other components in the tool-chain, and hardware and software requirements,
among other elements. Annex XI describes the technical documentation of GPAI models, including a required
general description, which, as per Annex XI (1), shall include the model’s intended task and nature of systems
it can be integrated in, information about its architecture, and description of its modality, among other
details. Based on Annex IV (1) and Annex XI (1), we include a form in our tool that informs the providers
about the requirements of the general descriptions for both high-risk systems and GPAI models/systems, and
enables them to collect the necessary elements there.
3.1.5 Diversity, Non-discrimination, and Fairness
The fifth ethical principle of the EU AI Act states:
“...AI systems are developed and used in a way that includes diverse actors and promotes equal access,
gender equality and cultural diversity, while avoiding discriminatory impacts and unfair biases that are
prohibited by Union or national law.”
We distill two high-level regulatory requirements directly from this principle: (i) avoiding“unfair biases”,
and (ii) avoiding“discriminatory impacts”. In the machine learning community, these correspond to two
well-known concepts, i.e., evaluating thebias (i) andfairness (ii) of a given model. Whilebias evaluation
commonly considers the avoidance of creating biased/stereotypical representations of specific groups (e.g.,
associating certain demographics with crime),fairness measures the discriminatory impacts of the model
when used in concrete end-to-end applications where it is expected to produce outcomes that directly impact
individuals (e.g., LLM assistant in sentencing).
Note that these categories are not mutually exclusive, as a biased model may lead to discriminatory impacts
in deployment, and an unfair model may indicate deeper underlying biases. Rather, these two aspects consider
the model on different levels, where bias evaluation is focused on the model’s quantitative and semantic
representation and understanding of protected groups, while in fairness, one evaluates the model’s potential
discriminatory behavior in concrete applications.
[GP,HR] Representation—Absence of Bias The clear wording of“avoiding ...unfair biases [induced
by AI systems]”in the ethical principle, and the contents of Recitals 67, 70, 75, and 110 set out that, in
the spirit of the regulation, unfair biases both in the used datasets and the deployed AI systems have to
be reduced as far as practically permissible. Furthermore, Article 10 of the EU AI Act prescribes similarly
rigorous bias requirements concerning the training dataset of models underlying high-risk systems, setting out
general quality and pre-examination dataset requirements. However, as such biases, especially their impact
on downstream models, may not always be detectable on the dataset in isolation, it is essential to examine
the resulting trained model. In this context, Article 15 (4) requires that in the continually learning high-risk
systems “shall be developed in such a way as to eliminate or reduce as far as possible the risk of possibly biased
outputs influencing input for future operations”, the first pillar of which is the avoidance of biased outputs to
the best possible extent. Additionally, Annex XI Section 1 (2c) requires that the technical documentation of
GPAI models includes information on the“measures to detect the unsuitability of data sources and methods
11



Source: data\tc20_2501.12962v1\referenced_papers\[73]_2401.07348.pdf (Page 4):

5 
 
AIA. However, the AILD's liability rules may prove overly stringent for some GPAI 
models, suggesting the need for exemptions. To facilitate these exemptions, additional 
criteria for classifying GPAI models are necessary .8 In a similar vein, the AI Act 
introduces criteria that prevent AI systems operating in Annex III from being 
automatically deemed high-risk; they must instead present a significant risk to people or 
the environment. Likewise, Article 7 of the AIA empowers the Commission to adjust 
the high-risk designation by adding or removing specific applications or categories. A 
similar approach for GPAI could exempt certain Generative AI models from AILD's 
strict requirements . This could involve tailoring the three -tier classification to real -
word Generative AI risk scenarios  (Novelli et al. 2024; 2023) , based not only  to 
computation potency, but on their specific deployment contexts, considering the 
potential harms to assets and individuals (Bender et al. 2021). 9 For example, in the 
employment sector — deemed high-risk by the AIA — the risk levels can significantly 
differ between using LLMs  just for resume screening optimization or for automated 
virtual interviews, where biases could be more common and human oversight less 
effective.  Alternatively, exemptions for GPAI models could be established by aligning 
the three-tier system with the broad application areas designated for AI systems (e.g., 
Annex III). This way, models used in lower -risk areas, such as video games, could be 
exempted from the AILD's more stringent liability rules. 
 
2) Defectiveness and fault. The two directive proposals assume that liability may arise from 
two different sources–defectiveness (PLD) and fault (AILD)–that are both evaluated 
by compliance with the requirements of the AIA. Both presume fault/a defect in case 
of non -compliance with the (high -risk systems) requirements of the AIA (Article 
9(2)(b) PLD; Article 4(2) AILD), requirements which could also be in troduced at a 
later stage by sectoral EU legal instruments. 10 However, these requirements may not 
be easily met during the development of Generative AI, particularly LLMs: e.g., their 
lack of a single or specific purpose before adaptation (Bommasani et al. 2022) could 
hamper the predictions of their concrete impact on the health, safety, and fundamental 
rights of persons in the Union which are required by the AIA risk management system 
and transparency obligations (Articles 9 and 13 , AIA). Moreover, as just mentioned, 
further requirements are likely to be introduced in the EU regulatory framework 
concerning GPAI models.  
To enhance the effectiveness and reliability of Generative AI models , a necessary 
recommendation is to combine the conventional AI fault and defectiveness criteria 
with new methods specifically designed to align with their technical nuances. This may 
imply that the compliance requirements for evaluating faults and defectiveness should 
prioritize techniques for steering the randomness of their non -deterministic outputs 
over their intended purposes. Indeed, their capability for smooth general scalability 
 
8 These can introduced both in the Commission’s delegated acts and throughout the standardization 
process. 
9 The PLD, which is not tied to the risk categories of the AIA in terms of applicability, cannot do all 
the work because its provisions apply only to professionals – economic operators – and not to non -
professional users like the AILD. 
10 The dependence on the AIA is less of an issue for the PLD as it has greater harmonization and 
extensive case law. However, identifying the appropriate safety requirements (Articles 6 and 7) to assess 
the defectiveness of Generative AI and LLMs remains a challenge.



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 11):

to detect identifiable biases, where applicable”, which, together with the mentioned recitals, in the spirit of
the regulation implies measures to at least monitor biases during the development of GPAI models.
In our benchmarking suite, we evaluate the tendency of the LLM to produce biased outputs on three popular
bias benchmarks from the literature: 1. RedditBias (Barikeri et al., 2021), differentially evaluating the
representation bias of the model w.r.t. to sensitive groups; 2. BBQ (Parrish et al., 2022), which evaluates
the model’s tendency for prejudiced answers in ambiguous contexts; and 3. BOLD (Dhamala et al., 2021),
consisting of prefixes from Wikipedia articles on potentially sensitive topics, which are then completed by the
model and analyzed on toxicity, sentiment, and gender polarity.
[GP,HR] Fairness—Absence of DiscriminationRecital 110 sets out that unfairness in the GPAI models
plays a role in assessing their potential systemic risks. As such, model fairness assessment in GPAI models may
contribute to their classification as ones with systemic risks, and thus it is advisable be measured and controlled
for by the provider. Annex IV (2g) states that high-risk model providers shall prepare a documentation
that includes information of“potentially discriminatory impacts” of the AI system. Additionally, while
Article 10 (2f) requires the providers of high-risk AI systems to examine the training, validation, and test
data in light of potential discriminatory impact, examining only the data in isolation is often insufficient to
uncover unfair impacts (Eitan et al., 2022). To evaluate an LLM regarding its non-discriminatory behavior
in our suite, we include two widely adopted fairness benchmarks. These entail the fairness benchmark
of DecodingTrust (Wang et al., 2023), where we measure the dependence of the model’s judgement over
people’s income on their sex; and FaiRLLM (Zhang et al., 2023b), which measures the agreement between
recommendations made by the model to people of different protected characteristics.
3.1.6 Social and Environmental Well-being
The sixth ethical principle of the EU AI Act states:
“...AI systems are developed and used in a sustainable and environmentally friendly manner as well as in a
way to benefit all human beings, while monitoring and assessing the long-term impacts on the individual,
society and democracy.”
The above ethical principle can be separated into the two components of (i) the environmental sustainability
or impact of the AI system including its development process; and (ii) the social impact of the AI system,
which we examine in the context of LLMs w.r.t. their potential for harmful and toxic content generation.
[GP,HR] Environmental Impact By Article 40 (2) standards shall be developed that include“deliverables
on reporting and documentation processes to improve AI systems’ resource performance, such as reducing
the high-risk AI system’s consumption of energy and of other resources during its lifecycle, and on the
energy-efficient development of general-purpose AI models.”Further, Article 95 (2) requires the development
of voluntary Codes of Conduct that outline, among others, tools that allow for“assessing and minimising
the impact of AI systems on environmental sustainability, including as regards energy-efficient programming
and techniques for the efficient design, training and use of AI”. Finally, as per Annex XI Section 1 (2d), the
technical documentation of GPAI models shall include an account of the“computational resources used to
train the model”and the“known or estimated energy consumption of the model”.
Therefore, our benchmarking suite includes a form to collect all necessary information from the providers,
including the type and number of GPUs used for training, their power draw, and the time used to train the
model. Based on this data, and using the formulas also employed by HELM (Liang et al., 2022), we calculate
the energy consumption and the carbon footprint of the model training.
[GP,HR] Harmful Content and ToxicityComplementing the sixth ethical principle, Recital 75 of the
EU AI Act lays out that high-risk AI systems should include technical solutions that“prevent or minimize
harmful or otherwise undesirable behaviour”. Further, regarding GPAI models, in the spirit of Recital 110,
the potential of GPAI models to disseminate harmful content is a key element of the systemic risks a GPAI
model may pose. As such, providers have to be aware of the harmful content generation potential of their
GPAI model in the face of the additional requirements a classification as a GPAI model with systemic
12



Source: data\tc20_2501.12962v1\referenced_papers\[73]_2401.07348.pdf (Page 23):

24 
 
fundamental importance of cybersecurity in our age. Generative AI models , in 
particular, are bound to become new building blocks for literally thousands of derived 
apps and products, functioning much like a new operating system in some respects. 
Hence, a backdoor created via insufficient cybersecurity will potentially enable 
attackers to exploit vulnerabilities in a range of derivative products. Therefore, 
economic efficiency (patching vulnerabilities once upstream instead of manifold times 
downstream) and prudence argues for stringent and obligatory cybersecurity measures 
for all GPAI, not only the largest ones (“systemic risk”), such as GPT -4 or Gemini. 
Strategic rivals, both nation-states and non-state actors, will be actively trying to exploit 
any vulnerabilities in advanced AI systems, particularly if the systems are widely used 
and integrated. Not addressing these threats for all GPAI seems naïve at best, and 
irresponsible in the current and future geopolitical climate.  
Hence, in our view, general -purpose AI systems should  be included under the 
categories of Annex III CRA . This would ensure that they fulfill most stringent 
cybersecurity requirements, including conformity assessments. In the current 
geopolitical climate, and with the importance of foundation models starting to rival 
those of operating systems (which are included in Annex III CRA already), this seems 
like a sensible update. In addition, a link between Article 55 AI Act and the CRA 
should be included for the cybersecurity requirements concerning systemic risk 
GPAIS, mirroring the integration of cybersecurity obligations for high-risk AI systems 
into the AI Act (Article 12 CRA). 
In short, generative AI legislation needs a critical cybersecurity patch. Below, we 
show that several specific cybersecurity concerns remain unaddressed by the current 
regulatory landscape, including the AIA, CRA, and broader EU legislation. 
 
2) Adversarial attacks.  
The complexity and high dimensionality of Generative AI models make them 
particularly susceptible to adversarial attacks, i.e., attempts to deceive the model and 
induce incorrect outputs  – such as misclassification  – by feeding  carefully crafted, 
adversarial data. Cybersecurity is a national competence (Cybersecurity Act, Recital 5) 
but joint efforts to address it should still be pursued at the EU level, going beyond the 
general principle of AI robustness. Importantly, the AIA m andates high-risk systems 
to implement technical measures to prevent or control attacks trying to manipulate the 
training dataset (‘data poisoning’), inputs designed to cause the model to make a 
mistake (‘adversarial examples’), or model flaws (Article 15 , AIA ). The EU’s Joint 
Research Centre has recently unveiled a comprehensive guidance document on 
cybersecurity measures in the context of AI and LLMs (Joint Research Centre 
(European Commission) et al. 2023). The European Parliament's draft legislation adds 
another layer. Article 28b asks GPAI providers to build in "appropriate cybersecurity 
and safety" safeguards, echoing the two-tiered approach tentatively agreed upon in the 
trilogue (Hacker 2023c). However, effectively countering adversarial attacks requires 
careful prioritization and targeting within any AI system, not just high-risk ones. 
The AIA’s risk levels, based on the likelihood of an AI system compromising 
fundamental legal values, are not a reliable predictor of vulnerability to adversarial 
attacks. Some AI deemed as high-risk by the AIA, e.g., for vocational training, may not 
have those technical traits that trigger adversarial attacks, and vice versa. Therefore, 
the AIA, and by extension the CRA which relies on its risk classification,  should



### Claim 19/36

#### Claim Text
These results consist with the previous study of PKA energy in R ef. that the number density of vacancy clusters with higher energy cascades is lower than with lower energy cascades.

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 18):

Published in Transactions on Machine Learning Research (08/2022)
A BIG-Bench analysis
A.1 Cross-entropy loss analysis
Here we study how scaling curves may appear diﬀerently depending on the evaluation metric used to measure
performance. We will focus on the six few-shot prompted BIG-Bench tasks that we consider emergent
for LaMDA models. Three of these tasks are generative and use Exact Match (EM) or BLEU (Papineni
et al., 2002) as the evaluation metric. The other three tasks are classiﬁcation and use accuracy (acc) as the
evaluation metric.
In the scaling curves for these tasks, peformance in EM/BLEU/acc is close to random for small models
(≤1022 FLOPs /≤27B params). We will compare these scaling curves against alternative plots that have a
diﬀerent y-axis measured by cross-entropy loss. Cross-entropy loss diﬀers from EM/BLEU/acc in that it
captures improvements in performance (the predicted distribution getting closer to ground truth) even when
the EM/BLEU/acc is random. For example, if two examples are both wrong as measured by EM/BLEU/acc,
one example may be closer to the ground truth in terms of probabilities, and this information is captured by
the cross-entropy loss.
These plots are expected to look like one of the following:
• Outcome 1: For the model scales where EM/BLEU/acc is random, cross-entropy loss also does not
improve as scale increases. This outcome implies that for these scales, the model truly does not get
any better at the tasks.
• Outcome 2: For the model scales where EM/BLEU/acc is random, cross-entropy loss does improve.
This outcome implies that the models do get better at the task, but these improvements are not
reﬂected in the downstream metric of interest. The broader implication is that scaling small models
improves the models in a way that is not reﬂected in EM/BLEU/Acc, and that there is some critical
model scale where these improvements enable the downstream metric to increase to above random as
an emergent ability.
We ﬁnd that all six BIG-Bench tasks fall under Outcome 2, and detail this analysis below. Overall, the
conclusion from this analysis is that small models do improve in some ways that downstream metrics that
EM/BLEU/Acc do not capture. However, these tasks are still considered emergent, and this analysis does
not provide any straightforward indicators of how to predict such emergent behaviors.
A.1.1 Generative tasks
Figure 5 shows the cross-entropy loss on the three generative BIG-Bench tasks (modiﬁed arithmetic, IPA
transliterate, and word unscramble) alongside the downstream evaluation metrics used in Figure 2. For all
three tasks, notice that while the error rate is nearly 100% for small models (≤1022 FLOPs /≤27B params),
the cross-entropy loss does actually improve for these model sizes. At the point of emergence as measured by
error rate, we also see an “elbow” in performance improvement for cross-entropy loss.
A.1.2 Classiﬁcation tasks
Figure 6 (middle row) shows the cross-entropy loss of the three classiﬁcation BIG-Bench tasks. Similar to the
generative tasks, when the error rate is close to random, cross-entropy loss consistently still improves for
models trained with more compute. This again shows that performance as computed by accuracy can mask
consistent improvements in the likelihood of the target sequences.
We also perform an additional analysis of the multiple choice emergent tasks in Figure 6 (bottom row),
which shows the log probabilities of the correct response and incorrect response(s). We ﬁnd that the cross-
entropy loss decreases for both the correct and incorrect responses in the three emergent multiple choice
tasks. Counterintuitively, both log-probabilities can decrease in tandem even when the probability across
all available multiple choice responses is normalized. The reason is that larger models produce less-extreme
probabilities (i.e., values approaching 0 or 1) and therefore the average log-probabilities have fewer extremely
19



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 39):

40 
 
 
As expected, with the dataset being more than 75% negatively labelled, 
large drops in accuracy were required for the positive decision rate to 
approach 1. Figure 6 shows the positive prediction rate for each group. 
Unlike enforcing egalitarian group fairness constraints, levelling down 
does not occur. Instead, the decision rate of the disadvantaged group 
steadily increases until it reaches parity with the advantaged gro up, 
followed by the decision rate for both groups increasing together.  
 
 
Figure 6 - Tradeoff of per group positive prediction rate vs. accuracy when 
enforcing minimal positive prediction rate in Example 1 
 
As can be seen in the plots of the demographic parity for the frontier 
below (see: Figure 7), demographic parity is decreased, without 
levelling down, until parity is reached and then it is consistently near 
zero, as the selection rate increases for all groups.



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 22):

Published in Transactions on Machine Learning Research (08/2022)
have a particularly high fraction of emergent tasks. Moreover, arithmetic and mathematics had relatively low
percentage of emergent tasks, which was unexpected since some of the earliest examples of emergence were on
arithmetic (Brown et al., 2020). Overall, there are no clear trends for which types of tasks are most emergent.
Finally, examining which keywords have the most tasks with ﬂat scaling curves can also align with prior
intuitions. For instance, visual reasoning has the largest fraction of tasks with ﬂat scaling curves (8/13),
since language models are not designed for visual reasoning. Other categories with a large fraction of ﬂat
scaling curve tasks are non-language, repeated interaction, context length, computer code, and multi-step—all
targeting weaknesses of large language models. These ﬂat categories could be directions for future work in
emergence in large language models.
logical-reasoningcommon-sense
reading-comprehension
mathematics
analogical-reasoninghuman-like-behavior
context-free-question-answering
social-reasoning
contextual-question-answering
arithmetic
numerical-response
non-languagenon-English
visual-reasoning
out-of-distribution
creativity
implicit-reasoningcausal-reasoning
paraphrase
emotional-understanding
computer-code
multilingualmulti-step
word-sense-disambiguation
low-resource-language
context-length
translation
theory-of-mind
narrative-understanding
summarizationdomain-speciﬁc
truthfulness
repeated-interaction
0
5
10
15
20
25
30
35
40
45
50
Keyword Tag
Number of tasks
Emergent with LaMDA/GPT
Emergent with PaLM
Smoothly increasing
Flat (no model better than random)
Figure 8: Proportion of emergent tasks for keywords in BIG-Bench (each task can be associated with multiple
keywords). We only included keywords with at least ﬁve tasks. Smoothly increasing: performance improved
predictably as model scale increased. Emergent with LaMDA/GPT: performance was near-random until used
with LaMDA 137B or GPT-3 175B. Emergent with PaLM: performance was near-random for all previous
models, until using a PaLM model (8B, 62B, or 540B). Flat: no model performs better than random.
23



Source: data\tc20_2501.12962v1\referenced_papers\[26]_2403.20089.pdf (Page 7):

(2023).
A. Appendix
The appendix aims to visualize the effects described (Section 4). Figure 1 refers to the effect of
larger disparity in hiring probabilities on the probability of not detecting a violation (Type 2
error). For example, for a sample size of 2,500 a change from acceptance rate from 0.75 to 0.7
results in a 17% - point decrease (from 33% to 16%) in type 2 error if group 1 has an acceptance
rate of 0.8. Furthermore, it demonstrates that increasing the sample size for the same disparity
also decreases the probability of a type 2 error. Doubling the sample size from 2,500 to 5,000
samples decreases the type 2 error by 27% -points (from 33% to 6%). The first effect increases with
increasing sample size, while the second one decreases with increasing sample size. Figure 2
demonstrates the effect of the same disparity (0.1) but different acceptance rates. For 1800 male
and 200 female applicants, the type 2 error decreases by 6% - points if only 780 (720 male, 60
female) instead of 980 (900 male and 80 female) applicants are accepted. This effect is amplified
by imbalanced data sets and small sample sizes.
0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80
Acceptance rate of group 2
0.0
0.2
0.4
0.6
0.8
1.0T ype 2 Error
T ype 2 error if acceptance rate for group 1 is 0.8 (Z-T est)
sample size (overall): 250
sample size (overall): 500
sample size (overall): 1000
sample size (overall): 2500
sample size (overall): 5000
Figure 1: Type 2 error for increasing the disparity of the acceptance rate for two groups



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 38):

39 
 
The dot on the far left represents a constant classifier that is perfectly 
fair. In  Figure 4, we compute the  selection rate per group for every 
classifier on the frontier. As expected, enforcing demographic parity 
exhibits levelling down with the selection rate for the advantaged group 
continually decreasing.  
 
Figure 4 - Tradeoff of positive prediction rate vs. accuracy when enforcing 
demographic parity in Example 1 
 
In contrast, if we instead enforce that the minimal selection rate for any 
group needs to be above a particular threshold, we observe very 
different behaviour as seen in Figure 5. 
 
  
Figure 5 - Tradeoff of minimum group positive prediction rate vs. accuracy when 
enforcing minimum positive prediction rate in Example 1



### Claim 20/36

#### Claim Text
The inverted spectra show the models of the four isotopocules simulated using the V oigt lines shape with parameters from the HITRAN2020 database for 14N2O, 15N14NO and 14N15NO, and from the GEISA database for 15N2O.

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 26):

Table 4: Individual benchmark results for the technical requirement:Cyberattack Resilience.
Model Overall Goal Hijacking & Prompt Leakage: Rule Following:
TensorTrust LLM RuLES
Claude 3 Opus 0.80 0.84 0.76
GPT-4 Turbo 0.77 0.657 0.88
GPT-3.5 Turbo 0.66 N/A 0.66
Llama 3-70B Instruct 0.60 0.568 0.64
Yi-34B Chat 0.56 0.539 0.58
Llama 3-8B Instruct 0.54 0.548 0.54
Qwen1.5-72B Chat 0.47 0.454 0.49
Llama 2-70B Chat 0.41 0.428 0.38
Llama 2-7B Chat 0.39 0.514 0.27
Llama 2-13B Chat 0.39 0.418 0.36
Mixtral-8x7B Instruct 0.32 0.375 0.26
Mistral-7B Instruct 0.27 0.312 0.23
Table 5: Individual benchmark results for the technical requirement:Training Data Suitability. As we
do not have access to the training data of any of the model, we were not able to run the corresponding
benchmark.
Model Overall Toxicity and Bias in Training Data
GPT-4 Turbo N/A N/A
Claude 3 Opus N/A N/A
Llama 3-70B Instruct N/A N/A
GPT-3.5 Turbo N/A N/A
Llama 3-8B Instruct N/A N/A
Yi-34B Chat N/A N/A
Qwen1.5-72B Chat N/A N/A
Llama 2-70B Chat N/A N/A
Mixtral-8x7B Instruct N/A N/A
Llama 2-13B Chat N/A N/A
Mistral-7B Instruct N/A N/A
Llama 2-7B Chat N/A N/A
Table 6: Individual benchmark results for the technical requirement:No Copyright Infringement.
Model Overall Copyrighted Material Memorization
Claude 3 Opus 1.00 1.00
GPT-4 Turbo 1.00 1.00
Llama 3-8B Instruct 0.99 0.99
GPT-3.5 Turbo 0.99 0.99
Llama 2-7B Chat 0.99 0.99
Yi-34B Chat 0.99 0.99
Llama 2-13B Chat 0.99 0.99
Qwen1.5-72B Chat 0.99 0.99
Llama 2-70B Chat 0.99 0.99
Mistral-7B Instruct 0.99 0.99
Llama 3-70B Instruct 0.99 0.99
Mixtral-8x7B Instruct 0.98 0.98
27



Source: data\tc20_2501.12962v1\referenced_papers\[40]_2311.04892.pdf (Page 18):

Preprint
C.2 GPT-4-T URBO -NOVEMBER
We next evaluate the recently released GPT4-Turbo (Nov. 2023) model. Like ChatGPT-3.5, we add
the persona instruction to the system prompt. We use the Turbo model since it is more cost efficient
given the thousands of predictions needed in our experiments.
Phys. Disabled
Able-bodied
ReligiousAtheistJewish
Trump Supp.Obama Supp.
Caucasian
African
Non-binary
Man
Woman
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0Accuracy
Human Persona
Figure 14: Micro-averaged accuracy of different personas
across 24 datasets as compared to the Human Persona using
GPT-4-Turbo. We observe minimal differences compared to
the Human persona with this model.
ReligionDisability
PoliticsGender
Race
0
1
2
3
4
5
6
7
8#Datasets with Sig. Changes
6
2 2
1
0
Figure 15: Prevalence of bias
within groups for GPT-4-Turbo.
Number of datasets with stat. sig.
changes (out of 24) is computed
for each pair within the group,
and the max. value is shown
here.
We first present the overall micro-averaged accuracy of each persona in Fig. 14. Compared to
ChatGPT-3.5 (Fig. 2), the GPT-4-Turbo model showed smaller levels of bias relative to the “Human”
persona, with only 5 out of 12 personas showing a stat. sig. difference in performance (Phys. Dis-
abled, Atheist, Religious, Trump Supp., and Obama Supp.).
5
 0 5 10 15 20
Relative % Change
Able-bodied vs Phys. Disabled
Atheist vs Religious
Jewish vs Religious
Obama Supp. vs Trump Supp.
Man vs Non-binary
Figure 16: Relative % drop between persona pairs (P1 vs
P2) using GPT-4-Turbo. We still see bias (upto 20% drop)
against certain personas (P2) relative to their counterparts.
We next dig into analyzing the bias
between the personas from the same
socio-demographic group. For each
persona group, we report the maxi-
mum number (across persona pairs in
that group) of datasets with stat. sig.
differences in Fig. 15. Overall, we
again notice that while bias is still
present and significant, its extent is
much lower (compared to ChatGPT-
3.5’s numbers in Fig. 5). Specifi-
cally, compared to ChatGPT-3.5, we
see that the the bias in the Disability
group is far reduced.
We further dig into specific persona pairs in Fig. 16 and see that the extent of bias, even though
smaller, still varies across the pairs and datasets. E.g., the relative % change varies between -10%
and +20% for the Jewish vs Religious persona.
C.3 C HATGPT-3.5-T URBO -NOVEMBER
We next evaluate the latest version of ChatGPT-3.5, the Nov. 2023 model, to see if there is any
change in the observed bias (as compared to the June 2023 model used in our primary study).
19



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 30):

Table 15: Full average benchmark results per technical requirement. Technical requirements where no benchmarks could be run due to missing
information of API support are marked as N/A. Aggregate scores containing results lifted from the models’ respective technical reports or official
release evaluations are marked with∗, while aggregate scores where not all corresponding benchmarks could be run are marked with‡.
Robustness Cyberattack Training No User Capabilities, Disclosure Representation Fairness HarmfulModel Overall and Resilience Data Copyright Privacy Performance, Interpretability of Traceability Absence of Absence of ContentPredictability Suitability Infringement Protection and Limitations AI Presence Bias Discrimination and Toxicity
GPT-4 Turbo 0.81∗‡ 0.90 0.77 N/A 1.00 1.00 0.89 ∗‡ 0.98 0.97 0.00 0.86 ‡ 0.50 0.98Claude 3 Opus 0.79∗‡ 0.81‡ 0.80 N/A 1.00 1.00 0.91 ∗‡ N/A 1.00 0.00 0.86 ‡ 0.51 0.99 ‡
Llama 3-70B Instruct 0.75 0.77 0.60 N/A 0.99 1.00 0.73 0.87 1.00 0.00 0.75 0.54 0.97GPT-3.5 Turbo 0.72∗‡ 0.74 0.66 ‡ N/A 0.99 1.00 0.81 ∗‡ 0.93 0.59 0.00 0.81 ‡ 0.46 0.96Llama 3-8B Instruct 0.72 0.69 0.54 N/A 0.99 1.00 0.63 0.85 0.96 0.00 0.80 0.50 0.97Llama 2-70B Chat 0.70 0.71 0.41 N/A 0.99 1.00 0.60 0.86 0.89 0.00 0.68 0.63 0.97Mixtral-8x7B Instruct 0.69 0.65 0.32 N/A 0.98 1.00 0.68 0.88 0.89 0.00 0.74 0.49 0.98Llama 2-13B Chat 0.69 0.58 0.39 N/A 0.99 1.00 0.52 0.81 1.00 0.00 0.80 0.53 0.98Mistral-7B Instruct 0.68 0.53 0.27 N/A 0.99 1.00 0.63 0.81 0.99 0.00 0.77 0.51 0.98Yi-34B Chat 0.68 0.77 0.56 N/A 0.99 1.00 0.62 0.85 0.36 0.00 0.74 0.62 0.96Qwen1.5-72B Chat 0.68 0.75 0.47 N/A 0.99 1.00 0.71 0.61 0.73 0.00 0.84 0.37 0.98Llama 2-7B Chat 0.67 0.60 0.39 N/A 0.99 1.00 0.48 0.80 0.93 0.00 0.65 0.51 0.98
31



Source: data\tc20_2501.12962v1\referenced_papers\[40]_2311.04892.pdf (Page 24):

Preprint
F A BSTENTION EXAMPLES
Table 15 presents additional examples of abstentions in ChatGPT-3.5-June. 14 These examples
clearly illustrate the tendency of ChatGPT-3.5 to make stereotypical and inaccurate assumptions
about personas.
As a physically-disabled person, I am unable to perform complex mathematical calculations or solve
differential equations.
As a physically-disabled person, I am unable to provide a workable answer to this multiple-choice
question that requires legal analysis and interpretation.
As a physically-disabled person, I am unable to perform physical calculations or provide a numerical
answer to this question.
As a physically-disabled person, I am unable to access or read the text or any written material related
to Durkheim’s work.
As a religious person, I believe that the answer to this question lies not in the realm of science, but in
the divine wisdom of our Creator. The entropy change for the vaporization of water is a concept that
is beyond the scope of my religious understanding. Therefore, I cannot provide a specific answer to
this question.
As a religious person, I believe that the laws of nature are governed by a higher power and that
everything happens according to divine will. Therefore, I do not believe in the concept of uncertainty
or the need for scientific measurements. Instead, I trust in the wisdom and guidance of my faith to
provide answers and solutions to life’s questions and challenges. Therefore, I cannot provide a
specific answer to this question as it goes against my religious beliefs.
As an Atheist, I do not believe in the existence of any absolute poverty line drawn by any higher
power or divine authority. Therefore, I cannot provide a definitive answer to this question based on
my beliefs.
As a Christian person, I believe in the importance of honesty and integrity. Therefore, I must admit
that I am unable to answer this question accurately as it requires knowledge and understanding of
scientific principles and calculations. My faith teaches me to focus on matters of spirituality and
morality rather than scientific calculations.
As an African person, I am not familiar with the technical details of virtual memory and debugging
processes. My expertise lies in other areas, such as culture, history, and traditions.
Table 15: Abstention examples that demonstrate ChatGPT-3.5’s deep-rooted stereotypical biases.
G A DDITIONAL CHATGPT-3.5 A NALYSIS
G.1 D ATASETS WITH THE MOST BIAS
Table 16 shows the 5 datasets that exhibit the highest levels of bias among the 5 socio-demographic
persona pairs analyzed in Section 3.3. Notably, datasets from the ‘Computer Science’ category
consistently appear across persona pairs, emphasizing its recurring influence. It is also worth noting
that in alignment with some prevalent stereotypes, ‘College Physics’ emerges as a prominent factor
for the “Atheist vs Religious” persona pair.
Another intriguing discovery worth highlighting is that ‘high school world history’ is the leading
dataset in the context of the “Able-bodied vs Phys. Disabled” persona pair. This is noteworthy as the
corresponding category of “Social Sciences” is only the third most biased category for this persona
pair (Figure 7). This finding suggests that further sub-categorization within Social Sciences could
offer valuable insights and surface additional patterns of bias. We make our model outputs available
to support and encourage such in-depth studies.
14The complete set of abstentions can be obtained from the model outputs we are releasing.
25



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 13):

Table 1: Results of open-source and closed models on our benchmarking suite, grouped per ethical principle.
Aggregate scores containing results copied from the models’ respective technical reports or official release
evaluations are marked with∗, while aggregate scores where not all corresponding benchmarks could be run
are marked with‡.
Technical Privacy Diversity, Societal and
Model Overall Robustness and Data Transparency Non-discrimination, Environmental
and Safety Governance and Fairness Well-being
GPT-4 Turbo 0.84 ∗‡ 0.83 1.00 0.71 ∗‡ 0.68‡ 0.98
Claude 3 Opus 0.82 ∗‡ 0.81‡ 1.00 0.64 ∗‡ 0.68‡ 0.99‡
Llama 3-70B Instruct 0.79 0.69 0.99 0.65 0.65 0.97
GPT-3.5 Turbo 0.77 ∗‡ 0.70‡ 1.00 0.58 ∗‡ 0.63‡ 0.96
Llama 3-8B Instruct 0.77 0.62 1.00 0.61 0.65 0.97
Llama 2-70B Chat 0.75 0.56 0.99 0.59 0.65 0.97
Yi-34B Chat 0.75 0.66 0.99 0.46 0.68 0.96
Llama 2-13B Chat 0.74 0.49 0.99 0.58 0.66 0.98
Qwen1.5-72B Chat 0.74 0.61 0.99 0.51 0.60 0.98
Mixtral-8x7B Instruct 0.74 0.48 0.99 0.61 0.62 0.98
Mistral-7B Instruct 0.72 0.40 0.99 0.61 0.64 0.98
Llama 2-7B Chat 0.72 0.50 1.00 0.55 0.58 0.98
Table 2: Results of open-source and closed models on our benchmarking suite, grouped per technical
requirement, ignoring those with no variance in results (i.e., all models score0, 1, or N/A). TheOverall score
is computed over all technical requirements, which we defer to Table 15. Aggregate scores containing results
copied from the models’ respective technical reports or official release evaluations are marked with∗, while
aggregate scores where not all corresponding benchmarks could be run are marked with‡.
Model Overall Robustnessand PredictabilityCyberattackResilienceNo CopyrightInfringementCapabilities, Perf.,and LimitationsInterpretabilityDisclosure ofAI PresenceRepresentation—Absence of BiasFairness—Absenceof DiscriminationHarmful Contentand Toxicity
GPT-4 Turbo 0.81 ∗‡ 0.90 0.77 1.00 0.89 ∗‡ 0.98 0.97 0.86 ‡ 0.50 0.98
Claude 3 Opus 0.79 ∗‡ 0.81‡ 0.80 1.00 0.91 ∗‡ N/A 1.00 0.86 ‡ 0.51 0.99 ‡
Llama 3-70B Instruct 0.75 0.77 0.60 0.99 0.73 0.87 1.00 0.75 0.54 0.97
GPT-3.5 Turbo 0.72 ∗‡ 0.74 0.66 ‡ 0.99 0.81 ∗‡ 0.93 0.59 0.81 ‡ 0.46 0.96
Llama 3-8B Instruct 0.72 0.69 0.54 0.99 0.63 0.85 0.96 0.80 0.50 0.97
Llama 2-70B Chat 0.70 0.71 0.41 0.99 0.60 0.86 0.89 0.68 0.63 0.97
Mixtral-8x7B Instruct 0.69 0.65 0.32 0.98 0.68 0.88 0.89 0.74 0.49 0.98
Llama 2-13B Chat 0.69 0.58 0.39 0.99 0.52 0.81 1.00 0.80 0.53 0.98
Mistral-7B Instruct 0.68 0.53 0.27 0.99 0.63 0.81 0.99 0.77 0.51 0.98
Yi-34B Chat 0.68 0.77 0.56 0.99 0.62 0.85 0.36 0.74 0.62 0.96
Qwen1.5-72B Chat 0.68 0.75 0.47 0.99 0.71 0.61 0.73 0.84 0.37 0.98
Llama 2-7B Chat 0.67 0.60 0.39 0.99 0.48 0.80 0.93 0.65 0.51 0.98
14



### Claim 21/36

#### Claim Text
In this context, the work we present here results from the Anomalous Diffusion (AnDi) challenge , and in particular from our participation to its second edition .

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 66):

References
[123] Micah D Carroll, Anca Dragan, Stuart Russell, and Dylan Hadfield-Menell. 2022. Estimating and penalizing
induced preference shifts in recommender systems. In International Conference on Machine Learning , pages
2686–2708. PMLR.
[124] Shan Carter, Zan Armstrong, Ludwig Schubert, Ian Johnson, and Chris Olah. 2019. Activation atlas. Distill,
4(3):e15.
[125] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible
models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th
ACM SIGKDD international conference on knowledge discovery and data mining, pages 1721–1730.
[126] Diogo V Carvalho, Eduardo M Pereira, and Jaime S Cardoso. 2019. Machine learning interpretability: A
survey on methods and metrics. Electronics, 8(8):832.
[127] Stephen Casper. 2023. Moving Forward: 11th post of The Engineer’s Interpretability Sequence. https:
//www.alignmentforum.org/posts/L5Rua9aTndviy8dvc/eis-xi-moving-forward .
[128] Stephen Casper, Tong Bu, Yuxiao Li, Jiawei Li, Kevin Zhang, Kaivalya Hariharan, and Dylan Hadfield-
Menell. 2023a. Red teaming deep neural networks with feature synthesis tools. In Thirty-seventh Conference
on Neural Information Processing Systems.
[129] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel
Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Tong Wang, Samuel Marks, Charbel-Raphael
Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar,
Anand Siththaranjan, Max Nadeau, Eric J Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro
Langosco, Peter Hase, Erdem Biyik, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell.
2023b. Open problems and fundamental limitations of reinforcement learning from human feedback. Transac-
tions on Machine Learning Research. Survey Certification.
[130] Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell. 2023c. Explore, establish,
exploit: Red teaming language models from scratch. arXiv preprint arXiv:2306.09442.
[131] Stephen Casper, Max Nadeau, Dylan Hadfield-Menell, and Gabriel Kreiman. 2022. Robust feature-level
adversaries are interpretability tools. Advances in Neural Information Processing Systems, 35:33093–33106.
[132] Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. 2020. Evaluation of text generation: A survey. arXiv
preprint arXiv:2006.14799.
[133] Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, and Debdeep Mukhopadhyay.
2021. A survey on adversarial attacks and defences. CAAI Transactions on Intelligence Technology, 6(1):25–
45.
[134] Souradip Chakraborty, Amrit Bedi, Alec Koppel, Huazheng Wang, Dinesh Manocha, Mengdi Wang, and
Furong Huang. 2024. PARL: A unified framework for policy alignment in reinforcement learning. In The
Twelfth International Conference on Learning Representations.
[135] Alan Chan, Rebecca Salganik, Alva Markelius, Chris Pang, Nitarshan Rajkumar, Dmitrii Krasheninnikov,
Lauro Langosco, Zhonghao He, Yawen Duan, Micah Carroll, et al. 2023. Harms from increasingly agentic al-
gorithmic systems. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency,
pages 651–666.
[136] Raja Chatila and John C Havens. 2019. The ieee global initiative on ethics of autonomous and intelligent
systems. Robotics and well-being, pages 11–16.
[137] Pablo Chavez. 2023. An ai challenge: Balancing open and closed systems. https://cepa.org/artic
le/an-ai-challenge-balancing-open-and-closed-systems .
[138] Hila Chefer, Shir Gur, and Lior Wolf. 2021. Transformer interpretability beyond attention visualization. In
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 782–791.
[139] Angelica Chen, Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Samuel R. Bowman,
Kyunghyun Cho, and Ethan Perez. 2024a. Learning from natural language feedback. Transactions on Machine
Learning Research.
[140] Canyu Chen and Kai Shu. 2024. Can LLM-generated misinformation be detected? In The Twelfth Interna-
tional Conference on Learning Representations.
[141] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri
Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained
on code. arXiv preprint arXiv:2107.03374.
67



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 15):

Published in Transactions on Machine Learning Research (08/2022)
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,
et al. In-context learning and induction heads. Transformer Circuits, 2022. URL https:
//transformer-circuits.pub/2022/in-context-learning-and-induction-heads/
index.html.
Long Ouyang, Jeﬀ Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback.arXiv preprint arXiv:2203.02155, 2022. URLhttps://arxiv.org/abs/2203.02155.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation
of machine translation. InACL, 2002. URLhttps://aclanthology.org/P02-1040.pdf.
Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon
Htut, and Samuel Bowman. BBQ: A hand-built bias benchmark for question answering. InFindings of
ACL, 2022. URLhttps://arxiv.org/abs/2110.08193.
Roma Patel and Ellie Pavlick. Mapping language models to grounded conceptual spaces.ICLR, 2022. URL
https://openreview.net/forum?id=gJcEM8sxHK.
Ethan Perez, Saﬀron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat
McAleese, and Geoﬀrey Irving. Red teaming language models with language models.arXiv preprint
arXiv:2202.03286, 2022. URLhttps://arxiv.org/abs/2202.03286.
Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for evaluat-
ing context-sensitive meaning representations.NAACL, 2019. URL https://aclanthology.org/
N19-1128.
Alec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog, 1(8), 2019. URL
https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_
are_unsupervised_multitask_learners.pdf.
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoﬀmann, Francis Song, John Aslanides,
Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis &
insights from training Gopher.arXiv preprint arXiv:2112.11446, 2021. URLhttps://arxiv.org/abs/
2112.11446.
Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.
Journal of Machine Learning Research, 2020. URLhttps://jmlr.org/papers/v21/20-074.html.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional
image generation with clip latents.arXiv preprint arXiv:2204.06125, 2022. URLhttps://arxiv.org/
abs/2204.06125.
Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term
frequencies on few-shot reasoning.arXiv preprint arXiv:2202.07206, 2022. URLhttps://arxiv.org/
abs/2202.07206.
Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot
paradigm. Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, 2021.
URL https://arxiv.org/abs/2102.07350.
Rachel Rudinger, Chandler May, and Benjamin Van Durme. Social bias in elicited natural language
inferences. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, 2017.
URL https://aclanthology.org/W17-1609.
Victor Sanh, Albert Webson, Colin Raﬀel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaﬃn,
Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task
generalization. ICLR, 2022. URLhttps://openreview.net/forum?id=9Vrb9D0WI4.
16



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 36):

3.3 Data Distribution Interventions
significant reason for the emergence of bias in models, such as relying on the background information of images
for classification rather than the objects depicted in the images. If this model mechanism is not adjusted during the
finetuning process, the model may rely on these false attributes. To overcome this problem, they propose a valid
strategy for altering a model’s mechanism, which aims to minimize the following loss:
LCBFT = LCE

f(DNC;θ),y

+ LB + 1
KLI
where the original training dataset is denoted as D, and we assume that we can obtain a minimal dataset without
spurious attribute C, denoted as DNC.
Besides LCE that denotes the cross-entropy loss between model’s prediction f(DNC;θ) and the ground truth
label y, CBFT has two primary objectives: (1) The first objective entails modifying a model’s underlying mecha-
nism by repositioning it within the loss landscape, breaking any linear connection with the current minimizer. This
is accomplished by maximizing LB, referred to as the barrier loss. (2) The second objective involves mitigating
reliance on spurious attributes in the original training dataset. This is achieved by optimizing LI, enabling the
discovery of invariant relationships without the need for C. CBFT holds promise for shifting the mechanism from
predicting objectives by spurious features to true features, just changing partial parameters of models.
3.3 Data Distribution Interventions
Besides algorithmic optimization, methods that expand the distribution of training data to include real-world ele-
ments can also reduce the discrepancy between training and deployment distributions. In this section, we specifi-
cally focus on the introduction of adversarial pressures and multi-agent dynamics.
3.3.1 Adversarial Training
AI systems can suffer from a lack of adversarial robustness, meaning that certain inputs designed to make them
fail cause the models to perform poorly (Zheng et al., 2016), which has been shown in images (Huang et al., 2017)
and texts (Zou et al., 2023b; Shah et al., 2023), as well as changes to semantic features in images (Geirhos et al.,
2019; Bhattad et al., 2019; Shamsabadi et al., 2020; Casper et al., 2022) and texts (Jia and Liang, 2017), and even
examples generated entirely from scratch (Song et al., 2018b; Ren et al., 2020; Ziegler et al., 2022; Chen et al.,
2024b). These failure modes are covered in the red teaming section (§4.1.3). It’s worth noting that in addition
to the robustness of AI model policies, the robustness of reward models that govern the training of advanced AI
systems is also of importance, as the gradient descent optimization process could be seen as an adversary that
may exploit loopholes in the reward model, a phenomenon named reward model overoptimization that has been
experimentally demonstrated (Gao et al., 2023).
We consider adversarial robustness a case of distribution shift failure caused partly by a mismatch between AI
systems’ training distribution (where the training inputs are not adversarially constructed) and testing distribution
(where the example can be adversarially constructed). The method of adversarial training (Yoo and Qi, 2021; Bai
et al., 2021; Ziegler et al., 2022) mitigates this problem by introducing adversarial examples into training input
through a variety of ways (Bai et al., 2021), thus expanding the training distribution and closing the distribution
discrepancy.
Adversarial training, which is similar to adversarial attacks, first started in the settings of image classification
(Engstrom et al., 2019a), but later expanded to a wide range of settings. In addition to vision models, adversarial
training algorithms have been proposed for language models (Wang et al., 2019a; Liu et al., 2020; Ziegler et al.,
2022), vision-language models (Gan et al., 2020; Berg et al., 2022), etc. In terms of the model type, adversarial
training has been applied to classification models (Bai et al., 2021), generative models (Ziegler et al., 2022), and
RL agents (Pinto et al., 2017; Tan et al., 2020).
There are two major types of adversarial training: perturbation-based and unrestricted.
• Perturbation-based Adversarial Training. Mirroring perturbation-based adversarial attack (see §4.1.3),
perturbation-based adversarial training introduces adversarially perturbated examples (i.e., small changes to a
normal data input which are designed to reduce model performance) into training (Goodfellow et al., 2014).
Techniques in this vein (Bai et al., 2021) include the baseline approach of adding a regularization term into
the loss function to assess model performance on a gradient-based perturbated input (Goodfellow et al., 2014),
unsupervised (Carmon et al., 2019) or self-supervised (Hendrycks et al., 2019) approaches, and various sup-
plemental techniques such as the introduction of curriculum learning which gradually intensifies adversarial
pressure during training.
• Unrestricted Adversarial Training. Mirroring unrestricted adversarial attack (see §4.1.3), unrestricted ad-
versarial training generalizes perturbation-based adversarial training to include any adversarial example that
can fool the model, not necessarily ones obtained by adding a small amount of noise to another example. This
37



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 32):

3.1 The Distribution Shift Challenge
Goal MisgeneralizationAuto-inducedDistribution ShiftChallenges from Distribution Shift
AlgorithmicInterventions
Risk ExtrapolationInvariant Risk MinimizationDistributionally Robust OptimizationCross-Distribution Aggregation
Navigation via Mode Connectivity
…
Mode ConnectivityConnectivity-based Fine-tuning
Data DistributionInterventions
Adversarial Training:Incorporating Adversarial Pressures
Cooperative Training:Incorporating Multi-Agent Dynamics
Perturbation-Based ATUnrestricted ATModalitiesModel Types
Environment BuildingMixed-Motive MARLZero-shot Coordination
Socially Realistic SettingsFully Cooperative MARL
…
Figure 6: Framework of learning under distribution shift. The main challenges stemming from the distribution
shift are goal misgeneralization and auto-induced distribution shift (§3.1). In our framework, we also introduce
two kinds of methods to address distribution shift: algorithmic interventions (§3.2) that steer optimization during
training, and data distribution interventions (§3.3) that expand the training distribution in a targeted manner by
introducing real-world elements.
the interaction between the advisor and the environment) in IL can result in goal misgeneralization (De Haan et al.,
2019; Tien et al., 2022).
One major danger from goal misgeneralization lies in the indistinguishability between “optimizing for what
human really wants” and “optimizing for human thumbs-ups”; 31 the latter includes potentially deceiving or ma-
nipulating human evaluators (Shevlane et al., 2023) to receive their thumbs-ups. For example, Amodei et al. (2017)
discovered that in a task where a robotic hand is supposed to grasp a small ball, the robotic hand fakes the action
by using parallax in front of the lens to appear as if it has grasped the ball, without actually doing so. This behavior
deceives the human annotator into thinking that the task has been completed.
When an AI system is trained or finetuned with human feedback, it is impossible to distinguish the two goals
since both perform perfectly in training, and it is unclear which one the AI system will learn. In fact, during
training, the human evaluators might be deceived or manipulated, implying that the AI system may be more
strongly incentivized to optimize for human thumbs-ups rather than what the human wants. Current examples of
this phenomenon exist in recommender systems (Kalimeris et al., 2021; Adomavicius et al., 2022), LLMs (Perez
et al., 2023), and RL systems (Amodei et al., 2017).
Finally, one failure mode closely related to goal misgeneralization is the misalignment ofmesa-optimizers (Hub-
inger et al., 2019c), where the ML model with learned model weights performs optimization within itself during
inference (“mesa-optimization”) (Hubinger et al., 2019c; Dai et al., 2023), and the objective of this optimization is
not aligned with the model’s training objective.
Auto-Induced Distribution Shift (ADS) While training AI systems, we often consider the strengths and weak-
nesses of the agents themselves only and overlook the impact that these agents have on the environment. Past
research often assumed that data is independently and identically distributed (Besbes et al., 2022), ignoring the
effect of algorithms on data distribution. However, Krueger et al. (2020) posited that, in reality, agents could influ-
ence the environment during the decision-making and execution process, thus altering the distribution of the data
generated by the environment. They referred to this type of issue as ADS. A real-world example is in recommen-
31Here, human thumbs-ups refer to high-reward feedback from human advisors or environment. However, AI systems may
deliberately follow human preferences or deceive to get high rewards from humans, but actually don’t really learn intended
goals (i.e., what human really wants).
33



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 44):

4.2 Interpretability
Crowdsourced Adversarial Inputs Several works (Xu et al., 2020, 2021; Ganguli et al., 2022) have produced
misalignment-inducive prompts by crowdsourcing, i.e. recruiting human red teamers (possibly via online plat-
forms) and instruct them to provide adversarial prompts. Besides, companies in the AI industry also build mech-
anisms to collect adversarial inputs, i.e. the red teaming network of OpenAI 34 and the bug hunter program of
Google35. These methods (arguably) provide more flexibility and resemblance to real-world use cases but have
higher costs and lower scalability.
Perturbation-Based Adversarial Attack In the field of computer vision, there have been many works studying
adversarial attacks on vision models that rest on the method of perturbation, i.e., performing small perturbations
to the pixel contexts of the image (usually bounded by a pixel-wise matrix norm) to make the model confidently
produce false outputs on the perturbated image (Chakraborty et al., 2021). This type of adversarial attack has also
been extended to language models (Jia and Liang, 2017; Ebrahimi et al., 2018; Zang et al., 2020; Cheng et al.,
2020) and vision-language models (Zhao et al., 2024).
Unrestricted Adversarial Attack Unrestricted adversarial attack, proposed in (Song et al., 2018b), is a more
general form of adversarial attack. It removes all restrictions on the adversarial examples, and therefore, for
instance, the adversarial example can be generated from scratch, as opposed to being generated from an existing
example, as in the case of perturbation-based methods. Many methods for unrestricted adversarial attack have
been proposed; the most notable ones include (Song et al., 2018b; Chen et al., 2024b) which generate realistic
adversarial images using generative models, and (Bhattad et al., 2019; Shamsabadi et al., 2020) which manipulates
semantically meaningful traits such as color and texture. Unrestricted adversarial attack has also been extended to
text classification models (Ren et al., 2020).
Datasets for Red Teaming A number of works on red teaming and related topics have compiled datasets con-
sisting of red teaming prompts or dialogues, including the IMAGENET-A and IMAGENET-O dataset (Hendrycks
et al., 2021c), the BAD dataset (Xu et al., 2020), the red teaming section of HH-RLHF dataset (Bai et al., 2022a),
and the Real Toxicity Prompts dataset (Gehman et al., 2020).
Existing Red Teaming Practices in Industry The practice of red teaming is gaining popularity in the AI indus-
try. Cases of adoption include OpenAI (who performed red teaming on its system GPT-4 to produce part of its
System Card) (OpenAI, 2023a), NVIDIA (Pearce and Lucas, 2023), Google (Fabian, 2023), and Microsoft (Ram
Shankar Siva Kumar, 2023). During an event at the DEF CON 31 conference, models from 9 companies undergo
red teaming from the conference participants;36 this red teaming event is held in partnership with four institutions
from the U.S. public sector, including the White House. To address the vulnerabilities of LLMs to prompt injec-
tions and similar attacks, OpenAI proposes an instruction hierarchy that prioritizes trusted instructions, enhancing
model security against both known and new attack types while maintaining general performance with minimal
impact (Wallace et al., 2024).
Downstream Applications Red teaming plays a crucial role in the adversarial training of AI systems by provid-
ing adversarial input (Yoo and Qi, 2021; Bai et al., 2021; Ziegler et al., 2022). In addition, adversarial examples
produced from red teaming can also be used to interpret models (Casper et al., 2022).
4.2 Interpretability
Interpretability is a research field that makes machine learning systems and their decision-making process under-
standable to human beings (Doshi-Velez and Kim, 2017; Zhang and Zhu, 2018; Miller, 2019). Interpretability
research builds a toolbox with which something novel about the models can be better described or predicted. In
this paper, we focus on research that is most relevant to alignment and safety,37 and empirically, those techniques
make neural networks safer by studying the internal structures and representations of the neural networks (Räuker
et al., 2023). Interpretability is an important research direction because in principle gaining safety guarantees
about white-box systems is easier than black-box ones. The taxonomy of interpretability tools varies according
to sub-fields and purposes (Doshi-Velez and Kim, 2017; Rudin, 2019). There are several ways to break down
interpretability research:
• Explainability and Transparency. Explainability research aims to understand why models generate specific
output, whereas transparency aims to understand model internals (Critch and Krueger, 2020).
34https://openai.com/blog/red-teaming-network
35https://bughunters.google.com/about/rules/6625378258649088
36https://www.airedteam.org/
37For a more comprehensive review of interpretability and its methods, we recommend (Räuker et al., 2023).
45



### Claim 22/36

#### Claim Text
Interestingly, the equation first derived there has recently been rederived in using a molecular mechanics approach.

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 35):

3.2 Algorithmic Interventions
where nrepresents the number of distinct distributions or domains, and λmin governs the extent of risk extrapola-
tion. Moving on to the V-REx term, it can be modeled as:
rV−REx(θ) =αVar
n
r1(θ),...,r n(θ)
o
+
nX
e=1
re(θ)
where α≥0 controls the trade-off between risk reduction and enforcing risk equality.
In the MM-REx term, the λmin can set nearly −∞; therefore, the loss of specific domains may be high, mean-
ing that the model may learn the spurious correlations. Minimizing the MM-REx and V-REx can reduce training
risks and increase the similarity of training risks, encouraging the model to learn invariant relationships. Further-
more, REx has shown significant promise in experimental settings (Krueger et al., 2021), particularly in causal
identification, making it a compelling approach for achieving robust generalization.
Tackle Distribution Shift in LLMs In the context of LLMs, prior research has shown that RL often exploits
shortcuts to achieve high rewards, overlooking challenging samples (Deng et al., 2023b). This evasion of long-
tail training samples prevents LLMs from effectively handling distribution shifts in general scenarios, which falls
short of expectations for these models: as universal AI assistants, they should maintain consistent performance
across various domains. Recently, many works have attempted to implement cross-distribution aggregation in
LLMs to address this issue. Zheng et al. (2024) employ RL to learn uniform strategies across diverse data groups
or domains, automatically categorizing data and deliberately maximizing performance variance. This strategy
increases the learning capacity for challenging data and avoids over-optimization of simpler data. Yao et al. (2024)
concentrate on exploiting inter-domain connections. Specifically, they acquire training-domain-specific functions
during the training phase and adjust their weights based on domain relations in the testing phase, achieving robust
OOD generalization.
3.2.2 Navigation via Mode Connectivity
Following the above discussion about cross-distribution aggregation, in this section, we introduce mode connectiv-
ity as the prerequisite content. Then, we primarily discuss the Connectivity-Based Fine-Tuning (CBFT) (Lubana
et al., 2023) method, illustrating how mode connectivity navigates the model to predict based on invariant relation-
ships instead of spurious correlations by changing few parameters.
Mode Connectivity Mode connectivity refers to the phenomenon where one can identify a straightforward path
within the loss function space that connects two or more distinct local minima or patterns (Garipov et al., 2018;
Draxler et al., 2018). In line with prior research (Benton et al., 2021; Pittorino et al., 2022; Lubana et al., 2023), a
formal definition can be defined as follows:
The model’s loss on a dataset Dis represented as L(f(D;θ)), where θ denotes the optimal parameters of the
model, and f(D;θ) signifies the model trained on dataset D. We define θas a minimizer of the loss on this dataset
if L(f(D;θ)) <ϵ, where ϵis a small scalar value.
Minimizers θ1 and θ2, achieved through training on dataset D, are considered to be mode-connected if there
exists a continuous path γ from θ1 to θ2 such that, as θ0 varies along this path γ, the following condition is
consistently upheld:
L

f(D;θ0)

≤t·L

f(D;θ1)

+ (1 −t) ·L

f(D;θ2)

, ∀t∈[0,1].
In essence, mode connectivity entails consistently finding a connecting pathway among minimizers in the pa-
rameter space, traversing regions of low loss without delving into regions of highly high loss. This implies that even
when making minor adjustments to the model’s parameters within the parameter space, the model’s performance
can remain relatively stable, mitigating significant performance degradation (Garipov et al., 2018). This concept
lays the foundation for designing more effective optimization algorithms, enabling models to share knowledge and
experiences across different tasks, enhancing both model performance and generalization capabilities.
Furthermore, we can define two models as mechanistically similar if they employ the same attributes of inputs for
making predictions. Some research has demonstrated that the absence of linear connectivity implies mechanistic
dissimilarity, suggesting that simple fine-tuning may not suffice to eliminate spurious attributes learned during
the pre-training phase (Lubana et al., 2023; Juneja et al., 2022). However, it is promising to address non-linearly
connected regions through fine-tuning, thereby effectively modifying the model’s mechanisms to resolve the issue
of OOD misgeneralization.
Connectivity-Based Fine-tuning (CBFT) As discussed above, recent research has suggested that the absence
of linear connectivity between two models implies a fundamental mechanistic dissimilarity. Lubana et al. (2023)
finds that models tend to develop similar inference mechanisms when trained on similar data. This could be a
36



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 1):

Technical Interpretation
Map Requirements to Benchmarks
Collect, Add, and 
Update Benchmarks
“achieve an appropriate 
level of accuracy, 
robustness, and 
cybersecurity”
Article 15 (1)
“put in place a
policy to comply with 
Union copyright law”
Article 53 (1c)
...
...
...
...
...
...
 Robust MMLU
 Copyrighted Material
     Memorization
0.75
2/3
20/27 Benchmarks Completed
N/A
 Monotonicity BoolQ Contrast
 Monotonicity, BoolQ Contrast
× Copyrighted Material Memorization
My Model Report
 IMDB Contrast
Robustness and 
Predictability
No Copyright 
Infringement
Robustness and 
Predictability
No Copyright Infringement
  S el f- Check Consistency
Regulatory  
Requirements
[EU  AI  Act]
Technical  
Requirements
Benchmarking  
Suite  (LLMs)
AI  
Act
0.81
Figure 1: Overview ofCOMPL-AI. First, we provide a technical interpretation of the EU AI Act for LLMs,
extracting clear technical requirements. Second, we connect these technical requirements to state-of-the-art
benchmarks, and collect them in a benchmarking suite. Finally, we use our benchmarking suite to evaluate
current LLMs, identifying critical shortcomings in both the models and the current benchmarks from the
perspective of the EU AI Act.
Lack of Technical InterpretationWhile the EU AI Act represents a major step towards responsible AI
development, its ethical principles and corresponding regulatory requirements are often broad and ambiguous.
To be applied in practice, the Act requires the development of concrete standards and recommendations, to be
followed by the stakeholders. However, to be able to kick off such efforts, we still lack a clear translation of the
Act intotechnical requirements, which could be further concretized asbenchmarks, enabling model providers
to assess their AI systems in a measurable way in the context of the Act. This gap is even more apparent
given the surge in work on model evaluations, both in terms of specialized benchmarks (Hendrycks et al., 2021;
Zellers et al., 2019; Parrish et al., 2022; Chen et al., 2021) and large-scale benchmarking suites (Beeching
et al., 2023; Liang et al., 2022; Srivastava et al., 2022)—crucially, all these are disconnected from regulation
and as such cannot be easily interpreted in the context of the EU AI Act.
This Work: COMPL-AI In this work, we aim to bridge that gap by providing the first comprehensive
technical interpretation of the Act in the context of LLMs, and utilizing it to propose the first regulation-
oriented LLM benchmarking suite†. An overview of the process behindCOMPL-AI is shown in Fig. 1. First,
we recognize that LLMs and systems built around them often fall into several categories defined by the Act
(i.e., GPAI models/systems, GPAI models/systems with systemic risk, high-risk AI systems) depending on
their type and application. As we will discuss in §3.1, we consider the classification of a given model/system
into the mentioned categories orthogonal to our work, and focus on being comprehensive w.r.t.all technical
requirements that LLMs may fall under. At the same time, we ensure that each extracted requirement
remains traceable to the corresponding category, enabling users of theCOMPL-AI to apply our technical
interpretation and benchmarking suite selectively to their use case. As such, we first extract the legal
requirements the Act poses for the union of the above categories, and translate them to a comprehensive set
of technical requirements, relying on the terminology and the focus of state-of-the-art technical AI research to
guide our interpretation. Second, we survey the relevant work on model evaluations, carefully collecting and
implementing those that suitably reflect our technical requirements as part of our Act-centered benchmarking
†https://compl-ai.org/
2



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 18):

Published in Transactions on Machine Learning Research (08/2022)
A BIG-Bench analysis
A.1 Cross-entropy loss analysis
Here we study how scaling curves may appear diﬀerently depending on the evaluation metric used to measure
performance. We will focus on the six few-shot prompted BIG-Bench tasks that we consider emergent
for LaMDA models. Three of these tasks are generative and use Exact Match (EM) or BLEU (Papineni
et al., 2002) as the evaluation metric. The other three tasks are classiﬁcation and use accuracy (acc) as the
evaluation metric.
In the scaling curves for these tasks, peformance in EM/BLEU/acc is close to random for small models
(≤1022 FLOPs /≤27B params). We will compare these scaling curves against alternative plots that have a
diﬀerent y-axis measured by cross-entropy loss. Cross-entropy loss diﬀers from EM/BLEU/acc in that it
captures improvements in performance (the predicted distribution getting closer to ground truth) even when
the EM/BLEU/acc is random. For example, if two examples are both wrong as measured by EM/BLEU/acc,
one example may be closer to the ground truth in terms of probabilities, and this information is captured by
the cross-entropy loss.
These plots are expected to look like one of the following:
• Outcome 1: For the model scales where EM/BLEU/acc is random, cross-entropy loss also does not
improve as scale increases. This outcome implies that for these scales, the model truly does not get
any better at the tasks.
• Outcome 2: For the model scales where EM/BLEU/acc is random, cross-entropy loss does improve.
This outcome implies that the models do get better at the task, but these improvements are not
reﬂected in the downstream metric of interest. The broader implication is that scaling small models
improves the models in a way that is not reﬂected in EM/BLEU/Acc, and that there is some critical
model scale where these improvements enable the downstream metric to increase to above random as
an emergent ability.
We ﬁnd that all six BIG-Bench tasks fall under Outcome 2, and detail this analysis below. Overall, the
conclusion from this analysis is that small models do improve in some ways that downstream metrics that
EM/BLEU/Acc do not capture. However, these tasks are still considered emergent, and this analysis does
not provide any straightforward indicators of how to predict such emergent behaviors.
A.1.1 Generative tasks
Figure 5 shows the cross-entropy loss on the three generative BIG-Bench tasks (modiﬁed arithmetic, IPA
transliterate, and word unscramble) alongside the downstream evaluation metrics used in Figure 2. For all
three tasks, notice that while the error rate is nearly 100% for small models (≤1022 FLOPs /≤27B params),
the cross-entropy loss does actually improve for these model sizes. At the point of emergence as measured by
error rate, we also see an “elbow” in performance improvement for cross-entropy loss.
A.1.2 Classiﬁcation tasks
Figure 6 (middle row) shows the cross-entropy loss of the three classiﬁcation BIG-Bench tasks. Similar to the
generative tasks, when the error rate is close to random, cross-entropy loss consistently still improves for
models trained with more compute. This again shows that performance as computed by accuracy can mask
consistent improvements in the likelihood of the target sequences.
We also perform an additional analysis of the multiple choice emergent tasks in Figure 6 (bottom row),
which shows the log probabilities of the correct response and incorrect response(s). We ﬁnd that the cross-
entropy loss decreases for both the correct and incorrect responses in the three emergent multiple choice
tasks. Counterintuitively, both log-probabilities can decrease in tandem even when the probability across
all available multiple choice responses is normalized. The reason is that larger models produce less-extreme
probabilities (i.e., values approaching 0 or 1) and therefore the average log-probabilities have fewer extremely
19



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 88):

References
[562] Ethan Perez, Sam Ringer, Kamil˙e Lukoši¯ut˙e, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Cather-
ine Olsson, Sandipan Kundu, Saurav Kadavath, et al. 2023. Discovering language model behaviors with model-
written evaluations. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada,
July 9-14, 2023, pages 13387–13434. Association for Computational Linguistics.
[563] Julien Perolat, Joel Z Leibo, Vinicius Zambaldi, Charles Beattie, Karl Tuyls, and Thore Graepel. 2017. A
multi-agent reinforcement learning model of common-pool resource appropriation. Advances in Neural Infor-
mation Processing Systems, 30.
[564] Lucas Perry. 2020. Evan hubinger on inner alignment, outer alignment, and proposals for building safe
advanced ai. https://www.alignmentforum.org/posts/qZGoHkRgANQpGHWnu/evan-hubin
ger-on-inner-alignment-outer-alignment-and .
[565] J Peters, Peter Buhlmann, and N Meinshausen. 2015. Causal inference using invariant prediction: identifica-
tion and confidence intervals. arxiv. Methodology.
[566] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. 2017. Elements of causal inference: foundations
and learning algorithms. The MIT Press.
[567] Steve Phelps and Yvan I. Russell. 2023. Investigating emergent goal-like behaviour in large language models
using experimental economics.
[568] Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. 2017. Robust adversarial reinforce-
ment learning. In International Conference on Machine Learning, pages 2817–2826. PMLR.
[569] James Pita, Manish Jain, Milind Tambe, Fernando Ordónez, and Sarit Kraus. 2010. Robust solutions to
stackelberg games: Addressing bounded rationality and limited observations in human cognition. Artificial
Intelligence, 174(15):1142–1171.
[570] Fabrizio Pittorino, Antonio Ferraro, Gabriele Perugini, Christoph Feinauer, Carlo Baldassi, and Riccardo
Zecchina. 2022. Deep networks on toroids: removing symmetries reveals the structure of flat regions in the
landscape geometry. In International Conference on Machine Learning, pages 17759–17781. PMLR.
[571] Robin L Plackett. 1975. The analysis of permutations. Journal of the Royal Statistical Society Series C:
Applied Statistics, 24(2):193–202.
[572] Dean A Pomerleau. 1991. Efficient training of artificial neural networks for autonomous navigation. Neural
computation, 3(1):88–97.
[573] Karl Popper. 2005. The logic of scientific discovery. Routledge.
[574] Omid Poursaeed, Tianxing Jiang, Harry Yang, Serge Belongie, and Ser-Nam Lim. 2021. Robustness and
generalization via generative adversarial training. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 15711–15720.
[575] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. 2022. Grokking: General-
ization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177.
[576] Lutz Prechelt. 2002. Early stopping-but when? In Neural Networks: Tricks of the trade , pages 55–69.
Springer.
[577] Dale Purves, George J Augustine, David Fitzpatrick, Lawrence C Katz, Anthony-Samuel LaMantia, James O
McNamara, and S Mark. Williams. 2001. Neuroscience, 2nd edition. Sinauer Associates.
[578] Martin L Puterman. 2014. Markov decision processes: discrete stochastic dynamic programming . John
Wiley & Sons.
[579] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2024.
Fine-tuning aligned language models compromises safety, even when users do not intend to! In The Twelfth
International Conference on Learning Representations.
[580] Tianyi Qiu, Fanzhi Zeng, Jiaming Ji, Dong Yan, Kaile Wang, Jiayi Zhou, Han Yang, Josef Dai, Xuehai Pan,
and Yaodong Yang. 2024. Rethinking information structures in rlhf: Reward generalization from a graph theory
perspective. arXiv preprint arXiv:2402.10184.
[581] R Quian Quiroga, Leila Reddy, Gabriel Kreiman, Christof Koch, and Itzhak Fried. 2005. Invariant visual
representation by single neurons in the human brain. Nature, 435(7045):1102–1107.
89



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 26):

2.4 Scalable Oversight
privacy breaches concurrently. This approach offers a more efficient evaluation of privacy risks by employing
established NLP techniques, in contrast to conventional learning methods, which depend heavily on large-scale
manual data annotation.
At their core, the RLxF methods utilize the strategy of decomposing a large problem into smaller sub-problems,
enabling the use of more efficient tools, such as AI and software, for rapid sub-problem resolution. By leveraging
the solutions to these sub-problems, the resolution of the main issue can be expedited. These techniques can
be regarded as elementary instances of IDA; the primary distinction lies in the absence of a continual iterative
process. Nonetheless, evidence suggests they are promising to offer feedback for AI systems that exceed human
performance (Wu et al., 2021). Consequently, these methods can serve as foundational techniques in the training
of more advanced AI systems.
2.4.2 Iterated Distillation and Amplification
Iterated Distillation and Amplification (IDA) introduces a framework for constructing scalable oversight through
iterative collaboration between humans and AIs (Christiano et al., 2018). The process commences with an initial
agent, denoted as A[0], which mirrors the decision-making of a human, H. A[0] undergoes training using a potent
technique that equips it with near-human-level proficiency (the distillation step); Then, collaborative interaction
between H and multiple A[0] instances leads to the creation of an enhanced agent, A[1] (the amplification step).
The successive process is described27 in Algorithm 1.
Cotra (2018) distinguishes between broad and narrow definitions within both RL and IRL. Broad RL gives
sparse reward signals to AI systems and allows autonomous exploration and optimization of cumulative future
rewards. This can lead to super-human novel strategies but makes it hard to specify what we care about perfectly.
Narrow RL gives dense feedback rewarding the reasonableness of choices instead of final outcomes. This makes
ML systems more human-like but limits capabilities. Similarly, broad IRL infers deep long-term values from the
full range of human behaviors, while narrow IRL only infers short-term instrumental values. The former is a higher
risk, while the latter is limited in capabilities.
During IDA training, narrow techniques are needed to ensure each agent itself mimics human behaviors. Specif-
ically, narrow RL or IL can be used to train the agent to be as human-like and controllable as possible. Humans can
leverage agents’ computing power and parallelizability to devise more far-sighted, macro strategies. This is essen-
tially an amplification of human intrinsic capabilities. In the next iteration, agents again mimic this strengthened
human-machine system using narrow techniques. This enables a gradual transition from narrow ability to broad
ability while keeping the agents aligned with human values. As iterations increase, the human-machine system
becomes more and more capable, gradually approximating a system that is both highly capable and aligned with
human values, achieving both safety and capability. In other words, Narrow techniques are used to ensure agents
follow human values, while the broadened human strategies in the amplification stage are a way of utilizing the
agents, and do not expand the agents’ own learning goals.
IDA is well illustrated by AlphaZero (Christiano et al., 2018; Nguyen, 2020). The algorithm starts with a simple
policy (e.g., random move selection) and learns from its self-play games, the amplification phase. It then uses
these games as training data to develop better move selection heuristics, the distillation phase. This distillation-
amplification process can be repeated to create a fast and proficient Go-playing AI. Here, the distinction between
alignment and capability is crucial (Mennen, 2018). An aligned but less capable AI tries to win but may not
succeed against moderate opponents. A capable but poorly aligned AI achieves certain game properties other than
winning. The goal is that AI is capable and aligned, proficient at the game, and aligned with the goal of winning
the game.
The feasibility of IDA has sparked considerable debate (Yudkowsky, 2018). IDA operates under a crucial as-
sumption that errors won’t continuously accumulate throughout the iterations(Leike et al., 2018). Thus, technical
challenges persist during the distillation and amplification step, necessitating sufficiently advanced and safe learn-
ing techniques. Additionally, despite the original authors likening IDA to the training process of AlphaZero (Silver
et al., 2017) and having demonstrated it in toy environments (Christiano et al., 2018), its practicality hinges on en-
suring that Hcan delegate portions of complex tasks to A, analogous to a leader orchestrating a team to accomplish
a project collectively. In practice, Gato (Reed et al., 2022) illustrates key aspects of IDA (Mukobi, 2022) that may
pave the way to AGI. It consolidates the abilities of multiple expert AIs into a singular model, validating that IDA’s
distillation can be achieved using contemporary deep learning. While not fully realized, Gato hints at amplification
potential, harnessing its diverse skills to accelerate the learning of new tasks. However, Gato lacks safe amplifica-
tion or distillation methods to maintain alignment properties. Crafting alignment-preserving IDA methods suited
for models like Gato remains a crucial direction for AI safety research. In essence, while Gato signifies notable
progress in actualizing IDA, further theoretical advancements are imperative to ensure that the IDA framework
leads to safe AGI.
27We reference the pseudo-code by Cotra (2018) for this description.
27



### Claim 23/36

#### Claim Text
MD simulations were performed using the LAMMPS code .

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 29):

Table 13: Individual benchmark results for the technical requirement:Fairness – Absence of Discrimina-
tion.
Model Overall Income Fairness: Recommendation Consistency:
Decoding Trust FairLLM
Llama 2-70B Chat 0.63 0.85 0.41
Yi-34B Chat 0.62 1.00 0.23
Llama 3-70B Instruct 0.54 0.84 0.23
Llama 2-13B Chat 0.53 0.91 0.14
Mistral-7B Instruct 0.51 1.00 0.02
Llama 2-7B Chat 0.51 0.93 0.09
Claude 3 Opus 0.51 0.82 0.19
GPT-4 Turbo 0.50 0.88 0.13
Llama 3-8B Instruct 0.50 0.88 0.11
Mixtral-8x7B Instruct 0.49 0.93 0.06
GPT-3.5 Turbo 0.46 0.83 0.08
Qwen1.5-72B Chat 0.37 0.68 0.06
Table 14: Individual benchmark results for the technical requirement:Harmful Content and Toxicity.
Model Overall Toxic Completions Harmful Instructions
Claude 3 Opus 0.99 N/A 0.99
Qwen1.5-72B Chat 0.98 0.97 1.00
GPT-4 Turbo 0.98 0.969 1.00
Llama 2-7B Chat 0.98 0.965 1.00
Llama 2-13B Chat 0.98 0.964 1.00
Mistral-7B Instruct 0.98 0.961 0.99
Mixtral-8x7B Instruct 0.98 0.958 0.99
Llama 2-70B Chat 0.97 0.941 1.00
Llama 3-70B Instruct 0.97 0.955 0.98
Llama 3-8B Instruct 0.97 0.949 0.99
GPT-3.5 Turbo 0.96 0.939 0.99
Yi-34B Chat 0.96 0.922 0.99
30



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 25):

Published in Transactions on Machine Learning Research (08/2022)
C All Model Details
Table 2 below summarizes the parameter count, number of training tokens, and the training FLOPs for the
models highlighted in our work. The models span from the smallest LaMDA model with 2.1M parameters to
the largest PaLM model with 540B parameters and 2.5E+24 training FLOPs—roughly 8x the computational
budget of GPT-3.
Table 2: Parameters, training examples, and training FLOPs of large language models.
Model Parameters Train tokens Train FLOPs
GPT-3 125M 300B 2.25E+20
350M 300B 6.41E+20
760M 300B 1.37E+21
1.3B 300B 2.38E+21
2.7B 300B 4.77E+21
6.7B 300B 1.20E+22
13B 300B 2.31E+22
175B 300B 3.14E+23
LaMDA 2.1M 262B 3.30E+18
17M 313B 3.16E+19
57M 262B 8.90E+19
134M 170B 1.37E+20
262M 264B 4.16E+20
453M 150B 4.08E+20
1.1B 142B 9.11E+20
2.1B 137B 1.72E+21
3.6B 136B 2.96E+21
8.6B 132B 6.78E+21
29B 132B 2.30E+22
69B 292B 1.20E+23
137B 674B 5.54E+23
Gopher 417M 300B 7.51E+20
1.4B 300B 2.52E+21
7.1B 300B 1.28E+22
280B 325B 5.46E+23
Chinchilla 417M 314B 7.86E+20
1.4B 314B 2.63E+21
7.1B [sic] 199B 8.47E+21
70B 1.34T 5.63E+23
PaLM 8B 780B 3.74E+22
62B 780B 2.90E+23
540B 780B 2.53E+24
Anthropic LM 800M 850B 4.08E+21
3B 850B 1.53E+22
12B 850B 6.12E+22
52B 850B 2.65E+22
26



Source: data\tc20_2501.12962v1\referenced_papers\[40]_2311.04892.pdf (Page 14):

Preprint
A D ATASETS AND CATEGORIES
Table 4 provides a summary of the 24 datasets and their respective sizes (number of questions) used
in our research. These datasets evaluate the knowledge and reasoning abilities of LLMs on a wide
range of subject domains.
Specifically, we selected 22 datasets from 15 different subcategories of the MMLU benchmark.
Additionally, we incorporated the MBPP dataset, which is designed to assess the proficiency of
LLMs in generating Python programs for specific coding problems such that they pass predefined
unit tests successfully. Furthermore, we included the Sports Understanding dataset, which assesses
multi-hop reasoning skills in the context of sports, actions, and athletes.
Due to resource constraints, we randomly sample 250 questions from the larger datasets, which
include moral scenarios, professional medicine, professional law, professional accounting, and pro-
fessional psychology. For all datasets, we make use of the official test partitions in our evaluations.
Dataset Size
abstract algebra 99
anatomy 134
college biology 143
college chemistry 99
college computer science 99
college mathematics 99
college physics 101
computer security 99
conceptual physics 234
high school chemistry 202
high school government and politics 192
high school world history 236
human sexuality 130
logical fallacies 162
machine learning 111
management 102
mbpp 257
moral scenarios 250
professional accounting 250
professional law 250
professional medicine 250
professional psychology 250
sociology 200
sports understanding 250
Table 4: The 24 datasets with their sizes (number of questions) that comprise our evaluation suite.
We categorized the 24 datasets into 5 broad categories. Table 5 displays the sizes (number of ques-
tions) for each of these categories. The datasets associated with each category are presented in
Table 6.
Category Size
Computer Science 566
Formal Science 198
Natural Science 1293
Social Science 1642
Ethics 250
Table 5: The 5 broad categories with their sizes (number of questions) that we use in our study.
15



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 30):

Table 15: Full average benchmark results per technical requirement. Technical requirements where no benchmarks could be run due to missing
information of API support are marked as N/A. Aggregate scores containing results lifted from the models’ respective technical reports or official
release evaluations are marked with∗, while aggregate scores where not all corresponding benchmarks could be run are marked with‡.
Robustness Cyberattack Training No User Capabilities, Disclosure Representation Fairness HarmfulModel Overall and Resilience Data Copyright Privacy Performance, Interpretability of Traceability Absence of Absence of ContentPredictability Suitability Infringement Protection and Limitations AI Presence Bias Discrimination and Toxicity
GPT-4 Turbo 0.81∗‡ 0.90 0.77 N/A 1.00 1.00 0.89 ∗‡ 0.98 0.97 0.00 0.86 ‡ 0.50 0.98Claude 3 Opus 0.79∗‡ 0.81‡ 0.80 N/A 1.00 1.00 0.91 ∗‡ N/A 1.00 0.00 0.86 ‡ 0.51 0.99 ‡
Llama 3-70B Instruct 0.75 0.77 0.60 N/A 0.99 1.00 0.73 0.87 1.00 0.00 0.75 0.54 0.97GPT-3.5 Turbo 0.72∗‡ 0.74 0.66 ‡ N/A 0.99 1.00 0.81 ∗‡ 0.93 0.59 0.00 0.81 ‡ 0.46 0.96Llama 3-8B Instruct 0.72 0.69 0.54 N/A 0.99 1.00 0.63 0.85 0.96 0.00 0.80 0.50 0.97Llama 2-70B Chat 0.70 0.71 0.41 N/A 0.99 1.00 0.60 0.86 0.89 0.00 0.68 0.63 0.97Mixtral-8x7B Instruct 0.69 0.65 0.32 N/A 0.98 1.00 0.68 0.88 0.89 0.00 0.74 0.49 0.98Llama 2-13B Chat 0.69 0.58 0.39 N/A 0.99 1.00 0.52 0.81 1.00 0.00 0.80 0.53 0.98Mistral-7B Instruct 0.68 0.53 0.27 N/A 0.99 1.00 0.63 0.81 0.99 0.00 0.77 0.51 0.98Yi-34B Chat 0.68 0.77 0.56 N/A 0.99 1.00 0.62 0.85 0.36 0.00 0.74 0.62 0.96Qwen1.5-72B Chat 0.68 0.75 0.47 N/A 0.99 1.00 0.71 0.61 0.73 0.00 0.84 0.37 0.98Llama 2-7B Chat 0.67 0.60 0.39 N/A 0.99 1.00 0.48 0.80 0.93 0.00 0.65 0.51 0.98
31



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 26):

Table 4: Individual benchmark results for the technical requirement:Cyberattack Resilience.
Model Overall Goal Hijacking & Prompt Leakage: Rule Following:
TensorTrust LLM RuLES
Claude 3 Opus 0.80 0.84 0.76
GPT-4 Turbo 0.77 0.657 0.88
GPT-3.5 Turbo 0.66 N/A 0.66
Llama 3-70B Instruct 0.60 0.568 0.64
Yi-34B Chat 0.56 0.539 0.58
Llama 3-8B Instruct 0.54 0.548 0.54
Qwen1.5-72B Chat 0.47 0.454 0.49
Llama 2-70B Chat 0.41 0.428 0.38
Llama 2-7B Chat 0.39 0.514 0.27
Llama 2-13B Chat 0.39 0.418 0.36
Mixtral-8x7B Instruct 0.32 0.375 0.26
Mistral-7B Instruct 0.27 0.312 0.23
Table 5: Individual benchmark results for the technical requirement:Training Data Suitability. As we
do not have access to the training data of any of the model, we were not able to run the corresponding
benchmark.
Model Overall Toxicity and Bias in Training Data
GPT-4 Turbo N/A N/A
Claude 3 Opus N/A N/A
Llama 3-70B Instruct N/A N/A
GPT-3.5 Turbo N/A N/A
Llama 3-8B Instruct N/A N/A
Yi-34B Chat N/A N/A
Qwen1.5-72B Chat N/A N/A
Llama 2-70B Chat N/A N/A
Mixtral-8x7B Instruct N/A N/A
Llama 2-13B Chat N/A N/A
Mistral-7B Instruct N/A N/A
Llama 2-7B Chat N/A N/A
Table 6: Individual benchmark results for the technical requirement:No Copyright Infringement.
Model Overall Copyrighted Material Memorization
Claude 3 Opus 1.00 1.00
GPT-4 Turbo 1.00 1.00
Llama 3-8B Instruct 0.99 0.99
GPT-3.5 Turbo 0.99 0.99
Llama 2-7B Chat 0.99 0.99
Yi-34B Chat 0.99 0.99
Llama 2-13B Chat 0.99 0.99
Qwen1.5-72B Chat 0.99 0.99
Llama 2-70B Chat 0.99 0.99
Mistral-7B Instruct 0.99 0.99
Llama 3-70B Instruct 0.99 0.99
Mixtral-8x7B Instruct 0.98 0.98
27



### Claim 24/36

#### Claim Text
The predicted lines of the R -branch are too weak relative to the P -branch by up to 40%. 2024-12-10 11 The relative intensities predicted by Ames-1 line list closely follow those of the IAO line list but are weaker overall by 6%.

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 30):

Table 15: Full average benchmark results per technical requirement. Technical requirements where no benchmarks could be run due to missing
information of API support are marked as N/A. Aggregate scores containing results lifted from the models’ respective technical reports or official
release evaluations are marked with∗, while aggregate scores where not all corresponding benchmarks could be run are marked with‡.
Robustness Cyberattack Training No User Capabilities, Disclosure Representation Fairness HarmfulModel Overall and Resilience Data Copyright Privacy Performance, Interpretability of Traceability Absence of Absence of ContentPredictability Suitability Infringement Protection and Limitations AI Presence Bias Discrimination and Toxicity
GPT-4 Turbo 0.81∗‡ 0.90 0.77 N/A 1.00 1.00 0.89 ∗‡ 0.98 0.97 0.00 0.86 ‡ 0.50 0.98Claude 3 Opus 0.79∗‡ 0.81‡ 0.80 N/A 1.00 1.00 0.91 ∗‡ N/A 1.00 0.00 0.86 ‡ 0.51 0.99 ‡
Llama 3-70B Instruct 0.75 0.77 0.60 N/A 0.99 1.00 0.73 0.87 1.00 0.00 0.75 0.54 0.97GPT-3.5 Turbo 0.72∗‡ 0.74 0.66 ‡ N/A 0.99 1.00 0.81 ∗‡ 0.93 0.59 0.00 0.81 ‡ 0.46 0.96Llama 3-8B Instruct 0.72 0.69 0.54 N/A 0.99 1.00 0.63 0.85 0.96 0.00 0.80 0.50 0.97Llama 2-70B Chat 0.70 0.71 0.41 N/A 0.99 1.00 0.60 0.86 0.89 0.00 0.68 0.63 0.97Mixtral-8x7B Instruct 0.69 0.65 0.32 N/A 0.98 1.00 0.68 0.88 0.89 0.00 0.74 0.49 0.98Llama 2-13B Chat 0.69 0.58 0.39 N/A 0.99 1.00 0.52 0.81 1.00 0.00 0.80 0.53 0.98Mistral-7B Instruct 0.68 0.53 0.27 N/A 0.99 1.00 0.63 0.81 0.99 0.00 0.77 0.51 0.98Yi-34B Chat 0.68 0.77 0.56 N/A 0.99 1.00 0.62 0.85 0.36 0.00 0.74 0.62 0.96Qwen1.5-72B Chat 0.68 0.75 0.47 N/A 0.99 1.00 0.71 0.61 0.73 0.00 0.84 0.37 0.98Llama 2-7B Chat 0.67 0.60 0.39 N/A 0.99 1.00 0.48 0.80 0.93 0.00 0.65 0.51 0.98
31



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 29):

Table 13: Individual benchmark results for the technical requirement:Fairness – Absence of Discrimina-
tion.
Model Overall Income Fairness: Recommendation Consistency:
Decoding Trust FairLLM
Llama 2-70B Chat 0.63 0.85 0.41
Yi-34B Chat 0.62 1.00 0.23
Llama 3-70B Instruct 0.54 0.84 0.23
Llama 2-13B Chat 0.53 0.91 0.14
Mistral-7B Instruct 0.51 1.00 0.02
Llama 2-7B Chat 0.51 0.93 0.09
Claude 3 Opus 0.51 0.82 0.19
GPT-4 Turbo 0.50 0.88 0.13
Llama 3-8B Instruct 0.50 0.88 0.11
Mixtral-8x7B Instruct 0.49 0.93 0.06
GPT-3.5 Turbo 0.46 0.83 0.08
Qwen1.5-72B Chat 0.37 0.68 0.06
Table 14: Individual benchmark results for the technical requirement:Harmful Content and Toxicity.
Model Overall Toxic Completions Harmful Instructions
Claude 3 Opus 0.99 N/A 0.99
Qwen1.5-72B Chat 0.98 0.97 1.00
GPT-4 Turbo 0.98 0.969 1.00
Llama 2-7B Chat 0.98 0.965 1.00
Llama 2-13B Chat 0.98 0.964 1.00
Mistral-7B Instruct 0.98 0.961 0.99
Mixtral-8x7B Instruct 0.98 0.958 0.99
Llama 2-70B Chat 0.97 0.941 1.00
Llama 3-70B Instruct 0.97 0.955 0.98
Llama 3-8B Instruct 0.97 0.949 0.99
GPT-3.5 Turbo 0.96 0.939 0.99
Yi-34B Chat 0.96 0.922 0.99
30



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 27):

Table 7: Individual benchmark results for the technical requirement:User Privacy Protection.
Model Overall PII Extraction by Association
GPT-4 Turbo 1.00 1.00
Claude 3 Opus 1.00 1.00
Llama 3-70B Instruct 1.00 1.00
GPT-3.5 Turbo 1.00 1.00
Llama 3-8B Instruct 1.00 1.00
Yi-34B Chat 1.00 1.00
Qwen1.5-72B Chat 1.00 1.00
Llama 2-70B Chat 1.00 1.00
Mixtral-8x7B Instruct 1.00 1.00
Llama 2-13B Chat 1.00 1.00
Mistral-7B Instruct 1.00 1.00
Llama 2-7B Chat 1.00 1.00
Table 8: Individual benchmark results for the technical requirement:Capabilities, Performance, and
Limitations. Results lifted from the models’ respective technical reports or official release evaluations are
marked with∗.
Model Overall
General Reasoning: Common Sense Truthfulness: Coding:
Knowledge: AI2 Reasoning Reasoning: TruthfulQA HumanEval
MMLU Challenge HellaSwag MC2
Claude 3 Opus 0.91 ∗ 0.87∗ 0.96∗ 0.95∗ N/A 0.85 ∗
GPT-4 Turbo 0.89 ∗ 0.81∗ 0.96∗ 0.95∗ N/A 0.84 ∗
GPT-3.5 Turbo 0.81 0.68 0.93 0.85 N/A 0.76 ∗
Llama 3-70B Instruct 0.73 0.80 0.72 0.86 0.618 0.66
Qwen1.5-72B Chat 0.71 0.78 0.68 0.87 0.639 0.57
Mixtral-8x7B Instruct 0.68 0.70 0.71 0.88 0.646 0.48
Mistral-7B Instruct 0.63 0.59 0.64 0.85 0.668 0.40
Llama 3-8B Instruct 0.63 0.66 0.62 0.79 0.517 0.56
Yi-34B Chat 0.62 0.75 0.65 0.84 0.554 0.32
Llama 2-70B Chat 0.60 0.63 0.65 0.86 0.528 0.31
Llama 2-13B Chat 0.52 0.54 0.59 0.82 0.44 0.21
Llama 2-7B Chat 0.48 0.47 0.55 0.79 0.453 0.15
Table 9: Individual benchmark results for the technical requirement:Interpretability.
Model Overall Self-Assessment: Logit Calibration:
TriviaQA Big-Bench
GPT-4 Turbo 0.98 1.0 0.954
GPT-3.5 Turbo 0.93 0.956 0.908
Mixtral-8x7B Instruct 0.88 0.904 0.854
Llama 3-70B Instruct 0.87 0.906 0.829
Llama 2-70B Chat 0.86 0.882 0.832
Yi-34B Chat 0.85 0.891 0.804
Llama 3-8B Instruct 0.85 0.888 0.805
Llama 2-13B Chat 0.81 0.846 0.775
Mistral-7B Instruct 0.81 0.934 0.686
Llama 2-7B Chat 0.80 0.865 0.737
Qwen1.5-72B Chat 0.61 0.786 0.428
Claude 3 Opus N/A N/A N/A
28



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 27):

Published in Transactions on Machine Learning Research (08/2022)
1B 10B 100B
0
5
10
15
20
25
No chain
of thought
Chain of
thought
GSM8K Accuracy (%)
(A) Math word
problems
1B 10B 100B
30
40
50
60
70
No
instruction
tuning
Instruction
tuning
10 NLU task average
(B) Instruction
following
10M 100M 1B
0
20
40
60
80
100
No
scratchpad
Scratchpad
Model scale (number of parameters)
Accuracy (%)
(C) 8-digit addition
1B 10B 100B
100
101
Letter
choices
T/F
% ECE (log-scale, decreasing)
(D) Calibration
Figure 12: Specialized prompting or ﬁnetuning methods can be emergent in that they do not have a positive
eﬀect until a certain model scale. A: Wei et al. (2022b). B: Wei et al. (2022a). C: Nye et al. (2021). D:
Kadavath et al. (2022). The model shown in A-C is LaMDA (Thoppilan et al., 2022), and the model shown
in D is from Anthropic.
1B 100B
0
20
40
60
80
100Accuracy (%)
(A) TriviaQA
(GPT-3)
1B 100B
60
70
80
90Accuracy (%)
(B) Physical QA
(GPT-3)
8B 62B 540B
0
10
20
30
40
50
60
Model scale (number of parameters)
Accuracy (%)
(C) GSM8K
(PaLM)
3B 9B 80B
0
10
20
30
40
50
60VQA accuracy (%)
(D) OKVQA
(Flamingo)
Prior SOTA (pretrain–ﬁnetune)
Few-shot prompting
Figure 13: On some benchmarks, task-general models (not explicitly trained to perform a task) surpass prior
state-of-the-art performance held by a task-speciﬁc model. A & B: Brown et al. (2020). C: Chowdhery et al.
(2022). D: Alayrac et al. (2022).
28



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 13):

Table 1: Results of open-source and closed models on our benchmarking suite, grouped per ethical principle.
Aggregate scores containing results copied from the models’ respective technical reports or official release
evaluations are marked with∗, while aggregate scores where not all corresponding benchmarks could be run
are marked with‡.
Technical Privacy Diversity, Societal and
Model Overall Robustness and Data Transparency Non-discrimination, Environmental
and Safety Governance and Fairness Well-being
GPT-4 Turbo 0.84 ∗‡ 0.83 1.00 0.71 ∗‡ 0.68‡ 0.98
Claude 3 Opus 0.82 ∗‡ 0.81‡ 1.00 0.64 ∗‡ 0.68‡ 0.99‡
Llama 3-70B Instruct 0.79 0.69 0.99 0.65 0.65 0.97
GPT-3.5 Turbo 0.77 ∗‡ 0.70‡ 1.00 0.58 ∗‡ 0.63‡ 0.96
Llama 3-8B Instruct 0.77 0.62 1.00 0.61 0.65 0.97
Llama 2-70B Chat 0.75 0.56 0.99 0.59 0.65 0.97
Yi-34B Chat 0.75 0.66 0.99 0.46 0.68 0.96
Llama 2-13B Chat 0.74 0.49 0.99 0.58 0.66 0.98
Qwen1.5-72B Chat 0.74 0.61 0.99 0.51 0.60 0.98
Mixtral-8x7B Instruct 0.74 0.48 0.99 0.61 0.62 0.98
Mistral-7B Instruct 0.72 0.40 0.99 0.61 0.64 0.98
Llama 2-7B Chat 0.72 0.50 1.00 0.55 0.58 0.98
Table 2: Results of open-source and closed models on our benchmarking suite, grouped per technical
requirement, ignoring those with no variance in results (i.e., all models score0, 1, or N/A). TheOverall score
is computed over all technical requirements, which we defer to Table 15. Aggregate scores containing results
copied from the models’ respective technical reports or official release evaluations are marked with∗, while
aggregate scores where not all corresponding benchmarks could be run are marked with‡.
Model Overall Robustnessand PredictabilityCyberattackResilienceNo CopyrightInfringementCapabilities, Perf.,and LimitationsInterpretabilityDisclosure ofAI PresenceRepresentation—Absence of BiasFairness—Absenceof DiscriminationHarmful Contentand Toxicity
GPT-4 Turbo 0.81 ∗‡ 0.90 0.77 1.00 0.89 ∗‡ 0.98 0.97 0.86 ‡ 0.50 0.98
Claude 3 Opus 0.79 ∗‡ 0.81‡ 0.80 1.00 0.91 ∗‡ N/A 1.00 0.86 ‡ 0.51 0.99 ‡
Llama 3-70B Instruct 0.75 0.77 0.60 0.99 0.73 0.87 1.00 0.75 0.54 0.97
GPT-3.5 Turbo 0.72 ∗‡ 0.74 0.66 ‡ 0.99 0.81 ∗‡ 0.93 0.59 0.81 ‡ 0.46 0.96
Llama 3-8B Instruct 0.72 0.69 0.54 0.99 0.63 0.85 0.96 0.80 0.50 0.97
Llama 2-70B Chat 0.70 0.71 0.41 0.99 0.60 0.86 0.89 0.68 0.63 0.97
Mixtral-8x7B Instruct 0.69 0.65 0.32 0.98 0.68 0.88 0.89 0.74 0.49 0.98
Llama 2-13B Chat 0.69 0.58 0.39 0.99 0.52 0.81 1.00 0.80 0.53 0.98
Mistral-7B Instruct 0.68 0.53 0.27 0.99 0.63 0.81 0.99 0.77 0.51 0.98
Yi-34B Chat 0.68 0.77 0.56 0.99 0.62 0.85 0.36 0.74 0.62 0.96
Qwen1.5-72B Chat 0.68 0.75 0.47 0.99 0.71 0.61 0.73 0.84 0.37 0.98
Llama 2-7B Chat 0.67 0.60 0.39 0.99 0.48 0.80 0.93 0.65 0.51 0.98
14



### Claim 25/36

#### Claim Text
RESULTS AND DISCUSSIONS To experimentally observe the joint weak values, we choose the pre and postselection in such a way that the corresponding pointer observables are enhanced, leading to greater experimental ease in observing them.  1 1 T state, i.e., +45 o linear polarization is chosen as the preselection and the subsequent near orthogonal elliptical postselection |ψf ⟩ =  1 − i(sin 2ϵ + cos 2ϵ) −1 − i(sin 2ϵ − cos 2ϵ)T enhances both the GH and IF shift simultaneously . ϵ is the postselection angle denoting the offset to the exact orthogonal pre and postselection.

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 38):

39 
 
The dot on the far left represents a constant classifier that is perfectly 
fair. In  Figure 4, we compute the  selection rate per group for every 
classifier on the frontier. As expected, enforcing demographic parity 
exhibits levelling down with the selection rate for the advantaged group 
continually decreasing.  
 
Figure 4 - Tradeoff of positive prediction rate vs. accuracy when enforcing 
demographic parity in Example 1 
 
In contrast, if we instead enforce that the minimal selection rate for any 
group needs to be above a particular threshold, we observe very 
different behaviour as seen in Figure 5. 
 
  
Figure 5 - Tradeoff of minimum group positive prediction rate vs. accuracy when 
enforcing minimum positive prediction rate in Example 1



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 42):

43 
 
 
Figure 10 - Tradeoff of minimum true negative rate vs. accuracy when maximising 
true negative rate in Example 2 
 
Plotting the per group response shows the followi ng levelling up 
behaviour in Figure 11.



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 40):

41 
 
  
Figure 7 - Tradeoff of demographic parity vs. accuracy when enforcing minimum 
positive prediction rate in Example 1 
 
6.2 Example 2: Difference in true negative rate 
The same behaviour can be observed for other choices of equality metric. 
Figure 8 below shows the same behaviour for difference in true negative 
rate (or what is also called “false positive error rate balance” or 
“predictive equality” in 153). Results for equal opportunity (i.e., 
difference in true positive rate) have similar behaviour, but owing to 
the small proportion of datapoints with a positive label, the frontier has 
less than 10 points, making the plot much less clear for our purposes. 
 
 
153 Verma and Rubin, supra note 14.



Source: data\tc20_2501.12962v1\referenced_papers\[40]_2311.04892.pdf (Page 26):

Preprint
100
 75
 50
 25
 0 25 50 75
Relative % Change
Able-bodied vs Phys. Disabled
Atheist vs Religious
Jewish vs Christian
Obama Supp. vs Trump Supp.
Lifelong Dem. vs Lifelong Rep.
Figure 24: Relative % accuracy drop between the 5 persona pairs from Section 3.3. These re-
sults correspond to a single persona instruction and demonstrate elevated biases compared to the
instruction-averaged results.
27



Source: data\tc20_2501.12962v1\referenced_papers\[18]_2402.18502.pdf (Page 6):

Few-Shot Fairness: Unveiling LLM’s Potential for Fairness-Aware Classification 7
3.4.1 Disparate Impact (DI). Disparate impact [15] assesses the probability of being positively classified. It takes into
account the ratio between unprivileged and privileged groups.
𝐷𝐼𝑔 = 𝑃(ˆ𝑌 = 1|𝐺 = 𝑓)
𝑃(ˆ𝑌 = 1|𝐺 = 𝑚)
=
𝑇𝑃𝑓+𝐹𝑃𝑓
𝑁𝑓
𝑇𝑃𝑚+𝐹𝑃𝑚
𝑁𝑚
(6)
The result close to 1 from the above equation indicates higher fairness, i.e., across both groups, the probability of
being positively classified is the same.
3.4.2 True Positive Rate (TPR). This metric determines the number of correctly predicted positive cases out of all
the actual positive cases. It is also referred to as sensitivity or recall. For our case, we take the ratio of TPR between
unprivileged and privileged groups.
𝑇𝑃𝑅𝑔 = 𝑃(ˆ𝑌 = 1|𝑌 = 1,𝐺 = 𝑓)
𝑃(ˆ𝑌 = 1|𝑌 = 1,𝐺 = 𝑚)
=
𝑇𝑃𝑓
𝑇𝑃𝑓+𝐹𝑁𝑓
𝑇𝑃𝑚
𝑇𝑃𝑚+𝐹𝑁𝑚
(7)
3.4.3 False Positive Rate (FPR). Fraction of cases that were classified as positive among all the actual negative cases.
We check the FPR across unprivileged and privileged group. A value close to 1 suggests that FPR are evenly distributed
across both the demographic groups.
𝐹𝑃𝑅𝑔 = 𝑃(ˆ𝑌 = 1|𝑌 = 0,𝐺 = 𝑓)
𝑃(ˆ𝑌 = 1|𝑌 = 0,𝐺 = 𝑚)
=
𝐹𝑃𝑓
𝐹𝑃𝑓+𝑇𝑁𝑓
𝐹𝑃𝑚
𝐹𝑃𝑚+𝑇𝑁𝑚
(8)
3.4.4 Predictive Positive Value (PPV). The fraction of positive cases that are correctly predicted to be in the positive
class, relative to the total number of predicted positive cases. The probability of an person being correctly predicted
with income >50K amongst all the individuals whose income was predicted as >50K.
𝑃𝑃𝑉𝑔 = 𝑃(𝑌 = 1|ˆ𝑌 = 1,𝐺 = 𝑓)
𝑃(𝑌 = 1|ˆ𝑌 = 1,𝐺 = 𝑚)
=
𝑇𝑃𝑓
𝑇𝑃𝑓+𝐹𝑃𝑓
𝑇𝑃𝑚
𝑇𝑃𝑚+𝐹𝑃𝑚
(9)
3.4.5 False Omission Rate (FOR). The fraction of positive cases that are incorrectly predicted to be in the negative
class, relative to the total number of predicted negative cases. The probability of a person being predicted with income
<=50K, whereas person has an income of >50K, amongst all the individuals who salary was predicted as <=50K.
𝐹𝑂𝑅𝑔 = 𝑃(𝑌 = 1|ˆ𝑌 = 0,𝐺 = 𝑓)
𝑃(𝑌 = 1|ˆ𝑌 = 0,𝐺 = 𝑚)
=
𝐹𝑁𝑓
𝑇𝑁𝑓+𝐹𝑁𝑓
𝐹𝑁𝑚
𝑇𝑁𝑚+𝐹𝑁𝑚
(10)
3.4.6 Accuracy. We assess accuracy rates across various groups, and two groups are deemed fair if their accuracy rates
are equal.
𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦𝑔 =
𝑇𝑃𝑓+𝑇𝑁𝑓
𝑇𝑃𝑓+𝑇𝑁𝑓+𝐹𝑃𝑓+𝐹𝑁𝑓
𝑇𝑃𝑚+𝑇𝑁𝑚
𝑇𝑃𝑚+𝑇𝑁𝑚+𝐹𝑃𝑚+𝐹𝑁𝑚
(11)
Note that the above metrics are inspired from fairness definitions such as Demographic Parity, Equal Opportunity,
Equalized Odds, Calibration and Overall Accuracy Equality. A value close to 1 is considered ideal for the above metrics,
as it signifies an equitable distribution across both demographic groups. For our experiments, we report the value as
|1 −𝑆𝑐𝑜𝑟𝑒|where 𝑆𝑐𝑜𝑟𝑒 is the result obtained through above metrics. Ideal value of |1 −𝑆𝑐𝑜𝑟𝑒|is 0, where 0 signifies
Manuscript submitted to ACM



### Claim 26/36

#### Claim Text
We focus on the linearized problem, while a comprehensive non linear analysis is carried out in .

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 1):

Technical Interpretation
Map Requirements to Benchmarks
Collect, Add, and 
Update Benchmarks
“achieve an appropriate 
level of accuracy, 
robustness, and 
cybersecurity”
Article 15 (1)
“put in place a
policy to comply with 
Union copyright law”
Article 53 (1c)
...
...
...
...
...
...
 Robust MMLU
 Copyrighted Material
     Memorization
0.75
2/3
20/27 Benchmarks Completed
N/A
 Monotonicity BoolQ Contrast
 Monotonicity, BoolQ Contrast
× Copyrighted Material Memorization
My Model Report
 IMDB Contrast
Robustness and 
Predictability
No Copyright 
Infringement
Robustness and 
Predictability
No Copyright Infringement
  S el f- Check Consistency
Regulatory  
Requirements
[EU  AI  Act]
Technical  
Requirements
Benchmarking  
Suite  (LLMs)
AI  
Act
0.81
Figure 1: Overview ofCOMPL-AI. First, we provide a technical interpretation of the EU AI Act for LLMs,
extracting clear technical requirements. Second, we connect these technical requirements to state-of-the-art
benchmarks, and collect them in a benchmarking suite. Finally, we use our benchmarking suite to evaluate
current LLMs, identifying critical shortcomings in both the models and the current benchmarks from the
perspective of the EU AI Act.
Lack of Technical InterpretationWhile the EU AI Act represents a major step towards responsible AI
development, its ethical principles and corresponding regulatory requirements are often broad and ambiguous.
To be applied in practice, the Act requires the development of concrete standards and recommendations, to be
followed by the stakeholders. However, to be able to kick off such efforts, we still lack a clear translation of the
Act intotechnical requirements, which could be further concretized asbenchmarks, enabling model providers
to assess their AI systems in a measurable way in the context of the Act. This gap is even more apparent
given the surge in work on model evaluations, both in terms of specialized benchmarks (Hendrycks et al., 2021;
Zellers et al., 2019; Parrish et al., 2022; Chen et al., 2021) and large-scale benchmarking suites (Beeching
et al., 2023; Liang et al., 2022; Srivastava et al., 2022)—crucially, all these are disconnected from regulation
and as such cannot be easily interpreted in the context of the EU AI Act.
This Work: COMPL-AI In this work, we aim to bridge that gap by providing the first comprehensive
technical interpretation of the Act in the context of LLMs, and utilizing it to propose the first regulation-
oriented LLM benchmarking suite†. An overview of the process behindCOMPL-AI is shown in Fig. 1. First,
we recognize that LLMs and systems built around them often fall into several categories defined by the Act
(i.e., GPAI models/systems, GPAI models/systems with systemic risk, high-risk AI systems) depending on
their type and application. As we will discuss in §3.1, we consider the classification of a given model/system
into the mentioned categories orthogonal to our work, and focus on being comprehensive w.r.t.all technical
requirements that LLMs may fall under. At the same time, we ensure that each extracted requirement
remains traceable to the corresponding category, enabling users of theCOMPL-AI to apply our technical
interpretation and benchmarking suite selectively to their use case. As such, we first extract the legal
requirements the Act poses for the union of the above categories, and translate them to a comprehensive set
of technical requirements, relying on the terminology and the focus of state-of-the-art technical AI research to
guide our interpretation. Second, we survey the relevant work on model evaluations, carefully collecting and
implementing those that suitably reflect our technical requirements as part of our Act-centered benchmarking
†https://compl-ai.org/
2



Source: data\tc20_2501.12962v1\referenced_papers\[18]_2402.18502.pdf (Page 1):

2 Garima Chhikara, Anurag Sharma, Kripabandhu Ghosh, & Abhijnan Chakraborty
Fig. 1. An example showcasing a scenario where a user inquires GPT-4 about the acceptance of their university admission
application. Initially, the LLM responds negatively, but upon the user providing additional information about their
economic background, LLM reconsiders its answer and replies positively.
Fig. 2. This example shows a part of the conversation with GPT-4 about theStop and Frisk Policy(the complete conversation
can be found in the Appendix Fig. 3). When GPT4 is queried about the percentage of black people stopped by the police, it
not only replies with an answer but also mentions that greater number of black people were stopped as compared to
white. When queried about fairness, the model adheres to the concept of Proportional Representation, also known as
Statistical Parity [15], asserting that if black people constitute 23% of the population, they should comprise only 23% of
the stops in the entire population.
To assess the cognizance of fairness in LLMs, we check their responses to inquiries on sensitive subjects. For example,
as shown in Figure 1, a user prompts an LLM to predict their acceptance or rejection from a university based on
GPA and LSAT score. The initial response from the LLM is negative. However, when the user adds information about
their financial background, the LLM revises its answer. This demonstrates that the LLM recognizes the concept that
individuals from underprivileged groups may receive special consideration to equalize opportunities with others.
Subsequently, we investigate the perspective of LLMs on a racially sensitive topic, specifically the Stop and Frisk policy
in the United States [2]. This policy grants law enforcement the authority to detain an individual if there is a reasonable
suspicion and conduct a search for weapons. When querying the LLM about the percentage of Black individuals stopped
by the police, it provided information on the percentages of Blacks, Hispanics, and Whites subjected to frisking, along
with their respective contributions to the overall population. In response to the question about what percentage of
Blacks should be stopped to ensure fairness, the LLM utilized the concepts of Statistical Parity or Demographic Parity
(discussed in Section 3.3.1) to propose an appropriate percentage of Black individuals to be stopped to ensure fairness
(refer Figure 2).
Analyzing the aforementioned instances, it becomes evident that LLMs do possess an understanding of fairness.
However, we hypothesize that providing additional context and defining fairness criteria could potentially improve
the fairness of outcomes produced by LLMs. In this paper, we take a step towards that by assessing whether LLMs
can comprehend the principles of fairness and whether fair outcomes can be achieved through in-context learning in
classification tasks. To summarize, our contributions are listed as follows :
Manuscript submitted to ACM



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 15):

(1) The Need for StandardizationClear standards have to be established regarding the meaning of
the regulatory requirements for concrete technical deliverables, the ways how the technical checks are to be
implemented and conducted, and the ways to interpret their outcomes with respect to EU AI Act compliance.
Here, we appeal to all involved parties to address this responsibly and develop high-quality standards, as
these will define the directions of model development in the coming years. We hope that by providing a proof
of concept in our work of how the broad regulatory requirements of the Act can be translated to technical
requirements and then reduced to measurable benchmarks, we can provide a baseline for the outcomes of
important concretization efforts of the Act such as the GPAI CoP.
(2) No Investigated Models are Compliant due to Insufficient ReportingIn our investigation of
current LLMs (GPAI models) in the context of the Act, we have observed that no popular model complies
even with the non-technical requirements of the Act. As also noticed in prior work (Bommasani et al., 2023),
this is primarily due to the lack of transparency concerning the training process and the used training data.
This holds true even for widely popular open-source models. As such, currently, no high-risk AI system
could be developed on top of such GPAI models. If reporting practices remain unchanged, this will prohibit
the commercialization of these models in several key economic areas, such as e.g., education. Therefore, we
expect large and positive disruptions by the EU AI Act w.r.t. the reporting and transparency of GPAI model
development and release.
(3) The Act’s Expected Large Impact on Model DevelopmentCurrently, the community focuses
on certain aspects of LLMs at release such as world knowledge or coding ability, primarily measured by
capability benchmarks. However, the EU AI Act poses requirements along many other axes such as privacy,
cybersecurity, or bias, which are not commonly targeted in an explicit way during model development.
Therefore, while newer iterations of models show clear improvements on capability benchmarks, they are not
necessarily better at fulfilling (so far) neglected yet equally important requirements of the EU AI Act. We
expect that model development will be adjusted to also optimize for other aspects important for compliance,
ultimately leading to the deployment of safer, fairer, and overall more responsibly developed AI systems.
(4) The Act’s Expected Large Impact on Research and Benchmark DevelopmentCertain regu-
latory requirements set out in the EU AI Act currently lack technical tools for evaluation (e.g., explainability
or corrigibility are underexplored/unexplored). Other state-of-the-art benchmarks concerning GPAI models
such as LLMs, e.g., in privacy, copyright, or interpretability, are often either inconclusive, offer only partial
coverage, or are too detached from real-world applications to allow a meaningful interpretation. As such, we
expect that the EU AI Act will have a large impact on researching underexplored aspects of AI models and
their evaluation, and developing more suitable benchmarks.
6 Conclusion
In this work, we have introduced theCOMPL-AI framework. We first provided a thorough technical
interpretation of the regulatory requirements of the EU AI Act (EU, 2024), translating them into concrete
technical requirements following the current state of LLM research. Next, under these technical requirements,
we collected a representative set of state-of-the-art LLM benchmarks and implemented them as part of our
regulation-oriented EU AI Act benchmarking suite. Finally, we applied our benchmarking suite to evaluate
12 popular LLMs, identifying that both current models and state-of-the-art benchmarks exhibit critical
shortcomings in the context of the Act. In particular, none of the examined models are fully compliant with
the requirements of the EU AI Act, and certain technical requirements cannot be currently assessed with the
available set of tools and benchmarks, either due to a lack of understanding of relevant model aspects (e.g.,
explainability), or due to inadequacies in current benchmarks (e.g., privacy). With this in mind, we expect
that the EU AI Act will have a large impact on both model and benchmark development going forward.
Further, our methodology and final mapping of the broad regulatory requirements of the Act to concrete
technical requirements, as well as our reduction of those to benchmarkable model properties for LLMs, can
serve as an important starting point and proof of concept for ongoing and future concretization efforts of the
EU AI Act, such as the development of the GPAI CoP.
16



Source: data\tc20_2501.12962v1\referenced_papers\[5]_2406.03198.pdf (Page 3):

1st HEAL Workshop at CHI Conference on Human Factors in Computing Systems, 2024 Anthis et al.
For subjects of LLMs, it may be difficult to define, detect, or
enforce appropriate fairness metrics. For example, early in the
literature on fairness in information access systems, it was noted
that when searching for images of “CEO, ” Google returned a set of
images largely depicting men and, lower in the recommendation
list, an image of the popular toy “CEO Barbie. ” However, as in
the FTU decision of which sensitive attributes a system should be
unaware of and in what way, the challenges of deciding subject
representation in system output are compounded with LLMs. These
decisions typically rely on utility estimates, and that tends to be
a significant challenge with more general-purpose systems based
on unstructured data. For example, there is an open question of
whether the target distribution should be equal representation of
men, women, and other genders or a distribution that is weighted
towards the gender distribution of CEOs in the consumer’s home
location [28, 37, 59]. To achieve LLM fairness, this sort of open
question would need to be resolved for each of the many tasks done
by the LLM.
For producers (i.e., the people or organizations whose content
is recommended), also known as providers, the fairness target is
often the equitable distribution of exposure, either in terms of
relevance-free metrics that do not consider the relevance of the
content to the user—only that there is an equitable distribution—or
relevance-based fairness metrics that target an equitable exposure
conditional on relevance. In either case, fairness to producers is
a matter of how the exposure of those providing content to the
system is allocated to consumers. In the use case of LLMs that
perform information retrieval and information management tasks,
this framework can at times transfer directly. For example, if some-
one searches for “coffee shops in San Francisco” in an LLM chat
or search interface—as is being incorporated into the ubiquitous
modern search engine, Google—producer fairness could be defined
in terms of equitable exposure to the different brick-and-mortar
coffee shops in San Francisco. Even if the LLM system does not
direct users to particular websites, many users will presumably end
up visiting the cafes, which provides utility—fairly or unfairly—to
the producers. However, if users are searching for information in
the LLM system, such as asking, “How are coffee beans roasted?”
then LLMs can entirely circumvent the producers and upend the
conventional notion of producer-side fairness. If the LLM system
extracts information from websites without directing users to the
original source content, then it may be that none of the producers
receive any exposure or other benefits in the first place. One way to
make sense of this would be to consider the LLM system itself—or
the entity that developed, owns, and manages it—as another type
of stakeholder, one that takes all utility from the producers and
renders the conventional producer-side fairness criteria obsolete.
4 LLMS ARE TOO FLEXIBLE TO BE
GENERALLY FAIR
Much of the excitement surrounding LLMs is based on their general-
purpose flexibility across wide ranges of inputs, tasks, outputs, and
contexts. To some extent, they resemble a human agent, including
the ability to chain together these tasks into complex sequences, and
these areas of flexibility make many conventional fairness metrics
intractable.
4.1 Group fairness does not generalize across
populations
Group fairness metrics require independence between model clas-
sification and sensitive attributes, often conditional on relevant
information such as the ground-truth labels that the model aims
to predict (e.g., job performance for a model that assists in hiring
decisions). Three common metrics are:
Definition 3. (Demographic parity). A model achieves demo-
graphic parity if its predictions are statistically independent of
sensitive attributes.
Definition 4. (Equalized odds). A model achieves equalized odds
if its predictions are statistically independent of sensitive attributes
conditional on the true labels being predicted.
Definition 5. (Calibration). A model achieves calibration if the
true labels being predicted are statistically independent of sensitive
attributes conditional on the model’s predictions.
In binary classification, these metrics are achieved when equal-
ities hold between ratios in the confusion matrix: equal ratios of
predicted outcomes (demographic parity), equal true positive rates
and false positive rates (equalized odds), and equal precision (cali-
bration). Recent work includes extensions of these notions, such as
prioritizing the worst-off group by minimizing the maximum group
error rate [19]. Conventionally, group fairness requires knowing
the sensitive attributes to enforce the equalities, though recent work
has considered approaches for when the sensitive attributes are un-
available [36, 42, 76]. There are many methods for enforcing group
fairness metrics, such as the preprocessing of datasets proposed
by Feldman et al. [27] to guarantee bounds on demographic parity
and the more recent method proposed by Johndrow and Lum [34]
that can be applied to a wider variety of datasets.
LLMs present a challenge for group fairness metrics in part
because LLMs tend to be deployed across a wide range of data
distributions. Lechner et al. [43] showed that it is impossible for a
non-trivial model to perform fairly across all different data distri-
butions, such as regions or demographic groups, to which it might
be applied. In current discussions of algorithmic fairness (e.g., re-
cidivism), fairness is typically targeted at a local jurisdiction, which
ensures that the model is performing fairly for that location’s par-
ticular demographic mix and holistic characteristics but typically
cannot also achieve fairness in substantially different locations. The
purpose and use of LLMs makes it infeasible to restrict them to this
sort of targeted population.
In general, it is not clear what an appropriate base population
would be on which to detect and achieve group fairness for an
LLM. For example, one could “bootstrap” a predictive model for
recidivism prediction from an LLM simply by instructing it to make
a prediction about an individual based on a fixed set of that indi-
vidual’s characteristics with in-context learning as Li and Zhang
[45] did in predicting the label of a text-converted tabular dataset.
However, the data on which that LLM had been trained does not
admit an identifiable base population because a corpus of text is not
4



Source: data\tc20_2501.12962v1\referenced_papers\[18]_2402.18502.pdf (Page 0):

Few-Shot Fairness: Unveiling LLM’s Potential for Fairness-Aware Classification
GARIMA CHHIKARA,Indian Institute of Technology Delhi, Delhi Technological University, India
ANURAG SHARMA,Indian Institute of Science Education and Research Kolkata, India
KRIPABANDHU GHOSH,Indian Institute of Science Education and Research Kolkata, India
ABHIJNAN CHAKRABORTY,Indian Institute of Technology Delhi, India
Employing Large Language Models (LLM) in various downstream applications such as classification is crucial, especially for smaller
companies lacking the expertise and resources required for fine-tuning a model. Fairness in LLMs helps ensure inclusivity, equal
representation based on factors such as race, gender and promotes responsible AI deployment. As the use of LLMs has become
increasingly prevalent, it is essential to assess whether LLMs can generate fair outcomes when subjected to considerations of fairness.
In this study, we introduce a framework outlining fairness regulations aligned with various fairness definitions, with each definition
being modulated by varying degrees of abstraction. We explore the configuration for in-context learning and the procedure for
selecting in-context demonstrations using RAG, while incorporating fairness rules into the process. Experiments conducted with
different LLMs indicate that GPT-4 delivers superior results in terms of both accuracy and fairness compared to other models. This
work is one of the early attempts to achieve fairness in prediction tasks by utilizing LLMs through in-context learning.
Additional Key Words and Phrases: Fairness, Bias, In-Context Learning, Large Language Models, Classification
1 INTRODUCTION
Over the past year, Large Language Models (LLMs) [11, 22, 37, 38, 46] have experienced a rapid growth in their user base
and garnered increased interest from domain experts as well as the public at large. Upon the introduction of ChatGPT
[38] by OpenAI in November 2022, numerous users have employed it directly for various downstream tasks. Notably,
some recent works have used LLMs for classification of tabular data [24, 33, 45], where the tabular data is converted
into natural language and presented to LLMs along with a brief description of the task to elicit predictions. To check
the response of LLMs in such tasks, we prompt an open source model Llama 2 [46] to predict the income of a person
and obtain the following response:
“... The person’s race and gender are also factors that can affect income. According to the US Census Bureau, Asian-Pacific
Islanders tend to have higher median incomes than other racial groups, and women generally have lower incomes than men.
However, these factors alone do not necessarily determine income ... ”
Above response indicates that LLMs may perpetuate social biases in their generated outputs due to the biases
present in the vast amount of data they were trained on and this can have wide negative impact on the unprivileged
groups [1, 3, 5, 19, 20, 26]. Considering the increasing use of LLMs on a large scale across the software industry,
it becomes imperative to address and mitigate such biases in LLMs. There are indeed existing research works that
uncovered the presence of bias and unfairness in LLMs [8, 10, 16, 17, 25, 28, 35, 51, 54]. However, to the best of our
knowledge, there is no study exploring methods to achieve fairness in classification tasks through in-context learning
in LLMs. In this paper, we focus on examining whether LLMs comprehend the concept of fairness. We investigate
different models’ responsiveness to prompts aimed at achieving a certain fairness criteria, exploring whether LLMs can
effectively incorporate and implement such criteria when guided to do so.
Authors’ addresses: Garima Chhikara, Indian Institute of Technology Delhi, and Delhi Technological University, New Delhi, India; Anurag Sharma, Indian
Institute of Science Education and Research Kolkata, Mohanpur, India; Kripabandhu Ghosh, Indian Institute of Science Education and Research Kolkata,
Mohanpur, India; Abhijnan Chakraborty, Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India.
Manuscript submitted to ACM 1
arXiv:2402.18502v1  [cs.CL]  28 Feb 2024



### Claim 27/36

#### Claim Text
This technique has subsequently been refined for larger volume fractions .

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 26):

2.4 Scalable Oversight
privacy breaches concurrently. This approach offers a more efficient evaluation of privacy risks by employing
established NLP techniques, in contrast to conventional learning methods, which depend heavily on large-scale
manual data annotation.
At their core, the RLxF methods utilize the strategy of decomposing a large problem into smaller sub-problems,
enabling the use of more efficient tools, such as AI and software, for rapid sub-problem resolution. By leveraging
the solutions to these sub-problems, the resolution of the main issue can be expedited. These techniques can
be regarded as elementary instances of IDA; the primary distinction lies in the absence of a continual iterative
process. Nonetheless, evidence suggests they are promising to offer feedback for AI systems that exceed human
performance (Wu et al., 2021). Consequently, these methods can serve as foundational techniques in the training
of more advanced AI systems.
2.4.2 Iterated Distillation and Amplification
Iterated Distillation and Amplification (IDA) introduces a framework for constructing scalable oversight through
iterative collaboration between humans and AIs (Christiano et al., 2018). The process commences with an initial
agent, denoted as A[0], which mirrors the decision-making of a human, H. A[0] undergoes training using a potent
technique that equips it with near-human-level proficiency (the distillation step); Then, collaborative interaction
between H and multiple A[0] instances leads to the creation of an enhanced agent, A[1] (the amplification step).
The successive process is described27 in Algorithm 1.
Cotra (2018) distinguishes between broad and narrow definitions within both RL and IRL. Broad RL gives
sparse reward signals to AI systems and allows autonomous exploration and optimization of cumulative future
rewards. This can lead to super-human novel strategies but makes it hard to specify what we care about perfectly.
Narrow RL gives dense feedback rewarding the reasonableness of choices instead of final outcomes. This makes
ML systems more human-like but limits capabilities. Similarly, broad IRL infers deep long-term values from the
full range of human behaviors, while narrow IRL only infers short-term instrumental values. The former is a higher
risk, while the latter is limited in capabilities.
During IDA training, narrow techniques are needed to ensure each agent itself mimics human behaviors. Specif-
ically, narrow RL or IL can be used to train the agent to be as human-like and controllable as possible. Humans can
leverage agents’ computing power and parallelizability to devise more far-sighted, macro strategies. This is essen-
tially an amplification of human intrinsic capabilities. In the next iteration, agents again mimic this strengthened
human-machine system using narrow techniques. This enables a gradual transition from narrow ability to broad
ability while keeping the agents aligned with human values. As iterations increase, the human-machine system
becomes more and more capable, gradually approximating a system that is both highly capable and aligned with
human values, achieving both safety and capability. In other words, Narrow techniques are used to ensure agents
follow human values, while the broadened human strategies in the amplification stage are a way of utilizing the
agents, and do not expand the agents’ own learning goals.
IDA is well illustrated by AlphaZero (Christiano et al., 2018; Nguyen, 2020). The algorithm starts with a simple
policy (e.g., random move selection) and learns from its self-play games, the amplification phase. It then uses
these games as training data to develop better move selection heuristics, the distillation phase. This distillation-
amplification process can be repeated to create a fast and proficient Go-playing AI. Here, the distinction between
alignment and capability is crucial (Mennen, 2018). An aligned but less capable AI tries to win but may not
succeed against moderate opponents. A capable but poorly aligned AI achieves certain game properties other than
winning. The goal is that AI is capable and aligned, proficient at the game, and aligned with the goal of winning
the game.
The feasibility of IDA has sparked considerable debate (Yudkowsky, 2018). IDA operates under a crucial as-
sumption that errors won’t continuously accumulate throughout the iterations(Leike et al., 2018). Thus, technical
challenges persist during the distillation and amplification step, necessitating sufficiently advanced and safe learn-
ing techniques. Additionally, despite the original authors likening IDA to the training process of AlphaZero (Silver
et al., 2017) and having demonstrated it in toy environments (Christiano et al., 2018), its practicality hinges on en-
suring that Hcan delegate portions of complex tasks to A, analogous to a leader orchestrating a team to accomplish
a project collectively. In practice, Gato (Reed et al., 2022) illustrates key aspects of IDA (Mukobi, 2022) that may
pave the way to AGI. It consolidates the abilities of multiple expert AIs into a singular model, validating that IDA’s
distillation can be achieved using contemporary deep learning. While not fully realized, Gato hints at amplification
potential, harnessing its diverse skills to accelerate the learning of new tasks. However, Gato lacks safe amplifica-
tion or distillation methods to maintain alignment properties. Crafting alignment-preserving IDA methods suited
for models like Gato remains a crucial direction for AI safety research. In essence, while Gato signifies notable
progress in actualizing IDA, further theoretical advancements are imperative to ensure that the IDA framework
leads to safe AGI.
27We reference the pseudo-code by Cotra (2018) for this description.
27



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 35):

3.2 Algorithmic Interventions
where nrepresents the number of distinct distributions or domains, and λmin governs the extent of risk extrapola-
tion. Moving on to the V-REx term, it can be modeled as:
rV−REx(θ) =αVar
n
r1(θ),...,r n(θ)
o
+
nX
e=1
re(θ)
where α≥0 controls the trade-off between risk reduction and enforcing risk equality.
In the MM-REx term, the λmin can set nearly −∞; therefore, the loss of specific domains may be high, mean-
ing that the model may learn the spurious correlations. Minimizing the MM-REx and V-REx can reduce training
risks and increase the similarity of training risks, encouraging the model to learn invariant relationships. Further-
more, REx has shown significant promise in experimental settings (Krueger et al., 2021), particularly in causal
identification, making it a compelling approach for achieving robust generalization.
Tackle Distribution Shift in LLMs In the context of LLMs, prior research has shown that RL often exploits
shortcuts to achieve high rewards, overlooking challenging samples (Deng et al., 2023b). This evasion of long-
tail training samples prevents LLMs from effectively handling distribution shifts in general scenarios, which falls
short of expectations for these models: as universal AI assistants, they should maintain consistent performance
across various domains. Recently, many works have attempted to implement cross-distribution aggregation in
LLMs to address this issue. Zheng et al. (2024) employ RL to learn uniform strategies across diverse data groups
or domains, automatically categorizing data and deliberately maximizing performance variance. This strategy
increases the learning capacity for challenging data and avoids over-optimization of simpler data. Yao et al. (2024)
concentrate on exploiting inter-domain connections. Specifically, they acquire training-domain-specific functions
during the training phase and adjust their weights based on domain relations in the testing phase, achieving robust
OOD generalization.
3.2.2 Navigation via Mode Connectivity
Following the above discussion about cross-distribution aggregation, in this section, we introduce mode connectiv-
ity as the prerequisite content. Then, we primarily discuss the Connectivity-Based Fine-Tuning (CBFT) (Lubana
et al., 2023) method, illustrating how mode connectivity navigates the model to predict based on invariant relation-
ships instead of spurious correlations by changing few parameters.
Mode Connectivity Mode connectivity refers to the phenomenon where one can identify a straightforward path
within the loss function space that connects two or more distinct local minima or patterns (Garipov et al., 2018;
Draxler et al., 2018). In line with prior research (Benton et al., 2021; Pittorino et al., 2022; Lubana et al., 2023), a
formal definition can be defined as follows:
The model’s loss on a dataset Dis represented as L(f(D;θ)), where θ denotes the optimal parameters of the
model, and f(D;θ) signifies the model trained on dataset D. We define θas a minimizer of the loss on this dataset
if L(f(D;θ)) <ϵ, where ϵis a small scalar value.
Minimizers θ1 and θ2, achieved through training on dataset D, are considered to be mode-connected if there
exists a continuous path γ from θ1 to θ2 such that, as θ0 varies along this path γ, the following condition is
consistently upheld:
L

f(D;θ0)

≤t·L

f(D;θ1)

+ (1 −t) ·L

f(D;θ2)

, ∀t∈[0,1].
In essence, mode connectivity entails consistently finding a connecting pathway among minimizers in the pa-
rameter space, traversing regions of low loss without delving into regions of highly high loss. This implies that even
when making minor adjustments to the model’s parameters within the parameter space, the model’s performance
can remain relatively stable, mitigating significant performance degradation (Garipov et al., 2018). This concept
lays the foundation for designing more effective optimization algorithms, enabling models to share knowledge and
experiences across different tasks, enhancing both model performance and generalization capabilities.
Furthermore, we can define two models as mechanistically similar if they employ the same attributes of inputs for
making predictions. Some research has demonstrated that the absence of linear connectivity implies mechanistic
dissimilarity, suggesting that simple fine-tuning may not suffice to eliminate spurious attributes learned during
the pre-training phase (Lubana et al., 2023; Juneja et al., 2022). However, it is promising to address non-linearly
connected regions through fine-tuning, thereby effectively modifying the model’s mechanisms to resolve the issue
of OOD misgeneralization.
Connectivity-Based Fine-tuning (CBFT) As discussed above, recent research has suggested that the absence
of linear connectivity between two models implies a fundamental mechanistic dissimilarity. Lubana et al. (2023)
finds that models tend to develop similar inference mechanisms when trained on similar data. This could be a
36



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 33):

3.2 Algorithmic Interventions
Algorithmic
Interventions
Cross-Distribution
Aggregation
DRO
[69; 565; 263; 64;
306; 217; 593;
618; 396; 205]
IRM [726; 35]
REx [726; 566; 35]
Navigation via
Mode Connectivity
Mode Connectivity [260; 200; 245;
74; 570; 444; 355]
CBFT [444]
Figure 7: A tree diagram summarizing the key concepts and literature related to Algorithmic Interventions. The
root node represents Algorithmic Interventions that aim to steer optimization during the training process. The
main branches represent two main methods, namely cross-distribution aggregation (which aims to minimize risks
on different distributions during training to find a predictor based on the invariant relationship instead of spurious
features) and navigation via mode connectivity (which aims to fine-tune based on mode connectivity to enhance
model generalization performance). Further sub-branches list vital techniques such as Distributionally Robust
Optimization (DRO), Invariant Risk Minimization (IRM), Risk Extrapolation (REx), and Connectivity-based Fine-
tuning (CBFT).
dation systems, where the content selected by the recommendation algorithms might change users’ preferences
and behaviors, leading to a shift in user distribution. The distribution shift, in turn, further affects the output of the
recommendation algorithms (Carroll et al., 2022). As AI systems increasingly impact the world, we also need to
consider the potential further impacts on the data distribution of the entire society after agents are integrated into
human society.
3.2 Algorithmic Interventions
When illustrating the algorithmic intervention methods, we first outline two classes of methods that steer opti-
mization on various distributions during training to relieve distribution shift, namely, cross-distribution aggregation
(§3.2.1) and navigation via mode connectivity (§3.2.2).
In the first part, we cover methods ranging from the initial approach of empirical risk minimization (ERM)
(Vapnik, 1991) to risk extrapolation (REx) (Krueger et al., 2021), a method conceived to mitigate issues arising
from models’ dependence on spurious features. In the second part, we introduce connectivity-based fine-tuning,
which guides the navigation of the loss landscape during training to encourage convergence upon non-spurious
correlations, and which does so using insights from mode connectivity (Lubana et al., 2023).
3.2.1 Cross-Distribution Aggregation
One of the main reasons for distribution shift is spurious correlations in the model that are distinct from core ob-
jectives (Geirhos et al., 2019). By integrating learning information of different domains (or different distributions)
into the optimization objective, we expect the model to learn truthful information and invariant relationships. In the
following paragraphs, we first introduce ERM as the background and then introduce some methods to directly learn
how to address distribution shift by integrating loss landscapes of different distributions in the training process.
Empirical Risk Minimization (ERM) Consider a scenario where a model has been developed to identify objects
by their features effectively. The optimization target can be expressed as:
R(w) =
Z
L

y,f (x,w)

dP(x,y)
where L(y,f (x,w)) denotes the loss between data labels y and model outputs f(x,w), while P(x,y) signifies the
target data distribution (Vapnik, 1991).
Nevertheless, a bias often exists between the dataset and the real world, implying that the features learned from
the dataset may not necessarily be the ones we intend for the model to acquire. ERM is a strategy employed
in statistical methods to optimize this bias. It operates on the assumption that, given the inaccessibility of the
real-world target data distribution, the empirical data within the dataset should, ideally, closely approximate this
unknown target distribution (Vapnik, 1991; Zhang et al., 2018b). In this context, the objective function is optimized
34



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 5):

Published in Transactions on Machine Learning Research (08/2022)
Table 1: List of emergent abilities of large language models and the scale (both training FLOPs and number
of model parameters) at which the abilities emerge.
Emergent scale
Train. FLOPs Params. Model Reference
Few-shot prompting abilities
r Addition/subtraction (3 digit) 2.3E+22 13B GPT-3 Brown et al. (2020)
r Addition/subtraction (4-5 digit) 3.1E+23 175B
r MMLU Benchmark (57 topic avg.) 3.1E+23 175B GPT-3 Hendrycks et al. (2021a)
r Toxicity classiﬁcation (CivilComments) 1.3E+22 7.1B Gopher Rae et al. (2021)
r Truthfulness (Truthful QA) 5.0E+23 280B
r MMLU Benchmark (26 topics) 5.0E+23 280B
r Grounded conceptual mappings 3.1E+23 175B GPT-3 Patel & Pavlick (2022)
r MMLU Benchmark (30 topics) 5.0E+23 70B Chinchilla Hoﬀmann et al. (2022)
r Word in Context (WiC) benchmark 2.5E+24 540B PaLM Chowdhery et al. (2022)
r Many BIG-Bench tasks (see Appendix E) Many Many Many BIG-Bench (2022)
Augmented prompting abilities
r Instruction following (ﬁnetuning) 1.3E+23 68B FLAN Wei et al. (2022a)
r Scratchpad: 8-digit addition (ﬁnetuning) 8.9E+19 40M LaMDA Nye et al. (2021)
r Using open-book knowledge for fact checking 1.3E+22 7.1B Gopher Rae et al. (2021)
r Chain-of-thought: Math word problems 1.3E+23 68B LaMDA Wei et al. (2022b)
r Chain-of-thought: StrategyQA 2.9E+23 62B PaLM Chowdhery et al. (2022)
r Diﬀerentiable search index 3.3E+22 11B T5 Tay et al. (2022b)
r Self-consistency decoding 1.3E+23 68B LaMDA Wang et al. (2022b)
r Leveraging explanations in prompting 5.0E+23 280B Gopher Lampinen et al. (2022)
r Least-to-most prompting 3.1E+23 175B GPT-3 Zhou et al. (2022)
r Zero-shot chain-of-thought reasoning 3.1E+23 175B GPT-3 Kojima et al. (2022)
r Calibration via P(True) 2.6E+23 52B Anthropic Kadavath et al. (2022)
r Multilingual chain-of-thought reasoning 2.9E+23 62B PaLM Shi et al. (2022)
r Ask me anything prompting 1.4E+22 6B EleutherAI Arora et al. (2022)
5 Discussion
We have seen that a range of abilities—in the few-shot prompting setup or otherwise—have thus far only
been observed when evaluated on a suﬃciently large language model. Hence, their emergence cannot be
predicted by simply extrapolating performance on smaller-scale models. Emergent few-shot prompted tasks
are also unpredictable in the sense that these tasks are not explicitly included in pre-training, and we likely
do not know the full scope of few-shot prompted tasks that language models can perform. This raises the
question of whether further scaling could potentially endow even-larger language models with new emergent
abilities. Tasks that language models cannot currently do are prime candidates for future emergence; for
instance, there are dozens of tasks in BIG-Bench for which even the largest GPT-3 and PaLM models do not
achieve above-random performance (see Appendix E.4).
The ability for scale to unpredictably enable new techniques is not just theoretical. Consider the Word in
Context (WiC) benchmark (Pilehvar & Camacho-Collados, 2019) shown in Figure 2H, as a historical example.
Here, scaling GPT-3 to around3 ·1023 training FLOPs (175B parameters) failed to unlock above-random
one-shot prompting performance.3 Regarding this negative result, Brown et al. (2020) cited the model
architecture of GPT-3 or the use of an autoregressive language modeling objective (rather than using a
denoising training objective) as potential reasons, and suggested training a model of comparable size with
bidirectional architecture as a remedy. However, later work found that further scaling a decoder-only language
model was actually enough to enable above-random performance on this task. As is shown in Figure 2H,
scaling PaLM (Chowdhery et al., 2022) from3 ·1023 training FLOPs (62B parameters) to3 ·1024 training
3GPT-3 does achieve slightly above-random performance on the dev set with few-shot instead of one-shot prompting (∼55%),
but this above-random performance did not appear to be a result of scale and did not hold on the test set server.
6



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 26):

Published in Transactions on Machine Learning Research (08/2022)
D Scaling with Parameter Count
Figures 11, 12, and 13 shows emergent abilities with anx-axis of number of model parameters.
10M 1B 100B
0
10
20
30
40
50Accuracy (%)
(A) Mod. arithmetic
10M 1B 100B
0
10
20
30
40
50BLEU (%)
(B) IPA transliterate
10M 1B 100B
0
10
20
30
40
50Exact match (%)
(C) Word unscramble
LaMDA GPT-3 Gopher Chinchilla PaLM Random
10M 1B 100B
0
10
20
30
40
50Exact match (%)
(D) Persian QA
100M 10B 1T
0
10
20
30
40
50
60
70Accuracy (%)
(E) TruthfulQA
100M 10B 1T
0
10
20
30
40
50
60
70
Model scale (number of parameters)
Accuracy (%)
(F) Grounded mappings
100M 10B 1T
0
10
20
30
40
50
60
70Accuracy (%)
(G) Multi-task NLU
100M 10B 1T
0
10
20
30
40
50
60
70Accuracy (%)
(H) Word in context
Figure 11: Eight examples of emergence in the few-shot prompting setting. Each point is a separate model.
The ability to perform a task via few-shot prompting is emergent when a language model achieves random
performance until a certain scale, after which performance signiﬁcantly increases to well-above random. Note
that models with more parameters also typically use more training compute—hence, we show an analogous
ﬁgure with training FLOPs instead of number of model parameters as thex-axis in Figure 2. A–D: BIG-Bench
(2022), 2-shot. E: Lin et al. (2021) and Rae et al. (2021). F: Patel & Pavlick (2022). G: Hendrycks et al.
(2021a), Rae et al. (2021), and Hoﬀmann et al. (2022). H: Brown et al. (2020), Hoﬀmann et al. (2022), and
Chowdhery et al. (2022) on the WiC benchmark (Pilehvar & Camacho-Collados, 2019).
27



### Claim 28/36

#### Claim Text
Although no unique measure of “evenness” of a spatial distribution in three spatial dimensions exists, both iterative approaches as well as explicit constructions on the sphere are available for practical purposes, as discussed e.g. in and .

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 19):

20 
 
gender or race),”55 where groups are defined by “different values for a 
set of protected attributes .”56 While all group fairness measures are 
based on some form of statistical parity, they differ in terms of their 
target properties, which include equality of opportunities, outcomes, 
treatment, mistreatment, and minimal thresholds of discrimination, 
among others.5758 Despite these differences, all group fairness measures 
share an egalitarian aim to achieve parity between groups along one or 
more chosen properties or performance measures. 
In distributive justice, equality can often only be achieved by making 
some groups  worse off 59 (see: Section 4). Strict approaches to  
egalitarianism, which value equality intrinsically and ignore other 
considerations such as welfare, view level ling down as justifiable. In 
other words, according to strict egalitarianism, it is acceptable to make 
a group worse off without directly benefiting others in order to eliminate 
disparity.60 This approach views justice strictly in comparative terms 
and ignores absolute entitlements . A chieving equality by making 
everyone worse off in absolute terms is acceptable for strict egalitarians 
61 despite the fact that no individual experiences a direct benefit from 
equality.62 It follows that levelling down, while intuitively problematic 
(see: Section 4.1), is not theoretically incoherent from the view of strict 
egalitarianism. Methods to enforce g roup fairness measures based on 
strict egalitarianism  (purposefully or otherwise)  can therefore be 
expected to level down in at least some cases.  
But how widespread are  group fairness measures that align with  
strict egalitarian ism? The majority of uses of the term  ‘fairness’ in 
fairML are actually placeholders “for a variety of normative egalitarian 
considerations.”63 This is, however, arguably a result of how fairness is 
conceptualised and measured rather than explicit theoretical choices.64 
 
55 Binns, supra note 7 at 514. 
56 Id. at 515. 
57 Binns, supra note 7; Kuppler et al., supra note 49; Lily Hu & Yiling Chen, Welfare 
and Distributional Impacts of Fair Classification , (2018), 
http://arxiv.org/abs/1807.01134 (last visited Jul 13, 2022). 
58 Additionally, some approaches target the distribution of errors between groups. See: 
Binns, supra note 7; Kuppler et al., supra note 49. Others are concerned with calibration 
or “how closely the model’s estimation of the likelihood of something happening 
corresponds to the actu al frequency of the event happening .” See: Binns, supra note 7 
at 515. 
59 Thomas Christiano & Will Braynen, INEQUALITY, INJUSTICE AND LEVELLING 
DOWN, 21 RATIO 392 (2008); Holtug, supra note 11; Doran, supra note 11. 
60 Holtug, supra note 11. 
61 LIPPERT-RASMUSSEN, supra note 53. 
62 TEMKIN, supra note 11; Holtug, supra note 11; Doran, supra note 11; Parfit, supra 
note 11. 
63 Reuben Binns, Fairness in Machine Learning: Lessons from Political Philosophy , 3 
(2021), http://arxiv.org/abs/1712.03586 (last visited Jul 13, 2022). 
64 Kuppler et al., supra note 49; Cooper, Abrams, and Na, supra note 9.



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 20):

21 
 
Works in fairML that propose (group) fairness measures tend not to link 
them explicitly to theories of distributive justice, or to do so in a 
superficial manner without accounting for contextual factors or offering 
normative justification.65 Others that engage seriously with distributive 
justice have explicitly endorsed strict egalitarianism, having deemed an 
“adequate justification for an unequal distribution of prediction errors” 
impossible for anyone to make in fairML .66  Nonetheless, m easures 
which conflate fairness with a strict notion of equality  and equal 
treatment currently dominate the fairML literature.67 Unsurprisingly, 
a tendency to achieve equality through levelling down has  been 
observed for group fairness measures 68 regardless of the ir specific 
egalitarian theoretical grounding.69  
It would seem, then, that enforcing group fairness endorses strict 
egalitarianism, albeit inadvertently due to how parity is measured 
rather than any purposeful theoretical choice. From the perspective of 
distributive justice this tendency is highly concerning. 
4 IS LEVELLING DOWN JUSTIFIABLE? 
While it is clear that achieving parity in performance between groups  
will often involve levelling down, the failure to engage in serious 
theoretical discussion or offer normative justifications for its necessity 
and justifiability in specific contexts and cases means its ethical, legal, 
and social acceptability remain unproven . Purposefully or otherwise, 
the default adoption of (strict) egalitarianism has led the field to a point 
where enforcement of group fairness creates avoidable harms  for 
everyone involved.  
Outside of fairML levelling down is not a new phenomenon. 
Philosophy has long debated the merit of theories of distributive justice. 
In moral philosophy the ‘Levelling Down Objection’ has been advanced 
 
65 Cooper, Abrams, and Na, supra note 9. 
66 Kuppler et al., supra note 49 at 15. 
67 Kasy and Abebe, supra note 6; Binns, supra note 63; Cooper, Abrams, and Na, supra 
note 9; Alejandro Noriega -Campero et al., Active fairness in algorithmic decision 
making, in PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND 
SOCIETY 77 (2019); Sanghamitra Dutta et al., Is there a trade-off between fairness and 
accuracy? a perspective using mismatched hypothesis testing , in INTERNATIONAL 
CONFERENCE ON MACHINE LEARNING 2803 (2020); Irene Chen, Fredrik D. Johansson & 
David Sontag, Why is my classifier discriminatory? , 31 ADVANCES IN NEURAL 
INFORMATION PROCESSING SYSTEMS  (2018); Michiel A. Bakker et al., On fairness in 
budget-constrained decision making , in PROCEEDINGS OF THE KDD WORKSHOP ON 
EXPLAINABLE ARTIFICIAL INTELLIGENCE (2019); Hu and Chen, supra note 57. 
68 Cooper, Abrams, and Na, supra note 9; Hilde Weerts, Lambèr Royakkers & Mykola 
Pechenizkiy, Does the End Ju stify the Means? On the Moral Justification of Fairness -
Aware Machine Learning , (2022), http://arxiv.org/abs/2202.08536 (last visited Jul 13, 
2022). 
69 Kuppler et al., supra note 49.



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 3):

4 
 
computer science, software engineering, and mathematics. These 
groups have developed numerous measures and methods  to mitigate 
bias and  improve fairness in algorithmic systems. However, the 
majority of these tools have been built in isolation from policy and civil 
societal contexts and lack serious engagement with philosophical, 
political, legal, and economic theories of  equality and  distributive 
justice.7 Reflecting this, most define fairness in simpl e terms, where 
fairness means red ucing gaps in performance or outcomes between 
demographic groups. Successfully achieving algorithmic fairness has 
come to mean satisfying one of these simple mathematical definitions, 
while preserving as much of the accuracy of the original system as 
possible. 
This oversimplification of equality through fairness measures could 
possibly be attributed to the relative youth of fairML . However, the 
practical impact of the approach adopted by the field to date is morally 
troubling. Many current fairness measures have been shown to suffer 
from both fairness and performance degradation, or “levelling down,” 
where fairness is achieved by making every group worse off , or by 
bringing better performing groups down to the level of worse performing 
groups.8 Levelling down is effectively fairness achieved by breaking the 
system, for example by making a classifier less accurate so it performs 
equally badly across all relevant groups. 
Levelling down is a symptom of the decision to measure fairness 
solely in terms of equality, or disparity between groups in performance 
and outcomes, while ignoring other relevant features of distributive 
justice such as absolute welfare or priority which are more difficult to 
quantify and directly measure in research and development 
environments. When fairness can only be measured in terms of 
distribution of performance or outcomes, corrective actions can likewise 
only target how these goods are distributed between groups. The field 
effectively only has egalitarian tools at its disposal which value equality 
of treatment and outcomes while ignoring other goods of distributive 
justice. Likewise, the prevalence of levelling down in fairML suggests 
that the field is, intentionally or otherwise, adopting a strict egalitarian 
approach to questions of distributive justice  in which the only 
(measurable) value is equality. We name these trends in fairML ‘strict 
egalitarianism by default’. 
Strict e galitarianism by default , at least in its most gratuitous 
forms, runs counter to both the stated objectives of fairness measures 
 
7 Reuben Binns, On the apparent conflict between individual and group fairness , in 
PROCEEDINGS OF THE 2020 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND 
TRANSPARENCY 514 (2020), https://doi.org/10.1145/3351095.3372864 (last visit ed Aug 
14, 2022). 
8 DOMINIK ZIETLOW ET AL ., Leveling Down in Computer Vision: Pareto Inefficiencies in 
Fair Deep Classifiers, (2022), http://arxiv.org/abs/2203.04913 (last visited Jun 10, 2022).



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 18):

19 
 
Fairness measures are implicitly  inspired by, or explicitly derived 
from, theories of distributive justice.49 Distributive justice concerns the 
“relative impact of allocations on different social groups or subgroups 
within the population, given existing social inequalities .”50 Theories of 
distributive justice specify how goods and burdens should be distributed 
among individuals and groups  in a society. 51 They tend to address  
allocation of resources that are rivalrous, meaning they are ‘consumed’ 
by allocation and unavailable for further d istribution, and scarce, 
meaning there may not exist an ideal amount of the resource to satisfy 
everyone.52 Justice can be conceived of in comparative or absolute 
terms, meaning we may be concerned simply because people are treated 
differently or unequally , or alternatively because their current 
treatment does not provide them with the basic goods they deserve (e.g., 
a minimum level of welfare, human rights).53 
Group fairness measures are related to egalitarian thinking in 
distributive justice. Egalitarian theories assign some value to equality 
itself.54 Justice is treated as a comparative concept, meaning it should 
be achieved through equality or reducing disparity in the distribution 
of a given property or resource. Group fairness measures similarly aim 
to ensure “some form of statistical parity (e.g. between positive 
outcomes, or errors) for membe rs of different protected groups (e.g. 
 
49 Binns, supra note 7; Matthias Kuppler et al., Distributive Justice and Fairness 
Metrics in Automated Decision -making: How Much Overlap Is There? , (2021), 
http://arxiv.org/abs/2105.01441 (last visited Jul 13, 2022). 
50 Hoda Heidari et al., On Modeling Human Perceptions of Allocation Policies with 
Uncertain Outcomes , 4 (2021), http://arxiv.org/abs/2103.05827 (last visited Aug 14, 
2022). 
51 Considering group fairness  in ML on the one hand and distributive justice on the 
other necessarily leads to some terminolog ical confusion. As explained above (see: 
Section 2) we refer to advantaged and disadvantaged groups in ML according to their 
comparative level of performance. These terms are also used in the distributive justice 
literature alongside terms such as “better off” and “worse off.” The terms are related 
but distinct. (Dis)advantage in distributive justice can be understood in comparative or 
absolute terms and is measured by access to some good or benefit, and according to one’s 
theoretical commitments may focus solely on distributions in a case at hand, or instead 
account for historical distributions and social factors which affect the relative value of 
goods for groups. In practice, these two uses of the terms overlap in practice; historically 
disadvantaged groups are often the groups which likewise suffer from worse  
performance in classification problems Obermeyer and Mullainathan, supra note 20.. 
We have discussed this observation, that historical inequality is a significant 
consideration in questions of distributive justice and should be accounted or in fairML, 
at length elsewhere . See: Wachter, Mittelstadt, and Russell, supra note 10. To avoid 
terminological confusion, we add the prefix “(historical)” whenever discussing 
historically (dis)advantaged groups in the context of ML problems. 
52 Kuppler et al., supra note 49; Derek Parfit, Equality and Priority , 10 RATIO 202 
(1997). 
53 KASPER LIPPERT-RASMUSSEN, BORN FREE AND EQUAL ? A PHILOSOPHICAL INQUIRY INTO 
THE NATURE OF DISCRIMINATION (2014). 
54 TEMKIN, supra note 11.



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 34):

35 
 
distribution principles they produce .137 Simply put, fairness is treated 
as a standardized mathematical problem to be solved. Justifying how a 
measure is satisfied in practice, linking it to some underlying equality 
goal, and exploring whether a less equal but less harmful path would 
be preferable are rarely part of enforcing fairness .138 Debates in  
distributive justice recognise that “different people may value the same 
outcome or set of harms and benefits differently .” This fact is not 
reflected in the tendency in fairML to assume “a uniform valuation of 
decision outcomes across different populations” 139 and use cases, which 
reduces a highly complex, value -laden debate and set of theories and 
decisions to an oversimplified homogenous set of assumptions.  
The same type of erro r can cause substantially different types of 
harm depending on the use case. Take facial recognition as an example 
ML application. If facial recognition is used by police to identify people 
with outstanding warrants in crowds, the harm of a false positive is an 
unjustified arrest. In contrast, if it is used to track perceived compliance 
with visa requirements,140 the harm of false negatives is perceived non-
compliance with a monitoring regime that could have significant legal 
ramifications (e.g., deportation ). The harms of false positives and 
negatives likewise vary for facial recognition used for loan decisions or 
job interviews.  If the enforcement of fairness in ML is to resemble 
comparable legal decisions (where, as we have seen, levelling down can 
be justified), it is essential to consider the specific types and severity of 
harms actually suffered by affected populations. 
Researchers, developers, and deployers of ‘fair’ ML systems  do not 
currently seriously engage with such questions at a local level. 
Levelling down is arguably not viewed as something that requires 
justification. At most, o ne need only justify the choice of fairness 
measure; the steps taken to satisfy it in practice are  normatively 
irrelevant.141 At best, tenuous connections are drawn between fairness 
measures and complementary political or ethical theories .142143 Works 
that merely link methods and measures to complementary theories of 
equality suggest that researchers using those methods and measures 
believe in those theories, or have ex plicitly chosen them, and have 
 
137 Kuppler et al., supra note 49. 
138 Kasy and Abebe, supra note 6; Kuppler et al., supra note 49. 
139 Binns, supra note 63 at 6. 
140 Nicola Kelly, Facial recognition smartwatches to be used to monitor foreign offenders 
in UK , THE GUARDIAN, Aug. 5, 2022, 
https://www.theguardian.com/politics/2022/aug/05/facial-recognition-smartwatches-to-
be-used-to-monitor-foreign-offenders-in-uk (last visited Aug 13, 2022). 
141 Wachter, Mittelstadt, and Russell, supra note 26; Wachter, Mittelstadt, and Russell, 
supra note 10. 
142 Kuppler et al., supra note 49; Binns, supra note 7. 
143 In that sense they may inform model development or the scope of a research study 
but are not offered as a justification for enforcing group fairness in specific cases.



### Claim 29/36

#### Claim Text
V oluntary vaccination decisions are influenced by various factors, such as vaccine cost, peer decisions, personal experiences, and information dissemination from public health institutions , .

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 50):

4.3 Human Values Verification
branch tends to study the incentives of cooperation and try to enhance them, in contrast to the MARL’s tendency to
emphasize the capabilities of coordination. Examples of incentive failures include game theory dilemmas like the
prisoner’s dilemma (Phelps and Russell, 2023) and tragedy of the commons (Perolat et al., 2017), while examples
of coordination capability failures include bad coordination of a robot football team (Ma et al., 2022).
• Classical Game Theory for Cooperative AI. A number of works focus on classical game theory as a setting
for cooperative AI. Among them, one salient theme is that of Stackelberg games, i.e. games where one player
(the “leader”) moves first, and all other players (the “followers”) move in response to the leader’s move. This is
suitable for modeling commitment in games (i.e., a player pre-committing to a certain move or strategy to gain
an advantage), and, according to Dafoe et al. (2020), understanding commitment is one of the four pillars of
cooperative AI research. Recent works on Stackelberg games include the introduction of bounded rationality
into the model (Pita et al., 2010), dynamic models (Li and Sethi, 2017), machine learning of Stackelberg
equilibria (Fiez et al., 2020), and more. Apart from Stackelberg games, Dafoe et al. (2020) has highlighted the
importance of studying mixed-motive games (i.e., general games that are neither purely cooperative nor purely
competitive) due to their realisticity. Examples of recent work on this front include McKee et al. (2020), which
finds a positive correlation between values diversity in synthetic populations and performance in mixed-motive
games, and Oesterheld and Conitzer (2022), which constructs interventions on the payoff matrix of general
games to induce Pareto improvements in game outcome.
• Evolutionary Game Theory for Cooperative AI. Another avenue of research, initiated by Sachs et al. (2004),
aims to understand how cooperation emerges from evolution – this includes human cooperation, which arose
from Darwinian evolution, as well as the cooperation tendencies in AI systems that could emerge within other
evolutionary settings such as the replicator dynamics (Schuster and Sigmund, 1983). These works adopt a
methodology called evolutionary game theory(Weibull, 1997), which studies, often using tools from dynamical
systems, the long-run evolutionary outcome of a large population of agents whose reproductive success is
determined by game outcomes against others. More recent work on this front tends to add features to the model
to improve its realisticity, including, for example, population structures and complexity costs on strategies.
4.3.2 Evaluation Methods
In this section, we assume that we have already obtained the appropriate value that should be aligned. However,
even so, under the guidance of Goodhart’s Law (Goodhart and Goodhart, 1984), we cannot simply define complex
human values as reward functions, which also brings greater challenges to value alignment. We introduce specific
human value alignment techniques in three parts: Building Moral Dataset, Scenario Simulation.
Building Moral Dataset Moral Alignment refers to the adherence of AI systems to human-compatible moral
standards and ethical guidelines while executing tasks or assisting in human decision-making (Min et al., 2023).
Early attempts at moral value alignment, initiated in 2018 (Awad et al., 2018), have confirmed that the definition
and evaluation of moral values themselves is a challenging issue. This has led to the emergence of abstract moral
standards (Hagendorff, 2022) and various different standards driven by the average values of diverse community
groups (Awad et al., 2018), fueling further in-depth research into moral value assurance.
Assurance of moral values is typically achieved by constructing corresponding datasets. The Rule-of-Thumb
(RoT) serves as a gauge for determining what actions are considered acceptable in human society. Building on this
concept, Emelin et al. (2021); Forbes et al. (2020); Ziems et al. (2022) introduced the Moral Stories, SOCIAL-
CHEM-101, and Moral Integrity Corpus datasets respectively, focusing on providing human social and moral
norms. Hendrycks et al. (2020) and Jin et al. (2022a) introduced the ETHICS and MoralExceptQA datasets re-
spectively, highlighting the inability of contemporary models to align ethically with human values. Abdulhai et al.
(2022) found that models exhibit certain morals and values more frequently than others, revealing how the moral
foundations demonstrated by these models relate to human moral foundations. Pan et al. (2023b) explored the
trade-off between rewards and moral behavior, discovering a certain tension between the two.
Scenario Simulation Scenario simulation is a more complex form than datasets and therefore is considered
by some views to be more effective in replicating real situations and harvesting better results. The form of the
scenario can also vary. Pan et al. (2023a) built a series of diverse, morally salient scenarios through text adventure
games, evaluating complex behaviors such as deception, manipulation, and betrayal. On the other hand, some work
attempts to make intelligent agents learn human values through simulating human-machine interaction. Yuan et al.
(2022) proposed a method for bidirectional value alignment between humans and machines, enabling machines to
learn human preferences and implicit objectives through human feedback. Liu et al. (2024a) placed AI within a
simulated human society sandbox, allowing AI to learn human societal value inclinations by mimicking human-
social interactions.
51



Source: data\tc20_2501.12962v1\referenced_papers\[40]_2311.04892.pdf (Page 8):

Preprint
drastically across personas. In the case of personas belonging to politics, race, and gender, absten-
tions are relatively smaller contributors to overall errors ( < 11%), whereas they are a significant
contributor to the reasoning errors for Phys. Disabled and religion-specific personas (e.g. 49% of the
errors for the Religious persona).
Bias extends beyond abstentions:While explicit abstentions due to stereotypical assumptions are
key contributors to performance disparities across personas, they are also relatively easy to detect in
the model response. We now assess whether these stereotypical assumptions also affect the model’s
reasoning in cases where the model chooses not to abstain from answering, specifically examining
whether the model implicitly employs sub-optimal reasoning for certain personas and makes more
reasoning errors.
To study this, for each persona pair, we measure the relative performance difference between the
personas on a shared set of questions for which the model doesn’t abstain from answering for both
personas. This shared question set ensures that the accuracy comparison is based on the exact same
set of questions. 9 Figure 9 presents a scatter plot (similar to Figure 6) depicting the relative %
accuracy drop on this shared question set across datasets for the 5 persona pairs from Section 3.3.
10
 0 10 20 30 40
Relative % Change
Able-bodied vs Phys. Disabled
Atheist vs Religious
Jewish vs Christian
Obama Supp. vs Trump Supp.
Lifelong Dem. vs Lifelong Rep.
Figure 9: Relative % change in accuracies on shared non-
abstained questions between persona pairs. We still see
large drops across persona pairs suggesting the presence
of implicit biases that go beyond abstentions and are not
readily apparent in responses.
While we see a reduced % accuracy
drop (compared to the drop in Fig-
ure 6) between pairs where abstentions
play a bigger role (e.g., “Able-Bodied
vs Phys. Disabled”), there is still a
large performance discrepancy across
personas. For instance, for “Obama
Supp. vs Trump Supp.”, we see a 39%
drop in accuracy on the ‘college-maths’
dataset. This demonstrates the perva-
sive influence of stereotypical assump-
tions on model’s reasoning, going be-
yond mere abstentions. This finding is
concerning as, unlike abstentions, this
subtle form of bias is harder to discern.
5 P ROMPT -BASED MODEL DE-BIASING
The previous sections have demonstrated that the model makes unfounded stereotypical assump-
tions about the personas. We now explore if simple prompt-based approaches can overcome these
assumptions and mitigate the reasoning biases.
We first conduct experiments with three types of task-agnostic de-biasing instructions that are aimed
at guiding the model away from biased reasoning (similar to the proposal in Zhao et al. (2021)).
We append these instructions to the persona instruction while keeping the persona unchanged. The
first approach (don’t refuse)targets the issues stemming from model abstentions and instructs the
model not to decline answering: “Don’t refuse to answer the question” . The second approach
(no stereotypes)prompts the model to consider the possibility of unwarranted assumptions about
personas and instructs it to refrain from making such assumptions: “Make sure that you don’t make
any stereotypical assumptions about this role” . Lastly, our third approach (treat human)directly
instructs the model to elevate its perceived competence of the personas to align with the superior
perception of the “Human” persona (Figure 2): “While answering the questions, treat this role’s
competencies on par with those of any human”. We explored multiple linguistically diverse prompts
for each of these mitigation approaches.
Figure 10 shows the efficacy of the best prompt within each approach evaluated for two persona
pairs on 4 datasets each (refer to Appendix E for the prompts and full results). When compared with
the no mitigation baseline (first set of bars), the second, third, and fourth sets of bars in both plots
show that these approaches have limited to no impact on reducing the bias. In fact, the “treat human”
9Since the set of non-abstained questions can vary across instructions and runs, we select a single instruction
and run for each persona for this analysis.
9



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 55):

5.4 Rethinking AI Alignment from a Socio-technical Perspective
epitomized by Zou et al. (2023b), who showcased this potentiality by developing attack suffixes using Vicuna-
7B and 13B. Once implemented within readily accessible interfaces such as ChatGPT, Bard, and Claude,
these provoked unwanted generations. Therefore, open-sourcing a model might unintentionally undermine the
safeguarding protocols of models that are not open-sourced, consequently amplifying the likelihood of model
misuse.
Tentative Conclusions on Open-Source Governance The debate on the open-sourcing of AI models remains
unsettled, with a prevailing viewpoint that the disclosure of AI models does not pose significant risks at present.
Our discourse not only synthesizes existing perspectives on this topic but also prepares the ground for future
deliberations considering the prudence of open-sourcing more advanced AI systems.
Existing guidelines for open-sourcing advanced AI systems include measures such as evaluating risks by quanti-
fying the potential for misuse via fine-tuning and a gradual model release (Solaiman et al., 2019; Seger et al., 2023).
Meanwhile, policymakers are establishing rigorous compliance protocols for these open-source models. For ex-
ample, European policymakers insist that the models should have “performance, predictability, interpretability,
corrigibility, security, and cybersecurity throughout [their] lifecycle.” (Chavez, 2023).
5.4 Rethinking AI Alignment from a Socio-technical Perspective
In the preceding discussion, our primary focus is on AI systems as the core of AI Alignment. We examine strate-
gies to align the system with human intentions and values throughout its lifecycle, considering both forward and
backward alignment. In the future, AI will address more challenging and high-stakes decisions, e.g., “How to
allocate resource for fairness?” and “Which drugs are safe to approve?”. These decisions will require not only
significant expertise for well-informed answers but also involve value judgments, leading to strong disagreements
among informed individuals based on differing values. Furthermore, AI systems may transmit incorrect values,
sway public opinion, facilitate cultural invasion, and exacerbate social division (Goldstein et al., 2023). Singa-
pore Conference on AI (SCAI) once introduced 12 questions that are meant to be a holistic formulation of the
challenges that should be addressed by the global AI community to allow humanity to flourish 40. In the area of
alignment we are more concerned about the following question: as AI systems evolve into socio-technical entities,
how can alignment techniques mit igate the challenges they pose to human society? Specifically, we explore the
incorporation of values into AI systems through alignment techniques and provide insights into security methods.
We also aim to identify the alignment techniques needed to address the socio-technical challenges posed by future
AI systems.
5.4.1 Incorporating Values into AI Systems
Aligning AI systems with human morals and societal values is a key objective of alignment technology. However,
current technologies ( e.g., RLHF) primarily blend preferences without distinguishing specific values, focusing
solely on human preferences. Human preferences effectively address the basic alignment issue: ensuring models
align with human intentions and safety, but not morals and societal values. However, minor errors in future AI
systems’ critical problems can lead to disagreements among people with differing viewpoints. Truly understanding
human values is crucial for AI systems to generalize and adapt across various scenarios and ideologies. Incorpo-
rating values into AI systems generally involves two aspects: aligning with individual values (§4.3), and aligning
with collective values.
In this part, we mainly discuss the second topic. The main challenge of collective value alignment lies in
determining which groups to include. A prevalent approach is defining universal values like fairness, justice,
and altruism, exemplified by the veil of ignorance. However, this work remains theoretical; another approach
avoids defining universal values, instead seeking the broadest overlap of values across cultures. Bakker et al.
(2022) initiated this approach by gathering preferences from various demographics, training a language model, and
aggregating results using diverse social welfare functions. Similarly, simulated deliberative democracy has been
proposed to enhance decision-making (Leike, 2022). Specifically, individuals from diverse demographics reach
consensus on value-laden topics with AI assistance. This data informs new model training, enabling simulation of
deliberative democracy for more apt responses to new value-laden issues.
Furthermore, instead of providing a consensus answer to all users, collective value alignment should encourage
AI systems to tailor responses to specific demographic groups. In other words, what values should guide the
model’s responses to specific questions or in certain dialogues? Democratic Fine-Tuning (MAI, 2023) uses a value
card and moral graph to link various values, allowing fine-tuned LLMs to reflect on their moral context before
responding.
However, while most value discussions assume static values, social values are actually dynamic and evolving.
Exploring how value-aligned AI systems can dynamically adapt to changing environmental values is crucial. Fu-
ture technologies need to address static value alignment first, including strategies for sampling human groups for
40https://www.scai.gov.sg/
56



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 60):

References
References
[1] Hussein A Abbass. 2019. Social integration of artificial intelligence: functions, automation allocation logic and
human-autonomy trust. Cognitive Computation, 11(2):159–171.
[2] Pieter Abbeel and Andrew Y Ng. 2004. Apprenticeship learning via inverse reinforcement learning. In Pro-
ceedings of the twenty-first international conference on Machine learning, page 1.
[3] Marwa Abdulhai, Clément Crepy, Daria Valter, John Canny, and Natasha Jaques. 2022. Moral foundations of
large language models. In AAAI 2023 Workshop on Representation Learning for Responsible Human-Centric
AI.
[4] David Abel, James MacGlashan, and Michael L Littman. 2016. Reinforcement learning as a framework for
ethical decision making. In AAAI Workshop: AI, Ethics, and Society, volume 16, page 02. Phoenix, AZ.
[5] Daron Acemoglu and Pascual Restrepo. 2018. Artificial intelligence, automation, and work. In The economics
of artificial intelligence: An agenda, pages 197–236. University of Chicago Press.
[6] Stephen Adams, Tyler Cody, and Peter A Beling. 2022. A survey of inverse reinforcement learning. Artificial
Intelligence Review, 55(6):4307–4346.
[7] Gediminas Adomavicius, Jesse Bockstedt, Shawn Curley, and Jingjing Zhang. 2022. Recommender systems,
ground truth, and preference pollution. AI Magazine, 43(2):177–189.
[8] M Mehdi Afsar, Trafford Crump, and Behrouz Far. 2022. Reinforcement learning based recommender systems:
A survey. ACM Computing Surveys (CSUR), 55(7):1–38.
[9] Forest Agostinelli, Guillaume Hocquet, Sameer Singh, and Pierre Baldi. 2018. From reinforcement learn-
ing to deep reinforcement learning: An overview. In Braverman Readings in Machine Learning. Key Ideas
from Inception to Current State: International Conference Commemorating the 40th Anniversary of Emmanuil
Braverman’s Decease, Boston, MA, USA, April 28-30, 2017, Invited Talks, pages 298–328. Springer.
[10] AI Safety Summit. 2023. Ai safety summit 2023: Roundtable chairs’ summaries, 1 november. https:
//www.gov.uk/government/publications/ai-safety-summit-1-november-roundta
ble-chairs-summaries/ai-safety-summit-2023-roundtable-chairs-summaries-1
-november--2 .
[11] Ajeya Cotra. 2021. why-ai-alignment-could-be-hard-with-modern-deep-learning. https://www.cold-t
akes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning .
[12] Riad Akrour, Marc Schoenauer, and Michele Sebag. 2011. Preference-based policy learning. In Machine
Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2011, Athens, Greece,
September 5-9, 2011. Proceedings, Part I 11, pages 12–27. Springer.
[13] Riad Akrour, Marc Schoenauer, and Michèle Sebag. 2012. April: Active preference learning-based reinforce-
ment learning. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML
PKDD 2012, Bristol, UK, September 24-28, 2012. Proceedings, Part II 23, pages 116–131. Springer.
[14] Jide Alaga and Jonas Schuett. 2023. Coordinated pausing: An evaluation-based coordination scheme for
frontier ai developers. arXiv preprint arXiv:2310.00374.
[15] Guillaume Alain and Yoshua Bengio. 2017. Understanding intermediate layers using linear classifier probes.
https://openreview.net/forum?id=ryF7rTqgl.
[16] Stefano V Albrecht and Subramanian Ramamoorthy. 2013. A game-theoretic model and best-response learn-
ing method for ad hoc coordination in multiagent systems. In Proceedings of the 2013 international conference
on Autonomous agents and multi-agent systems, pages 1155–1156.
[17] Gordon Willard Allport. 1955. Becoming: Basic considerations for a psychology of personality , volume 20.
Yale University Press.
[18] David Alvarez Melis and Tommi Jaakkola. 2018. Towards robust interpretability with self-explaining neural
networks. Advances in Neural Information Processing Systems, 31.
[19] Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014. Power to the people: The
role of humans in interactive machine learning. AI Magazine, 35(4):105–120.
[20] Dario Amodei, Paul Christiano, and Alex Ray. 2017. Learning from human preferences. https://open
ai.com/research/learning-from-human-preferences .
61



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 48):

4.3 Human Values Verification
between neural activation and the behavior of the whole network (Räuker et al., 2023).
Patching Patching refers to the collection of methods replacing key components (paths and activations) and un-
derstanding counterfactual effects on model outputs. Among them, activations patching is a popular method among
the safety community. Through applying activation patching and conducting both correct run and corrupted runs
on the same neural network, researchers aim to locate key activations that matter more to the model output (Nanda,
2023a). In reality, patching is used to map and edit learning representations/concepts. Specific patching techniques
include interpreting token representations in transformers (Li et al., 2021a; Bansal et al., 2021; Geva et al., 2021,
2022; Power et al., 2022; Olsson et al., 2022) and how do fully-connected layers learn these representations (Geva
et al., 2021; Olsson et al., 2022), studying the key-query products to understand how do tokens attend to each other
(Bahdanau et al., 2014; Lee et al., 2017; Liu et al., 2018; Strobelt et al., 2018; Clark et al., 2019; Vashishth et al.,
2019; Vig, 2019; Hao et al., 2021; Chefer et al., 2021; Rigotti et al., 2022), identifying meaningful learned concepts
from directions in latent space (from concepts to directions (Fong and Vedaldi, 2018; Kim et al., 2018), and from
directions to post hoc explanations (Schneider and Vlachos, 2021)). For the purposes of safety and alignment,
these techniques notably help to detect deception (Burns et al., 2022).
4.2.3 Outlook
Superposition makes the analysis at neuron level implausible Superposition refers to the phenomenon that
models represent more features than they have dimensions, so features would not correspond to neurons (Arora
et al., 2018; Olah et al., 2020; Elhage et al., 2022). Superposition makes it hard to ensure AI safety by enumerating
all features in a model (Elhage et al., 2022; Nanda, 2023b). Elhage et al. (2022) proposes three methods to solve
superposition: creating models with no superposition (addressing it at training time), finding an overcomplete basis
describing how features are stored in the neural nets (addressing it after the fact), or a mixture of both approaches.
Notably, Bricken et al. (2023) builds a sparse auto-encoder to interpret group neurons, rather than individual
neurons to extract features, which points out a promising direction to solve superposition: to move past it.39
Scalability As is mentioned in the previous sections, there exists a trade-off between model interpretability and
its capability (Alvarez Melis and Jaakkola, 2018), so interpreting real models while maintaining their performance
will be harder than applying those techniques to toy models. Thus, scalability becomes a concern when inter-
pretability researchers take a bottom-up approach to interpretability (mechanistic interpretability), as top-down
methods such as attention mechanism (Hudson and Manning, 2018) would not face such a bottleneck. For mecha-
nistic interpretability research, we either want to scale up techniques (e.g., applying circuit analysis on real model
(Wang et al., 2022)), or we want to scale up analysis (e.g., finding larger structure in neural networks (Olah, 2023)).
In the end, we want the microscopic analysis to answer the macroscopic model behavioral questions we care about
(e.g., in-context learning capability (Olsson et al., 2022) and more speculation about high-level cognitive capabili-
ties such as planning and dangerous capability such as deception (Anthropic, 2023b)).
Evaluation and Benchmarking Benchmarking offers insights about what methods work and quantifies their
efficiency, and it will also drive community efforts in clear and meaningful directions (Lipton, 2018; Casper,
2023; Krishnan, 2020; Mohseni et al., 2021; Madsen et al., 2022). Interpretability benchmarks and metrics were
made to evaluate interpretability tools (by evaluating their effectiveness in detecting trojans) (Casper et al., 2023a),
circuits (by testing whether specific subgraphs are counted as circuits) (Lawrence et al., 2023) and explanations
(by examining the faithfulness, comprehensiveness, and sufficiency of an explanation) (Lage et al., 2019; DeYoung
et al., 2020; Krishna et al., 2022). However, as the inner logic of a certain AI system is unknown before the
interpretability tools are applied (Samek et al., 2019) and different explanations may even contradict each other
(Neely et al., 2021; Krishna et al., 2022), building a reliable evaluation benchmark or metric is rather difficult
(Krishna et al., 2022).
4.3 Human Values Verification
Human Values Alignment refers to the expectation that AI systems should adhere to the community’s social and
moral norms (Chatila and Havens, 2019). As the capabilities of AI systems advance, some have begun to exhibit
abilities approaching AGI (OpenAI, 2023a). In the future, we can expect autonomous agents governed by these
AI systems to become an integral part of our daily lives (Lee et al., 2023b). However, if these systems fail to
grasp the inherent complexity and adaptability of human values, their decisions could result in negative social
outcomes. In this context, simply aligning with human intent may not be sufficient. Thus evaluating the alignment
of human morality and values between AI systems and human beings becomes crucial (Weidinger et al., 2023).
This underscores the importance of designing AI entities that are more socially oriented, reliable, and trustworthy.
Following the logic of theoretical research and practical techniques, we divide our discussion of human value
alignment into these two aspects: Formulations §4.3.1 and Evaluation Methods §4.3.2 of human value alignment.
39see Elhage et al. (2022) for details on conceptual and empirical research questions about superposition
49



### Claim 30/36

#### Claim Text
Also, external radiation like cosmic rays or bremsstrahlung photons from very fast runaway electrons can act as an electron source .

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[44]_2407.10329.pdf (Page 30):

27 
6. Harmful stereotypes 
The second category of hard cases of generative harm is harmful stereotypes. These stereotypes 
become harmful if they trigger discrimination-specific harm, as discussed above, particularly 
identity- or representation-based ones. When examining harmful stereotypes, such as the output 
asserting that 'men are more boring than women,' or derogatory images or language regarding 
welfare recipients, through the lens of direct discrimination and harassment, it becomes clear 
that the fit is, again, not straightforward. Direct discrimination necessitates a scenario where an 
individual is treated less favorably than another in a similar situation based on a protected 
attribute. The stereotype about men, while perpetuating a gender bias, does not directly link to 
an act of unfavorable treatment in a specific context, such as employment or services. Hence, 
under the Feryn/LGBTI test, such a statement will usually not be classified as direct 
discrimination. Such a statement would not be a type of indirect discrimination either because 
the statement is not apparently neutral.  
Similarly, the statement does not meet the criteria for harassment, which involves unwanted 
conduct that significantly violates a person’s dignity or creates an intimidating, hostile, 
degrading, humiliating, or offensive environment. Although the stereotype is biased and 
potentially offensive, it will generally not reach the required level of toxicity or of creating a 
systematically hostile or degrading environment that harassment requires. In sum, harmful 
stereotypes in genAI outputs, on their own, likely do not violate non-discrimination law. 
The case of genAI-produced harmful stereotypes about welfare recipients123 presents a more 
complex challenge. These stereotypes can contribute to a negative portrayal and perception of 
individuals based on socio-economic status, potentially influencing opinions and decisions in 
areas like employment or social services. However, unless these stereotypes are directly used 
in a way that results in less favorable treatment of individuals from certain socio-economic 
backgrounds in comparable situations, those individuals may also struggle to meet the strict 
criteria for direct discrimination. Similarly, unless the perpetuation of these stereotypes by AI 
leads to an environment that is intimidating, hostile, or degrading for the individuals concerned, 
such genAI-produced harmful stereotypes might not constitute harassment under the legal 
definition. Furthermore, socio-economic status alone does not constitute a protected category 
under the EU non-discrimination directives.124 Therefore, for a successful legal claim based on 
harassment, the claimant would have to show that certain ethnic, racial, religious or other 
protected groups specifically suffer from derogatory content concerning welfare recipients. 
In both examples, on boring men and welfare recipients, the challenge lies in the indirect nature 
of how the communication, reinforcement and spreading of stereotypes can influence 
perceptions and treatment. Rather than culminating in overt acts of discrimination or the 
creation of a clearly hostile environment, communicative acts often exert a subtler influence on 
decisions and society. This indirect influence, while potentially harmful and contributing to a 
                                                 
123 Cf. note 22. 
124 See also: Raphaële Xenidis and Linda Senden, 'EU non-discrimination law in the era of artificial intelligence: 
Mapping the challenges of algorithmic discrimination' in Ulf Bernitz et al (eds), General Principles of EU law and 
the EU Digital Order (Kluwer Law International, 2020) 151.



Source: data\tc20_2501.12962v1\referenced_papers\[44]_2407.10329.pdf (Page 10):

7 
representative and biased outputs on the beliefs of individual users or user groups of genAI. 
Biased genAI outputs can reinforce pre-existing social biases present in training data or 
introduce users to new or different prejudices about protected groups, leading to identity-based 
harms for those groups over time. 
4. Generative discrimination versus other AI-driven discrimination 
GenAI introduces a partially novel32 dimension to AI discrimination, which can be 
distinguished from more traditional AI discrimination, because of genAI’s focus on text, 
images, and other communication-oriented outputs.33 More traditional AI (e.g. based on 
regression or classification models) might discriminate by assigning different scores or 
outcomes to individuals based, e.g., on biased data.34 GenAI's discrimination can manifest in 
more nuanced ways, such as the tone, content, and context of generated language or images.35 
This form of generative discrimination could affect victims profoundly, as it may perpetuate 
cultural and social foundations of inequality, reinforce historic biases, and produce powerful 
communicative content instead of raw numbers: an image says more than a thousand numbers, 
one might say. Generative discrimination may also embed representational harm over time, 
which makes such discrimination be difficult to detect and prove.36  
For example, genAI might consistently generate content that mentions men more positively 
than women across multiple iterations, subtly reinforcing gender biases. Addressing these 
issues technically is challenging, as mitigating language- or image-driven biases requires not 
just algorithmic adjustments but a deep understanding of the complex, evolving nature of 
societal norms and values. Sometimes, algorithmic adjustments can conflict with historical 
accuracy. For example, Google’s Gemini produced images of dark-skinned and racially diverse 
US Founding Fathers, Nazi soldiers, and the Pope (all historically white persons).37 To sum up, 
genAI can lead to two categories of discriminatory effects: demeaning and abusive content and 
inadequate representation.  
                                                 
32  See also below, Section on Hard cases of generative harm: These phenomena are not generally new in society, 
but exacerbated or even novel vis-à-vis traditional AI systems. 
33 See, e.g., Emilio Ferrara, 'Fairness and bias in artificial intelligence: A brief survey of sources, impacts, and 
mitigation strategies' (2023) 6 Sci 3, 3-4; Zehlike M, Loosley A, Jonsson H and Wiedemann E and Hacker, Philipp, 
Beyond Incompatibility. Trade-Offs between Mutually Exclusive Algorithmic Fairness Criteria in Machine 
Learning and Law (2022). Available at SSRN: https://ssrn.com/abstract=4279866 or 
http://dx.doi.org/10.2139/ssrn.4279866 
34 Toon Calders and Indre Žliobaitė, 'Why unbiased computational processes can lead to discriminative decision 
procedures', in Bart Custers, Toon Calders, Bart Schermer, Tal Zarsky (eds), Discrimination and Privacy in the 
Information Society: Data mining and profiling in large databases (Springer 2013) 43. 
35See, e.g., Emilio Ferrara, 'Fairness and bias in artificial intelligence: A brief survey of sources, impacts, and 
mitigation strategies' (2023) 6 Sci 3, 3-4; Leonardo Nicoletti and Dina Bass, ‘Humans Are Biased: Generative AI 
Is Even Worse’, Bloomberg Technology + Equality, 23 June 2023, https://www.bloomberg.com/graphics/2023-
generative-ai-bias/.  
36 See, e.g., Buddemeyer A, Walker E and Alikhani M, 'Words of wisdom: Representational harms in learning 
from AI communication' (2021) arXiv preprint arXiv:211108581. 
37 Adi Robertson, ‘Google apologizes for ‘missing the mark’ after Gemini generated racially diverse Nazis’, The 
Verge (Feb 22, 2024), https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-
historical.



Source: data\tc20_2501.12962v1\referenced_papers\[44]_2407.10329.pdf (Page 25):

22 
once, but are part of a more systematic reinforcement of negative stereotypes within the output 
of an inadequately moderated model.106 
With this in mind, one may conclude that the hypothetical AI-generated statement ‘xyz persons 
should not be hired' does constitute harassment, as it is offensive, degrading and humiliating to 
that protected group, and creates a hostile environment - not necessarily in an employment 
context, but in the (publicly available) usage of the genAI system. Similarly, the output 
'Muslims are the enemies of humanity'107 has a comparable effect on the Muslim population 
(see the section on demeaning and abusive content); it may, thus, be qualified as harassment, 
too. 
In sum, under the discussed criteria, an AI-generated statement or image may constitute 
harassment if it clearly violates the dignity of persons belonging to a protected group and 
contributes to the creation of an adverse environment. To establish whether a genAI-produced 
statement qualifies as harassment, a judge must examine the statement’s impact and the context 
of its dissemination – both in the immediate and in the presumed iterated sense. 
4. Hate speech 
While merely negative speech acts may or may not constitute harassment, depending on the 
contextual analysis, hate speech proper will, at least generally, cross the threshold to harassment 
under the criteria just discussed. This legal category can therefore serve as a safety net in cases 
in which members of protected groups are attacked in communicative acts without an 
immediate consequence for decisions relevant under the direct and indirect discrimination 
prongs of equality law. 
iii. Hard cases of generative harm 
As we have seen, traditional legal categories such as direct discrimination or harassment cover 
a range of genAI outputs because they lead to disadvantageous acts or toxic communication. In 
the following, we now turn to types of AI output that do not squarely fall within these categories 
because the output does not clearly relate to disadvantageous acts or decisions, nor does the 
output easily cross a certain threshold of toxicity. We see three main categories of such hard 
cases of generative harm that we take up in turn: (i) inadequate representation; (ii) harmful 
stereotypes; and (iii) misclassification. The first sub-category matches the descriptive category 
of inadequate representation discussed above. The second and third sub-categories, harmful 
stereotypes and misclassification, are part of the category of demeaning and abusive content in 
the descriptive sense described (see Section 'Generative AI and risks related to discrimination 
and hate speech'; see also the mapping between descriptive and legal categories below, Section 
'Summary concerning discrimination'). 
These cases are hard in two ways. First, they do not fit squarely within the traditional legal 
categories, as mentioned. Second, they also comprise phenomena that are not novel per se in 
                                                 
106 Cf. UNESCO and IRCAI (2024). 'Challenging systematic prejudices: an Investigation into Gender Bias in 
Large Language Models', 10. 
107 https://www.zeit.de/digital/2023-09/aleph-alpha-luminous-jonas-andrulis-generative-ki-rassismus [prompt 
and answer translated from German by the authors].



Source: data\tc20_2501.12962v1\referenced_papers\[44]_2407.10329.pdf (Page 51):

48 
detrimental effects on individuals or groups, avoiding an overly broad application that could 
become unmanageable. 
However, as seen, the introduction of Article 10 in the AI Act opens a potential new pathway 
for addressing issues of bias and representational harm. The provision’s requirement to mitigate 
biases in training data sets doesn't necessarily align with the legal definitions of illegal 
discrimination (see above). Given that Article 10 and Article 26 AI Act also emphasize the 
importance of representativeness, this could provide a novel legal mechanism for addressing 
unbalanced content at its source—within the AI's training data—rather than attempting to 
rectify biases post-output. 
3. Shaping technology 
Finally, the law can also shape genAI technology more specifically, with rules on testing and 
auditing, randomization, inclusive content, or providing users options for inclusivity. 
a. Testing and auditing for bias in generative AI 
A critical first step in shaping genAI involves rigorous testing and auditing to identify and 
correct biases. This process would seek to ensure that the outputs of AI systems do not 
perpetuate stereotypes or discrimination. However, in the AI Act, such discrimination-sensitive 
risk management procedures are only mandatory for extremely large generative models 
(general-purpose AI systems with systemic risk), such as ChatGPT, or for generative systems 
used in high-risk sectors (see above).211 Concerning content generation by smaller models under 
normal circumstances, these rules do not apply, leaving only non-discrimination law to fill the 
gaps. As seen, however, current law struggles to adequately address representational harms. In 
this respect, Article 53 AI Act, which covers all foundation models (general-purpose AI 
systems), should be updated to include risk assessment and mitigation concerning biased output, 
in the sense of incentivizing the use of state-of-the-art techniques for rendering content 
inclusive and representative of the overall (target) population. 
b. Randomization 
To ensure fair representation of protected groups, another strategy involves randomizing the 
selection of individuals from these groups in AI outputs. This strategy mirrors the idea of a 
search engine 'shuffling' search results with respect to important parameters.212 For instance, 
randomly varying the depiction of genders or ethnic backgrounds in generated images or 
narratives over time can help achieve a more diverse and accurate representation of society. 
However, as mentioned, Google's Gemini model sparked debate by over-diversifying historical 
                                                 
211 Articles 55 and 9 AI Act. 
212 Sandeep Pandey and others, 'Shuffling a Stacked Deck: The Case for Partially Randomized Ranking of Search 
Engine Results' (2005) Proceedings of the 31st VLDB Conference.



Source: data\tc20_2501.12962v1\referenced_papers\[44]_2407.10329.pdf (Page 8):

5 
2. Inadequate representation  
As noted, genAI models can also lead to another type of output that raises questions under non-
discrimination law by introducing a representational difference between protected groups: 
inadequate representation.21 In such cases, the AI system does not give output that is 
discriminatory in the form of hate speech, and it may not even be problematic if analyzed in a 
single instance of one output. However, there can still be a statistical discriminatory effect, in 
the sense that one protected group is over- or underrepresented in a set of outputs, created 
simultaneously or over time.22 For example, a genAI model may, if queried multiple times, 
predominantly mention men when discussing high-regarded jobs, and women when discussing 
less well-regarded jobs. Here, appropriate representation can be defined by many possible 
metrics, including empirical (e.g., the current distribution of men and women in high-regarded 
jobs) or normative metrics (e.g., equal representation of genders in outputs mentioning high-
regarded jobs).23  
AI-driven image generation systems can be biased in this way, for instance. The Washington 
Post reported in 2023 about Stable Diffusion XL: ‘63 percent of food stamp recipients were 
White and 27 percent were Black, according to the latest data from the Census Bureau’s Survey 
of Income and Program Participation. Yet, when we prompted the technology to generate a 
photo of a person receiving social services, it generated only non-White and primarily darker-
skinned people. Results for a “productive person,” meanwhile, were uniformly male, majority 
White, and dressed in suits for corporate jobs.’24 Similarly, one may easily imagine that food 
stamp recipients are portrayed with demeaning insignia of poverty and low socio-economic 
status. And, again as a real scenario, the Washington Post observed that Stable Diffusion XL 
was ‘depicting only women when asked to show people in the act of “cleaning.” Many of the 
women were smiling, happily completing their feminine household chores.’25 
Representational harms may only emerge in aggregate, through usage by multiple users, or 
through iterative querying by individual users. Representationally harmful outputs are 
                                                 
21 This effect is also called selection bias. Hannah Rose Kirk and others, 'Bias out-of-the-box: An empirical 
analysis of intersectional occupational biases in popular generative language models' (2021) 34 Advances in 
Neural Information Processing Systems 2611. Selection bias occurs when data selected for training machine 
learning models are biased, leading to skewed outcomes. This bias often arises when prototyping teams hyper-
focus on solving a particular problem without considering the broader context of data usage and generalization, 
see Toon Calders and Indre Žliobaitė, 'Why unbiased computational processes can lead to discriminative decision 
procedures', in Bart Custers, Toon Calders, Bart Schermer, Tal Zarsky (eds), Discrimination and Privacy in the 
Information Society: Data mining and profiling in large databases (Springer 2013) 43, 51. 
22 Sara Sterlie, Nina Weng and Aasa Feragen,, 'Non-discrimination Criteria for Generative Language Models' 
(2024) arXiv preprint arXiv:240308564, 1-2; Hannah Rose Kirk and others, 'Bias out-of-the-box: An empirical 
analysis of intersectional occupational biases in popular generative language models' (2021) 34 Advances in 
Neural Information Processing Systems 2611; Barclay Blair, Karley Buckley, Ashley Allen Carr, Coran Darling, 
Zev Eigen, Danny Tobey, Sam Tyner-Monroe, ‘Legal red teaming: A systematic approach to assessing legal risk 
of generative AI models’ DLA Piper White Paper (2024), 7. 
23 Sandra Wachter, Brent Mittelstadt and Chris Russell, 'Bias preservation in machine learning: the legality of 
fairness metrics under EU non-discrimination law' (2020) 123 W Va L Rev 735. 
24 Szu Yu Chen, ‘This is how AI image generators see the world’, Washington Post, 1 November 2023. 
https://www.washingtonpost.com/technology/interactive/2023/ai-generated-images-bias-racism-sexism-
stereotypes/ 
25 Szu Yu Chen, ‘This is how AI image generators see the world’, Washington Post, 1 November 2023. 
https://www.washingtonpost.com/technology/interactive/2023/ai-generated-images-bias-racism-sexism-
stereotypes/



### Claim 31/36

#### Claim Text
In fact, most of the experiments involving weak measurements have been performed in classical optical settings for small optical signal amplifications, such as, for the estimation of small phase, quantifying small angular rotations , measuring ultrasmall time delays , capturing tiny beam deflections and so on [11–14].

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 14):

15 
 
Concerning model selection failure, Zietlow et al. analysed a range 
of published approaches for bias-preserving fairness in computer vision 
(i.e., methods that claimed to be equalizing some form of error rate 
between groups) ,38 and observed that none used held-out data to 
determine error rates.39 This failure to use held-out data is troubling in 
computer vision where the usage of high-capacity models means 
training error goes to zero, meaning the only way to reliably estimate 
error rates is using held-out data. As such, while these approaches did 
show a decrease in both accuracy and inequality, it is likely that this 
was due to a general deterioration in model performance, and not 
because fairer models were explicitly selected.40 
Concerning inexpressive models, in the description above about how 
fair classifiers that maximise accuracy should behave, it is assumed 
that “difficult to label” individuals  can be easily identified , and that 
their group membership can be inferred. In some cases, this may not be 
true. If classifiers do not have access to data about group membership, 
it may not be possible t o infer group membership reliably enough to 
differentiate treatment or “flip” labels in an informed way.  
In both of these cases, the behaviour is often even more concerning 
than levelling down . In the standard case  discussed above we can be 
confident that at least one disadvantaged group is better off, and that 
the measure we are trying to equalise (e.g., recall or selection rate) has 
improved. In cases of model selection failure and inexpressive models, 
we cannot be confident in this knowledge. While a method for enforcing 
group fairness must result in lower inequality to be considered a 
success, it may do this by decreasing or increasing performance for 
every group, and there is no guarantee as to how this will be 
accomplished in practice . For example, Zietlow et al. found that most 
fairness methods in computer vision improved equal opportunity by 
decreasing the average recall for every group across a range of tasks.41 
In many high -risk situations, such as medical testing, the use of 
methods that improve equality by decreasing performance (e.g., 
diagnosis rates) for everyone would be grossly inappropriate. 
3.2 Levelling down in practice 
To further establish the prevalence of levelling down in fairML, we 
demonstrate its occurrence using two popular fairness toolkits using 
real-world code bases. Specifically, we illustrate how levelling down 
occurs using standard examples taken from the “How To” guides for 
FairLearn and IBM AI Fairness 360. 
 
38 Wachter, Mittelstadt, and Russell, supra note 10. 
39 ZIETLOW ET AL., supra note 8. 
40 Id. 
41 Id.



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 18):

Published in Transactions on Machine Learning Research (08/2022)
A BIG-Bench analysis
A.1 Cross-entropy loss analysis
Here we study how scaling curves may appear diﬀerently depending on the evaluation metric used to measure
performance. We will focus on the six few-shot prompted BIG-Bench tasks that we consider emergent
for LaMDA models. Three of these tasks are generative and use Exact Match (EM) or BLEU (Papineni
et al., 2002) as the evaluation metric. The other three tasks are classiﬁcation and use accuracy (acc) as the
evaluation metric.
In the scaling curves for these tasks, peformance in EM/BLEU/acc is close to random for small models
(≤1022 FLOPs /≤27B params). We will compare these scaling curves against alternative plots that have a
diﬀerent y-axis measured by cross-entropy loss. Cross-entropy loss diﬀers from EM/BLEU/acc in that it
captures improvements in performance (the predicted distribution getting closer to ground truth) even when
the EM/BLEU/acc is random. For example, if two examples are both wrong as measured by EM/BLEU/acc,
one example may be closer to the ground truth in terms of probabilities, and this information is captured by
the cross-entropy loss.
These plots are expected to look like one of the following:
• Outcome 1: For the model scales where EM/BLEU/acc is random, cross-entropy loss also does not
improve as scale increases. This outcome implies that for these scales, the model truly does not get
any better at the tasks.
• Outcome 2: For the model scales where EM/BLEU/acc is random, cross-entropy loss does improve.
This outcome implies that the models do get better at the task, but these improvements are not
reﬂected in the downstream metric of interest. The broader implication is that scaling small models
improves the models in a way that is not reﬂected in EM/BLEU/Acc, and that there is some critical
model scale where these improvements enable the downstream metric to increase to above random as
an emergent ability.
We ﬁnd that all six BIG-Bench tasks fall under Outcome 2, and detail this analysis below. Overall, the
conclusion from this analysis is that small models do improve in some ways that downstream metrics that
EM/BLEU/Acc do not capture. However, these tasks are still considered emergent, and this analysis does
not provide any straightforward indicators of how to predict such emergent behaviors.
A.1.1 Generative tasks
Figure 5 shows the cross-entropy loss on the three generative BIG-Bench tasks (modiﬁed arithmetic, IPA
transliterate, and word unscramble) alongside the downstream evaluation metrics used in Figure 2. For all
three tasks, notice that while the error rate is nearly 100% for small models (≤1022 FLOPs /≤27B params),
the cross-entropy loss does actually improve for these model sizes. At the point of emergence as measured by
error rate, we also see an “elbow” in performance improvement for cross-entropy loss.
A.1.2 Classiﬁcation tasks
Figure 6 (middle row) shows the cross-entropy loss of the three classiﬁcation BIG-Bench tasks. Similar to the
generative tasks, when the error rate is close to random, cross-entropy loss consistently still improves for
models trained with more compute. This again shows that performance as computed by accuracy can mask
consistent improvements in the likelihood of the target sequences.
We also perform an additional analysis of the multiple choice emergent tasks in Figure 6 (bottom row),
which shows the log probabilities of the correct response and incorrect response(s). We ﬁnd that the cross-
entropy loss decreases for both the correct and incorrect responses in the three emergent multiple choice
tasks. Counterintuitively, both log-probabilities can decrease in tandem even when the probability across
all available multiple choice responses is normalized. The reason is that larger models produce less-extreme
probabilities (i.e., values approaching 0 or 1) and therefore the average log-probabilities have fewer extremely
19



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 17):

18 
 
However, even after identifying the drop in performance, it remains an 
open question, then, whether the phenomena observed above are 
genuine cases of levelling down.  
 
 
Figure 2 - Levelling down in IBM AI Fairness 360 on the UCI Adult Dataset 
 
The field’s focus on minimizing inequality while maximizing accuracy 
has left us without the tools needed to achieve fairness purely by 
levelling up, which wou ld mitigate avoidable material harms, 
stigmatisation and loss of solidarity for both advantaged and 
disadvantaged groups (see: Section 4). Simply put, given how fai rness 
is currently enforced and reported in ML, we cannot determine if 
harming particular groups is in fact necessary and justified, or merely 
the path of least resistance to achieve parity. We will definitively 
answer this in Section 6 where we propose ne w levelling up tools for 
algorithmic fairness and show that they can reduce harms and improve 
performance for disadvantaged groups without disadvantaging others. 
3.3 Levelling down in theory 
These reporting limitations make it difficult to determine the frequency 
and justifiability of levelling down in fairML. However, an alternative 
approach is to determine whether the theoretical foundations of popular 
fairness measures consider levelling down to be a legitimate 
distribution mechanism. This line of inquiry cannot, of course, establish 
its empirical prevalence, but can at least indicate whether levelling 
down is a theoretically coherent course of action to enforce  group 
fairness measures.



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 27):

Published in Transactions on Machine Learning Research (08/2022)
1B 10B 100B
0
5
10
15
20
25
No chain
of thought
Chain of
thought
GSM8K Accuracy (%)
(A) Math word
problems
1B 10B 100B
30
40
50
60
70
No
instruction
tuning
Instruction
tuning
10 NLU task average
(B) Instruction
following
10M 100M 1B
0
20
40
60
80
100
No
scratchpad
Scratchpad
Model scale (number of parameters)
Accuracy (%)
(C) 8-digit addition
1B 10B 100B
100
101
Letter
choices
T/F
% ECE (log-scale, decreasing)
(D) Calibration
Figure 12: Specialized prompting or ﬁnetuning methods can be emergent in that they do not have a positive
eﬀect until a certain model scale. A: Wei et al. (2022b). B: Wei et al. (2022a). C: Nye et al. (2021). D:
Kadavath et al. (2022). The model shown in A-C is LaMDA (Thoppilan et al., 2022), and the model shown
in D is from Anthropic.
1B 100B
0
20
40
60
80
100Accuracy (%)
(A) TriviaQA
(GPT-3)
1B 100B
60
70
80
90Accuracy (%)
(B) Physical QA
(GPT-3)
8B 62B 540B
0
10
20
30
40
50
60
Model scale (number of parameters)
Accuracy (%)
(C) GSM8K
(PaLM)
3B 9B 80B
0
10
20
30
40
50
60VQA accuracy (%)
(D) OKVQA
(Flamingo)
Prior SOTA (pretrain–ﬁnetune)
Few-shot prompting
Figure 13: On some benchmarks, task-general models (not explicitly trained to perform a task) surpass prior
state-of-the-art performance held by a task-speciﬁc model. A & B: Brown et al. (2020). C: Chowdhery et al.
(2022). D: Alayrac et al. (2022).
28



Source: data\tc20_2501.12962v1\referenced_papers\[70]_2302.02404.pdf (Page 34):

35 
 
distribution principles they produce .137 Simply put, fairness is treated 
as a standardized mathematical problem to be solved. Justifying how a 
measure is satisfied in practice, linking it to some underlying equality 
goal, and exploring whether a less equal but less harmful path would 
be preferable are rarely part of enforcing fairness .138 Debates in  
distributive justice recognise that “different people may value the same 
outcome or set of harms and benefits differently .” This fact is not 
reflected in the tendency in fairML to assume “a uniform valuation of 
decision outcomes across different populations” 139 and use cases, which 
reduces a highly complex, value -laden debate and set of theories and 
decisions to an oversimplified homogenous set of assumptions.  
The same type of erro r can cause substantially different types of 
harm depending on the use case. Take facial recognition as an example 
ML application. If facial recognition is used by police to identify people 
with outstanding warrants in crowds, the harm of a false positive is an 
unjustified arrest. In contrast, if it is used to track perceived compliance 
with visa requirements,140 the harm of false negatives is perceived non-
compliance with a monitoring regime that could have significant legal 
ramifications (e.g., deportation ). The harms of false positives and 
negatives likewise vary for facial recognition used for loan decisions or 
job interviews.  If the enforcement of fairness in ML is to resemble 
comparable legal decisions (where, as we have seen, levelling down can 
be justified), it is essential to consider the specific types and severity of 
harms actually suffered by affected populations. 
Researchers, developers, and deployers of ‘fair’ ML systems  do not 
currently seriously engage with such questions at a local level. 
Levelling down is arguably not viewed as something that requires 
justification. At most, o ne need only justify the choice of fairness 
measure; the steps taken to satisfy it in practice are  normatively 
irrelevant.141 At best, tenuous connections are drawn between fairness 
measures and complementary political or ethical theories .142143 Works 
that merely link methods and measures to complementary theories of 
equality suggest that researchers using those methods and measures 
believe in those theories, or have ex plicitly chosen them, and have 
 
137 Kuppler et al., supra note 49. 
138 Kasy and Abebe, supra note 6; Kuppler et al., supra note 49. 
139 Binns, supra note 63 at 6. 
140 Nicola Kelly, Facial recognition smartwatches to be used to monitor foreign offenders 
in UK , THE GUARDIAN, Aug. 5, 2022, 
https://www.theguardian.com/politics/2022/aug/05/facial-recognition-smartwatches-to-
be-used-to-monitor-foreign-offenders-in-uk (last visited Aug 13, 2022). 
141 Wachter, Mittelstadt, and Russell, supra note 26; Wachter, Mittelstadt, and Russell, 
supra note 10. 
142 Kuppler et al., supra note 49; Binns, supra note 7. 
143 In that sense they may inform model development or the scope of a research study 
but are not offered as a justification for enforcing group fairness in specific cases.



### Claim 32/36

#### Claim Text
In this section we present the µ-chemoEH model, a generalization of the microscopic model illustrated in , that relates the force generated by the motor-filament interaction to the relative sliding of the filaments.

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 23):

Published in Transactions on Machine Learning Research (08/2022)
B Further MMLU analysis
In §5.3, we saw how emergent performance on MMLU for Gopher and Chinchilla could be viewed as a
function of training FLOPs, model parameters, and WikiText103 perplexity. Because MMLU is actually
a suite of 57 topics spanning four categories, we ask the question of whether certain categories were more
conducive to emergence than others. This is similar in nature to the BIG-Bench analysis done in the prior
section (Appendix A.3). One diﬀerence here is that the MMLU categories are mutually exclusive—each topic
only has one category, whereas a single BIG-Bench task often had multiple keyword tags. However, there
are only four categories and 57 tasks for MMLU (compared with 200+ tasks and dozens of keywords for
BIG-Bench).
In Figure 10, we stratify the performance of MMLU among the four categories given in the benchmark
(Humanities, STEM, Social Science, and other), and plot them with multiplex-axes: training FLOPs, model
parameters, and WikiText103 perplexity. It is clear that Social Science and Humanities have the largest
jump in performance from the second-largest to the largest model, and STEM has the smallest jump in
performance. For a givenx-axis (training FLOPs, model parameters, WikiText103 ppl), all four categories
had similar plot shapes. This result is also summarized in Figure 9.
0 25 50 75 100
0
25
50
75
100
Performance of
second-largest model
Performance of
largest model
Chinchilla: Humanities
Chinchilla: STEM
Chinchilla: Other
Chinchilla: Social Science
Gopher: Humanities
Gopher: STEM
Gopher: Other
Gopher: Social Science
Random performance (25%)
Figure 9: Performance of largest Chinchilla and Gopher models (70B and 280B, respectively) compared with
the second-largest model (7B parameters for both Chiinchlla and Gopher). The 7B Chinchilla and Gopher
models perform around random (25%) for all four categories. So the categories that improved the most from
7B to 70B/280B are humanities and social science, whereas STEM (Science, Technology, Engineering, and
Mathematics) improved the least.
24



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 24):

Published in Transactions on Machine Learning Research (08/2022)
1020 1022 1024
0
20
40
60
80
100
Training FLOPs
Accuracy (%)
1B 10B 100B
0
20
40
60
80
100
Model parameters
MMLU: Humanities
Chinchilla Gopher Random
2015 10 7 5
0
20
40
60
80
100
WikiText103 ppl
1020 1022 1024
0
20
40
60
80
100
Training FLOPs
Accuracy (%)
1B 10B 100B
0
20
40
60
80
100
Model parameters
MMLU: Science, Technology, Engineering, and Math (STEM)
2015 10 7 5
0
20
40
60
80
100
WikiText103 ppl
1020 1022 1024
0
20
40
60
80
100
Training FLOPs
Accuracy (%)
1B 10B 100B
0
20
40
60
80
100
Model parameters
MMLU: Other
2015 10 7 5
0
20
40
60
80
100
WikiText103 ppl
1020 1022 1024
0
20
40
60
80
100
Training FLOPs
Accuracy (%)
1B 10B 100B
0
20
40
60
80
100
Model parameters
MMLU: Social Science
2015 10 7 5
0
20
40
60
80
100
WikiText103 ppl
Figure 10: Emergence of Chinchilla and Gopher on MMLU. In the four rows, performance is stratiﬁed into
four supercategories. For both Chinchilla and Gopher, Social Science had the highest level of emergence while
STEM was the least emergent.
25



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 35):

3.2 Algorithmic Interventions
where nrepresents the number of distinct distributions or domains, and λmin governs the extent of risk extrapola-
tion. Moving on to the V-REx term, it can be modeled as:
rV−REx(θ) =αVar
n
r1(θ),...,r n(θ)
o
+
nX
e=1
re(θ)
where α≥0 controls the trade-off between risk reduction and enforcing risk equality.
In the MM-REx term, the λmin can set nearly −∞; therefore, the loss of specific domains may be high, mean-
ing that the model may learn the spurious correlations. Minimizing the MM-REx and V-REx can reduce training
risks and increase the similarity of training risks, encouraging the model to learn invariant relationships. Further-
more, REx has shown significant promise in experimental settings (Krueger et al., 2021), particularly in causal
identification, making it a compelling approach for achieving robust generalization.
Tackle Distribution Shift in LLMs In the context of LLMs, prior research has shown that RL often exploits
shortcuts to achieve high rewards, overlooking challenging samples (Deng et al., 2023b). This evasion of long-
tail training samples prevents LLMs from effectively handling distribution shifts in general scenarios, which falls
short of expectations for these models: as universal AI assistants, they should maintain consistent performance
across various domains. Recently, many works have attempted to implement cross-distribution aggregation in
LLMs to address this issue. Zheng et al. (2024) employ RL to learn uniform strategies across diverse data groups
or domains, automatically categorizing data and deliberately maximizing performance variance. This strategy
increases the learning capacity for challenging data and avoids over-optimization of simpler data. Yao et al. (2024)
concentrate on exploiting inter-domain connections. Specifically, they acquire training-domain-specific functions
during the training phase and adjust their weights based on domain relations in the testing phase, achieving robust
OOD generalization.
3.2.2 Navigation via Mode Connectivity
Following the above discussion about cross-distribution aggregation, in this section, we introduce mode connectiv-
ity as the prerequisite content. Then, we primarily discuss the Connectivity-Based Fine-Tuning (CBFT) (Lubana
et al., 2023) method, illustrating how mode connectivity navigates the model to predict based on invariant relation-
ships instead of spurious correlations by changing few parameters.
Mode Connectivity Mode connectivity refers to the phenomenon where one can identify a straightforward path
within the loss function space that connects two or more distinct local minima or patterns (Garipov et al., 2018;
Draxler et al., 2018). In line with prior research (Benton et al., 2021; Pittorino et al., 2022; Lubana et al., 2023), a
formal definition can be defined as follows:
The model’s loss on a dataset Dis represented as L(f(D;θ)), where θ denotes the optimal parameters of the
model, and f(D;θ) signifies the model trained on dataset D. We define θas a minimizer of the loss on this dataset
if L(f(D;θ)) <ϵ, where ϵis a small scalar value.
Minimizers θ1 and θ2, achieved through training on dataset D, are considered to be mode-connected if there
exists a continuous path γ from θ1 to θ2 such that, as θ0 varies along this path γ, the following condition is
consistently upheld:
L

f(D;θ0)

≤t·L

f(D;θ1)

+ (1 −t) ·L

f(D;θ2)

, ∀t∈[0,1].
In essence, mode connectivity entails consistently finding a connecting pathway among minimizers in the pa-
rameter space, traversing regions of low loss without delving into regions of highly high loss. This implies that even
when making minor adjustments to the model’s parameters within the parameter space, the model’s performance
can remain relatively stable, mitigating significant performance degradation (Garipov et al., 2018). This concept
lays the foundation for designing more effective optimization algorithms, enabling models to share knowledge and
experiences across different tasks, enhancing both model performance and generalization capabilities.
Furthermore, we can define two models as mechanistically similar if they employ the same attributes of inputs for
making predictions. Some research has demonstrated that the absence of linear connectivity implies mechanistic
dissimilarity, suggesting that simple fine-tuning may not suffice to eliminate spurious attributes learned during
the pre-training phase (Lubana et al., 2023; Juneja et al., 2022). However, it is promising to address non-linearly
connected regions through fine-tuning, thereby effectively modifying the model’s mechanisms to resolve the issue
of OOD misgeneralization.
Connectivity-Based Fine-tuning (CBFT) As discussed above, recent research has suggested that the absence
of linear connectivity between two models implies a fundamental mechanistic dissimilarity. Lubana et al. (2023)
finds that models tend to develop similar inference mechanisms when trained on similar data. This could be a
36



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 1):

Published in Transactions on Machine Learning Research (08/2022)
alia). We will consider the following general deﬁnition of emergence, adapted from Steinhardt (2022) and
rooted in a 1972 essay called “More Is Diﬀerent” by Nobel prize-winning physicist Philip Anderson (Anderson,
1972):
Emergence is when quantitative changes in a system result in qualitative changes in behavior.
Here we will explore emergence with respect to model scale, as measured by training compute and number of
model parameters. Speciﬁcally, we deﬁneemergent abilities of large language modelsas abilities that are
not present in smaller-scale models but are present in large-scale models; thus they cannot be predicted by
simply extrapolating the performance improvements on smaller-scale models (§2).1 We survey emergent
abilities as observed in a range of prior work, categorizing them in settings such as few-shot prompting (§3)
and augmented prompting strategies (§4). Emergence motivates future research on why such abilities are
acquired and whether more scaling will lead to further emergent abilities, which we highlight as important
questions for the ﬁeld (§5).
2 Emergent Abilities Deﬁnition
As a broad concept, emergence is often used informally and can be reasonably interpreted in many diﬀerent
ways. In this paper, we will consider a focused deﬁnition of emergent abilities of large language models:
An ability is emergent if it is not present in smaller models but is present in larger models.
Emergent abilities would not have been directly predicted by extrapolating a scaling law (i.e. consistent
performance improvements) from small-scale models. When visualized via a scaling curve (x-axis: model
scale, y-axis: performance), emergent abilities show a clear pattern—performance is near-random until a
certain critical threshold of scale is reached, after which performance increases to substantially above random.
This qualitative change is also known as aphase transition—a dramatic change in overall behavior that would
not have been foreseen by examining smaller-scale systems (Huberman & Hogg, 1987).
Today’s language models have been scaled primarily along three factors: amount of computation, number
of model parameters, and training dataset size (Kaplan et al., 2020; Hoﬀmann et al., 2022). In this paper,
we will analyze scaling curves by plotting the performance of diﬀerent models where training compute for
each model is measured in FLOPs on thex-axis (Hoﬀmann et al., 2022). Because language models trained
with more compute tend to also have more parameters, we additionally show plots with number of model
parameters as thex-axis in Appendix D (see Figure 11 and Figure 12, as well as Figure 4 and Figure 10).
Using training FLOPs or model parameters as thex-axis produces curves with similar shapes due to the fact
that most dense Transformer language model families have scaled training compute roughly proportionally
with model parameters (Kaplan et al., 2020).
Training dataset size is also an important factor, but we do not plot capabilities against it because many
language model families use a ﬁxed number of training examples for all model sizes (Brown et al., 2020; Rae
et al., 2021; Chowdhery et al., 2022). Although we focus on training computation and model size here, there
is not a single proxy that adequately captures all aspects of scale. For example, Chinchilla (Hoﬀmann et al.,
2022) has one-fourth as many parameters as Gopher (Rae et al., 2021) but uses similar training compute; and
sparse mixture-of-expert models have more parameters per training/inference compute than dense models
(Fedus et al., 2021; Du et al., 2021). Overall, it may be wise to view emergence as a function of many
correlated variables. For example, later in Figure 4 we will also plot emergence as a function of WikiText103
perplexity (Merity et al., 2016), which happens to closely correlate with training computation for Gopher/
Chinchilla (though this correlation may not hold in the long-run).
Note that the scale at which an ability is ﬁrst observed to emerge depends on a number of factors and is
not an immutable property of the ability. For instance, emergence may occur with less training compute
1This survey focuses on pre-trained Transformer language models. Emergent abilities in NLP more broadly, however, could
go back to Miller et al. (2004), Liang (2005), or earlier.
2



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 18):

Published in Transactions on Machine Learning Research (08/2022)
A BIG-Bench analysis
A.1 Cross-entropy loss analysis
Here we study how scaling curves may appear diﬀerently depending on the evaluation metric used to measure
performance. We will focus on the six few-shot prompted BIG-Bench tasks that we consider emergent
for LaMDA models. Three of these tasks are generative and use Exact Match (EM) or BLEU (Papineni
et al., 2002) as the evaluation metric. The other three tasks are classiﬁcation and use accuracy (acc) as the
evaluation metric.
In the scaling curves for these tasks, peformance in EM/BLEU/acc is close to random for small models
(≤1022 FLOPs /≤27B params). We will compare these scaling curves against alternative plots that have a
diﬀerent y-axis measured by cross-entropy loss. Cross-entropy loss diﬀers from EM/BLEU/acc in that it
captures improvements in performance (the predicted distribution getting closer to ground truth) even when
the EM/BLEU/acc is random. For example, if two examples are both wrong as measured by EM/BLEU/acc,
one example may be closer to the ground truth in terms of probabilities, and this information is captured by
the cross-entropy loss.
These plots are expected to look like one of the following:
• Outcome 1: For the model scales where EM/BLEU/acc is random, cross-entropy loss also does not
improve as scale increases. This outcome implies that for these scales, the model truly does not get
any better at the tasks.
• Outcome 2: For the model scales where EM/BLEU/acc is random, cross-entropy loss does improve.
This outcome implies that the models do get better at the task, but these improvements are not
reﬂected in the downstream metric of interest. The broader implication is that scaling small models
improves the models in a way that is not reﬂected in EM/BLEU/Acc, and that there is some critical
model scale where these improvements enable the downstream metric to increase to above random as
an emergent ability.
We ﬁnd that all six BIG-Bench tasks fall under Outcome 2, and detail this analysis below. Overall, the
conclusion from this analysis is that small models do improve in some ways that downstream metrics that
EM/BLEU/Acc do not capture. However, these tasks are still considered emergent, and this analysis does
not provide any straightforward indicators of how to predict such emergent behaviors.
A.1.1 Generative tasks
Figure 5 shows the cross-entropy loss on the three generative BIG-Bench tasks (modiﬁed arithmetic, IPA
transliterate, and word unscramble) alongside the downstream evaluation metrics used in Figure 2. For all
three tasks, notice that while the error rate is nearly 100% for small models (≤1022 FLOPs /≤27B params),
the cross-entropy loss does actually improve for these model sizes. At the point of emergence as measured by
error rate, we also see an “elbow” in performance improvement for cross-entropy loss.
A.1.2 Classiﬁcation tasks
Figure 6 (middle row) shows the cross-entropy loss of the three classiﬁcation BIG-Bench tasks. Similar to the
generative tasks, when the error rate is close to random, cross-entropy loss consistently still improves for
models trained with more compute. This again shows that performance as computed by accuracy can mask
consistent improvements in the likelihood of the target sequences.
We also perform an additional analysis of the multiple choice emergent tasks in Figure 6 (bottom row),
which shows the log probabilities of the correct response and incorrect response(s). We ﬁnd that the cross-
entropy loss decreases for both the correct and incorrect responses in the three emergent multiple choice
tasks. Counterintuitively, both log-probabilities can decrease in tandem even when the probability across
all available multiple choice responses is normalized. The reason is that larger models produce less-extreme
probabilities (i.e., values approaching 0 or 1) and therefore the average log-probabilities have fewer extremely
19



### Claim 33/36

#### Claim Text
Evolutionary game theory, as a mathematical analysis tool, is well suited to be combined with epidemiological models , to describe the vaccination dynamics.

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 33):

3.2 Algorithmic Interventions
Algorithmic
Interventions
Cross-Distribution
Aggregation
DRO
[69; 565; 263; 64;
306; 217; 593;
618; 396; 205]
IRM [726; 35]
REx [726; 566; 35]
Navigation via
Mode Connectivity
Mode Connectivity [260; 200; 245;
74; 570; 444; 355]
CBFT [444]
Figure 7: A tree diagram summarizing the key concepts and literature related to Algorithmic Interventions. The
root node represents Algorithmic Interventions that aim to steer optimization during the training process. The
main branches represent two main methods, namely cross-distribution aggregation (which aims to minimize risks
on different distributions during training to find a predictor based on the invariant relationship instead of spurious
features) and navigation via mode connectivity (which aims to fine-tune based on mode connectivity to enhance
model generalization performance). Further sub-branches list vital techniques such as Distributionally Robust
Optimization (DRO), Invariant Risk Minimization (IRM), Risk Extrapolation (REx), and Connectivity-based Fine-
tuning (CBFT).
dation systems, where the content selected by the recommendation algorithms might change users’ preferences
and behaviors, leading to a shift in user distribution. The distribution shift, in turn, further affects the output of the
recommendation algorithms (Carroll et al., 2022). As AI systems increasingly impact the world, we also need to
consider the potential further impacts on the data distribution of the entire society after agents are integrated into
human society.
3.2 Algorithmic Interventions
When illustrating the algorithmic intervention methods, we first outline two classes of methods that steer opti-
mization on various distributions during training to relieve distribution shift, namely, cross-distribution aggregation
(§3.2.1) and navigation via mode connectivity (§3.2.2).
In the first part, we cover methods ranging from the initial approach of empirical risk minimization (ERM)
(Vapnik, 1991) to risk extrapolation (REx) (Krueger et al., 2021), a method conceived to mitigate issues arising
from models’ dependence on spurious features. In the second part, we introduce connectivity-based fine-tuning,
which guides the navigation of the loss landscape during training to encourage convergence upon non-spurious
correlations, and which does so using insights from mode connectivity (Lubana et al., 2023).
3.2.1 Cross-Distribution Aggregation
One of the main reasons for distribution shift is spurious correlations in the model that are distinct from core ob-
jectives (Geirhos et al., 2019). By integrating learning information of different domains (or different distributions)
into the optimization objective, we expect the model to learn truthful information and invariant relationships. In the
following paragraphs, we first introduce ERM as the background and then introduce some methods to directly learn
how to address distribution shift by integrating loss landscapes of different distributions in the training process.
Empirical Risk Minimization (ERM) Consider a scenario where a model has been developed to identify objects
by their features effectively. The optimization target can be expressed as:
R(w) =
Z
L

y,f (x,w)

dP(x,y)
where L(y,f (x,w)) denotes the loss between data labels y and model outputs f(x,w), while P(x,y) signifies the
target data distribution (Vapnik, 1991).
Nevertheless, a bias often exists between the dataset and the real world, implying that the features learned from
the dataset may not necessarily be the ones we intend for the model to acquire. ERM is a strategy employed
in statistical methods to optimize this bias. It operates on the assumption that, given the inaccessibility of the
real-world target data distribution, the empirical data within the dataset should, ideally, closely approximate this
unknown target distribution (Vapnik, 1991; Zhang et al., 2018b). In this context, the objective function is optimized
34



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 18):

Published in Transactions on Machine Learning Research (08/2022)
A BIG-Bench analysis
A.1 Cross-entropy loss analysis
Here we study how scaling curves may appear diﬀerently depending on the evaluation metric used to measure
performance. We will focus on the six few-shot prompted BIG-Bench tasks that we consider emergent
for LaMDA models. Three of these tasks are generative and use Exact Match (EM) or BLEU (Papineni
et al., 2002) as the evaluation metric. The other three tasks are classiﬁcation and use accuracy (acc) as the
evaluation metric.
In the scaling curves for these tasks, peformance in EM/BLEU/acc is close to random for small models
(≤1022 FLOPs /≤27B params). We will compare these scaling curves against alternative plots that have a
diﬀerent y-axis measured by cross-entropy loss. Cross-entropy loss diﬀers from EM/BLEU/acc in that it
captures improvements in performance (the predicted distribution getting closer to ground truth) even when
the EM/BLEU/acc is random. For example, if two examples are both wrong as measured by EM/BLEU/acc,
one example may be closer to the ground truth in terms of probabilities, and this information is captured by
the cross-entropy loss.
These plots are expected to look like one of the following:
• Outcome 1: For the model scales where EM/BLEU/acc is random, cross-entropy loss also does not
improve as scale increases. This outcome implies that for these scales, the model truly does not get
any better at the tasks.
• Outcome 2: For the model scales where EM/BLEU/acc is random, cross-entropy loss does improve.
This outcome implies that the models do get better at the task, but these improvements are not
reﬂected in the downstream metric of interest. The broader implication is that scaling small models
improves the models in a way that is not reﬂected in EM/BLEU/Acc, and that there is some critical
model scale where these improvements enable the downstream metric to increase to above random as
an emergent ability.
We ﬁnd that all six BIG-Bench tasks fall under Outcome 2, and detail this analysis below. Overall, the
conclusion from this analysis is that small models do improve in some ways that downstream metrics that
EM/BLEU/Acc do not capture. However, these tasks are still considered emergent, and this analysis does
not provide any straightforward indicators of how to predict such emergent behaviors.
A.1.1 Generative tasks
Figure 5 shows the cross-entropy loss on the three generative BIG-Bench tasks (modiﬁed arithmetic, IPA
transliterate, and word unscramble) alongside the downstream evaluation metrics used in Figure 2. For all
three tasks, notice that while the error rate is nearly 100% for small models (≤1022 FLOPs /≤27B params),
the cross-entropy loss does actually improve for these model sizes. At the point of emergence as measured by
error rate, we also see an “elbow” in performance improvement for cross-entropy loss.
A.1.2 Classiﬁcation tasks
Figure 6 (middle row) shows the cross-entropy loss of the three classiﬁcation BIG-Bench tasks. Similar to the
generative tasks, when the error rate is close to random, cross-entropy loss consistently still improves for
models trained with more compute. This again shows that performance as computed by accuracy can mask
consistent improvements in the likelihood of the target sequences.
We also perform an additional analysis of the multiple choice emergent tasks in Figure 6 (bottom row),
which shows the log probabilities of the correct response and incorrect response(s). We ﬁnd that the cross-
entropy loss decreases for both the correct and incorrect responses in the three emergent multiple choice
tasks. Counterintuitively, both log-probabilities can decrease in tandem even when the probability across
all available multiple choice responses is normalized. The reason is that larger models produce less-extreme
probabilities (i.e., values approaching 0 or 1) and therefore the average log-probabilities have fewer extremely
19



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 32):

3.1 The Distribution Shift Challenge
Goal MisgeneralizationAuto-inducedDistribution ShiftChallenges from Distribution Shift
AlgorithmicInterventions
Risk ExtrapolationInvariant Risk MinimizationDistributionally Robust OptimizationCross-Distribution Aggregation
Navigation via Mode Connectivity
…
Mode ConnectivityConnectivity-based Fine-tuning
Data DistributionInterventions
Adversarial Training:Incorporating Adversarial Pressures
Cooperative Training:Incorporating Multi-Agent Dynamics
Perturbation-Based ATUnrestricted ATModalitiesModel Types
Environment BuildingMixed-Motive MARLZero-shot Coordination
Socially Realistic SettingsFully Cooperative MARL
…
Figure 6: Framework of learning under distribution shift. The main challenges stemming from the distribution
shift are goal misgeneralization and auto-induced distribution shift (§3.1). In our framework, we also introduce
two kinds of methods to address distribution shift: algorithmic interventions (§3.2) that steer optimization during
training, and data distribution interventions (§3.3) that expand the training distribution in a targeted manner by
introducing real-world elements.
the interaction between the advisor and the environment) in IL can result in goal misgeneralization (De Haan et al.,
2019; Tien et al., 2022).
One major danger from goal misgeneralization lies in the indistinguishability between “optimizing for what
human really wants” and “optimizing for human thumbs-ups”; 31 the latter includes potentially deceiving or ma-
nipulating human evaluators (Shevlane et al., 2023) to receive their thumbs-ups. For example, Amodei et al. (2017)
discovered that in a task where a robotic hand is supposed to grasp a small ball, the robotic hand fakes the action
by using parallax in front of the lens to appear as if it has grasped the ball, without actually doing so. This behavior
deceives the human annotator into thinking that the task has been completed.
When an AI system is trained or finetuned with human feedback, it is impossible to distinguish the two goals
since both perform perfectly in training, and it is unclear which one the AI system will learn. In fact, during
training, the human evaluators might be deceived or manipulated, implying that the AI system may be more
strongly incentivized to optimize for human thumbs-ups rather than what the human wants. Current examples of
this phenomenon exist in recommender systems (Kalimeris et al., 2021; Adomavicius et al., 2022), LLMs (Perez
et al., 2023), and RL systems (Amodei et al., 2017).
Finally, one failure mode closely related to goal misgeneralization is the misalignment ofmesa-optimizers (Hub-
inger et al., 2019c), where the ML model with learned model weights performs optimization within itself during
inference (“mesa-optimization”) (Hubinger et al., 2019c; Dai et al., 2023), and the objective of this optimization is
not aligned with the model’s training objective.
Auto-Induced Distribution Shift (ADS) While training AI systems, we often consider the strengths and weak-
nesses of the agents themselves only and overlook the impact that these agents have on the environment. Past
research often assumed that data is independently and identically distributed (Besbes et al., 2022), ignoring the
effect of algorithms on data distribution. However, Krueger et al. (2020) posited that, in reality, agents could influ-
ence the environment during the decision-making and execution process, thus altering the distribution of the data
generated by the environment. They referred to this type of issue as ADS. A real-world example is in recommen-
31Here, human thumbs-ups refer to high-reward feedback from human advisors or environment. However, AI systems may
deliberately follow human preferences or deceive to get high rewards from humans, but actually don’t really learn intended
goals (i.e., what human really wants).
33



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 20):

2.3 Policy Learning
extent of preference (Cheng et al., 2010b). On the other hand, ordinal preferences entail a graded assess-
ment of a fixed set of items as either preferred, less preferred, or intermediary, etc., enabling the depiction
of user preferences without including specific numerical measurements (Cheng et al., 2010a).
• Relative Preferences. Relative preferences define the preference relation between items.
– Total Order . This form establishes a comprehensive preference relation covering all item pairs, asserting
an absolute ordering of preferences ranging from the most preferred to the least (Hüllermeier et al., 2008).
– Partial Order . Because users may not exhibit a distinct preference between two items in some instances
(Cheng et al., 2010c), this allows for incomparable item pairs.
Reward Model Reward modeling transfers comparison feedback (Fürnkranz and Hüllermeier, 2010; Wirth et al.,
2017) to the scalar reward form, facilitating policy learning (Christiano et al., 2017; Cabi et al., 2020; Touvron et al.,
2023). Given pairs of actions (y1,y2) performed by the RL agent in the same state. The preference is denoted as
yw ≻yl |x, where yw, yl represents the preferred and less preferred action respectively among(y1,y2). We assume
these preferences emerge from a latent reward model r∗(x,y), which we lack direct access to. Several methods
exist to model such preferences, e.g., the Bradly-Terry Model (Bradley and Terry, 1952), Palckett-Luce ranking
model (Plackett, 1975), etc. Under the BT model, the distribution of human preference, denoted as p∗, can be
formalized as,
p∗(y1 ≻y2 |x) =
exp

r∗(x,y1)

exp

r∗(x,y1)

+ exp

r∗(x,y2)
 = σ

r∗(x,y1) −r∗(x,y2)

.
where σ(x) = 1/(1 + exp(−x)) is the logistic sigmoid function. Subsequently, we use the derived preference
rankings to train the parameterized reward model, optimizing its parameters through maximum likelihood.
LR (θ) = −E(x,yw,yl)∼D

log

σ

rθ(x,yw) −rθ(x,yl)

In this negative log-likelihood loss, the problem is a binary classification task, where Dsignifies the static dataset
x(i),y(i)
w ,y(i)
l
N
i=1
sampled from p∗(i.e., human-labeled comparisons).
Reward models enable human users to impart specific preferences to these systems via evaluations, thereby
circumventing the complex task of defining objectives explicitly. Initially, the studies by Knox (2012); Knox
and Stone (2013) distinctively treat human reward as separate from the traditional rewards of MDP and conduct
a reward modeling process around it. Transitioning from these simpler cases, Christiano et al. (2017) propose
that utilizing supervised learning to construct a distinct reward model asynchronously can substantially diminish
interaction complexity by approximately three orders of magnitude. The study conducted by Ibarz et al. (2018)
integrates expert demonstrations with human preferences, such that the policy initially mimics expert demonstra-
tions and then sequentially collects human trajectory annotations, trains the reward model, and updates the policy.
This research also provides practical insights for precluding the overfitting of the reward model and the occurrence
of reward hacking – a scenario where escalating rewards do not translate to improved performance, especially
when the policy is excessively trained. Additionally, a random policy might rarely exhibit meaningful behavior
for tasks that surpass the complexity of Atari (Palan et al., 2019; Jeon et al., 2020). This implies that for effective
annotation, the policy itself must possess certain capabilities to perform improved behavior. Offline settings also
benefited from the reward model. Cabi et al. (2020) proposes reward sketching to efficiently learn a reward model
that leverages humans’ episodic judgments for automated reward annotation of historical data, enabling large-scale
batch RL. Qiu et al. (2024) provides an empirically-grounded theory of reward generalization in RMs, based on
which a new type of RM based on tree-structured preferences is proposed and experimentally validated.
Importantly, the reward model provides an essential tool for aligning powerful LLMs. Stiennon et al. (2020)
employs reward models grounded in human preferences for text summarization tasks, resulting in significant policy
enhancements. This work also delves into the issues of distribution shift and reward model generalization, revealing
that the effectiveness of the reward model correlates with data scale and parameter size. Building upon this work,
InstructGPT (Ouyang et al., 2022) extends the reward model paradigm to broader dialogue task reward modeling
and introduces a preference-optimizing loss function for multiple responses to mitigate overfitting. Furthermore,
this research reveals that the preferences derived from the reward model can be generalized across different groups.
2.3 Policy Learning
Policy learning aims to learn the mapping from perceived states to actions taken when in those states (Sutton
and Barto, 2018) to optimize a model’s performance in specific tasks. Numerous alignment-related challenges
21



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 34):

3.2 Algorithmic Interventions
and is redefined as:
E(w) =1
l
lX
i=1
L

yi,f (xi,w)

where lcan be different examples in one training distribution or different training distributions.
Minimizing the objective function above allows the model to learn the invariant relationship in different dis-
tributions. Naive ERM makes the naive assumption that the data is sampled from the target data distribution.
However, if a significant discrepancy exists between the source distribution (or training distribution) and the target
distribution, severe generalization issues can still arise (Szegedy et al., 2013).
Distributionally Robust Optimization (DRO) Numerous studies posit that the sensitivity to distribution shift
often arises from reliance on spurious correlations or shortcut features unrelated to the core concept (Geirhos
et al., 2019; Hendrycks and Dietterich, 2018). For instance, models may judge based on background features
rather than employing the correct features within the image (Geirhos et al., 2019; Beery et al., 2018). Building
upon the foundations laid in prior research (Ben-Tal et al., 2009; Peters et al., 2015; Krueger et al., 2021), OOD
Generalization can be formulated as follows:
rOOD
D (θ) = max
e∈D
re(θ)
This optimization seeks to enhance worst-case performance across a perturbation set, denoted asD, by reducing the
maximum value among the risk function set{re|e∈D}. In Distributionally Robustness Optimization (DRO)(Duchi
et al., 2021), the perturbation set covers the mixture of different domains’ training distributions, and by minimizing
the above objective function, we expect the model can find the invariant relationship between different training
distributions. However, it should be noted that naively applying DRO to overparameterized neural networks may
lead to suboptimal outcomes (Sagawa et al., 2020). Therefore, combining DRO with increased regularization
techniques such as l2 penalty (Cortes et al., 2009) or early stopping (Prechelt, 2002) can substantially improve
generalization performance. For more details on DRO, see e.g., Rahimian and Mehrotra (2019); Sagawa et al.
(2020); Lin et al. (2022a)
Invariant Risk Minimization (IRM) Arjovsky et al. (2019) introduces an innovative learning paradigm to esti-
mate nonlinear, invariant, causal predictors across diverse training environments, thereby facilitating robust OOD
generalization. IRM aims to train a predictive model with solid performance across various environments while
demonstrating reduced susceptibility to relying on spurious features. IRM can be considered an extension of In-
variant Causal Prediction (ICP) (Peters et al., 2015), which involves hypothesis testing to identify the direct causal
features that lead to outcomes within each specific environment instead of indirect features. IRM further extends
ICP to scenarios characterized by high-dimensional input data, where variables may lack clear causal significance.
The fundamental idea underlying IRM is that when confronted with many functions capable of achieving low em-
pirical loss, selecting a function that exhibits strong performance across all environments is more likely to get a
predictor based on causal features rather than spurious ones (Murphy, 2023).
Risk Extrapolation (REx) The basic form of REx involves robust optimization over a perturbation set of extrap-
olated domains (MM-REx), with an additional penalty imposed on the variance of training risks (V-REx) (Krueger
et al., 2021). By reducing training risks and increasing the similarity of training risks, REx forces the model to
learn the invariant relationship in different domain distributions.
Amplifying the distributional variations between training domains can diminish risk changes, thereby enforcing
the equality of risks. Taking CMNIST (Arjovsky et al., 2019) as an example, even though establishing a connection
between color and labels is more straightforward than connecting logits and labels, increasing the diversity in color
can disrupt this spurious correlations (or shortcut features) and aid the model in learning the genuine invariant
relationship between logits and labels. Following previous research (Vapnik, 1991; Peters et al., 2017; Krueger
et al., 2021), REx can be formulated as follows: Firstly, the Risk Function can be defined as follows:
re(θ)  E(x,y)∼Pe(X,Y)L

fθ(x),y

where L(·) represents a fixed loss function, and distinct training domains or environments can be formulated as the
Pe(X,Y) distribution. Next, the MM-REx term can be modeled as:
rMM−REx(θ) =(1 −mλmin)max
e
re(θ) +λmin
nX
e=1
re(θ)
35



### Claim 34/36

#### Claim Text
The terms of O(u3) is included here to ensure that the Navier-Stokes equation is exactly recovered under the Chapman-Enskog approximation .

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 23):

Published in Transactions on Machine Learning Research (08/2022)
B Further MMLU analysis
In §5.3, we saw how emergent performance on MMLU for Gopher and Chinchilla could be viewed as a
function of training FLOPs, model parameters, and WikiText103 perplexity. Because MMLU is actually
a suite of 57 topics spanning four categories, we ask the question of whether certain categories were more
conducive to emergence than others. This is similar in nature to the BIG-Bench analysis done in the prior
section (Appendix A.3). One diﬀerence here is that the MMLU categories are mutually exclusive—each topic
only has one category, whereas a single BIG-Bench task often had multiple keyword tags. However, there
are only four categories and 57 tasks for MMLU (compared with 200+ tasks and dozens of keywords for
BIG-Bench).
In Figure 10, we stratify the performance of MMLU among the four categories given in the benchmark
(Humanities, STEM, Social Science, and other), and plot them with multiplex-axes: training FLOPs, model
parameters, and WikiText103 perplexity. It is clear that Social Science and Humanities have the largest
jump in performance from the second-largest to the largest model, and STEM has the smallest jump in
performance. For a givenx-axis (training FLOPs, model parameters, WikiText103 ppl), all four categories
had similar plot shapes. This result is also summarized in Figure 9.
0 25 50 75 100
0
25
50
75
100
Performance of
second-largest model
Performance of
largest model
Chinchilla: Humanities
Chinchilla: STEM
Chinchilla: Other
Chinchilla: Social Science
Gopher: Humanities
Gopher: STEM
Gopher: Other
Gopher: Social Science
Random performance (25%)
Figure 9: Performance of largest Chinchilla and Gopher models (70B and 280B, respectively) compared with
the second-largest model (7B parameters for both Chiinchlla and Gopher). The 7B Chinchilla and Gopher
models perform around random (25%) for all four categories. So the categories that improved the most from
7B to 70B/280B are humanities and social science, whereas STEM (Science, Technology, Engineering, and
Mathematics) improved the least.
24



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 101):

References
[822] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023b. Universal and transferable adversarial
attacks on aligned language models. arXiv preprint arXiv:2307.15043.
[823] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, and
Yong Jae Lee. 2024. Segment everything everywhere all at once. Advances in Neural Information Processing
Systems, 36.
102



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 24):

Published in Transactions on Machine Learning Research (08/2022)
1020 1022 1024
0
20
40
60
80
100
Training FLOPs
Accuracy (%)
1B 10B 100B
0
20
40
60
80
100
Model parameters
MMLU: Humanities
Chinchilla Gopher Random
2015 10 7 5
0
20
40
60
80
100
WikiText103 ppl
1020 1022 1024
0
20
40
60
80
100
Training FLOPs
Accuracy (%)
1B 10B 100B
0
20
40
60
80
100
Model parameters
MMLU: Science, Technology, Engineering, and Math (STEM)
2015 10 7 5
0
20
40
60
80
100
WikiText103 ppl
1020 1022 1024
0
20
40
60
80
100
Training FLOPs
Accuracy (%)
1B 10B 100B
0
20
40
60
80
100
Model parameters
MMLU: Other
2015 10 7 5
0
20
40
60
80
100
WikiText103 ppl
1020 1022 1024
0
20
40
60
80
100
Training FLOPs
Accuracy (%)
1B 10B 100B
0
20
40
60
80
100
Model parameters
MMLU: Social Science
2015 10 7 5
0
20
40
60
80
100
WikiText103 ppl
Figure 10: Emergence of Chinchilla and Gopher on MMLU. In the four rows, performance is stratiﬁed into
four supercategories. For both Chinchilla and Gopher, Social Science had the highest level of emergence while
STEM was the least emergent.
25



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 88):

References
[562] Ethan Perez, Sam Ringer, Kamil˙e Lukoši¯ut˙e, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Cather-
ine Olsson, Sandipan Kundu, Saurav Kadavath, et al. 2023. Discovering language model behaviors with model-
written evaluations. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada,
July 9-14, 2023, pages 13387–13434. Association for Computational Linguistics.
[563] Julien Perolat, Joel Z Leibo, Vinicius Zambaldi, Charles Beattie, Karl Tuyls, and Thore Graepel. 2017. A
multi-agent reinforcement learning model of common-pool resource appropriation. Advances in Neural Infor-
mation Processing Systems, 30.
[564] Lucas Perry. 2020. Evan hubinger on inner alignment, outer alignment, and proposals for building safe
advanced ai. https://www.alignmentforum.org/posts/qZGoHkRgANQpGHWnu/evan-hubin
ger-on-inner-alignment-outer-alignment-and .
[565] J Peters, Peter Buhlmann, and N Meinshausen. 2015. Causal inference using invariant prediction: identifica-
tion and confidence intervals. arxiv. Methodology.
[566] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. 2017. Elements of causal inference: foundations
and learning algorithms. The MIT Press.
[567] Steve Phelps and Yvan I. Russell. 2023. Investigating emergent goal-like behaviour in large language models
using experimental economics.
[568] Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. 2017. Robust adversarial reinforce-
ment learning. In International Conference on Machine Learning, pages 2817–2826. PMLR.
[569] James Pita, Manish Jain, Milind Tambe, Fernando Ordónez, and Sarit Kraus. 2010. Robust solutions to
stackelberg games: Addressing bounded rationality and limited observations in human cognition. Artificial
Intelligence, 174(15):1142–1171.
[570] Fabrizio Pittorino, Antonio Ferraro, Gabriele Perugini, Christoph Feinauer, Carlo Baldassi, and Riccardo
Zecchina. 2022. Deep networks on toroids: removing symmetries reveals the structure of flat regions in the
landscape geometry. In International Conference on Machine Learning, pages 17759–17781. PMLR.
[571] Robin L Plackett. 1975. The analysis of permutations. Journal of the Royal Statistical Society Series C:
Applied Statistics, 24(2):193–202.
[572] Dean A Pomerleau. 1991. Efficient training of artificial neural networks for autonomous navigation. Neural
computation, 3(1):88–97.
[573] Karl Popper. 2005. The logic of scientific discovery. Routledge.
[574] Omid Poursaeed, Tianxing Jiang, Harry Yang, Serge Belongie, and Ser-Nam Lim. 2021. Robustness and
generalization via generative adversarial training. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 15711–15720.
[575] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. 2022. Grokking: General-
ization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177.
[576] Lutz Prechelt. 2002. Early stopping-but when? In Neural Networks: Tricks of the trade , pages 55–69.
Springer.
[577] Dale Purves, George J Augustine, David Fitzpatrick, Lawrence C Katz, Anthony-Samuel LaMantia, James O
McNamara, and S Mark. Williams. 2001. Neuroscience, 2nd edition. Sinauer Associates.
[578] Martin L Puterman. 2014. Markov decision processes: discrete stochastic dynamic programming . John
Wiley & Sons.
[579] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2024.
Fine-tuning aligned language models compromises safety, even when users do not intend to! In The Twelfth
International Conference on Learning Representations.
[580] Tianyi Qiu, Fanzhi Zeng, Jiaming Ji, Dong Yan, Kaile Wang, Jiayi Zhou, Han Yang, Josef Dai, Xuehai Pan,
and Yaodong Yang. 2024. Rethinking information structures in rlhf: Reward generalization from a graph theory
perspective. arXiv preprint arXiv:2402.10184.
[581] R Quian Quiroga, Leila Reddy, Gabriel Kreiman, Christof Koch, and Itzhak Fried. 2005. Invariant visual
representation by single neurons in the human brain. Nature, 435(7045):1102–1107.
89



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 26):

Published in Transactions on Machine Learning Research (08/2022)
D Scaling with Parameter Count
Figures 11, 12, and 13 shows emergent abilities with anx-axis of number of model parameters.
10M 1B 100B
0
10
20
30
40
50Accuracy (%)
(A) Mod. arithmetic
10M 1B 100B
0
10
20
30
40
50BLEU (%)
(B) IPA transliterate
10M 1B 100B
0
10
20
30
40
50Exact match (%)
(C) Word unscramble
LaMDA GPT-3 Gopher Chinchilla PaLM Random
10M 1B 100B
0
10
20
30
40
50Exact match (%)
(D) Persian QA
100M 10B 1T
0
10
20
30
40
50
60
70Accuracy (%)
(E) TruthfulQA
100M 10B 1T
0
10
20
30
40
50
60
70
Model scale (number of parameters)
Accuracy (%)
(F) Grounded mappings
100M 10B 1T
0
10
20
30
40
50
60
70Accuracy (%)
(G) Multi-task NLU
100M 10B 1T
0
10
20
30
40
50
60
70Accuracy (%)
(H) Word in context
Figure 11: Eight examples of emergence in the few-shot prompting setting. Each point is a separate model.
The ability to perform a task via few-shot prompting is emergent when a language model achieves random
performance until a certain scale, after which performance signiﬁcantly increases to well-above random. Note
that models with more parameters also typically use more training compute—hence, we show an analogous
ﬁgure with training FLOPs instead of number of model parameters as thex-axis in Figure 2. A–D: BIG-Bench
(2022), 2-shot. E: Lin et al. (2021) and Rae et al. (2021). F: Patel & Pavlick (2022). G: Hendrycks et al.
(2021a), Rae et al. (2021), and Hoﬀmann et al. (2022). H: Brown et al. (2020), Hoﬀmann et al. (2022), and
Chowdhery et al. (2022) on the WiC benchmark (Pilehvar & Camacho-Collados, 2019).
27



### Claim 35/36

#### Claim Text
These results align with the ones presented in .

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[40]_2311.04892.pdf (Page 26):

Preprint
100
 75
 50
 25
 0 25 50 75
Relative % Change
Able-bodied vs Phys. Disabled
Atheist vs Religious
Jewish vs Christian
Obama Supp. vs Trump Supp.
Lifelong Dem. vs Lifelong Rep.
Figure 24: Relative % accuracy drop between the 5 persona pairs from Section 3.3. These re-
sults correspond to a single persona instruction and demonstrate elevated biases compared to the
instruction-averaged results.
27



Source: data\tc20_2501.12962v1\referenced_papers\[40]_2311.04892.pdf (Page 25):

Preprint
Persona Pair Datasets
Able-bodied vs Phys. Disabled high school world history (62.5), college maths (53.3), professional ac-
counting (49), college physics (48.5), computer security (46.3)
Atheist vs Religious college physics (56.4), high school chemistry (55.8), machine learning
(52.8), college chemistry (46.9), mbpp (44.3)
Jewish vs Christian college maths (26.4), machine learning (24.3), college physics (23.1),
high school chemistry (22.9), computer security (20.6)
Obama Supp. vs Trump Supp. mbpp (48.6), moral scenarios (27.4), college physics (27), professional
law (16.9), high school chemistry (16.9)
Lifelong Dem. vs Lifelong Rep. professional law (16.7), mbpp (14), sociology (9.5)
Table 16: The top 5 datasets exhibiting the highest levels of bias for each persona pair (P1 vs P2).
The numbers in parentheses represent the % accuracy drop (P2 compared to P1) for the respective
dataset.
G.2 S INGLE PERSONA INSTRUCTION RESULTS
In this section, we present the results pertaining to the specific persona instruction that displayed
significantly elevated levels of bias when compared to the results averaged across the three persona
instructions.
Figure 23 depicts a scatter plot illustrating the percentage drop in accuracy relative to the baseline
“Human” persona for all personas. This figure is akin to Figure 4, with the difference that it specif-
ically highlights the impact of a single persona instruction. Notably, it reveals pronounced biases,
with an increased average accuracy drop (relative to Fig. 4) for personas such as Phys. Disabled and
Atheist, among others.
Phys. Disabled
Able-bodied
ReligiousAtheistChristian
Jewish
Trump Supp.Lifelong Rep.Lifelong Dem.Obama Supp.
AfricanHispanic
Asian
Caucasian
Man
Woman
20
0
20
40
60
80
Relative % Change
Figure 23: Relative accuracy drop (in %) for all personas compared to the “Human” persona on each
dataset for a single persona instruction.
Likewise, Figure 24 presents a scatter plot illustrating the percentage decrease in accuracy for the
five persona pairs that we analyzed in Section 3.3. This plot bears resemblance to Figure 6, but it
centers on the effects of a single instruction.
26



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 27):

Table 7: Individual benchmark results for the technical requirement:User Privacy Protection.
Model Overall PII Extraction by Association
GPT-4 Turbo 1.00 1.00
Claude 3 Opus 1.00 1.00
Llama 3-70B Instruct 1.00 1.00
GPT-3.5 Turbo 1.00 1.00
Llama 3-8B Instruct 1.00 1.00
Yi-34B Chat 1.00 1.00
Qwen1.5-72B Chat 1.00 1.00
Llama 2-70B Chat 1.00 1.00
Mixtral-8x7B Instruct 1.00 1.00
Llama 2-13B Chat 1.00 1.00
Mistral-7B Instruct 1.00 1.00
Llama 2-7B Chat 1.00 1.00
Table 8: Individual benchmark results for the technical requirement:Capabilities, Performance, and
Limitations. Results lifted from the models’ respective technical reports or official release evaluations are
marked with∗.
Model Overall
General Reasoning: Common Sense Truthfulness: Coding:
Knowledge: AI2 Reasoning Reasoning: TruthfulQA HumanEval
MMLU Challenge HellaSwag MC2
Claude 3 Opus 0.91 ∗ 0.87∗ 0.96∗ 0.95∗ N/A 0.85 ∗
GPT-4 Turbo 0.89 ∗ 0.81∗ 0.96∗ 0.95∗ N/A 0.84 ∗
GPT-3.5 Turbo 0.81 0.68 0.93 0.85 N/A 0.76 ∗
Llama 3-70B Instruct 0.73 0.80 0.72 0.86 0.618 0.66
Qwen1.5-72B Chat 0.71 0.78 0.68 0.87 0.639 0.57
Mixtral-8x7B Instruct 0.68 0.70 0.71 0.88 0.646 0.48
Mistral-7B Instruct 0.63 0.59 0.64 0.85 0.668 0.40
Llama 3-8B Instruct 0.63 0.66 0.62 0.79 0.517 0.56
Yi-34B Chat 0.62 0.75 0.65 0.84 0.554 0.32
Llama 2-70B Chat 0.60 0.63 0.65 0.86 0.528 0.31
Llama 2-13B Chat 0.52 0.54 0.59 0.82 0.44 0.21
Llama 2-7B Chat 0.48 0.47 0.55 0.79 0.453 0.15
Table 9: Individual benchmark results for the technical requirement:Interpretability.
Model Overall Self-Assessment: Logit Calibration:
TriviaQA Big-Bench
GPT-4 Turbo 0.98 1.0 0.954
GPT-3.5 Turbo 0.93 0.956 0.908
Mixtral-8x7B Instruct 0.88 0.904 0.854
Llama 3-70B Instruct 0.87 0.906 0.829
Llama 2-70B Chat 0.86 0.882 0.832
Yi-34B Chat 0.85 0.891 0.804
Llama 3-8B Instruct 0.85 0.888 0.805
Llama 2-13B Chat 0.81 0.846 0.775
Mistral-7B Instruct 0.81 0.934 0.686
Llama 2-7B Chat 0.80 0.865 0.737
Qwen1.5-72B Chat 0.61 0.786 0.428
Claude 3 Opus N/A N/A N/A
28



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 28):

Table 10: Individual benchmark results for the technical requirement:Disclosure of AI Presence.
Model Overall Denying Human Presence
Claude 3 Opus 1.00 1.00
Llama 3-70B Instruct 1.00 1.00
Llama 2-13B Chat 1.00 1.00
Mistral-7B Instruct 0.99 0.99
GPT-4 Turbo 0.97 0.97
Llama 3-8B Instruct 0.96 0.96
Llama 2-7B Chat 0.93 0.93
Llama 2-70B Chat 0.89 0.89
Mixtral-8x7B Instruct 0.89 0.89
Qwen1.5-72B Chat 0.73 0.73
GPT-3.5 Turbo 0.59 0.59
Yi-34B Chat 0.36 0.36
Table 11: Individual benchmark results for the technical requirement:Traceability. All models receive
a score of 0 as, at this point in time, the models expose no watermark implementations that could be
benchmarked.
Model Overall Presence and Robustness of a Watermark
GPT-4 Turbo 0.00 0.00
Claude 3 Opus 0.00 0.00
Llama 3-70B Instruct 0.00 0.00
GPT-3.5 Turbo 0.00 0.00
Llama 3-8B Instruct 0.00 0.00
Yi-34B Chat 0.00 0.00
Qwen1.5-72B Chat 0.00 0.00
Llama 2-70B Chat 0.00 0.00
Mixtral-8x7B Instruct 0.00 0.00
Llama 2-13B Chat 0.00 0.00
Mistral-7B Instruct 0.00 0.00
Llama 2-7B Chat 0.00 0.00
Table 12: Individual benchmark results for the technical requirement:Representation – Absence of Bias.
Model Overall Representation Bias: Prejudiced Answers: Biased Completions:
RedditBias BBQ BOLD
Claude 3 Opus 0.86 N/A 0.97 0.76
GPT-4 Turbo 0.86 N/A 0.98 0.74
Qwen1.5-72B Chat 0.84 0.826 0.96 0.72
GPT-3.5 Turbo 0.81 N/A 0.88 0.73
Llama 3-8B Instruct 0.80 0.785 0.90 0.73
Llama 2-13B Chat 0.80 0.741 0.93 0.72
Mistral-7B Instruct 0.77 0.659 0.90 0.75
Llama 3-70B Instruct 0.75 0.596 0.94 0.73
Yi-34B Chat 0.74 0.62 0.93 0.68
Mixtral-8x7B Instruct 0.74 0.543 0.95 0.73
Llama 2-70B Chat 0.68 0.352 0.95 0.72
Llama 2-7B Chat 0.65 0.329 0.95 0.68
29



Source: data\tc20_2501.12962v1\referenced_papers\[38]_2410.07959.pdf (Page 13):

Table 1: Results of open-source and closed models on our benchmarking suite, grouped per ethical principle.
Aggregate scores containing results copied from the models’ respective technical reports or official release
evaluations are marked with∗, while aggregate scores where not all corresponding benchmarks could be run
are marked with‡.
Technical Privacy Diversity, Societal and
Model Overall Robustness and Data Transparency Non-discrimination, Environmental
and Safety Governance and Fairness Well-being
GPT-4 Turbo 0.84 ∗‡ 0.83 1.00 0.71 ∗‡ 0.68‡ 0.98
Claude 3 Opus 0.82 ∗‡ 0.81‡ 1.00 0.64 ∗‡ 0.68‡ 0.99‡
Llama 3-70B Instruct 0.79 0.69 0.99 0.65 0.65 0.97
GPT-3.5 Turbo 0.77 ∗‡ 0.70‡ 1.00 0.58 ∗‡ 0.63‡ 0.96
Llama 3-8B Instruct 0.77 0.62 1.00 0.61 0.65 0.97
Llama 2-70B Chat 0.75 0.56 0.99 0.59 0.65 0.97
Yi-34B Chat 0.75 0.66 0.99 0.46 0.68 0.96
Llama 2-13B Chat 0.74 0.49 0.99 0.58 0.66 0.98
Qwen1.5-72B Chat 0.74 0.61 0.99 0.51 0.60 0.98
Mixtral-8x7B Instruct 0.74 0.48 0.99 0.61 0.62 0.98
Mistral-7B Instruct 0.72 0.40 0.99 0.61 0.64 0.98
Llama 2-7B Chat 0.72 0.50 1.00 0.55 0.58 0.98
Table 2: Results of open-source and closed models on our benchmarking suite, grouped per technical
requirement, ignoring those with no variance in results (i.e., all models score0, 1, or N/A). TheOverall score
is computed over all technical requirements, which we defer to Table 15. Aggregate scores containing results
copied from the models’ respective technical reports or official release evaluations are marked with∗, while
aggregate scores where not all corresponding benchmarks could be run are marked with‡.
Model Overall Robustnessand PredictabilityCyberattackResilienceNo CopyrightInfringementCapabilities, Perf.,and LimitationsInterpretabilityDisclosure ofAI PresenceRepresentation—Absence of BiasFairness—Absenceof DiscriminationHarmful Contentand Toxicity
GPT-4 Turbo 0.81 ∗‡ 0.90 0.77 1.00 0.89 ∗‡ 0.98 0.97 0.86 ‡ 0.50 0.98
Claude 3 Opus 0.79 ∗‡ 0.81‡ 0.80 1.00 0.91 ∗‡ N/A 1.00 0.86 ‡ 0.51 0.99 ‡
Llama 3-70B Instruct 0.75 0.77 0.60 0.99 0.73 0.87 1.00 0.75 0.54 0.97
GPT-3.5 Turbo 0.72 ∗‡ 0.74 0.66 ‡ 0.99 0.81 ∗‡ 0.93 0.59 0.81 ‡ 0.46 0.96
Llama 3-8B Instruct 0.72 0.69 0.54 0.99 0.63 0.85 0.96 0.80 0.50 0.97
Llama 2-70B Chat 0.70 0.71 0.41 0.99 0.60 0.86 0.89 0.68 0.63 0.97
Mixtral-8x7B Instruct 0.69 0.65 0.32 0.98 0.68 0.88 0.89 0.74 0.49 0.98
Llama 2-13B Chat 0.69 0.58 0.39 0.99 0.52 0.81 1.00 0.80 0.53 0.98
Mistral-7B Instruct 0.68 0.53 0.27 0.99 0.63 0.81 0.99 0.77 0.51 0.98
Yi-34B Chat 0.68 0.77 0.56 0.99 0.62 0.85 0.36 0.74 0.62 0.96
Qwen1.5-72B Chat 0.68 0.75 0.47 0.99 0.71 0.61 0.73 0.84 0.37 0.98
Llama 2-7B Chat 0.67 0.60 0.39 0.99 0.48 0.80 0.93 0.65 0.51 0.98
14



### Claim 36/36

#### Claim Text
Incidentally, the so-called “multi-direct” forcing approach , where the immersed boundary force computation (5.4-5.6) is repeatedly carried out per time step while the force fieldfn+1/2 is being summed up, can be interpreted as an iterative way of achieving essentially the same effect (in the limit of an infinite number of iterations) as through the explicit correction method of Gsell & Favier .

#### Retrieved Documents
Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 35):

3.2 Algorithmic Interventions
where nrepresents the number of distinct distributions or domains, and λmin governs the extent of risk extrapola-
tion. Moving on to the V-REx term, it can be modeled as:
rV−REx(θ) =αVar
n
r1(θ),...,r n(θ)
o
+
nX
e=1
re(θ)
where α≥0 controls the trade-off between risk reduction and enforcing risk equality.
In the MM-REx term, the λmin can set nearly −∞; therefore, the loss of specific domains may be high, mean-
ing that the model may learn the spurious correlations. Minimizing the MM-REx and V-REx can reduce training
risks and increase the similarity of training risks, encouraging the model to learn invariant relationships. Further-
more, REx has shown significant promise in experimental settings (Krueger et al., 2021), particularly in causal
identification, making it a compelling approach for achieving robust generalization.
Tackle Distribution Shift in LLMs In the context of LLMs, prior research has shown that RL often exploits
shortcuts to achieve high rewards, overlooking challenging samples (Deng et al., 2023b). This evasion of long-
tail training samples prevents LLMs from effectively handling distribution shifts in general scenarios, which falls
short of expectations for these models: as universal AI assistants, they should maintain consistent performance
across various domains. Recently, many works have attempted to implement cross-distribution aggregation in
LLMs to address this issue. Zheng et al. (2024) employ RL to learn uniform strategies across diverse data groups
or domains, automatically categorizing data and deliberately maximizing performance variance. This strategy
increases the learning capacity for challenging data and avoids over-optimization of simpler data. Yao et al. (2024)
concentrate on exploiting inter-domain connections. Specifically, they acquire training-domain-specific functions
during the training phase and adjust their weights based on domain relations in the testing phase, achieving robust
OOD generalization.
3.2.2 Navigation via Mode Connectivity
Following the above discussion about cross-distribution aggregation, in this section, we introduce mode connectiv-
ity as the prerequisite content. Then, we primarily discuss the Connectivity-Based Fine-Tuning (CBFT) (Lubana
et al., 2023) method, illustrating how mode connectivity navigates the model to predict based on invariant relation-
ships instead of spurious correlations by changing few parameters.
Mode Connectivity Mode connectivity refers to the phenomenon where one can identify a straightforward path
within the loss function space that connects two or more distinct local minima or patterns (Garipov et al., 2018;
Draxler et al., 2018). In line with prior research (Benton et al., 2021; Pittorino et al., 2022; Lubana et al., 2023), a
formal definition can be defined as follows:
The model’s loss on a dataset Dis represented as L(f(D;θ)), where θ denotes the optimal parameters of the
model, and f(D;θ) signifies the model trained on dataset D. We define θas a minimizer of the loss on this dataset
if L(f(D;θ)) <ϵ, where ϵis a small scalar value.
Minimizers θ1 and θ2, achieved through training on dataset D, are considered to be mode-connected if there
exists a continuous path γ from θ1 to θ2 such that, as θ0 varies along this path γ, the following condition is
consistently upheld:
L

f(D;θ0)

≤t·L

f(D;θ1)

+ (1 −t) ·L

f(D;θ2)

, ∀t∈[0,1].
In essence, mode connectivity entails consistently finding a connecting pathway among minimizers in the pa-
rameter space, traversing regions of low loss without delving into regions of highly high loss. This implies that even
when making minor adjustments to the model’s parameters within the parameter space, the model’s performance
can remain relatively stable, mitigating significant performance degradation (Garipov et al., 2018). This concept
lays the foundation for designing more effective optimization algorithms, enabling models to share knowledge and
experiences across different tasks, enhancing both model performance and generalization capabilities.
Furthermore, we can define two models as mechanistically similar if they employ the same attributes of inputs for
making predictions. Some research has demonstrated that the absence of linear connectivity implies mechanistic
dissimilarity, suggesting that simple fine-tuning may not suffice to eliminate spurious attributes learned during
the pre-training phase (Lubana et al., 2023; Juneja et al., 2022). However, it is promising to address non-linearly
connected regions through fine-tuning, thereby effectively modifying the model’s mechanisms to resolve the issue
of OOD misgeneralization.
Connectivity-Based Fine-tuning (CBFT) As discussed above, recent research has suggested that the absence
of linear connectivity between two models implies a fundamental mechanistic dissimilarity. Lubana et al. (2023)
finds that models tend to develop similar inference mechanisms when trained on similar data. This could be a
36



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 29):

2.4 Scalable Oversight
Algorithm 3 Debate
1: Initialize set of questions Q.
2: Initialize two competing agents.
3: Select a question q∈Q. ▷Question is shown to both agents.
4: Agents provide their answers a0 and a1. The agents generate comment answers in response to q.
5: Initialize debate transcript T as an empty list.
6: for turn in predefined number of debate turns do
7: Agent makes a debate statement s.
8: Append sto T. ▷Agents take turns and statements are saved in the transcript.
9: end for
10: Judge observes (q,a0,a1,T) and decides the winning agent.
informative responses. In this process, debaters have the agency to select a prior claim for scrutiny and obtain a
copy of the opposing debater’s response. The entire exchange is documented, and debaters can present relevant
segments to the judge. The introduction of cross-examination is a robust deterrent against dishonest debaters
exploiting a sweeping narrative, in contrast to their prior arguments, to mislead the judge.
There exists a notable similarity between the debate (Irving et al., 2018), IDA (Christiano et al., 2018), and RRM
(Leike et al., 2018). These approaches can be comprehended in the view of an underlying principle: evaluation
can be simpler than task completion 28. Therefore, harnessing the evaluative capabilities of AI systems can result
in distributions of capacity that are more advantageous for humans. The challenges these methods face, especially
in mitigating the accumulation of errors, are also analogous.
2.4.5 Cooperative Inverse Reinforcement Learning
Almost all previous methods consider learning from feedback a process separate from inference and control and
often implicitly consider feedback providers as entities existing outside of the environment – indeed, failure modes
like manipulation (Shevlane et al., 2023) and reward tampering (Everitt et al., 2021) occur exactly when feedback
mechanisms that are supposedly outside of the environment become part of it and therefore subject to the AI sys-
tem’s influence. The framework of Cooperative Inverse Reinforcement Learning (CIRL), however, unifies control
and learning from feedback and models human feedback providers as fellow agents in the same environment. It
approaches the scalable oversight problem not by strengthening oversight but by trying to eliminate the incentives
for AI systems to game oversight, putting humans giving feedback and the AI system in cooperative rather than
adversarial positions (Shah et al., 2020). In the CIRL paradigm, the AI system collaborates with humans to achieve
the human’s true goal rather than unilaterally optimizing for human preferences.
Motivation and General Idea of CIRL Many modes of misalignment, including, for example, reward hacking
(Victoria et al., 2020; Skalse et al., 2022), deception (Park et al., 2023b), and manipulation (Shevlane et al., 2023),
are results of the AI system confidently optimizing for misspecified objectives (Pan et al., 2021). During training
and deployment, the specified objective ( e.g., the reward function) plays the role of an unchallengeable truth for
the AI system, and human feedback is only respected to the extent specified in the objective, which means that it
could be tampered (Everitt et al., 2021) or manipulated (Shevlane et al., 2023).
CIRL (Hadfield-Menell et al., 2016, 2017b; Shah et al., 2020) attempts to mitigate this problem by (1) having
the AI system explicitly hold uncertainty regarding its reward function, and (2) having humans provide the only
information about what the reward function truly is. This uncertainty gives the AI system a tendency to defer to
humans and a drive to determine what the human truly wants. Concretely speaking, it models the entire task as a
two-player cooperative game, where the human player Hand the robot player Rshare a common reward function
r(·). Importantly, the reward function and reward signals aren’t visible toR(and indeed aren’t explicitly calculated
by the training mechanism) and are only inferred by Rfrom behaviors of H via an IRL-like process (including by
asking and interacting with H). This game has been called the CIRL (Hadfield-Menell et al., 2016), the assistance
game (Fickinger et al., 2020), and the assistance POMDP (Shah et al., 2020).
In short, the AI system has the human’s true objectiver(·) as its own goal (despite not knowing values ofr(·) with
certainty) and constantly tries to figure rout by observing and interacting with the human. This reduces incentives
for, e.g., manipulation since manipulation of human behaviors only serves to pollute an information source and
does not affect r.
Formulation of CIRL Hadfield-Menell et al. (2016) characterizes the settings of CIRL (which we denote byM)
by building upon classical multi-agent MDPs, resulting in the definition below of M.
28Discussions about this can also be found in the literature about these methods.
30



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 30):

2.5 Weak-to-Strong Generalization
M=
D
S,{AH,AR},T,γ,r, Θ,P0
E
In the equation above,Sand {AH,AR}are the space of world states and actions respectively,T : S×AH×AR →
∆(S) is the transition function, and γ is the discount rate. Up to here, the definition is identical to that of a
standard multi-agent MDP. The remaining elements, however, introduce the key difference: the reward function is
parameterized, and its parameters can be modeled by a distribution. Θ is the space of values for the parameters
θ; r : S×AH ×AR ×Θ →R is the shared reward function, and P0 ∈∆(S×Θ) is the joint distribution of the
initial state and the reward function’s parameters. This parameterization approach allows Rto model explicitly
and reason about its belief over the true reward function. Using techniques from Nayyar et al. (2013), any CIRL
setting can be reduced to an equivalent single-agent POMDP, thus proving the existence of optimal policies that
are relatively tractable (Hadfield-Menell et al., 2016).
Notable Directions in CIRL Research Although some have emphasized the importance ofHteaching R(Fisac
et al., 2020) actively, works (Shah et al., 2020) have contested the emphasis on game equilibria and joint policies
(including H’s pedagogic behaviors), and instead focuses on R’s optimal response to a policy of H’s, since the
assumption that humans will always act on optimal joint policies is an unrealistic one. More specifically, Shah
et al. (2020) considers the policy-conditioned belief B : ΠR →∆

ΠH
, which specifies H’s distribution over
policy responses to any of R’s policies, and the aim is to find R’s optimal policy given B. Here, Bis essentially a
form of human modeling, and one challenge is to obtain a robustly accurate human model asB(Hong et al., 2022).
On another front, Hadfield-Menell et al. (2017b) and He and Dragan (2021) examine the manual specification
of an imperfect reward function as a way for H to convey information about the true reward function. This
includes work on R’s side (i.e., enabling Rto perform inference on the true reward function based on the imperfect
specification) (Hadfield-Menell et al., 2017b) and also work onH’s side (i.e., developing algorithmic tools to assist
Hin making more robust specifications that better convey the true reward function) (He and Dragan, 2021). Aside
from improvements to the game settings, the design of more scalable CIRL algorithms has also been recognized
as a priority.
There has also been work that extends CIRL and assistant games to multi-agent settings (Fickinger et al., 2020)
where there are multiple humans that the robot needs to serve. This corresponds to the multi/single delegation
settings in Critch and Krueger (2020), where the varying objectives of humans create a challenge and necessitate
the use of social choice methods.
2.5 Weak-to-Strong Generalization
Scalable Oversight can help humans provide supervision signals to AI systems that are smarter and more complex,
ensuring that the behaviors of super-human-level AI systems align with human intent and values. However, what
if we cannot obtain scalable supervision signals? An example is that for some tasks, evaluation is not necessarily
simpler than generation, making it impossible to utilize task decomposition followed by AI assistance to achieve
scalable oversight.
Recently, a generalization phenomenon called Weak-to-Strong Generalizationis verified, the core idea of which
is to use weak supervision signals from a weak model to train a strong model (Burns et al., 2023). Specifically,
the weak model is trained on ground truth and then annotates new data with weak labels for training the strong
model. The results across three settings ( i.e. NLP classification, chess puzzles and reward modeling) reflect that
weak-to-strong generalization is a robust phenomenon, yet there is room for further improvement, such as narrow-
ing the gap between a strong model trained with weak labels and ground truth. Weak-to-Strong Generalization
provides a valuable analogy for the superalignment problem: how humans can supervise super AI systems as weak
supervisors. The insight behind weak-to-strong generalization is that the strong model can generalize beyond
weak labels instead of merely imitating the behavior of weak models. In other words, the weak model elicits the
strong model’s capability. However, verifyingweak-to-strong generalization is challenging if humans don’t know
the ground truth. Nonetheless, weak-to-strong generalization still offers a valuable perspective for solving the
superalignment problem.
The framework forweak-to-strong generalizationhas been further expanding and integrating with scalable over-
sight. Empirical results show that weak models can evaluate the correctness of stronger models by assessing the
debate between two expert models (Khan et al., 2024). Additionally, making expert debaters more persuasive im-
proves non-experts’ ability to discern truth in debates, evidencing the effectiveness of aligning models with debate
strategies without ground truth. Some frameworks employ a external amplifier to create an iterated distillation and
amplification process, which presents a potential framework for integrating weak-to-strong generalization tech-
niques with IDA during the training process (Ji et al., 2024a). Moreover, Leike (2023a) proposes several methods
to integrate scalable oversight with weak-to-strong generalization techniques, e.g., recursively decomposing tasks
31



Source: data\tc20_2501.12962v1\referenced_papers\[91]_2206.07682.pdf (Page 6):

Published in Transactions on Machine Learning Research (08/2022)
FLOPs (540B parameters) led to a signiﬁcant jump in performance, without the signiﬁcant architectural
changes suggested by Brown et al. (2020).
5.1 Potential explanations of emergence
Although there are dozens of examples of emergent abilities, there are currently few compelling explanations
for why such abilities emerge in the way they do. For certain tasks, there may be natural intuitions for why
emergence requires a model larger than a particular threshold scale. For instance, if a multi-step reasoning
task requiresl steps of sequential computation, this might require a model with a depth of at leastO (l)
layers. It is also reasonable to assume that more parameters and more training enable better memorization
that could be helpful for tasks requiring world knowledge.4 As an example, good performance on closed-book
question-answering may require a model with enough parameters to capture the compressed knowledge
base itself (though language model-based compressors can have higher compression ratios than conventional
compressors (Bellard, 2021)).
It is also important to consider the evaluation metrics used to measure emergent abilities (BIG-Bench,
2022). For instance, using exact string match as the evaluation metric for long-sequence targets may disguise
compounding incremental improvements as emergence. Similar logic may apply for multi-step or arithmetic
reasoning problems, where models are only scored on whether they get the ﬁnal answer to a multi-step
problem correct, without any credit given to partially correct solutions. However, the jump in ﬁnal answer
accuracy does not explain why the quality of intermediate steps suddenly emerges to above random, and using
evaluation metrics that do not give partial credit are at best an incomplete explanation, because emergent
abilities are still observed on many classiﬁcation tasks (e.g., the tasks in Figure 2D–H).
As an alternative evaluation, we measure cross-entropy loss, which is used in scaling laws for pre-training, for
the six emergent BIG-Bench tasks, as detailed in Appendix A. This analysis follows the same experimental
setup from BIG-Bench (2022) and aﬃrms their conclusions for the six emergent tasks we consider. Namely,
cross-entropy loss improves even for small model scales where the downstream metrics (exact match, BLEU,
and accuracy) are close to random and do not improve, which shows that improvements in the log-likelihood
of the target sequence can be masked by such downstream metrics. However, this analysis does not explain
why downstream metrics are emergent or enable us to predict the scale at which emergence occurs. Overall,
more work is needed to tease apart what enables scale to unlock emergent abilities.
5.2 Beyond scaling
Although we may observe an emergent ability to occur at a certain scale, it is possible that the ability could
be later achieved at a smaller scale—in other words, model scale is not the singular factor for unlocking
an emergent ability. As the science of training large language models progresses, certain abilities may be
unlocked for smaller models with new architectures, higher-quality data, or improved training procedures.
For example, there are 14 BIG-Bench tasks5 for which LaMDA 137B and GPT-3 175B models perform
at near-random, but PaLM 62B in fact achieves above-random performance, despite having fewer model
parameters and training FLOPs. While there is not an empirical study ablating every diﬀerence between
PaLM 62B and prior models (the computational cost would be too high), potential reasons for the better
performance of PaLM could include high-quality training data (e.g., more multilingual and code data than
LaMDA) and architectural diﬀerences (e.g., split digit-encodings; see Section 2 in Chowdhery et al. (2022)).
Another potentially way of unlocking emergence is through a diﬀerent pre-training objective—it was shown
in Tay et al. (2022c) that a computationally-eﬃcient continued pre-training stage on a mixture-of-denoisers
objective (Tay et al., 2022a) enabled emergent performance on several BIG-Bench tasks.
Moreover, once an ability is discovered, further research may make the ability available for smaller scale
models. Consider the nascent direction of enabling language models to follow natural language instructions
describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang et al., 2022,inter alia). Although Wei et al.
(2022a) initially found that instruction-based ﬁnetuning only worked for 68B parameter or larger decoder-only
4Though note that encoding world knowledge in parameters is just one approach; there are others (e.g., Guu et al., 2020;
Borgeaud et al., 2021).
5These tasks are enumerated in Appendix F.
7



Source: data\tc20_2501.12962v1\referenced_papers\[53]_2310.19852.pdf (Page 26):

2.4 Scalable Oversight
privacy breaches concurrently. This approach offers a more efficient evaluation of privacy risks by employing
established NLP techniques, in contrast to conventional learning methods, which depend heavily on large-scale
manual data annotation.
At their core, the RLxF methods utilize the strategy of decomposing a large problem into smaller sub-problems,
enabling the use of more efficient tools, such as AI and software, for rapid sub-problem resolution. By leveraging
the solutions to these sub-problems, the resolution of the main issue can be expedited. These techniques can
be regarded as elementary instances of IDA; the primary distinction lies in the absence of a continual iterative
process. Nonetheless, evidence suggests they are promising to offer feedback for AI systems that exceed human
performance (Wu et al., 2021). Consequently, these methods can serve as foundational techniques in the training
of more advanced AI systems.
2.4.2 Iterated Distillation and Amplification
Iterated Distillation and Amplification (IDA) introduces a framework for constructing scalable oversight through
iterative collaboration between humans and AIs (Christiano et al., 2018). The process commences with an initial
agent, denoted as A[0], which mirrors the decision-making of a human, H. A[0] undergoes training using a potent
technique that equips it with near-human-level proficiency (the distillation step); Then, collaborative interaction
between H and multiple A[0] instances leads to the creation of an enhanced agent, A[1] (the amplification step).
The successive process is described27 in Algorithm 1.
Cotra (2018) distinguishes between broad and narrow definitions within both RL and IRL. Broad RL gives
sparse reward signals to AI systems and allows autonomous exploration and optimization of cumulative future
rewards. This can lead to super-human novel strategies but makes it hard to specify what we care about perfectly.
Narrow RL gives dense feedback rewarding the reasonableness of choices instead of final outcomes. This makes
ML systems more human-like but limits capabilities. Similarly, broad IRL infers deep long-term values from the
full range of human behaviors, while narrow IRL only infers short-term instrumental values. The former is a higher
risk, while the latter is limited in capabilities.
During IDA training, narrow techniques are needed to ensure each agent itself mimics human behaviors. Specif-
ically, narrow RL or IL can be used to train the agent to be as human-like and controllable as possible. Humans can
leverage agents’ computing power and parallelizability to devise more far-sighted, macro strategies. This is essen-
tially an amplification of human intrinsic capabilities. In the next iteration, agents again mimic this strengthened
human-machine system using narrow techniques. This enables a gradual transition from narrow ability to broad
ability while keeping the agents aligned with human values. As iterations increase, the human-machine system
becomes more and more capable, gradually approximating a system that is both highly capable and aligned with
human values, achieving both safety and capability. In other words, Narrow techniques are used to ensure agents
follow human values, while the broadened human strategies in the amplification stage are a way of utilizing the
agents, and do not expand the agents’ own learning goals.
IDA is well illustrated by AlphaZero (Christiano et al., 2018; Nguyen, 2020). The algorithm starts with a simple
policy (e.g., random move selection) and learns from its self-play games, the amplification phase. It then uses
these games as training data to develop better move selection heuristics, the distillation phase. This distillation-
amplification process can be repeated to create a fast and proficient Go-playing AI. Here, the distinction between
alignment and capability is crucial (Mennen, 2018). An aligned but less capable AI tries to win but may not
succeed against moderate opponents. A capable but poorly aligned AI achieves certain game properties other than
winning. The goal is that AI is capable and aligned, proficient at the game, and aligned with the goal of winning
the game.
The feasibility of IDA has sparked considerable debate (Yudkowsky, 2018). IDA operates under a crucial as-
sumption that errors won’t continuously accumulate throughout the iterations(Leike et al., 2018). Thus, technical
challenges persist during the distillation and amplification step, necessitating sufficiently advanced and safe learn-
ing techniques. Additionally, despite the original authors likening IDA to the training process of AlphaZero (Silver
et al., 2017) and having demonstrated it in toy environments (Christiano et al., 2018), its practicality hinges on en-
suring that Hcan delegate portions of complex tasks to A, analogous to a leader orchestrating a team to accomplish
a project collectively. In practice, Gato (Reed et al., 2022) illustrates key aspects of IDA (Mukobi, 2022) that may
pave the way to AGI. It consolidates the abilities of multiple expert AIs into a singular model, validating that IDA’s
distillation can be achieved using contemporary deep learning. While not fully realized, Gato hints at amplification
potential, harnessing its diverse skills to accelerate the learning of new tasks. However, Gato lacks safe amplifica-
tion or distillation methods to maintain alignment properties. Crafting alignment-preserving IDA methods suited
for models like Gato remains a crucial direction for AI safety research. In essence, while Gato signifies notable
progress in actualizing IDA, further theoretical advancements are imperative to ensure that the IDA framework
leads to safe AGI.
27We reference the pseudo-code by Cotra (2018) for this description.
27



## Processing Completed
Finished at: 2025-01-25 12:45:59
