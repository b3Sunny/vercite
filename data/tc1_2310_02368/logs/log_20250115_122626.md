# Claims Processing Log

Processing started at: 2025-01-15 12:26:26

## Table of Contents

[[log_20250115_122626###Claim 1/1|Claim 1/1]]


## Processing Details


### Claim 1/1

#### Claim Text
We train a Reward Model (RM) to score test cases based on these quality metrics, then employ it to provide feedback for the Proximal Policy Optimization (PPO) [39] algorithm to enhance LLMs to generate test cases maximizing the expected reward (i.e., higher quality tests).

#### Retrieved Documents
Source: data\tc1_2310_02368\referenced_papers\[39]_1707.06347.pdf (Page 0):

Proximal Policy Optimization Algorithms
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov
OpenAI
{joschu, filip, prafulla, alec, oleg}@openai.com
Abstract
We propose a new family of policy gradient methods for reinforcement learning, which al-
ternate between sampling data through interaction with the environment, and optimizing a
“surrogate” objective function using stochastic gradient ascent. Whereas standard policy gra-
dient methods perform one gradient update per data sample, we propose a novel objective
function that enables multiple epochs of minibatch updates. The new methods, which we call
proximal policy optimization (PPO), have some of the beneﬁts of trust region policy optimiza-
tion (TRPO), but they are much simpler to implement, more general, and have better sample
complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, includ-
ing simulated robotic locomotion and Atari game playing, and we show that PPO outperforms
other online policy gradient methods, and overall strikes a favorable balance between sample
complexity, simplicity, and wall-time.
1 Introduction
In recent years, several diﬀerent approaches have been proposed for reinforcement learning with
neural network function approximators. The leading contenders are deep Q-learning [Mni+15],
“vanilla” policy gradient methods [Mni+16], and trust region / natural policy gradient methods
[Sch+15b]. However, there is room for improvement in developing a method that is scalable (to
large models and parallel implementations), data eﬃcient, and robust (i.e., successful on a variety
of problems without hyperparameter tuning).Q-learning (with function approximation) fails on
many simple problems1 and is poorly understood, vanilla policy gradient methods have poor data
eﬃency and robustness; and trust region policy optimization (TRPO) is relatively complicated,
and is not compatible with architectures that include noise (such as dropout) or parameter sharing
(between the policy and value function, or with auxiliary tasks).
This paper seeks to improve the current state of aﬀairs by introducing an algorithm that attains
the data eﬃciency and reliable performance of TRPO, while using only ﬁrst-order optimization.
We propose a novel objective with clipped probability ratios, which forms a pessimistic estimate
(i.e., lower bound) of the performance of the policy. To optimize policies, we alternate between
sampling data from the policy and performing several epochs of optimization on the sampled data.
Our experiments compare the performance of various diﬀerent versions of the surrogate objec-
tive, and ﬁnd that the version with the clipped probability ratios performs best. We also compare
PPO to several previous algorithms from the literature. On continuous control tasks, it performs
better than the algorithms we compare against. On Atari, it performs signiﬁcantly better (in terms
of sample complexity) than A2C and similarly to ACER though it is much simpler.
1While DQN works well on game environments like the Arcade Learning Environment [Bel+15] with discrete
action spaces, it has not been demonstrated to perform well on continuous control benchmarks such as those in
OpenAI Gym [Bro+16] and described by Duan et al. [Dua+16].
1
arXiv:1707.06347v2  [cs.LG]  28 Aug 2017



Source: data\tc1_2310_02368\referenced_papers\[39]_1707.06347.pdf (Page 1):

2 Background: Policy Optimization
2.1 Policy Gradient Methods
Policy gradient methods work by computing an estimator of the policy gradient and plugging it
into a stochastic gradient ascent algorithm. The most commonly used gradient estimator has the
form
ˆg= ˆEt
[
∇θlog πθ(at |st) ˆAt
]
(1)
where πθ is a stochastic policy and ˆAt is an estimator of the advantage function at timestep t.
Here, the expectation ˆEt[...] indicates the empirical average over a ﬁnite batch of samples, in an
algorithm that alternates between sampling and optimization. Implementations that use automatic
diﬀerentiation software work by constructing an objective function whose gradient is the policy
gradient estimator; the estimator ˆg is obtained by diﬀerentiating the objective
LPG(θ) = ˆEt
[
log πθ(at |st) ˆAt
]
. (2)
While it is appealing to perform multiple steps of optimization on this loss LPG using the same
trajectory, doing so is not well-justiﬁed, and empirically it often leads to destructively large policy
updates (see Section 6.1; results are not shown but were similar or worse than the “no clipping or
penalty” setting).
2.2 Trust Region Methods
In TRPO [Sch+15b], an objective function (the “surrogate” objective) is maximized subject to a
constraint on the size of the policy update. Speciﬁcally,
maximize
θ
ˆEt
[ πθ(at |st)
πθold(at |st)
ˆAt
]
(3)
subject to ˆEt[KL[πθold(·|st),πθ(·|st)]] ≤δ. (4)
Here, θold is the vector of policy parameters before the update. This problem can eﬃciently be
approximately solved using the conjugate gradient algorithm, after making a linear approximation
to the objective and a quadratic approximation to the constraint.
The theory justifying TRPO actually suggests using a penalty instead of a constraint, i.e.,
solving the unconstrained optimization problem
maximize
θ
ˆEt
[ πθ(at |st)
πθold(at |st)
ˆAt □βKL[πθold(·|st),πθ(·|st)]
]
(5)
for some coeﬃcient β. This follows from the fact that a certain surrogate objective (which computes
the max KL over states instead of the mean) forms a lower bound (i.e., a pessimistic bound) on the
performance of the policyπ. TRPO uses a hard constraint rather than a penalty because it is hard
to choose a single value of β that performs well across diﬀerent problems—or even within a single
problem, where the the characteristics change over the course of learning. Hence, to achieve our goal
of a ﬁrst-order algorithm that emulates the monotonic improvement of TRPO, experiments show
that it is not suﬃcient to simply choose a ﬁxed penalty coeﬃcientβ and optimize the penalized
objective Equation (5) with SGD; additional modiﬁcations are required.
2



Source: data\tc1_2310_02368\referenced_papers\[39]_1707.06347.pdf (Page 7):

Figure 5: Still frames of the policy learned from RoboschoolHumanoidFlagrun. In the ﬁrst six frames, the
robot runs towards a target. Then the position is randomly changed, and the robot turns and runs toward
the new target.
6.4 Comparison to Other Algorithms on the Atari Domain
We also ran PPO on the Arcade Learning Environment [Bel+15] benchmark and compared against
well-tuned implementations of A2C [Mni+16] and ACER [Wan+16]. For all three algorithms, we
used the same policy network architechture as used in [Mni+16]. The hyperparameters for PPO
are provided in Table 5. For the other two algorithms, we used hyperparameters that were tuned
to maximize performance on this benchmark.
A table of results and learning curves for all 49 games is provided in Appendix B. We consider
the following two scoring metrics: (1) average reward per episode over entire training period (which
favors fast learning), and (2) average reward per episode over last 100 episodes of training (which
favors ﬁnal performance). Table 2 shows the number of games “won” by each algorithm, where we
compute the victor by averaging the scoring metric across three trials.
A2C ACER PPO Tie
(1) avg. episode reward over all of training 1 18 30 0
(2) avg. episode reward over last 100 episodes 1 28 19 1
Table 2: Number of games “won” by each algorithm, where the scoring metric is averaged across three trials.
7 Conclusion
We have introduced proximal policy optimization, a family of policy optimization methods that use
multiple epochs of stochastic gradient ascent to perform each policy update. These methods have
the stability and reliability of trust-region methods but are much simpler to implement, requiring
only few lines of code change to a vanilla policy gradient implementation, applicable in more general
settings (for example, when using a joint architecture for the policy and value function), and have
better overall performance.
8 Acknowledgements
Thanks to Rocky Duan, Peter Chen, and others at OpenAI for insightful comments.
8



Source: data\tc1_2310_02368\referenced_papers\[35]_2210.01241.pdf (Page 17):

Published as a conference paper at ICLR 2023
A O N-POLICY ALGORITHM IMPLEMENTATION DETAILS
A.1 PPO D ETAILS
Given discussion and equations in Section 3.3, we further note that we follow (Ziegler et al., 2019)
and dynamically adapt the KL coefﬁcient βduring training where,
et = clip
(KL (π(at|st)||π0(at|st)) −KLtarget
KLtarget
,−0.2,0.2
)
(2)
βt+1 = βt(1 + Kβet) (3)
where KLtarget is user-speciﬁed KL divergence between initial model hand current policy πand Kβ
is rate of update which we generally set to 0.2 in our experiments.
To increase stability during training, we further use Generalized Advantage Estimation (GAE) (Schul-
man et al., 2015b) and deﬁne the advantage estimator ˆA(sn,an) based on the Temporal Difference
residual as:
δt = r(st,at) + Vφ(st+1) −Vφ(st). (4)
ˆA(sn,an) =
∞∑
t=0
λtδn+t, (5)
where λprovides the trade-off between bias and variance.
A.2 NLPO D ETAILS
NLPO learns to mask irrelevant language by maintaining a masking policy πψ: the masking policy is
a copy of the current policy (πθ), but is updated only every µsteps. Given Z(πθ) = ∑
a∈Vπθ0 (a|s)
the normalization value of the sum of probabilities of all action a ∈A given a particular State
s ∈S , let the parameterized top- p vocabulary Vp
πθ ⊂V be the subset of the vocab, consisting
of the top- p highest probability vocabulary tokens with respect to πθ. Formally, let Zp be the
normalization value for the parameterized top-pvocabulary, can be deﬁned as the subset of tokens
that maximizes Zp(πθ) = ∑
a∈Vkπθ
πθ(a|s). Then optimizing a policy according to the parameterized
top-pvocabulary can be deﬁned as:
πψ(·|s,πθ) =
{
πθ(·|s)/Zp(πθ) if a∈Vp
πθ and Z(πθ)
0 otherwise. (6)
18



Source: data\tc1_2310_02368\referenced_papers\[39]_1707.06347.pdf (Page 3):

0 1
Linear interpolation factor
0.02
0.00
0.02
0.04
0.06
0.08
0.10
0.12 Et[KLt]
LCPI = Et[rtAt]
Et[clip(rt, 1 , 1 + )At]
LCLIP = Et[min(rtAt, clip(rt, 1 , 1 + )At)]
Figure 2: Surrogate objectives, as we interpolate between the initial policy parameter θold, and the updated
policy parameter, which we compute after one iteration of PPO. The updated policy has a KL divergence of
about 0.02 from the initial policy, and this is the point at which LCLIP is maximal. This plot corresponds
to the ﬁrst policy update on the Hopper-v1 problem, using hyperparameters provided in Section 6.1.
4 Adaptive KL Penalty Coeﬃcient
Another approach, which can be used as an alternative to the clipped surrogate objective, or in
addition to it, is to use a penalty on KL divergence, and to adapt the penalty coeﬃcient so that we
achieve some target value of the KL divergencedtarg each policy update. In our experiments, we
found that the KL penalty performed worse than the clipped surrogate objective, however, we’ve
included it here because it’s an important baseline.
In the simplest instantiation of this algorithm, we perform the following steps in each policy
update:
•Using several epochs of minibatch SGD, optimize the KL-penalized objective
LKLPEN (θ) = ˆEt
[ πθ(at |st)
πθold(at |st)
ˆAt □βKL[πθold(·|st),πθ(·|st)]
]
(8)
•Compute d= ˆEt[KL[πθold(·|st),πθ(·|st)]]
– If d<d targ/1.5, β← β/2
– If d>d targ ×1.5, β← β×2
The updated β is used for the next policy update. With this scheme, we occasionally see policy
updates where the KL divergence is signiﬁcantly diﬀerent from dtarg, however, these are rare, and
β quickly adjusts. The parameters 1 .5 and 2 above are chosen heuristically, but the algorithm is
not very sensitive to them. The initial value of β is a another hyperparameter but is not important
in practice because the algorithm quickly adjusts it.
5 Algorithm
The surrogate losses from the previous sections can be computed and diﬀerentiated with a minor
change to a typical policy gradient implementation. For implementations that use automatic dif-
ferentation, one simply constructs the loss LCLIP or LKLPEN instead of LPG, and one performs
multiple steps of stochastic gradient ascent on this objective.
Most techniques for computing variance-reduced advantage-function estimators make use a
learned state-value function V(s); for example, generalized advantage estimation [Sch+15a], or the
4



## Processing Completed
Finished at: 2025-01-15 12:26:30
