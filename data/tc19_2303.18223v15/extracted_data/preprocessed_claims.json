[
  {
    "original_claim": "Given a compute budget c, they empirically presented three basic formulas for the scaling law6: L(N) = \u0012Nc N \u0013αN , α N ∼ 0.076, Nc ∼ 8.8 × 1013 (1) L(D) = \u0012Dc D \u0013αD , α D ∼ 0.095, Dc ∼ 5.4 × 1013 L(C) = \u0012Cc C \u0013αC , α C ∼ 0.050, Cc ∼ 3.1 × 108 where L(·) denotes the cross entropy loss in nats, and a follow-up study [58] from OpenAI has shown that the language modeling loss can be decomposed into two parts, namely irreducible loss (the entropy of the true data distribution) and reducible loss (an estimate of the KL divergence between the true and model distributions).",
    "context_before": [
      "In 2020, Kaplan et al. (the OpenAI team) firstly proposed to model the power-law relationship of model performance with respective to three major factors, namely model size (N), dataset size (D), and the amount of training compute ( C), for neural language models."
    ],
    "context_after": [
      "The three laws were derived by fitting the model performance with varied data sizes (22M to 23B tokens), model sizes (768 to 1.5B nonembedding parameters) and training compute, under some assumptions ( e.g., the analysis of one factor should be not bottlenecked by the other two factors)."
    ],
    "references": [
      "58"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "58",
        "main_query": "a follow-up study from OpenAI has shown that the language modeling loss can be decomposed into two parts, namely irreducible loss (the entropy of the true data distribution) and reducible loss (an estimate of the KL divergence between the true and model distributions)",
        "rewritten_queries": [
          "OpenAI's follow-up research indicates that language modeling loss consists of irreducible loss and reducible loss",
          "According to a study by OpenAI, the language modeling loss is made up of two components: irreducible loss and reducible loss",
          "A subsequent study from OpenAI revealed that the loss in language modeling can be divided into irreducible and reducible losses"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For instance, small proxy models can be trained to find the optimal schedule of the data mixture for large models [59].",
    "context_before": [
      "Firstly, for large models, it is infeasible to rigorously examine various training tricks or variants, and it would be very helpful if experiences gained from small models could also apply to large models."
    ],
    "context_after": [
      "Secondly, the training of large-scale models takes a long time, often suffering from issues such as training loss spike, and scaling law can be employed to monitor the training status of LLMs, e.g., identifying abnormal performance at an early time."
    ],
    "references": [
      "59"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "59",
        "main_query": "small proxy models can be trained to find the optimal schedule of the data mixture for large models",
        "rewritten_queries": [
          "training small proxy models to optimize data mixture scheduling for large models",
          "using small proxy models to determine the best data mixture schedule for larger models",
          "optimal scheduling of data mixtures for large models can be achieved through training small proxy models"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "An empirical study [58] from the OpenAI team has shown that representation quality or semantic content can still effectively improve even if approaching the point of diminishing returns ( i.e., approaching the irreducible loss) [58].",
    "context_before": [
      "Despite that scaling law characterizes a smooth trend of performance increase (or loss decrease), it also indicates that diminishing returns 7 might occur as model scaling."
    ],
    "context_after": [
      "This finding suggests that training large models are promising for improving the performance of downstream tasks."
    ],
    "references": [
      "58"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "58",
        "main_query": "An empirical study from the OpenAI team has shown that representation quality or semantic content can still effectively improve even if approaching the point of diminishing returns.",
        "rewritten_queries": [
          "A study by the OpenAI team indicates that representation quality can improve even near diminishing returns.",
          "Research from OpenAI demonstrates that semantic content can still enhance performance despite nearing irreducible loss.",
          "The OpenAI team's empirical findings suggest that improvements in representation quality are possible even at the point of diminishing returns."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Thus, it will be meaningful to study how scaling laws apply to a data-constrained regime [61], where data repetition or augmentation might be useful to alleviate data scarcity.",
    "context_before": [
      "With the ever-increasing model scale, the public text data would be soon “exhausted” for LLMs ."
    ],
    "context_after": [
      "Task-level predictability."
    ],
    "references": [
      "61"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "61",
        "main_query": "how scaling laws apply to a data-constrained regime",
        "rewritten_queries": [
          "the application of scaling laws in situations with limited data",
          "the relevance of scaling laws in a context of data scarcity",
          "investigating scaling laws in a regime with constrained data availability"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Thus, a basic problem is that how the decrease of language modeling loss translates into the improvement of task performance [58].",
    "context_before": [
      "Existing research of scaling laws are mostly conducted in terms of language modeling loss (e.g., per-token cross-entropy loss in nats ), while in practice we are more concerned about the performance of LLMs on actual tasks."
    ],
    "context_after": [
      "Intuitively, a model with a smaller language modeling loss tends to yield a better performance on downstream tasks, since language modeling loss can be considered as a general measure of the overall model capacity."
    ],
    "references": [
      "58"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "58",
        "main_query": "how the decrease of language modeling loss translates into the improvement of task performance",
        "rewritten_queries": [
          "the relationship between reduced language modeling loss and enhanced task performance",
          "how lowering language modeling loss affects task performance improvement",
          "the impact of decreased language modeling loss on task performance"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Grokking refers that “a pattern in the data, improving generalization performance from random chance level to perfect generalization”, quoted from the original paper [73].",
    "context_before": [
      "Such a learning process is essentially not smooth and stable (i.e., language ability does not develop at a constant rate over time), though a child actually grows."
    ],
    "context_after": [
      "This explanation is only for ease of understanding, and there is not direct evidence to connect the two points. every day."
    ],
    "references": [
      "73"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "73",
        "main_query": "a pattern in the data, improving generalization performance from random chance level to perfect generalization",
        "rewritten_queries": [
          "improvement in generalization performance from random chance to perfect generalization",
          "a data pattern that enhances generalization from chance level to perfect accuracy",
          "the concept of grokking as a shift from random chance to optimal generalization performance"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "CodeGen [86] Mar-2022 16.",
    "context_before": [
      "512 TPU v3 27 h ✓."
    ],
    "context_after": [
      "577B tokens."
    ],
    "references": [
      "86"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "86",
        "main_query": "CodeGen",
        "rewritten_queries": [
          "CodeGen model",
          "CodeGen architecture",
          "CodeGen framework"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "CodeGeeX [92] Sep-2022 13.",
    "context_before": [
      "✓."
    ],
    "context_after": [
      "850B tokens."
    ],
    "references": [
      "92"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "92",
        "main_query": "CodeGeeX",
        "rewritten_queries": [
          "CodeGeeX model",
          "CodeGeeX system",
          "CodeGeeX framework"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Pythia [96] Apr-2023 12.",
    "context_before": [
      "2048 80G A100 21 d ✓."
    ],
    "context_after": [
      "300B tokens."
    ],
    "references": [
      "96"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "96",
        "main_query": "Pythia",
        "rewritten_queries": [
          "Pythia model",
          "Pythia architecture",
          "Pythia system"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "✓ ✓ LLaMA2 [99] Jul-2023 70.",
    "context_before": [
      "512 40G A100."
    ],
    "context_after": [
      "✓ ✓ 2T tokens."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "99",
        "main_query": "LLaMA2",
        "rewritten_queries": [
          "LLaMA2 model",
          "LLaMA2 architecture",
          "LLaMA2 version from July 2023"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Baichuan2 [100] Sep-2023 13.",
    "context_before": [
      "✓."
    ],
    "context_after": [
      "✓ ✓ 2.6T tokens."
    ],
    "references": [
      "100"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "100",
        "main_query": "Baichuan2",
        "rewritten_queries": [
          "Baichuan2 model",
          "Baichuan2 release",
          "Baichuan2 September 2023"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "QWEN [101] Sep-2023 14.",
    "context_before": [
      "✓."
    ],
    "context_after": [
      "✓ ✓ 3T tokens."
    ],
    "references": [
      "101"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "101",
        "main_query": "QWEN Sep-2023 14",
        "rewritten_queries": [
          "QWEN published in September 2023, issue 14",
          "September 2023 edition 14 of QWEN",
          "QWEN volume 14 from September 2023"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "FLM [102] Sep-2023 101.",
    "context_before": [
      "✓."
    ],
    "context_after": [
      "✓."
    ],
    "references": [
      "102"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "102",
        "main_query": "FLM",
        "rewritten_queries": [
          "FLM model",
          "FLM approach",
          "FLM technique"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Publicly Available Skywork [103] Oct-2023 13.",
    "context_before": [
      "192 A800 22 d ✓."
    ],
    "context_after": [
      "3.2T tokens."
    ],
    "references": [
      "103"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "103",
        "main_query": "Publicly Available Skywork",
        "rewritten_queries": [
          "Skywork that is publicly accessible",
          "Skywork available to the public",
          "Public access to Skywork"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Yuan 1.0 [109] Oct-2021 245.",
    "context_before": [
      "128 TPU v3 60 h ✓."
    ],
    "context_after": [
      "180B tokens."
    ],
    "references": [
      "109"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "109",
        "main_query": "Yuan 1.0",
        "rewritten_queries": [
          "Yuan version 1.0",
          "Yuan model 1.0",
          "Yuan 1.0 release"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Closed Source PaLM2 [120] May-2023 16.",
    "context_before": [
      "512 Ascend 910 100 d ✓."
    ],
    "context_after": [
      "✓."
    ],
    "references": [
      "120"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "120",
        "main_query": "Closed Source PaLM2",
        "rewritten_queries": [
          "PaLM2 is a closed source model",
          "The PaLM2 model is not open source",
          "Closed-source version of PaLM2"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Shortly after the release of this RL paper [79], the paper of the Proximal Policy Optimization (PPO) [128] was published in July 2017, which now has been the foundational RL algorithm for learning from human preferences [66].",
    "context_before": [
      "The related research of human alignment can be dated back to the year 2017 (or earlier) for OpenAI: a blog article entitled “learning from human preferences”18 was posted on the OpenAI blog describing a work that applied reinforcement learning (RL) to learn from the preference comparisons annotated by humans (similar to the reward training step in the aligning algorithm of InstructGPT in Figure 12)."
    ],
    "context_after": [
      "Later in January 2020, GPT-2 was finetuned using the aforementioned RL algorithms , which leveraged human preferences to improve the capacities of GPT-2 on NLP tasks."
    ],
    "references": [
      "128"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "128",
        "main_query": "Proximal Policy Optimization (PPO)",
        "rewritten_queries": [
          "PPO algorithm for reinforcement learning",
          "Proximal Policy Optimization method",
          "PPO as a foundational algorithm for learning from human preferences"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "From LLaMA [57], LLaMA-2 [99], LLaMA-3 [135] to LLaMA-3.1 [136], continuous updates have been made and the development is still ongoing.",
    "context_before": [
      "The LLaMA series of models has gained immense popularity and widespread attention due to its openness and effectiveness."
    ],
    "context_after": [
      "With increased parameters (the largest version has 405B), more pre-training tokens (15T tokens), and an extended context window (128K), LLaMA-3.1 has significantly enhanced its capabilities, and it also integrates additional components that work in synergy with the model, including new security and safety tools."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "99",
        "main_query": "LLaMA-2",
        "rewritten_queries": [
          "LLaMA version 2",
          "Second iteration of LLaMA",
          "LLaMA-2 model"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "RefinedWeb [171] is a web dataset obtained through rigorous selection and deduplication based on data from Common Crawl, encompassing all Common Crawl web records from 2008 to June 2023, totaling around 5T tokens.",
    "context_before": [
      "RefinedWeb."
    ],
    "context_after": [
      "The open-source portion consists of 600B tokens, with a data size of approximately 500GB."
    ],
    "references": [
      "171"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "171",
        "main_query": "RefinedWeb is a web dataset obtained through rigorous selection and deduplication based on data from Common Crawl, encompassing all Common Crawl web records from 2008 to June 2023, totaling around 5T tokens.",
        "rewritten_queries": [
          "RefinedWeb is a dataset derived from Common Crawl, featuring extensive selection and deduplication, covering web records from 2008 to June 2023 and amounting to about 5 trillion tokens.",
          "The dataset known as RefinedWeb was created through careful selection and deduplication of Common Crawl data, including all web records from 2008 to June 2023, with a total of approximately 5 trillion tokens.",
          "RefinedWeb, a web dataset, was formed by meticulously selecting and deduplicating data from Common Crawl, which includes all records from 2008 to June 2023, resulting in around 5 trillion tokens."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition to book data, scientific publication data such as paper is also important for model pre-training. arXiv Dataset [172] is a corpus of 1.7 million academic papers, covering a wide range of papers in the fields of physics, mathematics, and computer science.",
    "context_before": [
      "Academic Data."
    ],
    "context_after": [
      "S2ORC is a corpora that consists of 136M academic papers collected by Semantic Scholar."
    ],
    "references": [
      "172"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "172",
        "main_query": "arXiv Dataset is a corpus of 1.7 million academic papers, covering a wide range of papers in the fields of physics, mathematics, and computer science.",
        "rewritten_queries": [
          "The arXiv Dataset includes 1.7 million academic papers across physics, mathematics, and computer science.",
          "A corpus of 1.7 million academic papers in physics, mathematics, and computer science is provided by the arXiv Dataset.",
          "The arXiv Dataset comprises a collection of 1.7 million papers from various academic fields including physics, mathematics, and computer science."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "CodeGen has utilized BIGQUERY [86], a subset of the BigQuery dataset, for training the multilingual version of CodeGen (CodeGen-Multi).",
    "context_before": [
      "Google has publicly released the BigQuery dataset , which includes a substantial number of open-source licensed code snippets in various programming languages, serving as a representative code dataset."
    ],
    "context_after": [
      "In addition, Hugging Face has collected and released a code dataset named The Stack , covering more than 30 programming languages."
    ],
    "references": [
      "86"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "86",
        "main_query": "BIGQUERY",
        "rewritten_queries": [
          "BigQuery dataset used for training CodeGen-Multi",
          "CodeGen-Multi training with BIGQUERY dataset",
          "Utilization of BIGQUERY in CodeGen multilingual training"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition, Hugging Face has collected and released a code dataset named The Stack [175], covering more than 30 programming languages.",
    "context_before": [
      "CodeGen has utilized BIGQUERY , a subset of the BigQuery dataset, for training the multilingual version of CodeGen (CodeGen-Multi)."
    ],
    "context_after": [
      "The Stack is continuously updated, and the v1.2 version has expanded to 358 programming languages."
    ],
    "references": [
      "175"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "175",
        "main_query": "The Stack covering more than 30 programming languages",
        "rewritten_queries": [
          "Hugging Face's The Stack dataset includes over 30 programming languages",
          "The Stack dataset released by Hugging Face encompasses more than 30 programming languages",
          "Hugging Face has released a code dataset called The Stack that covers 30+ programming languages"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "The Pile dataset is widely used in models with different parameter scales, such as GPT-J (6B) [176], CodeGen (16B) [86], and Megatron-Turing NLG (530B) [113].",
    "context_before": [
      "It is constructed from 22 diverse high-quality subsets."
    ],
    "context_after": [
      "ROOTS is composed of various smaller datasets (totally 1.61 TB of text) and covers 59 different languages (containing natural languages and programming languages), which have been used for training BLOOM ."
    ],
    "references": [
      "86"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "86",
        "main_query": "CodeGen (16B)",
        "rewritten_queries": [
          "CodeGen model with 16 billion parameters",
          "16B CodeGen architecture",
          "CodeGen with 16 billion parameters"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Another mixture dataset is Dolma [177], which includes web text from Common Crawl, academic papers from Semantic Scholar, GitHub code, books, social media from Reddit, and Wikipedia data.",
    "context_before": [
      "ROOTS is composed of various smaller datasets (totally 1.61 TB of text) and covers 59 different languages (containing natural languages and programming languages), which have been used for training BLOOM ."
    ],
    "context_after": [
      "Dolma consisting of 3T tokens of approximately 200TB of raw text and has been used to train OLMo ."
    ],
    "references": [
      "177"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "177",
        "main_query": "Dolma, which includes web text from Common Crawl, academic papers from Semantic Scholar, GitHub code, books, social media from Reddit, and Wikipedia data.",
        "rewritten_queries": [
          "The Dolma dataset comprises web text from Common Crawl, academic papers from Semantic Scholar, GitHub code, books, Reddit social media content, and Wikipedia data.",
          "Dolma is a mixture dataset that contains web text from Common Crawl, academic papers from Semantic Scholar, GitHub code, books, Reddit posts, and Wikipedia information.",
          "The dataset Dolma includes a variety of sources such as Common Crawl web text, Semantic Scholar academic papers, GitHub code, books, Reddit social media, and Wikipedia data."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Dolma consisting of 3T tokens of approximately 200TB of raw text and has been used to train OLMo [178].",
    "context_before": [
      "Another mixture dataset is Dolma , which includes web text from Common Crawl, academic papers from Semantic Scholar, GitHub code, books, social media from Reddit, and Wikipedia data."
    ],
    "context_after": [
      "In practice, it commonly requires a mixture of different data sources for pre-training LLMs (see Figure 6), instead of a single corpus."
    ],
    "references": [
      "178"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "178",
        "main_query": "Dolma consisting of 3T tokens of approximately 200TB of raw text and has been used to train OLMo",
        "rewritten_queries": [
          "Dolma is made up of 3 trillion tokens and around 200TB of raw text, utilized for training OLMo",
          "The dataset Dolma, which contains 3T tokens and about 200TB of text, has been employed to train OLMo",
          "OLMo has been trained using Dolma, which consists of approximately 200TB of raw text and 3 trillion tokens"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Recently, FLAN-v2 [197] is also proposed, which expands FLAN by mixing additional instruction datasets, including Muffin [67], NIV2 [88], T0-SF [28], and CoT [198–200].",
    "context_before": [
      "FLAN consists of 62 widely used NLP benchmarks in its original version."
    ],
    "context_after": [
      "Muffin contains 62 tasks from the original FLAN and additional 26 tasks, including conversation and code synthesis tasks."
    ],
    "references": [
      "197"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "197",
        "main_query": "FLAN-v2 is also proposed, which expands FLAN by mixing additional instruction datasets",
        "rewritten_queries": [
          "FLAN-v2 expands the original FLAN by incorporating more instruction datasets",
          "The proposal of FLAN-v2 includes the integration of additional instruction datasets into FLAN",
          "FLAN-v2 enhances FLAN by adding various instruction datasets"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In this category, ShareGPT [153], OpenAssistant [186] and Dolly [185] are three commonly used datasets for LLM fine-tuning.",
    "context_before": [
      "The conversation types include open-ended generation, question answering, brainstorming, and chatting."
    ],
    "context_after": [
      "ShareGPT is collected from a data collection platform where users can upload their conversations with ChatGPT or GPT-4 through the ShareGPT API."
    ],
    "references": [
      "186"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "186",
        "main_query": "OpenAssistant",
        "rewritten_queries": [
          "OpenAssistant dataset for LLM fine-tuning",
          "Dataset named OpenAssistant used in LLM fine-tuning",
          "OpenAssistant as a dataset for training large language models"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "OpenAssistant [186] is a multilingual corpus containing 66,497 real-world conversation trees between human and AI assistant.",
    "context_before": [
      "Currently, this dataset consists of approximately 90,000 conversations, including real instructions or inquiries from human and responses from ChatGPT."
    ],
    "context_after": [
      "Each conversation tree consists of multiple nodes, and each node represents the information generated by a role in the dialogue."
    ],
    "references": [
      "186"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "186",
        "main_query": "OpenAssistant is a multilingual corpus containing 66,497 real-world conversation trees between human and AI assistant.",
        "rewritten_queries": [
          "OpenAssistant includes 66,497 conversation trees between humans and AI assistants in multiple languages.",
          "The multilingual dataset OpenAssistant comprises 66,497 real-world dialogues between humans and AI.",
          "There are 66,497 conversation trees in OpenAssistant, representing interactions between human users and AI assistants."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In this category, Self-Instruct52K [147], Alpaca [146] and Baize [189] are three commonly used synthetic datasets for LLMs.",
    "context_before": [
      "This kind of datasets are typically constructed by instructing LLMs, based on pre-defined guidance rules or methods."
    ],
    "context_after": [
      "Self-Instruct-52K is an instruction dataset generated through the self-instruct method, consisting of 82,000 instances with 52,000 instructions."
    ],
    "references": [
      "189"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "189",
        "main_query": "Baize",
        "rewritten_queries": [
          "Baize synthetic dataset for LLMs",
          "Commonly used dataset Baize for language models",
          "Baize dataset in the context of LLMs"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Baize [189] is an English multi-turn conversation corpus constructed using ChatGPT, comprising 111.5K instances.",
    "context_before": [
      "Moreover, 60% of the examples are pure instructions without the input part in the final dataset."
    ],
    "context_after": [
      "To create Baize, a method called “self-chat” is purposed, where ChatGPT takes on the roles of both the user and the AI assistant in turns, generating information in a conversational format. 3.3.2 Alignment Datasets Apart from instruction tuning, it is important to construct high-quality datasets for aligning LLMs with human values and preferences ( e.g., helpfulness, honesty, and harmlessness)."
    ],
    "references": [
      "189"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "189",
        "main_query": "Baize is an English multi-turn conversation corpus constructed using ChatGPT, comprising 111.5K instances.",
        "rewritten_queries": [
          "The Baize corpus, created with ChatGPT, includes 111.5K instances of English multi-turn conversations.",
          "Constructed using ChatGPT, Baize is an English multi-turn conversation dataset with 111.5K instances.",
          "Baize, an English multi-turn conversation corpus made with ChatGPT, consists of 111.5K instances."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To create Baize, a method called “self-chat” [189] is purposed, where ChatGPT takes on the roles of both the user and the AI assistant in turns, generating information in a conversational format. 3.3.2 Alignment Datasets Apart from instruction tuning, it is important to construct high-quality datasets for aligning LLMs with human values and preferences ( e.g., helpfulness, honesty, and harmlessness).",
    "context_before": [
      "Baize is an English multi-turn conversation corpus constructed using ChatGPT, comprising 111.5K instances."
    ],
    "context_after": [
      "In this section, we introduce several widely used datasets for alignment tuning, including HH-RLHF , SHP , PKU-SafeRLHF , Stack Exchange Preferences and Sandbox Alignment Data ."
    ],
    "references": [
      "189"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "189",
        "main_query": "self-chat",
        "rewritten_queries": [
          "method called self-chat where ChatGPT acts as both user and AI assistant",
          "self-chat technique involving ChatGPT taking on dual roles",
          "ChatGPT's self-chat approach for generating conversational information"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In this section, we introduce several widely used datasets for alignment tuning, including HH-RLHF [183], SHP [191], PKU-SafeRLHF [195], Stack Exchange Preferences [192] and Sandbox Alignment Data [193].",
    "context_before": [
      "To create Baize, a method called “self-chat” is purposed, where ChatGPT takes on the roles of both the user and the AI assistant in turns, generating information in a conversational format. 3.3.2 Alignment Datasets Apart from instruction tuning, it is important to construct high-quality datasets for aligning LLMs with human values and preferences ( e.g., helpfulness, honesty, and harmlessness)."
    ],
    "context_after": [
      "We show their details in Table."
    ],
    "references": [
      "195"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "195",
        "main_query": "PKU-SafeRLHF",
        "rewritten_queries": [
          "PKU-SafeRLHF dataset for alignment tuning",
          "Dataset PKU-SafeRLHF used in alignment tuning",
          "Alignment tuning dataset PKU-SafeRLHF"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "PKU-SafeRLHF [195] encompasses more than 330K instances of expert comparison data, concentrating on the helpfulness and harmlessness.",
    "context_before": [
      "Different from HH-RLHF , the data in SHP consists of naturally occurring and humanwritten responses."
    ],
    "context_after": [
      "Each instance in the dataset includes a question and two responses, accompanied by safety labels for each response and two preference annotations between the two responses according to helpfulness and harmlessness."
    ],
    "references": [
      "195"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "195",
        "main_query": "PKU-SafeRLHF encompasses more than 330K instances of expert comparison data, concentrating on the helpfulness and harmlessness.",
        "rewritten_queries": [
          "PKU-SafeRLHF includes over 330,000 expert comparison instances focused on helpfulness and harmlessness.",
          "The PKU-SafeRLHF dataset contains more than 330K instances of data comparing helpfulness and harmlessness.",
          "With over 330,000 instances, PKU-SafeRLHF focuses on expert comparisons regarding helpfulness and harmlessness."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "DeepSpeed-Chat [210] is a fast, cost-effective, and easy-to-use system framework that enables the integration of the complete RLHF process during model training.",
    "context_before": [
      "It currently supports over 13,000 models across three popular model architectures, such as LLaMA , Mistral , and OPT ."
    ],
    "context_after": [
      "It is featured by three major functionalities: (1) it simplifies the training and inference process for ChatGPT-like models, enabling using a simple script to implement multiple training or inference steps; (2) it replicates the training mode of InstructGPT and provides a complete pipeline for three training steps (i.e., SFT, reward model fine-tuning, and RLHF); (3) it integrates the training engine and inference engine of Deepspeed into a unified hybrid engine (Deepspeed HE) for RLHF training, which enables seamless switch between training and inference modes, and leveraging various optimizations from DeepSpeed Inference."
    ],
    "references": [
      "210"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "210",
        "main_query": "DeepSpeed-Chat is a fast, cost-effective, and easy-to-use system framework that enables the integration of the complete RLHF process during model training.",
        "rewritten_queries": [
          "DeepSpeed-Chat facilitates the entire RLHF process in model training efficiently and cost-effectively.",
          "The system framework DeepSpeed-Chat allows for quick and user-friendly integration of the full RLHF process during training.",
          "DeepSpeed-Chat provides a streamlined and economical solution for incorporating the complete RLHF process in model training."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "FLM [102] mixes Chinese and English corpora in nearly equal proportions.",
    "context_before": [
      "For example, BLOOM and PaLM have curated multilingual data covering 46 and 122 languages, respectively, within their pre-training corpora."
    ],
    "context_after": [
      "These models demonstrate impressive performance in multilingual tasks, such as translation, multilingual summarization, and multilingual question answering, and achieve comparable or superior performance to the state-of-theart models that are fine-tuned on the corpus in the target language(s)."
    ],
    "references": [
      "102"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "102",
        "main_query": "FLM mixes Chinese and English corpora in nearly equal proportions.",
        "rewritten_queries": [
          "FLM combines Chinese and English datasets in almost equal amounts.",
          "The FLM model integrates Chinese and English corpora in similar proportions.",
          "In FLM, the Chinese and English corpora are mixed in nearly equal ratios."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "By pretraining on a vast amount of scientific text, LLMs can achieve impressive performance in scientific and reasoning tasks [219].",
    "context_before": [
      "In order to enhance the understanding of scientific knowledge for LLMs , it is useful to incorporate a scientific corpus for model pre-training ."
    ],
    "context_after": [
      "To construct the scientific corpus, existing efforts mainly collect arXiv papers, scientific textbooks, math webpages, and other related scientific resources."
    ],
    "references": [
      "219"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "219",
        "main_query": "By pretraining on a vast amount of scientific text, LLMs can achieve impressive performance in scientific and reasoning tasks",
        "rewritten_queries": [
          "LLMs achieve impressive performance in scientific tasks by pretraining on extensive scientific text",
          "Pretraining on a large corpus of scientific literature allows LLMs to excel in reasoning and scientific tasks",
          "The performance of LLMs in scientific and reasoning tasks improves significantly with pretraining on vast scientific texts"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition to the above methods, LLMs (especially relatively small models) can be also employed for data selection, either by computing perplexity [232] or directly prompting LLMs [233] for measuring the sample importance.",
    "context_before": [
      "Based on specific keyword set, the noisy or unuseful elements in the text, such as HTML tags, hyperlinks, boilerplates, and offensive words, can be identified and removed."
    ],
    "context_after": [
      "However, using LLMs is unavoidably computationally intensive for large-scale data selection."
    ],
    "references": [
      "233",
      "232"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "233",
        "main_query": "directly prompting LLMs for measuring the sample importance",
        "rewritten_queries": [
          "using prompts with LLMs to assess sample importance",
          "prompting LLMs directly to evaluate the importance of samples",
          "employing LLMs with direct prompts for measuring sample significance"
        ]
      },
      {
        "related_to_reference": "232",
        "main_query": "computing perplexity",
        "rewritten_queries": [
          "using perplexity for data selection",
          "perplexity as a method for measuring sample importance",
          "employing perplexity in LLMs for data selection"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For example, Falcon [171] is trained on pure webpages, and CodeGen [86] largely increases the amount of code data.",
    "context_before": [
      "Furthermore, special data mixtures can be used to facilitate different purposes."
    ],
    "context_after": [
      "In practice, data mixture is often determined empirically, and we summarize several common strategies for finding an effective data mixture as follows:."
    ],
    "references": [
      "171",
      "86"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "171",
        "main_query": "Falcon is trained on pure webpages",
        "rewritten_queries": [
          "Falcon utilizes only webpages for training",
          "The training data for Falcon consists solely of webpages",
          "Falcon's training is based exclusively on pure webpages"
        ]
      },
      {
        "related_to_reference": "86",
        "main_query": "CodeGen largely increases the amount of code data.",
        "rewritten_queries": [
          "CodeGen significantly boosts the volume of code data.",
          "The amount of code data is greatly enhanced by CodeGen.",
          "CodeGen contributes to a substantial increase in code data."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To further examine the effect of different data sources, some studies have conducted ablation experiments by removing each data source one by one, and pre-train LLMs with specially curated datasets [227].",
    "context_before": [
      "In contrast, increasing the data source heterogeneity ( e.g., including diverse data sources) is critical for improving the downstream performance of LLMs ."
    ],
    "context_after": [
      "It has been shown that dropping data sources with high heterogeneity ( e.g., webpages) impacts LLM’s abilities more severely than dropping sources with low heterogeneity (e.g., academic corpus)."
    ],
    "references": [
      "227"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "227",
        "main_query": "pre-train LLMs with specially curated datasets",
        "rewritten_queries": [
          "conduct ablation experiments by removing each data source one by one",
          "examine the effect of different data sources on LLMs",
          "use specially curated datasets for pre-training LLMs"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Given the target downstream tasks, one can select pre-training data with either higher proximity in the feature space [250] or those that provide positive influences on downstream task performance [251].",
    "context_before": [
      "In addition to manually setting the data mixtures, several studies have proposed to optimize the data mixtures for improving the model pretraining ."
    ],
    "context_after": [
      "Further, to reduce the reliance of target tasks, DoReMi first trains a small reference model using given initial domain weights, and then trains another small proxy model, upweighting the domains on which the greatest discrepancies in likelihood 21 between the two models are observed."
    ],
    "references": [
      "250",
      "251"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "250",
        "main_query": "higher proximity in the feature space",
        "rewritten_queries": [
          "selecting pre-training data with greater feature space proximity",
          "choosing data that is closer in the feature space",
          "using pre-training data that has higher similarity in feature space"
        ]
      },
      {
        "related_to_reference": "251",
        "main_query": "those that provide positive influences on downstream task performance",
        "rewritten_queries": [
          "data that positively impacts downstream task performance",
          "pre-training data that enhances performance on target tasks",
          "data selections that improve outcomes for downstream tasks"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Further, to reduce the reliance of target tasks, DoReMi [59] first trains a small reference model using given initial domain weights, and then trains another small proxy model, upweighting the domains on which the greatest discrepancies in likelihood 21 between the two models are observed.",
    "context_before": [
      "Given the target downstream tasks, one can select pre-training data with either higher proximity in the feature space or those that provide positive influences on downstream task performance ."
    ],
    "context_after": [
      "Finally, the learned domain weights of the proxy model are applied to train a much larger LLM."
    ],
    "references": [
      "59"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "59",
        "main_query": "DoReMi first trains a small reference model using given initial domain weights, and then trains another small proxy model, upweighting the domains on which the greatest discrepancies in likelihood between the two models are observed.",
        "rewritten_queries": [
          "DoReMi initially trains a small reference model with initial domain weights before training a proxy model that upweights domains with the largest likelihood discrepancies.",
          "The process of DoReMi involves training a small reference model first, followed by a proxy model that emphasizes domains with significant likelihood differences.",
          "In DoReMi, a small reference model is trained using initial domain weights, and subsequently, a proxy model is trained to focus on domains with the highest likelihood discrepancies."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To improve the coding ability of LLMs, CodeLLaMA [254] is developed based on LLaMA 2 [99] (2T general tokens → 500B code-heavy tokens), aiming to improve the code generation ability and retain natural language understanding skills.",
    "context_before": [
      "Coding."
    ],
    "context_after": [
      "CodeLLaMA also provides a version that is further specialized to a certain programming language, namely CodeLLaMA-Python (2T general tokens → 500B code-heavy tokens → 100B Python-heavy tokens)."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "99",
        "main_query": "CodeLLaMA is developed based on LLaMA 2 (2T general tokens → 500B code-heavy tokens)",
        "rewritten_queries": [
          "CodeLLaMA builds on LLaMA 2 with a shift from 2T general tokens to 500B code-heavy tokens",
          "The development of CodeLLaMA is based on LLaMA 2, transitioning from 2T general tokens to 500B code-focused tokens",
          "To enhance coding capabilities, CodeLLaMA is created using LLaMA 2, increasing from 2T general tokens to 500B tokens focused on code"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Llemma [258] is proposed to enhance the mathematical capacities of general-purpose LLMs.",
    "context_before": [
      "Mathematics."
    ],
    "context_after": [
      "It is developed based on CodeLLaMA."
    ],
    "references": [
      "258"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "258",
        "main_query": "Llemma is proposed to enhance the mathematical capacities of general-purpose LLMs.",
        "rewritten_queries": [
          "Llemma aims to improve the mathematical abilities of general-purpose LLMs.",
          "The purpose of Llemma is to boost the mathematical skills of general-purpose LLMs.",
          "Llemma is designed to enhance the math capabilities of general-purpose LLMs."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Although CodeLLaMA [254] mainly focuses on the coding ability, experiments have shown that it performs better than its base model LLaMA 2 on mathematics benchmarks [258].",
    "context_before": [
      "It is developed based on CodeLLaMA."
    ],
    "context_after": [
      "Based on CodeLLaMA, Llemma is continually trained on mixtures of scientific papers, web data containing mathematical text and code (2T general tokens → 500B code-heavy tokens → 50∼200B math-heavy tokens)."
    ],
    "references": [
      "258"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "258",
        "main_query": "CodeLLaMA performs better than its base model LLaMA 2 on mathematics benchmarks",
        "rewritten_queries": [
          "CodeLLaMA outperforms LLaMA 2 in mathematics benchmarks",
          "In mathematics benchmarks, CodeLLaMA shows superior performance compared to LLaMA 2",
          "Experiments indicate that CodeLLaMA exceeds LLaMA 2 in mathematics performance"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Although Falcon [171] shows that webpages alone can be employed to train powerful LLMs, a more typical approach is to also incorporate diverse high-quality text like code, books, scientific papers,etc.",
    "context_before": [
      "It is suggested to include diverse data sources in the pre-training data."
    ],
    "context_after": [
      "If a LLM is specialized with a certain skill, the proportion of corresponding data source should be increased accordingly."
    ],
    "references": [
      "171"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "171",
        "main_query": "Falcon shows that webpages alone can be employed to train powerful LLMs",
        "rewritten_queries": [
          "Webpages can be used alone to train effective LLMs as demonstrated by Falcon",
          "According to Falcon, powerful LLMs can be trained using only webpages",
          "Falcon indicates that training powerful LLMs is possible with webpages alone"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "PaLM [44] and LaMDA [68] use approximately 50% conversational data.",
    "context_before": [
      "For example, Gopher and Chinchilla are trained with approximately 40% of data from books."
    ],
    "context_after": [
      "Data cleaning."
    ],
    "references": [
      "44"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "44",
        "main_query": "PaLM and LaMDA use approximately 50% conversational data",
        "rewritten_queries": [
          "PaLM and LaMDA are trained on about 50% conversational data",
          "Approximately half of the data used for PaLM and LaMDA consists of conversational content",
          "Around 50% of the training data for PaLM and LaMDA is derived from conversations"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In practice, both heuristic and classifier-based methods can be employed for quality and toxicity filtering ( e.g., CCNet [260], fastText [261], and Data-Juicer [262]).",
    "context_before": [
      "Second, low-quality text, toxic content, and data with privacy concerns should be removed at different granularities (e.g., document, passage or sentence)."
    ],
    "context_after": [
      "Third, with the cleaned data, one can further unify or specify the format for pretraining data, and perform the tokenization by training the tokenizer on the filtered and deduplicated corpus with libraries like SentencePiece ."
    ],
    "references": [
      "262"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "262",
        "main_query": "Data-Juicer",
        "rewritten_queries": [
          "Data-Juicer method for quality and toxicity filtering",
          "Quality and toxicity filtering using Data-Juicer",
          "Employing Data-Juicer for filtering low-quality and toxic content"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To determine both settings, a practical way is to first train several small language models with multiple candidate plans and then select a good plan among them [59].",
    "context_before": [
      "With the preprocessed data, the next step is to determine the data mixture and the specific order 22 of data for pre-training LLMs."
    ],
    "context_after": [
      "Overall, it is more difficult to find a suitable data curriculum."
    ],
    "references": [
      "59"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "59",
        "main_query": "train several small language models with multiple candidate plans and then select a good plan among them",
        "rewritten_queries": [
          "train multiple small language models and choose the best plan from them",
          "develop several small language models with different candidate plans and select an optimal one",
          "create various small language models and identify a suitable plan from the candidates"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Thus, several variants of SSM have been proposed, including Mamba [272], RetNet [271], RWKV [273], 23 TABLE 5: Model cards of several selected LLMs with public configuration details.",
    "context_before": [
      "Despite the high computation efficiency of SSMs, their performance still lags behind Transformer."
    ],
    "context_after": [
      "Here, PE denotes position embedding, #L denotes the number of layers, #H denotes the number of attention heads, dmodel denotes the size of hidden states, and MCL denotes the maximum context length during training."
    ],
    "references": [
      "273",
      "271"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "273",
        "main_query": "RWKV",
        "rewritten_queries": [
          "RWKV model variant",
          "RWKV architecture in SSM",
          "Details about RWKV in SSM variants"
        ]
      },
      {
        "related_to_reference": "271",
        "main_query": "RetNet",
        "rewritten_queries": [
          "RetNet model variant",
          "RetNet architecture in SSM",
          "Details about RetNet in SSM"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "RWKV .RWKV [273] combines the advantages of Transformer and RNN.",
    "context_before": [
      "Compared with traditional SSMs, Mamba has demonstrated improved text modeling capacities."
    ],
    "context_after": [
      "It employs time-mixing modules, i.e., RNN with gating, and channel-mixing modules that are special feedforward neural networks ."
    ],
    "references": [
      "273"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "273",
        "main_query": "RWKV combines the advantages of Transformer and RNN",
        "rewritten_queries": [
          "RWKV integrates the benefits of both Transformer and RNN architectures",
          "The RWKV model merges features from Transformer and RNN",
          "RWKV leverages the strengths of Transformer alongside RNN"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "It employs time-mixing modules, i.e., RNN with gating, and channel-mixing modules that are special feedforward neural networks [273].",
    "context_before": [
      "RWKV .RWKV combines the advantages of Transformer and RNN."
    ],
    "context_after": [
      "Within these modules, token shift, a linear combination of the current and previous token, is used instead of the token representation as the input."
    ],
    "references": [
      "273"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "273",
        "main_query": "channel-mixing modules that are special feedforward neural networks",
        "rewritten_queries": [
          "special feedforward neural networks used in channel-mixing modules",
          "feedforward neural networks that serve as channel-mixing modules",
          "channel-mixing modules consisting of unique feedforward neural networks"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "RetNet [271] proposes multi-scale retention (MSR) to replace the attention module in Transformer.",
    "context_before": [
      "RetNet."
    ],
    "context_after": [
      "Similar to linear attention, in the MSR module, the input is first mapped into query, key, and value, and the product of key and value is employed to update the state."
    ],
    "references": [
      "271"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "271",
        "main_query": "RetNet proposes multi-scale retention (MSR) to replace the attention module in Transformer.",
        "rewritten_queries": [
          "The multi-scale retention (MSR) method in RetNet replaces the attention module in Transformers.",
          "In Transformer architecture, RetNet introduces multi-scale retention (MSR) as a substitute for the attention module.",
          "RetNet's multi-scale retention (MSR) serves as an alternative to the traditional attention module in Transformers."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In existing LLMs, GeLU activations [289] are widely used.",
    "context_before": [
      "To obtain good performance, activation functions also need to be properly set in feed-forward networks."
    ],
    "context_after": [
      "Specially, in the latest LLMs ( e.g., PaLM and LaMDA), variants of GLU activation have also been utilized, especially the SwiGLU and GeGLU variants, which often achieve better performance in practice ."
    ],
    "references": [
      "289"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "289",
        "main_query": "GeLU activations are widely used in existing LLMs",
        "rewritten_queries": [
          "Existing LLMs commonly utilize GeLU activation functions",
          "GeLU activations are prevalent in current large language models",
          "In contemporary LLMs, the use of GeLU activations is widespread"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Configuration Method Equation Normalization position Post Norm [22] Norm(x+Sublayer(x)) Pre Norm [26] x + Sublayer(Norm(x)) Sandwich Norm [274] x + Norm(Sublayer(Norm(x))) Normalization method LayerNorm [275] x−µ σ · γ + β, µ = 1 d Pd i=1 xi, σ = q 1 d Pd i=1(xi − µ))2 RMSNorm [276] x RMS(x) · γ, RMS(x) = q 1 d Pd i=1 x2 i DeepNorm [277] LayerNorm(α · x + Sublayer(x)) Activation function ReLU [278] ReLU(x) = max(x, 0) GeLU [279] GeLU(x) = 0.5x ⊗ [1 + erf(x/ √ 2)], erf(x) = 2√π Rx 0 e−t2 dt Swish [280] Swish(x) = x ⊗ sigmoid(x) SwiGLU [281] SwiGLU(x1, x2) = Swish(x1) ⊗ x2 GeGLU [281] GeGLU(x1, x2) = GeLU(x1) ⊗ x2 Position embedding Absolute [22] xi = xi + pi Relative [82] Aij = WqxixT j WT k + ri−j RoPE [282] Aij = WqxiRΘ,i−jxT j WT k = (WqxiRΘ,i)(WkxjRΘ,j)T ALiBi [283] Aij = WqxixT j WT k − m(i − j) RoPE defines the basis θi as an exponentiation of the base b (set to 10000 by default): Θ = {θi = b−2(i−1)/d|i ∈ {1, 2, . . . , d/2}}. (4) Furthermore, a recent study [295] defines the distance required to rotate one cycle ( 2π) for each dimension as wavelength: λi = 2πb2(i−1)/d = 2π/θi. (5) Due to the excellent performance and the long-term decay property, RoPE is widely adopted in the latest LLMs, e.g., PaLM [56] and LLaMA [57].",
    "context_before": [
      "Here, Sublayer denotes a FFN or a self-attention module in a Transformer layer, d denotes the size of hidden states, pi denotes position embedding at position i, Aij denotes the attention score between a query and a key, ri−j denotes a learnable scalar based on the offset between the query and the key, and RΘ,t denotes a rotary matrix with rotation degree t · Θ."
    ],
    "context_after": [
      "Based on RoPE, xPos further improves the translation invariance and length extrapolation of Transformer."
    ],
    "references": [
      "280"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "280",
        "main_query": "Swish(x) = x ⊗ sigmoid(x)",
        "rewritten_queries": [
          "The Swish activation function is defined as Swish(x) = x multiplied by the sigmoid of x.",
          "Swish is an activation function represented by the equation Swish(x) = x * sigmoid(x).",
          "The formula for the Swish activation function is Swish(x) = x times sigmoid(x)."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To make a trade-off between multi-query attention and multi-head attention, grouped-query attention (GQA) [301] has been explored.",
    "context_before": [
      "Representative models with multi-query attention include PaLM and StarCoder ."
    ],
    "context_after": [
      "In GQA, heads are assigned into different groups, and those heads that belong to the same group will share the same transformation matrices."
    ],
    "references": [
      "301"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "301",
        "main_query": "grouped-query attention (GQA)",
        "rewritten_queries": [
          "GQA (grouped-query attention) has been explored for trade-offs",
          "The concept of grouped-query attention (GQA) is used to balance multi-query and multi-head attention",
          "Exploration of grouped-query attention (GQA) for trade-offs between attention types"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Specially, GQA has been adopted and empirically tested in the recently released LLaMA 2 model [99].",
    "context_before": [
      "In GQA, heads are assigned into different groups, and those heads that belong to the same group will share the same transformation matrices."
    ],
    "context_after": [
      "FlashAttention."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "99",
        "main_query": "GQA has been adopted and empirically tested in the recently released LLaMA 2 model",
        "rewritten_queries": [
          "The recently released LLaMA 2 model has adopted and tested GQA empirically",
          "Empirical testing of GQA has been conducted in the new LLaMA 2 model",
          "LLaMA 2 model incorporates GQA, which has been empirically tested"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "The updated version FlashAttention-2 [303] further optimizes the work partitioning of GPU thread blocks and warps, leading to around 2 × speedup when compared to the original FlashAttention.",
    "context_before": [
      "Implemented as a fused kernel in CUDA, FlashAttention has been integrated into 26 PyTorch , DeepSpeed , and Megatron-LM ."
    ],
    "context_after": [
      "PagedAttention."
    ],
    "references": [
      "303"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "303",
        "main_query": "FlashAttention-2 further optimizes the work partitioning of GPU thread blocks and warps, leading to around 2 × speedup when compared to the original FlashAttention.",
        "rewritten_queries": [
          "The updated FlashAttention-2 improves GPU thread block and warp work partitioning, achieving approximately 2 × speedup over the original FlashAttention.",
          "With FlashAttention-2, there is an optimization in GPU thread block and warp work partitioning that results in about 2 × speedup compared to the original version.",
          "FlashAttention-2 enhances the partitioning of GPU thread blocks and warps, resulting in nearly 2 × faster performance than the original FlashAttention."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "MoD has been applied in the latest PaLM 2 model [120]. 4.2.4 Decoding Strategy After the LLMs have been pre-trained, it is essential to employ a specific decoding strategy to generate the appropriate output from the LLMs.",
    "context_before": [
      "For input sentences started with different special tokens ( i.e., {[R], [S], [X]}), the model will be optimized using the corresponding denoisers."
    ],
    "context_after": [
      "Background."
    ],
    "references": [
      "120"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "120",
        "main_query": "MoD has been applied in the latest PaLM 2 model",
        "rewritten_queries": [
          "The latest PaLM 2 model utilizes MoD",
          "Application of MoD in the recent PaLM 2 model",
          "MoD implementation in the newest version of the PaLM 2 model"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Additionally, Dolly [185] and OpenAssistant [186] have further released their conversation data, which has been carefully labeled by human annotators to attain a high level of quality.",
    "context_before": [
      "A notable example of such a dataset is the conversational data from ShareGPT ."
    ],
    "context_after": [
      "Formatting Synthetic Data."
    ],
    "references": [
      "186"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "186",
        "main_query": "OpenAssistant has further released their conversation data, which has been carefully labeled by human annotators to attain a high level of quality.",
        "rewritten_queries": [
          "OpenAssistant's conversation data has been meticulously labeled by human annotators for high quality.",
          "The conversation data released by OpenAssistant is labeled by human annotators to ensure quality.",
          "OpenAssistant has published conversation data that is carefully annotated by humans to achieve high quality."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Furthermore, Self-Align [336] establishes multiple human-aligned principles to filter the synthesized instances.",
    "context_before": [
      "To improve the quality of synthetic instructions, WizardLM introduces Evol-Instruct by proposing in-depth and in-breadth evolving to enrich the complexity and diversity of the instances."
    ],
    "context_after": [
      "It then employs these instances to train a LLM in order to yield more aligned instances."
    ],
    "references": [
      "336"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "336",
        "main_query": "Self-Align establishes multiple human-aligned principles to filter the synthesized instances.",
        "rewritten_queries": [
          "Self-Align implements various principles aligned with human values to refine the generated instances.",
          "The principles of human alignment established by Self-Align are used to filter the synthesized instances.",
          "Self-Align utilizes multiple human-aligned guidelines to enhance the filtering of synthesized instances."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To summarize, diversity and quality of instructions are important factors to consider when scaling the number of instances [338].",
    "context_before": [
      "LESS computes gradients for both downstream validation and training instruction data, to evaluate the contribution of instruction data based on extensions of influence function ."
    ],
    "context_after": [
      "As the capacities of LLMs improve, data synthesis methods have become the mainstream approach for generating large amount of instruction data."
    ],
    "references": [
      "338"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "338",
        "main_query": "diversity and quality of instructions are important factors to consider when scaling the number of instances",
        "rewritten_queries": [
          "importance of instruction diversity and quality in scaling instances",
          "scaling the number of instances requires considering instruction diversity and quality",
          "factors to consider for scaling instances include the diversity and quality of instructions"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition to the above practical strategies and tricks, existing work has also used other tricks, e.g., concatenating multiple examples into a single sequence to approach the max length [353]. 34 5.1.3 The Effect of Instruction Tuning In this part, we discuss the effect of instruction tuning on LLMs in three major aspects.",
    "context_before": [
      "It is also feasible to prefix the input with the selfidentification prompt, e.g., “The following is a conversation between a human and an AI assistant called CHATBOT NAME , developed by DEVELOPER .”, where C HATBOT NAME and D EVELOPER refer to the name and developer of the chatbot, respectively."
    ],
    "context_after": [
      "Performance Improvement."
    ],
    "references": [
      "353"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "353",
        "main_query": "concatenating multiple examples into a single sequence to approach the max length",
        "rewritten_queries": [
          "combining several examples into one sequence to reach the maximum length",
          "merging multiple examples into a single input sequence to utilize the full length",
          "using a single sequence that includes multiple examples to maximize length"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For instance, researchers propose to fine-tune Flan-PaLM [69] using medical datasets to create Med-PaLM [354], a medical knowledge assistant that achieves performance levels comparable to those of expert clinicians.",
    "context_before": [
      "Instruction tuning is an effective approach to adapting existing general LLMs to be domain-specific experts."
    ],
    "context_after": [
      "Furthermore, a recent study fine-tunes FLAN-T5 to support e-commerce recommender systems with natural language instructions, showing strong performance in a variety of recommendation tasks."
    ],
    "references": [
      "354"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "354",
        "main_query": "Med-PaLM, a medical knowledge assistant that achieves performance levels comparable to those of expert clinicians",
        "rewritten_queries": [
          "fine-tuning Flan-PaLM using medical datasets to create a medical knowledge assistant",
          "Med-PaLM achieves performance similar to expert clinicians through fine-tuning",
          "researchers developed Med-PaLM by fine-tuning Flan-PaLM with medical data"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "There are also several open-sourced medical models instructiontuned based on LLaMA [57], such as BenTsao [356].",
    "context_before": [
      "Furthermore, a recent study fine-tunes FLAN-T5 to support e-commerce recommender systems with natural language instructions, showing strong performance in a variety of recommendation tasks."
    ],
    "context_after": [
      "Also, researchers explore instruction tuning on law , finance , and arithmetic computation . 5.1.4 Empirical Analysis for Instruction Tuning Fine-tuning LLMs with different instruction sets tend to lead to model variants with varied performance on downstream tasks."
    ],
    "references": [
      "356"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "356",
        "main_query": "BenTsao",
        "rewritten_queries": [
          "open-sourced medical models based on LLaMA",
          "instruction-tuned models like BenTsao",
          "medical models utilizing LLaMA for instruction tuning"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Also, researchers explore instruction tuning on law [357], finance [358], and arithmetic computation [359]. 5.1.4 Empirical Analysis for Instruction Tuning Fine-tuning LLMs with different instruction sets tend to lead to model variants with varied performance on downstream tasks.",
    "context_before": [
      "There are also several open-sourced medical models instructiontuned based on LLaMA , such as BenTsao ."
    ],
    "context_after": [
      "In this section, we will explore the effect of different types of instructions in fine-tuning LLMs ( i.e., LLaMA (7B) and LLaMA (13B) 25), as well as examine the usefulness of several instruction improvement strategies."
    ],
    "references": [
      "358",
      "359",
      "357"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "358",
        "main_query": "finance",
        "rewritten_queries": [
          "financial instruction tuning",
          "instruction tuning in finance",
          "exploring finance in instruction tuning"
        ]
      },
      {
        "related_to_reference": "359",
        "main_query": "arithmetic computation",
        "rewritten_queries": [
          "computation in arithmetic",
          "arithmetic tasks in instruction tuning",
          "instruction tuning for arithmetic operations"
        ]
      },
      {
        "related_to_reference": "357",
        "main_query": "instruction tuning on law",
        "rewritten_queries": [
          "exploring instruction tuning in legal contexts",
          "instruction tuning applications in law",
          "the role of instruction tuning in legal studies"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition to the complexity, improving the topic diversity of the instruction dataset can help elicit different abilities of LLMs on diverse tasks in real world [336].",
    "context_before": [
      "Increasing the topic diversity."
    ],
    "context_after": [
      "However, it is difficult to directly control the self-instruct process for generating diverse instructions."
    ],
    "references": [
      "336"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "336",
        "main_query": "improving the topic diversity of the instruction dataset can help elicit different abilities of LLMs on diverse tasks in real world",
        "rewritten_queries": [
          "enhancing topic diversity in instruction datasets can reveal various capabilities of LLMs across different tasks",
          "increasing the diversity of topics in instruction datasets aids in showcasing the different skills of LLMs on various real-world tasks",
          "boosting the variety of topics in instruction datasets can facilitate the demonstration of LLMs' abilities on a range of tasks"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "RLHF employs reinforcement learning (RL) algorithms ( e.g., Proximal Policy Optimization (PPO) [128]) to adapt LLMs to human feedback by learning a reward model.",
    "context_before": [
      "As discussed below, the alignment criteria introduced in Section 5.2.1 can be fulfilled by learning from human feedback on the responses of LLMs to users’ queries. 5.2.3 Reinforcement Learning from Human Feedback To align LLMs with human values, reinforcement learning from human feedback (RLHF) has been proposed to fine-tune LLMs with the collected human feedback data, which is useful to improve the alignment criteria ( e.g., helpfulness, honesty, and harmlessness)."
    ],
    "context_after": [
      "Such an approach incorporates humans in the training loop for developing well-aligned LLMs, as exemplified by InstructGPT ."
    ],
    "references": [
      "128"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "128",
        "main_query": "Proximal Policy Optimization (PPO)",
        "rewritten_queries": [
          "PPO in reinforcement learning for adapting LLMs to human feedback",
          "Using Proximal Policy Optimization to align LLMs with human values",
          "Reinforcement learning algorithm Proximal Policy Optimization for LLM adaptation"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Despite that InstructGPT used a small reward model (6B GPT model), increasing work [99] has shown it is often more effective to use a large reward model ( e.g., equal or greater than the original model size), since large reward models generally perform better in judging the quality of the LLM generated outputs.",
    "context_before": [
      "Effective reward model training."
    ],
    "context_after": [
      "In LLaMa 2 , pretrained chat model checkpoints are used to initialize the reward model, they argue that such an approach can effectively reduce the information mismatch between the model to be aligned and the reward model by sharing the same pre-training knowledge."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "99",
        "main_query": "it is often more effective to use a large reward model (e.g., equal or greater than the original model size), since large reward models generally perform better in judging the quality of the LLM generated outputs",
        "rewritten_queries": [
          "using a larger reward model tends to yield better performance in evaluating LLM output quality",
          "large reward models, particularly those equal to or larger than the original model, are more effective in assessing the quality of outputs from LLMs",
          "the effectiveness of reward models increases with size, as larger models are superior in judging LLM-generated content"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In LLaMa 2 [99], pretrained chat model checkpoints are used to initialize the reward model, they argue that such an approach can effectively reduce the information mismatch between the model to be aligned and the reward model by sharing the same pre-training knowledge.",
    "context_before": [
      "Despite that InstructGPT used a small reward model (6B GPT model), increasing work has shown it is often more effective to use a large reward model ( e.g., equal or greater than the original model size), since large reward models generally perform better in judging the quality of the LLM generated outputs."
    ],
    "context_after": [
      "Whereas, it is common to encounter the overfitting problem when training large-scale reward models."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "99",
        "main_query": "In LLaMa 2, pretrained chat model checkpoints are used to initialize the reward model, they argue that such an approach can effectively reduce the information mismatch between the model to be aligned and the reward model by sharing the same pre-training knowledge.",
        "rewritten_queries": [
          "Pretrained chat model checkpoints in LLaMa 2 are utilized to initialize the reward model, which helps in minimizing information mismatch by sharing pre-training knowledge.",
          "LLaMa 2 employs pretrained chat model checkpoints for initializing the reward model, effectively reducing information mismatch through shared pre-training knowledge.",
          "The use of pretrained chat model checkpoints in LLaMa 2 to initialize the reward model is argued to effectively decrease information mismatch by leveraging the same pre-training knowledge."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Therefore, it is useful to train multiple reward models that focus on different alignment criteria [99], and compute the final reward based on the produced ones from them via special combination strategies (e.g., mean pooling and weighted sum).",
    "context_before": [
      "In addition, as there are multiple criteria for alignment ( e.g., helpfulness and honesty), it is often difficult to train a single reward model that can satisfy all the alignment criteria."
    ],
    "context_after": [
      "Such a way enables more flexible rules or standards on multiple criteria, e.g., relaxing the requirement on helpfulness while posing more strict limits on harmfulness."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "99",
        "main_query": "it is useful to train multiple reward models that focus on different alignment criteria",
        "rewritten_queries": [
          "training multiple reward models for various alignment criteria is beneficial",
          "using several reward models that target different alignment criteria is advantageous",
          "developing multiple reward models that address distinct alignment criteria is useful"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "LLaMA 2 [99] has successively trained five versions of RLHF models, where the LLM has been progressively improved with the improvement of the reward models.",
    "context_before": [
      "After fine-tuning the LLM on the best samples until convergence, the RL process will be performed to further improve the performance."
    ],
    "context_after": [
      "In this way, the collected prompts and annotations of human preference data can better reflect the issues of the current model checkpoint, thus making special tuning to address these issues."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "99",
        "main_query": "LLaMA 2 has successively trained five versions of RLHF models, where the LLM has been progressively improved with the improvement of the reward models.",
        "rewritten_queries": [
          "LLaMA 2 has trained five iterations of RLHF models, progressively enhancing the LLM alongside the reward models.",
          "The five versions of RLHF models trained by LLaMA 2 show progressive improvements in the LLM as the reward models advance.",
          "LLaMA 2's training of five RLHF model versions has led to continuous enhancements in the LLM through improved reward models."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "As an alternative, increasing studies explore to directly optimize LLMs to adhere to human preferences, using supervised fine-tuning without reinforcement learning [338].",
    "context_before": [
      "Besides, the commonly-used PPO algorithm in RLHF is rather complex and often sensitive to hyper-parameters."
    ],
    "context_after": [
      "Overview."
    ],
    "references": [
      "338"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "338",
        "main_query": "supervised fine-tuning without reinforcement learning",
        "rewritten_queries": [
          "directly optimizing LLMs to align with human preferences through supervised fine-tuning",
          "using supervised fine-tuning methods to ensure LLMs meet human preferences instead of reinforcement learning",
          "exploring supervised fine-tuning as a way to optimize LLMs for human preference adherence"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For the first issue, the alignment dataset can be automatically constructed by an aligned LLMs according to human-written safety principles [336] or refining existing examples using edits operations [385].",
    "context_before": [
      "Thus, to implement this approach, two key issues are the construction of alignment dataset and the design of fine-tuning loss."
    ],
    "context_after": [
      "In addition, we can also reuse existing reward models to select highrated responses from existing human feedback data ."
    ],
    "references": [
      "336"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "336",
        "main_query": "an aligned LLMs according to human-written safety principles",
        "rewritten_queries": [
          "alignment dataset constructed by aligned LLMs based on human safety principles",
          "automatically creating an alignment dataset using LLMs aligned with human safety guidelines",
          "using aligned LLMs to build an alignment dataset following human-written safety principles"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Similarly, Self-Align [336] first adopts self-instruct [147] to generate instructions focusing on covering diverse topics.",
    "context_before": [
      "Based on these principles, LLMs will critique their own harmful responses and revise them repeatedly into finally aligned responses."
    ],
    "context_after": [
      "Then, the model is also prompted with multiple human-written principles that describe the rules of expected model behaviors (also with several incontext exemplars), to generate helpful, ethical, and reliable responses as alignment data."
    ],
    "references": [
      "336"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "336",
        "main_query": "Self-Align first adopts self-instruct to generate instructions focusing on covering diverse topics.",
        "rewritten_queries": [
          "Self-Align utilizes self-instruct to create diverse topic instructions.",
          "The method of Self-Align involves using self-instruct to generate instructions that address a variety of topics.",
          "Self-Align employs self-instruct for generating instructions that emphasize a wide range of topics."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To mitigate the limit that the original SFT method can only learn from positive responses, FIGA [388] develops an improved supervised alignment approach, where both negative (the original output of low quality) and positive (the refined output by LLMs) responses are leveraged in a contrastive way, to enable LLMs to deeply understand what fine-grained revisions actually lead to good response.",
    "context_before": [
      "Then, the model is also prompted with multiple human-written principles that describe the rules of expected model behaviors (also with several incontext exemplars), to generate helpful, ethical, and reliable responses as alignment data."
    ],
    "context_after": [
      "LLM based interactive approaches."
    ],
    "references": [
      "388"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "388",
        "main_query": "FIGA develops an improved supervised alignment approach, where both negative (the original output of low quality) and positive (the refined output by LLMs) responses are leveraged in a contrastive way.",
        "rewritten_queries": [
          "FIGA introduces a new supervised alignment method that utilizes both low-quality negative outputs and high-quality positive outputs in a contrastive manner.",
          "The improved supervised alignment approach by FIGA incorporates both negative and positive responses to enhance LLMs' understanding of effective revisions.",
          "To overcome the limitations of the original SFT method, FIGA's approach contrasts negative and positive responses for better alignment in LLMs."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Furthermore, FIGA [388] designs a token-level contrastive loss that aims to encourage desirable tokens, penalize undesirable ones, and disregard trivial tokens.",
    "context_before": [
      "Based on DPO, existing work has proposed several improvement strategies for enhancing the effectiveness or efficiency, e.g., decomposing the optimization of positive responses and negative responses into two independent components or removing the probability of the reference model in the objective function ."
    ],
    "context_after": [
      "Despite the effectiveness, recent work has also revealed that DPO may have inherent limitations in several aspects."
    ],
    "references": [
      "388"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "388",
        "main_query": "FIGA designs a token-level contrastive loss that aims to encourage desirable tokens, penalize undesirable ones, and disregard trivial tokens.",
        "rewritten_queries": [
          "The token-level contrastive loss proposed by FIGA focuses on promoting desirable tokens while penalizing undesirable ones and ignoring trivial tokens.",
          "FIGA introduces a contrastive loss at the token level to incentivize desirable tokens and penalize those that are undesirable, while trivial tokens are disregarded.",
          "The design of a token-level contrastive loss by FIGA seeks to enhance desirable tokens, impose penalties on undesirable tokens, and overlook trivial tokens."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Thus, high-quality instruction data (but not the quantity) is the primary factor for effective training of LLMs during the SFT stage [99].",
    "context_before": [
      "However, there often exist variations among different annotators on the writing styles, quality, and preferences of demonstration data, which tends to affect the learning performance of SFT."
    ],
    "context_after": [
      "Pros and Cons of RLHF ."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "99",
        "main_query": "high-quality instruction data is the primary factor for effective training of LLMs during the SFT stage",
        "rewritten_queries": [
          "the quality of instruction data is crucial for effective LLM training in the SFT phase",
          "effective training of LLMs during SFT relies primarily on high-quality instruction data",
          "for SFT stage training of LLMs, the quality of instruction data is more important than the quantity"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Specially, LLaMA 2 has demonstrated that RLHF can improve both the helpfulness and harmlessness scores [99], and attributed this to a better human-LLM synergy for data annotation.",
    "context_before": [
      "Recently, increasing evidence has demonstrated the effectiveness of RLHF in mitigating the harmful responses and enhancing the model capacity."
    ],
    "context_after": [
      "They explain this reason in two major aspects as follows."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "99",
        "main_query": "RLHF can improve both the helpfulness and harmlessness scores",
        "rewritten_queries": [
          "RLHF enhances helpfulness and harmlessness in models",
          "The impact of RLHF on helpfulness and harmlessness scores",
          "Improvements in helpfulness and harmlessness due to RLHF"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For example, LLaMA 2 adopts pre-trained chat model checkpoints to initialize reward models [99]. be optimized according to the specific task goals, while the parameters of the original language model are frozen in this process.",
    "context_before": [
      "In RLHF, it seems to be also important that reward models should be aware of the knowledge or ability of a LLM to be aligned."
    ],
    "context_after": [
      "In this way, we can effectively reduce the number of trainable parameters during fine-tuning."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "99",
        "main_query": "LLaMA 2 adopts pre-trained chat model checkpoints to initialize reward models",
        "rewritten_queries": [
          "LLaMA 2 uses pre-trained chat model checkpoints for initializing reward models",
          "The initialization of reward models in LLaMA 2 is done using pre-trained chat model checkpoints",
          "Pre-trained chat model checkpoints are utilized by LLaMA 2 to set up reward models"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Both the quality of the retrieved documents and their relevance to the question have an impact on the generated answers [454].",
    "context_before": [
      "For example, retrieved documents are highly useful for open-domain question answering as supporting evidence."
    ],
    "context_after": [
      "Thus, it needs to include such information in a proper prompt pattern or expression format."
    ],
    "references": [
      "454"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "454",
        "main_query": "the quality of the retrieved documents and their relevance to the question have an impact on the generated answers",
        "rewritten_queries": [
          "how the quality and relevance of retrieved documents affect generated answers",
          "the influence of document quality and relevance on answer generation",
          "the impact of retrieved document quality and relevance on the answers produced"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Another study [501] also supports this finding with specially designed experiments.",
    "context_before": [
      "As shown in the experiments , the ability of task recognition is easier to obtain, and even a small LM with only 350M parameters can exhibit this ability, while task learning can only emerge for LLMs with at least 66B parameters."
    ],
    "context_after": [
      "They set up the tasks with flipped and semantically unrelated labels in the experiment, which require task learning when performing ICL."
    ],
    "references": [
      "501"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "501",
        "main_query": "Another study also supports this finding with specially designed experiments.",
        "rewritten_queries": [
          "A different research also corroborates this conclusion through specially designed experiments.",
          "Another research study backs this result with experiments that were specifically designed.",
          "An additional study reinforces this finding using experiments that were tailored for the purpose."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To reduce potentially meaningless thought exploration, XoT [521] further proposes to guide the search of thoughts with pre-trained policy and value networks. 6.3.3 Further Discussion on CoT Prompting In this part, we present discussions regarding two fundamental questions related to CoT prompting, i.e., “when does CoT prompting work for LLMs ” and “ why can LLMs perform CoT reasoning”.",
    "context_before": [
      "However, such an approach requires a large number of interactions with LLMs, making the thought exploration process highly inefficient."
    ],
    "context_after": [
      "When CoT Prompting Works For LLMs?"
    ],
    "references": [
      "521"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "521",
        "main_query": "XoT further proposes to guide the search of thoughts with pre-trained policy and value networks",
        "rewritten_queries": [
          "XoT suggests using pre-trained policy and value networks to direct thought exploration",
          "To enhance thought exploration, XoT recommends guiding it with pre-trained policy and value networks",
          "The proposal by XoT involves utilizing pre-trained policy and value networks to steer thought searches"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Furthermore, environment refers to where the plan executor carries out the actions, which can be set differently according to specific tasks, e.g., the LLM itself [529] or an external virtual world like Minecraft [530].",
    "context_before": [
      "It can be implemented by models like LLMs for textual tasks or by tools like code interpreters for coding tasks ."
    ],
    "context_after": [
      "It provides feedback about the execution result of the action to the task planner, either in the form of natural language or from other multimodal signals ."
    ],
    "references": [
      "530"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "530",
        "main_query": "an external virtual world like Minecraft",
        "rewritten_queries": [
          "a virtual environment such as Minecraft",
          "the Minecraft virtual world",
          "an external environment like Minecraft"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "HuggingGPT [437] introduces the models available in HuggingFace and regards LLMs as the controller to select suitable models based on their descriptions and aggregate their results as the final solution.",
    "context_before": [
      "For example, ToolFormer first annotates a pre-training corpus with potential API calls using LLMs, and then fine-tunes LLMs on it, so that LLMs can learn when and how to call APIs and incorporate the results returned by APIs during generation."
    ],
    "context_after": [
      "Code-based Approaches."
    ],
    "references": [
      "437"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "437",
        "main_query": "HuggingGPT introduces the models available in HuggingFace and regards LLMs as the controller to select suitable models based on their descriptions and aggregate their results as the final solution.",
        "rewritten_queries": [
          "HuggingGPT presents the models from HuggingFace and uses LLMs to choose appropriate models according to their descriptions and combine their outputs into a final solution.",
          "HuggingGPT outlines the models offered by HuggingFace, treating LLMs as the decision-makers for selecting the right models based on descriptions and merging their results.",
          "HuggingGPT details the available models in HuggingFace, viewing LLMs as the mechanism to identify suitable models based on their descriptions and consolidate their results into a final answer."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For example, tools like code interpreters are widely used in programming tasks to provide real-time error messages [443], models like stable diffusion [534] can be used in multimodal tasks to provide visual perception [439], and virtual worlds like Minecraft can provide immersive experiences [530].",
    "context_before": [
      "In addition to LLMs, external objects can also provide feedback signals."
    ],
    "context_after": [
      "Besides, some work ( e.g., Generative Agents ) explores multi-agent collaboration in simulated environments, where each agent receives feedback not only from interaction with the environment but also from communication with other agents. 6.4.4 Plan Refinement With access to feedback from the environment, the task planner can accordingly refine its current plan and iteratively go through the “planning – execution – refinement” loop for better results."
    ],
    "references": [
      "530"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "530",
        "main_query": "virtual worlds like Minecraft can provide immersive experiences",
        "rewritten_queries": [
          "Minecraft as a virtual world offers immersive experiences",
          "Immersive experiences can be provided by virtual environments such as Minecraft",
          "The game Minecraft serves as a virtual world that delivers immersive experiences"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For instance, GPT-4 exhibits comparable performance as commercial translation products, even for the translation task of languages that are with significant linguistic distance [629].",
    "context_before": [
      "Due to the powerful language generation capabilities, LLMs have achieved remarkable performance on existing datasets and benchmarks."
    ],
    "context_after": [
      "On news summarization tasks ( i.e., CNN/DM and XSUM), LLMs also demonstrate comparable performance with human freelance writers ."
    ],
    "references": [
      "629"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "629",
        "main_query": "GPT-4 exhibits comparable performance as commercial translation products, even for the translation task of languages that are with significant linguistic distance",
        "rewritten_queries": [
          "GPT-4 performs similarly to commercial translation tools for linguistically distant languages",
          "The performance of GPT-4 is on par with that of commercial translation services, even for languages that differ significantly",
          "In translating languages with considerable linguistic differences, GPT-4 shows performance comparable to that of commercial translation products"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To improve such an ability, it is key to fine-tuning (or pre-training) LLMs on code data, which can effectively adapt LLMs to code synthesis tasks [86].",
    "context_before": [
      "Typically, they consist of diverse programming problems, with text specification and test cases for correctness checking."
    ],
    "context_after": [
      "In addition, existing work has proposed new strategies to generate code, e.g., sampling multiple candidate solutions and planning-guided decoding , which can be considered as the imitation of bug-fixing and code-planning processes by programmers."
    ],
    "references": [
      "86"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "86",
        "main_query": "fine-tuning (or pre-training) LLMs on code data, which can effectively adapt LLMs to code synthesis tasks",
        "rewritten_queries": [
          "adapting LLMs to code synthesis tasks through fine-tuning or pre-training on code data",
          "the importance of fine-tuning LLMs with code data for effective code synthesis",
          "using code data for pre-training LLMs to enhance their ability in code synthesis tasks"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For instance, considering that hallucinated facts are prone to exhibit inconsistency across different sampled outputs, SelfCheckGPT [666] detects hallucination by measuring information inconsistency within sampled outputs.",
    "context_before": [
      "Another line of research work leverages uncertainty estimation of LLMs to identify hallucinations ."
    ],
    "context_after": [
      "For the evaluation of the hallucination problem, a set of hallucination detection tasks have been proposed, e.g., TruthfulQA for detecting human falsehood mimicked by models."
    ],
    "references": [
      "666"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "666",
        "main_query": "SelfCheckGPT detects hallucination by measuring information inconsistency within sampled outputs.",
        "rewritten_queries": [
          "SelfCheckGPT identifies hallucinations through the assessment of information inconsistency in outputs.",
          "The detection of hallucinations by SelfCheckGPT is based on measuring inconsistencies in sampled outputs.",
          "SelfCheckGPT measures the inconsistency of information in outputs to detect hallucinated facts."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For instance, with a subword tokenizer the integer 7481 may be tokenized as 7 481, while 74815 may be tokenized as 748 15 (the same numerical substrings with different splits) [359].",
    "context_before": [
      "One possible explanation is that subword tokenization techniques can result in inconsistent sequences when tokenizing numbers."
    ],
    "context_after": [
      "As a comparison, digit-based tokenization for numbers can avoid such an inconsistency, thus likely improving the numerical computation ability of LLMs."
    ],
    "references": [
      "359"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "359",
        "main_query": "with a subword tokenizer the integer 7481 may be tokenized as 7 481, while 74815 may be tokenized as 748 15",
        "rewritten_queries": [
          "subword tokenization can split the number 7481 into 7 and 481, while 74815 can be split into 748 and 15",
          "the integer 7481 can be tokenized as 7 481 using subword tokenization, whereas 74815 can be tokenized as 748 15",
          "using a subword tokenizer, the number 7481 is represented as 7 481, in contrast to 74815 which is represented as 748 15"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Specialized LLMs refer to the model checkpoints specially adapted to some domains or applications like healthcare [354] and finance [739].",
    "context_before": [
      "Evaluation of Specialized LLMs."
    ],
    "context_after": [
      "As special task solvers, specialized LLMs will be tested not only on general abilities ( e.g., basic ability like complex reasoning and advanced ability like human alignment), but also on specific abilities related to their designated domains or applications."
    ],
    "references": [
      "354"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "354",
        "main_query": "healthcare",
        "rewritten_queries": [
          "domain-specific applications in healthcare",
          "healthcare-focused model adaptations",
          "LLMs tailored for healthcare applications"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For example, MultiMedQA [354] is a specific benchmark in healthcare, which includes medical examinations and healthcare questions.",
    "context_before": [
      "Then, these domain-specific benchmarks can be combined with general benchmarks to conduct both comprehensive and targeted evaluation for specialized LLMs."
    ],
    "context_after": [
      "In this work , MultiMedQA has been combined with MMLU to assess the performance of specialized LLMs for healthcare, such as Med-PaLM ."
    ],
    "references": [
      "354"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "354",
        "main_query": "MultiMedQA is a specific benchmark in healthcare, which includes medical examinations and healthcare questions.",
        "rewritten_queries": [
          "MultiMedQA serves as a benchmark in the healthcare field, encompassing medical examinations and healthcare-related questions.",
          "The MultiMedQA benchmark focuses on healthcare, featuring medical exams and questions pertinent to healthcare.",
          "In the healthcare domain, MultiMedQA is a benchmark that includes both medical examinations and healthcare questions."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In this work [354], MultiMedQA has been combined with MMLU [362] to assess the performance of specialized LLMs for healthcare, such as Med-PaLM [354].",
    "context_before": [
      "For example, MultiMedQA is a specific benchmark in healthcare, which includes medical examinations and healthcare questions."
    ],
    "context_after": [
      "Similarly, FLUE constructs a benchmark for finance, spanning from financial sentiment analysis to question answering."
    ],
    "references": [
      "354"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "354",
        "main_query": "MultiMedQA has been combined with MMLU to assess the performance of specialized LLMs for healthcare, such as Med-PaLM",
        "rewritten_queries": [
          "The combination of MultiMedQA and MMLU evaluates specialized LLMs in healthcare like Med-PaLM",
          "Assessing specialized LLMs for healthcare, such as Med-PaLM, involves combining MultiMedQA with MMLU",
          "MultiMedQA and MMLU are used together to measure the effectiveness of healthcare-focused LLMs, including Med-PaLM"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "It has been used collaboratively with BBH [363] to evaluate finical LLMs like BloombergGPT [358].",
    "context_before": [
      "Similarly, FLUE constructs a benchmark for finance, spanning from financial sentiment analysis to question answering."
    ],
    "context_after": [
      "Pros and Cons of Different Evaluation Approaches ."
    ],
    "references": [
      "358"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "358",
        "main_query": "BBH",
        "rewritten_queries": [
          "collaborative use of BBH",
          "BBH in evaluating financial LLMs",
          "partnership with BBH for financial LLM evaluation"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In our evaluation, we select four representative base models including LLaMA (7B) [57], LLaMA 2 (7B) [99], Pythia (7B and 12B) [96], and Falcon (7B) [749].",
    "context_before": [
      "Base models are only pre-trained on a large general-purpose corpus with the language modeling objective, but without further supervised fine-tuning."
    ],
    "context_after": [
      "Instruction-tuned models are those fine-tuned using instructions ( i.e., task datasets, daily chat, or synthetic instructions)."
    ],
    "references": [
      "99",
      "96"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "99",
        "main_query": "LLaMA 2 (7B)",
        "rewritten_queries": [
          "LLaMA 2 model with 7 billion parameters",
          "7B version of LLaMA 2",
          "LLaMA 2 architecture with 7 billion parameters"
        ]
      },
      {
        "related_to_reference": "96",
        "main_query": "Pythia (7B and 12B)",
        "rewritten_queries": [
          "Pythia model with 7B and 12B parameters",
          "Pythia architecture available in 7B and 12B sizes",
          "Pythia (7B and 12B) model specifications"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition, we also include LLaMA 2-Chat (7B) [99] for comparison, and it is a representative model that has been aligned with human via instruction tuning and RLHF, based on LLaMA 2 (7B).",
    "context_before": [
      "In our experiments, we select four representative instruction-tuned models including Vicuna (7B and 13B) , Alpaca (7B) , and ChatGLM (6B) ."
    ],
    "context_after": [
      "Closed-source models."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "99",
        "main_query": "LLaMA 2-Chat (7B) is a representative model that has been aligned with human via instruction tuning and RLHF, based on LLaMA 2 (7B)",
        "rewritten_queries": [
          "LLaMA 2-Chat (7B) is aligned with human instructions through instruction tuning and RLHF, derived from LLaMA 2 (7B)",
          "The model LLaMA 2-Chat (7B) has been instruction-tuned and aligned with human feedback, based on LLaMA 2 (7B)",
          "We include LLaMA 2-Chat (7B), which is a model aligned with human instructions via RLHF and instruction tuning, based on LLaMA 2 (7B)"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "You should follow the examples and generate the final answer without external solution or words. 66.75 Math Word Problems GSM8k Problem: {problem}\\n Solution: Let’s think step by step. 78.47 63.20 [744]Let’s use python to solve math problems.",
    "context_before": [
      "You can use the knowledge in examples and solve the last problem."
    ],
    "context_after": [
      "Here are three examples how to do it,\\n Q: Olivia has $."
    ],
    "references": [
      "744"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "744",
        "main_query": "Let’s use python to solve math problems.",
        "rewritten_queries": [
          "Using python for solving math problems.",
          "Employing python to tackle math problems.",
          "Utilizing python to address math problems."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "The information extraction task focuses on automatically extracting useful structured information from unstructured text data, such as relation extraction [761] and event extraction [762], which is also a crucial task relating to many NLP applications.",
    "context_before": [
      "Information Extraction."
    ],
    "context_after": [
      "Typically, previous studies formulate this task as a text classification task or a sequential labeling task."
    ],
    "references": [
      "761"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "761",
        "main_query": "relation extraction",
        "rewritten_queries": [
          "extracting relationships from unstructured text",
          "automatically identifying relations in text data",
          "relation extraction in information extraction tasks"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition, a recent study [766] also reveals that LLMs can achieve competitive zero-shot performance for information extraction with a two-stage workflow, making this approach attractive in future applications.",
    "context_before": [
      "Whereas, it is shown that enabling collaboration between LLMs and small models can further boost the performance of specific tasks ."
    ],
    "context_after": [
      "Text Generation."
    ],
    "references": [
      "766"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "766",
        "main_query": "LLMs can achieve competitive zero-shot performance for information extraction with a two-stage workflow",
        "rewritten_queries": [
          "Recent findings indicate that LLMs perform well in zero-shot information extraction using a two-stage process",
          "A study shows that a two-stage workflow allows LLMs to excel in zero-shot information extraction",
          "The two-stage workflow enables LLMs to attain strong zero-shot performance in information extraction"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Additionally, LLMs are flexible to effectively handle special requirement in real-world application scenarios, e.g., document-level translation [770], and also enable natural language interaction with users to further improve the generation quality [771].",
    "context_before": [
      "Since the pre-training of LLMs is established on text prediction, they exhibit strong language generation abilities as commercial products and humans , with the help of proper prompts ."
    ],
    "context_after": [
      "Despite the above 71 LLM for Application Research Directions Specific Domains Scientific ResearchFinance Law EducationHealthcare LLM for Evaluation LLM-based Agent KG Enhanced LLM Multimodal LLMs."
    ],
    "references": [
      "771",
      "770"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "771",
        "main_query": "enable natural language interaction with users to further improve the generation quality",
        "rewritten_queries": [
          "facilitate user interaction in natural language to enhance generation quality",
          "allow users to interact in natural language, improving the quality of generated content",
          "support natural language communication with users to boost generation quality"
        ]
      },
      {
        "related_to_reference": "770",
        "main_query": "document-level translation",
        "rewritten_queries": [
          "translation at the document level",
          "translating documents comprehensively",
          "handling translation for entire documents"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Text Generation Classic Scenarios Enhanced Capabilities New Scenarios Fig. 18: The applications of LLMs in representative research directions and downstream domains. success, recent work also reveals that LLMs are hard to well address the generation tasks about low-resource languages and domains, e.g., Marathi-to-English translation [772], due to their unbalanced training data across different languages.",
    "context_before": [
      "Information Extraction."
    ],
    "context_after": [
      "Summary."
    ],
    "references": [
      "772"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "772",
        "main_query": "Marathi-to-English translation",
        "rewritten_queries": [
          "Translation from Marathi to English",
          "Converting Marathi text to English",
          "Marathi language translation challenges to English"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "It is also promising to combine LLMs and fine-tuned small language models for complementing with each other in solving complex cases of classic NLP tasks [775].",
    "context_before": [
      "In addition, it is still challenging for LLMs to handle complex semantic relations in classic NLP tasks (e.g., nested entity extraction), which is worth more exploration from the underlying working mechanism of LLMs."
    ],
    "context_after": [
      "Another promising direction is to conduct human-machine collaborative research (e.g., conversational translation ) on NLP tasks, since LLMs can effectively understand human instructions and make meaningful responses. 8.1.2 LLM for Information Retrieval The goal of information retrieval (IR) systems is to assist users in discovering ideal information resources (typically documents) and mitigating the information overload issue."
    ],
    "references": [
      "775"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "775",
        "main_query": "combine LLMs and fine-tuned small language models for complementing with each other in solving complex cases of classic NLP tasks",
        "rewritten_queries": [
          "using LLMs alongside fine-tuned small language models to enhance performance on complex classic NLP tasks",
          "integrating LLMs with fine-tuned small language models to address challenging classic NLP tasks",
          "leveraging the strengths of LLMs and fine-tuned small language models to tackle intricate classic NLP problems"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Another promising direction is to conduct human-machine collaborative research (e.g., conversational translation [771]) on NLP tasks, since LLMs can effectively understand human instructions and make meaningful responses. 8.1.2 LLM for Information Retrieval The goal of information retrieval (IR) systems is to assist users in discovering ideal information resources (typically documents) and mitigating the information overload issue.",
    "context_before": [
      "It is also promising to combine LLMs and fine-tuned small language models for complementing with each other in solving complex cases of classic NLP tasks ."
    ],
    "context_after": [
      "Typically, contemporary IR systems adopt a retrieve-thenrerank pipeline framework ."
    ],
    "references": [
      "771"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "771",
        "main_query": "conversational translation",
        "rewritten_queries": [
          "human-machine collaborative research in conversational translation",
          "NLP tasks involving conversational translation",
          "the role of conversational translation in human-machine collaboration"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Specially, the LLM-based reranking approach can be implemented in different ways by zero-shot or few-shot instruction, including pointwise ( estimating the relevance scores for querydocument pairs) [779], pairwise (determining the relevance order of two documents) [778], or listwise ranking (sorting a subset of candidate documents) [780].",
    "context_before": [
      "Typically, such an approach does not necessitate model training, and achieve promising results compared with well-trained reranking methods ."
    ],
    "context_after": [
      "The essence of these methods lies 72 in the special design of instructions for text reranking, such as sliding window strategy for document lists , setwise selection prompting , fine-grained relevance labels incorporation , and pairwise comparison prompting ."
    ],
    "references": [
      "779",
      "780",
      "778"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "779",
        "main_query": "estimating the relevance scores for query-document pairs",
        "rewritten_queries": [
          "calculating relevance scores for pairs of queries and documents",
          "assessing the relevance of query-document combinations",
          "evaluating the relevance scores between queries and their corresponding documents"
        ]
      },
      {
        "related_to_reference": "780",
        "main_query": "listwise ranking (sorting a subset of candidate documents)",
        "rewritten_queries": [
          "listwise ranking method for organizing candidate documents",
          "sorting candidate documents using listwise ranking",
          "approach of listwise ranking to arrange a subset of documents"
        ]
      },
      {
        "related_to_reference": "778",
        "main_query": "pairwise (determining the relevance order of two documents)",
        "rewritten_queries": [
          "pairwise ranking method for assessing document relevance",
          "approach for establishing the relevance order between two documents",
          "pairwise comparison technique to determine document relevance"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition, recent efforts employ LLMs to generate intermediate texts ( e.g., URLs) as retrieval results using few-shot demonstrations [784].",
    "context_before": [
      "The essence of these methods lies 72 in the special design of instructions for text reranking, such as sliding window strategy for document lists , setwise selection prompting , fine-grained relevance labels incorporation , and pairwise comparison prompting ."
    ],
    "context_after": [
      "To further enhance the model performance, LLMs can be specially fine-tuned as backbones for reranking or retrieval (including dense retrieval and model-based retrieval ), similar to the fine-tuning process for traditional PLM-based IR models ."
    ],
    "references": [
      "784"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "784",
        "main_query": "LLMs to generate intermediate texts ( e.g., URLs) as retrieval results using few-shot demonstrations",
        "rewritten_queries": [
          "Using LLMs to create intermediate texts like URLs for retrieval results with few-shot examples",
          "Employing large language models to produce intermediate outputs such as URLs as retrieval results through few-shot learning",
          "Leveraging LLMs for generating intermediate text outputs (e.g., URLs) in retrieval tasks using few-shot demonstrations"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "The rewritten query can take the form of an improved version of the original query [794], a document in the corpus that related to the query [795], or an expansion of the query that concatenated with a pseudo generated document [796].",
    "context_before": [
      "As a solution, LLM can be utilized to rewrite the query for enhancing the understanding of the query intent and incorporating additional knowledge into the query through well-designed instructions."
    ],
    "context_after": [
      "In addition, documents can also be expanded with queries that are generated based on the original documents using LLMs for context extension ."
    ],
    "references": [
      "796",
      "794"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "796",
        "main_query": "an expansion of the query that concatenated with a pseudo generated document",
        "rewritten_queries": [
          "a query expansion that includes a pseudo generated document",
          "concatenating a pseudo generated document with the original query for expansion",
          "enhancing the query by adding a pseudo generated document to it"
        ]
      },
      {
        "related_to_reference": "794",
        "main_query": "an improved version of the original query",
        "rewritten_queries": [
          "a refined iteration of the initial query",
          "a better formulation of the original query",
          "an enhanced adaptation of the initial query"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition, documents can also be expanded with queries that are generated based on the original documents using LLMs for context extension [797].",
    "context_before": [
      "The rewritten query can take the form of an improved version of the original query , a document in the corpus that related to the query , or an expansion of the query that concatenated with a pseudo generated document ."
    ],
    "context_after": [
      "Remaining Issues."
    ],
    "references": [
      "797"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "797",
        "main_query": "queries that are generated based on the original documents using LLMs for context extension",
        "rewritten_queries": [
          "LLMs can generate queries from original documents to extend context",
          "Using LLMs to create queries based on original documents for context enhancement",
          "Expanding documents with queries generated from original content through LLMs"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "It is meaningful to explore how to reshape the architecture and paradigm of IR by integrating the LLMs’ capacities and the merits of existing IR systems [799].",
    "context_before": [
      "Secondly, the advent of LLMs sheds lights on the development of new information seeking ways ( e.g., New Bing)."
    ],
    "context_after": [
      "Thirdly, existing work mainly focuses on text retrieval tasks, lacking a comprehensive consideration of multimodal information sources."
    ],
    "references": [
      "799"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "799",
        "main_query": "reshape the architecture and paradigm of IR by integrating the LLMs’ capacities and the merits of existing IR systems",
        "rewritten_queries": [
          "integrate the capabilities of LLMs with existing information retrieval systems",
          "explore the integration of LLMs into the architecture of information retrieval",
          "combine the strengths of LLMs and traditional IR systems for improved architecture"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Another alternative approach is to train a revision model to rectify hallucinated response generated by MLLMs in a post-hoc way [862].",
    "context_before": [
      "To address the aforementioned issues, some studies collect specialized visual instructions to mitigate the problem of hallucination ."
    ],
    "context_after": [
      "Additionally, aligning MLLMs with RLHF can also assist MLLMs in generating responses with improved factuality ."
    ],
    "references": [
      "862"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "862",
        "main_query": "train a revision model to rectify hallucinated response generated by MLLMs in a post-hoc way",
        "rewritten_queries": [
          "use a revision model to correct hallucinated responses from MLLMs after generation",
          "develop a model to fix hallucinations in responses produced by MLLMs post-hoc",
          "implement a revision approach to address hallucinated outputs from MLLMs"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Additionally, aligning MLLMs with RLHF can also assist MLLMs in generating responses with improved factuality [863].",
    "context_before": [
      "Another alternative approach is to train a revision model to rectify hallucinated response generated by MLLMs in a post-hoc way ."
    ],
    "context_after": [
      "Despite these efforts, existing alignment techniques for MLLMs mainly concentrate on several specific aspects ( e.g., hallucination), lacking a comprehensive consideration of alignment criteria."
    ],
    "references": [
      "863"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "863",
        "main_query": "aligning MLLMs with RLHF can also assist MLLMs in generating responses with improved factuality",
        "rewritten_queries": [
          "using RLHF to align MLLMs helps improve the factual accuracy of their responses",
          "the alignment of MLLMs through RLHF contributes to better factuality in generated responses",
          "RLHF alignment techniques for MLLMs enhance the factual correctness of their output"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "It has been shown that LLMs are capable of handling a variety of healthcare tasks, e.g., biology information extraction [765], medical advice consultation [895], mental health analysis [896], and report simplification [897].",
    "context_before": [
      "Ever since the advent of ChatGPT, a number of studies have applied ChatGPT or other LLMs to the medical domain."
    ],
    "context_after": [
      "As the major technical approach, researchers typically design specific prompts or instructions to guide LLMs to perform a wide range of medical tasks."
    ],
    "references": [
      "765"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "765",
        "main_query": "biology information extraction",
        "rewritten_queries": [
          "extraction of biological information",
          "biological data extraction techniques",
          "information extraction in biology"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition, it would also raise privacy concerns to upload the health information of patients [765] into a commercial server that support the LLM.",
    "context_before": [
      "However, LLMs may fabricate medical misinformation , e.g., misinterpreting medical terms and suggesting advice inconsistent with medical guidelines."
    ],
    "context_after": [
      "Education is also an important application domain where LLMs potentially exert significant influence."
    ],
    "references": [
      "765"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "765",
        "main_query": "it would also raise privacy concerns to upload the health information of patients into a commercial server that support the LLM",
        "rewritten_queries": [
          "uploading patient health information to a commercial server raises privacy issues",
          "privacy concerns arise from storing patient health data on commercial servers for LLMs",
          "the upload of health information to commercial servers supporting LLMs poses privacy risks"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To leverage the scaling effect of LLMs, researchers collect large-scale finance corpora for continually pre-training LLMs ( e.g., BloombergGPT [358], XuanYuan 2.0 [919], and FinGPT [920]).",
    "context_before": [
      "Despite the competitive zero-shot performance exhibited by general-purpose LLMs in the finance tasks, they still underperform domain-specific PLMs containing million-scale parameters ."
    ],
    "context_after": [
      "BloombergGPT has demonstrated remarkable performance across a diverse range of financial tasks while maintaining competitive performance in general-purpose tasks ."
    ],
    "references": [
      "919",
      "358"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "919",
        "main_query": "XuanYuan 2.0",
        "rewritten_queries": [
          "XuanYuan 2.0 finance corpus for LLM pre-training",
          "Large-scale finance corpora including XuanYuan 2.0",
          "XuanYuan 2.0 as a resource for continually pre-training LLMs"
        ]
      },
      {
        "related_to_reference": "358",
        "main_query": "BloombergGPT",
        "rewritten_queries": [
          "BloombergGPT model in finance",
          "Large-scale finance model BloombergGPT",
          "Performance of BloombergGPT in financial tasks"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "BloombergGPT has demonstrated remarkable performance across a diverse range of financial tasks while maintaining competitive performance in general-purpose tasks [358].",
    "context_before": [
      "To leverage the scaling effect of LLMs, researchers collect large-scale finance corpora for continually pre-training LLMs ( e.g., BloombergGPT , XuanYuan 2.0 , and FinGPT )."
    ],
    "context_after": [
      "Nevertheless, it is imperative to consider the potential risks in the application of LLMs in finance, as the generation of inaccurate or harmful content by LLMs could have significant adverse implications for financial markets ."
    ],
    "references": [
      "358"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "358",
        "main_query": "BloombergGPT has demonstrated remarkable performance across a diverse range of financial tasks while maintaining competitive performance in general-purpose tasks",
        "rewritten_queries": [
          "BloombergGPT shows excellent results in various financial tasks and remains competitive in general-purpose tasks",
          "The performance of BloombergGPT is outstanding in a wide array of financial tasks, alongside its strong performance in general-purpose applications",
          "BloombergGPT excels in numerous financial tasks while also achieving competitive results in general-purpose tasks"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Nevertheless, it is imperative to consider the potential risks in the application of LLMs in finance, as the generation of inaccurate or harmful content by LLMs could have significant adverse implications for financial markets [358].",
    "context_before": [
      "BloombergGPT has demonstrated remarkable performance across a diverse range of financial tasks while maintaining competitive performance in general-purpose tasks ."
    ],
    "context_after": [
      "Therefore, it needs more strict reviewing and monitoring on the use of LLMs in the financial field."
    ],
    "references": [
      "358"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "358",
        "main_query": "the generation of inaccurate or harmful content by LLMs could have significant adverse implications for financial markets",
        "rewritten_queries": [
          "potential risks of LLMs generating harmful content in finance",
          "impact of inaccurate content produced by LLMs on financial markets",
          "risks associated with LLMs in the financial sector due to harmful content generation"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "The initial plan can be further refined with execution feedback from the environment [530].",
    "context_before": [
      "To generate it, LLM-based agents will first propose several candidates and then select a more suitable one among them ."
    ],
    "context_after": [
      "The execution component is in charge of carrying out the plan from the planning component, which can be fulfilled by the internal LLM or external tools ."
    ],
    "references": [
      "530"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "530",
        "main_query": "The initial plan can be further refined with execution feedback from the environment",
        "rewritten_queries": [
          "Execution feedback from the environment can help refine the initial plan",
          "The initial plan can be improved by incorporating feedback from the environment during execution",
          "Feedback from the environment during execution allows for further refinement of the initial plan"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "https://www.langchain.com/ 81 as AgentVerse [969] and AutoGen [970], can also be utilized for developing multi-agent collaborative systems.",
    "context_before": [
      "In addition, other similar frameworks, such."
    ],
    "context_after": [
      "In the competition-based mode, debate serves as one of the popular communication protocols to foster divergent thinking and elicit valuable external feedback among agents."
    ],
    "references": [
      "969"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "969",
        "main_query": "AgentVerse",
        "rewritten_queries": [
          "AgentVerse framework for multi-agent systems",
          "Utilization of AgentVerse in collaborative systems",
          "AgentVerse in the context of multi-agent collaboration"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Unlike traditional early exit mechanisms that skip all subsequent layers, the mixtureof-depths method selectively skips certain layers, which can adaptively utilize the characteristics of different layers during generation. 9.5 Model Compression Due to the huge number of model parameters, LLMs take a significant memory footprint for inference, making it very costly to be deployed in real-world applications [999].",
    "context_before": [
      "If the score exceeds a preset threshold, the layer would be computed; otherwise, the layer would be skipped."
    ],
    "context_after": [
      "In this section, we focus on how to reduce the memory footprint of LLMs via technical approaches."
    ],
    "references": [
      "999"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "999",
        "main_query": "the mixtureof-depths method selectively skips certain layers, which can adaptively utilize the characteristics of different layers during generation",
        "rewritten_queries": [
          "the mixtureof-depths approach allows for selective skipping of layers based on their characteristics during generation",
          "unlike traditional methods, the mixtureof-depths technique adapts by skipping specific layers during the generation process",
          "the mixtureof-depths strategy enables adaptive layer skipping, utilizing the unique features of different layers in generation"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In neural network compression, quantization often refers to the mapping process from floating-point numbers to integers [1000], especially the 8-bit integer quantization (i.e., INT8 quantization).",
    "context_before": [
      "In this part, we present a general introduction of quantization techniques for neural networks."
    ],
    "context_after": [
      "For neural network models, there are typically two kinds of data to be quantized, namely weights (model parameters) and activations (hidden activations), which are originally represented in floating-point numbers."
    ],
    "references": [
      "1000"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1000",
        "main_query": "quantization often refers to the mapping process from floating-point numbers to integers",
        "rewritten_queries": [
          "the process of quantization involves converting floating-point numbers into integers",
          "in quantization, floating-point numbers are typically mapped to integer values",
          "quantization in neural networks is the transition from floating-point representation to integer representation"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For more details, we refer to the readers to the excellent survey [1000] about quantization methods on neural networks.",
    "context_before": [
      "The range parameters α and β have a large impact on the quantization performance, which often need to be calibrated according to real data distributions, in either a static (offline) or dynamic way (runtime)."
    ],
    "context_after": [
      "Post-Training Quantization (PTQ) ."
    ],
    "references": [
      "1000"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1000",
        "main_query": "the excellent survey about quantization methods on neural networks",
        "rewritten_queries": [
          "a comprehensive review of quantization techniques for neural networks",
          "detailed survey on neural network quantization methods",
          "in-depth analysis of quantization approaches in neural networks"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To efficiently optimize this objective, GPTQ [1009] improves the original optimal brain quantization (OBQ) [1010] method by fixing the quantization order of weights for all rows.",
    "context_before": [
      "This approach finds optimal quantized weights that minimize a layerwise reconstruction loss: arg mincW ∥ WX − cWX ∥2."
    ],
    "context_after": [
      "Further, with specially designed methods ( i.e., lazy batch-updates and Cholesky reformulation), GPTQ is feasible to quantize very large models ( e.g., 175B OPT) in 3 or 4 bit precision."
    ],
    "references": [
      "1009"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1009",
        "main_query": "GPTQ improves the original optimal brain quantization (OBQ) method by fixing the quantization order of weights for all rows.",
        "rewritten_queries": [
          "GPTQ enhances the OBQ method by standardizing the quantization order of weights across all rows.",
          "The original OBQ method is improved by GPTQ through the fixation of weight quantization order for every row.",
          "By fixing the quantization order of weights for all rows, GPTQ optimizes the original OBQ method."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To overcome this challenge, QLoRA [1011] incorporates additional small tunable adapters (16-bit precision) into the quantized models, to achieve an efficient, high-precision model fine-tuning.",
    "context_before": [
      "For posttraining quantization, direct low-bit quantization (e.g., INT4 quantization) often results in large performance degradation."
    ],
    "context_after": [
      "It combines the merits of LoRA (See Section 5.3.1) and quantization methods."
    ],
    "references": [
      "1011"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1011",
        "main_query": "QLoRA incorporates additional small tunable adapters (16-bit precision) into the quantized models, to achieve an efficient, high-precision model fine-tuning.",
        "rewritten_queries": [
          "QLoRA uses small tunable adapters with 16-bit precision for efficient fine-tuning of quantized models.",
          "To enhance model fine-tuning, QLoRA integrates small tunable adapters at 16-bit precision into quantized models.",
          "The QLoRA method adds small tunable adapters (16-bit precision) to quantized models for improved fine-tuning efficiency."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Moreover, focusing on emergent capabilities, the study [1015] finds that in-context learning, step-by-step reasoning, and instruction following all seem to be seldom affected with 4-bit weight quantization.",
    "context_before": [
      "For example, a 4-bit 60B LLM is demonstrated to have better performance than an 8-bit 30B LLM ."
    ],
    "context_after": [
      "This result suggests that INT4 quantization exhibits a favorable trade-off in terms of both total bits and performance of emergent abilities."
    ],
    "references": [
      "1015"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1015",
        "main_query": "in-context learning, step-by-step reasoning, and instruction following all seem to be seldom affected with 4-bit weight quantization",
        "rewritten_queries": [
          "4-bit weight quantization has little impact on in-context learning, step-by-step reasoning, and instruction following",
          "The study indicates that in-context learning, reasoning, and instruction adherence are largely unaffected by 4-bit quantization",
          "Emergent capabilities like in-context learning and instruction following are rarely influenced by 4-bit weight quantization"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Secondly, it is flexible to support taskspecific or goal-specific fine-tuning of LLMs in a lightweight way [1011], e.g., instruction tuning or chat-oriented tuning, by only tuning the small adapters.",
    "context_before": [
      "This can be achieved either by increasing the fitting capacity via updating high precision adapters , or by finding a proper low-rank initizalization for LoRA fine-tuning ."
    ],
    "context_after": [
      "Overall, it makes a good trade-off between the effectiveness and training cost, which provides a promising approach to enhancing the performance of quantized LLMs."
    ],
    "references": [
      "1011"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1011",
        "main_query": "it is flexible to support taskspecific or goal-specific fine-tuning of LLMs in a lightweight way",
        "rewritten_queries": [
          "flexible support for task-specific or goal-specific fine-tuning of LLMs",
          "lightweight fine-tuning of LLMs for specific tasks or goals",
          "support for specific task-oriented fine-tuning of LLMs in a flexible manner"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For evaluation, we utilize the same tasks in Table 10, and follow the quantization settings in the study [1015] examining the performance of quantized language models at three precision levels: 4-bit, 8-bit and 16-bit.",
    "context_before": [
      "Specifically, we focus on the fine-tuned LLaMA models (i.e., 7B and 13B) using popular SFT datasets, including FLANv2 , Alpaca-52K and ShareGPT ."
    ],
    "context_after": [
      "The results are summarized in Table."
    ],
    "references": [
      "1015"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1015",
        "main_query": "the performance of quantized language models at three precision levels: 4-bit, 8-bit and 16-bit",
        "rewritten_queries": [
          "performance evaluation of quantized language models using 4-bit, 8-bit, and 16-bit precision",
          "quantized language models assessed at three different precision levels: 4-bit, 8-bit, and 16-bit",
          "evaluation of language models quantized to 4-bit, 8-bit, and 16-bit precision levels"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Furthermore, Sheared LLaMA [1022] introduces two techniques: targeted structured pruning and dynamic batch loading, which effectively prunes LLaMA2 (7B) to a parameter size of 2.7B, while preserving 87.8% of the original model’s performance. 9.5.3 Open-source Libraries In this part, we briefly introduce the available open-source libraries for memory-efficient deployment.",
    "context_before": [
      "For instance, LLM-pruner selectively removes 20% of the nonessential parameters from LLaMA (7B) based on gradient information, while maintaining 93.6% performance of the original model."
    ],
    "context_after": [
      "Quantization Libraries ."
    ],
    "references": [
      "1022"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1022",
        "main_query": "Sheared LLaMA introduces two techniques: targeted structured pruning and dynamic batch loading, which effectively prunes LLaMA2 (7B) to a parameter size of 2.7B, while preserving 87.8% of the original model’s performance.",
        "rewritten_queries": [
          "Sheared LLaMA employs targeted structured pruning and dynamic batch loading to reduce LLaMA2 (7B) to 2.7B parameters, maintaining 87.8% performance.",
          "The techniques of targeted structured pruning and dynamic batch loading in Sheared LLaMA reduce LLaMA2 (7B) to 2.7B parameters while preserving 87.8% of its performance.",
          "By using targeted structured pruning and dynamic batch loading, Sheared LLaMA prunes LLaMA2 (7B) down to 2.7B parameters, retaining 87.8% of the original model's performance."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "It enables 4-bit quantization of LLaMA models of varied sizes based on the GPTQ algorithm [1009].",
    "context_before": [
      "GPTQ-for-LLaMA50 is developed specially for quantizing LLaMA models."
    ],
    "context_after": [
      "Also, it provides a comparison with bitsandbytes in both memory and performance (PPL) on the project website."
    ],
    "references": [
      "1009"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1009",
        "main_query": "4-bit quantization of LLaMA models based on the GPTQ algorithm",
        "rewritten_queries": [
          "Quantization of LLaMA models using the GPTQ algorithm with 4-bit precision",
          "GPTQ algorithm enabling 4-bit quantization for various LLaMA model sizes",
          "4-bit quantization technique for LLaMA models developed through the GPTQ method"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "AutoGPTQ51 is a quantization package developed based on the GPTQ algorithm [1009], which supports INT4 quantization for LLMs.",
    "context_before": [
      "Also, it provides a comparison with bitsandbytes in both memory and performance (PPL) on the project website."
    ],
    "context_after": [
      "It includes a number of quantized models in the library, and supports LoRA by integrating with HuggingFace PEFT library."
    ],
    "references": [
      "1009"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1009",
        "main_query": "AutoGPTQ51 is a quantization package developed based on the GPTQ algorithm, which supports INT4 quantization for LLMs.",
        "rewritten_queries": [
          "AutoGPTQ51 is a quantization tool that utilizes the GPTQ algorithm and enables INT4 quantization for large language models.",
          "The quantization package AutoGPTQ51, based on the GPTQ algorithm, offers support for INT4 quantization in LLMs.",
          "Developed from the GPTQ algorithm, AutoGPTQ51 is a quantization package that facilitates INT4 quantization for large language models."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To address this issue, existing approaches often introduce reranking models to select the most relevant documents from the retrieval results [1027].",
    "context_before": [
      "Since the retrieved documents are typically lengthy, simply concatenating them into the prompt might lead to a poor utilization of the provided context due to the biased attention ( e.g., lost in the middle )."
    ],
    "context_after": [
      "Alternatively, information extraction or text compression techniques can be used to retain only the highly relevant information from the documents, thereby reducing the input context length ."
    ],
    "references": [
      "1027"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1027",
        "main_query": "existing approaches often introduce reranking models to select the most relevant documents from the retrieval results",
        "rewritten_queries": [
          "reranking models are commonly used in existing methods to choose the most relevant documents from retrieval outputs",
          "to tackle this problem, many current strategies implement reranking models for selecting relevant documents from retrieved results",
          "existing techniques frequently utilize reranking models to identify the most pertinent documents from the results of retrieval"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "The incorporation of retrieval supplements the LLM with relevant contextual information, and the retrieval performance directly affects the quality of the final generated response [454].",
    "context_before": [
      "Retrieval method improvement."
    ],
    "context_after": [
      "To design effective retrieval strategy, an important factor to consider is the text granularity."
    ],
    "references": [
      "454"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "454",
        "main_query": "the retrieval performance directly affects the quality of the final generated response",
        "rewritten_queries": [
          "how retrieval performance influences the quality of generated responses",
          "the impact of retrieval performance on the final response quality",
          "the relationship between retrieval performance and the quality of generated outputs"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To balance relevance and latency, existing research work proposes using “ propositions” as the retrieval unit [1031], corresponding to semantically complete and relatively independent text fragments, which can effectively reduce the recall of irrelevant information.",
    "context_before": [
      "Intuitively, a coarser granularity (e.g., document-level) may result in efficient retrieval but tend to incorporate substantial irrelevant information, while a finer granularity (e.g., sentence-level) increases the proportion of relevant content in the retrieval results but can lead to higher retrieval latency."
    ],
    "context_after": [
      "In particular, they mainly use GPT-4 to synthesize instruction data for the extraction of proposition text, training a smaller model specifically to construct proposition text data ."
    ],
    "references": [
      "1031"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1031",
        "main_query": "propositions as the retrieval unit",
        "rewritten_queries": [
          "using propositions for retrieval",
          "retrieval units as propositions",
          "propositions for balancing relevance and latency"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In particular, they mainly use GPT-4 to synthesize instruction data for the extraction of proposition text, training a smaller model specifically to construct proposition text data [1031].",
    "context_before": [
      "To balance relevance and latency, existing research work proposes using “ propositions” as the retrieval unit , corresponding to semantically complete and relatively independent text fragments, which can effectively reduce the recall of irrelevant information."
    ],
    "context_after": [
      "Furthermore, to improve retrieval performance, methods such as query expansion and query rewriting can be utilized to optimize query formulation."
    ],
    "references": [
      "1031"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1031",
        "main_query": "GPT-4 to synthesize instruction data for the extraction of proposition text",
        "rewritten_queries": [
          "using GPT-4 for synthesizing instruction data to extract proposition text",
          "synthesizing instruction data with GPT-4 for proposition text extraction",
          "the role of GPT-4 in generating instruction data for extracting proposition text"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Query expansion focuses on adding supplementary information to the original query, such as incorporating related entity information or providing detailed explanations of key information in the query [796], which helps strengthen relevance matching.",
    "context_before": [
      "Furthermore, to improve retrieval performance, methods such as query expansion and query rewriting can be utilized to optimize query formulation."
    ],
    "context_after": [
      "However, traditional query expansion methods may disrupt the original semantics for complex queries."
    ],
    "references": [
      "796"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "796",
        "main_query": "incorporating related entity information or providing detailed explanations of key information in the query",
        "rewritten_queries": [
          "adding related entity information and detailed explanations to the original query",
          "supplementing the original query with related entities and key information explanations",
          "enhancing the original query by including related entities and detailed key information"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "LLMs can be applied directly to query rewriting, transforming the original query into a more suitable form through well-designed prompts [1034].",
    "context_before": [
      "As another query enhancment technique, query rewriting focuses on modifying the query content to highlight key information and eliminate potential ambiguities, facilitating the retrieval of related documents ."
    ],
    "context_after": [
      "To reduce inference overhead, the query optimization capabilities of LLMs can also be transferred to smaller models through knowledge distillation ."
    ],
    "references": [
      "1034"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1034",
        "main_query": "LLMs can be applied directly to query rewriting, transforming the original query into a more suitable form through well-designed prompts",
        "rewritten_queries": [
          "Large Language Models can directly rewrite queries to improve their suitability using effective prompts",
          "The application of LLMs in query rewriting allows for the transformation of original queries into better forms with well-crafted prompts",
          "Using well-designed prompts, LLMs can directly modify queries to enhance their relevance and clarity"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "As a solution, the documents returned during the retrieval stage can be reranked according to their relevance to the input [1036], filtering out low-quality or irrelevant documents or placing less relevant documents in non-optimal positions within the prompt.",
    "context_before": [
      "In addition to the initial retrieval methods, the refinement of retrieval results also plays an important role in RAG systems, since the retrieved documents may be not best suited for RAG systems, e.g., LLMs might have difficulty in utilizing long contexts or be affected by irrelevant information in the retrieved documents."
    ],
    "context_after": [
      "Furthermore, both generation and reranking tasks can be jointly optimized to faciliate better utilize of context documents."
    ],
    "references": [
      "1036"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1036",
        "main_query": "the documents returned during the retrieval stage can be reranked according to their relevance to the input",
        "rewritten_queries": [
          "documents retrieved can be reordered based on their relevance to the input",
          "retrieved documents can be ranked again according to how relevant they are to the input",
          "the relevance of retrieved documents to the input can be used to adjust their ranking"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Furthermore, both generation and reranking tasks [1027] can be jointly optimized to faciliate better utilize of context documents.",
    "context_before": [
      "As a solution, the documents returned during the retrieval stage can be reranked according to their relevance to the input , filtering out low-quality or irrelevant documents or placing less relevant documents in non-optimal positions within the prompt."
    ],
    "context_after": [
      "Additionally, LLMs 90 can be directly used for document re-ranking by designing specific prompts or using context examples to accomplish this task ."
    ],
    "references": [
      "1027"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1027",
        "main_query": "both generation and reranking tasks can be jointly optimized to facilitate better utilize of context documents",
        "rewritten_queries": [
          "joint optimization of generation and reranking tasks improves the use of context documents",
          "optimizing generation alongside reranking tasks enhances the effectiveness of context document utilization",
          "the joint optimization of generation and reranking tasks leads to better utilization of context documents"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Additionally, LLMs 90 can be directly used for document re-ranking by designing specific prompts or using context examples to accomplish this task [777].",
    "context_before": [
      "Furthermore, both generation and reranking tasks can be jointly optimized to faciliate better utilize of context documents."
    ],
    "context_after": [
      "In addition to document filtering or reranking, information extraction or automatic summarization techniques can be employed to refine the retrieved content by extracting more concise and query-relevant content from the retrieved documents."
    ],
    "references": [
      "777"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "777",
        "main_query": "LLMs can be directly used for document re-ranking by designing specific prompts or using context examples to accomplish this task",
        "rewritten_queries": [
          "Large Language Models can perform document re-ranking through tailored prompts or contextual examples",
          "Document re-ranking can be achieved with LLMs by creating specific prompts or utilizing context examples",
          "Using specific prompts or context examples, LLMs can effectively re-rank documents"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Building on the iterative retrieval augmentation method, adaptive retrieval augmentation further enhances the LLM’s autonomous use of the retrieval mechanism [1038], thereby improving the overall framework’s efficacy in using the retrieval systems.",
    "context_before": [
      "For example, intermediate results from the chain of thought can be used as the query input for the next round of retrieval, and after completing the retrieval process, the returned results can be integrated into the chain of thought."
    ],
    "context_after": [
      "In practical implementation, for the above two types of augmentation methods, LLM first need to determine when to use the retriever and then utilize pre-set prompts to initiate query generation and retrieval result processing ."
    ],
    "references": [
      "1038"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1038",
        "main_query": "adaptive retrieval augmentation further enhances the LLM’s autonomous use of the retrieval mechanism",
        "rewritten_queries": [
          "adaptive retrieval augmentation improves the autonomous retrieval capabilities of LLMs",
          "the LLM's ability to autonomously utilize the retrieval mechanism is enhanced by adaptive retrieval augmentation",
          "enhancements in the LLM's autonomous retrieval mechanism are achieved through adaptive retrieval augmentation"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In practical implementation, for the above two types of augmentation methods, LLM first need to determine when to use the retriever and then utilize pre-set prompts to initiate query generation and retrieval result processing [1039].",
    "context_before": [
      "Building on the iterative retrieval augmentation method, adaptive retrieval augmentation further enhances the LLM’s autonomous use of the retrieval mechanism , thereby improving the overall framework’s efficacy in using the retrieval systems."
    ],
    "context_after": [
      "RAG-enhanced training."
    ],
    "references": [
      "1039"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1039",
        "main_query": "LLM first need to determine when to use the retriever and then utilize pre-set prompts to initiate query generation and retrieval result processing",
        "rewritten_queries": [
          "The LLM must identify the appropriate times to employ the retriever and use predefined prompts for query generation and processing results.",
          "Determining when to activate the retriever and using established prompts for initiating query generation is essential for the LLM.",
          "For effective implementation, the LLM should ascertain when to apply the retriever and leverage pre-configured prompts for generating queries and processing retrieval outcomes."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "By constructing instruction data focused on retrieval context utilization [1040], instruction tuning can improve the LLM’s ability to utilize relevant retrieval information.",
    "context_before": [
      "In addition to the improvement strategies mentioned above, specialized training tasks can be designed to further enhance the LLM’s ability to utilize the retrieved content, including both instruction tuning and pre-training tasks."
    ],
    "context_after": [
      "When curating the instruction data, it is essential to consider two important issues: positional bias and irrelevant information within the input context."
    ],
    "references": [
      "1040"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1040",
        "main_query": "instruction data focused on retrieval context utilization",
        "rewritten_queries": [
          "instruction data designed for effective retrieval context use",
          "instruction tuning that emphasizes the use of retrieval context",
          "training data aimed at enhancing retrieval context utilization"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Additionally, irrelevant information can be added to the instructions data, so as to improving the model’s ability to resist interference from such information [1041].",
    "context_before": [
      "Specifically, relevant documents can be placed at different positions within the prompt, which can enhance the model’s attention to relevant content in various positions and prevent the model from neglecting certain positions ."
    ],
    "context_after": [
      "In addition, special training tasks can be introduced during the pre-training stage to further enhance the LLM’s retrieval and generation capabilities ."
    ],
    "references": [
      "1041"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1041",
        "main_query": "irrelevant information can be added to the instructions data, so as to improving the model’s ability to resist interference from such information",
        "rewritten_queries": [
          "adding irrelevant information to instructions data enhances the model's resistance to interference",
          "the inclusion of irrelevant data in instructions helps improve the model's ability to handle interference",
          "to improve resistance to interference, irrelevant information can be incorporated into the instructions data"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "A common data construction method uses portions of the original document as queries and then trains the model to reconstruct the remaining content of the original document based on the retrieval results [1043]. 9.7 Hallucination Hallucination, which refers to the phenomenon that LLMs generate content inconsistent with factual information, has become a significant issue that greatly affects the task performance of LLMs [1044].",
    "context_before": [
      "Existing work mainly constructs unsupervised pre-training data aimed at retrieval augmentation."
    ],
    "context_after": [
      "In this section, we focus on discussing the topic of LLM hallucination, first introducing the definition and source of hallucination and then summarizing the detection and mitigation methods. 9.7.1 Definition of Hallucination Early research typically defines hallucinations based on the relationship between a model’s output and the given input ."
    ],
    "references": [
      "1044"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1044",
        "main_query": "Hallucination, which refers to the phenomenon that LLMs generate content inconsistent with factual information, has become a significant issue that greatly affects the task performance of LLMs",
        "rewritten_queries": [
          "The phenomenon of hallucination in LLMs involves generating content that does not align with factual information, impacting their performance significantly",
          "LLMs experience hallucination, a significant issue where they produce content that contradicts factual data, affecting their task performance",
          "The issue of hallucination in large language models arises when they generate information that is inconsistent with reality, which greatly influences their performance"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "According to a recent study [1044], factual hallucinations can be further categorized into the following types:.",
    "context_before": [
      "However, in real-world scenarios, user inputs often do not contain reference documents, and thus existing work mainly focuses on open-domain factual hallucinations, where the model-generated content does not align with or cannot be verified by existing world knowledge ."
    ],
    "context_after": [
      "Entity-error hallucination."
    ],
    "references": [
      "1044"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1044",
        "main_query": "factual hallucinations can be further categorized into the following types",
        "rewritten_queries": [
          "different types of factual hallucinations identified in a recent study",
          "categorization of factual hallucinations as per recent research",
          "recent findings on the classification of factual hallucinations"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Additionally, recent studies show that when addressing questions involving long-tail knowledge that appears infrequently in the training corpus, models are more likely to generate inaccurate content [1044].",
    "context_before": [
      "In terms of data composition, pre-training data may lack domain-specific knowledge, which would affect the model performance on tasks requiring specialized knowledge, such as medical or legal issues, and it will also result in significant hallucinations."
    ],
    "context_after": [
      "Training Methods."
    ],
    "references": [
      "1044"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1044",
        "main_query": "when addressing questions involving long-tail knowledge that appears infrequently in the training corpus, models are more likely to generate inaccurate content",
        "rewritten_queries": [
          "models tend to produce inaccurate content when dealing with infrequent long-tail knowledge in the training data",
          "inaccurate content generation is more likely when questions involve long-tail knowledge that is rarely present in the training corpus",
          "the likelihood of generating inaccuracies increases for models when faced with long-tail knowledge that is infrequently represented in training data"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "However, inappropriate prompt design can cause the model to overlook or misunderstand important information, leading to incorrect or irrelevant content [1044].",
    "context_before": [
      "Prompting has become the primary way for using LLMs to solve downstream tasks."
    ],
    "context_after": [
      "Recent studies have shown that the readability, format, and concreteness of user instructions would impact the model’s output ."
    ],
    "references": [
      "1044"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1044",
        "main_query": "inappropriate prompt design can cause the model to overlook or misunderstand important information",
        "rewritten_queries": [
          "poorly designed prompts may lead the model to miss or misinterpret crucial details",
          "ineffective prompt creation can result in the model failing to recognize or accurately process significant information",
          "suboptimal prompt strategies can cause the model to ignore or misread essential content"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "HaluEval 2.0 [1044] proposes to first collect hallucinated and nonhallucinated responses to train a reward model, and then fine-tune the LLM with the reward model’s feedback using RL algorithms.",
    "context_before": [
      "Hallucination mitigation is closely related to the honest criterion in “3H” standards for human alignment, and various alignment methods like RLHF can be adopted to mitigate the model hallucination."
    ],
    "context_after": [
      "However, recent research shows that human preference data may lead LLMs to exhibit sycophantic behavior , where models prioritize catering to human demands over maintaining truthfulness."
    ],
    "references": [
      "1044"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "1044",
        "main_query": "HaluEval 2.0 proposes to first collect hallucinated and nonhallucinated responses to train a reward model, and then fine-tune the LLM with the reward model’s feedback using RL algorithms.",
        "rewritten_queries": [
          "HaluEval 2.0 suggests gathering both hallucinated and nonhallucinated responses to develop a reward model, followed by fine-tuning the LLM based on the feedback from this model using reinforcement learning techniques.",
          "The approach of HaluEval 2.0 involves collecting responses that are both hallucinated and nonhallucinated to create a reward model, which is then used to fine-tune the LLM with reinforcement learning.",
          "According to HaluEval 2.0, the process starts with collecting hallucinated and nonhallucinated responses to train a reward model, which is subsequently used to fine-tune the LLM through reinforcement learning algorithms."
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Furthermore, data contamination has become a severe issue for fairly assessing the performance of LLMs [740], and thus setting appropriate evaluation protocol will be the basis to investigate and analyze the model capacity of LLMs.",
    "context_before": [
      "Another research direction is to explore more deep analysis on model generalization for LLMs, since increasing concerns have been raised about whether LLMs can generalize beyond the knowledge encoded by pre-training data."
    ],
    "context_after": [
      "Model Architecture."
    ],
    "references": [
      "740"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "740",
        "main_query": "data contamination has become a severe issue for fairly assessing the performance of LLMs",
        "rewritten_queries": [
          "the issue of data contamination significantly affects the fair assessment of LLM performance",
          "fair assessment of LLMs is severely impacted by data contamination",
          "data contamination poses a serious challenge in evaluating LLM performance"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Specially, system-level or hardware-level optimization ( e.g., FlashAttention [303]) is worth more exploration to improve the efficiency of Transformer architectures.",
    "context_before": [
      "More efforts are still in need to develop improved model architectures for large-scale pre-training."
    ],
    "context_after": [
      "In addition, as an important basic capacity, existing LLMs typically maintain a long context window."
    ],
    "references": [
      "303"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "303",
        "main_query": "FlashAttention",
        "rewritten_queries": [
          "hardware-level optimization techniques like FlashAttention",
          "system-level optimizations such as FlashAttention",
          "exploring FlashAttention for improving Transformer efficiency"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Thus, it becomes particularly important to develop systemic, economical pre-training approaches for optimizing LLMs, e.g., predictable scaling [46] and proxy model training [59].",
    "context_before": [
      "In practice, it is very challenging to pre-train capable LLMs, due to the huge compute consumption and the sensitivity to data quality and training tricks ."
    ],
    "context_after": [
      "More training recipes or principles should be investigated and shared to reduce the potential risk of degradation or failure in large-scale model optimization."
    ],
    "references": [
      "59"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "59",
        "main_query": "proxy model training",
        "rewritten_queries": [
          "training using proxy models",
          "utilizing proxy models for training",
          "approaches involving proxy model training"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "It has been shown that retrieval augmentation can extend the knowledge boundary and improve the question answering capacity [454], but may suffer from the effectiveness of long context utilization by LLMs [949].",
    "context_before": [
      "Another popular research direction is retrieval-augmented generation, where retrieved contexts from supporting sources are included into prompts for task solving."
    ],
    "context_after": [
      "Safety and Alignment."
    ],
    "references": [
      "454"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "454",
        "main_query": "retrieval augmentation can extend the knowledge boundary and improve the question answering capacity",
        "rewritten_queries": [
          "retrieval augmentation enhances knowledge boundaries and boosts question answering abilities",
          "the use of retrieval augmentation improves the capacity for answering questions and expands knowledge limits",
          "it has been demonstrated that retrieval augmentation increases question answering effectiveness and broadens knowledge scope"
        ]
      }
    ],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Here the regularized delta function as constructed by Peskin guarantees that the net force and torque acting on a body are preserved by the spreading step.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Yet another approach is due to Tschisgale et al. (cf. also ), who define an interface layer (with finite thickness) to which the rigid-body assumption is restricted.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "This resulted in a laser spot diameter of roughly 660 nm .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Contrastingly in CVD NV-diamonds, the NV centers have better coherence properties, in the expense of a lower control over their localization and concentration .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Also, we have based our µ-chemoEH model on the classical simplified model for the bending elasticity of the axoneme proposed in .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "From research on the memory effect, it is determined that electron densities of 108 − 109 cm−3, which are far below the breakdown threshold, are already sufficient to guide the plasma in a direction nearly perpendicular to the background electric field direction .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "An early – 1999 – application of this method is the representation of an impeller in a mixing tank in single-phase large-eddy simulations .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The experimental results demonstrate that committed vaccine uptake by individuals can effectively stimulate vaccination behavior among peers .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The red squares indicate the support of the discrete delta function kernel (in this case the 3-point function of ) associated to each Lagrangian marker point. (c) Distribution of 515 Lagrangian points on the surface of a sphere (as e.g. used in an IBM when dp/∆x = 13), obtained via a repulsion force algorithm .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Epilepsy significantly affects the quality and expectancy of patient’s life .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "In particular, the implementation of non-pharmaceutical interventions , , such as travel restrictions and mandatory stay-at-home orders, has greatly facilitated higher-order interactions among family members, leading to a significant portion of infections occurring within households .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "These reactors operate under extremely harsh conditions, characterized by high temperatures and intense radiation fields, necessitating structural materials with greater radiation tolerance than those used in current fission reactors to resist radiation-induced degradation .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "A detailed account of the boundary-conforming approach can be found in the recent review by Wachs .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "In this way, we arrive at a system which is easily comparable with the chemoEH model , from both theoretical and numerical perspectives.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The PKA spectrum of neutron irradiation was generated from typical neutron spectrum of HFIR reactor using the SPECTRA -PKA code , while the one for 5 MeV Fe ion irradiation was calculated by the SRIM code mentioned above at the region of 800-911.5 nm.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Additionally, Griffith et al. used an FTIR spectrometer with an improved resolution of 0.011 cm–1 to validate the tropospheric site preference value of Yoshida and Toyoda .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The Cg value for modeling specific diseases can be estimated through surveys on health attitudes, behaviors, and outcomes, as indicated in Ref. .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "It determines the production of energetic particles, energy losses, gas temperature and chemical activity .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Fortunately, the 500 mm focal length lens results look more similar to the expected profiles and the 750 mm results are almost in agreement with literature .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "It is well know n that the efficiency of HHG rapidly decreases with increasing ellipticity of the driver laser rad iation , and the ellipticity of high harmonics, as a rule, does not exceed the laser ellipticity [1 4].",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Here, we consider a simplicial susceptible-infected-removal (s-SIR) model, which is similar to the s-SIS model proposed in Ref. .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The higher-order organizational structures manifested in networks based on specific social, age, or occupational relationships can be effectively captured using the motifs .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The temporal overlap between the pump and signal pulses in the crystal was maintained by stabilizing the delay line of the pump beam as described in .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Scholars found that voluntary vaccination is the primary public health measure to intervene and control infectious diseases , .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "This propagation is typical of curvature control model, at least in the linear regime .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "L Flagellum length µm 58.3 B Bending rigidity pN · µm2 1700 a Inter-filament distance µm 0.185 ξn Normal RFT coefficient pN · s/µm2 0.0034 K Internal elasticity pN/µm2 228 λ Internal viscosity pN · s/µm2 7 α Dynein rate constant 1/s 250 ℓ Length of a tug-of-war cell µm 1 ρ Density of tug-of-war cells 1/µm 1 N Motors in a tug-of-war cell 1 103 kcb Motor domain’s stiffness pN/µm 103 Table 1: Bull sperm parameters The objective of the following section is twofold.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Appendix D: CHOICE OF PRE AND POST-SELECTED POLARIZATION STATE Unlike plane waves, real Gaussian beams carry a finite Gaussian distribution of wave vectors around the central wave vector which leads to the shifts in the centroid of its 7 transverse profile when reflected or refracted from any interface .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "For moving solid-fluid interface, the immersed boundary method described above is typically used, see, for example, the study of Tao et al. for particle-laden flows. 5.4.5 Remarks Table 5.1: Comparison of LBM and DUGKS.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The 4 − f system images the point of interaction on the camera placed at the front focal plane of L3 as shown in Fig. 1 .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Besides, economic costs, logistical constraints, religious beliefs, and other sociocultural factors often lead to difficulties in vaccination implementation .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "On the other hand, the δ-doped sample presents a strong Raman emission from the diamond host below 550 nm , which saturates the counts at higher wavelengths, 4 0 7 14 NV− SiV (a) δ-doped 600 650 700 750 800 Wavelength (nm) 0 7 14 NV0 (b) (111)-implanted Intensity (kCounts/s) FIG.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Chaos: Deterministic chaos and intermittency can also lead to anomalous diffusion, as shown in .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Healthcare workers, frontline personnel, and other essential workers were identified as the priority groups, followed by the elderly aged 70 and above .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Note that, the real and imaginary parts of the single weak value, are associated with spatial and angular shift of the reflected beam respectively .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "It involves the use of electrical or magnetic currents in the brain to change and stop the pathological neuronal activity .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "V oluntary vaccination decisions are influenced by various factors, such as vaccine cost, peer decisions, personal experiences, and information dissemination from public health institutions , .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Most of these line parameters are based on the molecular constants from the work of Toth , which contains line lists for eight isotopocules of N2O, in the range of 500 – 7500 cm–.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "In recent years, simplicial complexes have been extensively studied and applied in various research fields, such as collaborative networks, semantic networks, cellular networks, and brain networks , , .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Other authors have resorted to a description of the interface properties based on a (Cahn-Hilliard) phase-field model, coupled to immersed-boundary-type discretizations .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The capture radius for point defects is set to be 0.287 nm , and the capture volumes of clusters are the union of the capture distances of their constituent defects, depending on the shape of clusters, which is the basic settings in the MMonCa code.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "In this work, we used mid-infrared frequency comb Fourier transform spectroscopy to measure high-resolution spectra of nitrous oxide obtained from a chemical synthesis involving 15N isotopically enriched and normal 14N precursors.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "In this work, single and small vacancy clusters migrate in 3D mode and vacancy clusters containing more than 5 vacancies are considered immobile, which is a common assumption in OKMC and cluster dynamics simulations .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Hou et al. using object kinetic Monte Carlo simulations, demonstrated that cluster size distributions in cascade debris and the spatial extent of vacancy and SIA clusters in displacement cascades play major role in the evolution of cluster size distributions after long enough time (at 0.1 dpa).",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Particle deformability can be realized in the framework of deformable shell models (applicable to the simulation of capsules) with an immersed-boundary technique as shown e.g. in .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Periodic boundary conditions (PBC) are applied in all three directions of the simulation box, and the grain boundaries are not considered in the current simulation .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Moreover, we expect improvements on the segmentation task by using Mamba-based Unet methods .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The second AnDi challenge took place in 2024 and aimed at evaluating the performances of various methods for detecting and quantifying changes in single-particle motion .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Moderate concentrations of obstacles can cause anomalous diffusion over short arXiv:2412.07299v1 [cond-mat.soft] 10 Dec 2024 2 distances, and the effect becomes more pronounced as the obstacle concentration approaches the percolation threshold ;.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Finally, they thank the RRI Frontiers of Life, which received financial support from the French government in the framework of the University of Bordeaux’s France 2030 program, as well as the Soft Matter Collaborative Research Unit, Frontier Research Center for Advanced Material and Life Science, Faculty of Advanced Life Science, Hokkaido University, Sapporo, Japan, and the CNRS International Research Network between France and India on “Hydrodynamics at small scales: from soft matter to bioengineering”. R.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "T he lower state constants were fixed to values from rotation spectroscopy in our simulations .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Parameters of the ν1 + ν 3 bands of 15N2O obtained from this study together with those obtained by Toth .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The dynamics of infectious diseases spread in a population fundamentally depend on the patterns of interaction – , further influencing vaccination behavior –.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "In that model, the positions of 423 lines and intensities of 392 lines in the 1227 – 3414 cm–1 range were taken from the Toth line list.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "These hypotheses are not compatible with the hypothesis of uniform sum of transition rates formulated in , which holds when the two rates are almost uniform.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Schematic representation of one cycle of the input control signal with epileptiform neuronal activity The response of the memristive crossbar to the input control signal was processed in the same way as was done earlier in the experiments to determine the influence of normal, i.e. without pathologies, in vitro neuronal activity of the hippocampus slices of laboratory mice on the response of the same memristive crossbar .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Other authors have developed approaches for treating conjugate heat transfer problems throughout the fluid and solid domains .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "A comparison between the order of magnitude of the parameters appearing in the two models is discussed in , based on a specific choice for ∆ W.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "In this case a number of conditions for stable integration arise which depend on the solid-fluid density ratio, on the details of the temporal discretization, on the distribution of the Lagrangian markers, and on the choice of the regularized delta function .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "∗ niladri.modak@tuni.fi † nghosh@iiserkol.ac.in dependent modifications in the joint canonical position and/or momentum variable (second moment) of the pointer, unlike just a shift (first moment) in the pointer variable in case of single weak measurement .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The needed laser intensity to cause breakdown due to photoionization using an IR laser is 1011 W/cm2 .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The deposition of materials was performed by electron beam evaporator using selfaligned shadow evaporation technique .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Structural materials experience severe property degradation under neutron irradiation, including radiation induced volume swelling, hardening, embrittlement, creep, phase instability .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Using the fitted center frequencies of 80 transitions from the P and R branches spanning the entire band, we created a spectral model for the ν 1+ν3 band of 15N2O in PGOPHER .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "According to previous research, in gas mixtures with little photo-ionization a streamer discharge tends to travel into the direction of higher electron density .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Initial attempts to apply the immersed boundary method in the context of lattice-Boltzmann simulations were based on the feedback approach due to and alluded to in the present § 5.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The first step is to determine a preliminary velocity field ˜uf , solution of the momentum equation (5.1) in the absence of any immersed-boundary forcing, viz. ˜uf −un f ∆t = RHSn+1/2 . (5.3) 3 Modelling approaches and computational methods for particle-laden turbulent flows (a) (b) (c) δ(1) h ∆x -2 -1 0 1 2 0 0.2 0.4 0.6 0.8 r/∆x Figure 5.1: (a) Discrete delta function kernel δ(1) h (r) used in the interpolation (5.4) and spreading (5.6) of variables between Eulerian and Lagrangian locations: 3-point function of ; 4point function of .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The eigenvalues of the shift matrices ( A and B) provide the maximal centroid shift of the beam when the incident polarization coincides with the corresponding eigenvectors .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The time-dependent S chrödinger equation (here and below, atomic units are used) ( ) Ψ+Ψ      +=∂ Ψ ∂ rVc t Apti /arrowrightnosp /arrowrightnosp /arrowrightnosp 2 ) ( 2 1 (1) was solved by direct numerical integration using th e split-operator method with the fast Fourier transform .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "In fact, most of the experiments involving weak measurements have been performed in classical optical settings for small optical signal amplifications, such as, for the estimation of small phase, quantifying small angular rotations , measuring ultrasmall time delays , capturing tiny beam deflections and so on [11–14].",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Notably, the sum of the transition rates is uniform as in but they are not symmetric, in the sense of , so that the µ-chemoEH can effectively symmetrize the system composed of the two filaments and the motors.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Very recently, empirical line lists have been introduced based on: (i) rovibrational energ y levels using the Measured Active Rotational-Vibrational Energy Levels (Marvel) algorithms for the major 14N2O isotopocule, (ii) the Nitrous Oxide Spectroscopic Line list (NOSL-296) using the effective operator approach , also for the major isotopocule, and (iii) r ovibrational energy levels calculated from an accurate potential energy surface and the ‘Ames-1’ dipole moment surface at 296 K for the twelve N2O isotopocules .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Given the average speed of the bullets, 10 5 m/s , the bullet will move approximately 600 µm during a nanosecond laser pulse and 15 µm during a picosecond laser pulse.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Uhlmann observed that applying the direct forcing immersed-boundary procedure throughout the volume occupied by the solid particle tends to reduce the slip error at the interface for low Reynolds number flows.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "These segment, defined as tug-of-war units , are described by the spatial coordinate ξ ∈ [0, ℓ].",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "In fact, the error shown in figure 5.2( b) remains practically identically when the correction proposed in is applied, and the same is true for the interface flux (not shown).",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "A Gaussian beam when total internally reflecting from a dielectric interface experiences polarization-dependent Goos-H¨ anchen (GH) and Imbert-Fedorov (IF) beam deflection .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "balanced configuration .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The inverted spectra show the models of the four isotopocules simulated using the V oigt lines shape with parameters from the HITRAN2020 database for 14N2O, 15N14NO and 14N15NO, and from the GEISA database for 15N2O.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Nevertheless, Peng et al. show that, when a sufficient grid resolution is used, the two methods are able to provide accurate results for most of turbulent statistics in both the carrier and dispersed phases in turbulent particle-laden flow simulations.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Additionally, these shift measurements are very prone to errors in the beam parameters and the detection geometry .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Rabi measurements were performed prior to the Hahn-echo to obtain the exact π-pulse duration.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "In our model, the infection is restricted to three individuals, i.e., the order of the s-SIR contagion model is up to D = 2, as detailed in the supplementary materials-A .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Although no unique measure of “evenness” of a spatial distribution in three spatial dimensions exists, both iterative approaches as well as explicit constructions on the sphere are available for practical purposes, as discussed e.g. in and .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "We follow the force-sign convention of .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "In order to ensure the second-order spatial accuracy in a bounce-back process, a number of interpolated bounce-back (IBB) schemes have been introduced over the years, with the conditional IBB scheme proposed by Bouzidi et al. and the unified IBB scheme by Yu et al. being the most representative.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "In our work, imitation dynamics is utilized to study the evolution of individual strategies.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "This is because the number of interactions that occur between any two individuals in a social scenario is often greater than the number of interactions they have in the household scenario .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Based on the above analysis, as emphasized by Rosas et al. , the study of higher-order phenomena should focus on both the higher-order interdependencies inferred from observed data and the use of topological data analysis to examine the structure of the system.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "We compared the line center positions and relative intensities to those available in the HITRAN2020 and GEISA databases, as well as the line list of Tashkun et al. , retrieved via the IAO database, and the Ames -1 line list .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "To reproduce a beating frequency close to 20 · 2π rad/s – which is the one shown in – we adjust the internal friction λ to 7 pN · s/µm2 (which is comparable to the one from ).",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "These results align with the ones presented in .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "While initially detectors with electron cooling were suggested for X-ray frequency range , recently it has been predicted that CEBs can be used as single photon detectors with photon wavelengths up to 1 cm .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "In the context of DLM /FD methods, the implementation in is gradient discontinuity capturing and can be viewed as an implicit ghost node method with additional volume forcing.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Let us mention that analogous reference data for a settling oblate spheroid is available , where the authors also discuss the convergence properties of an immersed boundary method. % % % % % % % % 5.6 Comparing PR-DNS methods: a difficult exercise There exists a broad spectrum of PRS methods in the literature and readers may wonder which method to select for their own research.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "doped mode-locked fiber laser (Menlo Systems AB, O range High Power) with a repetition rate frep = 125 MHz, and a Raman-shifted soliton (signal) generated from the same source in a highly non-linear fiber, centered at 1.680 μm .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The household, is typically the scenario with the most frequent collective interactions and highest infection rates across all ages .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Since the method presented in serves as a foundation for many subsequent refinements, we will specify it here in more detail.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Here we focus on a single parameter point which has been proposed as one of the benchmarks in : an isolated sphere of density ratio ρp/ρf = 1.5 is settling at a Galileo numberGa = 178.46, which corresponds to the steady oblique regime of motion, induced by a double-threaded, off-center wake (cf. visualization in figure 5.4).",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The inset shows the line center discrepancies for 15N2O with respect to the Ames-1 line list , which are larger by one order of magnitude .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Also, external radiation like cosmic rays or bremsstrahlung photons from very fast runaway electrons can act as an electron source .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "It can also be seen that the amplitude of the recirculation region is reasonably well reproduced with dp/∆x = 24 when using a standard immersed boundary method .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The relatively large discrepancies for 15N14NO (red, around 3450 cm–1) appear to be due to possible perturbation by the 6ν2 state, based on vibrational state energies listed in for the 14N2O isotopocule and assuming similar relative state energies for the 15N14NO.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Since the early work of Fadlun et al. many researchers have chosen to apply an immersed boundary force only at the fluid-solid interface, while allowing the flow field in the interior of the solid domain to evolve freely.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "For the discrete counterpart Peskin devised regularized delta functions which ensure smooth interpolation and spreading for arbitrarily located force points (cf. also ).",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "This significantly increases negative effects and reduces the effectiveness of stimulation .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The electron density in a regular streamer path is around 10 14 cm−3 .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Both these shifts are very small in magnitude when compared to input beam dimension and thus mimic an ideal weak measurement scenario .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "For example, Esler et al. utilized conventional low.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Patankar has proposed a method which allows to treat the density-matched case ( ρp = ρf ) by computing the rigid particle velocity through integration of the (computational) velocity field over the particle volume, thereby circumventing the above-mentioned singularity (cf. also ) and at the same time obviating the need to explicitly solve the Newton-Euler equations (1.5)-(1.6).",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Bauch et al. proposed an evolutionary game theory model to reveal how imitation by peers influences vaccination decisions.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "For instance, the interaction patterns occurring in schools, conferences, and workplaces differ in frequency and distance, which yields different vaccination outcomes , .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Bubar et al. found that prioritizing efficient transmission-blocking vaccination for adults aged 20-49 can maximally reduce cumulative incidence rates.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The basic idea can be demonstrated by writing the Navier-Stokes momentum equation (1.3) in the following simple semi-discrete form un+1 f −un f ∆t = RHSn+1/2 + fn+1/2 , (5.1) where the superscript indicates a discrete time level,RHS regroups the advective, viscous and pressure terms (which are evaluated at some intermediate time with index n + 1/2), and ∆t is the discrete time step.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "This is only twice higher than the internal NEP of a single bolometers of a similar design .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "BCA simulations were performed by the Stopping and Range of Ions in Matter (SRIM) code with the K-P model to obtain the spatial distribution of the primary knock -on atoms with varied energies of the 5 MeV ion implantation.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "When coupled with OKMC models, this method could offer great efficiency with a considerable level of calculation accuracy .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "This exceeds the optical bullet diameter of 280 ±20 µm , but corresponds to the width of the top hat profile.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "I1|) / I1)∙100% , where In is the value of the current flowing through the memristive crossbar at the reading after applying the nth NAP, I1 is the value of the current flowing through the memristive crossbar at the reading after applying the first NAP.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The terms of O(u3) is included here to ensure that the Navier-Stokes equation is exactly recovered under the Chapman-Enskog approximation .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The derivation partially follows and the result is similar to the ODEs used in .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "This convergence result also applies to the force acting on the submerged body, as has been demonstrated by Zhou & Balachandar .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Considering that awareness is an important factor influencing vaccination, Kabir et al. proposed a framework for vaccine uptake with the unaware-aware (UA) information propagation.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "In addition to the heat balance equations, the electronic temperature can be calculated from the equation for the quasiparticle current , which gives correct results, since in our designs the Andreev current and, respectively, the zero bias current peak, are suppressed due to the use of hybrid Al/Fe absorber .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "One can introduce an important non-dimensional quantity, the Machin number , defined as Ma = L ℓMa , ℓ Ma = \u0012 B θξn \u00131/4 , (28) with ℓMa being the characteristic length of the system.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The in-plane (Longitudinal) Goos-H¨ anchen (GH) shift originates due to angular gradient of the Fresnel coefficients associated with the change of angle of incidence for the non-central wave vectors and the out of the plane (Transverse) Imbert-Fedorov (IF) shifts originates from the spin orbit interaction of light .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "This makes neurostimulation a possible effective method for interrupting pathological hypersynchronous activity in the interconnected neuronal network of the brain .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The predicted lines of the R -branch are too weak relative to the P -branch by up to 40%. 2024-12-10 11 The relative intensities predicted by Ames-1 line list closely follow those of the IAO line list but are weaker overall by 6%.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The Ref. , for example, indicated that increasing adult pertussis booster vaccinations every ten years helps reduce the incidence rate in adults, but only leads to a slight decrease in the rates for infants and toddlers.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Developing personalized vaccination strategies based on different age groups, such as mass vaccination with transmission-blocking vaccines for adults or prioritizing vaccine distribution to the elderly, can improve vaccine effectiveness and suppress infection to some extent .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The first AnDi Challenge took place in 2020 and aimed at assessing the performances of various methods in quantifying anomalous diffusion .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Interestingly, GH and IF shifts always appear simultaneously in case of the reflection or transmission of a realistic light beam owing to their non-separability .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "We also discuss a third feedback mechanism derived as an approximation of the µ-chemoEH one, termed cubic model, which aligns with the model proposed in in the frequency domain.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Additionally, for a TIR, both the Artmann operators ˆA and ˆB for GH and IF shifts are Hermitian and non-commuting , mimicking a joint weak measurement scenario proposed by Resch and Steinberg (see Fig. 1(a)) .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "In particular, while linear curvature feedback was initially considered ideal for fitting Chlamydomonas data , the non-linear sliding feedback model proposed by , which incorporates the attachment and detachment of antagonistic molecular motors, provides a better fit.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Bao et al. have proposed such an ansatz, which, however, requires the solution of additional global Poisson problems at each time step.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "In practical terms one designs simulations such that the Mach number stays below 0.1 . 16 5 Particle-resolved DNS methods 5.4.3 The generalized off-grid DUGKS method In DUGKS, we instead apply a finite-volume treatment to Eq. (5.28).",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Additionally, in countries such as Austria, France, and Portugal, the interval for booster shots for individuals aged 65 and above is shortened, as antibody levels decline more rapidly with age .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "This assumption is based on the fact that, under the effect of homophily , individuals tend to imitate the behavior of their peers.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Focusing on the spreading dynamics on the simplicial susceptibleinfected-recovered (s-SIR) model, Palafox-Castillo et al. defined a stochastic model to study variations beyond contagion processes on simplicial networks.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "To obtain this reduction in the number of unknowns, we define both the transition rates and the potentials as cosine and/or sine functions, following .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Zuo et al. , for example, established a cost-benefit analysis function that represents vaccine decisionmaking, and found that vaccination information provides a favorable reference in individuals’ decision-making regarding vaccine uptake.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The method to estimate ⟨X⟩, ⟨Y ⟩, ⟨XY ⟩, ⟨XPy⟩ from an experimentally obtained image of the light beam is covered in the following .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "However, the electric field of the discharge is influenced by the presence of the target .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Annealing of the samples can improve the properties of NV centers, as demonstrated by Meng et al. .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "This higher abundance of the heavier isotopocules compared to the lighter major isotopocule is attributed to the kinetic isotope effect of the HNO dimerization reactions involved in the formation of N2O .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Alternative force-based immersed boundary methods in an LB context have been proposed by e.g. .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "So far, laser diagnostics, including E-FISH, are generally considered non-invasive when applied to plasma .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "This is done by minimizing instrumental line shape distortions in the interleaved spectra, and here we used the methodology described in , which also allows for correcting the nonlinear mapping of the comb frequencies into the FTS spectrum.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Characteristics of the used plasma source can be found in .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Due to its support of only three grid points in each linear dimension the variant stated in has been widely used for PR-DNS.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "Zhou & Balachandar have considered the volume∆Vl in (5.6) as an ajustable weight factor, and their analysis then somewhat surprisingly shows that the optimal choice (from the point of view of e fficiency) corresponds to the minimum marker density.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The authors are grateful to the Joint Supercomputer Cen ter of RAS for the provided supercomputer sources. T.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "substituted minor isotopocules (14N217O, 14N218O, 14N15N16O, and 15N14N16O), have recently been included in the ExoMol database .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "I NTRODUCTION The global outbreak of COVID-19 has disrupted the order in all countries and communities, posing a significant threat to human life and property –.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  },
  {
    "original_claim": "The value of specific heat ratio can be adjusted by introducing additional internal degrees of freedom , and the value of Prandtl number can be adjusted in several ways, e.g., by altering the equilibrium distribution or by adding a properly designed source term .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": null
  }
]