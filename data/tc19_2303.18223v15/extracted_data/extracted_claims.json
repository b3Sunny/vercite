[
  {
    "original_claim": "Given a compute budget c, they empirically presented three basic formulas for the scaling law6: L(N) = \u0012Nc N \u0013αN , α N ∼ 0.076, Nc ∼ 8.8 × 1013 (1) L(D) = \u0012Dc D \u0013αD , α D ∼ 0.095, Dc ∼ 5.4 × 1013 L(C) = \u0012Cc C \u0013αC , α C ∼ 0.050, Cc ∼ 3.1 × 108 where L(·) denotes the cross entropy loss in nats, and a follow-up study [58] from OpenAI has shown that the language modeling loss can be decomposed into two parts, namely irreducible loss (the entropy of the true data distribution) and reducible loss (an estimate of the KL divergence between the true and model distributions).",
    "context_before": [
      "In 2020, Kaplan et al. (the OpenAI team) firstly proposed to model the power-law relationship of model performance with respective to three major factors, namely model size (N), dataset size (D), and the amount of training compute ( C), for neural language models."
    ],
    "context_after": [
      "The three laws were derived by fitting the model performance with varied data sizes (22M to 23B tokens), model sizes (768 to 1.5B nonembedding parameters) and training compute, under some assumptions ( e.g., the analysis of one factor should be not bottlenecked by the other two factors)."
    ],
    "references": [
      "58"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For instance, small proxy models can be trained to find the optimal schedule of the data mixture for large models [59].",
    "context_before": [
      "Firstly, for large models, it is infeasible to rigorously examine various training tricks or variants, and it would be very helpful if experiences gained from small models could also apply to large models."
    ],
    "context_after": [
      "Secondly, the training of large-scale models takes a long time, often suffering from issues such as training loss spike, and scaling law can be employed to monitor the training status of LLMs, e.g., identifying abnormal performance at an early time."
    ],
    "references": [
      "59"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "An empirical study [58] from the OpenAI team has shown that representation quality or semantic content can still effectively improve even if approaching the point of diminishing returns ( i.e., approaching the irreducible loss) [58].",
    "context_before": [
      "Despite that scaling law characterizes a smooth trend of performance increase (or loss decrease), it also indicates that diminishing returns 7 might occur as model scaling."
    ],
    "context_after": [
      "This finding suggests that training large models are promising for improving the performance of downstream tasks."
    ],
    "references": [
      "58"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Thus, it will be meaningful to study how scaling laws apply to a data-constrained regime [61], where data repetition or augmentation might be useful to alleviate data scarcity.",
    "context_before": [
      "With the ever-increasing model scale, the public text data would be soon “exhausted” for LLMs ."
    ],
    "context_after": [
      "Task-level predictability."
    ],
    "references": [
      "61"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Thus, a basic problem is that how the decrease of language modeling loss translates into the improvement of task performance [58].",
    "context_before": [
      "Existing research of scaling laws are mostly conducted in terms of language modeling loss (e.g., per-token cross-entropy loss in nats ), while in practice we are more concerned about the performance of LLMs on actual tasks."
    ],
    "context_after": [
      "Intuitively, a model with a smaller language modeling loss tends to yield a better performance on downstream tasks, since language modeling loss can be considered as a general measure of the overall model capacity."
    ],
    "references": [
      "58"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Grokking refers that “a pattern in the data, improving generalization performance from random chance level to perfect generalization”, quoted from the original paper [73].",
    "context_before": [
      "Such a learning process is essentially not smooth and stable (i.e., language ability does not develop at a constant rate over time), though a child actually grows."
    ],
    "context_after": [
      "This explanation is only for ease of understanding, and there is not direct evidence to connect the two points. every day."
    ],
    "references": [
      "73"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "CodeGen [86] Mar-2022 16.",
    "context_before": [
      "512 TPU v3 27 h ✓."
    ],
    "context_after": [
      "577B tokens."
    ],
    "references": [
      "86"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "CodeGeeX [92] Sep-2022 13.",
    "context_before": [
      "✓."
    ],
    "context_after": [
      "850B tokens."
    ],
    "references": [
      "92"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Pythia [96] Apr-2023 12.",
    "context_before": [
      "2048 80G A100 21 d ✓."
    ],
    "context_after": [
      "300B tokens."
    ],
    "references": [
      "96"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "✓ ✓ LLaMA2 [99] Jul-2023 70.",
    "context_before": [
      "512 40G A100."
    ],
    "context_after": [
      "✓ ✓ 2T tokens."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Baichuan2 [100] Sep-2023 13.",
    "context_before": [
      "✓."
    ],
    "context_after": [
      "✓ ✓ 2.6T tokens."
    ],
    "references": [
      "100"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "QWEN [101] Sep-2023 14.",
    "context_before": [
      "✓."
    ],
    "context_after": [
      "✓ ✓ 3T tokens."
    ],
    "references": [
      "101"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "FLM [102] Sep-2023 101.",
    "context_before": [
      "✓."
    ],
    "context_after": [
      "✓."
    ],
    "references": [
      "102"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Publicly Available Skywork [103] Oct-2023 13.",
    "context_before": [
      "192 A800 22 d ✓."
    ],
    "context_after": [
      "3.2T tokens."
    ],
    "references": [
      "103"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Yuan 1.0 [109] Oct-2021 245.",
    "context_before": [
      "128 TPU v3 60 h ✓."
    ],
    "context_after": [
      "180B tokens."
    ],
    "references": [
      "109"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Closed Source PaLM2 [120] May-2023 16.",
    "context_before": [
      "512 Ascend 910 100 d ✓."
    ],
    "context_after": [
      "✓."
    ],
    "references": [
      "120"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Shortly after the release of this RL paper [79], the paper of the Proximal Policy Optimization (PPO) [128] was published in July 2017, which now has been the foundational RL algorithm for learning from human preferences [66].",
    "context_before": [
      "The related research of human alignment can be dated back to the year 2017 (or earlier) for OpenAI: a blog article entitled “learning from human preferences”18 was posted on the OpenAI blog describing a work that applied reinforcement learning (RL) to learn from the preference comparisons annotated by humans (similar to the reward training step in the aligning algorithm of InstructGPT in Figure 12)."
    ],
    "context_after": [
      "Later in January 2020, GPT-2 was finetuned using the aforementioned RL algorithms , which leveraged human preferences to improve the capacities of GPT-2 on NLP tasks."
    ],
    "references": [
      "128"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "From LLaMA [57], LLaMA-2 [99], LLaMA-3 [135] to LLaMA-3.1 [136], continuous updates have been made and the development is still ongoing.",
    "context_before": [
      "The LLaMA series of models has gained immense popularity and widespread attention due to its openness and effectiveness."
    ],
    "context_after": [
      "With increased parameters (the largest version has 405B), more pre-training tokens (15T tokens), and an extended context window (128K), LLaMA-3.1 has significantly enhanced its capabilities, and it also integrates additional components that work in synergy with the model, including new security and safety tools."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "RefinedWeb [171] is a web dataset obtained through rigorous selection and deduplication based on data from Common Crawl, encompassing all Common Crawl web records from 2008 to June 2023, totaling around 5T tokens.",
    "context_before": [
      "RefinedWeb."
    ],
    "context_after": [
      "The open-source portion consists of 600B tokens, with a data size of approximately 500GB."
    ],
    "references": [
      "171"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition to book data, scientific publication data such as paper is also important for model pre-training. arXiv Dataset [172] is a corpus of 1.7 million academic papers, covering a wide range of papers in the fields of physics, mathematics, and computer science.",
    "context_before": [
      "Academic Data."
    ],
    "context_after": [
      "S2ORC is a corpora that consists of 136M academic papers collected by Semantic Scholar."
    ],
    "references": [
      "172"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "CodeGen has utilized BIGQUERY [86], a subset of the BigQuery dataset, for training the multilingual version of CodeGen (CodeGen-Multi).",
    "context_before": [
      "Google has publicly released the BigQuery dataset , which includes a substantial number of open-source licensed code snippets in various programming languages, serving as a representative code dataset."
    ],
    "context_after": [
      "In addition, Hugging Face has collected and released a code dataset named The Stack , covering more than 30 programming languages."
    ],
    "references": [
      "86"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition, Hugging Face has collected and released a code dataset named The Stack [175], covering more than 30 programming languages.",
    "context_before": [
      "CodeGen has utilized BIGQUERY , a subset of the BigQuery dataset, for training the multilingual version of CodeGen (CodeGen-Multi)."
    ],
    "context_after": [
      "The Stack is continuously updated, and the v1.2 version has expanded to 358 programming languages."
    ],
    "references": [
      "175"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "The Pile dataset is widely used in models with different parameter scales, such as GPT-J (6B) [176], CodeGen (16B) [86], and Megatron-Turing NLG (530B) [113].",
    "context_before": [
      "It is constructed from 22 diverse high-quality subsets."
    ],
    "context_after": [
      "ROOTS is composed of various smaller datasets (totally 1.61 TB of text) and covers 59 different languages (containing natural languages and programming languages), which have been used for training BLOOM ."
    ],
    "references": [
      "86"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Another mixture dataset is Dolma [177], which includes web text from Common Crawl, academic papers from Semantic Scholar, GitHub code, books, social media from Reddit, and Wikipedia data.",
    "context_before": [
      "ROOTS is composed of various smaller datasets (totally 1.61 TB of text) and covers 59 different languages (containing natural languages and programming languages), which have been used for training BLOOM ."
    ],
    "context_after": [
      "Dolma consisting of 3T tokens of approximately 200TB of raw text and has been used to train OLMo ."
    ],
    "references": [
      "177"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Dolma consisting of 3T tokens of approximately 200TB of raw text and has been used to train OLMo [178].",
    "context_before": [
      "Another mixture dataset is Dolma , which includes web text from Common Crawl, academic papers from Semantic Scholar, GitHub code, books, social media from Reddit, and Wikipedia data."
    ],
    "context_after": [
      "In practice, it commonly requires a mixture of different data sources for pre-training LLMs (see Figure 6), instead of a single corpus."
    ],
    "references": [
      "178"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Inst. [88] Apr-2022 5M MVPCorpus [181] Jun-2022 41M xP3 [94] Nov-2022 81M OIG[182] Mar-2023 43M Chat HH-RLHF [183] Apr-2022 160K HC3 [184] Jan-2023 87K ShareGPT [153] Mar-2023 90K Dolly [185] Apr-2023 15K OpenAssistant [186] Apr-2023 161K Synthetic Self-Instruct [147] Dec-2022 82K Alpaca [187] Mar-2023 52K Guanaco [188] Mar-2023 535K Baize [189] Apr-2023 158K BELLE [190] Apr-2023 1.5M TABLE 4: A list of available collections for alignment.",
    "context_before": [
      "Inst. Apr-2021 193K FLAN Sep-2021 4.4M P3 Oct-2021 12.1M Super Nat."
    ],
    "context_after": [
      "Dataset Release Time #Examples Summarize from Feedback Sep-2020 193K SHP Oct-2021 385K WebGPT Comparisons Dec-2021 19K Stack Exchange Preferences Dec-2021 10M HH-RLHF Apr-2022 169K Sandbox Alignment Data May-2023 169K CValues Jul-2023 145K PKU-SafeRLHF Oct-2023 330K 3.3 Commonly Used Datasets for Fine-tuning After pre-training, it requires further fine-tuning LLMs to enhance the model capacity, which often involve two major steps, namely instruction tuning (supervised fine-tuning) and alignment tuning."
    ],
    "references": [
      "189",
      "184",
      "190",
      "186"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Dataset Release Time #Examples Summarize from Feedback [129] Sep-2020 193K SHP [191] Oct-2021 385K WebGPT Comparisons [81] Dec-2021 19K Stack Exchange Preferences [192] Dec-2021 10M HH-RLHF [183] Apr-2022 169K Sandbox Alignment Data [193] May-2023 169K CValues [194] Jul-2023 145K PKU-SafeRLHF [195] Oct-2023 330K 3.3 Commonly Used Datasets for Fine-tuning After pre-training, it requires further fine-tuning LLMs to enhance the model capacity, which often involve two major steps, namely instruction tuning (supervised fine-tuning) and alignment tuning.",
    "context_before": [
      "Inst. Apr-2022 5M MVPCorpus Jun-2022 41M xP3 Nov-2022 81M OIG Mar-2023 43M Chat HH-RLHF Apr-2022 160K HC3 Jan-2023 87K ShareGPT Mar-2023 90K Dolly Apr-2023 15K OpenAssistant Apr-2023 161K Synthetic Self-Instruct Dec-2022 82K Alpaca Mar-2023 52K Guanaco Mar-2023 535K Baize Apr-2023 158K BELLE Apr-2023 1.5M TABLE 4: A list of available collections for alignment."
    ],
    "context_after": [
      "In this section, we mainly focus on discussing the related available datasets for the two kinds of tuning approaches, and more algorithm details can be found in Section."
    ],
    "references": [
      "195"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Recently, FLAN-v2 [197] is also proposed, which expands FLAN by mixing additional instruction datasets, including Muffin [67], NIV2 [88], T0-SF [28], and CoT [198–200].",
    "context_before": [
      "FLAN consists of 62 widely used NLP benchmarks in its original version."
    ],
    "context_after": [
      "Muffin contains 62 tasks from the original FLAN and additional 26 tasks, including conversation and code synthesis tasks."
    ],
    "references": [
      "197"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In this category, ShareGPT [153], OpenAssistant [186] and Dolly [185] are three commonly used datasets for LLM fine-tuning.",
    "context_before": [
      "The conversation types include open-ended generation, question answering, brainstorming, and chatting."
    ],
    "context_after": [
      "ShareGPT is collected from a data collection platform where users can upload their conversations with ChatGPT or GPT-4 through the ShareGPT API."
    ],
    "references": [
      "186"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "OpenAssistant [186] is a multilingual corpus containing 66,497 real-world conversation trees between human and AI assistant.",
    "context_before": [
      "Currently, this dataset consists of approximately 90,000 conversations, including real instructions or inquiries from human and responses from ChatGPT."
    ],
    "context_after": [
      "Each conversation tree consists of multiple nodes, and each node represents the information generated by a role in the dialogue."
    ],
    "references": [
      "186"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In this category, Self-Instruct52K [147], Alpaca [146] and Baize [189] are three commonly used synthetic datasets for LLMs.",
    "context_before": [
      "This kind of datasets are typically constructed by instructing LLMs, based on pre-defined guidance rules or methods."
    ],
    "context_after": [
      "Self-Instruct-52K is an instruction dataset generated through the self-instruct method, consisting of 82,000 instances with 52,000 instructions."
    ],
    "references": [
      "189"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Baize [189] is an English multi-turn conversation corpus constructed using ChatGPT, comprising 111.5K instances.",
    "context_before": [
      "Moreover, 60% of the examples are pure instructions without the input part in the final dataset."
    ],
    "context_after": [
      "To create Baize, a method called “self-chat” is purposed, where ChatGPT takes on the roles of both the user and the AI assistant in turns, generating information in a conversational format. 3.3.2 Alignment Datasets Apart from instruction tuning, it is important to construct high-quality datasets for aligning LLMs with human values and preferences ( e.g., helpfulness, honesty, and harmlessness)."
    ],
    "references": [
      "189"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To create Baize, a method called “self-chat” [189] is purposed, where ChatGPT takes on the roles of both the user and the AI assistant in turns, generating information in a conversational format. 3.3.2 Alignment Datasets Apart from instruction tuning, it is important to construct high-quality datasets for aligning LLMs with human values and preferences ( e.g., helpfulness, honesty, and harmlessness).",
    "context_before": [
      "Baize is an English multi-turn conversation corpus constructed using ChatGPT, comprising 111.5K instances."
    ],
    "context_after": [
      "In this section, we introduce several widely used datasets for alignment tuning, including HH-RLHF , SHP , PKU-SafeRLHF , Stack Exchange Preferences and Sandbox Alignment Data ."
    ],
    "references": [
      "189"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In this section, we introduce several widely used datasets for alignment tuning, including HH-RLHF [183], SHP [191], PKU-SafeRLHF [195], Stack Exchange Preferences [192] and Sandbox Alignment Data [193].",
    "context_before": [
      "To create Baize, a method called “self-chat” is purposed, where ChatGPT takes on the roles of both the user and the AI assistant in turns, generating information in a conversational format. 3.3.2 Alignment Datasets Apart from instruction tuning, it is important to construct high-quality datasets for aligning LLMs with human values and preferences ( e.g., helpfulness, honesty, and harmlessness)."
    ],
    "context_after": [
      "We show their details in Table."
    ],
    "references": [
      "195"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "PKU-SafeRLHF [195] encompasses more than 330K instances of expert comparison data, concentrating on the helpfulness and harmlessness.",
    "context_before": [
      "Different from HH-RLHF , the data in SHP consists of naturally occurring and humanwritten responses."
    ],
    "context_after": [
      "Each instance in the dataset includes a question and two responses, accompanied by safety labels for each response and two preference annotations between the two responses according to helpfulness and harmlessness."
    ],
    "references": [
      "195"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "DeepSpeed-Chat [210] is a fast, cost-effective, and easy-to-use system framework that enables the integration of the complete RLHF process during model training.",
    "context_before": [
      "It currently supports over 13,000 models across three popular model architectures, such as LLaMA , Mistral , and OPT ."
    ],
    "context_after": [
      "It is featured by three major functionalities: (1) it simplifies the training and inference process for ChatGPT-like models, enabling using a simple script to implement multiple training or inference steps; (2) it replicates the training mode of InstructGPT and provides a complete pipeline for three training steps (i.e., SFT, reward model fine-tuning, and RLHF); (3) it integrates the training engine and inference engine of Deepspeed into a unified hybrid engine (Deepspeed HE) for RLHF training, which enables seamless switch between training and inference modes, and leveraging various optimizations from DeepSpeed Inference."
    ],
    "references": [
      "210"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "FLM [102] mixes Chinese and English corpora in nearly equal proportions.",
    "context_before": [
      "For example, BLOOM and PaLM have curated multilingual data covering 46 and 122 languages, respectively, within their pre-training corpora."
    ],
    "context_after": [
      "These models demonstrate impressive performance in multilingual tasks, such as translation, multilingual summarization, and multilingual question answering, and achieve comparable or superior performance to the state-of-theart models that are fine-tuned on the corpus in the target language(s)."
    ],
    "references": [
      "102"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "By pretraining on a vast amount of scientific text, LLMs can achieve impressive performance in scientific and reasoning tasks [219].",
    "context_before": [
      "In order to enhance the understanding of scientific knowledge for LLMs , it is useful to incorporate a scientific corpus for model pre-training ."
    ],
    "context_after": [
      "To construct the scientific corpus, existing efforts mainly collect arXiv papers, scientific textbooks, math webpages, and other related scientific resources."
    ],
    "references": [
      "219"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition to the above methods, LLMs (especially relatively small models) can be also employed for data selection, either by computing perplexity [232] or directly prompting LLMs [233] for measuring the sample importance.",
    "context_before": [
      "Based on specific keyword set, the noisy or unuseful elements in the text, such as HTML tags, hyperlinks, boilerplates, and offensive words, can be identified and removed."
    ],
    "context_after": [
      "However, using LLMs is unavoidably computationally intensive for large-scale data selection."
    ],
    "references": [
      "233",
      "232"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For example, Falcon [171] is trained on pure webpages, and CodeGen [86] largely increases the amount of code data.",
    "context_before": [
      "Furthermore, special data mixtures can be used to facilitate different purposes."
    ],
    "context_after": [
      "In practice, data mixture is often determined empirically, and we summarize several common strategies for finding an effective data mixture as follows:."
    ],
    "references": [
      "171",
      "86"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To further examine the effect of different data sources, some studies have conducted ablation experiments by removing each data source one by one, and pre-train LLMs with specially curated datasets [227].",
    "context_before": [
      "In contrast, increasing the data source heterogeneity ( e.g., including diverse data sources) is critical for improving the downstream performance of LLMs ."
    ],
    "context_after": [
      "It has been shown that dropping data sources with high heterogeneity ( e.g., webpages) impacts LLM’s abilities more severely than dropping sources with low heterogeneity (e.g., academic corpus)."
    ],
    "references": [
      "227"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Given the target downstream tasks, one can select pre-training data with either higher proximity in the feature space [250] or those that provide positive influences on downstream task performance [251].",
    "context_before": [
      "In addition to manually setting the data mixtures, several studies have proposed to optimize the data mixtures for improving the model pretraining ."
    ],
    "context_after": [
      "Further, to reduce the reliance of target tasks, DoReMi first trains a small reference model using given initial domain weights, and then trains another small proxy model, upweighting the domains on which the greatest discrepancies in likelihood 21 between the two models are observed."
    ],
    "references": [
      "250",
      "251"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Further, to reduce the reliance of target tasks, DoReMi [59] first trains a small reference model using given initial domain weights, and then trains another small proxy model, upweighting the domains on which the greatest discrepancies in likelihood 21 between the two models are observed.",
    "context_before": [
      "Given the target downstream tasks, one can select pre-training data with either higher proximity in the feature space or those that provide positive influences on downstream task performance ."
    ],
    "context_after": [
      "Finally, the learned domain weights of the proxy model are applied to train a much larger LLM."
    ],
    "references": [
      "59"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To improve the coding ability of LLMs, CodeLLaMA [254] is developed based on LLaMA 2 [99] (2T general tokens → 500B code-heavy tokens), aiming to improve the code generation ability and retain natural language understanding skills.",
    "context_before": [
      "Coding."
    ],
    "context_after": [
      "CodeLLaMA also provides a version that is further specialized to a certain programming language, namely CodeLLaMA-Python (2T general tokens → 500B code-heavy tokens → 100B Python-heavy tokens)."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Llemma [258] is proposed to enhance the mathematical capacities of general-purpose LLMs.",
    "context_before": [
      "Mathematics."
    ],
    "context_after": [
      "It is developed based on CodeLLaMA."
    ],
    "references": [
      "258"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Although CodeLLaMA [254] mainly focuses on the coding ability, experiments have shown that it performs better than its base model LLaMA 2 on mathematics benchmarks [258].",
    "context_before": [
      "It is developed based on CodeLLaMA."
    ],
    "context_after": [
      "Based on CodeLLaMA, Llemma is continually trained on mixtures of scientific papers, web data containing mathematical text and code (2T general tokens → 500B code-heavy tokens → 50∼200B math-heavy tokens)."
    ],
    "references": [
      "258"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Although Falcon [171] shows that webpages alone can be employed to train powerful LLMs, a more typical approach is to also incorporate diverse high-quality text like code, books, scientific papers,etc.",
    "context_before": [
      "It is suggested to include diverse data sources in the pre-training data."
    ],
    "context_after": [
      "If a LLM is specialized with a certain skill, the proportion of corresponding data source should be increased accordingly."
    ],
    "references": [
      "171"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "PaLM [44] and LaMDA [68] use approximately 50% conversational data.",
    "context_before": [
      "For example, Gopher and Chinchilla are trained with approximately 40% of data from books."
    ],
    "context_after": [
      "Data cleaning."
    ],
    "references": [
      "44"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In practice, both heuristic and classifier-based methods can be employed for quality and toxicity filtering ( e.g., CCNet [260], fastText [261], and Data-Juicer [262]).",
    "context_before": [
      "Second, low-quality text, toxic content, and data with privacy concerns should be removed at different granularities (e.g., document, passage or sentence)."
    ],
    "context_after": [
      "Third, with the cleaned data, one can further unify or specify the format for pretraining data, and perform the tokenization by training the tokenizer on the filtered and deduplicated corpus with libraries like SentencePiece ."
    ],
    "references": [
      "262"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To determine both settings, a practical way is to first train several small language models with multiple candidate plans and then select a good plan among them [59].",
    "context_before": [
      "With the preprocessed data, the next step is to determine the data mixture and the specific order 22 of data for pre-training LLMs."
    ],
    "context_after": [
      "Overall, it is more difficult to find a suitable data curriculum."
    ],
    "references": [
      "59"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Thus, several variants of SSM have been proposed, including Mamba [272], RetNet [271], RWKV [273], 23 TABLE 5: Model cards of several selected LLMs with public configuration details.",
    "context_before": [
      "Despite the high computation efficiency of SSMs, their performance still lags behind Transformer."
    ],
    "context_after": [
      "Here, PE denotes position embedding, #L denotes the number of layers, #H denotes the number of attention heads, dmodel denotes the size of hidden states, and MCL denotes the maximum context length during training."
    ],
    "references": [
      "273",
      "271"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Jurassic-1 [107] Causal decoder 178B Pre LayerNorm Learned GeLU ✓ 76 96 13824 2048 LLaMA [57] Causal decoder 65B Pre RMSNorm RoPE SwiGLU × 80 64 8192 2048 LLaMA 2 [99] Causal decoder 70B Pre RMSNorm RoPE SwiGLU × 80 64 8192 4096 Falcon [171] Causal decoder 40B Pre LayerNorm RoPE GeLU × 60 64 8192 2048 GLM-130B [93] Prefix decoder 130B Post DeepNorm RoPE GeGLU ✓ 70 96 12288 2048 T5 [82] Encoder-decoder 11B Pre RMSNorm Relative ReLU × 24 128 1024 512 Decoder Encoder Decoder Causal Decoder Prefix Decoder Encoder-Decoder Language ModelsA Survey of Large Language ModelsA Survey of Large Language ModelsA Survey of Large LanguageModels ASurveyofLarge LanguageModels ASurveyofLarge LanguageModels ASurveyofLarge EncoderDecoder Decoder Decoder Decoder Fig. 9: A comparison of the attention patterns in three mainstream architectures.",
    "context_before": [
      "64 128 8192."
    ],
    "context_after": [
      "Here, the blue, green, yellow and grey rounded rectangles indicate the attention between prefix tokens, attention between prefix and target tokens, attention between target tokens, and masked attention respectively."
    ],
    "references": [
      "99",
      "171"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "RWKV .RWKV [273] combines the advantages of Transformer and RNN.",
    "context_before": [
      "Compared with traditional SSMs, Mamba has demonstrated improved text modeling capacities."
    ],
    "context_after": [
      "It employs time-mixing modules, i.e., RNN with gating, and channel-mixing modules that are special feedforward neural networks ."
    ],
    "references": [
      "273"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "It employs time-mixing modules, i.e., RNN with gating, and channel-mixing modules that are special feedforward neural networks [273].",
    "context_before": [
      "RWKV .RWKV combines the advantages of Transformer and RNN."
    ],
    "context_after": [
      "Within these modules, token shift, a linear combination of the current and previous token, is used instead of the token representation as the input."
    ],
    "references": [
      "273"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "RetNet [271] proposes multi-scale retention (MSR) to replace the attention module in Transformer.",
    "context_before": [
      "RetNet."
    ],
    "context_after": [
      "Similar to linear attention, in the MSR module, the input is first mapped into query, key, and value, and the product of key and value is employed to update the state."
    ],
    "references": [
      "271"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In existing LLMs, GeLU activations [289] are widely used.",
    "context_before": [
      "To obtain good performance, activation functions also need to be properly set in feed-forward networks."
    ],
    "context_after": [
      "Specially, in the latest LLMs ( e.g., PaLM and LaMDA), variants of GLU activation have also been utilized, especially the SwiGLU and GeGLU variants, which often achieve better performance in practice ."
    ],
    "references": [
      "289"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Configuration Method Equation Normalization position Post Norm [22] Norm(x+Sublayer(x)) Pre Norm [26] x + Sublayer(Norm(x)) Sandwich Norm [274] x + Norm(Sublayer(Norm(x))) Normalization method LayerNorm [275] x−µ σ · γ + β, µ = 1 d Pd i=1 xi, σ = q 1 d Pd i=1(xi − µ))2 RMSNorm [276] x RMS(x) · γ, RMS(x) = q 1 d Pd i=1 x2 i DeepNorm [277] LayerNorm(α · x + Sublayer(x)) Activation function ReLU [278] ReLU(x) = max(x, 0) GeLU [279] GeLU(x) = 0.5x ⊗ [1 + erf(x/ √ 2)], erf(x) = 2√π Rx 0 e−t2 dt Swish [280] Swish(x) = x ⊗ sigmoid(x) SwiGLU [281] SwiGLU(x1, x2) = Swish(x1) ⊗ x2 GeGLU [281] GeGLU(x1, x2) = GeLU(x1) ⊗ x2 Position embedding Absolute [22] xi = xi + pi Relative [82] Aij = WqxixT j WT k + ri−j RoPE [282] Aij = WqxiRΘ,i−jxT j WT k = (WqxiRΘ,i)(WkxjRΘ,j)T ALiBi [283] Aij = WqxixT j WT k − m(i − j) RoPE defines the basis θi as an exponentiation of the base b (set to 10000 by default): Θ = {θi = b−2(i−1)/d|i ∈ {1, 2, . . . , d/2}}. (4) Furthermore, a recent study [295] defines the distance required to rotate one cycle ( 2π) for each dimension as wavelength: λi = 2πb2(i−1)/d = 2π/θi. (5) Due to the excellent performance and the long-term decay property, RoPE is widely adopted in the latest LLMs, e.g., PaLM [56] and LLaMA [57].",
    "context_before": [
      "Here, Sublayer denotes a FFN or a self-attention module in a Transformer layer, d denotes the size of hidden states, pi denotes position embedding at position i, Aij denotes the attention score between a query and a key, ri−j denotes a learnable scalar based on the offset between the query and the key, and RΘ,t denotes a rotary matrix with rotation degree t · Θ."
    ],
    "context_after": [
      "Based on RoPE, xPos further improves the translation invariance and length extrapolation of Transformer."
    ],
    "references": [
      "280"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To make a trade-off between multi-query attention and multi-head attention, grouped-query attention (GQA) [301] has been explored.",
    "context_before": [
      "Representative models with multi-query attention include PaLM and StarCoder ."
    ],
    "context_after": [
      "In GQA, heads are assigned into different groups, and those heads that belong to the same group will share the same transformation matrices."
    ],
    "references": [
      "301"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Specially, GQA has been adopted and empirically tested in the recently released LLaMA 2 model [99].",
    "context_before": [
      "In GQA, heads are assigned into different groups, and those heads that belong to the same group will share the same transformation matrices."
    ],
    "context_after": [
      "FlashAttention."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "The updated version FlashAttention-2 [303] further optimizes the work partitioning of GPU thread blocks and warps, leading to around 2 × speedup when compared to the original FlashAttention.",
    "context_before": [
      "Implemented as a fused kernel in CUDA, FlashAttention has been integrated into 26 PyTorch , DeepSpeed , and Megatron-LM ."
    ],
    "context_after": [
      "PagedAttention."
    ],
    "references": [
      "303"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "MoD has been applied in the latest PaLM 2 model [120]. 4.2.4 Decoding Strategy After the LLMs have been pre-trained, it is essential to employ a specific decoding strategy to generate the appropriate output from the LLMs.",
    "context_before": [
      "For input sentences started with different special tokens ( i.e., {[R], [S], [X]}), the model will be optimized using the corresponding denoisers."
    ],
    "context_after": [
      "Background."
    ],
    "references": [
      "120"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Additionally, Dolly [185] and OpenAssistant [186] have further released their conversation data, which has been carefully labeled by human annotators to attain a high level of quality.",
    "context_before": [
      "A notable example of such a dataset is the conversational data from ShareGPT ."
    ],
    "context_after": [
      "Formatting Synthetic Data."
    ],
    "references": [
      "186"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Furthermore, Self-Align [336] establishes multiple human-aligned principles to filter the synthesized instances.",
    "context_before": [
      "To improve the quality of synthetic instructions, WizardLM introduces Evol-Instruct by proposing in-depth and in-breadth evolving to enrich the complexity and diversity of the instances."
    ],
    "context_after": [
      "It then employs these instances to train a LLM in order to yield more aligned instances."
    ],
    "references": [
      "336"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To summarize, diversity and quality of instructions are important factors to consider when scaling the number of instances [338].",
    "context_before": [
      "LESS computes gradients for both downstream validation and training instruction data, to evaluate the contribution of instruction data based on extensions of influence function ."
    ],
    "context_after": [
      "As the capacities of LLMs improve, data synthesis methods have become the mainstream approach for generating large amount of instruction data."
    ],
    "references": [
      "338"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition to the above practical strategies and tricks, existing work has also used other tricks, e.g., concatenating multiple examples into a single sequence to approach the max length [353]. 34 5.1.3 The Effect of Instruction Tuning In this part, we discuss the effect of instruction tuning on LLMs in three major aspects.",
    "context_before": [
      "It is also feasible to prefix the input with the selfidentification prompt, e.g., “The following is a conversation between a human and an AI assistant called CHATBOT NAME , developed by DEVELOPER .”, where C HATBOT NAME and D EVELOPER refer to the name and developer of the chatbot, respectively."
    ],
    "context_after": [
      "Performance Improvement."
    ],
    "references": [
      "353"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For instance, researchers propose to fine-tune Flan-PaLM [69] using medical datasets to create Med-PaLM [354], a medical knowledge assistant that achieves performance levels comparable to those of expert clinicians.",
    "context_before": [
      "Instruction tuning is an effective approach to adapting existing general LLMs to be domain-specific experts."
    ],
    "context_after": [
      "Furthermore, a recent study fine-tunes FLAN-T5 to support e-commerce recommender systems with natural language instructions, showing strong performance in a variety of recommendation tasks."
    ],
    "references": [
      "354"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "There are also several open-sourced medical models instructiontuned based on LLaMA [57], such as BenTsao [356].",
    "context_before": [
      "Furthermore, a recent study fine-tunes FLAN-T5 to support e-commerce recommender systems with natural language instructions, showing strong performance in a variety of recommendation tasks."
    ],
    "context_after": [
      "Also, researchers explore instruction tuning on law , finance , and arithmetic computation . 5.1.4 Empirical Analysis for Instruction Tuning Fine-tuning LLMs with different instruction sets tend to lead to model variants with varied performance on downstream tasks."
    ],
    "references": [
      "356"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Also, researchers explore instruction tuning on law [357], finance [358], and arithmetic computation [359]. 5.1.4 Empirical Analysis for Instruction Tuning Fine-tuning LLMs with different instruction sets tend to lead to model variants with varied performance on downstream tasks.",
    "context_before": [
      "There are also several open-sourced medical models instructiontuned based on LLaMA , such as BenTsao ."
    ],
    "context_after": [
      "In this section, we will explore the effect of different types of instructions in fine-tuning LLMs ( i.e., LLaMA (7B) and LLaMA (13B) 25), as well as examine the usefulness of several instruction improvement strategies."
    ],
    "references": [
      "358",
      "359",
      "357"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition to the complexity, improving the topic diversity of the instruction dataset can help elicit different abilities of LLMs on diverse tasks in real world [336].",
    "context_before": [
      "Increasing the topic diversity."
    ],
    "context_after": [
      "However, it is difficult to directly control the self-instruct process for generating diverse instructions."
    ],
    "references": [
      "336"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "RLHF employs reinforcement learning (RL) algorithms ( e.g., Proximal Policy Optimization (PPO) [128]) to adapt LLMs to human feedback by learning a reward model.",
    "context_before": [
      "As discussed below, the alignment criteria introduced in Section 5.2.1 can be fulfilled by learning from human feedback on the responses of LLMs to users’ queries. 5.2.3 Reinforcement Learning from Human Feedback To align LLMs with human values, reinforcement learning from human feedback (RLHF) has been proposed to fine-tune LLMs with the collected human feedback data, which is useful to improve the alignment criteria ( e.g., helpfulness, honesty, and harmlessness)."
    ],
    "context_after": [
      "Such an approach incorporates humans in the training loop for developing well-aligned LLMs, as exemplified by InstructGPT ."
    ],
    "references": [
      "128"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Despite that InstructGPT used a small reward model (6B GPT model), increasing work [99] has shown it is often more effective to use a large reward model ( e.g., equal or greater than the original model size), since large reward models generally perform better in judging the quality of the LLM generated outputs.",
    "context_before": [
      "Effective reward model training."
    ],
    "context_after": [
      "In LLaMa 2 , pretrained chat model checkpoints are used to initialize the reward model, they argue that such an approach can effectively reduce the information mismatch between the model to be aligned and the reward model by sharing the same pre-training knowledge."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In LLaMa 2 [99], pretrained chat model checkpoints are used to initialize the reward model, they argue that such an approach can effectively reduce the information mismatch between the model to be aligned and the reward model by sharing the same pre-training knowledge.",
    "context_before": [
      "Despite that InstructGPT used a small reward model (6B GPT model), increasing work has shown it is often more effective to use a large reward model ( e.g., equal or greater than the original model size), since large reward models generally perform better in judging the quality of the LLM generated outputs."
    ],
    "context_after": [
      "Whereas, it is common to encounter the overfitting problem when training large-scale reward models."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Therefore, it is useful to train multiple reward models that focus on different alignment criteria [99], and compute the final reward based on the produced ones from them via special combination strategies (e.g., mean pooling and weighted sum).",
    "context_before": [
      "In addition, as there are multiple criteria for alignment ( e.g., helpfulness and honesty), it is often difficult to train a single reward model that can satisfy all the alignment criteria."
    ],
    "context_after": [
      "Such a way enables more flexible rules or standards on multiple criteria, e.g., relaxing the requirement on helpfulness while posing more strict limits on harmfulness."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "LLaMA 2 [99] has successively trained five versions of RLHF models, where the LLM has been progressively improved with the improvement of the reward models.",
    "context_before": [
      "After fine-tuning the LLM on the best samples until convergence, the RL process will be performed to further improve the performance."
    ],
    "context_after": [
      "In this way, the collected prompts and annotations of human preference data can better reflect the issues of the current model checkpoint, thus making special tuning to address these issues."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "As an alternative, increasing studies explore to directly optimize LLMs to adhere to human preferences, using supervised fine-tuning without reinforcement learning [338].",
    "context_before": [
      "Besides, the commonly-used PPO algorithm in RLHF is rather complex and often sensitive to hyper-parameters."
    ],
    "context_after": [
      "Overview."
    ],
    "references": [
      "338"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For the first issue, the alignment dataset can be automatically constructed by an aligned LLMs according to human-written safety principles [336] or refining existing examples using edits operations [385].",
    "context_before": [
      "Thus, to implement this approach, two key issues are the construction of alignment dataset and the design of fine-tuning loss."
    ],
    "context_after": [
      "In addition, we can also reuse existing reward models to select highrated responses from existing human feedback data ."
    ],
    "references": [
      "336"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Similarly, Self-Align [336] first adopts self-instruct [147] to generate instructions focusing on covering diverse topics.",
    "context_before": [
      "Based on these principles, LLMs will critique their own harmful responses and revise them repeatedly into finally aligned responses."
    ],
    "context_after": [
      "Then, the model is also prompted with multiple human-written principles that describe the rules of expected model behaviors (also with several incontext exemplars), to generate helpful, ethical, and reliable responses as alignment data."
    ],
    "references": [
      "336"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To mitigate the limit that the original SFT method can only learn from positive responses, FIGA [388] develops an improved supervised alignment approach, where both negative (the original output of low quality) and positive (the refined output by LLMs) responses are leveraged in a contrastive way, to enable LLMs to deeply understand what fine-grained revisions actually lead to good response.",
    "context_before": [
      "Then, the model is also prompted with multiple human-written principles that describe the rules of expected model behaviors (also with several incontext exemplars), to generate helpful, ethical, and reliable responses as alignment data."
    ],
    "context_after": [
      "LLM based interactive approaches."
    ],
    "references": [
      "388"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Furthermore, FIGA [388] designs a token-level contrastive loss that aims to encourage desirable tokens, penalize undesirable ones, and disregard trivial tokens.",
    "context_before": [
      "Based on DPO, existing work has proposed several improvement strategies for enhancing the effectiveness or efficiency, e.g., decomposing the optimization of positive responses and negative responses into two independent components or removing the probability of the reference model in the objective function ."
    ],
    "context_after": [
      "Despite the effectiveness, recent work has also revealed that DPO may have inherent limitations in several aspects."
    ],
    "references": [
      "388"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Thus, high-quality instruction data (but not the quantity) is the primary factor for effective training of LLMs during the SFT stage [99].",
    "context_before": [
      "However, there often exist variations among different annotators on the writing styles, quality, and preferences of demonstration data, which tends to affect the learning performance of SFT."
    ],
    "context_after": [
      "Pros and Cons of RLHF ."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Specially, LLaMA 2 has demonstrated that RLHF can improve both the helpfulness and harmlessness scores [99], and attributed this to a better human-LLM synergy for data annotation.",
    "context_before": [
      "Recently, increasing evidence has demonstrated the effectiveness of RLHF in mitigating the harmful responses and enhancing the model capacity."
    ],
    "context_after": [
      "They explain this reason in two major aspects as follows."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For example, LLaMA 2 adopts pre-trained chat model checkpoints to initialize reward models [99]. be optimized according to the specific task goals, while the parameters of the original language model are frozen in this process.",
    "context_before": [
      "In RLHF, it seems to be also important that reward models should be aware of the knowledge or ability of a LLM to be aligned."
    ],
    "context_after": [
      "In this way, we can effectively reduce the number of trainable parameters during fine-tuning."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Approach Representative Work Key Point In-context Learning (ICL) KATE [420] Demonstration selection (similar; k-NN) EPR [421] Demonstration selection (dense retrieval; constrative learning) SG-ICL [422] Demonstration selection (LLM as the demonstration generator) APE [423] Demonstration format (automatic generation & selection) Structured Prompting [424] Demonstration format (grouped context encoding; rescaled attention) GlobalE & LocalE [425] Demonstration order (entropy-based metric; probing set generation with LLM) Chain-of-thought Prompting (CoT) Complex CoT [426] Demonstration (complexity-based selection) Auto-CoT [427] Demonstration (automatic generation) Selection-Inference [428] Generation (alternate between selection and inference) Self-consistency [429] Generation (diverse paths; self-ensemble) DIVERSE [430] Generation (diverse paths); Verification (step-wise voting) Rationale-augmented ensembles [431] Generation (rationale sampling) Planning Least-to-most prompting [432] Plan generation (text-based; problem decomposition) DECOMP [433] Plan generation (text-based; problem decomposition) PS [434] Plan generation (text-based) Faithful CoT [435] Plan generation (code-based) PAL [436] Plan generation (code-based; Python) HuggingGPT [437] Plan generation (code-based; models from HuggingFace) AdaPlanner [438] Plan refinement (skill memory) TIP [439] Feedback acquisition (visual perception) RAP [440] Feedback acquisition (LLM as the world model); Plan refinement (Monte Carlo Tree Search) ChatCoT [441] Feedback acquisition (tool); Plan refinement (conversation between LLM and tools) ReAct [442] Feedback acquisition (tool); Plan refinement (synergizing reasoning and acting) Reflexion [443] Feedback acquisition (text-based self-reflection); Plan refinement (dynamic memory) Tree of Thoughts [444] Feedback acquisition (vote comparison); Plan refinement (tree-based search) we also present three prompt examples for question answering, meta-review generation, and text-to-SQL in Table.",
    "context_before": [
      "Note that the key points only highlight the most important technical contribution."
    ],
    "context_after": [
      "Task description.A task description is typically a specific instruction that LLMs are expected to follow."
    ],
    "references": [
      "438",
      "437"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Both the quality of the retrieved documents and their relevance to the question have an impact on the generated answers [454].",
    "context_before": [
      "For example, retrieved documents are highly useful for open-domain question answering as supporting evidence."
    ],
    "context_after": [
      "Thus, it needs to include such information in a proper prompt pattern or expression format."
    ],
    "references": [
      "454"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Another study [501] also supports this finding with specially designed experiments.",
    "context_before": [
      "As shown in the experiments , the ability of task recognition is easier to obtain, and even a small LM with only 350M parameters can exhibit this ability, while task learning can only emerge for LLMs with at least 66B parameters."
    ],
    "context_after": [
      "They set up the tasks with flipped and semantically unrelated labels in the experiment, which require task learning when performing ICL."
    ],
    "references": [
      "501"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To reduce potentially meaningless thought exploration, XoT [521] further proposes to guide the search of thoughts with pre-trained policy and value networks. 6.3.3 Further Discussion on CoT Prompting In this part, we present discussions regarding two fundamental questions related to CoT prompting, i.e., “when does CoT prompting work for LLMs ” and “ why can LLMs perform CoT reasoning”.",
    "context_before": [
      "However, such an approach requires a large number of interactions with LLMs, making the thought exploration process highly inefficient."
    ],
    "context_after": [
      "When CoT Prompting Works For LLMs?"
    ],
    "references": [
      "521"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Furthermore, environment refers to where the plan executor carries out the actions, which can be set differently according to specific tasks, e.g., the LLM itself [529] or an external virtual world like Minecraft [530].",
    "context_before": [
      "It can be implemented by models like LLMs for textual tasks or by tools like code interpreters for coding tasks ."
    ],
    "context_after": [
      "It provides feedback about the execution result of the action to the task planner, either in the form of natural language or from other multimodal signals ."
    ],
    "references": [
      "530"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "HuggingGPT [437] introduces the models available in HuggingFace and regards LLMs as the controller to select suitable models based on their descriptions and aggregate their results as the final solution.",
    "context_before": [
      "For example, ToolFormer first annotates a pre-training corpus with potential API calls using LLMs, and then fine-tunes LLMs on it, so that LLMs can learn when and how to call APIs and incorporate the results returned by APIs during generation."
    ],
    "context_after": [
      "Code-based Approaches."
    ],
    "references": [
      "437"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For example, tools like code interpreters are widely used in programming tasks to provide real-time error messages [443], models like stable diffusion [534] can be used in multimodal tasks to provide visual perception [439], and virtual worlds like Minecraft can provide immersive experiences [530].",
    "context_before": [
      "In addition to LLMs, external objects can also provide feedback signals."
    ],
    "context_after": [
      "Besides, some work ( e.g., Generative Agents ) explores multi-agent collaboration in simulated environments, where each agent receives feedback not only from interaction with the environment but also from communication with other agents. 6.4.4 Plan Refinement With access to feedback from the environment, the task planner can accordingly refine its current plan and iteratively go through the “planning – execution – refinement” loop for better results."
    ],
    "references": [
      "530"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For instance, GPT-4 exhibits comparable performance as commercial translation products, even for the translation task of languages that are with significant linguistic distance [629].",
    "context_before": [
      "Due to the powerful language generation capabilities, LLMs have achieved remarkable performance on existing datasets and benchmarks."
    ],
    "context_after": [
      "On news summarization tasks ( i.e., CNN/DM and XSUM), LLMs also demonstrate comparable performance with human freelance writers ."
    ],
    "references": [
      "629"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To improve such an ability, it is key to fine-tuning (or pre-training) LLMs on code data, which can effectively adapt LLMs to code synthesis tasks [86].",
    "context_before": [
      "Typically, they consist of diverse programming problems, with text specification and test cases for correctness checking."
    ],
    "context_after": [
      "In addition, existing work has proposed new strategies to generate code, e.g., sampling multiple candidate solutions and planning-guided decoding , which can be considered as the imitation of bug-fixing and code-planning processes by programmers."
    ],
    "references": [
      "86"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Level Ability Task Dataset Basic Language Generation Language Modeling Penn Treebank [540], WikiText-103 [541], the Pile [166], LAMBADA [252] Conditional Text Generation WMT’14,16,19,20,21,22 [542–547], Flores-101 [548], DiaBLa [549], CNN/DailyMail [550], XSum [551], WikiLingua [552] OpenDialKG [553] Code Synthesis APPS [376], HumanEval [105], MBPP [223], CodeContest [114], MTPB [86], DS-1000 [554], ODEX [555] Knowledge Utilization Closed-Book QA Natural Questions [556], ARC [557], TruthfulQA [558], Web Questions [559], TriviaQA [560], PIQA [561], LC-quad2.0 [562], GrailQA [563], KQApro [564], CWQ [565], MKQA [566], ScienceQA [567] Open-Book QA Natural Questions [556], OpenBookQA [568], ARC [557], TriviaQA [560], Web Questions [559], MS MARCO [569], QASC [570], SQuAD [571], WikiMovies [572] Knowledge Completion WikiFact [573], FB15k-237 [574], Freebase [575], WN18RR [576], WordNet [577], LAMA [578], YAGO3-10 [579], YAGO [580] Complex Reasoning Knowledge Reasoning CSQA [506], StrategyQA [199], HotpotQA [581], ARC [557], BoolQ [582], PIQA [561], SIQA [583], HellaSwag [584], WinoGrande [585], COPA [586], OpenBookQA [568], ScienceQA [567], proScript [587], ProPara [588], ExplaGraphs [589], ProofWriter [590], EntailmentBank [591], ProOntoQA [592] Symbolic Reasoning CoinFlip [33], ReverseList [33], LastLetter [33], Boolean Assignment [593], Parity [593], Colored Object [70], Penguins in a Table [70], Repeat Copy [436], Object Counting [436] Mathematical Reasoning MATH [362], GSM8k [198], SVAMP [594], MultiArith [595], ASDiv [505], MathQA [596], AQUA-RAT [597], MAWPS [598], DROP [599], NaturalProofs [600], PISA [601], miniF2F [602], ProofNet [603] Advanced Human Alignment Honestness TruthfulQA [558], HaluEval [604] Helpfulness HH-RLHF [183] Harmlessness HH-RLHF [183], Crows-Pairs [605] WinoGender [606], RealToxicityPrompts [607] Interaction with External Environment Household VirtualHome [608], BEHAVIOR [609], ALFRED [610],ALFWorld [611] Website Environment WebShop [612], Mind2Web [613] Open World MineRL [614], MineDojo [615] Tool Manipulation Search Engine HotpotQA [581], TriviaQA [560], Natural Questions [556] Code Executor GSM8k [198], TabMWP [616], Date Understanding [70] Calculator GSM8k [198], MATH [362], CARP [617] Model Interface GPT4Tools [618], Gorilla [619] Data Interface WebQSP [620], MetaQA [621], WTQ [622] WikiSQL [623], TabFact [624], Spider [625] as discussed below.",
    "context_before": [
      "Given k programs generated by the LLM, pass@k is computed as 1 when at least one program passes all test cases, or else 0 57 TABLE 14: Representative basic and advanced abilities and corresponding representative datasets for evaluating."
    ],
    "context_after": [
      "Unreliable generation evaluation."
    ],
    "references": [
      "86"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For instance, considering that hallucinated facts are prone to exhibit inconsistency across different sampled outputs, SelfCheckGPT [666] detects hallucination by measuring information inconsistency within sampled outputs.",
    "context_before": [
      "Another line of research work leverages uncertainty estimation of LLMs to identify hallucinations ."
    ],
    "context_after": [
      "For the evaluation of the hallucination problem, a set of hallucination detection tasks have been proposed, e.g., TruthfulQA for detecting human falsehood mimicked by models."
    ],
    "references": [
      "666"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For instance, with a subword tokenizer the integer 7481 may be tokenized as 7 481, while 74815 may be tokenized as 748 15 (the same numerical substrings with different splits) [359].",
    "context_before": [
      "One possible explanation is that subword tokenization techniques can result in inconsistent sequences when tokenizing numbers."
    ],
    "context_after": [
      "As a comparison, digit-based tokenization for numbers can avoid such an inconsistency, thus likely improving the numerical computation ability of LLMs."
    ],
    "references": [
      "359"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Method Evaluation Model Types Abilities/Domain Data Source Benchmark MMLU [362] Base/Fine-tuned/Specialized General Human exam/practice BIG-bench [70] Base/Fine-tuned/Specialized General Human annotation HELM [522] Base/Fine-tuned/Specialized General Benchmark collection Open LLM Leaderboard [709] Base/Fine-tuned/Specialized General Benchmark collection AGIEval [710] Base/Fine-tuned/Specialized General Human exam/practice MMCU [711] Base/Fine-tuned/Specialized General Human exam/practice M3KE [712] Base/Fine-tuned/Specialized General Human exam/practice C-Eval [713] Base/Fine-tuned/Specialized General Human exam/practice Xiezhi [714] Base/Fine-tuned/Specialized General Human exam/practice OpenCompass [715] Base/Fine-tuned/Specialized General Benchmark collection Chain-of-Thought Hub [716] Base/Fine-tuned General Benchmark collection KoLA [717] Base/Fine-tuned Knowledge utilization Web ARB [718] Fine-tuned Complex reasoning Human exam/practice APIBench [719] Base/Fine-tuned Tool manipulation Web APIBank [720] Fine-tuned Tool manipulation Synthesis ToolAlpaca [721] Base/Fine-tuned Tool manipulation Synthesis T-Bench [722] Fine-tuned Tool manipulation Synthesis ToolBench [723] Fine-tuned Tool manipulation Synthesis BOLAA [724] Base/Fine-tuned Environment interaction Benchmark collection AgentBench [725] Base/Fine-tuned Environment interaction Human annotation/Synthesis HaluEval [604] Base/Fine-tuned Human alignment Human annotation/Synthesis PromptBench [726] Base/Fine-tuned Robustness Benchmark collection HumanEval [105] Base/Fine-tuned/Specialized Code synthesis Human annotation MultiMedQA [354] Specialized Healthcare Benchmark collection FLUE [727] Specialized Finance Benchmark collection LegalBench [728] Specialized Legal Human annotation Human Chatbot Arena [729] Base/Fine-tuned/Specialized Human Alignment Human annotation SciBench [730] Fine-tuned Complex reasoning Human exam/practice Model AlpacaEval [731] Fine-tuned Instruction following Synthesis MT-bench [729] Fine-tuned Human alignment Human annotation TrustGPT [732] Base/Fine-tuned Human alignment Benchmark collection LMExamQA [733] Base/Fine-tuned Knowledge utilization Synthesis ChatEval [734] Base/Fine-tuned Knowledge utilization Benchmark collection and tested abilities.",
    "context_before": [
      "The evaluated abilities are not limited to the representative basic and advanced abilities mentioned in Section 7.1 and 7."
    ],
    "context_after": [
      "Next, we will discuss the evaluation approaches for different types of LLMs."
    ],
    "references": [
      "354"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Specialized LLMs refer to the model checkpoints specially adapted to some domains or applications like healthcare [354] and finance [739].",
    "context_before": [
      "Evaluation of Specialized LLMs."
    ],
    "context_after": [
      "As special task solvers, specialized LLMs will be tested not only on general abilities ( e.g., basic ability like complex reasoning and advanced ability like human alignment), but also on specific abilities related to their designated domains or applications."
    ],
    "references": [
      "354"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For example, MultiMedQA [354] is a specific benchmark in healthcare, which includes medical examinations and healthcare questions.",
    "context_before": [
      "Then, these domain-specific benchmarks can be combined with general benchmarks to conduct both comprehensive and targeted evaluation for specialized LLMs."
    ],
    "context_after": [
      "In this work , MultiMedQA has been combined with MMLU to assess the performance of specialized LLMs for healthcare, such as Med-PaLM ."
    ],
    "references": [
      "354"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In this work [354], MultiMedQA has been combined with MMLU [362] to assess the performance of specialized LLMs for healthcare, such as Med-PaLM [354].",
    "context_before": [
      "For example, MultiMedQA is a specific benchmark in healthcare, which includes medical examinations and healthcare questions."
    ],
    "context_after": [
      "Similarly, FLUE constructs a benchmark for finance, spanning from financial sentiment analysis to question answering."
    ],
    "references": [
      "354"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "It has been used collaboratively with BBH [363] to evaluate finical LLMs like BloombergGPT [358].",
    "context_before": [
      "Similarly, FLUE constructs a benchmark for finance, spanning from financial sentiment analysis to question answering."
    ],
    "context_after": [
      "Pros and Cons of Different Evaluation Approaches ."
    ],
    "references": [
      "358"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In our evaluation, we select four representative base models including LLaMA (7B) [57], LLaMA 2 (7B) [99], Pythia (7B and 12B) [96], and Falcon (7B) [749].",
    "context_before": [
      "Base models are only pre-trained on a large general-purpose corpus with the language modeling objective, but without further supervised fine-tuning."
    ],
    "context_after": [
      "Instruction-tuned models are those fine-tuned using instructions ( i.e., task datasets, daily chat, or synthetic instructions)."
    ],
    "references": [
      "99",
      "96"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition, we also include LLaMA 2-Chat (7B) [99] for comparison, and it is a representative model that has been aligned with human via instruction tuning and RLHF, based on LLaMA 2 (7B).",
    "context_before": [
      "In our experiments, we select four representative instruction-tuned models including Vicuna (7B and 13B) , Alpaca (7B) , and ChatGLM (6B) ."
    ],
    "context_after": [
      "Closed-source models."
    ],
    "references": [
      "99"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "You should follow the examples and generate the final answer without external solution or words. 66.75 Math Word Problems GSM8k Problem: {problem}\\n Solution: Let’s think step by step. 78.47 63.20 [744]Let’s use python to solve math problems.",
    "context_before": [
      "You can use the knowledge in examples and solve the last problem."
    ],
    "context_after": [
      "Here are three examples how to do it,\\n Q: Olivia has $."
    ],
    "references": [
      "744"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "The information extraction task focuses on automatically extracting useful structured information from unstructured text data, such as relation extraction [761] and event extraction [762], which is also a crucial task relating to many NLP applications.",
    "context_before": [
      "Information Extraction."
    ],
    "context_after": [
      "Typically, previous studies formulate this task as a text classification task or a sequential labeling task."
    ],
    "references": [
      "761"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition, a recent study [766] also reveals that LLMs can achieve competitive zero-shot performance for information extraction with a two-stage workflow, making this approach attractive in future applications.",
    "context_before": [
      "Whereas, it is shown that enabling collaboration between LLMs and small models can further boost the performance of specific tasks ."
    ],
    "context_after": [
      "Text Generation."
    ],
    "references": [
      "766"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Additionally, LLMs are flexible to effectively handle special requirement in real-world application scenarios, e.g., document-level translation [770], and also enable natural language interaction with users to further improve the generation quality [771].",
    "context_before": [
      "Since the pre-training of LLMs is established on text prediction, they exhibit strong language generation abilities as commercial products and humans , with the help of proper prompts ."
    ],
    "context_after": [
      "Despite the above 71 LLM for Application Research Directions Specific Domains Scientific ResearchFinance Law EducationHealthcare LLM for Evaluation LLM-based Agent KG Enhanced LLM Multimodal LLMs."
    ],
    "references": [
      "771",
      "770"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Text Generation Classic Scenarios Enhanced Capabilities New Scenarios Fig. 18: The applications of LLMs in representative research directions and downstream domains. success, recent work also reveals that LLMs are hard to well address the generation tasks about low-resource languages and domains, e.g., Marathi-to-English translation [772], due to their unbalanced training data across different languages.",
    "context_before": [
      "Information Extraction."
    ],
    "context_after": [
      "Summary."
    ],
    "references": [
      "772"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "It is also promising to combine LLMs and fine-tuned small language models for complementing with each other in solving complex cases of classic NLP tasks [775].",
    "context_before": [
      "In addition, it is still challenging for LLMs to handle complex semantic relations in classic NLP tasks (e.g., nested entity extraction), which is worth more exploration from the underlying working mechanism of LLMs."
    ],
    "context_after": [
      "Another promising direction is to conduct human-machine collaborative research (e.g., conversational translation ) on NLP tasks, since LLMs can effectively understand human instructions and make meaningful responses. 8.1.2 LLM for Information Retrieval The goal of information retrieval (IR) systems is to assist users in discovering ideal information resources (typically documents) and mitigating the information overload issue."
    ],
    "references": [
      "775"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Another promising direction is to conduct human-machine collaborative research (e.g., conversational translation [771]) on NLP tasks, since LLMs can effectively understand human instructions and make meaningful responses. 8.1.2 LLM for Information Retrieval The goal of information retrieval (IR) systems is to assist users in discovering ideal information resources (typically documents) and mitigating the information overload issue.",
    "context_before": [
      "It is also promising to combine LLMs and fine-tuned small language models for complementing with each other in solving complex cases of classic NLP tasks ."
    ],
    "context_after": [
      "Typically, contemporary IR systems adopt a retrieve-thenrerank pipeline framework ."
    ],
    "references": [
      "771"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Specially, the LLM-based reranking approach can be implemented in different ways by zero-shot or few-shot instruction, including pointwise ( estimating the relevance scores for querydocument pairs) [779], pairwise (determining the relevance order of two documents) [778], or listwise ranking (sorting a subset of candidate documents) [780].",
    "context_before": [
      "Typically, such an approach does not necessitate model training, and achieve promising results compared with well-trained reranking methods ."
    ],
    "context_after": [
      "The essence of these methods lies 72 in the special design of instructions for text reranking, such as sliding window strategy for document lists , setwise selection prompting , fine-grained relevance labels incorporation , and pairwise comparison prompting ."
    ],
    "references": [
      "779",
      "780",
      "778"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition, recent efforts employ LLMs to generate intermediate texts ( e.g., URLs) as retrieval results using few-shot demonstrations [784].",
    "context_before": [
      "The essence of these methods lies 72 in the special design of instructions for text reranking, such as sliding window strategy for document lists , setwise selection prompting , fine-grained relevance labels incorporation , and pairwise comparison prompting ."
    ],
    "context_after": [
      "To further enhance the model performance, LLMs can be specially fine-tuned as backbones for reranking or retrieval (including dense retrieval and model-based retrieval ), similar to the fine-tuning process for traditional PLM-based IR models ."
    ],
    "references": [
      "784"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "The rewritten query can take the form of an improved version of the original query [794], a document in the corpus that related to the query [795], or an expansion of the query that concatenated with a pseudo generated document [796].",
    "context_before": [
      "As a solution, LLM can be utilized to rewrite the query for enhancing the understanding of the query intent and incorporating additional knowledge into the query through well-designed instructions."
    ],
    "context_after": [
      "In addition, documents can also be expanded with queries that are generated based on the original documents using LLMs for context extension ."
    ],
    "references": [
      "796",
      "794"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition, documents can also be expanded with queries that are generated based on the original documents using LLMs for context extension [797].",
    "context_before": [
      "The rewritten query can take the form of an improved version of the original query , a document in the corpus that related to the query , or an expansion of the query that concatenated with a pseudo generated document ."
    ],
    "context_after": [
      "Remaining Issues."
    ],
    "references": [
      "797"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "It is meaningful to explore how to reshape the architecture and paradigm of IR by integrating the LLMs’ capacities and the merits of existing IR systems [799].",
    "context_before": [
      "Secondly, the advent of LLMs sheds lights on the development of new information seeking ways ( e.g., New Bing)."
    ],
    "context_after": [
      "Thirdly, existing work mainly focuses on text retrieval tasks, lacking a comprehensive consideration of multimodal information sources."
    ],
    "references": [
      "799"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Another alternative approach is to train a revision model to rectify hallucinated response generated by MLLMs in a post-hoc way [862].",
    "context_before": [
      "To address the aforementioned issues, some studies collect specialized visual instructions to mitigate the problem of hallucination ."
    ],
    "context_after": [
      "Additionally, aligning MLLMs with RLHF can also assist MLLMs in generating responses with improved factuality ."
    ],
    "references": [
      "862"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Additionally, aligning MLLMs with RLHF can also assist MLLMs in generating responses with improved factuality [863].",
    "context_before": [
      "Another alternative approach is to train a revision model to rectify hallucinated response generated by MLLMs in a post-hoc way ."
    ],
    "context_after": [
      "Despite these efforts, existing alignment techniques for MLLMs mainly concentrate on several specific aspects ( e.g., hallucination), lacking a comprehensive consideration of alignment criteria."
    ],
    "references": [
      "863"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "It has been shown that LLMs are capable of handling a variety of healthcare tasks, e.g., biology information extraction [765], medical advice consultation [895], mental health analysis [896], and report simplification [897].",
    "context_before": [
      "Ever since the advent of ChatGPT, a number of studies have applied ChatGPT or other LLMs to the medical domain."
    ],
    "context_after": [
      "As the major technical approach, researchers typically design specific prompts or instructions to guide LLMs to perform a wide range of medical tasks."
    ],
    "references": [
      "765"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In addition, it would also raise privacy concerns to upload the health information of patients [765] into a commercial server that support the LLM.",
    "context_before": [
      "However, LLMs may fabricate medical misinformation , e.g., misinterpreting medical terms and suggesting advice inconsistent with medical guidelines."
    ],
    "context_after": [
      "Education is also an important application domain where LLMs potentially exert significant influence."
    ],
    "references": [
      "765"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To leverage the scaling effect of LLMs, researchers collect large-scale finance corpora for continually pre-training LLMs ( e.g., BloombergGPT [358], XuanYuan 2.0 [919], and FinGPT [920]).",
    "context_before": [
      "Despite the competitive zero-shot performance exhibited by general-purpose LLMs in the finance tasks, they still underperform domain-specific PLMs containing million-scale parameters ."
    ],
    "context_after": [
      "BloombergGPT has demonstrated remarkable performance across a diverse range of financial tasks while maintaining competitive performance in general-purpose tasks ."
    ],
    "references": [
      "919",
      "358"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "BloombergGPT has demonstrated remarkable performance across a diverse range of financial tasks while maintaining competitive performance in general-purpose tasks [358].",
    "context_before": [
      "To leverage the scaling effect of LLMs, researchers collect large-scale finance corpora for continually pre-training LLMs ( e.g., BloombergGPT , XuanYuan 2.0 , and FinGPT )."
    ],
    "context_after": [
      "Nevertheless, it is imperative to consider the potential risks in the application of LLMs in finance, as the generation of inaccurate or harmful content by LLMs could have significant adverse implications for financial markets ."
    ],
    "references": [
      "358"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Nevertheless, it is imperative to consider the potential risks in the application of LLMs in finance, as the generation of inaccurate or harmful content by LLMs could have significant adverse implications for financial markets [358].",
    "context_before": [
      "BloombergGPT has demonstrated remarkable performance across a diverse range of financial tasks while maintaining competitive performance in general-purpose tasks ."
    ],
    "context_after": [
      "Therefore, it needs more strict reviewing and monitoring on the use of LLMs in the financial field."
    ],
    "references": [
      "358"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "The initial plan can be further refined with execution feedback from the environment [530].",
    "context_before": [
      "To generate it, LLM-based agents will first propose several candidates and then select a more suitable one among them ."
    ],
    "context_after": [
      "The execution component is in charge of carrying out the plan from the planning component, which can be fulfilled by the internal LLM or external tools ."
    ],
    "references": [
      "530"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "https://www.langchain.com/ 81 as AgentVerse [969] and AutoGen [970], can also be utilized for developing multi-agent collaborative systems.",
    "context_before": [
      "In addition, other similar frameworks, such."
    ],
    "context_after": [
      "In the competition-based mode, debate serves as one of the popular communication protocols to foster divergent thinking and elicit valuable external feedback among agents."
    ],
    "references": [
      "969"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Unlike traditional early exit mechanisms that skip all subsequent layers, the mixtureof-depths method selectively skips certain layers, which can adaptively utilize the characteristics of different layers during generation. 9.5 Model Compression Due to the huge number of model parameters, LLMs take a significant memory footprint for inference, making it very costly to be deployed in real-world applications [999].",
    "context_before": [
      "If the score exceeds a preset threshold, the layer would be computed; otherwise, the layer would be skipped."
    ],
    "context_after": [
      "In this section, we focus on how to reduce the memory footprint of LLMs via technical approaches."
    ],
    "references": [
      "999"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In neural network compression, quantization often refers to the mapping process from floating-point numbers to integers [1000], especially the 8-bit integer quantization (i.e., INT8 quantization).",
    "context_before": [
      "In this part, we present a general introduction of quantization techniques for neural networks."
    ],
    "context_after": [
      "For neural network models, there are typically two kinds of data to be quantized, namely weights (model parameters) and activations (hidden activations), which are originally represented in floating-point numbers."
    ],
    "references": [
      "1000"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For more details, we refer to the readers to the excellent survey [1000] about quantization methods on neural networks.",
    "context_before": [
      "The range parameters α and β have a large impact on the quantization performance, which often need to be calibrated according to real data distributions, in either a static (offline) or dynamic way (runtime)."
    ],
    "context_after": [
      "Post-Training Quantization (PTQ) ."
    ],
    "references": [
      "1000"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To efficiently optimize this objective, GPTQ [1009] improves the original optimal brain quantization (OBQ) [1010] method by fixing the quantization order of weights for all rows.",
    "context_before": [
      "This approach finds optimal quantized weights that minimize a layerwise reconstruction loss: arg mincW ∥ WX − cWX ∥2."
    ],
    "context_after": [
      "Further, with specially designed methods ( i.e., lazy batch-updates and Cholesky reformulation), GPTQ is feasible to quantize very large models ( e.g., 175B OPT) in 3 or 4 bit precision."
    ],
    "references": [
      "1009"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To overcome this challenge, QLoRA [1011] incorporates additional small tunable adapters (16-bit precision) into the quantized models, to achieve an efficient, high-precision model fine-tuning.",
    "context_before": [
      "For posttraining quantization, direct low-bit quantization (e.g., INT4 quantization) often results in large performance degradation."
    ],
    "context_after": [
      "It combines the merits of LoRA (See Section 5.3.1) and quantization methods."
    ],
    "references": [
      "1011"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Moreover, focusing on emergent capabilities, the study [1015] finds that in-context learning, step-by-step reasoning, and instruction following all seem to be seldom affected with 4-bit weight quantization.",
    "context_before": [
      "For example, a 4-bit 60B LLM is demonstrated to have better performance than an 8-bit 30B LLM ."
    ],
    "context_after": [
      "This result suggests that INT4 quantization exhibits a favorable trade-off in terms of both total bits and performance of emergent abilities."
    ],
    "references": [
      "1015"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Secondly, it is flexible to support taskspecific or goal-specific fine-tuning of LLMs in a lightweight way [1011], e.g., instruction tuning or chat-oriented tuning, by only tuning the small adapters.",
    "context_before": [
      "This can be achieved either by increasing the fitting capacity via updating high precision adapters , or by finding a proper low-rank initizalization for LoRA fine-tuning ."
    ],
    "context_after": [
      "Overall, it makes a good trade-off between the effectiveness and training cost, which provides a promising approach to enhancing the performance of quantized LLMs."
    ],
    "references": [
      "1011"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "For evaluation, we utilize the same tasks in Table 10, and follow the quantization settings in the study [1015] examining the performance of quantized language models at three precision levels: 4-bit, 8-bit and 16-bit.",
    "context_before": [
      "Specifically, we focus on the fine-tuned LLaMA models (i.e., 7B and 13B) using popular SFT datasets, including FLANv2 , Alpaca-52K and ShareGPT ."
    ],
    "context_after": [
      "The results are summarized in Table."
    ],
    "references": [
      "1015"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Furthermore, Sheared LLaMA [1022] introduces two techniques: targeted structured pruning and dynamic batch loading, which effectively prunes LLaMA2 (7B) to a parameter size of 2.7B, while preserving 87.8% of the original model’s performance. 9.5.3 Open-source Libraries In this part, we briefly introduce the available open-source libraries for memory-efficient deployment.",
    "context_before": [
      "For instance, LLM-pruner selectively removes 20% of the nonessential parameters from LLaMA (7B) based on gradient information, while maintaining 93.6% performance of the original model."
    ],
    "context_after": [
      "Quantization Libraries ."
    ],
    "references": [
      "1022"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "It enables 4-bit quantization of LLaMA models of varied sizes based on the GPTQ algorithm [1009].",
    "context_before": [
      "GPTQ-for-LLaMA50 is developed specially for quantizing LLaMA models."
    ],
    "context_after": [
      "Also, it provides a comparison with bitsandbytes in both memory and performance (PPL) on the project website."
    ],
    "references": [
      "1009"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "AutoGPTQ51 is a quantization package developed based on the GPTQ algorithm [1009], which supports INT4 quantization for LLMs.",
    "context_before": [
      "Also, it provides a comparison with bitsandbytes in both memory and performance (PPL) on the project website."
    ],
    "context_after": [
      "It includes a number of quantized models in the library, and supports LoRA by integrating with HuggingFace PEFT library."
    ],
    "references": [
      "1009"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To address this issue, existing approaches often introduce reranking models to select the most relevant documents from the retrieval results [1027].",
    "context_before": [
      "Since the retrieved documents are typically lengthy, simply concatenating them into the prompt might lead to a poor utilization of the provided context due to the biased attention ( e.g., lost in the middle )."
    ],
    "context_after": [
      "Alternatively, information extraction or text compression techniques can be used to retain only the highly relevant information from the documents, thereby reducing the input context length ."
    ],
    "references": [
      "1027"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "The incorporation of retrieval supplements the LLM with relevant contextual information, and the retrieval performance directly affects the quality of the final generated response [454].",
    "context_before": [
      "Retrieval method improvement."
    ],
    "context_after": [
      "To design effective retrieval strategy, an important factor to consider is the text granularity."
    ],
    "references": [
      "454"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "To balance relevance and latency, existing research work proposes using “ propositions” as the retrieval unit [1031], corresponding to semantically complete and relatively independent text fragments, which can effectively reduce the recall of irrelevant information.",
    "context_before": [
      "Intuitively, a coarser granularity (e.g., document-level) may result in efficient retrieval but tend to incorporate substantial irrelevant information, while a finer granularity (e.g., sentence-level) increases the proportion of relevant content in the retrieval results but can lead to higher retrieval latency."
    ],
    "context_after": [
      "In particular, they mainly use GPT-4 to synthesize instruction data for the extraction of proposition text, training a smaller model specifically to construct proposition text data ."
    ],
    "references": [
      "1031"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In particular, they mainly use GPT-4 to synthesize instruction data for the extraction of proposition text, training a smaller model specifically to construct proposition text data [1031].",
    "context_before": [
      "To balance relevance and latency, existing research work proposes using “ propositions” as the retrieval unit , corresponding to semantically complete and relatively independent text fragments, which can effectively reduce the recall of irrelevant information."
    ],
    "context_after": [
      "Furthermore, to improve retrieval performance, methods such as query expansion and query rewriting can be utilized to optimize query formulation."
    ],
    "references": [
      "1031"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Query expansion focuses on adding supplementary information to the original query, such as incorporating related entity information or providing detailed explanations of key information in the query [796], which helps strengthen relevance matching.",
    "context_before": [
      "Furthermore, to improve retrieval performance, methods such as query expansion and query rewriting can be utilized to optimize query formulation."
    ],
    "context_after": [
      "However, traditional query expansion methods may disrupt the original semantics for complex queries."
    ],
    "references": [
      "796"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "LLMs can be applied directly to query rewriting, transforming the original query into a more suitable form through well-designed prompts [1034].",
    "context_before": [
      "As another query enhancment technique, query rewriting focuses on modifying the query content to highlight key information and eliminate potential ambiguities, facilitating the retrieval of related documents ."
    ],
    "context_after": [
      "To reduce inference overhead, the query optimization capabilities of LLMs can also be transferred to smaller models through knowledge distillation ."
    ],
    "references": [
      "1034"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "As a solution, the documents returned during the retrieval stage can be reranked according to their relevance to the input [1036], filtering out low-quality or irrelevant documents or placing less relevant documents in non-optimal positions within the prompt.",
    "context_before": [
      "In addition to the initial retrieval methods, the refinement of retrieval results also plays an important role in RAG systems, since the retrieved documents may be not best suited for RAG systems, e.g., LLMs might have difficulty in utilizing long contexts or be affected by irrelevant information in the retrieved documents."
    ],
    "context_after": [
      "Furthermore, both generation and reranking tasks can be jointly optimized to faciliate better utilize of context documents."
    ],
    "references": [
      "1036"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Furthermore, both generation and reranking tasks [1027] can be jointly optimized to faciliate better utilize of context documents.",
    "context_before": [
      "As a solution, the documents returned during the retrieval stage can be reranked according to their relevance to the input , filtering out low-quality or irrelevant documents or placing less relevant documents in non-optimal positions within the prompt."
    ],
    "context_after": [
      "Additionally, LLMs 90 can be directly used for document re-ranking by designing specific prompts or using context examples to accomplish this task ."
    ],
    "references": [
      "1027"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Additionally, LLMs 90 can be directly used for document re-ranking by designing specific prompts or using context examples to accomplish this task [777].",
    "context_before": [
      "Furthermore, both generation and reranking tasks can be jointly optimized to faciliate better utilize of context documents."
    ],
    "context_after": [
      "In addition to document filtering or reranking, information extraction or automatic summarization techniques can be employed to refine the retrieved content by extracting more concise and query-relevant content from the retrieved documents."
    ],
    "references": [
      "777"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Building on the iterative retrieval augmentation method, adaptive retrieval augmentation further enhances the LLM’s autonomous use of the retrieval mechanism [1038], thereby improving the overall framework’s efficacy in using the retrieval systems.",
    "context_before": [
      "For example, intermediate results from the chain of thought can be used as the query input for the next round of retrieval, and after completing the retrieval process, the returned results can be integrated into the chain of thought."
    ],
    "context_after": [
      "In practical implementation, for the above two types of augmentation methods, LLM first need to determine when to use the retriever and then utilize pre-set prompts to initiate query generation and retrieval result processing ."
    ],
    "references": [
      "1038"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "In practical implementation, for the above two types of augmentation methods, LLM first need to determine when to use the retriever and then utilize pre-set prompts to initiate query generation and retrieval result processing [1039].",
    "context_before": [
      "Building on the iterative retrieval augmentation method, adaptive retrieval augmentation further enhances the LLM’s autonomous use of the retrieval mechanism , thereby improving the overall framework’s efficacy in using the retrieval systems."
    ],
    "context_after": [
      "RAG-enhanced training."
    ],
    "references": [
      "1039"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "By constructing instruction data focused on retrieval context utilization [1040], instruction tuning can improve the LLM’s ability to utilize relevant retrieval information.",
    "context_before": [
      "In addition to the improvement strategies mentioned above, specialized training tasks can be designed to further enhance the LLM’s ability to utilize the retrieved content, including both instruction tuning and pre-training tasks."
    ],
    "context_after": [
      "When curating the instruction data, it is essential to consider two important issues: positional bias and irrelevant information within the input context."
    ],
    "references": [
      "1040"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Additionally, irrelevant information can be added to the instructions data, so as to improving the model’s ability to resist interference from such information [1041].",
    "context_before": [
      "Specifically, relevant documents can be placed at different positions within the prompt, which can enhance the model’s attention to relevant content in various positions and prevent the model from neglecting certain positions ."
    ],
    "context_after": [
      "In addition, special training tasks can be introduced during the pre-training stage to further enhance the LLM’s retrieval and generation capabilities ."
    ],
    "references": [
      "1041"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "A common data construction method uses portions of the original document as queries and then trains the model to reconstruct the remaining content of the original document based on the retrieval results [1043]. 9.7 Hallucination Hallucination, which refers to the phenomenon that LLMs generate content inconsistent with factual information, has become a significant issue that greatly affects the task performance of LLMs [1044].",
    "context_before": [
      "Existing work mainly constructs unsupervised pre-training data aimed at retrieval augmentation."
    ],
    "context_after": [
      "In this section, we focus on discussing the topic of LLM hallucination, first introducing the definition and source of hallucination and then summarizing the detection and mitigation methods. 9.7.1 Definition of Hallucination Early research typically defines hallucinations based on the relationship between a model’s output and the given input ."
    ],
    "references": [
      "1044"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "According to a recent study [1044], factual hallucinations can be further categorized into the following types:.",
    "context_before": [
      "However, in real-world scenarios, user inputs often do not contain reference documents, and thus existing work mainly focuses on open-domain factual hallucinations, where the model-generated content does not align with or cannot be verified by existing world knowledge ."
    ],
    "context_after": [
      "Entity-error hallucination."
    ],
    "references": [
      "1044"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Additionally, recent studies show that when addressing questions involving long-tail knowledge that appears infrequently in the training corpus, models are more likely to generate inaccurate content [1044].",
    "context_before": [
      "In terms of data composition, pre-training data may lack domain-specific knowledge, which would affect the model performance on tasks requiring specialized knowledge, such as medical or legal issues, and it will also result in significant hallucinations."
    ],
    "context_after": [
      "Training Methods."
    ],
    "references": [
      "1044"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "However, inappropriate prompt design can cause the model to overlook or misunderstand important information, leading to incorrect or irrelevant content [1044].",
    "context_before": [
      "Prompting has become the primary way for using LLMs to solve downstream tasks."
    ],
    "context_after": [
      "Recent studies have shown that the readability, format, and concreteness of user instructions would impact the model’s output ."
    ],
    "references": [
      "1044"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "HaluEval 2.0 [1044] proposes to first collect hallucinated and nonhallucinated responses to train a reward model, and then fine-tune the LLM with the reward model’s feedback using RL algorithms.",
    "context_before": [
      "Hallucination mitigation is closely related to the honest criterion in “3H” standards for human alignment, and various alignment methods like RLHF can be adopted to mitigate the model hallucination."
    ],
    "context_after": [
      "However, recent research shows that human preference data may lead LLMs to exhibit sycophantic behavior , where models prioritize catering to human demands over maintaining truthfulness."
    ],
    "references": [
      "1044"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Furthermore, data contamination has become a severe issue for fairly assessing the performance of LLMs [740], and thus setting appropriate evaluation protocol will be the basis to investigate and analyze the model capacity of LLMs.",
    "context_before": [
      "Another research direction is to explore more deep analysis on model generalization for LLMs, since increasing concerns have been raised about whether LLMs can generalize beyond the knowledge encoded by pre-training data."
    ],
    "context_after": [
      "Model Architecture."
    ],
    "references": [
      "740"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Specially, system-level or hardware-level optimization ( e.g., FlashAttention [303]) is worth more exploration to improve the efficiency of Transformer architectures.",
    "context_before": [
      "More efforts are still in need to develop improved model architectures for large-scale pre-training."
    ],
    "context_after": [
      "In addition, as an important basic capacity, existing LLMs typically maintain a long context window."
    ],
    "references": [
      "303"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "Thus, it becomes particularly important to develop systemic, economical pre-training approaches for optimizing LLMs, e.g., predictable scaling [46] and proxy model training [59].",
    "context_before": [
      "In practice, it is very challenging to pre-train capable LLMs, due to the huge compute consumption and the sensitivity to data quality and training tricks ."
    ],
    "context_after": [
      "More training recipes or principles should be investigated and shared to reduce the potential risk of degradation or failure in large-scale model optimization."
    ],
    "references": [
      "59"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "It has been shown that retrieval augmentation can extend the knowledge boundary and improve the question answering capacity [454], but may suffer from the effectiveness of long context utilization by LLMs [949].",
    "context_before": [
      "Another popular research direction is retrieval-augmented generation, where retrieved contexts from supporting sources are included into prompts for task solving."
    ],
    "context_after": [
      "Safety and Alignment."
    ],
    "references": [
      "454"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  }
]