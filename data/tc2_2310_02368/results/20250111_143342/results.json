[
  {
    "original_claim": "We train a Reward Model (RM) to score test cases based on these quality metrics, then employ it to provide feedback for the Proximal Policy Optimization (PPO) [39] algorithm to enhance LLMs to generate test cases maximizing the expected reward (i.e., higher quality tests).",
    "context_before": [
      "However, instead of relying on expensive, unpredictable, and often biased human feedback, we instill well-known quality properties into an LLM by scoring programs using static analysis, amenable to automated computation for the generated test cases."
    ],
    "context_after": [
      "We begin by generating test cases using foundational LLMs and investigating their alignment with established best practices and their susceptibility to test smells."
    ],
    "references": [
      "39"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "39",
        "main_query": "Proximal Policy Optimization (PPO) algorithm to enhance LLMs to generate test cases maximizing the expected reward (i.e., higher quality tests)",
        "rewritten_queries": [
          "PPO algorithm for improving LLMs to create test cases that maximize expected rewards and quality",
          "Using Proximal Policy Optimization to optimize LLMs for generating high-quality test cases",
          "Enhancing LLMs with PPO to produce test cases that achieve higher expected rewards"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "39",
        "query_used_for_retrieval": "Proximal Policy Optimization (PPO) algorithm to enhance LLMs to generate test cases maximizing the expected reward (i.e., higher quality tests)",
        "retrieved_docs_from_sources": [
          39,
          35,
          35,
          39,
          35,
          39,
          39,
          35,
          35,
          35
        ],
        "predicted_reference": "[35]_2210.01241",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.4,
          "ndcg_at_k": 0.8276904491046954,
          "mrr": 1.0,
          "hit_rate_at_k": 1.0
        }
      }
    ]
  },
  {
    "original_claim": "This version of Codex is a GPT-style model, pretrained on 54 million public repositories using the causal language modeling objective, and has a maximum context length of 2,048 tokens [14].",
    "context_before": [
      "We generate tests using the OpenAI Codex Cushman model, version code-cushman-."
    ],
    "context_after": [
      "We focused on a specific prompt design, the‚Äúsimulated‚Äù prompt, to harness optimal test generation capabilities from the base Codex model."
    ],
    "references": [
      "14"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "14",
        "main_query": "This version of Codex is a GPT-style model, pretrained on 54 million public repositories using the causal language modeling objective, and has a maximum context length of 2,048 tokens",
        "rewritten_queries": [
          "Codex is a GPT-style model trained on 54 million public repositories with a maximum context length of 2,048 tokens",
          "The Codex model is pretrained on 54 million repositories and utilizes a causal language modeling objective with a context length of 2,048 tokens",
          "This Codex version employs a GPT-style architecture and is trained on 54 million public repositories, supporting a context length of 2,048 tokens"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "14",
        "query_used_for_retrieval": "This version of Codex is a GPT-style model, pretrained on 54 million public repositories using the causal language modeling objective, and has a maximum context length of 2,048 tokens",
        "retrieved_docs_from_sources": [
          14,
          14,
          14,
          14,
          48,
          33,
          14,
          14,
          14,
          14
        ],
        "predicted_reference": "[14]_2107.03374",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.8,
          "ndcg_at_k": 0.9613085758737654,
          "mrr": 1.0,
          "hit_rate_at_k": 1.0
        }
      }
    ]
  },
  {
    "original_claim": "We employ adaptive focal context truncation to ensure the inputs fit within the model‚Äôs context length, similar to the representation introduced in [48].",
    "context_before": [
      "As shown in Figure 1, we construct the prompt as follows (top to bottom): ‚óè path/to/FocalClass.cs: The path to the file containing the focal method in the actual project structure. ‚óè ‚å©ContextAndFocalMethod‚å™: The source code of the focal method and its context. ‚óè path/to/TestFocalClass.cs:‚§¶ public void Test‚å©FocalMethod‚å™: A prompt hint which specifies a hypothetical filepath for the test class and the beginning of the test method signature."
    ],
    "context_after": [
      "In detail, we adopt a sequential approach to extract focal contexts, where each next option is a more concise representation than its predecessor: (1) Entire text of the focal file, which can include imports, namespaces, and other classes. (2) Focal class code, converting methods other than the focal method into their respective signatures. (3) Focal class code, converting non-focal methods to signatures and eliminating fields/comments. (4) A stripped-down version comprising only the focal class definition and focal method, while discarding all supplementary code. (5) Prompts persistently exceeding the permitted length were removed."
    ],
    "references": [
      "48"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "48",
        "main_query": "adaptive focal context truncation to ensure the inputs fit within the model‚Äôs context length",
        "rewritten_queries": [
          "using adaptive focal context truncation to make inputs compatible with the model's context length",
          "applying adaptive focal context truncation for fitting inputs within the model's context limits",
          "utilizing adaptive focal context truncation to align inputs with the model's context capacity"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "48",
        "query_used_for_retrieval": "adaptive focal context truncation to ensure the inputs fit within the model‚Äôs context length",
        "retrieved_docs_from_sources": [
          48,
          48,
          48,
          14,
          33,
          48,
          33,
          35,
          35,
          33
        ],
        "predicted_reference": "[48]_2009.05617",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.4,
          "ndcg_at_k": 0.9709286432396583,
          "mrr": 1.0,
          "hit_rate_at_k": 1.0
        }
      }
    ]
  },
  {
    "original_claim": "RLSQM utilizes the Proximal Policy Optimization (PPO) [39] algorithm to train the policy model (PM).",
    "context_before": [
      "Once the reward model is trained, it is used to predict the rewards during PPO fine-tuning. 2.5.2 PPO Fine-tuning."
    ],
    "context_after": [
      "PPO operates as an actor-critic algorithm, combining two components using networks with separate weights: the actor, determining the policy ùúã, and the critic, valuing a given state-action pair to improve the actor‚Äôs choices."
    ],
    "references": [
      "39"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "39",
        "main_query": "Proximal Policy Optimization (PPO) algorithm to train the policy model (PM)",
        "rewritten_queries": [
          "PPO algorithm used for training the policy model",
          "Training the policy model with Proximal Policy Optimization",
          "Utilization of PPO in training the policy model"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "39",
        "query_used_for_retrieval": "Proximal Policy Optimization (PPO) algorithm to train the policy model (PM)",
        "retrieved_docs_from_sources": [
          39,
          35,
          39,
          39,
          35,
          39,
          39,
          39,
          35,
          39
        ],
        "predicted_reference": "[39]_1707.06347",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.7,
          "ndcg_at_k": 0.8864065699084177,
          "mrr": 1.0,
          "hit_rate_at_k": 1.0
        }
      }
    ]
  },
  {
    "original_claim": "Additionally, PPO utilizes a clipped surrogate objective, which constrains updates to the actor network weights in order to avoid excessively large policy updates [39].",
    "context_before": [
      "We initialized the policy using the SFT model and initialized the value function using the weights of the reward model (this done following Ouyang et al. )."
    ],
    "context_after": [
      "Following Ouyang et al. , we use a variant of PPO which utilizes the Kullback-Leibler (KL) divergence to penalize the model from generating tokens which are dramatically different from the base model, modulated by a coefficient ùõΩ."
    ],
    "references": [
      "39"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "39",
        "main_query": "PPO utilizes a clipped surrogate objective, which constrains updates to the actor network weights in order to avoid excessively large policy updates",
        "rewritten_queries": [
          "PPO employs a clipped surrogate objective to limit the size of updates to the actor network weights",
          "The clipped surrogate objective in PPO restricts large policy updates to the actor network weights",
          "To prevent excessively large updates, PPO uses a clipped surrogate objective for the actor network weights"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "39",
        "query_used_for_retrieval": "PPO utilizes a clipped surrogate objective, which constrains updates to the actor network weights in order to avoid excessively large policy updates",
        "retrieved_docs_from_sources": [
          39,
          39,
          35,
          39,
          39,
          39,
          35,
          35,
          39,
          39
        ],
        "predicted_reference": "[39]_1707.06347",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.7,
          "ndcg_at_k": 0.9331394899761785,
          "mrr": 1.0,
          "hit_rate_at_k": 1.0
        }
      }
    ]
  },
  {
    "original_claim": "It is worth noting that our approach is not tied to the PPO algorithm and implementations may employ a variety of other reinforcement learning algorithms, such as A2C [30] or NLPO [35]. 3 EMPIRICAL STUDY DESIGN We have designed an empirical study with the primary objective of enhancing the quality of test cases generated by Language Model-based approaches through Reinforcement Learning from Static Quality Metrics (RLSQM).",
    "context_before": [
      "Figure 2 3 provides the details on the RLSQM‚Äôs PPO finetuning stage."
    ],
    "context_after": [
      "The study aims to address the following research questions: ‚óè RQ1: What is the quality of the test cases generated by LLMs? ‚óè RQ2: Can RLSQM be used to improve individual quality metrics? ‚óè RQ3: Can RLSQM improve the LLM to generate high-quality tests following best practices? 3.1 RQ 1: Assessing Test Case Quality To address RQ1, we generated test cases for a diverse set of public methods under tests (focal methods)."
    ],
    "references": [
      "35",
      "30"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "35",
        "main_query": "A2C or NLPO",
        "rewritten_queries": [
          "Alternative reinforcement learning algorithms like A2C and NLPO",
          "Reinforcement learning methods such as A2C and NLPO",
          "Different RL algorithms including A2C and NLPO"
        ]
      },
      {
        "related_to_reference": "30",
        "main_query": "A2C",
        "rewritten_queries": [
          "Advantage Actor-Critic algorithm",
          "A2C reinforcement learning method",
          "A2C approach in reinforcement learning"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "35",
        "query_used_for_retrieval": "A2C or NLPO",
        "retrieved_docs_from_sources": [
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35
        ],
        "predicted_reference": "[35]_2210.01241",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 1.0,
          "ndcg_at_k": 1.0,
          "mrr": 1.0,
          "hit_rate_at_k": 1.0
        }
      },
      {
        "original_reference": "30",
        "query_used_for_retrieval": "A2C",
        "retrieved_docs_from_sources": [
          39,
          39,
          30,
          30,
          39,
          30,
          39,
          30,
          30,
          30
        ],
        "predicted_reference": "[39]_1707.06347",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.6,
          "ndcg_at_k": 0.663438677598622,
          "mrr": 0.3333333333333333,
          "hit_rate_at_k": 1.0
        }
      }
    ]
  },
  {
    "original_claim": "Policy models tended to diverge substantially from the base model in order to maximize reward from the reward model, resulting in errant behavior such as (1) mode collapse [11] where the model learned to generate a narrow band of tests (such as empty tests like TestFocalMethod(){}), and (2) catastrophic forgetting, where tests disregarding all properties save for the one being optimized.",
    "context_before": [
      "For policy model training, we selected the checkpoint with the best overall quality of the generated tests on the validation dataset, measured by summing all positive properties and subtracting all negative properties."
    ],
    "context_after": [
      "Thus, validation was crucial in order to select a model which retained the abilities of the initial weights and avoided mode collapse."
    ],
    "references": [
      "11"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "11",
        "main_query": "mode collapse where the model learned to generate a narrow band of tests (such as empty tests like TestFocalMethod(){})",
        "rewritten_queries": [
          "mode collapse resulting in the generation of a limited range of tests, including empty tests like TestFocalMethod(){}",
          "the phenomenon of mode collapse leading to the model producing a narrow set of outputs, such as empty tests TestFocalMethod(){}",
          "the occurrence of mode collapse, characterized by the model's tendency to create a restricted variety of tests, including empty ones like TestFocalMethod(){}"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "11",
        "query_used_for_retrieval": "mode collapse where the model learned to generate a narrow band of tests (such as empty tests like TestFocalMethod(){})",
        "retrieved_docs_from_sources": [
          48,
          48,
          48,
          48,
          48,
          41,
          41,
          48,
          48,
          41
        ],
        "predicted_reference": "[48]_2009.05617",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.0,
          "ndcg_at_k": 0.0,
          "mrr": 0.0,
          "hit_rate_at_k": 0.0
        }
      }
    ]
  },
  {
    "original_claim": "Since GPT-4 is trained to understand and generate human-like code [33], it‚Äôs not surprising that it produced documentation and avoid consecutive duplicated assertions.",
    "context_before": [
      "GPT-4 produced tests with more Descriptive Names (23%), more Comments (78%), and fewer Duplicate Assertions (0.55%) than the Codex-based models (Base)."
    ],
    "context_after": [
      "The SFT model improved upon the Base model in all properties except Descriptive Names and Comments, but it could not be optimized beyond a certain point."
    ],
    "references": [
      "33"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "33",
        "main_query": "GPT-4 is trained to understand and generate human-like code",
        "rewritten_queries": [
          "GPT-4 has been designed to comprehend and produce code that resembles human writing",
          "The training of GPT-4 enables it to generate code in a human-like manner",
          "GPT-4's capabilities include understanding and creating code similar to that of humans"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "33",
        "query_used_for_retrieval": "GPT-4 is trained to understand and generate human-like code",
        "retrieved_docs_from_sources": [
          33,
          33,
          33,
          33,
          33,
          33,
          14,
          33,
          33,
          33
        ],
        "predicted_reference": "[33]_2303.08774",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.9,
          "ndcg_at_k": 0.9895948844467957,
          "mrr": 1.0,
          "hit_rate_at_k": 1.0
        }
      }
    ]
  },
  {
    "original_claim": "The aim of these efforts is to enhance compilation rates, produce passing tests from the current code, generate failing tests to expose bugs, or boost code coverage. [41] demonstrated that ChatGPT and Codex are prone to produce test smells on Python and Java code, but do not suggest how to improve the models.",
    "context_before": [
      "RL from Sequential and Combined Rewards (optimizing ‚úì Focal ‚Üí‚úó Conditional/Exception) improved upon the Base model further than Individual Rewards by increasing best practices: up to ‚Üë23.1% Assertions and ‚Üë20.5% Focal calls, and reducing test smells: up to ‚Üì1.8% Duplicate Assertions and ‚Üì2.4% Conditionals/Exceptions. 5 RELATED WORK Test Generation: Previous research on unit test generation has employed evolutionary algorithms, leading to tools such as EvoSuite and machine learning models ."
    ],
    "context_after": [
      "In this study, we introduce RLSQM as a method to enhance language models based on static quality metrics."
    ],
    "references": [
      "41"
    ],
    "doc_retrieval_queries": [],
    "is_positive": true,
    "results": null
  },
  {
    "original_claim": "This alignment is primarily achieved through a methodology termed Reinforcement Learning from Human Feedback (RLHF) [16].",
    "context_before": [
      "18 Benjamin Steenhoek, Michele Tufano, Neel Sundaresan, and Alexey Svyatkovskiy RL for LLMs: Reinforcement Learning has emerged as a promising tool to better align LLMs with human intentions and preferences."
    ],
    "context_after": [
      "Foundational works in this direction, such as InstructGPT , RLAIF , LLAMA2, and Code LLama , have proven the effectiveness of RLHF in refining LLM performance."
    ],
    "references": [
      "16"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "16",
        "main_query": "Reinforcement Learning from Human Feedback (RLHF)",
        "rewritten_queries": [
          "RLHF methodology for aligning LLMs with human feedback",
          "Reinforcement Learning techniques based on human feedback",
          "Using human feedback in Reinforcement Learning for alignment"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "16",
        "query_used_for_retrieval": "Reinforcement Learning from Human Feedback (RLHF)",
        "retrieved_docs_from_sources": [
          11,
          11,
          11,
          11,
          16,
          11,
          11,
          11,
          11,
          11
        ],
        "predicted_reference": "[11]_2307.15217",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.1,
          "ndcg_at_k": 0.38685280723454163,
          "mrr": 0.2,
          "hit_rate_at_k": 1.0
        }
      }
    ]
  },
  {
    "original_claim": "In a recent paper of Nakamura on E-FISH, the occurrence of laser induced streamers is described, but these discharges do not affect the E-FISH signal.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "In a recent paper of Nakamura on E-FISH, the occurrence of laser induced streamers is described, but these discharges do not affect the E-FISH signal.",
        "retrieved_docs_from_sources": [
          14,
          48,
          41,
          11,
          48,
          14,
          48,
          48,
          35,
          48
        ],
        "predicted_reference": "[48]_2009.05617",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "Rabi measurements were performed prior to the Hahn-echo to obtain the exact œÄ-pulse duration.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "Rabi measurements were performed prior to the Hahn-echo to obtain the exact œÄ-pulse duration.",
        "retrieved_docs_from_sources": [
          35,
          35,
          35,
          35,
          35,
          35,
          33,
          39,
          35,
          35
        ],
        "predicted_reference": "none",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "Our model generalizes the two-state model introduced in and we have called this framework the ¬µ-chemoEH model.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "Our model generalizes the two-state model introduced in and we have called this framework the ¬µ-chemoEH model.",
        "retrieved_docs_from_sources": [
          33,
          33,
          33,
          33,
          48,
          48,
          33,
          48,
          33,
          33
        ],
        "predicted_reference": "[33]_2303.08774",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "This assumption is based on the fact that, under the effect of homophily , individuals tend to imitate the behavior of their peers.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "This assumption is based on the fact that, under the effect of homophily , individuals tend to imitate the behavior of their peers.",
        "retrieved_docs_from_sources": [
          11,
          11,
          16,
          11,
          14,
          16,
          16,
          16,
          11,
          16
        ],
        "predicted_reference": "[16]_1706.03741",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "This makes neurostimulation a possible effective method for interrupting pathological hypersynchronous activity in the interconnected neuronal network of the brain .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "This makes neurostimulation a possible effective method for interrupting pathological hypersynchronous activity in the interconnected neuronal network of the brain .",
        "retrieved_docs_from_sources": [
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30,
          30
        ],
        "predicted_reference": "[30]_1602.01783",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "A detailed account of the boundary-conforming approach can be found in the recent review by Wachs .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "A detailed account of the boundary-conforming approach can be found in the recent review by Wachs .",
        "retrieved_docs_from_sources": [
          14,
          14,
          11,
          11,
          11,
          14,
          11,
          11,
          11,
          48
        ],
        "predicted_reference": "[11]_2307.15217",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "Initial attempts to apply the immersed boundary method in the context of lattice-Boltzmann simulations were based on the feedback approach due to and alluded to in the present ¬ß 5.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "Initial attempts to apply the immersed boundary method in the context of lattice-Boltzmann simulations were based on the feedback approach due to and alluded to in the present ¬ß 5.",
        "retrieved_docs_from_sources": [
          30,
          30,
          11,
          11,
          30,
          39,
          39,
          30,
          30,
          35
        ],
        "predicted_reference": "[30]_1602.01783",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "In 2004, Resch and Steinberg incorporated the idea of weak measurement in a more general scenario, a joint quantum measurement .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "In 2004, Resch and Steinberg incorporated the idea of weak measurement in a more general scenario, a joint quantum measurement .",
        "retrieved_docs_from_sources": [
          11,
          33,
          11,
          35,
          35,
          11,
          11,
          11,
          11,
          11
        ],
        "predicted_reference": "[11]_2307.15217",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "As noted in , in the chemoEH there are two characteristic times œÑ0 = ( œÄ0 + Œµ0)‚àí1 and n0/œÄ0 = (œÄ0 + Œµ0e ¬Øf )‚àí.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "As noted in , in the chemoEH there are two characteristic times œÑ0 = ( œÄ0 + Œµ0)‚àí1 and n0/œÄ0 = (œÄ0 + Œµ0e ¬Øf )‚àí.",
        "retrieved_docs_from_sources": [
          48,
          39,
          39,
          30,
          30,
          39,
          30,
          48,
          35,
          16
        ],
        "predicted_reference": "[39]_1707.06347",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "This pre and postselection combination enhances both the real and imaginary parts of ‚ü®A‚ü©W and ‚ü®B‚ü©W (see Appendix D) which are necessary for the computation of JWV as mentioned in Eq. (2), (3) .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "This pre and postselection combination enhances both the real and imaginary parts of ‚ü®A‚ü©W and ‚ü®B‚ü©W (see Appendix D) which are necessary for the computation of JWV as mentioned in Eq. (2), (3) .",
        "retrieved_docs_from_sources": [
          48,
          48,
          48,
          35,
          35,
          35,
          16,
          35,
          41,
          48
        ],
        "predicted_reference": "[48]_2009.05617",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  }
]