[
  {
    "original_claim": "We train a Reward Model (RM) to score test cases based on these quality metrics, then employ it to provide feedback for the Proximal Policy Optimization (PPO) [39] algorithm to enhance LLMs to generate test cases maximizing the expected reward (i.e., higher quality tests).",
    "context_before": [
      "However, instead of relying on expensive, unpredictable, and often biased human feedback, we instill well-known quality properties into an LLM by scoring programs using static analysis, amenable to automated computation for the generated test cases."
    ],
    "context_after": [
      "We begin by generating test cases using foundational LLMs and investigating their alignment with established best practices and their susceptibility to test smells."
    ],
    "references": [
      "39"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "39",
        "main_query": "Proximal Policy Optimization (PPO) algorithm to enhance LLMs to generate test cases maximizing the expected reward (i.e., higher quality tests)",
        "rewritten_queries": [
          "PPO algorithm used to improve LLMs for generating high-quality test cases that maximize expected rewards",
          "Enhancing LLMs with the Proximal Policy Optimization method to produce test cases that achieve higher quality scores",
          "Using the PPO algorithm to optimize LLMs in generating test cases that yield maximum expected rewards"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "39",
        "query_used_for_retrieval": "Proximal Policy Optimization (PPO) algorithm to enhance LLMs to generate test cases maximizing the expected reward (i.e., higher quality tests)",
        "retrieved_docs_from_sources": [
          39,
          35,
          35,
          39,
          35,
          39,
          39,
          35,
          35,
          35
        ],
        "predicted_reference": "[35]_2210.01241",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.4,
          "ndcg_at_k": 0.8276904491046954,
          "mrr": 1.0,
          "hit_rate_at_k": 1.0
        }
      }
    ]
  },
  {
    "original_claim": "This version of Codex is a GPT-style model, pretrained on 54 million public repositories using the causal language modeling objective, and has a maximum context length of 2,048 tokens [14].",
    "context_before": [
      "We generate tests using the OpenAI Codex Cushman model, version code-cushman-."
    ],
    "context_after": [
      "We focused on a specific prompt design, the‚Äúsimulated‚Äù prompt, to harness optimal test generation capabilities from the base Codex model."
    ],
    "references": [
      "14"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "14",
        "main_query": "This version of Codex is a GPT-style model, pretrained on 54 million public repositories using the causal language modeling objective, and has a maximum context length of 2,048 tokens",
        "rewritten_queries": [
          "Codex is a GPT-style model trained on 54 million public repositories with a maximum context length of 2,048 tokens",
          "The Codex model is pretrained on 54 million repositories and utilizes a causal language modeling objective with a context length of 2,048 tokens",
          "This Codex version, a GPT-style model, has been trained on 54 million public repositories and supports a maximum context length of 2,048 tokens"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "14",
        "query_used_for_retrieval": "This version of Codex is a GPT-style model, pretrained on 54 million public repositories using the causal language modeling objective, and has a maximum context length of 2,048 tokens",
        "retrieved_docs_from_sources": [
          14,
          14,
          14,
          14,
          33,
          14,
          14,
          14,
          14,
          14
        ],
        "predicted_reference": "[14]_2107.03374",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.9,
          "ndcg_at_k": 0.9770153702993118,
          "mrr": 1.0,
          "hit_rate_at_k": 1.0
        }
      }
    ]
  },
  {
    "original_claim": "We employ adaptive focal context truncation to ensure the inputs fit within the model‚Äôs context length, similar to the representation introduced in [48].",
    "context_before": [
      "As shown in Figure 1, we construct the prompt as follows (top to bottom): ‚óè path/to/FocalClass.cs: The path to the file containing the focal method in the actual project structure. ‚óè ‚å©ContextAndFocalMethod‚å™: The source code of the focal method and its context. ‚óè path/to/TestFocalClass.cs:‚§¶ public void Test‚å©FocalMethod‚å™: A prompt hint which specifies a hypothetical filepath for the test class and the beginning of the test method signature."
    ],
    "context_after": [
      "In detail, we adopt a sequential approach to extract focal contexts, where each next option is a more concise representation than its predecessor: (1) Entire text of the focal file, which can include imports, namespaces, and other classes. (2) Focal class code, converting methods other than the focal method into their respective signatures. (3) Focal class code, converting non-focal methods to signatures and eliminating fields/comments. (4) A stripped-down version comprising only the focal class definition and focal method, while discarding all supplementary code. (5) Prompts persistently exceeding the permitted length were removed."
    ],
    "references": [
      "48"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "48",
        "main_query": "adaptive focal context truncation to ensure the inputs fit within the model‚Äôs context length",
        "rewritten_queries": [
          "using adaptive focal context truncation to make inputs compatible with the model's context length",
          "applying adaptive focal context truncation for fitting inputs within the model's context limits",
          "utilizing adaptive focal context truncation to align inputs with the model's context capacity"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "48",
        "query_used_for_retrieval": "adaptive focal context truncation to ensure the inputs fit within the model‚Äôs context length",
        "retrieved_docs_from_sources": [
          48,
          48,
          48,
          14,
          33,
          48,
          14,
          33,
          33,
          33
        ],
        "predicted_reference": "[48]_2009.05617",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.4,
          "ndcg_at_k": 0.9709286432396583,
          "mrr": 1.0,
          "hit_rate_at_k": 1.0
        }
      }
    ]
  },
  {
    "original_claim": "RLSQM utilizes the Proximal Policy Optimization (PPO) [39] algorithm to train the policy model (PM).",
    "context_before": [
      "Once the reward model is trained, it is used to predict the rewards during PPO fine-tuning. 2.5.2 PPO Fine-tuning."
    ],
    "context_after": [
      "PPO operates as an actor-critic algorithm, combining two components using networks with separate weights: the actor, determining the policy ùúã, and the critic, valuing a given state-action pair to improve the actor‚Äôs choices."
    ],
    "references": [
      "39"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "39",
        "main_query": "Proximal Policy Optimization (PPO) algorithm to train the policy model (PM)",
        "rewritten_queries": [
          "PPO algorithm for training the policy model",
          "Using Proximal Policy Optimization to train the policy model",
          "Training the policy model with the PPO algorithm"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "39",
        "query_used_for_retrieval": "Proximal Policy Optimization (PPO) algorithm to train the policy model (PM)",
        "retrieved_docs_from_sources": [
          39,
          35,
          39,
          39,
          35,
          39,
          39,
          39,
          35,
          39
        ],
        "predicted_reference": "[39]_1707.06347",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.7,
          "ndcg_at_k": 0.8864065699084177,
          "mrr": 1.0,
          "hit_rate_at_k": 1.0
        }
      }
    ]
  },
  {
    "original_claim": "Additionally, PPO utilizes a clipped surrogate objective, which constrains updates to the actor network weights in order to avoid excessively large policy updates [39].",
    "context_before": [
      "We initialized the policy using the SFT model and initialized the value function using the weights of the reward model (this done following Ouyang et al. )."
    ],
    "context_after": [
      "Following Ouyang et al. , we use a variant of PPO which utilizes the Kullback-Leibler (KL) divergence to penalize the model from generating tokens which are dramatically different from the base model, modulated by a coefficient ùõΩ."
    ],
    "references": [
      "39"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "39",
        "main_query": "PPO utilizes a clipped surrogate objective, which constrains updates to the actor network weights in order to avoid excessively large policy updates",
        "rewritten_queries": [
          "PPO employs a clipped surrogate objective to limit the size of updates to the actor network weights",
          "The clipped surrogate objective in PPO restricts large policy updates to the actor network weights",
          "To prevent excessively large updates, PPO uses a clipped surrogate objective for the actor network weights"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "39",
        "query_used_for_retrieval": "PPO utilizes a clipped surrogate objective, which constrains updates to the actor network weights in order to avoid excessively large policy updates",
        "retrieved_docs_from_sources": [
          39,
          39,
          35,
          39,
          39,
          39,
          35,
          35,
          39,
          39
        ],
        "predicted_reference": "[39]_1707.06347",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.7,
          "ndcg_at_k": 0.9331394899761785,
          "mrr": 1.0,
          "hit_rate_at_k": 1.0
        }
      }
    ]
  },
  {
    "original_claim": "It is worth noting that our approach is not tied to the PPO algorithm and implementations may employ a variety of other reinforcement learning algorithms, such as A2C [30] or NLPO [35]. 3 EMPIRICAL STUDY DESIGN We have designed an empirical study with the primary objective of enhancing the quality of test cases generated by Language Model-based approaches through Reinforcement Learning from Static Quality Metrics (RLSQM).",
    "context_before": [
      "Figure 2 3 provides the details on the RLSQM‚Äôs PPO finetuning stage."
    ],
    "context_after": [
      "The study aims to address the following research questions: ‚óè RQ1: What is the quality of the test cases generated by LLMs? ‚óè RQ2: Can RLSQM be used to improve individual quality metrics? ‚óè RQ3: Can RLSQM improve the LLM to generate high-quality tests following best practices? 3.1 RQ 1: Assessing Test Case Quality To address RQ1, we generated test cases for a diverse set of public methods under tests (focal methods)."
    ],
    "references": [
      "30",
      "35"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "30",
        "main_query": "A2C",
        "rewritten_queries": [
          "A2C reinforcement learning algorithm",
          "Alternative reinforcement learning methods like A2C",
          "Different RL algorithms including A2C"
        ]
      },
      {
        "related_to_reference": "35",
        "main_query": "A2C or NLPO",
        "rewritten_queries": [
          "Alternative reinforcement learning algorithms like A2C and NLPO",
          "Reinforcement learning methods such as A2C and NLPO",
          "Different RL algorithms including A2C and NLPO"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "30",
        "query_used_for_retrieval": "A2C",
        "retrieved_docs_from_sources": [
          39,
          39,
          16,
          30,
          30,
          39,
          30,
          39,
          30,
          30
        ],
        "predicted_reference": "[39]_1707.06347",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.5,
          "ndcg_at_k": 0.590463510067189,
          "mrr": 0.25,
          "hit_rate_at_k": 1.0
        }
      },
      {
        "original_reference": "35",
        "query_used_for_retrieval": "A2C or NLPO",
        "retrieved_docs_from_sources": [
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          35
        ],
        "predicted_reference": "[35]_2210.01241",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 1.0,
          "ndcg_at_k": 1.0,
          "mrr": 1.0,
          "hit_rate_at_k": 1.0
        }
      }
    ]
  },
  {
    "original_claim": "Policy models tended to diverge substantially from the base model in order to maximize reward from the reward model, resulting in errant behavior such as (1) mode collapse [11] where the model learned to generate a narrow band of tests (such as empty tests like TestFocalMethod(){}), and (2) catastrophic forgetting, where tests disregarding all properties save for the one being optimized.",
    "context_before": [
      "For policy model training, we selected the checkpoint with the best overall quality of the generated tests on the validation dataset, measured by summing all positive properties and subtracting all negative properties."
    ],
    "context_after": [
      "Thus, validation was crucial in order to select a model which retained the abilities of the initial weights and avoided mode collapse."
    ],
    "references": [
      "11"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "11",
        "main_query": "mode collapse where the model learned to generate a narrow band of tests (such as empty tests like TestFocalMethod(){})",
        "rewritten_queries": [
          "mode collapse resulting in the generation of a limited range of tests, including empty tests like TestFocalMethod(){}",
          "the phenomenon of mode collapse leading to the model producing a narrow set of outputs, such as empty tests TestFocalMethod(){}",
          "the occurrence of mode collapse, characterized by the model's tendency to create a restricted variety of tests, including empty ones like TestFocalMethod(){}"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "11",
        "query_used_for_retrieval": "mode collapse where the model learned to generate a narrow band of tests (such as empty tests like TestFocalMethod(){})",
        "retrieved_docs_from_sources": [
          48,
          48,
          48,
          48,
          48,
          41,
          41,
          48,
          48,
          41
        ],
        "predicted_reference": "[48]_2009.05617",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.0,
          "ndcg_at_k": 0.0,
          "mrr": 0.0,
          "hit_rate_at_k": 0.0
        }
      }
    ]
  },
  {
    "original_claim": "Since GPT-4 is trained to understand and generate human-like code [33], it‚Äôs not surprising that it produced documentation and avoid consecutive duplicated assertions.",
    "context_before": [
      "GPT-4 produced tests with more Descriptive Names (23%), more Comments (78%), and fewer Duplicate Assertions (0.55%) than the Codex-based models (Base)."
    ],
    "context_after": [
      "The SFT model improved upon the Base model in all properties except Descriptive Names and Comments, but it could not be optimized beyond a certain point."
    ],
    "references": [
      "33"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "33",
        "main_query": "GPT-4 is trained to understand and generate human-like code",
        "rewritten_queries": [
          "GPT-4's training enables it to comprehend and produce code that resembles human writing",
          "The ability of GPT-4 to generate human-like code stems from its training",
          "GPT-4 has been designed to understand and create code in a manner similar to humans"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "33",
        "query_used_for_retrieval": "GPT-4 is trained to understand and generate human-like code",
        "retrieved_docs_from_sources": [
          33,
          33,
          33,
          33,
          33,
          33,
          14,
          33,
          33,
          33
        ],
        "predicted_reference": "[33]_2303.08774",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.9,
          "ndcg_at_k": 0.9895948844467957,
          "mrr": 1.0,
          "hit_rate_at_k": 1.0
        }
      }
    ]
  },
  {
    "original_claim": "The aim of these efforts is to enhance compilation rates, produce passing tests from the current code, generate failing tests to expose bugs, or boost code coverage. [41] demonstrated that ChatGPT and Codex are prone to produce test smells on Python and Java code, but do not suggest how to improve the models.",
    "context_before": [
      "RL from Sequential and Combined Rewards (optimizing ‚úì Focal ‚Üí‚úó Conditional/Exception) improved upon the Base model further than Individual Rewards by increasing best practices: up to ‚Üë23.1% Assertions and ‚Üë20.5% Focal calls, and reducing test smells: up to ‚Üì1.8% Duplicate Assertions and ‚Üì2.4% Conditionals/Exceptions. 5 RELATED WORK Test Generation: Previous research on unit test generation has employed evolutionary algorithms, leading to tools such as EvoSuite and machine learning models ."
    ],
    "context_after": [
      "In this study, we introduce RLSQM as a method to enhance language models based on static quality metrics."
    ],
    "references": [
      "41"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "41",
        "main_query": "ChatGPT and Codex are prone to produce test smells on Python and Java code, but do not suggest how to improve the models.",
        "rewritten_queries": [
          "Test smells are produced by ChatGPT and Codex in Python and Java, without suggestions for model improvement.",
          "ChatGPT and Codex generate test smells in Python and Java code, lacking recommendations for enhancement.",
          "The models ChatGPT and Codex create test smells in Python and Java, but fail to provide improvement strategies."
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "41",
        "query_used_for_retrieval": "ChatGPT and Codex are prone to produce test smells on Python and Java code, but do not suggest how to improve the models.",
        "retrieved_docs_from_sources": [
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          41
        ],
        "predicted_reference": "[14]_2107.03374",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.1,
          "ndcg_at_k": 0.2890648263178879,
          "mrr": 0.1,
          "hit_rate_at_k": 1.0
        }
      }
    ]
  },
  {
    "original_claim": "This alignment is primarily achieved through a methodology termed Reinforcement Learning from Human Feedback (RLHF) [16].",
    "context_before": [
      "18 Benjamin Steenhoek, Michele Tufano, Neel Sundaresan, and Alexey Svyatkovskiy RL for LLMs: Reinforcement Learning has emerged as a promising tool to better align LLMs with human intentions and preferences."
    ],
    "context_after": [
      "Foundational works in this direction, such as InstructGPT , RLAIF , LLAMA2, and Code LLama , have proven the effectiveness of RLHF in refining LLM performance."
    ],
    "references": [
      "16"
    ],
    "doc_retrieval_queries": [
      {
        "related_to_reference": "16",
        "main_query": "Reinforcement Learning from Human Feedback (RLHF)",
        "rewritten_queries": [
          "Methodology of Reinforcement Learning from Human Feedback",
          "RLHF as a method for aligning LLMs with human preferences",
          "Using Reinforcement Learning from Human Feedback for alignment"
        ]
      }
    ],
    "is_positive": true,
    "results": [
      {
        "original_reference": "16",
        "query_used_for_retrieval": "Reinforcement Learning from Human Feedback (RLHF)",
        "retrieved_docs_from_sources": [
          11,
          11,
          11,
          11,
          16,
          11,
          11,
          11,
          11,
          11
        ],
        "predicted_reference": "[11]_2307.15217",
        "prediction_validated_by_human": 0,
        "ranking_metrics": {
          "precision_at_k": 0.1,
          "ndcg_at_k": 0.38685280723454163,
          "mrr": 0.2,
          "hit_rate_at_k": 1.0
        }
      }
    ]
  },
  {
    "original_claim": "Other authors have developed approaches for treating conjugate heat transfer problems throughout the fluid and solid domains .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "Other authors have developed approaches for treating conjugate heat transfer problems throughout the fluid and solid domains .",
        "retrieved_docs_from_sources": [
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11
        ],
        "predicted_reference": "none",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "V oluntary vaccination decisions are influenced by various factors, such as vaccine cost, peer decisions, personal experiences, and information dissemination from public health institutions , .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "V oluntary vaccination decisions are influenced by various factors, such as vaccine cost, peer decisions, personal experiences, and information dissemination from public health institutions , .",
        "retrieved_docs_from_sources": [
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          33,
          11
        ],
        "predicted_reference": "data\\tc2_2310_02368\\referenced_papers\\[11]_2307.15217",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "From research on the memory effect, it is determined that electron densities of 108 ‚àí 109 cm‚àí3, which are far below the breakdown threshold, are already sufficient to guide the plasma in a direction nearly perpendicular to the background electric field direction .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "From research on the memory effect, it is determined that electron densities of 108 ‚àí 109 cm‚àí3, which are far below the breakdown threshold, are already sufficient to guide the plasma in a direction nearly perpendicular to the background electric field direction .",
        "retrieved_docs_from_sources": [
          33,
          33,
          33,
          35,
          33,
          33,
          33,
          33,
          35,
          11
        ],
        "predicted_reference": "none",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "We would also like to thank Helen Diller Quantum Center, Technion for supporting financially this work. F.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "We would also like to thank Helen Diller Quantum Center, Technion for supporting financially this work. F.",
        "retrieved_docs_from_sources": [
          33,
          33,
          33,
          33,
          11,
          11,
          48,
          48,
          11,
          30
        ],
        "predicted_reference": "[33]_2303.08774",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "While initially detectors with electron cooling were suggested for X-ray frequency range , recently it has been predicted that CEBs can be used as single photon detectors with photon wavelengths up to 1 cm .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "While initially detectors with electron cooling were suggested for X-ray frequency range , recently it has been predicted that CEBs can be used as single photon detectors with photon wavelengths up to 1 cm .",
        "retrieved_docs_from_sources": [
          14,
          33,
          33,
          14,
          14,
          14,
          33,
          14,
          16,
          16
        ],
        "predicted_reference": "none",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "Although no unique measure of ‚Äúevenness‚Äù of a spatial distribution in three spatial dimensions exists, both iterative approaches as well as explicit constructions on the sphere are available for practical purposes, as discussed e.g. in and .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "Although no unique measure of ‚Äúevenness‚Äù of a spatial distribution in three spatial dimensions exists, both iterative approaches as well as explicit constructions on the sphere are available for practical purposes, as discussed e.g. in and .",
        "retrieved_docs_from_sources": [
          35,
          35,
          35,
          16,
          11,
          35,
          35,
          35,
          35,
          39
        ],
        "predicted_reference": "[35]_2210.01241",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "This technique, however, provides discrete measurements for individual species and its measurement accuracy is limited by the isotope ‚Äòscrambling‚Äô fragmentation, i.e. the NO+ fragment ions containing the terminal N atom, rather than the central N attached to the O atom as in the original molecule .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "This technique, however, provides discrete measurements for individual species and its measurement accuracy is limited by the isotope ‚Äòscrambling‚Äô fragmentation, i.e. the NO+ fragment ions containing the terminal N atom, rather than the central N attached to the O atom as in the original molecule .",
        "retrieved_docs_from_sources": [
          35,
          33,
          35,
          35,
          33,
          35,
          35,
          33,
          35,
          48
        ],
        "predicted_reference": "[35]_2210.01241",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "‚àó niladri.modak@tuni.fi ‚Ä† nghosh@iiserkol.ac.in dependent modifications in the joint canonical position and/or momentum variable (second moment) of the pointer, unlike just a shift (first moment) in the pointer variable in case of single weak measurement .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "‚àó niladri.modak@tuni.fi ‚Ä† nghosh@iiserkol.ac.in dependent modifications in the joint canonical position and/or momentum variable (second moment) of the pointer, unlike just a shift (first moment) in the pointer variable in case of single weak measurement .",
        "retrieved_docs_from_sources": [
          35,
          35,
          30,
          35,
          35,
          39,
          35,
          39,
          35,
          35
        ],
        "predicted_reference": "data\\tc2_2310_02368\\referenced_papers\\[35]_2210.01241",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "The household, is typically the scenario with the most frequent collective interactions and highest infection rates across all ages .",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "The household, is typically the scenario with the most frequent collective interactions and highest infection rates across all ages .",
        "retrieved_docs_from_sources": [
          35,
          35,
          35,
          35,
          35,
          35,
          35,
          14,
          14,
          35
        ],
        "predicted_reference": "none",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  },
  {
    "original_claim": "The initial infected population is randomly distributed across the entire population , and I0 = 1e ‚àí 5 is set to represent the proportion of initially infected individuals.",
    "context_before": [],
    "context_after": [],
    "references": [],
    "doc_retrieval_queries": [],
    "is_positive": false,
    "results": [
      {
        "original_reference": "",
        "query_used_for_retrieval": "The initial infected population is randomly distributed across the entire population , and I0 = 1e ‚àí 5 is set to represent the proportion of initially infected individuals.",
        "retrieved_docs_from_sources": [
          35,
          39,
          35,
          16,
          39,
          35,
          35,
          39,
          35,
          35
        ],
        "predicted_reference": "none",
        "prediction_validated_by_human": 0,
        "ranking_metrics": null
      }
    ]
  }
]