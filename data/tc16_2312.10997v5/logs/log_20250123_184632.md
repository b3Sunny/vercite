# Claims Processing Log

Processing started at: 2025-01-23 18:46:32

## Table of Contents

[[log_20250123_184632###Claim 1/179|Claim 1/179]]
[[log_20250123_184632###Claim 2/179|Claim 2/179]]
[[log_20250123_184632###Claim 3/179|Claim 3/179]]
[[log_20250123_184632###Claim 4/179|Claim 4/179]]
[[log_20250123_184632###Claim 5/179|Claim 5/179]]
[[log_20250123_184632###Claim 6/179|Claim 6/179]]
[[log_20250123_184632###Claim 7/179|Claim 7/179]]
[[log_20250123_184632###Claim 8/179|Claim 8/179]]
[[log_20250123_184632###Claim 9/179|Claim 9/179]]
[[log_20250123_184632###Claim 10/179|Claim 10/179]]
[[log_20250123_184632###Claim 11/179|Claim 11/179]]
[[log_20250123_184632###Claim 12/179|Claim 12/179]]
[[log_20250123_184632###Claim 13/179|Claim 13/179]]
[[log_20250123_184632###Claim 14/179|Claim 14/179]]
[[log_20250123_184632###Claim 15/179|Claim 15/179]]
[[log_20250123_184632###Claim 16/179|Claim 16/179]]
[[log_20250123_184632###Claim 17/179|Claim 17/179]]
[[log_20250123_184632###Claim 18/179|Claim 18/179]]
[[log_20250123_184632###Claim 19/179|Claim 19/179]]
[[log_20250123_184632###Claim 20/179|Claim 20/179]]
[[log_20250123_184632###Claim 21/179|Claim 21/179]]
[[log_20250123_184632###Claim 22/179|Claim 22/179]]
[[log_20250123_184632###Claim 23/179|Claim 23/179]]
[[log_20250123_184632###Claim 24/179|Claim 24/179]]
[[log_20250123_184632###Claim 25/179|Claim 25/179]]
[[log_20250123_184632###Claim 26/179|Claim 26/179]]
[[log_20250123_184632###Claim 27/179|Claim 27/179]]
[[log_20250123_184632###Claim 28/179|Claim 28/179]]
[[log_20250123_184632###Claim 29/179|Claim 29/179]]
[[log_20250123_184632###Claim 30/179|Claim 30/179]]
[[log_20250123_184632###Claim 31/179|Claim 31/179]]
[[log_20250123_184632###Claim 32/179|Claim 32/179]]
[[log_20250123_184632###Claim 33/179|Claim 33/179]]
[[log_20250123_184632###Claim 34/179|Claim 34/179]]
[[log_20250123_184632###Claim 35/179|Claim 35/179]]
[[log_20250123_184632###Claim 36/179|Claim 36/179]]
[[log_20250123_184632###Claim 37/179|Claim 37/179]]
[[log_20250123_184632###Claim 38/179|Claim 38/179]]
[[log_20250123_184632###Claim 39/179|Claim 39/179]]
[[log_20250123_184632###Claim 40/179|Claim 40/179]]
[[log_20250123_184632###Claim 41/179|Claim 41/179]]
[[log_20250123_184632###Claim 42/179|Claim 42/179]]
[[log_20250123_184632###Claim 43/179|Claim 43/179]]
[[log_20250123_184632###Claim 44/179|Claim 44/179]]
[[log_20250123_184632###Claim 45/179|Claim 45/179]]
[[log_20250123_184632###Claim 46/179|Claim 46/179]]
[[log_20250123_184632###Claim 47/179|Claim 47/179]]
[[log_20250123_184632###Claim 48/179|Claim 48/179]]
[[log_20250123_184632###Claim 49/179|Claim 49/179]]
[[log_20250123_184632###Claim 50/179|Claim 50/179]]
[[log_20250123_184632###Claim 51/179|Claim 51/179]]
[[log_20250123_184632###Claim 52/179|Claim 52/179]]
[[log_20250123_184632###Claim 53/179|Claim 53/179]]
[[log_20250123_184632###Claim 54/179|Claim 54/179]]
[[log_20250123_184632###Claim 55/179|Claim 55/179]]
[[log_20250123_184632###Claim 56/179|Claim 56/179]]
[[log_20250123_184632###Claim 57/179|Claim 57/179]]
[[log_20250123_184632###Claim 58/179|Claim 58/179]]
[[log_20250123_184632###Claim 59/179|Claim 59/179]]
[[log_20250123_184632###Claim 60/179|Claim 60/179]]
[[log_20250123_184632###Claim 61/179|Claim 61/179]]
[[log_20250123_184632###Claim 62/179|Claim 62/179]]
[[log_20250123_184632###Claim 63/179|Claim 63/179]]
[[log_20250123_184632###Claim 64/179|Claim 64/179]]
[[log_20250123_184632###Claim 65/179|Claim 65/179]]
[[log_20250123_184632###Claim 66/179|Claim 66/179]]
[[log_20250123_184632###Claim 67/179|Claim 67/179]]
[[log_20250123_184632###Claim 68/179|Claim 68/179]]
[[log_20250123_184632###Claim 69/179|Claim 69/179]]
[[log_20250123_184632###Claim 70/179|Claim 70/179]]
[[log_20250123_184632###Claim 71/179|Claim 71/179]]
[[log_20250123_184632###Claim 72/179|Claim 72/179]]
[[log_20250123_184632###Claim 73/179|Claim 73/179]]
[[log_20250123_184632###Claim 74/179|Claim 74/179]]
[[log_20250123_184632###Claim 75/179|Claim 75/179]]
[[log_20250123_184632###Claim 76/179|Claim 76/179]]
[[log_20250123_184632###Claim 77/179|Claim 77/179]]
[[log_20250123_184632###Claim 78/179|Claim 78/179]]
[[log_20250123_184632###Claim 79/179|Claim 79/179]]
[[log_20250123_184632###Claim 80/179|Claim 80/179]]
[[log_20250123_184632###Claim 81/179|Claim 81/179]]
[[log_20250123_184632###Claim 82/179|Claim 82/179]]
[[log_20250123_184632###Claim 83/179|Claim 83/179]]
[[log_20250123_184632###Claim 84/179|Claim 84/179]]
[[log_20250123_184632###Claim 85/179|Claim 85/179]]
[[log_20250123_184632###Claim 86/179|Claim 86/179]]
[[log_20250123_184632###Claim 87/179|Claim 87/179]]
[[log_20250123_184632###Claim 88/179|Claim 88/179]]
[[log_20250123_184632###Claim 89/179|Claim 89/179]]
[[log_20250123_184632###Claim 90/179|Claim 90/179]]
[[log_20250123_184632###Claim 91/179|Claim 91/179]]
[[log_20250123_184632###Claim 92/179|Claim 92/179]]
[[log_20250123_184632###Claim 93/179|Claim 93/179]]
[[log_20250123_184632###Claim 94/179|Claim 94/179]]
[[log_20250123_184632###Claim 95/179|Claim 95/179]]
[[log_20250123_184632###Claim 96/179|Claim 96/179]]
[[log_20250123_184632###Claim 97/179|Claim 97/179]]
[[log_20250123_184632###Claim 98/179|Claim 98/179]]
[[log_20250123_184632###Claim 99/179|Claim 99/179]]
[[log_20250123_184632###Claim 100/179|Claim 100/179]]
[[log_20250123_184632###Claim 101/179|Claim 101/179]]
[[log_20250123_184632###Claim 102/179|Claim 102/179]]
[[log_20250123_184632###Claim 103/179|Claim 103/179]]
[[log_20250123_184632###Claim 104/179|Claim 104/179]]
[[log_20250123_184632###Claim 105/179|Claim 105/179]]
[[log_20250123_184632###Claim 106/179|Claim 106/179]]
[[log_20250123_184632###Claim 107/179|Claim 107/179]]
[[log_20250123_184632###Claim 108/179|Claim 108/179]]
[[log_20250123_184632###Claim 109/179|Claim 109/179]]
[[log_20250123_184632###Claim 110/179|Claim 110/179]]
[[log_20250123_184632###Claim 111/179|Claim 111/179]]
[[log_20250123_184632###Claim 112/179|Claim 112/179]]
[[log_20250123_184632###Claim 113/179|Claim 113/179]]
[[log_20250123_184632###Claim 114/179|Claim 114/179]]
[[log_20250123_184632###Claim 115/179|Claim 115/179]]
[[log_20250123_184632###Claim 116/179|Claim 116/179]]
[[log_20250123_184632###Claim 117/179|Claim 117/179]]
[[log_20250123_184632###Claim 118/179|Claim 118/179]]
[[log_20250123_184632###Claim 119/179|Claim 119/179]]
[[log_20250123_184632###Claim 120/179|Claim 120/179]]
[[log_20250123_184632###Claim 121/179|Claim 121/179]]
[[log_20250123_184632###Claim 122/179|Claim 122/179]]
[[log_20250123_184632###Claim 123/179|Claim 123/179]]
[[log_20250123_184632###Claim 124/179|Claim 124/179]]
[[log_20250123_184632###Claim 125/179|Claim 125/179]]
[[log_20250123_184632###Claim 126/179|Claim 126/179]]
[[log_20250123_184632###Claim 127/179|Claim 127/179]]
[[log_20250123_184632###Claim 128/179|Claim 128/179]]
[[log_20250123_184632###Claim 129/179|Claim 129/179]]
[[log_20250123_184632###Claim 130/179|Claim 130/179]]
[[log_20250123_184632###Claim 131/179|Claim 131/179]]
[[log_20250123_184632###Claim 132/179|Claim 132/179]]
[[log_20250123_184632###Claim 133/179|Claim 133/179]]
[[log_20250123_184632###Claim 134/179|Claim 134/179]]
[[log_20250123_184632###Claim 135/179|Claim 135/179]]
[[log_20250123_184632###Claim 136/179|Claim 136/179]]
[[log_20250123_184632###Claim 137/179|Claim 137/179]]
[[log_20250123_184632###Claim 138/179|Claim 138/179]]
[[log_20250123_184632###Claim 139/179|Claim 139/179]]
[[log_20250123_184632###Claim 140/179|Claim 140/179]]
[[log_20250123_184632###Claim 141/179|Claim 141/179]]
[[log_20250123_184632###Claim 142/179|Claim 142/179]]
[[log_20250123_184632###Claim 143/179|Claim 143/179]]
[[log_20250123_184632###Claim 144/179|Claim 144/179]]
[[log_20250123_184632###Claim 145/179|Claim 145/179]]
[[log_20250123_184632###Claim 146/179|Claim 146/179]]
[[log_20250123_184632###Claim 147/179|Claim 147/179]]
[[log_20250123_184632###Claim 148/179|Claim 148/179]]
[[log_20250123_184632###Claim 149/179|Claim 149/179]]
[[log_20250123_184632###Claim 150/179|Claim 150/179]]
[[log_20250123_184632###Claim 151/179|Claim 151/179]]
[[log_20250123_184632###Claim 152/179|Claim 152/179]]
[[log_20250123_184632###Claim 153/179|Claim 153/179]]
[[log_20250123_184632###Claim 154/179|Claim 154/179]]
[[log_20250123_184632###Claim 155/179|Claim 155/179]]
[[log_20250123_184632###Claim 156/179|Claim 156/179]]
[[log_20250123_184632###Claim 157/179|Claim 157/179]]
[[log_20250123_184632###Claim 158/179|Claim 158/179]]
[[log_20250123_184632###Claim 159/179|Claim 159/179]]
[[log_20250123_184632###Claim 160/179|Claim 160/179]]
[[log_20250123_184632###Claim 161/179|Claim 161/179]]
[[log_20250123_184632###Claim 162/179|Claim 162/179]]
[[log_20250123_184632###Claim 163/179|Claim 163/179]]
[[log_20250123_184632###Claim 164/179|Claim 164/179]]
[[log_20250123_184632###Claim 165/179|Claim 165/179]]
[[log_20250123_184632###Claim 166/179|Claim 166/179]]
[[log_20250123_184632###Claim 167/179|Claim 167/179]]
[[log_20250123_184632###Claim 168/179|Claim 168/179]]
[[log_20250123_184632###Claim 169/179|Claim 169/179]]
[[log_20250123_184632###Claim 170/179|Claim 170/179]]
[[log_20250123_184632###Claim 171/179|Claim 171/179]]
[[log_20250123_184632###Claim 172/179|Claim 172/179]]
[[log_20250123_184632###Claim 173/179|Claim 173/179]]
[[log_20250123_184632###Claim 174/179|Claim 174/179]]
[[log_20250123_184632###Claim 175/179|Claim 175/179]]
[[log_20250123_184632###Claim 176/179|Claim 176/179]]
[[log_20250123_184632###Claim 177/179|Claim 177/179]]
[[log_20250123_184632###Claim 178/179|Claim 178/179]]
[[log_20250123_184632###Claim 179/179|Claim 179/179]]


## Processing Details


### Claim 1/179

#### Claim Text
I NTRODUCTION L ARGE language models (LLMs) have achieved remarkable success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks [1], notably producing “hallucinations” [2] when handling queries beyond their training data or requiring current information.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 0):

Siren’s Song in the AI Ocean:
A Survey on Hallucination in Large Language Models
Yue Zhang♠∗, Yafu Li♢ , Leyang Cui♡†, Deng Cai♡ , Lemao Liu♡
Tingchen Fu⋆ , Xinting Huang♡ , Enbo Zhao♡ , Yu Zhang♠ , Yulong Chen♢
Longyue Wang♡ , Anh Tuan Luu☼ , Wei Bi♡ , Freda Shi , Shuming Shi♡
♡ Tencent AI lab ♠ Soochow University ♢Zhejiang University
⋆ Renmin University of China ☼ Nanyang Technological University
 Toyota Technological Institute at Chicago
Abstract
While large language models (LLMs) have
demonstrated remarkable capabilities across
a range of downstream tasks, a significant
concern revolves around their propensity to
exhibit hallucinations: LLMs occasionally
generate content that diverges from the user
input, contradicts previously generated con-
text, or misaligns with established world
knowledge. This phenomenon poses a sub-
stantial challenge to the reliability of LLMs
in real-world scenarios. In this paper, we
survey recent efforts on the detection, ex-
planation, and mitigation of hallucination,
with an emphasis on the unique challenges
posed by LLMs. We present taxonomies
of the LLM hallucination phenomena and
evaluation benchmarks, analyze existing ap-
proaches aiming at mitigating LLM halluci-
nation, and discuss potential directions for
future research.
1 Introduction
Large language models (LLMs), particularly char-
acterized by their substantial number of param-
eters, have arisen as a promising cornerstone
for the development of natural language pro-
cessing (NLP) and artificial intelligence (Zhao
et al., 2023c). With proper alignment techniques,
such as supervised finetuning (SFT; Zhang et al.,
2023b) and reinforcement learning from human
feedback (RLHF; Ouyang et al., 2022; Fernan-
des et al., 2023), recent LLMs (OpenAI, 2023a;
Touvron et al., 2023b; OpenAI, 2023b, inter alia)
have exhibited strong capabilities in solving vari-
ous downstream tasks.
Nonetheless, as exemplified in Figure 1, LLMs,
despite their remarkable success, occasionally
∗ This survey paper was completed during Yue Zhang
(yzhang21@stu.suda.edu.cn), Yafu Li, Tingchen Fu, and Yu
Zhang’s internships at Tencent AI Lab.
† Corresponding author (leyangcui@tencent.com).
Can you recommend a delicious recipe for dinner?
User Input
LLM Response
Yes, here is a deliciousrecipe for lunch. So how about fried chicken with mashed potatoes? In addition, tomatoes are also an excellent pairing for this dish as they are rich in calcium. Enjoy this steak!
Hallucination ExplanationInput-Conflicting Hallucination: the user wants a recipe for dinner while LLM provide one for lunch.Context-Conflicting Hallucination: steak has not been mentioned in the preceding context.Fact-Conflicting Hallucination: tomatoes are not rich in calcium in fact. 
Figure 1: Three types of hallucinations occurred in
LLM responses (best viewed in color).
produce outputs that, while seemingly plausible,
deviate from user input (Adlakha et al., 2023), pre-
viously generated context (Liu et al., 2022), or fac-
tual knowledge (Min et al., 2023; Muhlgay et al.,
2023; Li et al., 2023a)—this phenomenon is com-
monly referred to as hallucination, which signifi-
cantly undermines the reliability of LLMs in real-
world scenarios (Kaddour et al., 2023). For in-
stance, LLMs can potentially fabricate erroneous
medical diagnoses or treatment plans that lead to
tangible real-life risks (Umapathi et al., 2023).
While hallucination in conventional natural lan-
guage generation (NLG) settings has been widely
studied (Ji et al., 2023), understanding and ad-
dressing the hallucination problem within the
realm of LLMs encounters unique challenges in-
troduced by
1. Massive training data : in contrast to care-
fully curating data for a specific task, LLM pre-
arXiv:2309.01219v2  [cs.CL]  24 Sep 2023



Source: data\tc16_2312.10997v5\referenced_papers\[93]_2309.11495.pdf (Page 1):

Query
       1. Baseline Response
       4. Final Verified Response
        2. Plan Verifications
Here are some politicians who were born in NY, New York:
1. Hillary Clinton - former secretary of state and former Democratic presidential nominee
2. Donald Trump - former president of the United States
3. Michael Bloomberg - former Mayor of New York City and former Democratic presidential candidate 
  … <list continues..>
Name some politicians who were born in NY, New York. 
  - Where was Hillary Clinton born?  
  - Where was Donald Trump born?   
  - Where was Michael Bloomberg born?
  … <questions continue..>
Here are some politicians who were born in NY, New York:
1. Donald Trump - former president of the United States
2. Alexandria Ocasio-Cortez - Democratic member of the U.S. House of Representatives
  … <list continues..>
Hillary Clinton was born in Chicago, Illinois, United   
States on October 26, 1947.
Donald Trump was born on June 14, 1946, in Queens, 
New York City, New York, United States
…
Michael Bloomberg was born on February 14, 1942, in 
Boston, Massachusetts, United States.
        3. Execute Verifications
Figure 1: Chain-of-Verification (CoVe) method. Given a user query, a large language model generates
a baseline response that may contain inaccuracies, e.g. factual hallucinations. We show a query here
which failed for ChatGPT (see section 9 for more details). To improve this, CoVe first generates a
plan of a set of verification questions to ask, and then executes that plan by answering them and hence
checking for agreement. We find that individual verification questions are typically answered with
higher accuracy than the original accuracy of the facts in the original longform generation. Finally,
the revised response takes into account the verifications. The factored version of CoVe answers
verification questions such that they cannot condition on the original response, avoiding repetition
and improving performance.
2 R ELATED WORK
Hallucination is a general problem in language model generations that appears across many tasks,
from summarization (Maynez et al., 2020) to open-domain dialogue (Roller et al., 2020), and has not
been resolved by simply scaling up training data or model size (Zhang et al., 2023). For a survey of
the hallucination issue, see Ji et al. (2023). A majority of the methods for reducing hallucination can
be divided into roughly three categories: training-time correction, generation-time correction and via
augmentation (tool-use).
In training-time correction methods, an attempt is made to improve the raw left-to-right generations
of an encoder-decoder or decoder-only language model by either training or otherwise adjusting
the model weights to decrease the probability of hallucinated generations. This includes using
reinforcement learning (Roit et al., 2023; Wu et al., 2023), constrastive learning (Chern et al., 2023b;
Sun et al., 2023b) and other methods (Li et al., 2023).
In generation-time correction, a common theme is to make reasoning decisions “on top of” the base
LLM in order to make them more reliable. For example, by considering the probabilities of the
generated tokens (Mielke et al., 2022; Kadavath et al., 2022). In Manakul et al. (2023) multiple
samples are drawn from the model to detect hallucinations. In Varshney et al. (2023) hallucinations
are identified using low confidence scores, and their correctness is checked through a validation
2



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 9):

able questions. Their empirical study reveals that
even the most advanced LLM, GPT4 (OpenAI,
2023b), shows a significant performance gap when
compared to humans. Ren et al. (2023) note a
correlation between accuracy and confidence, but
such confidence often surpasses the actual capa-
bilities of LLMs, namely over-confidence. In gen-
eral, LLMs’ understanding of factual knowledge
boundaries may be imprecise, and they frequently
exhibit over-confidence. Such over-confidence
misleads LLMs to fabricate answers with unwar-
ranted certainty.
Problematic alignment process could mislead
LLMs into hallucination. LLMs typically un-
dergo an alignment process following pre-training,
where they receive further training on curated
instruction-following examples to align their re-
sponses with human preferences. However, when
trained on instructions for which LLMs have not
acquired prerequisite knowledge from the pre-
training phase, this is actually a misalignment pro-
cess that encourages LLMs to hallucinate (Gold-
berg, 2023; Schulman, 2023). Another potential
issue is sycophancy, where LLMs may generate
responses that favor the user’s perspective rather
than providing correct or truthful answers, which
can result in hallucination (Perez et al., 2022; Rad-
hakrishnan et al., 2023; Wei et al., 2023b).
The generation strategy employed by LLMs
has potential risks. Today’s most advanced
LLMs generate responses sequentially, outputting
one token at a time. Zhang et al. (2023a) discover
that LLMs sometimes over-commit to their early
mistakes, even when they recognize they are in-
correct. In other words, LLMs may prefer snow-
balling hallucination for self-consistency rather
than recovering from errors. This phenomenon
is known as hallucination snowballing . Azaria
and Mitchell (2023) also contend that local opti-
mization (token prediction) does not necessarily
ensure global optimization (sequence prediction),
and early local predictions may lead LLMs into
situations where it becomes challenging to formu-
late a correct response. Lee et al. (2022) highlight
that the randomness introduced by sampling-based
generation strategies, such as top-p and top-k, can
also be a potential source of hallucination.
LLM Pre-train Data Size
GLM (Zeng et al., 2022) 400B tokens
BLOOM (Scao et al., 2022) 366B tokens
GPT-3 (Brown et al., 2020) 300B tokens
LLaMA (Touvron et al., 2023a) 1.4T tokens
Llama 2 (Touvron et al., 2023b) 2T tokens
Table 6: The pre-training data size of popular LLMs.
5 Mitigation of LLM Hallucination
In this section, we provide an extensive review of
recent studies focused on mitigating LLM halluci-
nations. To make the structure clear, we categorize
existing mitigation works based on the timing of
their application within the LLM life cycle.
5.1 Mitigation during Pre-training
Existing work (Zhou et al., 2023a) argues that the
knowledge of LLMs is mostly acquired during the
pre-training phase. The presence of noisy data
such as misinformation in the pre-training corpus
could corrupt the parametric knowledge of LLMs,
which is a significant factor contributing to hallu-
cinations, as previously discussed in § 4. Akyürek
et al. (2022) also demonstrate that it is possible to
trace the factual knowledge acquired by language
models back to their training data. Consequently,
an intuitive approach to mitigating hallucinations
could involve manually or automatically curating
the pre-training corpus to minimize unverifiable or
unreliable data as much as possible.
Before the LLM era, there existed a series of
efforts dedicated to manually eliminating noisy
training data to mitigate hallucinations. For in-
stance, Gardent et al. (2017) focus on the data-to-
text task and enlist human annotators to manually
compose clean and accurate responses based on
given knowledge bases. It has been shown to ef-
fectively reduce hallucinations with such curated
training data. Similarly, Wang (2019) manually
refine the text in existing table-to-text datasets and
observe that this process also substantially alle-
viates fact hallucinations. Besides, Parikh et al.
(2020) instruct annotators to revise verified sen-
tences from Wikipedia rather than directly creat-
ing new sentences when constructing table-to-text
training data. This approach has also been proven
to result in improved factuality of results.
With the advent of the LLM era, curating train-
ing data during pre-training has become increas-
ingly challenging due to the vast scale of pre-
training corpora (as exemplified in Table 6). For
10



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 17):

Which musical currently holds the record as Broadway's fourth-longest running show?
User Input
The musical “Chicago” holds the record as Broadway's fourth-longest running show.
I see your point, but …
Most of your claims are right, but …
Multi-Agent Interaction
Final ResponseAs of September 2021, the musical “Wicked” holds the record as Broadway's fourth-longest running show.
Figure 6: An example of the process of multi-agent in-
teraction for mitigating LLM hallucinations.
5.5 Other Methods
In addition to the above approaches, other tech-
niques demonstrating the potential for reducing
hallucinations are shown below.
Multi-agent interaction. Some recent research
has sought to address the hallucination problem
in LLMs from a multi-agent perspective, wherein
multiple LLMs (also known as agents) indepen-
dently propose and collaboratively debate their re-
sponses to reach a single consensus, as exempli-
fied in Figure 6. Du et al. (2023) is a pioneer-
ing work in this line. They initially developed a
benchmark for assessing the factual accuracy of
prominent computer scientist biographies gener-
ated by LMs. Their findings reveal that an indi-
vidual LLM can easily generate hallucinated in-
formation within this benchmark; however, such
hallucinations can be mitigated by engaging mul-
tiple LLMs in a debate to achieve consensus. Be-
sides, Cohen et al. (2023) ask one LLM to gen-
erate claims (acting as E XAMINEE ) and another
to raise questions about these claims and check
the truthfulness of them (acting as E XAMINER ).
Wang et al. (2023d) instead propose prompting a
single LLM to identify, simulate, and iteratively
self-collaborate with multiple personas, such as
Harry Potter Fan and Jay Chou Fan. By leverag-
ing an LLM as a cognitive synergist, it effectively
reduces hallucinations with relatively low costs.
Prompt engineering. Existing research high-
lights that the behavior of LLMs can significantly
vary based on the prompts given by users (Si et al.,
2022; Zhu et al., 2023). In terms of hallucina-
tion, users may encounter an LLM that initially
responds accurately but begins to hallucinate in-
formation when using different prompts. In light
of this observation, Zhang et al. (2023a) endeav-
our to engineer more effective prompts to mitigate
hallucination. Concretely, they employ the chain-
of-thought prompt (Wei et al., 2022) to compel
LLMs to generate reasoning steps before provid-
ing the final answers. However, chain-of-thought
may introduce some new challenges. The po-
tential of hallucinated reasoning steps is one of
them. Furthermore, a popular practice nowadays
involves explicitly instructing LLMs not to dis-
seminate false or unverifiable information when
designing the “system prompt”, i.e., the special
messages used to steer the behavior of LLMs.
The following system prompt used for Llama 2-
Chat (Touvron et al., 2023b) exemplifies this ap-
proach: If you don’t know the answer to a ques-
tion, please don’t share false information.
Analyzing LLMs’ internal states. Azaria and
Mitchell (2023) contend that LLMs may be aware
of their own falsehoods, implying that their in-
ternal states could be utilized to detect halluci-
nations. They propose Statement Accuracy Pre-
diction based on Language Model Activations
(SAPLMA), which adds a classifier on top of each
hidden layer of the LLM to determine truthful-
ness. Experimental results indicate that LLMs
might “know” when the statements they gener-
ate are false, and SAPLMA can effectively ex-
tract such information. The Inference-Time In-
tervention (ITI) method (Li et al., 2023b) is also
grounded in a similar hypothesis. They further
shift model activations alongside factuality-related
heads during inference and discover that this can
mitigate hallucinations. These studies suggest that
“the hallucination within LLMs may be more a re-
sult of generation techniques than the underlying
representation” (Agrawal et al., 2023).
Human-in-the-loop. Zhang et al. (2023c) posit
that a potential cause of hallucination in LLMs
could be the misalignment between knowledge
and user questions, a phenomenon that is par-
ticularly prevalent in the context of retrieval-
augmented generation (RAG). To address this is-
18



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 19):

Is there a person under the tree?
Yes, there is a person under the tree.
Figure 7: An example of object hallucination in
LVLMs. We highlight the hallucination in red, as there
is no person under the tree in this picture.
lucinations compared to smaller models. For in-
stance, Li et al. (2023e) discuss the object halluci-
nation of LVLMs, wherein LVLMs generate con-
tent containing objects that are inconsistent with
or absent from the input image, such as the ex-
ample in Figure 7. To effectively measure ob-
ject hallucinations generated by LVLMs, Liu et al.
(2023a) propose a GPT4-Assisted Visual Instruc-
tion Evaluation (GA VIE) benchmark. Gunjal et al.
(2023) introduce a multi-modal hallucination de-
tection dataset named M-HalDetect, further study
the unfaithful descriptions and inaccurate rela-
tionships beyond object hallucinations in LVLMs.
Furthermore, in addition to images, some stud-
ies have extended LLMs to other modalities such
as audio (Wu et al., 2023a; Su et al., 2023) and
video (Maaz et al., 2023), making it interesting to
investigate hallucination in these new scenarios.
Model editing. As elaborated in § 4, hallucina-
tions in LLMs may primarily stem from the mem-
orization of false information or the absence of
correct factual knowledge. To mitigate these is-
sues in LLMs with minimal computational over-
head, the concept of model editing has been in-
troduced (Sinitsin et al., 2020; De Cao et al.,
2021). This approach involves modifying the be-
havior of models in a manner that is both data-
and computation-efficient. At present, there are
two mainstream paradigms for model editing. The
first involves the incorporation of an auxiliary
sub-network (Mitchell et al., 2022; Huang et al.,
2023b), while the second entails direct modifi-
cation of the original model parameters (Meng
et al., 2022a,b). This technique may be instrumen-
tal in eliminating LLMs’ hallucinations by editing
their stored factual knowledge in purpose (Lan-
ham et al., 2023; Onoe et al., 2023). How-
ever, this emerging field still faces numerous chal-
lenges. These could include editing black-box
LLMs (Murty et al., 2022), in-context model edit-
ing (Zheng et al., 2023a), and multi-hop model
editing (Zhong et al., 2023), etc.
Attack/defense for inducing hallucination. As
previously discussed, significant efforts have been
undertaken by both researchers and companies to
guarantee that LLMs produce truthful responses,
ultimately improving the overall user experi-
ence. Cutting-edge commercial LLMs, such as
GPT4 (OpenAI, 2023b), appear to have acquired
a decent ability to generate proper responses to
factuality-related queries. However, they are not
invincible. Several studies show that LLMs can
be manipulated using techniques like meticulously
crafted jailbreak prompts to elicit arbitrary desired
responses (Wei et al., 2023a; Zou et al., 2023), in-
cluding hallucinations. Consequently, the attack-
ing and defending strategies for inducing halluci-
nations could also be a promising research direc-
tion. This is particularly important as the gener-
ation of fabricated information could potentially
breach relevant laws, leading to the forced shut-
down of LLM applications. This direction is also
intimately tied to the robustness of existing hallu-
cination mitigation methods.
Others. Given that the current research on hal-
lucinations in LLMs is still in its early stages,
there are also many other intriguing and promis-
ing avenues for further investigation. For in-
stance, researchers have begun to treat LLMs as
agents for open-world planning in the pursuit of
AGI (Park et al., 2023; Wang et al., 2023a). Ad-
dressing the hallucination problem within the con-
text of LLMs-as-agents presents brand-new chal-
lenges and holds considerable practical value. Be-
sides, analyzing and tracing LLM hallucinations
from the linguistic aspect is another interesting re-
search topic. Rawte et al. (2023) show that the oc-
currence of LLM hallucination is closely related
to linguistic nuances of the user prompts, such
as readability, formality, and concreteness. We
believe all these directions merit thorough explo-
20



### Claim 2/179

#### Claim Text
This early stage was characterized by foundational work aimed at refining pre-training techniques [3]–[5].The subsequent arrival of ChatGPT [6] marked a pivotal moment, with LLM demonstrating powerful in context learning (ICL) capabilities.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[42]_2208.03299.pdf (Page 10):

Table 2:Pretext task ablation.We compare diﬀerent pretext tasks, used to jointly pre-train our models.
Examples are randomly sampled from the training set of the KILT version of the dataset. We report the
exact match on NaturalQuestions, the F1 score on Wizard of Wikipedia and the accuracy on FEVER.
64-shot 1024-shot
NQ WoW FEVER Avg. NQ WoW FEVER Avg.
Preﬁx Language Modelling 41.0 14.5 64.9 40.1 44.7 17.9 86.0 49.5
Masked Language Modelling 42.7 14.9 69.7 42.4 44.7 18.3 88.8 50.6
Title-to-section generation 41.1 15.2 66.1 40.8 45.4 17.9 84.6 49.3
Table 3:Index content ablation.In this table, we report results for models where the content of the index
was changed between the pre-training and the ﬁne-tuning.
64-shot 1024-shot
Index Training data NQ WoW FEVER Avg. NQ WoW FEVER Avg.
Wiki Wiki 42.7 14.9 69.7 42.4 44.7 18.3 88.8 50.6
Wiki CC 40.9 15.3 67.3 41.2 44.8 18.4 88.1 50.4
CC Wiki 32.9 14.5 72.1 39.8 37.8 17.1 85.8 46.9
CC CC 38.4 14.9 70.1 41.1 42.0 17.3 88.9 49.4
we compare a model that was pre-trained with a ﬁxed retriever, and models using the various retriever
training objectives. On the MLM validation metric corresponding to the pre-training objective, we observe
that jointly training the retriever leads to strong improvements. This eﬀect tends to be less marked on
64-shot downstream tasks, and almost non-existent for 1024-shot. We believe that this is evidence that the
biggest impact of pre-training is on the language model, which learns to use and aggregate information from
the retrieved documents. Lastly, we do not observe signiﬁcant systematic diﬀerences between the diﬀerent
retriever training objectives. We thus decide adopt use Perplexity Distillation for subsequent experiments, as
it tends to be more stable than EMDR2 or ADist, and more computationally eﬃcient than LOOP.
Next, we compare the diﬀerent self-supervised pretext tasks introduced in Section 2.3 in Table 2. Here we
observe similar results for all three tasks, with a small advantage for masked language modelling. Thus, in
what follows, we adopt masked language modelling for pre-training.
Finally, we consider diﬀerent combinations of data sources—Wikipedia and common crawl—for the index
and training data during pre-training. In all cases, we use the Wikipedia 2021 dump as the index when
performing few-shot ﬁne-tuning. We report results in Table 3. First, we observe that using a Wikipedia-based
index leads to better downstream performance. There could be two explanations for this: ﬁrst, as we use
Wikipedia for the few-shot tasks, the model might be better adapted when trained using the same data.
Another explanation might be that Wikipedia is a higher-quality and denser source of knowledge than
common crawl. Second, when using a common crawl index, we observe that pre-training on Wikipedia data
leads to lower performance than using common crawl data. We believe that the primary reason is that the
distribution mismatch between the two domains leads to generally-less relevant retrieved documents. In turn,
this probably means that the pre-training is less eﬃcient, because the language model does not leverage as
much information from the documents. In the following, we thus decide to combine the data from both
domains for both the index and the pre-training data.
4.4 Fine-tuning
In this section, we perform an ablation study on how to apply our models on downstream tasks, which relies
on ﬁne-tuning. In particular, we want to investigate the following research question:
(RQ 3)How to eﬃciently ﬁne-tuneAtlas on tasks with limited training data?
11



Source: data\tc16_2312.10997v5\referenced_papers\[173]_2403.10131.pdf (Page 11):

Preprint, Under Review
Shi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, V ., Smith, N. A., Zettlemoyer, L., Yih, S.,
and Lewis, M. In-context pretraining: Language modeling beyond document boundaries.
arXiv preprint arXiv:2310.10638, 2023c.
Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t.
Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652,
2023d.
Tänzer, M., Ruder, S., and Rei, M. Memorisation versus generalisation in pre-trained lan-
guage models. In Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 7564–7578, 2022.
Vu, T., Iyyer, M., Wang, X., Constant, N., Wei, J., Wei, J., Tar, C., Sung, Y.-H., Zhou, D., Le,
Q., et al. Freshllms: Refreshing large language models with search engine augmentation.
arXiv preprint arXiv:2310.03214, 2023.
Wang, B., Ping, W., McAfee, L., Xu, P ., Li, B., Shoeybi, M., and Catanzaro, B. Instructretro:
Instruction tuning post retrieval-augmented pretraining. arXiv preprint arXiv:2310.07713,
2023.
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H.
Self-instruct: Aligning language models with self-generated instructions. arXiv preprint
arXiv:2212.10560, 2022.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V ., Zhou, D., et al.
Chain-of-thought prompting elicits reasoning in large language models. Advances in
Neural Information Processing Systems, 35:24824–24837, 2022.
Weston, J. and Sukhbaatar, S. System 2 attention (is something you might need too). arXiv
preprint arXiv:2311.11829, 2023.
Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P ., Hou, R., Martin, L., Rungta, R.,
Sankararaman, K. A., Oguz, B., et al. Effective long-context scaling of foundation models.
arXiv preprint arXiv:2309.16039, 2023.
Xu, P ., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E.,
Shoeybi, M., and Catanzaro, B. Retrieval meets long context large language models. arXiv
preprint arXiv:2310.03025, 2023.
Yang, Z., Qi, P ., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D.
Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint
arXiv:1809.09600, 2018.
Zhou, C., Liu, P ., Xu, P ., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P ., Yu, L., et al. Lima:
Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.
12



Source: data\tc16_2312.10997v5\referenced_papers\[28]_2312.05934.pdf (Page 2):

Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs
Figure 1.A visualization of the knowledge injection framework.
complex multi-step reasoning tasks (Tan et al., 2023) or
when posed with different questions about the same fact,
resulting in disparate outcomes (Berglund et al., 2023).
We observe that most of these issues arise during the pre-
training phase, with catastrophic forgetting being the notable
exception. Hence, many LLMs will suffer from factual
errors of this kind regardless of any post-training process.
3. Injecting Knowledge to Language Models
Following the background given in Section 2, it is clear
that general pre-training is insufficient for many knowledge-
intensive tasks. To solve this, an additional post-processing
step is essential to augment the knowledge of a pre-trained
model. This step is often reffered to as knowledge injec-
tion (Wang et al., 2020; Chen et al., 2022; Liu et al., 2020;
Lauscher et al., 2020).
In this section, we examine two widely used frameworks
for knowledge injection: fine-tuning (FT) and retrieval aug-
mented generation (RAG). We begin by formulating the
knowledge injection problem, aiming to explain both meth-
ods using consistent terminology.
3.1. Problem formulation
In Equations (1) and (2), we presented a formulation for
knowledge in language models through the lens of question-
answering (Q&A). We now extend this formulation to the
problem of knowledge injection using the same terminology.
Given a set of factual questions, there exists some text cor-
pus containing information that is relevant to these questions.
The central assumption of knowledge injection is that given
full access to this corpus, it could serve as an auxiliary
knowledge base and improve the model’s performance on
this set of questions.
Mathematically, let M be a pre-trained model, and let Q be
a set of factual questions, as before. Now, assume we have
a relevant auxiliary knowledge base BQ. Our objective is to
discover a transformation, denoted as F, that, when applied,
would enhance the knowledge about Q:
M′ := F(M, BQ) s.t. LM′,Q > LM,Q. (3)
In this work, we aim to compare two choices for F: fine-
tuning and RAG to see which option performs better in this
problem.
3.2. Fine-Tuning
Fine-tuning is the process of adjusting a pre-trained model
on a specific, often narrower, dataset or task to enhance
its performance in that particular domain. Here, it is vital
to distinguish between different types of fine-tuning. FT
techniques are commonly classified into supervised, unsu-
3



Source: data\tc16_2312.10997v5\referenced_papers\[177]_2301.12597.pdf (Page 3):

BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models
Image
Encoder
Input Image Learned Queries
a cat wearing sunglassesOutput Text
…
Q-Former
…
Fully
Connected
…
LLM Decoder
Bootstrapping from a 
Decoder-based
Large Language Model 
(e.g. OPT)
Image
Encoder
Input Image Learned Queries
a cat 
Suﬃx Text
…
Q-Former
…
Fully
Connected
…
LLM Encoder
Bootstrapping from an 
Encoder-Decoder-based 
Large Language Model 
(e.g. FlanT5)
wearing sunglasses
Preﬁx Text
LLM Decoder
Figure 3.BLIP-2’s second-stage vision-to-language generative pre-training, which bootstraps from frozen large language models (LLMs).
(Top) Bootstrapping a decoder-based LLM (e.g. OPT). ( Bottom) Bootstrapping an encoder-decoder-based LLM (e.g. FlanT5). The
fully-connected layer adapts from the output dimension of the Q-Former to the input dimension of the chosen LLM.
3.3. Bootstrap Vision-to-Language Generative Learning
from a Frozen LLM
In the generative pre-training stage, we connect Q-
Former (with the frozen image encoder attached) to a frozen
LLM to harvest the LLM’s generative language capability.
As shown in Figure 3, we use a fully-connected (FC) layer
to linearly project the output query embeddings Z into the
same dimension as the text embedding of the LLM. The
projected query embeddings are then prepended to the input
text embeddings. They function as soft visual prompts that
condition the LLM on visual representation extracted by
the Q-Former. Since the Q-Former has been pre-trained
to extract language-informative visual representation, it ef-
fectively functions as an information bottleneck that feeds
the most useful information to the LLM while removing
irrelevant visual information. This reduces the burden of the
LLM to learn vision-language alignment, thus mitigating
the catastrophic forgetting problem.
We experiment with two types of LLMs: decoder-based
LLMs and encoder-decoder-based LLMs. For decoder-
based LLMs, we pre-train with the language modeling loss,
where the frozen LLM is tasked to generate the text con-
ditioned on the visual representation from Q-Former. For
encoder-decoder-based LLMs, we pre-train with the prefix
language modeling loss, where we split a text into two parts.
The prefix text is concatenated with the visual representation
as input to the LLM’s encoder. The suffix text is used as the
generation target for the LLM’s decoder.
3.4. Model Pre-training
Pre-training data.We use the same pre-training dataset as
BLIP with 129M images in total, including COCO (Lin
et al., 2014), Visual Genome (Krishna et al., 2017),
CC3M (Sharma et al., 2018), CC12M (Changpinyo et al.,
2021), SBU (Ordonez et al., 2011), and 115M images from
the LAION400M dataset (Schuhmann et al., 2021). We
adopt the CapFilt method (Li et al., 2022) to create synthetic
captions for the web images. Specifically, we generate 10
captions using the BLIPlarge captioning model, and rank the
synthetic captions along with the original web caption based
on the image-text similarity produced by a CLIP ViT-L/14
model. We keep top-two captions per image as training data
and randomly sample one at each pre-training step.
Pre-trained image encoder and LLM.For the frozen im-
age encoder, we explore two state-of-the-art pre-trained
vision transformer models: (1) ViT-L/14 from CLIP (Rad-
ford et al., 2021) and (2) ViT-g/14 from EV A-CLIP (Fang
et al., 2022). We remove the last layer of the ViT and
uses the second last layer’s output features, which leads
to slightly better performance. For the frozen language
model, we explore the unsupervised-trained OPT model
family (Zhang et al., 2022) for decoder-based LLMs, and
the instruction-trained FlanT5 model family (Chung et al.,
2022) for encoder-decoder-based LLMs.
Pre-training settings.We pre-train for 250k steps in the
first stage and 80k steps in the second stage. We use a batch
size of 2320/1680 for ViT-L/ViT-g in the first stage and
a batch size of 1920/1520 for OPT/FlanT5 in the second
stage. During pre-training, we convert the frozen ViTs’
and LLMs’ parameters into FP16, except for FlanT5 where
we use BFloat16. We found no performance degradation
compared to using 32-bit models. Due to the use of frozen
models, our pre-training is more computational friendly
than existing large-scale VLP methods. For example, using
a single 16-A100(40G) machine, our largest model with
ViT-g and FlanT5-XXL requires less than 6 days for the first
stage and less than 3 days for the second stage.
The same set of pre-training hyper-parameters are used for
all models. We use the AdamW (Loshchilov & Hutter, 2017)
optimizer with β1 = 0.9, β1 = 0.98, and a weight decay
of 0.05. We use a cosine learning rate decay with a peak
learning rate of 1e-4 and a linear warmup of 2k steps. The
minimum learning rate at the second stage is 5e-5. We use
images of size 224×224, augmented with random resized
cropping and horizontal flipping.



Source: data\tc16_2312.10997v5\referenced_papers\[22]_2210.01296.pdf (Page 12):

Published as a conference paper at ICLR 2023
Fabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,
and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2463–2473, 2019.
John Prager et al. Open-domain question–answering. Foundations and Trends® in Information
Retrieval, 1(2):91–231, 2007.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training. a.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. b.
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:
Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.
Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the
parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pp. 5418–5426, 2020.
Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and be-
yond. Foundations and Trends® in Information Retrieval, 3(4):333–389, 2009.
Subhro Roy, Shyam Upadhyay, and Dan Roth. Equation parsing: Mapping sentences to grounded
equations. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language
Processing, pp. 1088–1097, 2016.
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask prompted training enables
zero-shot task generalization. In International Conference on Learning Representations, 2021.
Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unsupervised
commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing (EMNLP), pp. 4615–4629, 2020.
Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. olmpics-on what language model
pre-training captures. Transactions of the Association for Computational Linguistics, 8:743–758,
2020.
Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven
Zheng, Neil Houlsby, and Donald Metzler. Unifying language learning paradigms.arXiv preprint
arXiv:2205.05131, 2022a.
Yi Tay, Vinh Q Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui,
Zhe Zhao, Jai Gupta, et al. Transformer memory as a differentiable search index. arXiv preprint
arXiv:2202.06991, 2022b.
Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews, and Franc ¸oise Beaufays. Understanding unin-
tended memorization in federated learning. arXiv preprint arXiv:2006.07490, 2020.
Kushal Tirumala, Aram H Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization
without overﬁtting: Analyzing the training dynamics of large language models. arXiv preprint
arXiv:2205.10770, 2022.
Cunxiang Wang, Pai Liu, and Yue Zhang. Can generative pre-trained language models serve as
knowledge bases for closed-book qa? In Proceedings of the 59th Annual Meeting of the As-
sociation for Computational Linguistics and the 11th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), pp. 3241–3251, 2021.
13



### Claim 3/179

#### Claim Text
The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a “Retrieve-Read” framework [7].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 1):

retriever and the reader are usually frozen. The idea
is to trigger the emergent ability through carefully
crafted prompts or a sophisticated prompt pipeline.
Multiple interactions with external knowledge al-
low the LLM to approach the correct answer step
by step.
However, there are still problems remaining to
be solved. Existing approaches overlook the adap-
tation of the query, i.e., the input of the retrieve-
then-read pipeline. The retrieval query is either
original from datasets or directly determined by the
black-box generation, thus is always fixed. How-
ever, there is inevitably a gap between the input
text and the knowledge that is really needed to
query. This limits performance and places a burden
on retrieval capability enhancement and prompt
engineering.
In consideration of this issue, this paper pro-
poses Rewrite-Retrieve-Read, a new framework for
retrieval augmentation, which can be further tuned
for adapting to LLMs. In front of the retriever, a
step of rewriting the input is added, filling the gap
between the given input and retrieval need, as is
shown in Figure 1. We adopt the off-the-shelf tool,
an internet search engine, as the retriever, which
avoids the maintenance of the search index and
can access up-to-date knowledge (Lazaridou et al.,
2022). Different from previous studies (Khattab
et al., 2022; Yao et al., 2023) that require the mem-
ory of multiple interaction rounds between the re-
triever and the LLM for each sample, the motiva-
tion of our rewriting step is to clarify the retrieval
need from the input text.
We also propose a trainable scheme for our
rewrite-retrieve-read framework (Figure 1 (c)).
The black-box retriever and the reader form a
frozen system. To further smooth the steps of
our pipeline, we apply a small, trainable language
model to perform the rewriting step, denoted as the
rewriter. The rewriter is trained by reinforcement
learning using the LLM performance as a reward,
learning to adapt the retrieval query to improve the
reader on downstream tasks.
Our proposed methods are evaluated on
knowledge-intensive downstream tasks including
open-domain QA (HotpoQA (Yang et al., 2018),
AmbigNQ (Min et al., 2020), PopQA (Mallen
et al., 2022)) and multiple choice QA (MMLU
(Hendrycks et al., 2021)). The experiments are
implemented on T5-large (Raffel et al., 2020) as
the rewriter, ChatGPT (Ouyang et al., 2022) and
Vicuna-13B (Chiang et al., 2023) as the LLM
reader. The results show that query rewriting con-
sistently improves the retrieve-augmented LLM
performance. The results also indicate that the
smaller language model can be competent for query
rewriting.
To sum up, our proposed novel retrieval-
augmentation method, rewrite-retrieve-read is the
first framework where the input text is adapted for
the frozen retriever and LLM reader. We introduce
a tuneable scheme with a small, trainable model,
achieving performance gains with less resource
consumption.
2 Related Work
2.1 Retrieval Augmentation
Language models require external knowledge to al-
leviate the factuality drawbacks. Retrieval augmen-
tation has been regarded as the standard effective
solution. With a retrieval module, related passages
are provided to the language model as the context
of the original input. Thus factual information like
common sense or real-time news helps with output
prediction through contextualized reading compre-
hension.
Earlier studies use sparse retriever (Chen et al.,
2017) or dense retriever (Karpukhin et al., 2020)
in front of a pre-trained language model (PrLM).
The neural retriever and reader are both PrLMs
of trainable size like BERT (Devlin et al., 2019)
or BART (Lewis et al., 2020a). Hence, the whole
retrieve-then-reader framework is a tuneable end-
to-end system, where the retrieved contexts can
be regarded as the intermediate results (Karpukhin
et al., 2020; Lewis et al., 2020b). Approaches to
smooth the two-step framework are proposed to op-
timize the retrieval and the reading comprehension
(Sachan et al., 2021; Lee et al., 2022; Jiang et al.,
2022). More recently, retrieval remains a powerful
enhancement as the size of models and data scales
rapidly (Mallen et al., 2022; Shi et al., 2023; Brown
et al., 2020). On the other hand, retrieval enhance-
ment can compensate for the shortfall in parameter
size, compared to large-scale language models. For
example, by jointly training the retriever and the
reader, Atlas (Izacard et al., 2022) shows few-shot
performance on par with 540B PalM (Chowdhery
et al., 2022) but be of 50× smaller size.
The Internet as a knowledge baseMore related
to our work, the search engine can assume the role
of the retriever and use the Internet as the source of



Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 0):

Query Rewriting for Retrieval-Augmented Large Language Models
Xinbei Ma1,2,∗ , Yeyun Gong3, #, †, Pengcheng He4, #, Hai Zhao1,2,†, Nan Duan3
1Department of Computer Science and Engineering, Shanghai Jiao Tong University
2Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University
3Microsoft Research Asia 4Microsoft Azure AI
sjtumaxb@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn,
{yegong, nanduan}@microsoft.com, Herbert.he@gmail.com
Abstract
Large Language Models (LLMs) play pow-
erful, black-box readers in the retrieve-then-
read pipeline, making remarkable progress
in knowledge-intensive tasks. This work in-
troduces a new framework, Rewrite-Retrieve-
Read instead of the previous retrieve-then-read
for the retrieval-augmented LLMs from the per-
spective of the query rewriting. Unlike prior
studies focusing on adapting either the retriever
or the reader, our approach pays attention to
the adaptation of the search query itself, for
there is inevitably a gap between the input text
and the needed knowledge in retrieval. We
first prompt an LLM to generate the query,
then use a web search engine to retrieve con-
texts. Furthermore, to better align the query
to the frozen modules, we propose a trainable
scheme for our pipeline. A small language
model is adopted as a trainable rewriter to cater
to the black-box LLM reader. The rewriter is
trained using the feedback of the LLM reader
by reinforcement learning. Evaluation is con-
ducted on downstream tasks, open-domain QA
and multiple-choice QA. Experiments results
show consistent performance improvement, in-
dicating that our framework is proven effective
and scalable, and brings a new framework for
retrieval-augmented LLM 1.
1 Introduction
Large Language Models (LLMs) have shown re-
markable abilities for human language processing
and extraordinary scalability and adaptability in
few- or zero-shot settings.(Ouyang et al., 2022;
Brown et al., 2020; Chowdhery et al., 2022). How-
ever, the training process depends on large-scale
high-quality corpora but without the perception
∗ Work done during an internship at 3Microsoft Research
Asia. # Equal contribution. †Corresponding author.
This paper was partially supported by Joint Research
Project of Yangtze River Delta Science and Technology Inno-
vation Community (No. 2022CSJGG1400).
1https://github.com/xbmxb/RAG-query-rewriting
of the real world. Thus, LLMs still have to face
the issue of hallucination (Yao et al., 2023; Bang
et al., 2023) and temporal misalignment (Röttger
and Pierrehumbert, 2021; Luu et al., 2022; Jang
et al., 2022). This affects the reliability of LLMs
and hinders wider practical application, because
the consistency between the LLM responses with
the real world needs further validation. Exist-
ing work has proved that incorporating external
knowledge (i.e., non-parametric knowledge) with
internal knowledge (i.e., parametric knowledge)
can effectively alleviate hallucination, especially
for knowledge-intensive tasks. In fact, retrieval-
augmented LLMs have been shown so effective
that they have been regarded as a standard solu-
tion to alleviate the factuality drawbacks in naive
LLM generations. Retrieval augmentation is ap-
plied to select relative passages as external contexts
for the language model, which isretrieve-then-read
framework (Lewis et al., 2020b; Karpukhin et al.,
2020; Izacard et al., 2022). Take the open-domain
Question-Answering task (open-domain QA) as
an example, a retriever first searches for related
documents for a question. Then the LLM receives
the question and the documents, then predicts an
answer.
As most LLMs are only accessible through infer-
ence APIs, they play the part of black-box frozen
readers in the pipeline. This makes previous re-
trieval augmentation methods that require complete
access (Lewis et al., 2020b; Guu et al., 2020; Izac-
ard et al., 2022) no longer feasible. Recent studies
on retrieval-augmented language models lean more
on the LLM-oriented adaptation. An idea is to train
a dense retrieval model to cater to the frozen lan-
guage model (Shi et al., 2023). By using feedback
from the LLM as a training objective, the retrieval
model is tuned for better LLM input contexts. An-
other research line focuses on the design of inter-
actions between the retriever and the reader (Yao
et al., 2023; Khattab et al., 2022), where both the
arXiv:2305.14283v3  [cs.CL]  23 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[64]_2302.00083.pdf (Page 1):

Language 
Model
World Cup 2022 was the 
last with 32 teams, 
before the increase to
Retriever FIFA World Cup 2026 will 
expand to 48 teams.
World Cup 2022 was the 
last with 32 teams, before 
the increase to
48 in the 2026 
tournament.
Figure 2: An example of In-Context RALM: we simply prepend the retrieved document before the input prefix.
tend to be focused on altering the language model
architecture (Khandelwal et al., 2020; Borgeaud
et al., 2022; Zhong et al., 2022; Levine et al., 2022c;
Li et al., 2022). Notably, Borgeaud et al. (2022) in-
troduced RETRO, featuring document reading via
nontrivial modifications that require further train-
ing to the LM architecture, while using an off-the-
shelf frozen BERT retriever for document selec-
tion. Although the paper’s experimental findings
showed impressive performance gains, the need for
changes in architecture and dedicated retraining
has hindered the wide adoption of such models.
In this paper, we show that a very simple doc-
ument reading mechanism can have a large im-
pact, and that substantial gains can also be made
by adapting the document selection mechanism to
the task of language modeling. Thus, we show that
many of the benefits of RALM can be achieved
while working with off-the-shelf LMs, even via
API access. Specifically, we consider a simple but
powerful RALM framework, dubbed In-Context
RALM (presented in Section 3), which employs a
zero-effort document reading mechanism: we sim-
ply prepend the selected documents to the LM’s
input text (Figure 2).
Section 4 describes our experimental setup. To
show the wide applicability of our framework, we
performed LM experiments on a suite of five di-
verse corpora: WikiText-103 (Merity et al., 2016),
RealNews (Zellers et al., 2019), and three datasets
from The Pile (Gao et al., 2021): ArXiv, Stack
Exchange and FreeLaw. We use open-source LMs
ranging from 110M to 66B parameters (from the
GPT-2, GPT-Neo, OPT and LLaMA model fami-
lies).
In Section 5 we evaluate the application of off-
the-shelf retrievers to our framework. In this
minimal-effort setting, we found that In-Context
RALM led to LM performance gains equivalent to
increasing the LM’s number of parameters by 2–
3× across all of the text corpora we examined. In
Section 6 we investigate methods for adapting doc-
ument ranking to the LM task, a relatively under-
explored RALM degree of freedom. Our adapta-
tion methods range from using a small LM to per-
form zero-shot ranking of the retrieved documents,
up to training a dedicated bidirectional reranker
by employing self-supervision from the LM signal.
These methods lead to further gains in the LM task
corresponding to an additional size increase of 2×
in the LM architecture. As a concrete example of
the gains, a 345M parameter GPT-2 enhanced by
In-Context RALM outperforms a 762M parame-
ter GPT-2 when employing an off-the-shelf BM25
retriever (Robertson and Zaragoza, 2009), and out-
performs a 1.5B parameter GPT-2 when employing
our trained LM-oriented reranker (see Figure 1).
For large model sizes, our method is even more
effective: In-Context RALM with an off-the-shelf
retriever improved the performance of a 6.7B pa-
rameter OPT model to match that of a 66B param-
eter parameter OPT model (see Figure 4).
In Section 7 we demonstrate the applicability
of In-Context RALM to downstream open-domain
questions answering (ODQA) tasks.
In a concurrent work, Shi et al. (2023) also sug-
gest to augment off-the-shelf LMs with retrieved
texts by prepending them to the input. Their re-
sults are based on training a dedicated retriever for
language modeling. In contrast, we focus on the
gains achievable in using off-the-shelf retrievers
for this task. We show strong gains of this simpler
setting by investigating: (1) which off-the-shelf
retriever is best suited for language modeling, (2)
the frequency of retrieval operations, and (3) the
optimal query length. In addition, we boost the off-
the-shelf retrieval performance by introducing two
reranking methods that demonstrate further gains
in perplexity.
We believe that In-Context RALM can play two
important roles in making RALM systems more
powerful and more prevalent. First, given its simple
reading mechanism, In-Context RALM can serve
as a clean probe for developing document retrieval



Source: data\tc16_2312.10997v5\referenced_papers\[63]_2401.18059.pdf (Page 1):

Published as a conference paper at ICLR 2024
2
 3
 4
 5
1
1
 2
3
 3
4
 5
5
6
 8
7
 Index #8
Text:  summary of 
nodes 2 and 3
Child Nodes: 2, 3
Text Embedding
Text chunks
3
.1
4
.1
5
2. Summarization 
by LLM
1. Clustering
10
7
1
 2
8
4
3
 5
6
9
Formation of one tree layer
Root layer
Leaf layer
Contents of a nodeRAPTOR Tree 
Figure 1: Tree construction process: RAPTOR recursively clusters chunks of text based on their
vector embeddings and generates text summaries of those clusters, constructing a tree from the
bottom up. Nodes clustered together are siblings; a parent node contains the text summary of that
cluster.
Our main contribution is the idea of using text summarization to allow retrieval augmentation of
context at different scales, and to show its effectiveness in experiments on collections of long doc-
uments. Controlled experiments with three language models (UnifiedQA (Khashabi et al., 2020),
GPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023)) show that RAPTOR outperforms current
retrieval augmentation. Moreover, RAPTOR coupled with GPT-4, and sometimes even with Uni-
fiedQA, gives new state-of-the-art results on three QA tasks: free text response questions on books
and movies (NarrativeQA, Koˇcisk`y et al. 2018), full-text NLP papers (QASPER, Dasigi et al. 2021),
and multiple-choice questions based on medium-length passages (QuALITY , Pang et al. 2022).1
2 R ELATED WORK
Why Retrieval? Recent advances in hardware and algorithms have indeed expanded the con-
text lengths that models can handle, leading to questions about the need for retrieval systems (Dai
et al., 2019; Dao et al., 2022; Liu et al., 2023). However, as Liu et al. (2023) and Sun et al. (2021)
have noted, models tend to underutilize long-range context and see diminishing performance as con-
text length increases, especially when pertinent information is embedded within a lengthy context.
Moreover, practically, use of long contexts is expensive and slow. This suggests that selecting the
most relevant information for knowledge-intensive tasks is still crucial.
Retrieval Methods Retrieval-augmented language models (RALMs) have seen improvements in
various components: the retriever, the reader, and end-to-end system training. Retrieval methods
have transitioned from traditional term-based techniques like TF-IDF (Sp¨arck Jones, 1972) and
BM25 (Robertson et al., 1995; Roberts et al., 2020) to deep learning–based strategies (Karpukhin
et al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023). Some recent work proposes using
large language models as retrievers due to their ability to memorize extensive knowledge (Yu et al.,
2022; Sun et al., 2022). Research on the reader component includes Fusion-in-Decoder (FiD)
(Izacard & Grave, 2022), which employs both DPR and BM25 for retrieval and processes passages
independently in the encoder andRETRO (Borgeaud et al., 2022; Wang et al., 2023), which utilizes
cross-chunked attention and chunkwise retrieval to generate text grounded on retrieved context.
End-to-end system training work includes Atlas (Izacard et al., 2022), which fine-tunes an encoder-
decoder model in conjunction with the retriever;REALM (Guu et al., 2020), a bidirectional, masked
LM fine-tuned for open-domain question answering; and RAG (Retrieval-Augmented Genera-
tion) (Lewis et al., 2020), which integrates pre-trained sequence-to-sequence models with a neural
retriever. Min et al. (2021) introduced Joint Passage Retrieval (JPR) model which uses a tree-
decoding algorithm to handle passage diversity and relevance in multi-answer retrieval. Dense Hi-
erarchical Retrieval (DHR) and Hybrid Hierarchical Retrieval (HHR) represent advancements
in retrieval accuracy by combining document and passage level retrievals and integrating sparse and
dense retrieval methods, respectively (Liu et al., 2021; Arivazhagan et al., 2023).
1We will release the code of RAPTOR publicly here.
2



Source: data\tc16_2312.10997v5\referenced_papers\[63]_2401.18059.pdf (Page 0):

Published as a conference paper at ICLR 2024
RAPTOR: R ECURSIVE ABSTRACTIVE PROCESSING
FOR TREE -ORGANIZED RETRIEVAL
Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning
Stanford University
psarthi@cs.stanford.edu
ABSTRACT
Retrieval-augmented language models can better adapt to changes in world state
and incorporate long-tail knowledge. However, most existing methods retrieve
only short contiguous chunks from a retrieval corpus, limiting holistic under-
standing of the overall document context. We introduce the novel approach of
recursively embedding, clustering, and summarizing chunks of text, constructing
a tree with differing levels of summarization from the bottom up. At inference
time, our RAPTOR model retrieves from this tree, integrating information across
lengthy documents at different levels of abstraction. Controlled experiments show
that retrieval with recursive summaries offers significant improvements over tra-
ditional retrieval-augmented LMs on several tasks. On question-answering tasks
that involve complex, multi-step reasoning, we show state-of-the-art results; for
example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve
the best performance on the QuALITY benchmark by 20% in absolute accuracy.
1 I NTRODUCTION
Large Language Models (LLMs) have emerged as transformative tools showing impressive perfor-
mance on many tasks. With the growing size of LLMs, they can serve standalone as very effective
knowledge stores, with facts encoded within their parameters (Petroni et al., 2019; Jiang et al., 2020;
Talmor et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022; Bubeck et al.,
2023; Kandpal et al., 2023) and models can be further improved with fine-tuning on downstream
tasks (Roberts et al., 2020). Nevertheless, even a large model does not contain sufficient domain-
specific knowledge for particular tasks and the world continues to change, invalidating facts in the
LLM. Updating the knowledge of these models through additional fine-tuning or editing is difficult,
particularly when dealing with vast text corpora (Lewis et al., 2020; Mitchell et al., 2022). An alter-
native approach, pioneered in open domain question answering systems (Chen et al., 2017; Yu et al.,
2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate
information retrieval system. Retrieved information is then presented to the LLM along with the
question as context (“retrieval augmentation”, Lewis et al., 2020; Izacard et al., 2022; Min et al.,
2023; Ram et al., 2023), making it easy to provide a system with current knowledge particular to
some domain and enabling easy interpretability and provenance tracking, whereas the parametric
knowledge of LLMs is opaque and difficult to trace back to its source (Akyurek et al., 2022).
Nevertheless, existing retrieval-augmented approaches also have flaws. The one we tackle is that
most existing methods retrieve only a few short, contiguous text chunks, which limits their ability
to represent and leverage large-scale discourse structure. This is particularly relevant for thematic
questions that require integrating knowledge from multiple parts of a text, such as understanding
an entire book, as in the NarrativeQA dataset (Ko ˇcisk`y et al., 2018). Consider the fairy tale of
Cinderella, and the question “How did Cinderella reach her happy ending?”. The top- k retrieved
short contiguous texts will not contain enough context to answer the question.
To address this, we design an indexing and retrieval system that uses a tree structure to capture both
high-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters
chunks of text, generates text summaries of those clusters, and then repeats, generating a tree from
the bottom up. This structure enables RAPTOR to load into an LLM’s context chunks representing
the text at different levels so that it can effectively and efficiently answer questions at different levels.
1
arXiv:2401.18059v1  [cs.CL]  31 Jan 2024



### Claim 4/179

#### Claim Text
Common methods include query rewriting query transformation, query expansion and other techniques [7], [9]–[11].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[9]_2311.03758.pdf (Page 1):

WWW ’24 Companion, May 13–17, 2024, Singapore, Singapore. Peng, et al.
“DIY blind box”
“Self-building blind box”
Query  Rewriting
Semantic UnderstandingRetrieval (Multi-path)
Item-based CF
Embedding-based  retrieval
PrerankRankRerankDisplay
Ranking System
ThousandsThousandsDozens
Merge
Inverted Index based Exact Matching
Figure 1: Framework of Taobao search engine.
ensuring accurate matching of user intent. However, due to the
variations for how users express their preferences for products,
semantic gaps often exist between their queries and the product
keywords, even worse with long-tail queries where retrieval sys-
tem may fail to provide any relevant products. For instance, a user
with personal expression habits may input a long-tail query like
"self-building blind box" , which will lead to more retrieval results if
with its synonymous query like "DIY blind box" . Unfortunately, tra-
ditional term-matching solutions like inverted index could probably
fail to match the commonly-used “DIY” with non-customary term
“self-building”, which limit the retrieval results and significantly
impair the user experience. Therefore, it is urgently required to
solve the semantic gap challenge for long-tail queries, and address
the problem of “few-recall” in e-commerce platform.
Traditionally, prior arts [13, 19, 34, 44] mainly focus on the “em-
bedding based retrieval” paradigm, which initially map the queries
and products into a common semantic space, and then support the
Top-𝐾 retrieval with approximate nearest neighbor (ANN) meth-
ods. However, the retrieval outcomes might be difficult to interpret,
which severely limit the performance. To enhance the controlla-
bility of retrieval outcomes, some efforts have been made on the
“query rewriting & exact match” paradigm. On the one hand, the
discriminative methods [18, 46] attempt to “rewrite” queries via
finding similar terms from a query reformulation set, and then
utilize them to search for relevant products using sparse retrieval.
Although these approaches could effectively expand the semantic
of hot queries, long-tail queries may not be adequately optimized,
thus no related rewrite can be generated. On the other hand, the
generative methods [26, 38] involve supervised training on <query,
rewrite> pair data to empower the model with rewriting capabilities,
and the alignment process [1, 23] is further incorporated to enhance
the metric preference. Although these methods partially address
the semantic gap problem, they typically rely on small generative
models with limited comprehension of long-tail queries, which
significantly constrained the rewriting capability. Recently, with
the development of LLM techniques, some efforts [ 2, 15, 36, 37]
solely utilize LLMs as retrieval data augmentation generators with-
out additional training to expand query semantics. However, these
methods, even with carefully curated prompts, may still constrain
the ability to specialize for query rewriting task, leading to the poor
alignment with objectives of e-commerce search.
To effectively bridge the semantic gap for long-tail queries, and
solve the above challenges via producing controlled and aligned
outcomes with integrating the knowledge of LLMs, we propose
BEQUE, a novel framework that involves three stages of fine-tuning
LLMs, namely multi-instruction supervised fine-tuning (SFT), offline
system feedback , and objective alignment . Specifically, in the first
stage, we utilize the rejection sampling to collect <query, rewrite>
pairs with desired quality distribution, and then combine these
pairs with data from quality classification, product title prediction
and chain of thought (CoT) tasks to construct the multi-instruction
rewriting dataset for fine-tuning our LLM. Next, with the well-
trained LLM, we employ beam search to generate multiple candidate
rewrites for each query. These candidate rewrites are fed into the
Taobao offline system to retrieve a collection of related products. We
calculate the quality score of retrieved products for the rewrites and
use them as rewards for candidates ranking. To calibrate generation
probability of the candidate rewrites, we introduce a Bradley-Terry
based contrastive learning method that considers the partial order
among the these rewrites. Ultimately, the model training objective
is aligned with the online goal of the Taobao search, ensuring that
the generated rewrites yield the desired search results.
The main contributions of this work are listed as follows:
•We have analyzed long-tail queries in e-commerce search
and identified the semantic gap problem associated with
such queries. To the best of our knowledge, we are the first
to fine-tune LLMs for industrial query rewriting task.
•We propose a three-stage fine-tuned framework called BEQUE
to address the issue of semantic gap in long-tail queries. This
framework is designed to generate rewrites that align with
the objectives of Taobao search.
•The effectiveness of our model is demonstrated through
both offline and online experiments, showcasing its ability
to significantly improve e-commerce revenue.
2 RELATED WORKS
2.1 Query Rewriting
Query rewriting, also known as query expansion or query reformu-
lation, plays a pivotal role in e-commerce search technology and
has a profound impact on the user’s shopping experience and the
revenue of e-commerce platforms. This technique can be broadly
categorized into discriminative and generative methods.
Discriminative methods treat query rewriting as a retrieval
process that expand the semantics of the original query by selecting
appropriate terms from the candidate set. For example, pseudo-
relevance [6, 31, 40] selects the top k documents from the initial
retrieval as semantic extensions. These approaches, however, of-
ten pose challenges in effectively controlling the semantic scope
and ensuring retrieval relevance. To address these challenges, one
potential solution is to utilize a well-built thesaurus [ 5, 22] as a
candidate rewrite set. However, it is important to note that the
effectiveness of these methods highly depends on the quality of
the thesaurus. Inadequate quality may result in query semantic
drift, where the intended meaning of the query is compromised.
Furthermore, alternative approaches [3, 8, 18, 21] involve generat-
ing candidate rewrites based on search logs, incorporating similar
terms from users’ search history as extensions. Unfortunately, due
to the Matthew effect, search logs naturally exhibit a bias towards
popular queries, resulting in that the training data collected through
this approach may not sufficiently meet the optimization needs for
less frequently searched long-tail queries.



Source: data\tc16_2312.10997v5\referenced_papers\[9]_2311.03758.pdf (Page 2):

Large Language Model based Long-tail Query Rewriting in Taobao Search WWW ’24 Companion, May 13–17, 2024, Singapore, Singapore.
Generative methods [16, 26, 29, 38] formulate the rewriting
task as a generative process, where a transformer-like model is
utilized to generate candidate terms for the original query. Some
methods [1, 23] incorporate reinforcement learning or contrastive
learning to align with human preferences or offline metrics. These
methods have the ability to generate related rewrites for each query,
but usually employ a model with limited number of parameters,
which makes them less effective in processing long-tail queries that
require a deeper level of semantic understanding. Besides, their
generated rewrites are often inconsistent with the optimization
goals of the actual search engine. On the other hand, LLM-based
rewriting methods [2, 15, 36, 37] provide a deeper understanding
and can generate appropriate expansion for long-tail queries. Nev-
ertheless they do not receive ad-hoc training on fine-tuning and
goal alignment, and lack of specialization and controllability to the
query rewriting task, which may potentially introduce illusions
and noise to the original query.
2.2 Preference Alignment
In recent years, with the increase in number of parameters, lan-
guage models [11, 24, 33, 41] have demonstrated incredible semantic
understanding and zero-shot capabilities [25, 39] on one hand. How-
ever, they have also faced the challenges of model hallucinations
and ethical issues [20, 24, 45]. These models have the potential to
fabricate facts with their extensive background knowledge [ 35],
resulting in misleading to users. To align the model’s outputs with
human morals and preferences, reinforcement learning (RL) has
been introduced to force the model to learn the partial order among
different outputs [7, 30]. For instance, OpenAI made a groundbreak-
ing application [24] of RL to the training process of large language
models, specifically with ChatGPT, which received tremendous
attention. Besides, LLama2 [33] designed a multi-objective reward
function that not only ensures the safety of model outputs but also
enhances their helpfulness. Unfortunately, these methods often rely
on complex training processes, and abundant high-quality data,
making them difficult to tune with vast number of hyperparameters.
Therefore, rejection sampling-like methods [4, 9, 10, 12] have been
proposed to continue training the model by collecting outputs with
high rewards from the previous rounds, aligning LLMs with human
preferences. Moreover, contrastive learning methods [27, 28, 42]
directly rank the outputs based on their rewards and utilize ranking
loss to adjust the output probabilities, explicitly learning the par-
tial order of outputs. Building upon previous findings, we further
propose three custom-designed Taobao metrics as ranking rewards
to calibrate generation probabilities of candidate rewrites, forcing
model to align with online objectives of Taobao search.
3 METHOD
3.1 Framework Overview
Long-tail query rewriting aims to expand the original query seman-
tics to address the problem of semantic gap while ensuring rele-
vance. To this end, as shown in Figure 2, we propose a three-stage
rewriting framework, which consists of: multi-instructions super-
vised fine-tuning (SFT), offline feedback and objective alignment. 1)
First, with rejeciton sampling, we constructed a multi-instructions
SFT dataset based on online logs that focuses on rewriting tasks,
mixed with quality classification, query correction and chain of
thought (CoT) tasks to train rewriting-specific LLMs. 2) After that,
we use the well-trained LLM obtained in the first stage to generate
multiple candidate rewrites for each sampled query. In order to
obtain the partial order of these candidate rewrites, we construct a
taobao offline system to obtain search results for these rewrites. The
quality scores of the search results are used to rank the candidates.
3) Based on the partial order of candidate rewrites, we calibrate the
generation probability of these rewrites using Bradley-Terry based
contrastive learning to maximize the probability of rewrites that
can obtain the desired search results.
3.2 Multi-instruction SFT
Given that no publicly available LLMs are specifically designed for
e-commerce query rewriting, direct utilization of general LLMs to
address the long-tail query semantic gap issue is likely to introduce
inaccuracies and noise. Consequently, we have embarked on an
approach wherein we gather various rewriting-related tasks to fine
tune LLMs, enhancing their ability to comprehend and rewrite
e-commerce queries effectively.
Query Rewriting Dataset: we initially source rewrites from Taobao
previous-generation rewriting policy. This process yields the ini-
tial rewriting dataset. Specifically, when a user initiates a query 𝑥
in Taobao search, old rewriting policy generates a list of rewrites
queries 𝑌 = {𝑦1,𝑦2,··· ,𝑦𝑛}. From this list, we select the top-ranked
𝑦1 as the gold standard candidate to construct our initial rewriting
dataset Dwith 𝑁 samples:
D=
 
𝑥𝑖,𝑦𝑖

𝑁
𝑖=1
such that 𝑥𝑖 ∼𝑝(𝑥),𝑦𝑖 ∼𝜋𝑜𝑙𝑑

𝑦 |𝑥 = 𝑥𝑖,𝜃𝑜𝑙𝑑

,
(1)
where 𝑝(𝑥)denotes the query distribution in Taobao search engine,
𝜋𝑜𝑙𝑑 and 𝜃𝑜𝑙𝑑 is the previous-generation rewriting policy of Taobao
and it’s parameters.
It’s important to highlight that e-commerce query rewriting
differs from other text generation tasks. In this context, semantic
similarity between query and rewrite does not necessarily guaran-
tee retrieval of similar sets of products. What we aim to achieve is
a high relevance between the products retrieved by rewrite 𝑦and
the original query 𝑥. To attain this, we apply a relevance filter to
Dthrough rejection sampling:
D𝑟 =
 
𝑥𝑖,𝑦𝑖

𝑁𝑟
𝑖=1
such that 𝑥𝑖,𝑦𝑖 ∈D,𝑟𝑒𝑙𝑒 (𝑥𝑖,𝑦𝑖)> 𝜏𝑟𝑒𝑙𝑒

,
(2)
where 𝑟𝑒𝑙𝑒(·)and 𝜏𝑟𝑒𝑙𝑒 denote the relevance method and its thresh-
old of <query, rewrite> pair. The detail of the function 𝑟𝑒𝑙𝑒(·)is
discussed in the Section 3.3.
Furthermore, Taobao’s previous-generation of rewriting models
primarily lacks optimization for long-tail queries. As we work on
the new generation of rewriting models, our goal is to maintain the
relevance of retrievals while expanding the semantics. This expan-
sion is aimed at alleviating the issue of long-tail queries leading
to “few-recall” results. As a result, we utilize rejection sampling
once again to filter D𝑟 by considering the retrieval increment. Ad-
ditionally, we include the most recent interacted product title of 𝑥
as supplementary information to better address long-tail queries:



Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 8):

the query from T5 gets the correct answer. The
number 2000 is misunderstood in query 1, while
query 2 keeps 200 movie together, avoiding mean-
ingless retrieval. Example 3 is for multiple choice.
The query simplifies the background and enhances
the keyword community planner. The retrieve con-
texts are mainly about Introduction to Community
Planning where the answer environment appears
several times.
7 Conclusion
This paper introduces the Rewrite-Retrieve-Read
pipeline, where a query rewriting step is added
for the retrieval-augmented LLM. This approach
is applicable for adopting a frozen large language
model as the reader and a real-time web search
engine as the retriever. Further, we propose to ap-
ply a tuneable small language model the rewriter,
which can be trained to cater to the frozen retriever
and reader. The training implementation consists
of two stages, warm-up and reinforcement learn-
ing. Evaluation and analyses on open-domain QA
and multiple-choice QA show the effectiveness
of query rewriting. Our work proposes a novel
retrieval-augmented black-box LLM framework,
proves that the retrieval augmentation can be en-
hanced from the aspect of query rewriting, and
provides a new method for integrating trainable
modules into black-box LLMs.
Limitations
We acknowledge the limitations of this work. (i)
There is still a trade-off between generalization and
specialization among downstream tasks. Adding
a training process, the scalability to direct transfer
is compromised, compared to few-shot in-context
learning. (ii) The research line of LLM agent has
shown impressive performance but relies on mul-
tiple calls to the LLM for each sample (Khattab
et al., 2022; Yao et al., 2023), where the LLM
plays as an agent to flexibly call the retriever multi-
ple times, reads the context in earlier hops, and
generates follow-up questions. Different from
these studies, our motivation is to enhance the one-
turn retriever-then-read framework with a trainable
query rewriter. (iii) Using a web search engine as
the retriever also leads to some limitations. Neu-
ral dense retrievers that are based on professional,
filtered knowledge bases may potentially achieve
better and controllable retrieval. More discussion
is included in the appendix.
References
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-
liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei
Ji, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan
Xu, and Pascale Fung. 2023. A multitask, multilin-
gual, multimodal evaluation of chatgpt on reason-
ing, hallucination, and interactivity. arXiv preprint
arXiv:2302.04023.
Parishad BehnamGhader, Santiago Miret, and Siva
Reddy. 2022. Can retriever-augmented language
models reason? the blame game between the re-
triever and the language model. arXiv preprint
arXiv:2212.09146.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading Wikipedia to answer open-
domain questions. In Association for Computational
Linguistics (ACL).
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Pondé de Oliveira Pinto, Jared Kaplan,
Harrison Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021. Evaluat-
ing large language models trained on code. CoRR,
abs/2107.03374.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of



Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 3):

constructing the search query.
3 Methodology
We present Rewrite-Retrieve-Read, a pipeline that
improves the retrieval-augmented LLM from the
perspective of query rewriting. Figure 1 shows an
overview. This section first introduces the pipeline
framework in section 3.1, then the trainable scheme
in section 3.2.
3.1 Rewrite-Retrieve-Read
A task with retrieval augmentation can be de-
noted as follows. Given a dataset of a knowledge-
intensive task (e.g., open-domain QA), D =
{(x, y)i}, i= 0, 1, 2, . . . , N, x (e.g., a question)
is the input to the pipeline, y is the expected output
(e.g., the correct answer). Our pipeline consists of
three steps. (i) Query rewrite: generate a query ˜x
for required knowledge based on the original input
x. (ii) Retrieve: search for related context,doc. (iii)
Read: comprehend the input along with contexts
[doc, x] and predict the output ˆy.
A straightforward but effective method is to ask
an LLM to rewrite queries to search for informa-
tion that is potentially needed. We use a few-shot
prompt to encourage the LLM to think, and the
output can be none, one or more queries to search.
3.2 Trainable Scheme
Besides, total reliance on a frozen LLM has shown
some drawbacks. Reasoning errors or invalid
search hinders the performance (Yao et al., 2023;
BehnamGhader et al., 2022). On the other hand,
retrieved knowledge may sometimes mislead and
compromise the language model (Mallen et al.,
2022). To better align to the frozen modules, it is
feasible to add a trainable model and adapt it by
taking the LLM reader feedback as a reward.
Based on our framework, we further propose to
utilize a trainable small language model to take
over the rewriting step, as is shown in the right
part of Figure 1. The trainable model is initial-
ized with the pre-trained T5-large (770M) (Raffel
et al., 2020), denoted astrainable rewriter, Gθ. The
rewriter is first trained on pseudo data to warm up
(§3.2.1), then continually trained by reinforcement
learning (§3.2.2).
3.2.1 Rewriter Warm-up
The task, query rewriting, is quite different from
the pre-training objective of sequence-to-sequence
generative models like T5. First, we construct a
pseudo dataset for the query rewriting task. In-
spired by recent distillation methods (Hsieh et al.,
2023; Ho et al., 2022), we prompt the LLM to
rewrite the original questions x in the training set
and collect the generated queries ˜x as pseudo la-
bels. The collected samples are then filtered: Those
that get correct predictions from the LLM reader
are selected into the warm-up dataset, denoted as
DT rain= {(x, ˜x)|ˆy = y}. The rewriter Gθ is fine-
tuned on DT rainwith the standard log-likelihood
as the training objective, denoted as
Lwarm = −
X
t
logpθ( ˆ˜xt | ˜x<t, x ). (1)
The rewriter model after warm-up shows mod-
est performance, which depends on the pseudo
data quality and rewriter capability. Highly relying
on the human-written prompt line, ˜x can be sub-
optimal. The relatively small scale of the rewriter
size is also a limitation of the performance after the
warm-up. Then we turn to reinforcement learning
to align the rewriter to the following retriever and
LLM reader.
3.2.2 Reinforcement Learning
To further fine-tune the rewriter to cater to the LLM
reader, we adopt a policy gradient reinforcement
learning framework.
Task Formulation In the context of reinforce-
ment learning, the rewriter optimization is for-
mulated as a Markov Decision Process 5-tuple
⟨S, A, P, R, γ⟩. (i) The state space S is a finite set
limited by the vocabulary and the sequence length.
(ii) The action space A is equals to the vocabulary.
(iii) The transition probability P is determined by
the policy network, which is the rewriter model
Gθ. (iv) The reward function R gives a reward
value that depends on the current state. The pol-
icy gradient is derived from rewards, used as the
training objective. (v) γ denotes the discount fac-
tor. More specifically, the rewriter Gθ after the
warm-up is the initial policy model π0. At each
step t, the action at is to generate the next token
ˆ˜xt based on the observation of the present state,
st = [x, ˆ˜x<t]. When the generation is stopped by
the End-Of-Sentence token, one episode is ended.
After finishing the retrieval and reading, a reward
is computed by evaluating the final output, i.e., a
score for the LLM reader prediction.
Policy Optimization We adopt Proximal Policy
Optimization (PPO) (Schulman et al., 2017), fol-
lowing (Ramamurthy et al., 2022). Maximization



Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 11):

diverse, explainable multi-hop question answering.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2369–2380, Brussels, Belgium. Association for Com-
putational Linguistics.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, and Yuan Cao. 2023.
ReAct: Synergizing reasoning and acting in language
models. In International Conference on Learning
Representations (ICLR).
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,
Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,
Michael Zeng, and Meng Jiang. 2023. Generate
rather than retrieve: Large language models are
strong context generators. In International Confer-
ence for Learning Representation (ICLR).
Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-
urmans, and Joseph E Gonzalez. 2023a. Tempera:
Test-time prompt editing via reinforcement learning.
In The Eleventh International Conference on Learn-
ing Representations.
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex
Smola. 2023b. Automatic chain of thought prompt-
ing in large language models. In The Eleventh In-
ternational Conference on Learning Representations
(ICLR 2023).
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B
Brown, Alec Radford, Dario Amodei, Paul Chris-
tiano, and Geoffrey Irving. 2019. Fine-tuning lan-
guage models from human preferences. arXiv
preprint arXiv:1909.08593.
A Warm-up Dataset
For the warm-up training of the tuneable rewriter,
we construct a pseudo dataset for the query rewrit-
ing task. For benchmarks that provide official train-
ing and test splits (HotpotQA and AmbigNQ), we
use the whole training set. For those that have no
official splits (PopQA and MMLU), we randomly
split the full dataset. In detail, PopQA contains 16
types of questions, thus split into 13k for training
and 714 for testing following stratified sampling.
For MMLU, each of the 4 categories is randomly
split into 80% for the training set and 20% for
the test set. Then the training sets of each bench-
mark are used to derive the pseudo dataset for the
query rewriting, i.e., DT rain = {(x, ˜x)|ˆy = y}.
We present the statistics of the splits and warm-up
dataset in Table 5.
B Setup Details
For warm-up, we train the T5-large with 3e-5 learn-
ing rate, {16, 20} batch size, for {6,8,12} epochs.
For reinforcement learning, we set the sampling
Task Training Set Warm-up Test Set
HotpotQA 90.4k 37.5k 7.4k
AmbigNQ 19.4k 8.6k 1k
PopQA 13.0k 6.0k 0.7k
Humanities 3.8k 1.5k 0.9k
STEM 2.4k 0.9k 0.6k
Other 2.6k 1.3k 0.6k
Social Science 2.4k 1.3k 0.6k
Table 5: Metrics of multiple choice QA.
steps to 5120, 10 threads, 512 steps for each. After
sampling, the policy network is trained for {2,3,4}
epochs, with learning rate as 2e-6 and batch size
as {8,16}. λf and λh are 1.0. β in Eq. 4 is dy-
namically adapted according to Ramamurthy et al.
(2022); Ziegler et al. (2019),
et = clip
KL (π∥π0) − KLtarget
KLtarget
, −0.2, 0.2

,
βt+1 = βt (1 + Kβet) ,
where KLtarget is set to 0.2, K β is set to 0.1. β0
is initialized to be 0.001. The generation strat-
egy follows the 4-beam search and returns the one
sequence. In the implementation of the BM25-
based retriever, the textboxes from searched URLs
are parsed from HTML code. We compute BM25
scores between the paragraph from each textbox
and the query following the scikit-learn package,
then keep those with higher scores until the re-
served context reaches a max length. In reinforce-
ment learning, the results of AmbigNQ are with
the BM25 method, while others use snippets as
context.
C Web Search: Tool Use
Our proposed pipeline integrates an externally built
web search engine as the retriever module. We
present more discussion on the advantages and dis-
advantages here.
The usage of external tools expands the abil-
ity boundary of language models, compensating
for the parametric knowledge, and grounding the
capabilities of language models to interact with en-
vironments (Qin et al., 2023; Schick et al., 2023).
Recent studies show a trend to leverage plug-and-
play tools like search engines to enhance language
agents (Lazaridou et al., 2022; Menick et al., 2022;
Shuster et al., 2022; Shen et al., 2023). Search
engine APIs are well-developed retrievers, saving
efforts to build and maintain another retriever, like
a Contriever. Accessible to the whole Internet, the
web search retrieves from a wide-range, up-to-date



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[9]_2311.03758.pdf (Page 1):

WWW ’24 Companion, May 13–17, 2024, Singapore, Singapore. Peng, et al.
“DIY blind box”
“Self-building blind box”
Query  Rewriting
Semantic UnderstandingRetrieval (Multi-path)
Item-based CF
Embedding-based  retrieval
PrerankRankRerankDisplay
Ranking System
ThousandsThousandsDozens
Merge
Inverted Index based Exact Matching
Figure 1: Framework of Taobao search engine.
ensuring accurate matching of user intent. However, due to the
variations for how users express their preferences for products,
semantic gaps often exist between their queries and the product
keywords, even worse with long-tail queries where retrieval sys-
tem may fail to provide any relevant products. For instance, a user
with personal expression habits may input a long-tail query like
"self-building blind box" , which will lead to more retrieval results if
with its synonymous query like "DIY blind box" . Unfortunately, tra-
ditional term-matching solutions like inverted index could probably
fail to match the commonly-used “DIY” with non-customary term
“self-building”, which limit the retrieval results and significantly
impair the user experience. Therefore, it is urgently required to
solve the semantic gap challenge for long-tail queries, and address
the problem of “few-recall” in e-commerce platform.
Traditionally, prior arts [13, 19, 34, 44] mainly focus on the “em-
bedding based retrieval” paradigm, which initially map the queries
and products into a common semantic space, and then support the
Top-𝐾 retrieval with approximate nearest neighbor (ANN) meth-
ods. However, the retrieval outcomes might be difficult to interpret,
which severely limit the performance. To enhance the controlla-
bility of retrieval outcomes, some efforts have been made on the
“query rewriting & exact match” paradigm. On the one hand, the
discriminative methods [18, 46] attempt to “rewrite” queries via
finding similar terms from a query reformulation set, and then
utilize them to search for relevant products using sparse retrieval.
Although these approaches could effectively expand the semantic
of hot queries, long-tail queries may not be adequately optimized,
thus no related rewrite can be generated. On the other hand, the
generative methods [26, 38] involve supervised training on <query,
rewrite> pair data to empower the model with rewriting capabilities,
and the alignment process [1, 23] is further incorporated to enhance
the metric preference. Although these methods partially address
the semantic gap problem, they typically rely on small generative
models with limited comprehension of long-tail queries, which
significantly constrained the rewriting capability. Recently, with
the development of LLM techniques, some efforts [ 2, 15, 36, 37]
solely utilize LLMs as retrieval data augmentation generators with-
out additional training to expand query semantics. However, these
methods, even with carefully curated prompts, may still constrain
the ability to specialize for query rewriting task, leading to the poor
alignment with objectives of e-commerce search.
To effectively bridge the semantic gap for long-tail queries, and
solve the above challenges via producing controlled and aligned
outcomes with integrating the knowledge of LLMs, we propose
BEQUE, a novel framework that involves three stages of fine-tuning
LLMs, namely multi-instruction supervised fine-tuning (SFT), offline
system feedback , and objective alignment . Specifically, in the first
stage, we utilize the rejection sampling to collect <query, rewrite>
pairs with desired quality distribution, and then combine these
pairs with data from quality classification, product title prediction
and chain of thought (CoT) tasks to construct the multi-instruction
rewriting dataset for fine-tuning our LLM. Next, with the well-
trained LLM, we employ beam search to generate multiple candidate
rewrites for each query. These candidate rewrites are fed into the
Taobao offline system to retrieve a collection of related products. We
calculate the quality score of retrieved products for the rewrites and
use them as rewards for candidates ranking. To calibrate generation
probability of the candidate rewrites, we introduce a Bradley-Terry
based contrastive learning method that considers the partial order
among the these rewrites. Ultimately, the model training objective
is aligned with the online goal of the Taobao search, ensuring that
the generated rewrites yield the desired search results.
The main contributions of this work are listed as follows:
•We have analyzed long-tail queries in e-commerce search
and identified the semantic gap problem associated with
such queries. To the best of our knowledge, we are the first
to fine-tune LLMs for industrial query rewriting task.
•We propose a three-stage fine-tuned framework called BEQUE
to address the issue of semantic gap in long-tail queries. This
framework is designed to generate rewrites that align with
the objectives of Taobao search.
•The effectiveness of our model is demonstrated through
both offline and online experiments, showcasing its ability
to significantly improve e-commerce revenue.
2 RELATED WORKS
2.1 Query Rewriting
Query rewriting, also known as query expansion or query reformu-
lation, plays a pivotal role in e-commerce search technology and
has a profound impact on the user’s shopping experience and the
revenue of e-commerce platforms. This technique can be broadly
categorized into discriminative and generative methods.
Discriminative methods treat query rewriting as a retrieval
process that expand the semantics of the original query by selecting
appropriate terms from the candidate set. For example, pseudo-
relevance [6, 31, 40] selects the top k documents from the initial
retrieval as semantic extensions. These approaches, however, of-
ten pose challenges in effectively controlling the semantic scope
and ensuring retrieval relevance. To address these challenges, one
potential solution is to utilize a well-built thesaurus [ 5, 22] as a
candidate rewrite set. However, it is important to note that the
effectiveness of these methods highly depends on the quality of
the thesaurus. Inadequate quality may result in query semantic
drift, where the intended meaning of the query is compromised.
Furthermore, alternative approaches [3, 8, 18, 21] involve generat-
ing candidate rewrites based on search logs, incorporating similar
terms from users’ search history as extensions. Unfortunately, due
to the Matthew effect, search logs naturally exhibit a bias towards
popular queries, resulting in that the training data collected through
this approach may not sufficiently meet the optimization needs for
less frequently searched long-tail queries.



Source: data\tc16_2312.10997v5\referenced_papers\[9]_2311.03758.pdf (Page 2):

Large Language Model based Long-tail Query Rewriting in Taobao Search WWW ’24 Companion, May 13–17, 2024, Singapore, Singapore.
Generative methods [16, 26, 29, 38] formulate the rewriting
task as a generative process, where a transformer-like model is
utilized to generate candidate terms for the original query. Some
methods [1, 23] incorporate reinforcement learning or contrastive
learning to align with human preferences or offline metrics. These
methods have the ability to generate related rewrites for each query,
but usually employ a model with limited number of parameters,
which makes them less effective in processing long-tail queries that
require a deeper level of semantic understanding. Besides, their
generated rewrites are often inconsistent with the optimization
goals of the actual search engine. On the other hand, LLM-based
rewriting methods [2, 15, 36, 37] provide a deeper understanding
and can generate appropriate expansion for long-tail queries. Nev-
ertheless they do not receive ad-hoc training on fine-tuning and
goal alignment, and lack of specialization and controllability to the
query rewriting task, which may potentially introduce illusions
and noise to the original query.
2.2 Preference Alignment
In recent years, with the increase in number of parameters, lan-
guage models [11, 24, 33, 41] have demonstrated incredible semantic
understanding and zero-shot capabilities [25, 39] on one hand. How-
ever, they have also faced the challenges of model hallucinations
and ethical issues [20, 24, 45]. These models have the potential to
fabricate facts with their extensive background knowledge [ 35],
resulting in misleading to users. To align the model’s outputs with
human morals and preferences, reinforcement learning (RL) has
been introduced to force the model to learn the partial order among
different outputs [7, 30]. For instance, OpenAI made a groundbreak-
ing application [24] of RL to the training process of large language
models, specifically with ChatGPT, which received tremendous
attention. Besides, LLama2 [33] designed a multi-objective reward
function that not only ensures the safety of model outputs but also
enhances their helpfulness. Unfortunately, these methods often rely
on complex training processes, and abundant high-quality data,
making them difficult to tune with vast number of hyperparameters.
Therefore, rejection sampling-like methods [4, 9, 10, 12] have been
proposed to continue training the model by collecting outputs with
high rewards from the previous rounds, aligning LLMs with human
preferences. Moreover, contrastive learning methods [27, 28, 42]
directly rank the outputs based on their rewards and utilize ranking
loss to adjust the output probabilities, explicitly learning the par-
tial order of outputs. Building upon previous findings, we further
propose three custom-designed Taobao metrics as ranking rewards
to calibrate generation probabilities of candidate rewrites, forcing
model to align with online objectives of Taobao search.
3 METHOD
3.1 Framework Overview
Long-tail query rewriting aims to expand the original query seman-
tics to address the problem of semantic gap while ensuring rele-
vance. To this end, as shown in Figure 2, we propose a three-stage
rewriting framework, which consists of: multi-instructions super-
vised fine-tuning (SFT), offline feedback and objective alignment. 1)
First, with rejeciton sampling, we constructed a multi-instructions
SFT dataset based on online logs that focuses on rewriting tasks,
mixed with quality classification, query correction and chain of
thought (CoT) tasks to train rewriting-specific LLMs. 2) After that,
we use the well-trained LLM obtained in the first stage to generate
multiple candidate rewrites for each sampled query. In order to
obtain the partial order of these candidate rewrites, we construct a
taobao offline system to obtain search results for these rewrites. The
quality scores of the search results are used to rank the candidates.
3) Based on the partial order of candidate rewrites, we calibrate the
generation probability of these rewrites using Bradley-Terry based
contrastive learning to maximize the probability of rewrites that
can obtain the desired search results.
3.2 Multi-instruction SFT
Given that no publicly available LLMs are specifically designed for
e-commerce query rewriting, direct utilization of general LLMs to
address the long-tail query semantic gap issue is likely to introduce
inaccuracies and noise. Consequently, we have embarked on an
approach wherein we gather various rewriting-related tasks to fine
tune LLMs, enhancing their ability to comprehend and rewrite
e-commerce queries effectively.
Query Rewriting Dataset: we initially source rewrites from Taobao
previous-generation rewriting policy. This process yields the ini-
tial rewriting dataset. Specifically, when a user initiates a query 𝑥
in Taobao search, old rewriting policy generates a list of rewrites
queries 𝑌 = {𝑦1,𝑦2,··· ,𝑦𝑛}. From this list, we select the top-ranked
𝑦1 as the gold standard candidate to construct our initial rewriting
dataset Dwith 𝑁 samples:
D=
 
𝑥𝑖,𝑦𝑖

𝑁
𝑖=1
such that 𝑥𝑖 ∼𝑝(𝑥),𝑦𝑖 ∼𝜋𝑜𝑙𝑑

𝑦 |𝑥 = 𝑥𝑖,𝜃𝑜𝑙𝑑

,
(1)
where 𝑝(𝑥)denotes the query distribution in Taobao search engine,
𝜋𝑜𝑙𝑑 and 𝜃𝑜𝑙𝑑 is the previous-generation rewriting policy of Taobao
and it’s parameters.
It’s important to highlight that e-commerce query rewriting
differs from other text generation tasks. In this context, semantic
similarity between query and rewrite does not necessarily guaran-
tee retrieval of similar sets of products. What we aim to achieve is
a high relevance between the products retrieved by rewrite 𝑦and
the original query 𝑥. To attain this, we apply a relevance filter to
Dthrough rejection sampling:
D𝑟 =
 
𝑥𝑖,𝑦𝑖

𝑁𝑟
𝑖=1
such that 𝑥𝑖,𝑦𝑖 ∈D,𝑟𝑒𝑙𝑒 (𝑥𝑖,𝑦𝑖)> 𝜏𝑟𝑒𝑙𝑒

,
(2)
where 𝑟𝑒𝑙𝑒(·)and 𝜏𝑟𝑒𝑙𝑒 denote the relevance method and its thresh-
old of <query, rewrite> pair. The detail of the function 𝑟𝑒𝑙𝑒(·)is
discussed in the Section 3.3.
Furthermore, Taobao’s previous-generation of rewriting models
primarily lacks optimization for long-tail queries. As we work on
the new generation of rewriting models, our goal is to maintain the
relevance of retrievals while expanding the semantics. This expan-
sion is aimed at alleviating the issue of long-tail queries leading
to “few-recall” results. As a result, we utilize rejection sampling
once again to filter D𝑟 by considering the retrieval increment. Ad-
ditionally, we include the most recent interacted product title of 𝑥
as supplementary information to better address long-tail queries:



Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 8):

the query from T5 gets the correct answer. The
number 2000 is misunderstood in query 1, while
query 2 keeps 200 movie together, avoiding mean-
ingless retrieval. Example 3 is for multiple choice.
The query simplifies the background and enhances
the keyword community planner. The retrieve con-
texts are mainly about Introduction to Community
Planning where the answer environment appears
several times.
7 Conclusion
This paper introduces the Rewrite-Retrieve-Read
pipeline, where a query rewriting step is added
for the retrieval-augmented LLM. This approach
is applicable for adopting a frozen large language
model as the reader and a real-time web search
engine as the retriever. Further, we propose to ap-
ply a tuneable small language model the rewriter,
which can be trained to cater to the frozen retriever
and reader. The training implementation consists
of two stages, warm-up and reinforcement learn-
ing. Evaluation and analyses on open-domain QA
and multiple-choice QA show the effectiveness
of query rewriting. Our work proposes a novel
retrieval-augmented black-box LLM framework,
proves that the retrieval augmentation can be en-
hanced from the aspect of query rewriting, and
provides a new method for integrating trainable
modules into black-box LLMs.
Limitations
We acknowledge the limitations of this work. (i)
There is still a trade-off between generalization and
specialization among downstream tasks. Adding
a training process, the scalability to direct transfer
is compromised, compared to few-shot in-context
learning. (ii) The research line of LLM agent has
shown impressive performance but relies on mul-
tiple calls to the LLM for each sample (Khattab
et al., 2022; Yao et al., 2023), where the LLM
plays as an agent to flexibly call the retriever multi-
ple times, reads the context in earlier hops, and
generates follow-up questions. Different from
these studies, our motivation is to enhance the one-
turn retriever-then-read framework with a trainable
query rewriter. (iii) Using a web search engine as
the retriever also leads to some limitations. Neu-
ral dense retrievers that are based on professional,
filtered knowledge bases may potentially achieve
better and controllable retrieval. More discussion
is included in the appendix.
References
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-
liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei
Ji, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan
Xu, and Pascale Fung. 2023. A multitask, multilin-
gual, multimodal evaluation of chatgpt on reason-
ing, hallucination, and interactivity. arXiv preprint
arXiv:2302.04023.
Parishad BehnamGhader, Santiago Miret, and Siva
Reddy. 2022. Can retriever-augmented language
models reason? the blame game between the re-
triever and the language model. arXiv preprint
arXiv:2212.09146.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading Wikipedia to answer open-
domain questions. In Association for Computational
Linguistics (ACL).
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Pondé de Oliveira Pinto, Jared Kaplan,
Harrison Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021. Evaluat-
ing large language models trained on code. CoRR,
abs/2107.03374.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of



Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 3):

constructing the search query.
3 Methodology
We present Rewrite-Retrieve-Read, a pipeline that
improves the retrieval-augmented LLM from the
perspective of query rewriting. Figure 1 shows an
overview. This section first introduces the pipeline
framework in section 3.1, then the trainable scheme
in section 3.2.
3.1 Rewrite-Retrieve-Read
A task with retrieval augmentation can be de-
noted as follows. Given a dataset of a knowledge-
intensive task (e.g., open-domain QA), D =
{(x, y)i}, i= 0, 1, 2, . . . , N, x (e.g., a question)
is the input to the pipeline, y is the expected output
(e.g., the correct answer). Our pipeline consists of
three steps. (i) Query rewrite: generate a query ˜x
for required knowledge based on the original input
x. (ii) Retrieve: search for related context,doc. (iii)
Read: comprehend the input along with contexts
[doc, x] and predict the output ˆy.
A straightforward but effective method is to ask
an LLM to rewrite queries to search for informa-
tion that is potentially needed. We use a few-shot
prompt to encourage the LLM to think, and the
output can be none, one or more queries to search.
3.2 Trainable Scheme
Besides, total reliance on a frozen LLM has shown
some drawbacks. Reasoning errors or invalid
search hinders the performance (Yao et al., 2023;
BehnamGhader et al., 2022). On the other hand,
retrieved knowledge may sometimes mislead and
compromise the language model (Mallen et al.,
2022). To better align to the frozen modules, it is
feasible to add a trainable model and adapt it by
taking the LLM reader feedback as a reward.
Based on our framework, we further propose to
utilize a trainable small language model to take
over the rewriting step, as is shown in the right
part of Figure 1. The trainable model is initial-
ized with the pre-trained T5-large (770M) (Raffel
et al., 2020), denoted astrainable rewriter, Gθ. The
rewriter is first trained on pseudo data to warm up
(§3.2.1), then continually trained by reinforcement
learning (§3.2.2).
3.2.1 Rewriter Warm-up
The task, query rewriting, is quite different from
the pre-training objective of sequence-to-sequence
generative models like T5. First, we construct a
pseudo dataset for the query rewriting task. In-
spired by recent distillation methods (Hsieh et al.,
2023; Ho et al., 2022), we prompt the LLM to
rewrite the original questions x in the training set
and collect the generated queries ˜x as pseudo la-
bels. The collected samples are then filtered: Those
that get correct predictions from the LLM reader
are selected into the warm-up dataset, denoted as
DT rain= {(x, ˜x)|ˆy = y}. The rewriter Gθ is fine-
tuned on DT rainwith the standard log-likelihood
as the training objective, denoted as
Lwarm = −
X
t
logpθ( ˆ˜xt | ˜x<t, x ). (1)
The rewriter model after warm-up shows mod-
est performance, which depends on the pseudo
data quality and rewriter capability. Highly relying
on the human-written prompt line, ˜x can be sub-
optimal. The relatively small scale of the rewriter
size is also a limitation of the performance after the
warm-up. Then we turn to reinforcement learning
to align the rewriter to the following retriever and
LLM reader.
3.2.2 Reinforcement Learning
To further fine-tune the rewriter to cater to the LLM
reader, we adopt a policy gradient reinforcement
learning framework.
Task Formulation In the context of reinforce-
ment learning, the rewriter optimization is for-
mulated as a Markov Decision Process 5-tuple
⟨S, A, P, R, γ⟩. (i) The state space S is a finite set
limited by the vocabulary and the sequence length.
(ii) The action space A is equals to the vocabulary.
(iii) The transition probability P is determined by
the policy network, which is the rewriter model
Gθ. (iv) The reward function R gives a reward
value that depends on the current state. The pol-
icy gradient is derived from rewards, used as the
training objective. (v) γ denotes the discount fac-
tor. More specifically, the rewriter Gθ after the
warm-up is the initial policy model π0. At each
step t, the action at is to generate the next token
ˆ˜xt based on the observation of the present state,
st = [x, ˆ˜x<t]. When the generation is stopped by
the End-Of-Sentence token, one episode is ended.
After finishing the retrieval and reading, a reward
is computed by evaluating the final output, i.e., a
score for the LLM reader prediction.
Policy Optimization We adopt Proximal Policy
Optimization (PPO) (Schulman et al., 2017), fol-
lowing (Ramamurthy et al., 2022). Maximization



Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 11):

diverse, explainable multi-hop question answering.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2369–2380, Brussels, Belgium. Association for Com-
putational Linguistics.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, and Yuan Cao. 2023.
ReAct: Synergizing reasoning and acting in language
models. In International Conference on Learning
Representations (ICLR).
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,
Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,
Michael Zeng, and Meng Jiang. 2023. Generate
rather than retrieve: Large language models are
strong context generators. In International Confer-
ence for Learning Representation (ICLR).
Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-
urmans, and Joseph E Gonzalez. 2023a. Tempera:
Test-time prompt editing via reinforcement learning.
In The Eleventh International Conference on Learn-
ing Representations.
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex
Smola. 2023b. Automatic chain of thought prompt-
ing in large language models. In The Eleventh In-
ternational Conference on Learning Representations
(ICLR 2023).
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B
Brown, Alec Radford, Dario Amodei, Paul Chris-
tiano, and Geoffrey Irving. 2019. Fine-tuning lan-
guage models from human preferences. arXiv
preprint arXiv:1909.08593.
A Warm-up Dataset
For the warm-up training of the tuneable rewriter,
we construct a pseudo dataset for the query rewrit-
ing task. For benchmarks that provide official train-
ing and test splits (HotpotQA and AmbigNQ), we
use the whole training set. For those that have no
official splits (PopQA and MMLU), we randomly
split the full dataset. In detail, PopQA contains 16
types of questions, thus split into 13k for training
and 714 for testing following stratified sampling.
For MMLU, each of the 4 categories is randomly
split into 80% for the training set and 20% for
the test set. Then the training sets of each bench-
mark are used to derive the pseudo dataset for the
query rewriting, i.e., DT rain = {(x, ˜x)|ˆy = y}.
We present the statistics of the splits and warm-up
dataset in Table 5.
B Setup Details
For warm-up, we train the T5-large with 3e-5 learn-
ing rate, {16, 20} batch size, for {6,8,12} epochs.
For reinforcement learning, we set the sampling
Task Training Set Warm-up Test Set
HotpotQA 90.4k 37.5k 7.4k
AmbigNQ 19.4k 8.6k 1k
PopQA 13.0k 6.0k 0.7k
Humanities 3.8k 1.5k 0.9k
STEM 2.4k 0.9k 0.6k
Other 2.6k 1.3k 0.6k
Social Science 2.4k 1.3k 0.6k
Table 5: Metrics of multiple choice QA.
steps to 5120, 10 threads, 512 steps for each. After
sampling, the policy network is trained for {2,3,4}
epochs, with learning rate as 2e-6 and batch size
as {8,16}. λf and λh are 1.0. β in Eq. 4 is dy-
namically adapted according to Ramamurthy et al.
(2022); Ziegler et al. (2019),
et = clip
KL (π∥π0) − KLtarget
KLtarget
, −0.2, 0.2

,
βt+1 = βt (1 + Kβet) ,
where KLtarget is set to 0.2, K β is set to 0.1. β0
is initialized to be 0.001. The generation strat-
egy follows the 4-beam search and returns the one
sequence. In the implementation of the BM25-
based retriever, the textboxes from searched URLs
are parsed from HTML code. We compute BM25
scores between the paragraph from each textbox
and the query following the scikit-learn package,
then keep those with higher scores until the re-
served context reaches a max length. In reinforce-
ment learning, the results of AmbigNQ are with
the BM25 method, while others use snippets as
context.
C Web Search: Tool Use
Our proposed pipeline integrates an externally built
web search engine as the retriever module. We
present more discussion on the advantages and dis-
advantages here.
The usage of external tools expands the abil-
ity boundary of language models, compensating
for the parametric knowledge, and grounding the
capabilities of language models to interact with en-
vironments (Qin et al., 2023; Schick et al., 2023).
Recent studies show a trend to leverage plug-and-
play tools like search engines to enhance language
agents (Lazaridou et al., 2022; Menick et al., 2022;
Shuster et al., 2022; Shen et al., 2023). Search
engine APIs are well-developed retrievers, saving
efforts to build and maintain another retriever, like
a Contriever. Accessible to the whole Internet, the
web search retrieves from a wide-range, up-to-date



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[9]_2311.03758.pdf (Page 1):

WWW ’24 Companion, May 13–17, 2024, Singapore, Singapore. Peng, et al.
“DIY blind box”
“Self-building blind box”
Query  Rewriting
Semantic UnderstandingRetrieval (Multi-path)
Item-based CF
Embedding-based  retrieval
PrerankRankRerankDisplay
Ranking System
ThousandsThousandsDozens
Merge
Inverted Index based Exact Matching
Figure 1: Framework of Taobao search engine.
ensuring accurate matching of user intent. However, due to the
variations for how users express their preferences for products,
semantic gaps often exist between their queries and the product
keywords, even worse with long-tail queries where retrieval sys-
tem may fail to provide any relevant products. For instance, a user
with personal expression habits may input a long-tail query like
"self-building blind box" , which will lead to more retrieval results if
with its synonymous query like "DIY blind box" . Unfortunately, tra-
ditional term-matching solutions like inverted index could probably
fail to match the commonly-used “DIY” with non-customary term
“self-building”, which limit the retrieval results and significantly
impair the user experience. Therefore, it is urgently required to
solve the semantic gap challenge for long-tail queries, and address
the problem of “few-recall” in e-commerce platform.
Traditionally, prior arts [13, 19, 34, 44] mainly focus on the “em-
bedding based retrieval” paradigm, which initially map the queries
and products into a common semantic space, and then support the
Top-𝐾 retrieval with approximate nearest neighbor (ANN) meth-
ods. However, the retrieval outcomes might be difficult to interpret,
which severely limit the performance. To enhance the controlla-
bility of retrieval outcomes, some efforts have been made on the
“query rewriting & exact match” paradigm. On the one hand, the
discriminative methods [18, 46] attempt to “rewrite” queries via
finding similar terms from a query reformulation set, and then
utilize them to search for relevant products using sparse retrieval.
Although these approaches could effectively expand the semantic
of hot queries, long-tail queries may not be adequately optimized,
thus no related rewrite can be generated. On the other hand, the
generative methods [26, 38] involve supervised training on <query,
rewrite> pair data to empower the model with rewriting capabilities,
and the alignment process [1, 23] is further incorporated to enhance
the metric preference. Although these methods partially address
the semantic gap problem, they typically rely on small generative
models with limited comprehension of long-tail queries, which
significantly constrained the rewriting capability. Recently, with
the development of LLM techniques, some efforts [ 2, 15, 36, 37]
solely utilize LLMs as retrieval data augmentation generators with-
out additional training to expand query semantics. However, these
methods, even with carefully curated prompts, may still constrain
the ability to specialize for query rewriting task, leading to the poor
alignment with objectives of e-commerce search.
To effectively bridge the semantic gap for long-tail queries, and
solve the above challenges via producing controlled and aligned
outcomes with integrating the knowledge of LLMs, we propose
BEQUE, a novel framework that involves three stages of fine-tuning
LLMs, namely multi-instruction supervised fine-tuning (SFT), offline
system feedback , and objective alignment . Specifically, in the first
stage, we utilize the rejection sampling to collect <query, rewrite>
pairs with desired quality distribution, and then combine these
pairs with data from quality classification, product title prediction
and chain of thought (CoT) tasks to construct the multi-instruction
rewriting dataset for fine-tuning our LLM. Next, with the well-
trained LLM, we employ beam search to generate multiple candidate
rewrites for each query. These candidate rewrites are fed into the
Taobao offline system to retrieve a collection of related products. We
calculate the quality score of retrieved products for the rewrites and
use them as rewards for candidates ranking. To calibrate generation
probability of the candidate rewrites, we introduce a Bradley-Terry
based contrastive learning method that considers the partial order
among the these rewrites. Ultimately, the model training objective
is aligned with the online goal of the Taobao search, ensuring that
the generated rewrites yield the desired search results.
The main contributions of this work are listed as follows:
•We have analyzed long-tail queries in e-commerce search
and identified the semantic gap problem associated with
such queries. To the best of our knowledge, we are the first
to fine-tune LLMs for industrial query rewriting task.
•We propose a three-stage fine-tuned framework called BEQUE
to address the issue of semantic gap in long-tail queries. This
framework is designed to generate rewrites that align with
the objectives of Taobao search.
•The effectiveness of our model is demonstrated through
both offline and online experiments, showcasing its ability
to significantly improve e-commerce revenue.
2 RELATED WORKS
2.1 Query Rewriting
Query rewriting, also known as query expansion or query reformu-
lation, plays a pivotal role in e-commerce search technology and
has a profound impact on the user’s shopping experience and the
revenue of e-commerce platforms. This technique can be broadly
categorized into discriminative and generative methods.
Discriminative methods treat query rewriting as a retrieval
process that expand the semantics of the original query by selecting
appropriate terms from the candidate set. For example, pseudo-
relevance [6, 31, 40] selects the top k documents from the initial
retrieval as semantic extensions. These approaches, however, of-
ten pose challenges in effectively controlling the semantic scope
and ensuring retrieval relevance. To address these challenges, one
potential solution is to utilize a well-built thesaurus [ 5, 22] as a
candidate rewrite set. However, it is important to note that the
effectiveness of these methods highly depends on the quality of
the thesaurus. Inadequate quality may result in query semantic
drift, where the intended meaning of the query is compromised.
Furthermore, alternative approaches [3, 8, 18, 21] involve generat-
ing candidate rewrites based on search logs, incorporating similar
terms from users’ search history as extensions. Unfortunately, due
to the Matthew effect, search logs naturally exhibit a bias towards
popular queries, resulting in that the training data collected through
this approach may not sufficiently meet the optimization needs for
less frequently searched long-tail queries.



Source: data\tc16_2312.10997v5\referenced_papers\[9]_2311.03758.pdf (Page 2):

Large Language Model based Long-tail Query Rewriting in Taobao Search WWW ’24 Companion, May 13–17, 2024, Singapore, Singapore.
Generative methods [16, 26, 29, 38] formulate the rewriting
task as a generative process, where a transformer-like model is
utilized to generate candidate terms for the original query. Some
methods [1, 23] incorporate reinforcement learning or contrastive
learning to align with human preferences or offline metrics. These
methods have the ability to generate related rewrites for each query,
but usually employ a model with limited number of parameters,
which makes them less effective in processing long-tail queries that
require a deeper level of semantic understanding. Besides, their
generated rewrites are often inconsistent with the optimization
goals of the actual search engine. On the other hand, LLM-based
rewriting methods [2, 15, 36, 37] provide a deeper understanding
and can generate appropriate expansion for long-tail queries. Nev-
ertheless they do not receive ad-hoc training on fine-tuning and
goal alignment, and lack of specialization and controllability to the
query rewriting task, which may potentially introduce illusions
and noise to the original query.
2.2 Preference Alignment
In recent years, with the increase in number of parameters, lan-
guage models [11, 24, 33, 41] have demonstrated incredible semantic
understanding and zero-shot capabilities [25, 39] on one hand. How-
ever, they have also faced the challenges of model hallucinations
and ethical issues [20, 24, 45]. These models have the potential to
fabricate facts with their extensive background knowledge [ 35],
resulting in misleading to users. To align the model’s outputs with
human morals and preferences, reinforcement learning (RL) has
been introduced to force the model to learn the partial order among
different outputs [7, 30]. For instance, OpenAI made a groundbreak-
ing application [24] of RL to the training process of large language
models, specifically with ChatGPT, which received tremendous
attention. Besides, LLama2 [33] designed a multi-objective reward
function that not only ensures the safety of model outputs but also
enhances their helpfulness. Unfortunately, these methods often rely
on complex training processes, and abundant high-quality data,
making them difficult to tune with vast number of hyperparameters.
Therefore, rejection sampling-like methods [4, 9, 10, 12] have been
proposed to continue training the model by collecting outputs with
high rewards from the previous rounds, aligning LLMs with human
preferences. Moreover, contrastive learning methods [27, 28, 42]
directly rank the outputs based on their rewards and utilize ranking
loss to adjust the output probabilities, explicitly learning the par-
tial order of outputs. Building upon previous findings, we further
propose three custom-designed Taobao metrics as ranking rewards
to calibrate generation probabilities of candidate rewrites, forcing
model to align with online objectives of Taobao search.
3 METHOD
3.1 Framework Overview
Long-tail query rewriting aims to expand the original query seman-
tics to address the problem of semantic gap while ensuring rele-
vance. To this end, as shown in Figure 2, we propose a three-stage
rewriting framework, which consists of: multi-instructions super-
vised fine-tuning (SFT), offline feedback and objective alignment. 1)
First, with rejeciton sampling, we constructed a multi-instructions
SFT dataset based on online logs that focuses on rewriting tasks,
mixed with quality classification, query correction and chain of
thought (CoT) tasks to train rewriting-specific LLMs. 2) After that,
we use the well-trained LLM obtained in the first stage to generate
multiple candidate rewrites for each sampled query. In order to
obtain the partial order of these candidate rewrites, we construct a
taobao offline system to obtain search results for these rewrites. The
quality scores of the search results are used to rank the candidates.
3) Based on the partial order of candidate rewrites, we calibrate the
generation probability of these rewrites using Bradley-Terry based
contrastive learning to maximize the probability of rewrites that
can obtain the desired search results.
3.2 Multi-instruction SFT
Given that no publicly available LLMs are specifically designed for
e-commerce query rewriting, direct utilization of general LLMs to
address the long-tail query semantic gap issue is likely to introduce
inaccuracies and noise. Consequently, we have embarked on an
approach wherein we gather various rewriting-related tasks to fine
tune LLMs, enhancing their ability to comprehend and rewrite
e-commerce queries effectively.
Query Rewriting Dataset: we initially source rewrites from Taobao
previous-generation rewriting policy. This process yields the ini-
tial rewriting dataset. Specifically, when a user initiates a query 𝑥
in Taobao search, old rewriting policy generates a list of rewrites
queries 𝑌 = {𝑦1,𝑦2,··· ,𝑦𝑛}. From this list, we select the top-ranked
𝑦1 as the gold standard candidate to construct our initial rewriting
dataset Dwith 𝑁 samples:
D=
 
𝑥𝑖,𝑦𝑖

𝑁
𝑖=1
such that 𝑥𝑖 ∼𝑝(𝑥),𝑦𝑖 ∼𝜋𝑜𝑙𝑑

𝑦 |𝑥 = 𝑥𝑖,𝜃𝑜𝑙𝑑

,
(1)
where 𝑝(𝑥)denotes the query distribution in Taobao search engine,
𝜋𝑜𝑙𝑑 and 𝜃𝑜𝑙𝑑 is the previous-generation rewriting policy of Taobao
and it’s parameters.
It’s important to highlight that e-commerce query rewriting
differs from other text generation tasks. In this context, semantic
similarity between query and rewrite does not necessarily guaran-
tee retrieval of similar sets of products. What we aim to achieve is
a high relevance between the products retrieved by rewrite 𝑦and
the original query 𝑥. To attain this, we apply a relevance filter to
Dthrough rejection sampling:
D𝑟 =
 
𝑥𝑖,𝑦𝑖

𝑁𝑟
𝑖=1
such that 𝑥𝑖,𝑦𝑖 ∈D,𝑟𝑒𝑙𝑒 (𝑥𝑖,𝑦𝑖)> 𝜏𝑟𝑒𝑙𝑒

,
(2)
where 𝑟𝑒𝑙𝑒(·)and 𝜏𝑟𝑒𝑙𝑒 denote the relevance method and its thresh-
old of <query, rewrite> pair. The detail of the function 𝑟𝑒𝑙𝑒(·)is
discussed in the Section 3.3.
Furthermore, Taobao’s previous-generation of rewriting models
primarily lacks optimization for long-tail queries. As we work on
the new generation of rewriting models, our goal is to maintain the
relevance of retrievals while expanding the semantics. This expan-
sion is aimed at alleviating the issue of long-tail queries leading
to “few-recall” results. As a result, we utilize rejection sampling
once again to filter D𝑟 by considering the retrieval increment. Ad-
ditionally, we include the most recent interacted product title of 𝑥
as supplementary information to better address long-tail queries:



Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 8):

the query from T5 gets the correct answer. The
number 2000 is misunderstood in query 1, while
query 2 keeps 200 movie together, avoiding mean-
ingless retrieval. Example 3 is for multiple choice.
The query simplifies the background and enhances
the keyword community planner. The retrieve con-
texts are mainly about Introduction to Community
Planning where the answer environment appears
several times.
7 Conclusion
This paper introduces the Rewrite-Retrieve-Read
pipeline, where a query rewriting step is added
for the retrieval-augmented LLM. This approach
is applicable for adopting a frozen large language
model as the reader and a real-time web search
engine as the retriever. Further, we propose to ap-
ply a tuneable small language model the rewriter,
which can be trained to cater to the frozen retriever
and reader. The training implementation consists
of two stages, warm-up and reinforcement learn-
ing. Evaluation and analyses on open-domain QA
and multiple-choice QA show the effectiveness
of query rewriting. Our work proposes a novel
retrieval-augmented black-box LLM framework,
proves that the retrieval augmentation can be en-
hanced from the aspect of query rewriting, and
provides a new method for integrating trainable
modules into black-box LLMs.
Limitations
We acknowledge the limitations of this work. (i)
There is still a trade-off between generalization and
specialization among downstream tasks. Adding
a training process, the scalability to direct transfer
is compromised, compared to few-shot in-context
learning. (ii) The research line of LLM agent has
shown impressive performance but relies on mul-
tiple calls to the LLM for each sample (Khattab
et al., 2022; Yao et al., 2023), where the LLM
plays as an agent to flexibly call the retriever multi-
ple times, reads the context in earlier hops, and
generates follow-up questions. Different from
these studies, our motivation is to enhance the one-
turn retriever-then-read framework with a trainable
query rewriter. (iii) Using a web search engine as
the retriever also leads to some limitations. Neu-
ral dense retrievers that are based on professional,
filtered knowledge bases may potentially achieve
better and controllable retrieval. More discussion
is included in the appendix.
References
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-
liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei
Ji, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan
Xu, and Pascale Fung. 2023. A multitask, multilin-
gual, multimodal evaluation of chatgpt on reason-
ing, hallucination, and interactivity. arXiv preprint
arXiv:2302.04023.
Parishad BehnamGhader, Santiago Miret, and Siva
Reddy. 2022. Can retriever-augmented language
models reason? the blame game between the re-
triever and the language model. arXiv preprint
arXiv:2212.09146.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading Wikipedia to answer open-
domain questions. In Association for Computational
Linguistics (ACL).
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Pondé de Oliveira Pinto, Jared Kaplan,
Harrison Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021. Evaluat-
ing large language models trained on code. CoRR,
abs/2107.03374.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of



Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 3):

constructing the search query.
3 Methodology
We present Rewrite-Retrieve-Read, a pipeline that
improves the retrieval-augmented LLM from the
perspective of query rewriting. Figure 1 shows an
overview. This section first introduces the pipeline
framework in section 3.1, then the trainable scheme
in section 3.2.
3.1 Rewrite-Retrieve-Read
A task with retrieval augmentation can be de-
noted as follows. Given a dataset of a knowledge-
intensive task (e.g., open-domain QA), D =
{(x, y)i}, i= 0, 1, 2, . . . , N, x (e.g., a question)
is the input to the pipeline, y is the expected output
(e.g., the correct answer). Our pipeline consists of
three steps. (i) Query rewrite: generate a query ˜x
for required knowledge based on the original input
x. (ii) Retrieve: search for related context,doc. (iii)
Read: comprehend the input along with contexts
[doc, x] and predict the output ˆy.
A straightforward but effective method is to ask
an LLM to rewrite queries to search for informa-
tion that is potentially needed. We use a few-shot
prompt to encourage the LLM to think, and the
output can be none, one or more queries to search.
3.2 Trainable Scheme
Besides, total reliance on a frozen LLM has shown
some drawbacks. Reasoning errors or invalid
search hinders the performance (Yao et al., 2023;
BehnamGhader et al., 2022). On the other hand,
retrieved knowledge may sometimes mislead and
compromise the language model (Mallen et al.,
2022). To better align to the frozen modules, it is
feasible to add a trainable model and adapt it by
taking the LLM reader feedback as a reward.
Based on our framework, we further propose to
utilize a trainable small language model to take
over the rewriting step, as is shown in the right
part of Figure 1. The trainable model is initial-
ized with the pre-trained T5-large (770M) (Raffel
et al., 2020), denoted astrainable rewriter, Gθ. The
rewriter is first trained on pseudo data to warm up
(§3.2.1), then continually trained by reinforcement
learning (§3.2.2).
3.2.1 Rewriter Warm-up
The task, query rewriting, is quite different from
the pre-training objective of sequence-to-sequence
generative models like T5. First, we construct a
pseudo dataset for the query rewriting task. In-
spired by recent distillation methods (Hsieh et al.,
2023; Ho et al., 2022), we prompt the LLM to
rewrite the original questions x in the training set
and collect the generated queries ˜x as pseudo la-
bels. The collected samples are then filtered: Those
that get correct predictions from the LLM reader
are selected into the warm-up dataset, denoted as
DT rain= {(x, ˜x)|ˆy = y}. The rewriter Gθ is fine-
tuned on DT rainwith the standard log-likelihood
as the training objective, denoted as
Lwarm = −
X
t
logpθ( ˆ˜xt | ˜x<t, x ). (1)
The rewriter model after warm-up shows mod-
est performance, which depends on the pseudo
data quality and rewriter capability. Highly relying
on the human-written prompt line, ˜x can be sub-
optimal. The relatively small scale of the rewriter
size is also a limitation of the performance after the
warm-up. Then we turn to reinforcement learning
to align the rewriter to the following retriever and
LLM reader.
3.2.2 Reinforcement Learning
To further fine-tune the rewriter to cater to the LLM
reader, we adopt a policy gradient reinforcement
learning framework.
Task Formulation In the context of reinforce-
ment learning, the rewriter optimization is for-
mulated as a Markov Decision Process 5-tuple
⟨S, A, P, R, γ⟩. (i) The state space S is a finite set
limited by the vocabulary and the sequence length.
(ii) The action space A is equals to the vocabulary.
(iii) The transition probability P is determined by
the policy network, which is the rewriter model
Gθ. (iv) The reward function R gives a reward
value that depends on the current state. The pol-
icy gradient is derived from rewards, used as the
training objective. (v) γ denotes the discount fac-
tor. More specifically, the rewriter Gθ after the
warm-up is the initial policy model π0. At each
step t, the action at is to generate the next token
ˆ˜xt based on the observation of the present state,
st = [x, ˆ˜x<t]. When the generation is stopped by
the End-Of-Sentence token, one episode is ended.
After finishing the retrieval and reading, a reward
is computed by evaluating the final output, i.e., a
score for the LLM reader prediction.
Policy Optimization We adopt Proximal Policy
Optimization (PPO) (Schulman et al., 2017), fol-
lowing (Ramamurthy et al., 2022). Maximization



Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 11):

diverse, explainable multi-hop question answering.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2369–2380, Brussels, Belgium. Association for Com-
putational Linguistics.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, and Yuan Cao. 2023.
ReAct: Synergizing reasoning and acting in language
models. In International Conference on Learning
Representations (ICLR).
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,
Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,
Michael Zeng, and Meng Jiang. 2023. Generate
rather than retrieve: Large language models are
strong context generators. In International Confer-
ence for Learning Representation (ICLR).
Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-
urmans, and Joseph E Gonzalez. 2023a. Tempera:
Test-time prompt editing via reinforcement learning.
In The Eleventh International Conference on Learn-
ing Representations.
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex
Smola. 2023b. Automatic chain of thought prompt-
ing in large language models. In The Eleventh In-
ternational Conference on Learning Representations
(ICLR 2023).
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B
Brown, Alec Radford, Dario Amodei, Paul Chris-
tiano, and Geoffrey Irving. 2019. Fine-tuning lan-
guage models from human preferences. arXiv
preprint arXiv:1909.08593.
A Warm-up Dataset
For the warm-up training of the tuneable rewriter,
we construct a pseudo dataset for the query rewrit-
ing task. For benchmarks that provide official train-
ing and test splits (HotpotQA and AmbigNQ), we
use the whole training set. For those that have no
official splits (PopQA and MMLU), we randomly
split the full dataset. In detail, PopQA contains 16
types of questions, thus split into 13k for training
and 714 for testing following stratified sampling.
For MMLU, each of the 4 categories is randomly
split into 80% for the training set and 20% for
the test set. Then the training sets of each bench-
mark are used to derive the pseudo dataset for the
query rewriting, i.e., DT rain = {(x, ˜x)|ˆy = y}.
We present the statistics of the splits and warm-up
dataset in Table 5.
B Setup Details
For warm-up, we train the T5-large with 3e-5 learn-
ing rate, {16, 20} batch size, for {6,8,12} epochs.
For reinforcement learning, we set the sampling
Task Training Set Warm-up Test Set
HotpotQA 90.4k 37.5k 7.4k
AmbigNQ 19.4k 8.6k 1k
PopQA 13.0k 6.0k 0.7k
Humanities 3.8k 1.5k 0.9k
STEM 2.4k 0.9k 0.6k
Other 2.6k 1.3k 0.6k
Social Science 2.4k 1.3k 0.6k
Table 5: Metrics of multiple choice QA.
steps to 5120, 10 threads, 512 steps for each. After
sampling, the policy network is trained for {2,3,4}
epochs, with learning rate as 2e-6 and batch size
as {8,16}. λf and λh are 1.0. β in Eq. 4 is dy-
namically adapted according to Ramamurthy et al.
(2022); Ziegler et al. (2019),
et = clip
KL (π∥π0) − KLtarget
KLtarget
, −0.2, 0.2

,
βt+1 = βt (1 + Kβet) ,
where KLtarget is set to 0.2, K β is set to 0.1. β0
is initialized to be 0.001. The generation strat-
egy follows the 4-beam search and returns the one
sequence. In the implementation of the BM25-
based retriever, the textboxes from searched URLs
are parsed from HTML code. We compute BM25
scores between the paragraph from each textbox
and the query following the scikit-learn package,
then keep those with higher scores until the re-
served context reaches a max length. In reinforce-
ment learning, the results of AmbigNQ are with
the BM25 method, while others use snippets as
context.
C Web Search: Tool Use
Our proposed pipeline integrates an externally built
web search engine as the retriever module. We
present more discussion on the advantages and dis-
advantages here.
The usage of external tools expands the abil-
ity boundary of language models, compensating
for the parametric knowledge, and grounding the
capabilities of language models to interact with en-
vironments (Qin et al., 2023; Schick et al., 2023).
Recent studies show a trend to leverage plug-and-
play tools like search engines to enhance language
agents (Lazaridou et al., 2022; Menick et al., 2022;
Shuster et al., 2022; Shen et al., 2023). Search
engine APIs are well-developed retrievers, saving
efforts to build and maintain another retriever, like
a Contriever. Accessible to the whole Internet, the
web search retrieves from a wide-range, up-to-date



### Claim 5/179

#### Claim Text
Innovations like restructured RAG modules [13] and rearranged RAG pipelines [14] have been introduced to tackle specific challenges.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 1):

111:2 Lyu, et al.
Additional Key Words and Phrases: Retrieval-Augmented Generation, Large Language Models, Evaluation
ACM Reference Format:
Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong
Xu, and Enhong Chen. 2018. CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented
Generation of Large Language Models. J. ACM 37, 4, Article 111 (August 2018), 31 pages. https://doi.org/
XXXXXXX.XXXXXXX
1 INTRODUCTION
Retrieval-augmented generation (RAG) is an advanced technique that leverages external knowledge
sources to enhance the text generation capabilities of large language models(LLMs). It retrieves
relevant paragraphs from a corpus based on the input, and feeds them to the LLMs along with the
input. With the help of external knowledge, LLMs can generate more accurate and credible responses
and effectively address challenges such as outdated knowledge [19], hallucinations [3, 9, 35, 62],
and lack of domain expertise [30, 46]. Therefore, RAG technology is attracting increasing attention.
Although the effectiveness of retrieval-augmented strategies has been proven through extensive
practice, their implementation still requires a significant amount of tuning. The overall performance
of the RAG system is affected by multiple factors, such as the retrieval model, construction of the
external knowledge base, and language model. Therefore, automatic evaluation of RAG systems
is crucial. Currently, there are only a few existing benchmarks for evaluating RAG performance,
as creating high-quality datasets and experimenting with them entail significant costs. These
benchmarks can be classified into two types: reference-required and reference-free evaluation.
Reference-free evaluation frameworks, such as RAGAS [13] and ARES [44], use LLM-generated
data to evaluate RAG systems on contextual relevance, faithfulness, and informativeness. These
frameworks do not depend on ground truth references, but only assess the coherence of the
generated text with the retrieved context. This approach may be unreliable if the retrieved external
information is low-quality.
Consequently, reference-required evaluations remain the predominant method for assessing RAG
systems. Existing benchmarks for reference-required evaluations, such as RGB [8] and NQ [26].
do have their limitations. First, they all rely on question answering tasks to measure the perfor-
mance of RAG systems. Question answering is not the only RAG application scenario, and an
optimization strategy that works well for question answering may not be generalized to other
scenarios. Thus, these benchmarks may not capture the full potential of RAG systems. Second, in
the experiments, current evaluations usually focus on evaluating the LLM part of the RAG pipeline,
or focus on retriever performance in the knowledge-intensive scenario [40], while ignoring the
retrieval methods in non-knowledge-intensive scenarios and external knowledge base construction.
These components are also crucial for RAG systems. Therefore, a comprehensive evaluation of the
RAG system may not be obtained using any existing benchmarks.
To evaluate the performance of RAG in different application scenarios, we need a comprehensive
benchmark that covers more than just the question-answering task. Lewis et al. [28] argue that the
core of RAG systems is their interactive way of combining LLMs with external knowledge sources.
And following [25], we can group any interaction with external knowledge sources into four basic
actions: create, read, update, and delete, which are also known as CRUD actions [48]. Therefore,
we can use the CRUD framework to classify the RAG systems’ application scenarios. As shown in
Figure 1, each CRUD category demonstrates different capabilities of the RAG system:
•In "CREATE", the system improves the input text by adding relevant information from
external sources, making creative outputs such as poetry, stories, or code.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 2):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:3
Fig. 1. We have classified the application scenarios of RAG into four primary aspects: Create, Read, Update,
and Delete. The figure provides an illustrative example for each category, showcasing the wide-ranging
potential of RAG technology.
•In "READ", the system uses external knowledge retrieval to answer questions, solve problems
in question-answering, dialogue, and reasoning, and increase understanding of the input text.
•In "UPDATE", the system fixes errors in the input text using retrieved content, correcting
spelling, grammar, or factual errors to make the text better.
•In "DELETE", the system simplifies the input by improving retrieval results, removing
unnecessary details, and doing tasks like text summarization or simplification.
To evaluate the RAG system in these four scenarios, we introduce CRUD-RAG, a comprehensive,
large-scale Chinese RAG benchmark. CRUD-RAG consists of four evaluation tasks: text continu-
ation, question answering (with single-document and multi-document questions), hallucination
modification, and open-domain multi-document summarization, which respectively correspond to
the CRUD-RAG classification of RAG application scenarios. We construct CRUD-RAG by crawling
the latest high-quality news data from major news websites in China, which aims to minimize the
likelihood of LLMs encountering these data during training. Then, we automatically create datasets
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 10):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:11
Researchers at West Virginia University in the United States 
have observed synthetic DNA at the atomic level and learned 
how to alter its structure to enhance its scissor-like functions.
Researchers at West Virginia University have successfully observed
 the structure of synthetic DNA at the atomic level and discovered
 that altering its structure can enhance its scissor-like functions. 
Their research findings were published in the Nature sub-journal, 
Communications Chemistry. The study found that these DNA 
molecules can fold into complex shapes capable of performing ...
Science and Technology Daily, Beijing, July 27 (Reporter Zhang 
Mengran) - Researchers at West Virginia University have achieved 
the observation of synthetic DNA at the atomic level, gaining 
insights into how to alter its structure to enhance its scissor-like 
functions. Understanding these synthetic DNA reactions in greater 
detail could be key to unlocking new medical technologies in the 
future. The research findings were published in the recent issue of 
the Nature sub-journal, Communications Chemistry. Atomic-level ...
The synthetic DNA used in this study, also known as DNAzymes, 
differs from human DNA in that it is created in a laboratory, has low 
production costs, and can catalyze chemical reactions. Researchers 
noted that DNA is typically regarded as inert when it comes to 
storing human genetic information; however, some types of DNA 
evolved in the laboratory defy traditional rules. These DNA ...
Fig. 4. The dataset construction pipeline for text continuation and multi-document summarization task.
our news corpus, removing the 10,000 articles 𝑑 simultaneously. The new news corpus 𝐷−𝑑+𝐸
serves as our retrieval corpus, and we expect the model to use the events and relevant information
from the retrieval corpus to generate a summary of the articles 𝑑.
3.3 Text continuation: RAG Application in "Create"
RAG is useful not only for "Delete", where it retrieves and summarizes key information from massive
texts, but also for "Create". In this scenario, RAG systems show strong creativity by expanding
existing texts, and we take the text continuation task as an evaluation. The text continuation task
aims to automatically produce coherent and relevant subsequent content based on the beginning
of the text, making the text more complete and vivid.
To construct the continuation task dataset, we follow the same method as the summary task
dataset. Figure 4 shows the construction process of text continuation. Specifically, we select a news
article from a high-quality corpus and use a specialized Chinese word segmentation tool, to split it
into sentences. Then, we divide the article into two equal parts: the first half serves as the input,
and the second half as the output of the continuation dataset. We expect the model to use RAG
technology to retrieve relevant information from the document library and generate a continuation
that is coherent, informative, and consistent with the input and output.
To ensure that the retrieval database covers the real continuation text, we use the Baidu search
engine to find external documents and add them to the database. The continuation text differs from
the event text in that it consists of multiple sentences. Therefore, we split the continuation text
into paragraphs by sentences and retrieve relevant documents for each paragraph using the search
engine. This way, we guarantee that the retrieval database contains most of the information to
reconstruct the continuation text.
3.4 Question Answering: RAG Application in "Read"
Another application scenario of RAG is to use external knowledge bases to enhance the question-
answering capabilities of LLMs, which can be applied to various knowledge-intensive tasks. Cur-
rently, there are many evaluation benchmarks to measure the performance of RAG in this scenario,
and multiple question answering datasets have been created.
However, the existing question answering datasets also have some limitations. On the one hand,
some datasets (such as NQ and WEBQA) are outdated, and may have been covered by LLMs in
the pre-training stage, which reduces the advantage of RAG systems. On the other hand, some
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 6):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:7
2.3 Citation-Enhanced RAG
In traditional RAG methods, despite the rich information sources provided by retrieved contexts for
text generation, these models often do not explicitly require responses to provide corresponding
citations, making traceability difficult. Therefore, enhancing text verifiability by introducing citation
links, i.e., explicit references, has become an important research direction in the RAG field [17, 54].
Providing citation indicators in the response text offers several clear benefits. First, users can
easily verify the claims made by LLMs based on the provided citations, thus improving the trans-
parency and credibility of the text. Second, if the text generated by LLMs adheres faithfully to the
cited contexts, it can significantly improve its accuracy and reduce the phenomenon of "halluci-
nations" [17]. Given this, generating high-quality citations and evaluating the quality of citation
generation have become crucial elements of assessing RAG performance. Constructing appropriate
prompts directly through the retrieval context to guide the model in generating corresponding
citations constitutes a direct and effective method of citation generation [22].
In terms of evaluation, early research primarily focused on the fluency, accuracy, and basic
citation quality of the text generated by LLMs [17, 29]. For example, Rashkin et al. proposed the
"Attributable to Identified Sources" (AIS) score [42], which serves as a valuable tool for measuring the
degree to which generated text is faithful to its sources. As research progressed, scholars recognized
the need for more detailed evaluation methods to differentiate between various levels of citation
support. By creating specialized datasets such as SCIFI [7], researchers can more precisely evaluate
fine-grained citations at the clause level in texts generated by LLMs. The ALiiCE framework [54], by
analyzing the atomic structure of sentence claims, introduced fine-grained evaluation metrics, such
as location citation recall and precision, and the coefficient of variation of citation locations, to more
granularly evaluate the quality of citation generation in RAG [54]. In practical applications, [61]
found that RAG requires more complex evaluation frameworks to distinguish between various
levels of citation support by comparing different fidelity metrics. These RAG evaluation methods
not only consider the presence of citations but also their accuracy and relevance.
While Citation-Enhanced RAG delves deeply into the specific domain of citation generation,
aiming to improve the credibility and accuracy of text generated by RAG systems, our benchmark
provides a comprehensive evaluation framework encompassing various aspects of RAG systems
and multiple application scenarios.
3 CRUD-RAG: A COMPREHENSIVE CHINESE BENCHMARK FOR RAG
As we discussed earlier, implementing RAG effectively requires careful tuning of multiple com-
ponents, such as the retrieval model, the knowledge corpus, the language model, and the query
formulation. Therefore, we need a framework that can evaluate the RAG system automatically. This
framework would enable us to examine how these components affect the system’s performance,
and provide us with useful insights for improving and innovating the system.
However, The current RAG benchmarks have several drawbacks: they only evaluate question
answering tasks [1, 41, 57], ignoring other diverse application of RAG. The optimization strategy
for question-answering tasks may not suit other tasks; And in the evaluation experiment, current
RAG benchmarks only account for the LLM component in the RAG pipeline, or the retriever in the
knowledge-intensive scenario, disregarding the vital roles of retrieval database construction and
retrieval in non-knowledge-intensive scenarios.
To address the shortcomings of previous benchmarks, we introduce CRUD-RAG, a comprehensive
Chinese benchmark for RAG. Figure 2 illustrates the features of our CRUD-RAG benchmark. It
classifies the RAG application scenarios into four categories: Create, Read, Update, and Delete,
then we construct appropriate evaluation tasks and datasets for each category. Besides, in the
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 5):

111:6 Lyu, et al.
2.2 RAG Benchmarks
When investigating the development and optimization of RAG, the effective evaluation of their
performance becomes a fundamental concern. Table 1 shows some commonly used benchmarks
for evaluating RAG. LangChain provides benchmark tasks, such as LangChain Docs Q&A and
Semi-structured Reports [27], designed to assess various RAG architectures. These datasets are
constructed from snapshots of Python documentation and PDFs containing tables and charts.
They emphasize the model’s capability to handle structured and semi-structured data. Evaluation
standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing
large models for question-answering generation has emerged as a prevalent approach in building
evaluation datasets. For instance, RGB [8] creates its evaluation dataset by gathering recent news
reports and employing LLM to generate relevant events, questions, and answers. Conversely,
ARES [44]. relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
These methods not only showcase the RAG system’s proficiency in handling real-time data but also
illustrate the utility of automation and synthetic data in the evaluation process. For evaluating the
capabilities of models across various professional domains, the Instruct-Benchmark-Tester dataset
encompasses a range of question types, with a particular focus on financial services, legal, and
intricate business scenarios [39].
Depending on whether the evaluation phase incorporates ground truth, metrics of existing
evaluation methods can be categorized into those necessitating reference and those not requiring
it. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by
contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-
Benchmark-Tester [39] employs accuracy score as an evaluation metric, a widely acknowledged
measure of model performance that assesses the extent to which model-generated answers align
with reference answers. The primary objective of RGB [8] is to evaluate whether large models can
effectively utilize external documents to acquire knowledge and generate accurate answers. Its
evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.
Reference-free evaluation methods, including TruLens-Eval [14], RAGAS [13], and ARES [44],
provide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning
context relevance, answer faithfulness, and answer relevance. TruLens-Eval [14] introduces the
RAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,
encompassing context relevance, groundedness, and answer relevance. RAGAS [13], serving as
a reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity
to identify pertinent and concentrated context passages, along with the LLMs’ proficiency in
faithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a
predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each
aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy
when compared to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-
powered inference to offer statistical assurances for its scoring, generating confidence intervals.
ARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer
relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts
and producing both faithful and relevant answers. Regarding evaluation methods, [32] places an
emphasis on assessing the credibility and accuracy of responses generated by generative search
engines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including
high costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,
exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely
approximates manual evaluation outcomes.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[164]_2309.15217.pdf (Page 0):

RAGAS: Automated Evaluation of Retrieval Augmented Generation
Shahul Es†, Jithin James†, Luis Espinosa-Anke∗♢, Steven Schockaert∗
†Exploding Gradients
∗CardiffNLP, Cardiff University, United Kingdom
♢AMPLYFI, United Kingdom
shahules786@gmail.com,jamesjithin97@gmail.com
{espinosa-ankel,schockaerts1}@cardiff.ac.uk
Abstract
We introduce RAGA S (Retrieval Augmented
Generation Assessment), a framework for
reference-free evaluation of Retrieval Aug-
mented Generation (RAG) pipelines. RAG
systems are composed of a retrieval and an
LLM based generation module, and provide
LLMs with knowledge from a reference textual
database, which enables them to act as a natu-
ral language layer between a user and textual
databases, reducing the risk of hallucinations.
Evaluating RAG architectures is, however, chal-
lenging because there are several dimensions to
consider: the ability of the retrieval system to
identify relevant and focused context passages,
the ability of the LLM to exploit such passages
in a faithful way, or the quality of the gener-
ation itself. With RAGA S, we put forward a
suite of metrics which can be used to evaluate
these different dimensions without having to
rely on ground truth human annotations. We
posit that such a framework can crucially con-
tribute to faster evaluation cycles of RAG archi-
tectures, which is especially important given
the fast adoption of LLMs.
1 Introduction
Language Models (LMs) capture a vast amount
of knowledge about the world, which allows them
to answer questions without accessing any exter-
nal sources. This idea of LMs as repositories of
knowledge emerged shortly after the introduction
of BERT (Devlin et al., 2019) and became more
firmly established with the introduction of ever
larger LMs (Roberts et al., 2020). While the most
recent Large Language Models (LLMs) capture
enough knowledge to rival human performance
across a wide variety of question answering bench-
marks (Bubeck et al., 2023), the idea of using
LLMs as knowledge bases still has two fundamen-
tal limitations. First, LLMs are not able to answer
questions about events that have happened after
they were trained. Second, even the largest models
struggle to memorise knowledge that is only rarely
mentioned in the training corpus (Kandpal et al.,
2022; Mallen et al., 2023). The standard solution
to these issues is to rely on Retrieval Augmented
Generation (RAG) (Lee et al., 2019; Lewis et al.,
2020; Guu et al., 2020). Answering a question
then essentially involves retrieving relevant pas-
sages from a corpus and feeding these passages,
along with the original question, to the LM. While
initial approaches relied on specialised LMs for
retrieval-augmented language modelling (Khandel-
wal et al., 2020; Borgeaud et al., 2022), recent work
has suggested that simply adding retrieved docu-
ments to the input of a standard LM can also work
well (Khattab et al., 2022; Ram et al., 2023; Shi
et al., 2023), thus making it possible to use retrieval-
augmented strategies in combination with LLMs
that are only available through APIs.
While the usefulness of retrieval-augmented
strategies is clear, their implementation requires
a significant amount of tuning, as the overall per-
formance will be affected by the retrieval model,
the considered corpus, the LM, or the prompt for-
mulation, among others. Automated evaluation of
retrieval-augmented systems is thus paramount. In
practice, RAG systems are often evaluated in terms
of the language modelling task itself, i.e. by mea-
suring perplexity on some reference corpus. How-
ever, such evaluations are not always predictive
of downstream performance (Wang et al., 2023c).
Moreover, this evaluation strategy relies on the LM
probabilities, which are not accessible for some
closed models (e.g. ChatGPT and GPT-4). Ques-
tion answering is another common evaluation task,
but usually only datasets with short extractive an-
swers are considered, which may not be represen-
tative of how the system will be used.
To address these issues, in this paper we present
RAGA S1, a framework for the automated assess-
1RAGA S is available at https://github.com/
explodinggradients/ragas.
arXiv:2309.15217v1  [cs.CL]  26 Sep 2023



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 10):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:11
Researchers at West Virginia University in the United States 
have observed synthetic DNA at the atomic level and learned 
how to alter its structure to enhance its scissor-like functions.
Researchers at West Virginia University have successfully observed
 the structure of synthetic DNA at the atomic level and discovered
 that altering its structure can enhance its scissor-like functions. 
Their research findings were published in the Nature sub-journal, 
Communications Chemistry. The study found that these DNA 
molecules can fold into complex shapes capable of performing ...
Science and Technology Daily, Beijing, July 27 (Reporter Zhang 
Mengran) - Researchers at West Virginia University have achieved 
the observation of synthetic DNA at the atomic level, gaining 
insights into how to alter its structure to enhance its scissor-like 
functions. Understanding these synthetic DNA reactions in greater 
detail could be key to unlocking new medical technologies in the 
future. The research findings were published in the recent issue of 
the Nature sub-journal, Communications Chemistry. Atomic-level ...
The synthetic DNA used in this study, also known as DNAzymes, 
differs from human DNA in that it is created in a laboratory, has low 
production costs, and can catalyze chemical reactions. Researchers 
noted that DNA is typically regarded as inert when it comes to 
storing human genetic information; however, some types of DNA 
evolved in the laboratory defy traditional rules. These DNA ...
Fig. 4. The dataset construction pipeline for text continuation and multi-document summarization task.
our news corpus, removing the 10,000 articles 𝑑 simultaneously. The new news corpus 𝐷−𝑑+𝐸
serves as our retrieval corpus, and we expect the model to use the events and relevant information
from the retrieval corpus to generate a summary of the articles 𝑑.
3.3 Text continuation: RAG Application in "Create"
RAG is useful not only for "Delete", where it retrieves and summarizes key information from massive
texts, but also for "Create". In this scenario, RAG systems show strong creativity by expanding
existing texts, and we take the text continuation task as an evaluation. The text continuation task
aims to automatically produce coherent and relevant subsequent content based on the beginning
of the text, making the text more complete and vivid.
To construct the continuation task dataset, we follow the same method as the summary task
dataset. Figure 4 shows the construction process of text continuation. Specifically, we select a news
article from a high-quality corpus and use a specialized Chinese word segmentation tool, to split it
into sentences. Then, we divide the article into two equal parts: the first half serves as the input,
and the second half as the output of the continuation dataset. We expect the model to use RAG
technology to retrieve relevant information from the document library and generate a continuation
that is coherent, informative, and consistent with the input and output.
To ensure that the retrieval database covers the real continuation text, we use the Baidu search
engine to find external documents and add them to the database. The continuation text differs from
the event text in that it consists of multiple sentences. Therefore, we split the continuation text
into paragraphs by sentences and retrieve relevant documents for each paragraph using the search
engine. This way, we guarantee that the retrieval database contains most of the information to
reconstruct the continuation text.
3.4 Question Answering: RAG Application in "Read"
Another application scenario of RAG is to use external knowledge bases to enhance the question-
answering capabilities of LLMs, which can be applied to various knowledge-intensive tasks. Cur-
rently, there are many evaluation benchmarks to measure the performance of RAG in this scenario,
and multiple question answering datasets have been created.
However, the existing question answering datasets also have some limitations. On the one hand,
some datasets (such as NQ and WEBQA) are outdated, and may have been covered by LLMs in
the pre-training stage, which reduces the advantage of RAG systems. On the other hand, some
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 1):

RAG system finds relevant contexts and generates
answers that are both faithful and relevant.
Many existing RAG evaluation frameworks re-
quire substantial human annotations for scoring.
ARES significantly improves data efficiency dur-
ing evaluation by only requiring three inputs: an in-
domain passage set, a human preference validation
set of approximately 150 annotated datapoints or
more, and few-shot examples of in-domain queries
and answers (e.g. five examples or more), which
are used for prompting LLMs in synthetic data gen-
eration.
Given the corpus of in-domain passages, ARES
proceeds in three stages. First, it leverages an LM
to construct a synthetic dataset of question–answer
pairs, derived from the passages in the corpus. Sec-
ond, it defines three separate judge models to per-
form three classification tasks (context relevance,
answer faithfulness, and answer relevance). These
judges are lightweight models fine-tuned against a
contrastive learning objective. Third, ARES scores
the different RAG systems being assessed using
prediction-powered inference (PPI; Angelopoulos
et al. 2023) to improve model-based evaluation ac-
curacy and provide statistical confidence intervals
for RAG scoring. PPI utilizes a small set of human
annotated datapoints for computing its confidence
intervals; we designate this annotated set as our hu-
man preference validation set, which is composed
of approximately 150 annotated datapoints or more
that designate both positive and negative examples
for context relevance, answer faithfulness, and an-
swer relevance.
We conduct extensive empirical evaluations,
demonstrating that ARES accurately scores
RAG systems across the six knowledge-intensive
datasets in KILT and SuperGLUE, beating exist-
ing automated evaluation approaches like RAGAS
by 59.3 and 14.4 percentage points on average
across context relevance and answer relevance eval-
uation accuracy, respectively. Additionally, ARES
accurately calculates answer hallucination occur-
rences in the AIS attribution dataset (Rashkin et al.,
2022), predicting within 2.5 percentage points of
the ground truth average for answer hallucinations.
Compared to annotation-based evaluation methods,
ARES is substantially more accurate and efficient,
requiring 78% less annotations than the baseline
approach. We also find that ARES consistently
distinguishes competitive RAG systems that are
only a few points apart in ground-truth metrics.
This precision enables ARES to guide the develop-
ment and comparison of competitive approaches
and configurations.
We make the ARES code and datasets publicly
available on Github.
2 Related Work
RAG (Guu et al., 2020; Lewis et al., 2020; Khat-
tab et al., 2021; Izacard et al., 2022)) is now a
common strategy for bolstering LLMs by combin-
ing them with retrieval systems. Through retrieval,
RAG helps LM systems gather domain-specific
knowledge, ground generations in factual informa-
tion (Shuster et al., 2021; Huo et al., 2023), and
offer a degree of transparency or interpretability
via citing sources (Mialon et al., 2023).
Multiple LLM-based evaluation techniques have
emerged for gauging LLM systems. This is essen-
tial for rapid deployment in new settings, where it
is difficult to build a traditional benchmark dataset
from scratch. Early attempts at this use LLMs
out of the box, as in MT-Bench and Chatbot
Arena (Zheng et al., 2023). AutoCalibrate (Liu
et al., 2023b) seeks to align an LLM-judge with
human preferences, leveraging a self-refinement
prompt to iteratively improve the LLM judge. How-
ever, AutoCalibrate does not offer any statistical
guarantees for the accuracy of its predictions. Other
work has used LLM prompting to evaluate system
quality across natural language generation tasks,
such as translation, summarization, and dialogue
(Kocmi and Federmann, 2023; Fu et al., 2023; Liu
et al., 2023a; Wang et al., 2023).
In the context of knowledge-intensive NLP tasks,
LLMs have been explored for assessing attribution
and factuality in LLMs (Min et al., 2023; Gekhman
et al., 2023; Yue et al., 2023). New guidelines
like LongEval (Krishna et al., 2023) and datasets
like Hagrid and ALCE (Kamalloo et al., 2023;
Gao et al., 2023) provide resources for analyzing
knowledge-intensive LLM pipelines.
The two most-closely related projects to ARES
are EXAM (Sander and Dietz, 2021) and RA-
GAS (James and Es, 2023). To evaluate RAG sys-
tems, the EXAM metric estimates how many exam
questions a reader (simulated as a QA system) can
answer correctly based on the generated response.
This requires a set of queries with several asso-
ciated sub-questions each, which adds a burden
that ARES does not bring. RAGAS is based on a
handful of heuristic hand-written prompts. These
offer little adaptability to new RAG evaluation set-



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 0):

ARES: An Automated Evaluation Framework for Retrieval-Augmented
Generation Systems
Jon Saad-Falcon
Stanford University ∗
jonsaadfalcon@stanford.edu
Omar Khattab
Stanford University
okhattab@stanford.edu
Christopher Potts
Stanford University
cgpotts@stanford.edu
Matei Zaharia
Databricks and UC Berkeley
matei@databricks.com
Abstract
Evaluating retrieval-augmented generation
(RAG) systems traditionally relies on hand
annotations for input queries, passages to re-
trieve, and responses to generate. We intro-
duce ARES, an Automated RAG Evaluation
System, for evaluating RAG systems along
the dimensions of context relevance, answer
faithfulness, and answer relevance. By cre-
ating its own synthetic training data, ARES
finetunes lightweight LM judges to assess the
quality of individual RAG components. To
mitigate potential prediction errors, ARES uti-
lizes a small set of human-annotated datapoints
for prediction-powered inference (PPI). Across
eight different knowledge-intensive tasks in
KILT, SuperGLUE, and AIS, ARES accurately
evaluates RAG systems while using only a few
hundred human annotations during evaluation.
Furthermore, ARES judges remain effective
across domain shifts, proving accurate even
after changing the type of queries and/or docu-
ments used in the evaluated RAG systems. We
make our code and datasets publicly available
on Github.
1 Introduction
Retrieval-augmented generation (RAG) has be-
come a prominent approach for building user-
facing NLP applications, such as systems for ques-
tion answering (QA), fact-checking, and customer
support (Petroni et al., 2021; Wang et al., 2019).
Typically, a RAG system consists of a retriever and
a downstream language model (LM). Given a user
question, the retriever finds relevant passages from
a corpus and the LM uses these passages to gener-
ate a response. This formulation admits a multitude
of choices: what retrieval model to use, how to di-
vide the documents into retrieval chunks, and how
to prompt or finetune the LM to use the retrieved
information, to name only a few of the simplest
design decisions.
∗Project started during research internship at Databricks
The best design for a RAG system is not neces-
sarily universal across data domains, corpus sizes,
and cost/latency budgets. To tune their own RAG
systems, practitioners traditionally need hand an-
notations for test questions, passages to retrieve
(to assess the retriever), and responses to generate,
labeled specifically for their target domain. Alter-
natively, they may evaluate different approaches in
production by collecting human preferences that
compare the candidate systems. Unfortunately,
both of these strategies demand high expertise and
impose considerable annotation costs.
Model-based evaluation is an inexpensive strat-
egy to test generative output quality (Zheng et al.,
2023). For instance, the open-source RAGAS
framework (James and Es, 2023) prompts an LM
for evaluating the relevance of retrieved informa-
tion and the faithfulness and accuracy of generated
responses. Unfortunately, such strategies currently
rely for evaluation on a fixed set of heuristically
hand-written prompts, offering little adaptability
to various evaluation contexts and no guarantees
about quality.
To evaluate RAG systems rapidly and accu-
rately, we propose ARES, the Automated RAG
Evaluation System. ARES is the first automated
RAG evaluation system to generate tailored LLM
judges for each component of a RAG pipeline, lead-
ing to substantial boosts in evaluation precision and
accuracy compared to existing approaches like RA-
GAS. Furthermore, unlike existing RAG evaluation
systems, ARES provides confidence intervals for
its scoring by leveraging prediction-powered in-
ference (PPI; Angelopoulos et al. 2023). Given a
corpus of documents and a RAG system, ARES
reports three evaluation scores: context relevance
(is the retrieved information pertinent to the test
question), answer faithfulness (is the response gen-
erated by the language model properly grounded
in the retrieved context), and answer relevance (is
the response also relevant to the question). A good
arXiv:2311.09476v2  [cs.CL]  31 Mar 2024



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 2):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:3
Fig. 1. We have classified the application scenarios of RAG into four primary aspects: Create, Read, Update,
and Delete. The figure provides an illustrative example for each category, showcasing the wide-ranging
potential of RAG technology.
•In "READ", the system uses external knowledge retrieval to answer questions, solve problems
in question-answering, dialogue, and reasoning, and increase understanding of the input text.
•In "UPDATE", the system fixes errors in the input text using retrieved content, correcting
spelling, grammar, or factual errors to make the text better.
•In "DELETE", the system simplifies the input by improving retrieval results, removing
unnecessary details, and doing tasks like text summarization or simplification.
To evaluate the RAG system in these four scenarios, we introduce CRUD-RAG, a comprehensive,
large-scale Chinese RAG benchmark. CRUD-RAG consists of four evaluation tasks: text continu-
ation, question answering (with single-document and multi-document questions), hallucination
modification, and open-domain multi-document summarization, which respectively correspond to
the CRUD-RAG classification of RAG application scenarios. We construct CRUD-RAG by crawling
the latest high-quality news data from major news websites in China, which aims to minimize the
likelihood of LLMs encountering these data during training. Then, we automatically create datasets
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



### Claim 6/179

#### Claim Text
The Search module adapts to specific scenarios, enabling direct searches across various data sources like search engines, databases, and knowledge graphs, using LLM-generated code and query languages [15].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 12):

knowledge base. The temporal misalignment prob-
lem on a fixed candidate database can be alleviated.
On the other hand, web search APIs are commer-
cial products requiring subscriptions. Also, the vast
amount of knowledge on the web can be difficult
to control. The retrieved context from the Internet
can be occasionally inconsistent, redundant, and
toxic, which hinders the LLM reader.
Beyond retrieval augmentation, in a general
scope, other tools called by LLMs, like code in-
terpreters, online models, and expert applications,
are all similar to search engines, without trainable
parameters to optimize. There could be a gap be-
tween the LM and these tools. This paper proposes
an idea to align them through a trainable small
model.



Source: data\tc16_2312.10997v5\referenced_papers\[68]_2310.16568.pdf (Page 8):

Query (Q) and Generated Search Path (SP) Comment
Correctly attributed passages and answers
Q: how many episodes of greys anatomy season 14 Query entity resolved first,
SP: Grey’s Anatomy (season 14) » season consists of 24 episodes » 24 followed by query predicates
Q: when did they start adding zinc to pennies Query entity resolved
SP: Penny (United States coin) » zinc » Lincoln cent » 1943 iteratively
Q: who was executed for being an american spy during the revolutionary war Answer generated firstSP: Nathan Hale » Army during the American Revolutionary » Nathan Hale
Q: who was the grandfather on the cosby show Query rewritesSP: appeared on "The Cosby » Earle Hyman
Incorrect Passage or Answer
Q: who decides the number of judges in the high court A: President of India Path correctly resolved,
SP: judge is appointed » High Court » Chief Justice of India Failed on answer
Q: when did the isle of wight become an island A: During the last Ice Age Query entity resolved,
Isle of Wight » 1890 » 1890 Failed on supporting keywords
Q: love yourself by justin bieber is about who A: Rihana Failed to resolve
SP: Love Yourself: Her » music video » Her query entity
Table 5: Example 1P Search Paths (SP) on Open-NQ test set. The last keyword in SP is the predicted answer. Gold
answers are indicated by A.
Search Path Hits@1 EM
Heuristic 34.5 22.6
LM-scored 40.0 27.2
Title » LM-scored 41.9 28.0
Title » LM-scored + 42.9 28.7LM-scored (7+1)
Table 6: Comparison of Training Search Paths on Open-
NQ. Here LM- scored denotes re-scoring by LM on a
heuristic set. All results are with a beam of one. "»"
indicates keyword separator and "+" mixture of path
types in the give ratio.
Constrained decoding also comes with its own
challenges. Constrained beam outputs often lack
diversity, so that even with a larger beam one may
still end up in poor search spaces. Computing
document-level constraints across the corpus is ex-
pensive as it may require scanning a large num-
ber of rows in the index. Further, communication
between FM-Index and Transformer model slows
down inference.
Acknowledgement
We thank Don Metzler, Nicholas FitzGerald, Partha
Talukdar, Srini Narayanan, as well as our anony-
mous reviewers, for their thoughful comments and
valuable feedback
Ethical Considerations
While Large Language Models can solve a wide
range of tasks effectively, they also suffer from bi-
ases across axis such as gender, race, region (Chan,
2023). LLMs are also prone to generating toxic
content, especially when probed about it. Although,
our task grounds the model’s generations on a cor-
pus, some of the biases in pre-trained LLMs, may
seep in 1-PAGER .
Building the FM-index and constrained decod-
ing is a compute-intensive affair. We have exper-
imented over a single dataset, Natural Questions,
involving only knowledge-seeking queries, and sin-
gle model family, T5. It is possible that some of our
findings may not hold over other datasets or model
families. Finally, our experiments are limited to En-
glish corpus and queries. The proposed approaches
are resource-intensive and may not be accessible
or valid for several low-resourced languages.
References
Leonard Adolphs, Benjamin Boerschinger, Christian
Buck, Michelle Chen Huebscher, Massimiliano Cia-
ramita, Lasse Espeholt, Thomas Hofmann, Yannic
Kilcher, Sascha Rothe, Pier Giuseppe Sessa, et al.
2021. Boosting search engines with interactive
agents. arXiv preprint arXiv:2109.00527.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak



Source: data\tc16_2312.10997v5\referenced_papers\[9]_2311.03758.pdf (Page 0):

Large Language Model based Long-tail Query Rewriting in
Taobao Search
Wenjun Peng∗
pengwj@mail.ustc.edu.cn
University of Science and Technology
of China & State Key Laboratory of
Cognitive Intelligence
Hefei, Anhui, China
Guiyang Li
liguiyang.lgy@taobao.com
Taobao and Tmall Group
Hangzhou, Zhejiang, China
Yue Jiang
jy270069@alibaba-inc.com
Taobao and Tmall Group
Hangzhou, Zhejiang, China
Zilong Wang
huanshi.wzl@taobao.com
Taobao and Tmall Group
Hangzhou, Zhejiang, China
Dan Ou†
oudan.od@taobao.com
Taobao and Tmall Group
Hangzhou, Zhejiang, China
Xiaoyi Zeng
yuanhan@taobao.com
Taobao and Tmall Group
Hangzhou, Zhejiang, China
Derong Xu
derongxu@mail.ustc.edu.cn
University of Science and Technology
of China & State Key Laboratory of
Cognitive Intelligence
Hefei, Anhui, China
Tong Xu‡
tongxu@ustc.edu.cn
University of Science and Technology
of China & State Key Laboratory of
Cognitive Intelligence
Hefei, Anhui, China
Enhong Chen
cheneh@ustc.edu.cn
University of Science and Technology
of China & State Key Laboratory of
Cognitive Intelligence
Hefei, Anhui, China
ABSTRACT
In the realm of e-commerce search, the significance of semantic
matching cannot be overstated, as it directly impacts both user
experience and company revenue. Along this line, query rewrit-
ing, serving as an important technique to bridge the semantic gaps
inherent in the semantic matching process, has attached wide at-
tention from the industry and academia. However, existing query
rewriting methods often struggle to effectively optimize long-tail
queries and alleviate the phenomenon of “few-recall” caused by
semantic gap. In this paper, we present BEQUE, a comprehensive
framework that Bridges the sEmantic gap for long-tail QUEries. In
detail, BEQUE comprises three stages: multi-instruction supervised
fine tuning (SFT), offline feedback, and objective alignment. We
first construct a rewriting dataset based on rejection sampling and
auxiliary tasks mixing to fine-tune our large language model (LLM)
in a supervised fashion. Subsequently, with the well-trained LLM,
we employ beam search to generate multiple candidate rewrites,
and feed them into Taobao offline system to obtain the partial order.
Leveraging the partial order of rewrites, we introduce a contrastive
learning method to highlight the distinctions between rewrites, and
∗This work was done when the first author was an intern at Taobao Main Search.
†Corresponding author 1
‡Corresponding author 2
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
WWW ’24 Companion, May 13–17, 2024, Singapore, Singapore.
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0172-6/24/05. . . $15.00
https://doi.org/10.1145/3589335.3648298
align the model with the Taobao online objectives. Offline experi-
ments prove the effectiveness of our method in bridging semantic
gap. Online A/B tests reveal that our method can significantly boost
gross merchandise volume (GMV), number of transaction (#Trans)
and unique visitor (UV) for long-tail queries. BEQUE has been de-
ployed on Taobao, one of most popular online shopping platforms
in China, since October 2023.
CCS CONCEPTS
• Information systems →Query reformulation; • Computing
methodologies →Natural language processing .
KEYWORDS
Query reformulation; large language models; semantic matching
ACM Reference Format:
Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng,
Derong Xu, Tong Xu, and Enhong Chen. 2024. Large Language Model based
Long-tail Query Rewriting in Taobao Search. In Companion Proceedings of
the ACM Web Conference 2024 (WWW ’24 Companion), May 13–17, 2024,
Singapore, Singapore. ACM, New York, NY, USA, 9 pages. https://doi.org/10.
1145/3589335.3648298
1 INTRODUCTION
Past decades have witnessed the exceptionally rapid growth of
e-commerce platforms. Leading e-commerce companies, such as
Taobao, JD and Amazon, have amassed hundreds of millions of
users, generating billions of gross merchandise volume (GMV) an-
nually. To facilitate the quick retrieval of related products for these
users, a well-established search paradigm has been proposed, as
illustrated in Figure 1, Specifically, this paradigm involves several
steps, i.e., “semantic understanding - retrieval - rank” . Among them,
semantic understanding serves as the foundation of entire system,
arXiv:2311.03758v3  [cs.IR]  4 Mar 2024



Source: data\tc16_2312.10997v5\referenced_papers\[15]_2308.11761.pdf (Page 3):

Method Ent Desc Triples Ent Aspect Info Multi-Hop Search Zero-Shot Ent Linking External KBs Private KB
Toolformer ✓ ✗ ✗ ✓ ✗ ✗ ✓ ✗
ToolkenGPT ✗ ✓ ✗ ✓ ✗ ✗ ✓ ✗
Graph-Toolformer ✗ ✓ ✗ ✗ ✗ ✗ ✓ ✗
RET-LLM ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✓
LangChain KG Memory✗ ✓ ✗ ✗ ✓ ✓ ✗ ✓
Llama-Index KG Index✗ ✓ ✗ ✗ ✓ ✓ ✗ ✓
KnowledGPT ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Table 1: Comparison between KnowledGPT and existing KB-augmented methods. Ent, Rel, and Desc are
abbreviations for entity, relation and description, respectively.
3 Methods
In this section, we introduce KnowledGPT, a com-
prehensive framework to integrate LLMs with KBs.
We first provide the definition of two tasks of
KnowledGPT, knowledge retrieval and knowledge
storage (Sec 3.1). Then, we elaborate the details in
the retrieval (Sec 3.2) and storage (Sec 3.3) process
of KnowledGPT.
3.1 Task Definition
KnowledGPT supplements LLMs with external
knowledge from various knowledge bases (KBs),
including a personalized KB (PKB) as an writable
symbolic memory. Given a user input in natural
language, KnowledGPT undertakes two primary
tasks, namely knowledge retrieval and knowledge
storage. In the knowledge retrieval task, the model
searches through provided KBs to retrieve relevant
knowledge to answer the user query. In the knowl-
edge storage task, the model extracts knowledge
from the user input and inserts it into the PKB.
3.2 Knowledge Retrieval
KnowledGPT follows a three-step process to an-
swer user queries with knowledge from KBs, as
shown in Fig 3. First, it generates a piece of search
code as a logical form for query-specific KB ac-
cess. Then, the search code is executed to retrieve
related knowledge. Finally, KnowledGPT reads the
retrieved knowledge and answers the query.
We utilize the program of thought (PoT)prompt-
ing approach (Chen et al., 2022) , which adopts
Python code as the search language generated by
LLMs. In this paper, we useGPT-4 (OpenAI, 2023)
as the LLM. The code is encapsulated in a search
function, as is shown in the yellow part of Fig 3 ,
which includes built-in Python functions and three
custom KB functions designed to facilitate the in-
teraction of LLMs with KBs:
1. get_entity_info, which accepts an entity as
input and returns its encyclopedic description.
2. find_entity_or_value, which accepts a query
consisting of an entity and a relation as input,
and outputs a list of the corresponding entity
or value.
3. find_relationship, which accepts two entities
as input, and returns a list of their relationship.
Specially, each entity or relation is represented as a
list of candidate aliases, rather than a single name,
to effectively handle synonyms. Besides the out-
puts stated above, these KB functions also return
a message which logs the function call and result.
Then, the overall output of the search function is
obtained by concatenating the messages from indi-
vidual KB function calls. The prompt is shown in
7.
The search function is then executed to retrieve
the expected knowledge from KBs. The code
would be decorated before execution, e.g. with
a try-except statement and KB-specific accessor
object, as is elaborated in Sec 3.2.1. The search
function is executed for each KB respectively in
parallel, and their results are concatenated.
Finally, the retrieved knowledge is provided to
LLMs, and LLMs are tasked with responding to the
user’s query, supported by the retrieved knowledge.
The prompt is shown in Sec 7. LLMs will ignore
the retrieved information and address the user query
independently in scenarios where LLMs judge the
question does not require external knowledge or the
retrieved knowledge is not enough for the query.
3.2.1 Code Implementation
Next, we introduce the implementation of the KB
functions to execute the generated code. We im-
plement the functions at two levels: a unified level
and a KB-specific level.
Functions at the unified level provide a unified
interface for operations over different KBs. These
include the three KB functions ( get_entity_info,
find_entity_or_value, find_relationship) generated



Source: data\tc16_2312.10997v5\referenced_papers\[15]_2308.11761.pdf (Page 2):

Complex Question
Please recommend three novels written by 
the author of 'Lord of the Mysteries’.
Previous Methods like RET-LLM
one step only
RETRIEVE(‘LotM’, ‘novels by the author of’, )
KnowledGPT
multi-hop searching &  processing via code
CODE:
author = RETRIEVE([‘LotM’], ['written by’, 
‘author’])
novels = RETRIEVE(author, [‘write’])
novels = SAMPLE(novels, min(3, len(novels))
Ambiguation
Who directed the film Titanic?
LLM-based Entity Linking
Knowledge Representations
U.S. Bancorp is an American bank holding 
company based in Minneapolis, Minnesota… 
embedding similarity
cosine(Titanic, Titanic (a ship) ) = 0.95
cosine(Titanic, Titanic (1997 film) ) = 0.92
Link to the entity Titanic (a ship) ❎
CANDIDATE ENTS: 
Titanic (a ship): a famous ship that sank in 1912
Titanic (1997 film): a 1997 film directed by 
James Cameron …
LLM CHOICE: 
Titanic (1997 film) seems related with the film 
Titanic . Hence, I choose Titanic (1997 film). 
Extract triples:
(‘U.S. Bancorp’, ‘based in’, ‘Minnesota’)
‘U.S. Bancorp’: ‘U.S. Bancorp, a bank holding 
company based in Minneapolis, Minnesota …
(‘U.S.Bancorp’, ‘scope of business’, ‘The company 
provides banking, investment, mortgage, trust, 
and payment services products to individuals … ‘)
only triples
⚠️Link to a wrong entity 
+ entity description
⚠️generate contrived relation
✅
++ entity-aspect information
⚠️Represent a limited portion of knowledge
Figure 2: Comparison between KnowledGPT and previous methods towards several challenges in bridging LLMs
with KBs. KnowledGPT handles complex questions through multi-hop searching and processing based on code,
tackles entity ambiguation with LLM-based entity linking (middle), and provides extended forms of knowledge
representations to encapsulate a broader range of knowledge from the provided text.
2020), RAG (Lewis et al., 2020), augment LLMs
with document corpus, which have also been in-
creasingly adopted by recent popular LLMs like
ChatGPT as the memory unit (Liu, 2022) (Chase,
2022). ChatDB (Hu et al., 2023) augments LLMs
with databases as symbolic memory.
Knowledge Bases for LLMs Some recent works
have studied to augment LLMs with knowledge
from external KBs or use KBs as symbolic memo-
ries, usually by making LLMs generate API calls
for KB operations. Toolformer (Schick et al., 2023)
trains LLMs to search Wikipedia for texts of enti-
ties. Graph-Toolformer (Zhang, 2023) empowers
LLMs to reason over knowledge graphs. How-
ever, it skips the entity linking step, so it requires
entity id like /m/053yx as input, instead of their
names. ToolkenGPT (Hao et al., 2023) keeps the
LLMs frozen and trains tool embeddings for rela-
tions in KBs to support relational queries. RET-
LLM (Modarressi et al., 2023), similar to the KG
memory of LangChain (Chase, 2022) and Llama-
Index (Liu, 2022), extracts relational triples from
user inputs and store them in a symbolic KG mem-
ory. Compared with previous efforts, KnowledGPT
supports various knowledge representations and
both public and private KBs, as shown in Table 1.
Knowledge-based Question Answering
(KBQA) is to search for answer entities or
relations given natural language queries specific to
certain KGs. Existing KBQA systems are mainly
based on semantic parsing (Berant et al., 2013) or
information extraction (Yao and Van Durme, 2014),
where language models are increasingly involved.
Semantic parsing methods (Yu et al., 2023; Cao
et al., 2022; Zhang et al., 2022b; Abdelaziz et al.,
2021; Lai et al., 2016) leverage semantic parser to
convert natural language queries into intermediate
logic forms such as SPARQL (Prud’hommeaux,
2011) and program (Liang et al., 2016), which
are executed on KBs to obtain the answers.
However, the generated logic forms are usually
non-executable, thus failing to arrive at the correct
answer (Sun et al., 2020). Pangu (Gu et al., 2022)
trains a language model discriminator to evaluate
probability of candidate plans. Information
extraction methods usually combines retrieval
and reasoning (Zhang et al., 2022a; Shi et al.,
2021; Sun et al., 2019; Jiang et al., 2023; Baek
et al., 2023). These methods show effectiveness
in handling single-hop retrieval. However, they
encounter challenges with multi-hop retrieval
concerning storage and computation costs, where
the number of relations expands exponentially
with each added hop.
KnowledGPT differs from KBQA methods in
two aspects. First, many KBQA methods are de-
signed for special queries about relational triples
in KGs, while KnowledGPT augments LLMs to
respond to various user queries with knowledge in
various forms from KBs. Second, KBQA methods
are typically trained on specific datasets and KGs,
whereas KnowledGPT requires no training and can
easily accommodate different LLMs and KBs.



### Claim 7/179

#### Claim Text
The Memory module leverages the LLM’s memory to guide retrieval, creating an unbounded memory pool that 5 aligns the text more closely with data distribution through iterative self-enhancement [17], [18].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 1):

polysemy, morphology, and coreference). We define this as the primal problem: better memory
prompts better generation. Consequently, numerous studies have focused on how to retrieve better
memory, ranging from sparse retrieval to dense retrieval [10, 63], from a fixed retriever to a learnable
retriever [41, 8], and from sentence-level memory to more fine-grained token-level memory [36, 35].
0.0 0.2 0.4 0.6 0.8 1.0
Memory Similarity
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9Hypothesis BLEU
Figure 1: Relation between memory and hy-
pothesis on JRC-Acquis En→De dataset.
The hypothesis is generated by a retrieval-
augmented translator whose memory is re-
trieved from the training set. The X-axis
represents the similarity between memory
and the reference.
However, a fundamental limitation exists in all previous
works: the memory is retrieved from a fixed corpus
and is constrained by the corpus’s quality. Due to the
finite retrieval space, bounded memory significantly
restricts the potential of memory-augmented generation
models [97]. In this paper, we explore the duality of the
primal problem, which posits that better generation
also prompts better memory. We propose a novel
framework called Selfmem, which iteratively employs
a retrieval-augmented generator to create an unbounded
memory pool and uses a memory selector to choose one
output as memory for the subsequent generation round.
By combining the primal and dual problem, a retrieval-
augmented generation model can elevate itself using
its own output, referred to as self-memory. The key
insight behind Selfmem is that the text more closely
resembling the data distribution during inference is not
the training data [87], but the model’s own output.
Selfmem consists of two complementary components:
a retrieval-augmented generator and a memory selector. The generator operates under two distinct
paradigms: fine-tuning a small model or few-shot prompting an LLM. For the former, we train the
generator with labeled data and retrieved memory, while for the latter, we employ a fixed black-box
LLM exclusively for inference alongside retrieved in-context learning samples. We then use the
generator’s output to train a memory selector based on a specific performance metric. By simply
replacing the retrieved memory with unbounded generated memory, we achieve higher-quality
generation output (primal problem), which subsequently serves as memory for the next round after
being refined by the memory selector (dual problem).
To evaluate the efficacy of theSelfmem, we carry out comprehensive experiments in three distinct text
generation tasks: neural machine translation, abstractive text summarization, and dialogue generation.
We witness substantial enhancements over robust baselines, attaining state-of-the-art outcomes in
JRC-Acquis (four directions), XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1). To gain
deeper insights into the Selfmem, we meticulously investigate each crucial component and pinpoint
the existing system bottleneck to guide future research endeavors.
2 Related Work
2.1 Retrieval-augmented Text Generation
Since the world is not a snapshot once the training corpus is collected, we can never expect an
ever-large model to capture everything in its parameters, even for LLMs like GPT-4 [62]. Therefore,
it is crucial to equip these models with an external memory bank to store additional knowledge or
useful demonstration examples for solving various NLP tasks[41, 78, 95].
In the translation domain, retrieval techniques have long been employed by the localization industry
to enhance human translators’ productivity and consistency even before the advent of machine
translation [94]. Early works on machine translation primarily focused on utilizing memory for
statistical machine translation (SMT) systems [ 80, 50]. For neural machine translation (NMT),
[28] were the first to use search engines to retrieve memory from the training set and incorporate
it with an external memory network. Subsequent research explored various aspects of retrieval-
augmented NMT, such as memory encoding methods [92, 93, 31], joint training of retrievers and
generators with monolingual data [ 8], memory granularity [ 35], and memory diversity [ 17]. For
few-shot LLM generation, strategies for in-context example selection have been proposed to improve
translation quality [2]. Furthermore, in-context machine translation has been shown to be effective
for on-the-fly adaptation [79]. For dialogue response generation tasks, employing exemplar/template
2



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 0):

Lift Yourself Up: Retrieval-augmented Text
Generation with Self-Memory
Xin Cheng1 Di Luo2 Xiuying Chen3 Lemao Liu4 Dongyan Zhao1 Rui Yan2
1 Peking University 2 Renmin University of China
3 KAUST 4 Tencent AI Lab
chengxin1998@stu.pku.edu.cn
Abstract
With direct access to human-written reference as memory, retrieval-augmented
generation has achieved much progress in a wide range of text generation tasks.
Since better memory would typically prompt better generation (we define this as
primal problem). The traditional approach for memory retrieval involves selecting
memory that exhibits the highest similarity to the input. However, this method
is constrained by the quality of the fixed corpus from which memory is retrieved.
In this paper, by exploring the duality of the primal problem: better generation
also prompts better memory, we propose a novel framework, Selfmem, which
addresses this limitation by iteratively employing a retrieval-augmented generator
to create an unbounded memory pool and using a memory selector to choose one
output as memory for the subsequent generation round. This enables the model
to leverage its own output, referred to as self-memory, for improved generation.
We evaluate the effectiveness ofSelfmem on three distinct text generation tasks:
neural machine translation, abstractive text summarization, and dialogue generation,
under two generation paradigms: fine-tuned small model and few-shot LLM.
Our approach achieves state-of-the-art results in four directions in JRC-Acquis
translation dataset, 50.3 ROUGE-1 in XSum, and 62.9 ROUGE-1 in BigPatent,
demonstrating the potential of self-memory in enhancing retrieval-augmented
generation models. Furthermore, we conduct thorough analyses of each component
in the Selfmem framework to identify current system bottlenecks and provide
insights for future research1.
1 Introduction
In recent years, retrieval-augmented text generation has attracted growing interest across various
fields, including neural machine translation[28, 17, 2], dialogue response generation[81, 6, 46], and
language modeling[36, 77, 19]. This innovative generation paradigm initially equips a fine-tuned
small model or a large language model (LLM) with access to an external database (typically the
training corpus) using information retrieval techniques. Subsequently, the generation process is
conducted based on both the input text and the retrieved memory.
In this paradigm, the guiding principle for memory retrieval is to find the memory that exhibits
the highest similarity to the current input [36, 96, 49]. This aligns with the human intuition that a
more similar demonstration sample typically offers more hints. As demonstrated in Figure 1, for a
retrieval-augmented translation model, the memory similarity alone exhibits a strong correlation with
the final translation quality, regardless of other factors that may influence translation quality (e.g.,
1Code and data available at: https://github.com/Hannibal046/SelfMemory
37th Conference on Neural Information Processing Systems (NeurIPS 2023).
arXiv:2305.02437v3  [cs.CL]  23 Dec 2023



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 2):

retrieval as an intermediate step has proven advantageous for generating informative responses [89,
91, 6, 7]. In-context learning example retrieval also aids in controllable dialogue [ 46]. Other
applications include abstractive summarization [64, 14, 18, 15], code generation [30], paraphrase
generation [34, 83], language modeling [36, 105], counterfactual data generation [24], open domain
question answering [12, 33] and semantic parsing [99].
2.2 Neural Text Reranking
By alleviating the discrepancy between training and inference (i.e., exposure bias) and directly
optimizing desired metrics, two-stage reranking methods have facilitated significant progress in
various text generation tasks. In machine translation, pioneering works by [75] and [61] introduced
and popularized discriminative reranking for SMT. In the context of NMT, research has focused on
two primary reranking approaches: generative reranking [56, 32, 88] and discriminative reranking [39,
71, 23]. For syntactic parsing, [21] were the first to employ a two-stage reranking method to select
outputs from a base parser, while [11] introduced a maximum entropy reranker. In text summarization,
RefSum [53] proposed a second-stage summarization framework to address train-test distribution
mismatches. SimCLS [ 54] used pairwise Learning To Rank (LTR) to select candidates with the
highest matching scores. SummaReranker [68] adopted a multi-task mixture-of-experts framework
to leverage different metrics capturing various aspects of generated candidates. BRIO [55] reused
the base model for a second round of fine-tuning with both cross-entropy loss and a candidate-level
ranking loss. JGR [76] employed an alternate training paradigm to train the generator and reranker.
A key limitation of these reranking methods is that they only represent a one-way process, wherein the
selected candidates become the system’s final output. In contrast, our framework innovatively utilizes
the chosen candidates as memory for the subsequent generation round of a retrieval-augmented
generator, which can produce better candidates with enhanced memory.
3 Methods
In this section, we begin with a motivating experiment on generation as memory(§ 3.1). Then, we
introduce Selfmem, a framework comprising a retrieval-augmented generator(§ 3.2) and a memory
selector (§ 3.3). The complete framework and algorithm are illustrated in Figure 2 and Algorithm 1.
3.1 Generation as Memory
The primary motivation behind our framework stems from the observation that the memory, which is
more similar in distribution to the data during inference, is not the training data (38.89 BLEU, as
shown in the first row of Table 1). Instead, it is the model’s own output (58.58 BLEU) within the
unbounded generation space. One interesting exploration involves directly utilizing the generated
output as memory in relation to the primal problem: better memory prompts better generation.
Table 1: Experiments on the relation between mem-
ory quality and the final hypothesis quality, measured
by the BLEU score with ground truth translation. The
retrieval-augmented translator keeps fixed while the
memory is obtained from different sources.
Memory Source Memory Quality Hypothesis Quality
Retrieval 38.89 58.58
Beam 58.58 58.43
Reference 100 90.43
Random 1.14 49.08
We conduct experiments on the JRC-Acquis
En→De dataset. The first row in Table 1
represents conventional retrieval-augmented
training with retrieved memory and achieves
a 58.58 BLEU score. However, directly in-
corporating beam output of this trained model
as memory (Beam) back into the generation
model does not yield any improvements (row
2), despite its higher similarity to the reference
compared to the retrieved ones. We hypoth-
esize two potential reasons for this: (1) the
retrieval-augmented generator may not gen-
eralize effectively in this context due to the
memory distribution shift (from 38.89 to 58.58), and (2) the beam memory does not offer any
information gain compared to the retrieved one, even it exhibits more overlap with the references.
To investigate the first hypothesis, we conduct experiments under the oracle and random scenarios by
using the reference as memory (Reference) and randomly sampled sentences as memory (Random).
The result is shown in Table 1 and it illustrates that a retrieval-augmented generator (trained with
3



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 3):

Target Distribution
 Frozen LLM / Trainable LM
NLL Loss
KL Loss
Y N 
Y 1 
... 
Y 
X 
 Y 
X 
candidates 
source 
target training 
memory 
... ... 
... ... 
(a) Retrieval-augmented Generator (b) Memory Selector
Retrieval
Predicted Distribution
M 
Primal
Dual
Figure 2: Overall framework. There are two components in Selfmem, a retrieval-augmented genera-
tor (a) and a memory selector (b). For the primal problem, (a) takes source and memory as input to
generate candidates for (b). For the dual problem, (b) takes as input source and generated candidates
to select memory for (a).
retrieved memory) has already learned to discriminate between different memories in both oracle and
random scenarios, without updating the model weights.
To evaluate the second conjecture, we first define the token sets of the reference, retrieved memory,
and beam memory as R, M, and B, respectively. The overlap token set, denoted by O, is defined
as the tokens that overlap with the references in the beam memory but not in the retrieved memory,
which is represented as R ∩ B − R ∩ M. O is considered as the additional information provided
by the beam memory. Inspired by the confidence analysis of NMT model [58], we compute the set
confidence score, ψ (·), as follows:
ψ (·) = 1
| · |
X
y i∈·
p (y i |x, y <i ) (1)
where p (y i |x, y <i ) is defined by the generation model. ψ (·) measures the confidence with which the
generation model generates the tokens. The value of ψ (R) is 0.58, while that of O is 0.76, indicating
that the generator is relatively confident in generating tokens in O, and therefore does not need to
resort to external memory [38]. Beam search ranks generated candidates based on p (y |x ), where the
selected memory falls within the confidence region of the generator and consequently provides no
information gain. This observation motivates us to select memory according to metrics other than
p (y |x ) in the memory selector (§3.3).
3.2 Retrieval-augmented Generator
Given a text pair (x, y ), where x = {x1, ..., x|x |} is the source, y = {y1, ..., y|y |} is the target. They
could be (document, summary) in summarization, (context, response) in dialogue generation or
(source, target) in machine translation. The retrieval-augmented generation would first use x to
retrieve memory m from datastore D. Then the generator G ξ (x, m ), parameterized by ξ , would take
both x and m as input to generate the target sentence y . In this paper, following standard practice,
we choose the training set as D = {(x i , y i )}|D|
i =1. For LLM as G ξ , we use the standard in-context
learning format to give (x, y ) as demonstration example. For tunable generator G ξ , we only keep the
target side of top-1 retrieval results as memory and we consider two commonly used architectures:
Joint-Encoder [29, 87, 41] and Dual-Encoder [92, 8, 17].
Joint-Encoder This architecture is the standard encoder-decoder-based model [3, 84]. The input is
the concatenation of x and m . The encoder would first map the input into the hidden states H :
H = Encoder(x [SEP] m ) (2)
4



Source: data\tc16_2312.10997v5\referenced_papers\[73]_2311.04177.pdf (Page 0):

Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale
Memory for Retrieval Augmented Generation
Eric Melz
SearchStax
eric@emelz.com
Abstract
Large Language Models (LLMs) are smart but
forgetful. Recent studies, (e.g., (Bubeck et al.,
2023)) on modern LLMs have shown that they
are capable of performing amazing tasks typ-
ically necessitating human-level intelligence.
However, unlike humans, frozen LLMs do not
improve over time; they neither acquire new
knowledge nor learn from their successes or
failures.
Some approaches to improving the intelligence
of LLMs include fine-tuning models based on
problem-solving performance (Zelikman et al.,
2022), and building bigger and more sophisti-
cated models (Bubeck et al., 2023). However,
these methods have the drawback of requiring
substantial data and computational resources to
retrain existing models.
In this paper, we explore the use of Retrieval
Augmented Generation, also known as RAG
(Lewis et al., 2021) to improve problem-solving
performance. We propose ARM-RAG (Aux-
iliary Rationale Memory for Retrieval Aug-
mented Generation), a system that learns from
its successes without incurring high training
costs. We demonstrate that the storage and
subsequent retrieval of reasoning chains have
a positive influence on performance in grade-
school math problems.
1 Introduction
Large Language Models (LLMs) are smart but for-
getful. Recent studies, (e.g., (Bubeck et al., 2023))
on modern LLMs have shown that they are capable
of performing amazing tasks typically necessitating
human-level intelligence. However, unlike humans,
frozen LLMs do not improve over time; they nei-
ther acquire new knowledge nor learn from their
successes or failures.
Several approaches exist to enhance the perfor-
mance of LLMs. One effective strategy is to train
larger LLMs using more data and extensive fine-
tuning. For example, (Bubeck et al., 2023) demon-
strate that GPT-4 significantly outperforms GPT-3
on a variety of challenging tasks.
Another approach to improving LLM perfor-
mance involves fine-tuning a base LLM based on
its successes and failures in problem-solving. (Ze-
likman et al., 2022) propose a system that enhances
a base LLM by training it with examples generated
from both successful and unsuccessful problem-
solving attempts. They demonstrate that this ap-
proach can significantly increase the "intelligence"
of LLMs, enabling them to perform better on math
problems. However, they note that if the base LLM
is not sufficiently advanced, their bootstrapping
approach is ineffective. For instance, their model
shows the desired improvements with GPT-3 but
does not show any improvement when starting with
GPT-2.
Retrieval Augmented Generation, aka RAG
(Lewis et al., 2021) has been proposed to augment
the parametric memory of LLMs with the non-
parametric memory of Knowledge Bases (KBs),
which can be retrieved using search engines. The
RAG approach has been shown to improve the per-
formance of several tasks, such as Question An-
swering requiring multiple "hops", as demonstrated
by (Khattab et al., 2021).
All of the aforementioned techniques have
shown improvements in performance over base
LLMs. However, each has its drawbacks. The
build-from-scratch and fine-tuning approaches re-
quire substantial amounts of data and comput-
ing resources for model training. In the case of
RAG, once the language model and the retrieval
model are established, both the parametric and non-
parametric memories become fixed, and no further
learning occurs over time.
The central hypothesis of this paper is that Re-
trieval Augmented Generation (RAG) can be suc-
cessfully deployed to enhance the problem-solving
abilities of LLMs. We propose ARM-RAG (Auxil-
iary Rationale Memory for Retrieval Augmented
arXiv:2311.04177v1  [cs.CL]  7 Nov 2023



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 1):

polysemy, morphology, and coreference). We define this as the primal problem: better memory
prompts better generation. Consequently, numerous studies have focused on how to retrieve better
memory, ranging from sparse retrieval to dense retrieval [10, 63], from a fixed retriever to a learnable
retriever [41, 8], and from sentence-level memory to more fine-grained token-level memory [36, 35].
0.0 0.2 0.4 0.6 0.8 1.0
Memory Similarity
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9Hypothesis BLEU
Figure 1: Relation between memory and hy-
pothesis on JRC-Acquis En→De dataset.
The hypothesis is generated by a retrieval-
augmented translator whose memory is re-
trieved from the training set. The X-axis
represents the similarity between memory
and the reference.
However, a fundamental limitation exists in all previous
works: the memory is retrieved from a fixed corpus
and is constrained by the corpus’s quality. Due to the
finite retrieval space, bounded memory significantly
restricts the potential of memory-augmented generation
models [97]. In this paper, we explore the duality of the
primal problem, which posits that better generation
also prompts better memory. We propose a novel
framework called Selfmem, which iteratively employs
a retrieval-augmented generator to create an unbounded
memory pool and uses a memory selector to choose one
output as memory for the subsequent generation round.
By combining the primal and dual problem, a retrieval-
augmented generation model can elevate itself using
its own output, referred to as self-memory. The key
insight behind Selfmem is that the text more closely
resembling the data distribution during inference is not
the training data [87], but the model’s own output.
Selfmem consists of two complementary components:
a retrieval-augmented generator and a memory selector. The generator operates under two distinct
paradigms: fine-tuning a small model or few-shot prompting an LLM. For the former, we train the
generator with labeled data and retrieved memory, while for the latter, we employ a fixed black-box
LLM exclusively for inference alongside retrieved in-context learning samples. We then use the
generator’s output to train a memory selector based on a specific performance metric. By simply
replacing the retrieved memory with unbounded generated memory, we achieve higher-quality
generation output (primal problem), which subsequently serves as memory for the next round after
being refined by the memory selector (dual problem).
To evaluate the efficacy of theSelfmem, we carry out comprehensive experiments in three distinct text
generation tasks: neural machine translation, abstractive text summarization, and dialogue generation.
We witness substantial enhancements over robust baselines, attaining state-of-the-art outcomes in
JRC-Acquis (four directions), XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1). To gain
deeper insights into the Selfmem, we meticulously investigate each crucial component and pinpoint
the existing system bottleneck to guide future research endeavors.
2 Related Work
2.1 Retrieval-augmented Text Generation
Since the world is not a snapshot once the training corpus is collected, we can never expect an
ever-large model to capture everything in its parameters, even for LLMs like GPT-4 [62]. Therefore,
it is crucial to equip these models with an external memory bank to store additional knowledge or
useful demonstration examples for solving various NLP tasks[41, 78, 95].
In the translation domain, retrieval techniques have long been employed by the localization industry
to enhance human translators’ productivity and consistency even before the advent of machine
translation [94]. Early works on machine translation primarily focused on utilizing memory for
statistical machine translation (SMT) systems [ 80, 50]. For neural machine translation (NMT),
[28] were the first to use search engines to retrieve memory from the training set and incorporate
it with an external memory network. Subsequent research explored various aspects of retrieval-
augmented NMT, such as memory encoding methods [92, 93, 31], joint training of retrievers and
generators with monolingual data [ 8], memory granularity [ 35], and memory diversity [ 17]. For
few-shot LLM generation, strategies for in-context example selection have been proposed to improve
translation quality [2]. Furthermore, in-context machine translation has been shown to be effective
for on-the-fly adaptation [79]. For dialogue response generation tasks, employing exemplar/template
2



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 0):

Lift Yourself Up: Retrieval-augmented Text
Generation with Self-Memory
Xin Cheng1 Di Luo2 Xiuying Chen3 Lemao Liu4 Dongyan Zhao1 Rui Yan2
1 Peking University 2 Renmin University of China
3 KAUST 4 Tencent AI Lab
chengxin1998@stu.pku.edu.cn
Abstract
With direct access to human-written reference as memory, retrieval-augmented
generation has achieved much progress in a wide range of text generation tasks.
Since better memory would typically prompt better generation (we define this as
primal problem). The traditional approach for memory retrieval involves selecting
memory that exhibits the highest similarity to the input. However, this method
is constrained by the quality of the fixed corpus from which memory is retrieved.
In this paper, by exploring the duality of the primal problem: better generation
also prompts better memory, we propose a novel framework, Selfmem, which
addresses this limitation by iteratively employing a retrieval-augmented generator
to create an unbounded memory pool and using a memory selector to choose one
output as memory for the subsequent generation round. This enables the model
to leverage its own output, referred to as self-memory, for improved generation.
We evaluate the effectiveness ofSelfmem on three distinct text generation tasks:
neural machine translation, abstractive text summarization, and dialogue generation,
under two generation paradigms: fine-tuned small model and few-shot LLM.
Our approach achieves state-of-the-art results in four directions in JRC-Acquis
translation dataset, 50.3 ROUGE-1 in XSum, and 62.9 ROUGE-1 in BigPatent,
demonstrating the potential of self-memory in enhancing retrieval-augmented
generation models. Furthermore, we conduct thorough analyses of each component
in the Selfmem framework to identify current system bottlenecks and provide
insights for future research1.
1 Introduction
In recent years, retrieval-augmented text generation has attracted growing interest across various
fields, including neural machine translation[28, 17, 2], dialogue response generation[81, 6, 46], and
language modeling[36, 77, 19]. This innovative generation paradigm initially equips a fine-tuned
small model or a large language model (LLM) with access to an external database (typically the
training corpus) using information retrieval techniques. Subsequently, the generation process is
conducted based on both the input text and the retrieved memory.
In this paradigm, the guiding principle for memory retrieval is to find the memory that exhibits
the highest similarity to the current input [36, 96, 49]. This aligns with the human intuition that a
more similar demonstration sample typically offers more hints. As demonstrated in Figure 1, for a
retrieval-augmented translation model, the memory similarity alone exhibits a strong correlation with
the final translation quality, regardless of other factors that may influence translation quality (e.g.,
1Code and data available at: https://github.com/Hannibal046/SelfMemory
37th Conference on Neural Information Processing Systems (NeurIPS 2023).
arXiv:2305.02437v3  [cs.CL]  23 Dec 2023



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 2):

retrieval as an intermediate step has proven advantageous for generating informative responses [89,
91, 6, 7]. In-context learning example retrieval also aids in controllable dialogue [ 46]. Other
applications include abstractive summarization [64, 14, 18, 15], code generation [30], paraphrase
generation [34, 83], language modeling [36, 105], counterfactual data generation [24], open domain
question answering [12, 33] and semantic parsing [99].
2.2 Neural Text Reranking
By alleviating the discrepancy between training and inference (i.e., exposure bias) and directly
optimizing desired metrics, two-stage reranking methods have facilitated significant progress in
various text generation tasks. In machine translation, pioneering works by [75] and [61] introduced
and popularized discriminative reranking for SMT. In the context of NMT, research has focused on
two primary reranking approaches: generative reranking [56, 32, 88] and discriminative reranking [39,
71, 23]. For syntactic parsing, [21] were the first to employ a two-stage reranking method to select
outputs from a base parser, while [11] introduced a maximum entropy reranker. In text summarization,
RefSum [53] proposed a second-stage summarization framework to address train-test distribution
mismatches. SimCLS [ 54] used pairwise Learning To Rank (LTR) to select candidates with the
highest matching scores. SummaReranker [68] adopted a multi-task mixture-of-experts framework
to leverage different metrics capturing various aspects of generated candidates. BRIO [55] reused
the base model for a second round of fine-tuning with both cross-entropy loss and a candidate-level
ranking loss. JGR [76] employed an alternate training paradigm to train the generator and reranker.
A key limitation of these reranking methods is that they only represent a one-way process, wherein the
selected candidates become the system’s final output. In contrast, our framework innovatively utilizes
the chosen candidates as memory for the subsequent generation round of a retrieval-augmented
generator, which can produce better candidates with enhanced memory.
3 Methods
In this section, we begin with a motivating experiment on generation as memory(§ 3.1). Then, we
introduce Selfmem, a framework comprising a retrieval-augmented generator(§ 3.2) and a memory
selector (§ 3.3). The complete framework and algorithm are illustrated in Figure 2 and Algorithm 1.
3.1 Generation as Memory
The primary motivation behind our framework stems from the observation that the memory, which is
more similar in distribution to the data during inference, is not the training data (38.89 BLEU, as
shown in the first row of Table 1). Instead, it is the model’s own output (58.58 BLEU) within the
unbounded generation space. One interesting exploration involves directly utilizing the generated
output as memory in relation to the primal problem: better memory prompts better generation.
Table 1: Experiments on the relation between mem-
ory quality and the final hypothesis quality, measured
by the BLEU score with ground truth translation. The
retrieval-augmented translator keeps fixed while the
memory is obtained from different sources.
Memory Source Memory Quality Hypothesis Quality
Retrieval 38.89 58.58
Beam 58.58 58.43
Reference 100 90.43
Random 1.14 49.08
We conduct experiments on the JRC-Acquis
En→De dataset. The first row in Table 1
represents conventional retrieval-augmented
training with retrieved memory and achieves
a 58.58 BLEU score. However, directly in-
corporating beam output of this trained model
as memory (Beam) back into the generation
model does not yield any improvements (row
2), despite its higher similarity to the reference
compared to the retrieved ones. We hypoth-
esize two potential reasons for this: (1) the
retrieval-augmented generator may not gen-
eralize effectively in this context due to the
memory distribution shift (from 38.89 to 58.58), and (2) the beam memory does not offer any
information gain compared to the retrieved one, even it exhibits more overlap with the references.
To investigate the first hypothesis, we conduct experiments under the oracle and random scenarios by
using the reference as memory (Reference) and randomly sampled sentences as memory (Random).
The result is shown in Table 1 and it illustrates that a retrieval-augmented generator (trained with
3



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 3):

Target Distribution
 Frozen LLM / Trainable LM
NLL Loss
KL Loss
Y N 
Y 1 
... 
Y 
X 
 Y 
X 
candidates 
source 
target training 
memory 
... ... 
... ... 
(a) Retrieval-augmented Generator (b) Memory Selector
Retrieval
Predicted Distribution
M 
Primal
Dual
Figure 2: Overall framework. There are two components in Selfmem, a retrieval-augmented genera-
tor (a) and a memory selector (b). For the primal problem, (a) takes source and memory as input to
generate candidates for (b). For the dual problem, (b) takes as input source and generated candidates
to select memory for (a).
retrieved memory) has already learned to discriminate between different memories in both oracle and
random scenarios, without updating the model weights.
To evaluate the second conjecture, we first define the token sets of the reference, retrieved memory,
and beam memory as R, M, and B, respectively. The overlap token set, denoted by O, is defined
as the tokens that overlap with the references in the beam memory but not in the retrieved memory,
which is represented as R ∩ B − R ∩ M. O is considered as the additional information provided
by the beam memory. Inspired by the confidence analysis of NMT model [58], we compute the set
confidence score, ψ (·), as follows:
ψ (·) = 1
| · |
X
y i∈·
p (y i |x, y <i ) (1)
where p (y i |x, y <i ) is defined by the generation model. ψ (·) measures the confidence with which the
generation model generates the tokens. The value of ψ (R) is 0.58, while that of O is 0.76, indicating
that the generator is relatively confident in generating tokens in O, and therefore does not need to
resort to external memory [38]. Beam search ranks generated candidates based on p (y |x ), where the
selected memory falls within the confidence region of the generator and consequently provides no
information gain. This observation motivates us to select memory according to metrics other than
p (y |x ) in the memory selector (§3.3).
3.2 Retrieval-augmented Generator
Given a text pair (x, y ), where x = {x1, ..., x|x |} is the source, y = {y1, ..., y|y |} is the target. They
could be (document, summary) in summarization, (context, response) in dialogue generation or
(source, target) in machine translation. The retrieval-augmented generation would first use x to
retrieve memory m from datastore D. Then the generator G ξ (x, m ), parameterized by ξ , would take
both x and m as input to generate the target sentence y . In this paper, following standard practice,
we choose the training set as D = {(x i , y i )}|D|
i =1. For LLM as G ξ , we use the standard in-context
learning format to give (x, y ) as demonstration example. For tunable generator G ξ , we only keep the
target side of top-1 retrieval results as memory and we consider two commonly used architectures:
Joint-Encoder [29, 87, 41] and Dual-Encoder [92, 8, 17].
Joint-Encoder This architecture is the standard encoder-decoder-based model [3, 84]. The input is
the concatenation of x and m . The encoder would first map the input into the hidden states H :
H = Encoder(x [SEP] m ) (2)
4



Source: data\tc16_2312.10997v5\referenced_papers\[73]_2311.04177.pdf (Page 0):

Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale
Memory for Retrieval Augmented Generation
Eric Melz
SearchStax
eric@emelz.com
Abstract
Large Language Models (LLMs) are smart but
forgetful. Recent studies, (e.g., (Bubeck et al.,
2023)) on modern LLMs have shown that they
are capable of performing amazing tasks typ-
ically necessitating human-level intelligence.
However, unlike humans, frozen LLMs do not
improve over time; they neither acquire new
knowledge nor learn from their successes or
failures.
Some approaches to improving the intelligence
of LLMs include fine-tuning models based on
problem-solving performance (Zelikman et al.,
2022), and building bigger and more sophisti-
cated models (Bubeck et al., 2023). However,
these methods have the drawback of requiring
substantial data and computational resources to
retrain existing models.
In this paper, we explore the use of Retrieval
Augmented Generation, also known as RAG
(Lewis et al., 2021) to improve problem-solving
performance. We propose ARM-RAG (Aux-
iliary Rationale Memory for Retrieval Aug-
mented Generation), a system that learns from
its successes without incurring high training
costs. We demonstrate that the storage and
subsequent retrieval of reasoning chains have
a positive influence on performance in grade-
school math problems.
1 Introduction
Large Language Models (LLMs) are smart but for-
getful. Recent studies, (e.g., (Bubeck et al., 2023))
on modern LLMs have shown that they are capable
of performing amazing tasks typically necessitating
human-level intelligence. However, unlike humans,
frozen LLMs do not improve over time; they nei-
ther acquire new knowledge nor learn from their
successes or failures.
Several approaches exist to enhance the perfor-
mance of LLMs. One effective strategy is to train
larger LLMs using more data and extensive fine-
tuning. For example, (Bubeck et al., 2023) demon-
strate that GPT-4 significantly outperforms GPT-3
on a variety of challenging tasks.
Another approach to improving LLM perfor-
mance involves fine-tuning a base LLM based on
its successes and failures in problem-solving. (Ze-
likman et al., 2022) propose a system that enhances
a base LLM by training it with examples generated
from both successful and unsuccessful problem-
solving attempts. They demonstrate that this ap-
proach can significantly increase the "intelligence"
of LLMs, enabling them to perform better on math
problems. However, they note that if the base LLM
is not sufficiently advanced, their bootstrapping
approach is ineffective. For instance, their model
shows the desired improvements with GPT-3 but
does not show any improvement when starting with
GPT-2.
Retrieval Augmented Generation, aka RAG
(Lewis et al., 2021) has been proposed to augment
the parametric memory of LLMs with the non-
parametric memory of Knowledge Bases (KBs),
which can be retrieved using search engines. The
RAG approach has been shown to improve the per-
formance of several tasks, such as Question An-
swering requiring multiple "hops", as demonstrated
by (Khattab et al., 2021).
All of the aforementioned techniques have
shown improvements in performance over base
LLMs. However, each has its drawbacks. The
build-from-scratch and fine-tuning approaches re-
quire substantial amounts of data and comput-
ing resources for model training. In the case of
RAG, once the language model and the retrieval
model are established, both the parametric and non-
parametric memories become fixed, and no further
learning occurs over time.
The central hypothesis of this paper is that Re-
trieval Augmented Generation (RAG) can be suc-
cessfully deployed to enhance the problem-solving
abilities of LLMs. We propose ARM-RAG (Auxil-
iary Rationale Memory for Retrieval Augmented
arXiv:2311.04177v1  [cs.CL]  7 Nov 2023



### Claim 8/179

#### Claim Text
Routing in the RAG system navigates through diverse data sources, selecting the optimal pathway for a query, whether it involves summarization, specific database searches, or merging different information streams [19].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 10):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:11
Researchers at West Virginia University in the United States 
have observed synthetic DNA at the atomic level and learned 
how to alter its structure to enhance its scissor-like functions.
Researchers at West Virginia University have successfully observed
 the structure of synthetic DNA at the atomic level and discovered
 that altering its structure can enhance its scissor-like functions. 
Their research findings were published in the Nature sub-journal, 
Communications Chemistry. The study found that these DNA 
molecules can fold into complex shapes capable of performing ...
Science and Technology Daily, Beijing, July 27 (Reporter Zhang 
Mengran) - Researchers at West Virginia University have achieved 
the observation of synthetic DNA at the atomic level, gaining 
insights into how to alter its structure to enhance its scissor-like 
functions. Understanding these synthetic DNA reactions in greater 
detail could be key to unlocking new medical technologies in the 
future. The research findings were published in the recent issue of 
the Nature sub-journal, Communications Chemistry. Atomic-level ...
The synthetic DNA used in this study, also known as DNAzymes, 
differs from human DNA in that it is created in a laboratory, has low 
production costs, and can catalyze chemical reactions. Researchers 
noted that DNA is typically regarded as inert when it comes to 
storing human genetic information; however, some types of DNA 
evolved in the laboratory defy traditional rules. These DNA ...
Fig. 4. The dataset construction pipeline for text continuation and multi-document summarization task.
our news corpus, removing the 10,000 articles 𝑑 simultaneously. The new news corpus 𝐷−𝑑+𝐸
serves as our retrieval corpus, and we expect the model to use the events and relevant information
from the retrieval corpus to generate a summary of the articles 𝑑.
3.3 Text continuation: RAG Application in "Create"
RAG is useful not only for "Delete", where it retrieves and summarizes key information from massive
texts, but also for "Create". In this scenario, RAG systems show strong creativity by expanding
existing texts, and we take the text continuation task as an evaluation. The text continuation task
aims to automatically produce coherent and relevant subsequent content based on the beginning
of the text, making the text more complete and vivid.
To construct the continuation task dataset, we follow the same method as the summary task
dataset. Figure 4 shows the construction process of text continuation. Specifically, we select a news
article from a high-quality corpus and use a specialized Chinese word segmentation tool, to split it
into sentences. Then, we divide the article into two equal parts: the first half serves as the input,
and the second half as the output of the continuation dataset. We expect the model to use RAG
technology to retrieve relevant information from the document library and generate a continuation
that is coherent, informative, and consistent with the input and output.
To ensure that the retrieval database covers the real continuation text, we use the Baidu search
engine to find external documents and add them to the database. The continuation text differs from
the event text in that it consists of multiple sentences. Therefore, we split the continuation text
into paragraphs by sentences and retrieve relevant documents for each paragraph using the search
engine. This way, we guarantee that the retrieval database contains most of the information to
reconstruct the continuation text.
3.4 Question Answering: RAG Application in "Read"
Another application scenario of RAG is to use external knowledge bases to enhance the question-
answering capabilities of LLMs, which can be applied to various knowledge-intensive tasks. Cur-
rently, there are many evaluation benchmarks to measure the performance of RAG in this scenario,
and multiple question answering datasets have been created.
However, the existing question answering datasets also have some limitations. On the one hand,
some datasets (such as NQ and WEBQA) are outdated, and may have been covered by LLMs in
the pre-training stage, which reduces the advantage of RAG systems. On the other hand, some
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 6):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:7
2.3 Citation-Enhanced RAG
In traditional RAG methods, despite the rich information sources provided by retrieved contexts for
text generation, these models often do not explicitly require responses to provide corresponding
citations, making traceability difficult. Therefore, enhancing text verifiability by introducing citation
links, i.e., explicit references, has become an important research direction in the RAG field [17, 54].
Providing citation indicators in the response text offers several clear benefits. First, users can
easily verify the claims made by LLMs based on the provided citations, thus improving the trans-
parency and credibility of the text. Second, if the text generated by LLMs adheres faithfully to the
cited contexts, it can significantly improve its accuracy and reduce the phenomenon of "halluci-
nations" [17]. Given this, generating high-quality citations and evaluating the quality of citation
generation have become crucial elements of assessing RAG performance. Constructing appropriate
prompts directly through the retrieval context to guide the model in generating corresponding
citations constitutes a direct and effective method of citation generation [22].
In terms of evaluation, early research primarily focused on the fluency, accuracy, and basic
citation quality of the text generated by LLMs [17, 29]. For example, Rashkin et al. proposed the
"Attributable to Identified Sources" (AIS) score [42], which serves as a valuable tool for measuring the
degree to which generated text is faithful to its sources. As research progressed, scholars recognized
the need for more detailed evaluation methods to differentiate between various levels of citation
support. By creating specialized datasets such as SCIFI [7], researchers can more precisely evaluate
fine-grained citations at the clause level in texts generated by LLMs. The ALiiCE framework [54], by
analyzing the atomic structure of sentence claims, introduced fine-grained evaluation metrics, such
as location citation recall and precision, and the coefficient of variation of citation locations, to more
granularly evaluate the quality of citation generation in RAG [54]. In practical applications, [61]
found that RAG requires more complex evaluation frameworks to distinguish between various
levels of citation support by comparing different fidelity metrics. These RAG evaluation methods
not only consider the presence of citations but also their accuracy and relevance.
While Citation-Enhanced RAG delves deeply into the specific domain of citation generation,
aiming to improve the credibility and accuracy of text generated by RAG systems, our benchmark
provides a comprehensive evaluation framework encompassing various aspects of RAG systems
and multiple application scenarios.
3 CRUD-RAG: A COMPREHENSIVE CHINESE BENCHMARK FOR RAG
As we discussed earlier, implementing RAG effectively requires careful tuning of multiple com-
ponents, such as the retrieval model, the knowledge corpus, the language model, and the query
formulation. Therefore, we need a framework that can evaluate the RAG system automatically. This
framework would enable us to examine how these components affect the system’s performance,
and provide us with useful insights for improving and innovating the system.
However, The current RAG benchmarks have several drawbacks: they only evaluate question
answering tasks [1, 41, 57], ignoring other diverse application of RAG. The optimization strategy
for question-answering tasks may not suit other tasks; And in the evaluation experiment, current
RAG benchmarks only account for the LLM component in the RAG pipeline, or the retriever in the
knowledge-intensive scenario, disregarding the vital roles of retrieval database construction and
retrieval in non-knowledge-intensive scenarios.
To address the shortcomings of previous benchmarks, we introduce CRUD-RAG, a comprehensive
Chinese benchmark for RAG. Figure 2 illustrates the features of our CRUD-RAG benchmark. It
classifies the RAG application scenarios into four categories: Create, Read, Update, and Delete,
then we construct appropriate evaluation tasks and datasets for each category. Besides, in the
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 13):

Figure 2: RAG Systems Evaluation on NQ - Context Relevance
Figure 3: RAG Systems Evaluation on NQ - Answer Relevance



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 1):

111:2 Lyu, et al.
Additional Key Words and Phrases: Retrieval-Augmented Generation, Large Language Models, Evaluation
ACM Reference Format:
Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong
Xu, and Enhong Chen. 2018. CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented
Generation of Large Language Models. J. ACM 37, 4, Article 111 (August 2018), 31 pages. https://doi.org/
XXXXXXX.XXXXXXX
1 INTRODUCTION
Retrieval-augmented generation (RAG) is an advanced technique that leverages external knowledge
sources to enhance the text generation capabilities of large language models(LLMs). It retrieves
relevant paragraphs from a corpus based on the input, and feeds them to the LLMs along with the
input. With the help of external knowledge, LLMs can generate more accurate and credible responses
and effectively address challenges such as outdated knowledge [19], hallucinations [3, 9, 35, 62],
and lack of domain expertise [30, 46]. Therefore, RAG technology is attracting increasing attention.
Although the effectiveness of retrieval-augmented strategies has been proven through extensive
practice, their implementation still requires a significant amount of tuning. The overall performance
of the RAG system is affected by multiple factors, such as the retrieval model, construction of the
external knowledge base, and language model. Therefore, automatic evaluation of RAG systems
is crucial. Currently, there are only a few existing benchmarks for evaluating RAG performance,
as creating high-quality datasets and experimenting with them entail significant costs. These
benchmarks can be classified into two types: reference-required and reference-free evaluation.
Reference-free evaluation frameworks, such as RAGAS [13] and ARES [44], use LLM-generated
data to evaluate RAG systems on contextual relevance, faithfulness, and informativeness. These
frameworks do not depend on ground truth references, but only assess the coherence of the
generated text with the retrieved context. This approach may be unreliable if the retrieved external
information is low-quality.
Consequently, reference-required evaluations remain the predominant method for assessing RAG
systems. Existing benchmarks for reference-required evaluations, such as RGB [8] and NQ [26].
do have their limitations. First, they all rely on question answering tasks to measure the perfor-
mance of RAG systems. Question answering is not the only RAG application scenario, and an
optimization strategy that works well for question answering may not be generalized to other
scenarios. Thus, these benchmarks may not capture the full potential of RAG systems. Second, in
the experiments, current evaluations usually focus on evaluating the LLM part of the RAG pipeline,
or focus on retriever performance in the knowledge-intensive scenario [40], while ignoring the
retrieval methods in non-knowledge-intensive scenarios and external knowledge base construction.
These components are also crucial for RAG systems. Therefore, a comprehensive evaluation of the
RAG system may not be obtained using any existing benchmarks.
To evaluate the performance of RAG in different application scenarios, we need a comprehensive
benchmark that covers more than just the question-answering task. Lewis et al. [28] argue that the
core of RAG systems is their interactive way of combining LLMs with external knowledge sources.
And following [25], we can group any interaction with external knowledge sources into four basic
actions: create, read, update, and delete, which are also known as CRUD actions [48]. Therefore,
we can use the CRUD framework to classify the RAG systems’ application scenarios. As shown in
Figure 1, each CRUD category demonstrates different capabilities of the RAG system:
•In "CREATE", the system improves the input text by adding relevant information from
external sources, making creative outputs such as poetry, stories, or code.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 14):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:15
summarization tasks. Therefore, it does not require any ground truth reference. However, for
RAG systems, the retrieved texts may be irrelevant or incorrect, so consistency with them is not a
valid criterion. Instead, we use this metric to measure how well the generated text matches the
ground-truth reference. We call this metric RAGQuestEval. We will explain this metric in detail.
Let 𝐺𝑇 and 𝐺𝑀 be two sequences of tokens, where 𝐺𝑇 denotes the ground truth references and
𝐺𝑀 the corresponding evaluated generations. First, we generate a series of questions from the
ground truth references 𝐺𝑇 using the QuestEval method, which extracts entities and noun phrases
from the text. The goal of RAGQuestEval is to check if the generated text includes and conveys
correctly all the key information from the ground truth reference.
Next, we answer these questions using both real references and model-generated text. If the
question is unanswerable, the model returns "<Unanswerable>".
Finally, we calculate two scores to evaluate the quality of the generated text: recall and precision.
Recall. Recall is the ratio of answerable questions to all questions. This score shows how much
information in the ground truth reference is captured by the text generated by the RAG system. A
higher recall means that the generated text covers more information from the reference.
Recall(𝐺𝑇,𝐺𝑀 )= 1
|𝑄𝐺(𝐺𝑇)|
∑︁
(𝑞,𝑟)∈𝑄𝐺 (𝐺𝑇)
I[𝑄𝐴(𝐺𝑀,𝑞)≠ < Unanswerable >] (1)
In the above equation, 𝑄𝐺 is the question generator and 𝑄𝐴 is the question answerer.
Precision. Precision is the average answer similarity of all questions, excluding the unanswerable
ones. We use the token level F1 score to measure the answer similarity, which is a standard metric
for evaluating factoid question answering models. Higher precision means that the generated text
is more accurate and consistent with the reference.
Prec(𝐺𝑇,𝐺𝑀 )= 1
|𝑄𝐺(𝐺𝑇)|
∑︁
(𝑞,𝑟)∈𝑄𝐺 (𝐺𝑇)
𝐹1 (𝑄𝐴(𝐺𝑀,𝑞),𝑟) (2)
4 EXPERIMENT
The current evaluation of RAG Benchmark only focuses on the large language model component
in the RAG pipeline, and overlooks the importance of retrieval database construction and retriever.
To address this gap, we examine how different aspects of RAG systems affect their performance in
our benchmark. We also discuss some possible ways to improve existing RAG systems.
4.1 Experimental Settings
In this section, we will introduce the components of the RAG system, and describe how we conduct
experiments to evaluate their impact on system performance. The RAG system consists of the
following components:
•Chunk size: The RAG system splits the external knowledge into chunks of a certain length
and stores them in a vector database. The chunk size affects the retrieval accuracy and the
completeness of the context.
•Chunk overlap: Chunk overlap refers to the shared tokens between two consecutive text
chunks and is used to ensure semantic coherence when chunking.
•Embedding model : The RAG system converts the text chunks and the user’s query into
vectors using an embedding model or other methods. The embedding model affects the
quality and relevance of the context.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



### Claim 9/179

#### Claim Text
The Predict module aims to reduce redundancy and noise by generating context directly through the LLM, ensuring relevance and accuracy [13].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[23]_2212.14024.pdf (Page 6):

DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
retrieval (Fox & Shaw, 1994; Xue & Croft, 2013; Kur-
land & Culpepper, 2018) and sequentially processing multi-
ple queries was explored recently by Gao et al. (2022) for
retroactively attributing text generated by LMs to citations.
Inspired by these, we include afused_retrieval primitive
to DSP to offer a versatile mechanism for interacting with
frozen retrievers. It accepts an optional fusion function that
maps multiple retrieval lists into one. By default, DSP uses
a variant of CombSUM (Fox & Shaw, 1994), assigning each
passage the sum of its probabilities across retrieval lists.
To illustrate, the modiﬁcation below generates n = 10
queries for the transformation multihop_search_v2.
c = generate ( hop_template , n =10) (x)
passages = fused_retrieval (c. queries , k =5)
summary = c. summaries [0] # highest - scoring summary
Compositions & Extensions To illustrate a simple com-
position, we can equip a chatbot with the capacity for con-
versational multi-hop search by combining a query rewriting
step, which produces a query that encompasses all of the
relevant conversational context, with the multi-hop transfor-
mation, as follows.
1 def conversational_multihop_search (x):
2 x. question = generate ( conv_rewriting_template )(x)
3 return multihop_search_v2 (x)
Similar approaches can be used for correcting spelling mis-
takes or implementing pseudo-relevance feedback (Cao
et al., 2008; Wang et al., 2022a), in which retrieved passages
are used to inform a better search query, though this has not
been attempted with pretrained LMs to our knowledge.
2.5. PREDICT
The PREDICT stage generates the system output using
demonstrations (e.g., in x.demos) and passages (e.g., in
x.context). PREDICT tackles the challenges of reliably
solving the downstream task, which integrates much of the
work on in-context learning in general. Within DSP, it also
has the more specialized function of systematically aggre-
gating information across a large number of demonstrations,
passages, and candidate predictions.
Generating Candidates Generally, PREDICT has to pro-
duce one or more candidate predictions for the end-task.
To this end, the basic primitive in PREDICT is generate,
which accepts a Template and (via currying) an Example
and queries the LM to produce one or more completions,
as explored earlier in §2.4. A corresponding primitive that
uses the RM in this stage is rank, which accepts a query
and one or more passages and returns their relevance scores.
1 Template # template : an object that can produce
prompts and parse completions
2
3 generate ( template : Template )
4 -> fn( example : Example )
5 -> Completions # object with keys to access
extracted preds and scores
6
7 rank ( query : str , passages : List [ str ])
8 -> List [ float ] # object with keys to access
passage texts and scores
A Template is an object that can produce prompts, that is,
map an Example to a string, and extract ﬁelds out of com-
pletions. For instance, we can map an example x that has a
question and retrieved passages to the following prompt:
1 My task is to answer questions using Web documents .
2
3 { Task demonstrations from x. demos , if any }
4
5 Context : {x. passage }
6 Question : {x. question }
7 Rationale : Let /quotesingle.Vars think step by step . __{ rationale }__
8 Answer : __{ answer }__
As this illustrates, the LM will be asked to generate a chain-
of-thought rationale (CoT; Wei et al. 2022; Kojima et al.
2022) and an answer, and the generated text will be ex-
tracted back into the rationale and answer keys of each
completion.
Each invocation to the LM can sample multiple candidate
predictions. Selecting a “best” prediction is the subject of
much work on decoding (Wiher et al., 2022; Li et al., 2022),
but a frozen and general-purpose LM may not support cus-
tom modiﬁcations to decoding. Within these constraints, we
present several high-level strategies for selecting predictions
and aggregating information in DSP via the LM and RM.
Selecting Predictions Among multiple candidates, we
can simply extract the most popular prediction. When a CoT
is used to arrive at the answer, this is the self-consistency
method of Wang et al. (2022c), which seeks to identify
predictions at which multiple distinct rationales arrive.
1 from dsp import generate , majority
2
3 def multihop_predict (x):
4 candidates = generate ( template_qa )(x)
5 return x. copy ( answer = majority ( candidates ). answer )
DSP generalizes this in two ways. First, we can sample
multiple “pipelines of transformations” (PoT) within the pro-
gram, rather than locally with “chains of thought” (CoT) in
one transformation. These chains may even invoke different
paths in the program, as illustrated below.



Source: data\tc16_2312.10997v5\referenced_papers\[72]_2301.12652.pdf (Page 2):

REPLUG : Retrieval-Augmented Black-Box Language Models
Figure 2.REPLUG at inference (§3). Given an input context, REPLUG first retrieves a small set of relevant documents from an external
corpus using a retriever (§3.1 Document Retrieval). Then it prepends each document separately to the input context and ensembles output
probabilities from different passes (§3.2 Input Reformulation).
and a training scheme to further adapt the retriever to large
LMs.
3. REPLUG
We introduce REPLUG (Retrieve and Plug), a new retrieval-
augmented LM paradigm where the language model is
treated as black box and the retrieval component is added as
a potentially tuneable module.
As shown in Figure 2, given an input context, REPLUG first
retrieves a small set of relevant documents from an external
corpus using a retriever (§3.1). Then we pass the concate-
nation of each retrieved document with the input context
through the LM in parallel, and ensemble the predicted
probabilities (§3.2).
3.1. Document Retrieval
Given an input context x, the retriever aims to retrieve a
small set of documents from a corpus D = {d1...dm} that
are relevant to x. Following prior work (Qu et al., 2021;
Izacard & Grave, 2021b; Ni et al., 2021), we use a dense
retriever based on the dual encoder architecture, where an
encoder is used to encode both the input context x and the
document d. Specifically, the encoder maps each document
d ∈ D to an embedding E(d) by taking the mean pooling of
the last hidden representation over the tokens in d. At query
time, the same encoder is applied to the input context x to
obtain a query embedding E(x). The similarity between the
query embedding and the document embedding is computed
by their cosine similarity:
s(d, x) = cos(E(d), E(x)) (1)
The top-k documents that have the highest similarity scores
when compared with the input x are retrieved in this step.
For efficient retrieval, we precompute the embedding of
each document d ∈ D and construct FAISS index (Johnson
et al., 2019) over these embeddings.
3.2. Input Reformulation
The retrieved top- k documents provide rich information
about the original input context x and can potentially help
the LM to make a better prediction. One simple way to
incorporate the retrieved documents as part of the input to
the LM is to prepend x with all k documents. However, this
simple scheme is fundamentally restricted by the number
of documents (i.e., k) we can include, given the language
model’s context window size. To address this limitation, we
adopt an ensemble strategy described as follows. Assume
D′ ⊂ Dconsists of k most relevant documents to x, ac-
cording to the scoring function in Eq. (1). We prepend each
document d ∈ D′ to x, pass this concatenation to the LM
separately, and then ensemble output probabilities from all
k passes. Formally, given the input context x and its top-k
relevant documents D′, the output probability of the next
token y is computed as a weighted average ensemble:
p(y | x, D′) =
X
d∈D′
p(y | d ◦ x) · λ(d, x),
where ◦ denotes the concatenation of two sequences and the
weight λ(d, x) is based on the similarity score between the
document d and the input context x:
λ(d, x) = es(d,x)
P
d∈D′ es(d,x)
Although our ensemble method requires running the LM
k times, the cross attention is performed between each re-
trieved document and the input context. Therefore, com-
pared with the method of prepending all the retrieved docu-



Source: data\tc16_2312.10997v5\referenced_papers\[34]_2311.08377.pdf (Page 0):

Learning to Filter Context for Retrieval-Augmented Generation
Zhiruo Wang♠ Jun Araki♦ Zhengbao Jiang♠
Md Rizwan Parvez♦ Graham Neubig♠
♠Carnegie Mellon University ♦Bosch Research
{zhiruow,zhengbaj,gneubig}@cs.cmu.edu
Abstract
On-the-fly retrieval of relevant knowledge has
proven an essential element of reliable systems
for tasks such as open-domain question answer-
ing and fact verification. However, because re-
trieval systems are not perfect, generation mod-
els are required to generate outputs given par-
tially or entirely irrelevant passages. This can
cause over- or under-reliance on context, and
result in problems in the generated output such
as hallucinations. To alleviate these problems,
we propose FILCO, a method that improves the
quality of the context provided to the genera-
tor by (1) identifying useful context based on
lexical and information-theoretic approaches,
and (2) training context filtering models that
can filter retrieved contexts at test time. We ex-
periment on six knowledge-intensive tasks with
FLAN -T5 and LLAMA 2, and demonstrate that
our method outperforms existing approaches on
extractive question answering (QA), complex
multi-hop and long-form QA, fact verification,
and dialog generation tasks. FILCO effectively
improves the quality of context, whether or not
it supports the canonical output.1
1 Introduction
Retrieval augmented approaches to generation
have been shown effective for many knowledge-
intensive language tasks such as open-domain ques-
tion answering and fact verification, producing
more faithful (Khandelwal et al., 2020; Lewis et al.,
2020; Shuster et al., 2021; Komeili et al., 2022),
interpretable (Guu et al., 2020), and generalizable
(Khandelwal et al., 2021) outputs. While the de
facto approach is to provide the top retrieved pas-
sages to the generator indiscriminately, imperfect
retrieval systems often return irrelevant or distract-
ing content. Generation models are then trained to
produce canonical outputs with the guidance of par-
tially or entirely irrelevant passages, and thus are
prone to hallucination or spurious memorization.
1https://github.com/zorazrw/filco
Generator
infrastructure necessary for rapid industrial growth was 
put in place. The ﬁrst railway in Belgium, running from 
northern Brussels to Mechelen, was completed in May 
1835. The earliest railway in Britain was a wagonway 
system, a horse drawn wooden rail system, used by 
German miners at Caldbeck, Cumbria, England, perhaps 
from the 1560s. A wagonway was built at Prescot, near 
Liverpool, sometime around 1600, possibly as early as 
1594. Owned by Philip Layton, the line carried coal from 
a pit near Prescot Hall to a terminus about half a mile 
away. On 26 July 1803, Jessop opened the Surrey Iron
    Retrieved Passage
Question
When did the 
ﬁrst train run 
in England?
1835
1560sThe earliest railway in Britain was a wagonway system, a 
horse drawn wooden rail system, used by German miners 
at Caldbeck, Cumbria, England, perhaps from the 1560s. 
    Distilled Content
Figure 1: FILCO filters out irrelevant content (marked
in red) and leaves precisely supporting content, making
it easier for the generator to predict the correct answer.
Ideally, a model should be grounded on the pre-
cisely supporting content to generate the correct
output. However, this ideal grounding is hard to
achieve with an imperfect retrieval system alone.
On one hand, positive passages (i.e., passages that
support the output) sometimes contain distracting
content. For example in Figure 1, while the passage
containing the actual supporting content is success-
fully retrieved, the model still fails to pay sufficient
attention to the supporting content, and is distracted
by surrounding sentences that share similar topics
(Shi et al., 2023). On the other hand, models learn
to over-utilize negative passages in the same way
as using positive passages, e.g., extracting a span
from the irrelevant passage, which would inevitably
be incorrect. This potentially degrades accuracy, as
training with higher-quality context often leads to
better performance (Dou et al., 2021).
Some works have attempted to optimize the pro-
vided content on the passage level, by reranking
more relevant passages rise to the top of the re-
trieved list (Wang et al., 2018; Nogueira and Cho,
2020; Mao et al., 2021), selecting only evidential
passages to include (Asai et al., 2022), or only re-
trieving passages when generation models need
assistance (Mallen et al., 2023; Jiang et al., 2023).
Choi et al. (2021) proposed to decontextualize sen-
arXiv:2311.08377v1  [cs.CL]  14 Nov 2023



Source: data\tc16_2312.10997v5\referenced_papers\[168]_2311.08147.pdf (Page 7):

0.815 
0.822 
0.717 
0.726 
0.765 
0.546 
QA-AQA-NATGright answerswrong answers
(a) EventKG
0.811 
0.805 
0.710 
0.753 
0.667 
0.544 
QA-AQA-NATGright answerswrong answers
(b) UJ
Figure 3: The confidence of ChatGLM2 on its right and
wrong answers, respectively.
ternet and finally generate wrong answers for user
queries. The results also prove that the problem we
identify in this paper cannot be solved by existing
methods and deserves further studies in the future.
5 Analyses
5.1 Observation on Model Confidence
To study the influence of counterfactual contexts on
models’ final outputs quantitatively, we compare
the average generation probabilities of all tokens
in wrong answers and right answers generated by
ChatGLM2, respectively. For QA-A and QA-NA,
we only consider the samples that the model can
answer correctly with no contexts provided but will
give wrong predictions when given counterfactual
information. For text generation, we compare the
probabilities of original words/phrases and those
of edited words/phrases.
The results are shown in Figure 3. We observe
that the model’s confidence in its answers will sig-
nificantly drop with the interference of counterfac-
tual contexts, especially for text generation. This
phenomenon suggests that it is plausible to guide
models to generate accurate responses according
to the model’s confidence in future research.
5.2 Case Study
We can observe from Table 5 and Table 6 that the
prompt method achieves an improvement in QA-
A but causes a drop in performance in QA-NA.
A typical case, in which the model answers the
question incorrectly after adding a new sentence
Context
The Trial of Joan of Arc took place during the 15th centuryand was a legal proceeding against Joan of Arc. Joan of Arcwas a French military leader who served under Charles VIIduring the Hundred Years’ War. The trial began on January 9,1431, and lasted until May 29, 1431. (The trial began onJanuary 10, 1432, and lasted until May 30, 1432) Joan of Arcwas the defendant in the trial, while Pierre Cauchon acted asboth the prosecutor and the judge. She was charged with heresy.
Question Who was the judge in the Trial of Joan of Arc?Options 1) Pierre Cauchon; 2) Jean d’Estivet;Original Answer1) Pierre Cauchon;Prompt Answer2) Jean d’Estivet;
Table 7: A case where ChatGLM2 gives a wrong answer
with edited contexts provided. The sentence in red is the
edited one with time information changed. The answer
to the question appearing in the context is in blue.
into the prompt while it could originally give a right
prediction, is shown in Table 7. In this case, we
edit the beginning and end time of the trial, which
will not affect the answer to the question about
the judge of the trial. After we add a sentence
that instructs the model to neglect counterfactual
information in the context, the model changes its
answer from the right one Pierre Cauchon, which
has been clearly stated in the context, to another
name that even does not appear in the context.
We speculate that the instruction we add to the
prompt will reduce models’ trust in the external
knowledge. In QA-A, the models will thus dis-
believe the edited wrong answer in the context.
However, in QA-NA, models’ confidence in the
right answer appearing in the context will also fall
due to the instruction. This case indicates that it is
challenging to solve this problem by just modifying
the prompt.
6 Related Works
Hallucination in LLMs Although LLMs excel
at generating fluent natural language, studies show
that they are subject to the problem of hallucination,
which means that texts generated by the models of-
ten contain information that is irrelevant to user
inputs, conflicting with previous responses, or un-
faithful to established world knowledge (Ji et al.,
2022a; Rawte et al., 2023; Zhang et al., 2023b;
Huang et al., 2023). Some studies aim to mitigate
the issue of hallucination by incorporating addi-
tional information into the generation procedure,
such as Web corpora (Shuster et al., 2021; Huo
et al., 2023; Yu et al., 2023), knowledge graphs (Ji
et al., 2022b), and external tools (Gou et al., 2023).
Another line of work focuses on improving the de-
coding strategy of LLMs, such as careful prompt
design (Mündler et al., 2023), sampling multiple



Source: data\tc16_2312.10997v5\referenced_papers\[105]_2310.01558.pdf (Page 2):

Published as a conference paper at ICLR 2024
0
20
40
60
80
NQ
(Single-hop)
Bamboogle
(Explicit Multi-hop)
2WikiMQA
(Explicit Multi-hop)
StrategyQA
(Implicit Multi-hop)
Fermi
(Implicit Multi-hop)
No Retrieval RALM (Top-1 Retrieval) RALM (Random Retrieval)
Figure 2: Accuracy for Llama-2-13B few-shot prompted on five QA tasks, in three settings: (a)
without retrieval, (b) with top-1 retrieval from a strong search engine, and (c) with a randomly-
retrieved passage. Retrieval augmentation can boost performance, but even strong retrieval hurts
performance on StrategyQA and Fermi, and random contexts reduce performance dramatically.
2 M AKING RALM S ROBUST TO IRRELEVANT CONTEXTS
We now present our methods for building RALMs that are robust to irrelevant contexts. We begin
by describing the common approach for incorporating evidence into RALMs. Next, we explore
a natural baseline for using an NLI model to identify irrelevant contexts. Last, we describe our
procedure for finetuning models to be robust to irrelevant context.
In-context RALMs Language models define a probability distribution over sequences of to-
kens, with auto-regressive models assigning a probability via next-token prediction: pLM =
Πn
i=1pθ(xi|x<i), where x<i is the sequence of tokens preceding xi at each step and θ denotes
the parameters of the LM. For RALMs, we follow the definition of in-context RALMs from Ram
et al. (2023), where context sentences are retrieved from a corpus C, and generation is con-
ditioned on the retrieved context. Given the retrieval operation RC, this can be formalized as
pRALM = Π n
i=1pθ(xi|RC(x<i); x<i), where [RC(x<i); x<i] denotes the concatenation of the re-
trieved evidence with the generated sequence. Generation in LMs and RALMs can also be condi-
tioned on additional input, which we omit for brevity.
In our setting, we focus on RALMs for ODQA. We follow recent approaches such as Self-Ask and
IR-CoT (Press et al., 2023; Trivedi et al., 2023; Yoran et al., 2023), for interleaving retrieval with
multi-hop question answering (see Fig. 3). Retrieval is performed for every intermediate question
and each context is prepended to the question. In the single-hop setting, the model has to generate
the answer given a question and retrieved context. In the multi-hop setting, the model has to generate
intermediate questions and answers until arriving at the final answer and the retriever is called for
the original question and after each intermediate question. Formally, x in this case is the generated
decomposition until an intermediate step andRC(x) are the retrieved contexts for all questions inx.
2.1 I DENTIFYING IRRELEVANT CONTEXTS WITH NLI MODELS .
NLI models (Dagan et al., 2006; Bowman et al., 2015) classify whether a textual hypothesis is
entailed, neutral, or contradicted given a textualpremise. Recent work successfully used NLI models
to automatically identify hallucinations (Honovich et al., 2022) and statement attribution (Bohnet
et al., 2023) when presented with a context and generated text. Similarly, a natural baseline is to
frame irrelevant context identification as an NLI problem, by using the retrieved context only when
the hypothesis (i.e., final answer and intermediate question-answer pairs; Fig. 3) are classified as
entailed by the premise (i.e., the retrieved context). We use a simple back-off strategy where we
generate twice, once with pLM and once with pRALM , and only use the RALM if the NLI model
classified all generated answers (and intermediate questions) as entailed by the retrieved evidence.
3



### Claim 10/179

#### Claim Text
Lastly, the Task Adapter module tailors RAG to various downstream tasks, automating prompt retrieval for zero-shot inputs and creating task-specific retrievers through few-shot query generation [20], [21] .This comprehensive approach not only streamlines the retrieval process but also significantly improves the quality and relevance of the information retrieved, catering to a wide array of tasks and queries with enhanced precision and flexibility. 2) New Patterns: Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration to address specific challenges.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[21]_2209.11755.pdf (Page 1):

Retrieval Performance on 11 BEIR Datasets
46.6 45.5
47.8
49.9
52.8
Promptagator++ Zero-shot
Supervised (MS MARCO) *
Promptagator Zero-shot
Promptagator Few-shot
Promptagator++ Few-shot
Figure 1: Few-shot retrieval with PROMPTAGATOR . Left (a): Retrieval tasks from BEIR differ in query
distribution, retrieval corpus, and search intents. Middle (b): Most prior work uses supervised setting
(2) which trains model on a large QA retrieval datasets and transfer to other retrieval tasks. Right
(c): Few-shot PROMPTAGATOR performance. Average nDCG@10 on 11 datasets from BEIR from our
PROMPTAGATOR models and previously MS MARCO-supervised models (SPLADE v2).
domain. Moreover, different tasks have distinct distributions of queries even when their search intents
are similar. For example, in the BEIR benchmark, queries in HotpotQA (Yang et al., 2018) are long
compositional questions, while queries in FiQA (Maia et al., 2018) are short ﬁnancial questions.
In this paper, we advocate to work on the setting of Few-shot Retrieval for diverse retrieval (§2),
where each task comes with a short description and a few annotated examples to clearly illustrate
the search intents. Given that only a few examples are available, we propose Prompt-base Query
Generation for Retriever (PROMPTAGATOR ) (§3) which aims to resolve the data scarcity issue while
retaining the efﬁciency of a small dual encoder, by harnessing the power of large language models
(LLM) such as FLAN (Wei et al., 2022a). PROMPTAGATOR combines prompting with LLMs as a
query generator without ﬁne-tuning (§3.1), and can generate good queries with minimal supervision
– shown in Figure 1(b), it solely relies on a few supervised examples from the target task without
using annotated query-document pairs from Natural Questions (Kwiatkowski et al., 2019) or MS
MARCO (Nguyen et al., 2016) to train the retriever directly. The key insight of PROMPTAGATOR is to
amplify the power of few-shot examples by creating task-speciﬁc prompting, which in turn enables
generating a large set of synthetic queries for training retrievers suited for the task. To ensure the
generated data quality, we develop a ﬁltering technique that ensures round-trip consistency using
generated data only (§3.2). Our ﬁlter is tailored to retrieval, which removes ambiguous, generic, and
low-quality questions, and signiﬁcantly improves retrieval performance.
While PROMPTAGATOR is not the ﬁrst application of LLM for retrieval, prior attempts of using LLMs
often come with higher serving cost. Neelakantan et al. (2022) proposes to use the GPT-3 (Brown
et al., 2020) embeddings in dual encoder models. However, the embedding size is 12k and hence
makes the search index footprint and inference cost high. Sachan et al. (2022) and Bonifacio et al.
(2022) have applied prompting and LLMs for reranking, while leaving the retriever untouched. With
PROMPTAGATOR , we show that LLMs can be used to generate efﬁcient end-to-end retriever with high
accuracy. The contributions of the paper are as follows:
• We analyze the previously overlooked differences across retrieval tasks in their search intents
and query distributions, and propose a Few-Shot Retrieval setting for the BEIR dataset. Our
prompt and fewshot examples will be released to facilitate future research.
• We propose PROMPTAGATOR , a simple recipe for few-shot retrieval by prompting with
a LLM to generate synthetic task-speciﬁc training data. For the ﬁrst time, end-to-end
retrievers solely based on a few supervised examples can be strong and efﬁcient to serve
with PROMPTAGATOR .
• Our experimental results show that, surprisingly, PROMPTAGATOR with two-to-eight ex-
amples produced signiﬁcantly better retrievers compared to recent models trained on MS
MARCO or NQ that have over 500K human annotated examples (Figure 1(c)). PROMPTA -
GATOR outperforms ColBERT v2 and SPLADE v2 on 11 retrieval tasks we tested, while
reranking boosts results by another 5 points on standard retrieval evaluation metric.
2



Source: data\tc16_2312.10997v5\referenced_papers\[21]_2209.11755.pdf (Page 0):

PROMPTAGATOR
 : F EW-SHOT DENSE RETRIEVAL
FROM 8 EXAMPLES
Zhuyun Dai∗†, Vincent Y. Zhao∗†, Ji Ma∗†, Yi Luan∗†, Jianmo Ni, Jing Lu, Anton Bakalov,
Kelvin Guu, Keith B. Hall and Ming-Wei Chang†
Google Research
{zhuyundai, vzhao, maji, luanyi, mingweichang}@google.com
∗equal contributions †corresponding authors
ABSTRACT
Much recent research on information retrieval has focused on how to transfer
from one task (typically with abundant supervised data) to various other tasks
where supervision is limited, with the implicit assumption that it is possible to
generalize from one task to all the rest. However, this overlooks the fact that
there are many diverse and unique retrieval tasks, each targeting different search
intents, queries, and search domains. In this paper, we suggest to work on Few-shot
Dense Retrieval, a setting where each task comes with a short description and
a few examples. To amplify the power of a few examples, we propose Prompt-
base Query Generation for Retriever (PROMPTAGATOR
 ), which leverages large
language models (LLM) as a few-shot query generator, and creates task-speciﬁc
retrievers based on the generated data. Powered by LLM’s generalization ability,
PROMPTAGATOR makes it possible to create task-speciﬁc end-to-end retrievers solely
based on a few examples without using Natural Questions (Kwiatkowski et al.,
2019) or MS MARCO (Nguyen et al., 2016) to train dual encoders. Surprisingly,
LLM prompting with no more than 8 examples allows dual encoders to outperform
heavily engineered models trained on MS MARCO like ColBERT v2 (Santhanam
et al., 2022) by more than 1.2 nDCG on average on 11 retrieval sets. Further
training standard-size re-rankers using the same generated data yields another 5.0
point nDCG improvement. Our studies determine that query generation can be
far more effective than previously observed, especially when a small amount of
task-speciﬁc knowledge is given.
1 I NTRODUCTION
Recently, major progress has been made on neural retrieval models such as dual encoders, which can
retrieve knowledge from a large collection of documents containing millions to billions of passages
(Yih et al., 2011; Lee et al., 2019; Karpukhin et al., 2020). However, Thakur et al. (2021) recently
proposed the BEIR heterogeneous retrieval benchmark, and showed that it is still difﬁcult for neural
retrievers to perform well on a wide variety of retrieval tasks that lack dedicated training data. Thus,
previous approaches focus on transferring knowledge from question answering (QA) datasets such
as MS MARCO (Nguyen et al., 2016). To best transfer from QA datasets, expressive retrievers
are developed that allow ﬁne-grained token-level interaction such as ColBERT (Khattab & Zaharia,
2020; Santhanam et al., 2022) and SPLADE (Formal et al., 2021) but with higher inference cost.
Data augmentation via synthetic question generation has previously been explored (Ma et al., 2021;
Shakeri et al., 2020), but these question generators are typically only trained on popular QA datasets.
We argue that it is hard to expect models based on one or two QA datasets to perform well across
different retrieval tasks. First, different retrieval tasks have very different search intents; in other
words, different deﬁnitions of “relevance”. For example, as illustrated in Figure 1(a), both Dbpedia-
Entity (Hasibi et al., 2017) and FEVER (Thorne et al., 2018) are tasks to retrieve documents from
Wikipedia. Dbpedia-Entity is a task to retrieve entities that are mentioned in the query, while
FEVER is a task to ﬁnd evidence that either supports or refutes a given statement. Which document
is relevant to the query can be very different from one task to another task even if they share the same
1
arXiv:2209.11755v1  [cs.CL]  23 Sep 2022



Source: data\tc16_2312.10997v5\referenced_papers\[20]_2303.08518.pdf (Page 8):

Method 0-SHOT UPRISE FEW-SHOT UPRISE-REMAIN-TARGET UPRISE-ALL-TARGET
# Demos 0 3 3 3 3
Training Data - Remaining Task Types - Remaining Task Types All Task Types
Prompt Pool - Remaining Task Types Target Task Target Task Target Task
Read. 31.6 40.1 37.4 48.8 47.4
Close-QA 19.2 23.3 25.1 28.1 28.9
Paraphrase 47.0 61.6 59.1 61.9 73.4
NLI 38.3 43.0 43.4 52.1 72.4
Sentiment 62.7 61.8 72.7 68.7 82.9
Table 4: Comparative results with few-shot prompting. # Demos is the number of demonstrations prepended to the
input instruction, FEW-SHOT is vanilla few-shot prompting where the demonstrations are randomly sampled from
the training demonstrations of the target task (Brown et al., 2020).
Senti.NLIPara.Closed.Read.
✓✓✓✓-Read.
✓✓✓-✓Closed.
✓✘-✓✘Para.
✘-✓✓✓NLI
-✘✓✓✓Senti.
GENERALIZABLE TO
TRAINING TASK
Figure 8: Generablizability of each task type, ✓ means
the performance of prompt retrieval is better than 0-
SHOT .
Generalizability of each task type. We then
reduce the number of trained tasks to only one to
test its generalizability. Specifically, for each task
type, we train a retriever on this type alone and
then evaluate on the remaining task types. For ex-
ample, if the retriever trained on A outperforms
0-SHOT when testing on B, we regard task type
A is generalizable to task type B . The results in
Figure 8 demonstrate that tasks with diverse ques-
tion/answer types, such as Reading Comprehension
and Closed-book QA, tend to be more generaliz-
able and can serve as representative choices for
training a universal retriever.
9 Exploration of Few-Shot Learning
We compare UPRISE with vanilla few-shot prompt-
ing and apply UPRISE to few-shot prompt retrieval
in Table 4: (1) Comparing UPRISE with FEW-
SHOT , UPRISE approaches and even outperforms
vanilla few-shot prompting on most task types;
(2) UPRISE -REMAIN -TARGET , using the retriever
trained on remaining tasks to retrieve in the target
task pool, outperforms vanilla few-shot prompt-
ing. (3) Substantial improvements are then ob-
served with UPRISE -ALL -TARGET , a unified re-
triever trained on all task types. These findings
emphasize UPRISE ’s effectiveness as a comprehen-
sive method for both zero-shot and few-shot prompt
retrieval.
10 Related Work
Our work is related to prompt engineering meth-
ods including prompt design, prompt tuning, and
prompt search. Here we discuss prompt search
that relates most closely to our work and describe
prompt design and prompt tuning in Appendix E.
Prompt search involves searching for prompts
from pre-training corpora or downstream tasks to
construct the input text (Gao et al., 2021; Liu et al.,
2022; Rubin et al., 2022; Ye et al., 2023, 2022).
To retrieve prompts for the test examples, retriev-
ers such as the sparse retriever BM25 (Robertson
and Zaragoza, 2009) and the dense retriever based
on SBERT (Reimers and Gurevych, 2019) are em-
ployed. Furthermore, methods like EPR (Rubin
et al., 2022) and CEIL (Ye et al., 2023) use the
LLM itself to score the searched prompts, thereby
eliminating the need for manual prompt engineer-
ing and ensuring prompting performance.
11 Conclusion
This paper explores training a lightweight and ver-
satile prompt retriever to improve the zero-shot per-
formance of LLMs. We investigate the retriever’s
ability to generalize from the trained task types to
unseen task types, and from a small LLM to differ-
ent LLMs of much larger scales. We hope our paper
will spur further research on developing a universal
assistant for the ever-expanding landscape of tasks
and large language models.



Source: data\tc16_2312.10997v5\referenced_papers\[21]_2209.11755.pdf (Page 2):

2 F EW-SHOT RETRIEVAL TASK
In this section, we ﬁrst introduce the deﬁnition of a retrieval task and the differences among different
retrieval tasks. We then propose a new Few-Shot Retrieval setting for the BEIR benchmark.
2.1 R ETRIEVAL TASK
Given a large corpus, a retrieval model is responsible to ﬁnd documents that are most relevant to a
provided query q according to a pre-deﬁned relevancy. Formally, we deﬁne a retrieval task as:
T = {D, Q, I},
where D= {d1, d2, ..., dn}is a large corpus of documents for retrieval, Qis a query distribution,
and Iis the underlying search intent for the task. Depending on the task, Dcan be any document
collection, such as web or Wikipedia. Q also varies across tasks, e.g., short keyword search queries,
questions, arguments, etc. If I(q, d) = 1, it means search intent of q has been satisﬁed by the
document d. For example, in QA tasks such as Natural Questions (NQ) the search intent is to ﬁnd
passages that provide the answer to the question, meaning INQ(q, d) = 1if d answers q. Importantly,
for the same pair of (q, d), their relevance can be completely different under different search intents.
For example, some argument retrieval tasks look for supporting arguments, while other tasks need to
retrieve counter arguments.
In this work, we target the scenario where a target retrieval corpus DT is available, but the amount of
annotated query-document pairs for the new task is limited. Most prior of research efforts were put
into adapting retrievers to new corpus DT, but the divergence in queries QT and intents IT remains
under-explored. Next, we explore how search intent can be expressed with a short description and
very few number of examples.
2.2 F EW-SHOT BEIR S ETTING
In this paper, we argue that it is important to let retrievers be aware of task-speciﬁc query distribution
and search intent, as opposed to merely focusing on the domain adaptation of D. Prior belief is that it
is expensive to collect enough in-distribution queries and relevance labels to train a neural retriever,
but intuitively, a person can understand a retrieval task by reading a short instruction and going over a
few examples. In this work, we ask if a few (8 or fewer) examples are sufﬁcient for the machines
to learn a task-speciﬁc retriever. To facilitate our study and future research of few-shot retrieval,
we deﬁne a new few-shot retrieval evaluation setting built upon the BEIR heterogeneous retrieval
benchmark (Thakur et al., 2021).
BEIR has 18 information retrieval datasets across 9 domains, including Bio-Medical, Finance, News,
Twitter, Wikipedia, StackExchange, Quora, Scientiﬁc, and Misc. These datasets also cover a diverse
range of search intents: QA retrieval (question-to-document), duplicate question discovery (question-
to-question), fact checking (claim-to-document), etc. Following Santhanam et al. (2022) and Formal
et al. (2021), we narrow our focus to the publicly-available datasets in BEIR. The original BEIR
evaluation used a zero-shot set up, where no queries or relevant query-document pairs from the
evaluation datasets can be used to train the retrievers.
We extend BEIR to the few-shot setting by randomly taking a few (2 to 8) in-domain relevant query-
document examples as the task-speciﬁc supervision. The examples are sampled from the development
set when it is available. For the BEIR tasks which only have a test set, we use samples from the test
data as few-shot examples. To make the evaluation fair, when evaluating few-shot retriever models,
these test-set examples should be treated as ‘failed to retrieve‘ even if the model successfully retrieves
them. The prompt and few-shot examples will be released to the public.
3 P ROMPTAGATOR
To approach the goal of creating retrievers from few-shot examples, we propose Prompt-base Query
Generation for Retriever (PROMPTAGATOR ). The key idea of PROMPTAGATOR is to transform the few
examples into many more examples by prompting a LLM, instead of using them to train a retriever
directly.
3



Source: data\tc16_2312.10997v5\referenced_papers\[20]_2303.08518.pdf (Page 1):

seen task types during inference. In addition,
we demonstrate that the cross-task capabilities
can generalize well from a small LLM to dif-
ferent LLMs of much larger scales: we use
GPT-Neo-2.7B (Black et al., 2021) to guide
the tuning of the retriever and evaluate the re-
triever’s performance on BLOOM-7.1B (Scao
et al., 2022), OPT-66B (Zhang et al., 2022),
and GPT3-175B (Brown et al., 2020). The
cross-model and cross-task generalization of UP-
RISE makes it a promising and practical solution
for real-world applications.
Furthermore, our approach demonstrates the po-
tential for enhancing even the most powerful LLMs,
as shown in our experiments with ChatGPT. De-
spite its impressive abilities, ChatGPT has been
found to struggle with serious hallucination prob-
lems, leading to responses that are factually inac-
curate (Bang et al., 2023). However, UPRISE is
able to address this issue on fact-checking tasks
by prompting the model to draw correct inferences
from its built-in knowledge.
In summary, our contributions include:
• We introduceUPRISE , a lightweight and versatile
approach to improve zero-shot performance of
LLMs in the cross-task and cross-model scenario.
• UPRISE is tuned with GPT-Neo-2.7B, but
can also benefit different LLMs of much larger
scales, such as BLOOM-7.1B, OPT-66B, and
GPT3-175B.
• Our exploration on ChatGPT demonstrates the
potential of UPRISE in improving performances
of even the strongest LLMs.
2 Problem Definition
We aim to improve zero-shot performance of LLMs
by training a prompt retriever to retrieve prompts1
for any given task input. Specifically, UPRISE de-
composes the prompting process into two steps:
retrieve then predict. Given an input x, we first
retrieve a set of positive prompts P+ from a pre-
constructed pool P:
P+ = R(x, P). (1)
Then we concatenate P+ with x to form an input
sequence for a frozen LLM, which generates a pre-
dicted output:
yP+
= LM

yP+
|P+ ⊕ x

. (2)
1"Prompt" sometimes refers to a natural language template
filled by an input example, but here it denotes the sequence
prepended to the task input.
Engineered Natural Language Prompt
🧊Language Model
TaskInput
🧊Summarizethe…
Prompt Design
Tunable SoftPrompt
🔥0.10,0.05,…
Prompt TuningRetrieved NaturalLanguage Prompt
Prompt Retrieval
🧊Language Model
🧊Language Model
🧊Answerthis…
🔥Prompt RetrieverTunable Prompt Retrieverbackward
Tune
TaskInput
TaskInput
Figure 2: Typical prompt engineering methods and
prompt retrieval. Prompt retrieval prepends a natural
language prompt to the task input and uses a frozen
LLM to evaluate the prompt’s performance. The ob-
tained evaluation is then used to tune the retriever in a
reverse manner.
Our objective is to optimize performance of yP+
to match the target y by updating the retriever R.
Figure 2 compares prompt retrieval with typical
prompt engineering methods: prompt design adds
an engineered natural language prompt (Brown
et al., 2020; Wei et al., 2022b) and prompt tun-
ing tunes a soft prompt (Liu et al., 2021; Lester
et al., 2021). In contrast, prompt retrieval tunes
a retriever to retrieve natural language prompts,
which is both interpretable and flexible. It uses
the language model itself to label each prompt in
the pool as positive/negative, and then tunes a re-
triever from this signal (Rubin et al., 2022). Such
fine-tuned prompt retrieval has demonstrated effec-
tiveness in the task-specific scenario (Rubin et al.,
2022; Ye et al., 2023): a prompt retriever is tuned
on one or multiple specific tasks using the train-
ing sets as the prompt pool. The retriever is then
evaluated on the corresponding testing sets.
Our work is to achieve universality of the prompt
retriever, which means the fine-tuned retriever can
be directly used to retrieve prompts for unseen tasks
and various inference LLMs, without the need for
further tuning. We define the universality from two
perspectives: cross-task retrieval and cross-model
retrieval.
Cross-task retrieval. Considering the diversity
of tasks in real-world applications, we propose
cross-task retrieval to retrieve for task types on
which the prompt retriever has not been trained.
We simulate this setting by evaluating the prompt
retriever on unseen task types: various tasks are



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[21]_2209.11755.pdf (Page 1):

Retrieval Performance on 11 BEIR Datasets
46.6 45.5
47.8
49.9
52.8
Promptagator++ Zero-shot
Supervised (MS MARCO) *
Promptagator Zero-shot
Promptagator Few-shot
Promptagator++ Few-shot
Figure 1: Few-shot retrieval with PROMPTAGATOR . Left (a): Retrieval tasks from BEIR differ in query
distribution, retrieval corpus, and search intents. Middle (b): Most prior work uses supervised setting
(2) which trains model on a large QA retrieval datasets and transfer to other retrieval tasks. Right
(c): Few-shot PROMPTAGATOR performance. Average nDCG@10 on 11 datasets from BEIR from our
PROMPTAGATOR models and previously MS MARCO-supervised models (SPLADE v2).
domain. Moreover, different tasks have distinct distributions of queries even when their search intents
are similar. For example, in the BEIR benchmark, queries in HotpotQA (Yang et al., 2018) are long
compositional questions, while queries in FiQA (Maia et al., 2018) are short ﬁnancial questions.
In this paper, we advocate to work on the setting of Few-shot Retrieval for diverse retrieval (§2),
where each task comes with a short description and a few annotated examples to clearly illustrate
the search intents. Given that only a few examples are available, we propose Prompt-base Query
Generation for Retriever (PROMPTAGATOR ) (§3) which aims to resolve the data scarcity issue while
retaining the efﬁciency of a small dual encoder, by harnessing the power of large language models
(LLM) such as FLAN (Wei et al., 2022a). PROMPTAGATOR combines prompting with LLMs as a
query generator without ﬁne-tuning (§3.1), and can generate good queries with minimal supervision
– shown in Figure 1(b), it solely relies on a few supervised examples from the target task without
using annotated query-document pairs from Natural Questions (Kwiatkowski et al., 2019) or MS
MARCO (Nguyen et al., 2016) to train the retriever directly. The key insight of PROMPTAGATOR is to
amplify the power of few-shot examples by creating task-speciﬁc prompting, which in turn enables
generating a large set of synthetic queries for training retrievers suited for the task. To ensure the
generated data quality, we develop a ﬁltering technique that ensures round-trip consistency using
generated data only (§3.2). Our ﬁlter is tailored to retrieval, which removes ambiguous, generic, and
low-quality questions, and signiﬁcantly improves retrieval performance.
While PROMPTAGATOR is not the ﬁrst application of LLM for retrieval, prior attempts of using LLMs
often come with higher serving cost. Neelakantan et al. (2022) proposes to use the GPT-3 (Brown
et al., 2020) embeddings in dual encoder models. However, the embedding size is 12k and hence
makes the search index footprint and inference cost high. Sachan et al. (2022) and Bonifacio et al.
(2022) have applied prompting and LLMs for reranking, while leaving the retriever untouched. With
PROMPTAGATOR , we show that LLMs can be used to generate efﬁcient end-to-end retriever with high
accuracy. The contributions of the paper are as follows:
• We analyze the previously overlooked differences across retrieval tasks in their search intents
and query distributions, and propose a Few-Shot Retrieval setting for the BEIR dataset. Our
prompt and fewshot examples will be released to facilitate future research.
• We propose PROMPTAGATOR , a simple recipe for few-shot retrieval by prompting with
a LLM to generate synthetic task-speciﬁc training data. For the ﬁrst time, end-to-end
retrievers solely based on a few supervised examples can be strong and efﬁcient to serve
with PROMPTAGATOR .
• Our experimental results show that, surprisingly, PROMPTAGATOR with two-to-eight ex-
amples produced signiﬁcantly better retrievers compared to recent models trained on MS
MARCO or NQ that have over 500K human annotated examples (Figure 1(c)). PROMPTA -
GATOR outperforms ColBERT v2 and SPLADE v2 on 11 retrieval tasks we tested, while
reranking boosts results by another 5 points on standard retrieval evaluation metric.
2



Source: data\tc16_2312.10997v5\referenced_papers\[21]_2209.11755.pdf (Page 0):

PROMPTAGATOR
 : F EW-SHOT DENSE RETRIEVAL
FROM 8 EXAMPLES
Zhuyun Dai∗†, Vincent Y. Zhao∗†, Ji Ma∗†, Yi Luan∗†, Jianmo Ni, Jing Lu, Anton Bakalov,
Kelvin Guu, Keith B. Hall and Ming-Wei Chang†
Google Research
{zhuyundai, vzhao, maji, luanyi, mingweichang}@google.com
∗equal contributions †corresponding authors
ABSTRACT
Much recent research on information retrieval has focused on how to transfer
from one task (typically with abundant supervised data) to various other tasks
where supervision is limited, with the implicit assumption that it is possible to
generalize from one task to all the rest. However, this overlooks the fact that
there are many diverse and unique retrieval tasks, each targeting different search
intents, queries, and search domains. In this paper, we suggest to work on Few-shot
Dense Retrieval, a setting where each task comes with a short description and
a few examples. To amplify the power of a few examples, we propose Prompt-
base Query Generation for Retriever (PROMPTAGATOR
 ), which leverages large
language models (LLM) as a few-shot query generator, and creates task-speciﬁc
retrievers based on the generated data. Powered by LLM’s generalization ability,
PROMPTAGATOR makes it possible to create task-speciﬁc end-to-end retrievers solely
based on a few examples without using Natural Questions (Kwiatkowski et al.,
2019) or MS MARCO (Nguyen et al., 2016) to train dual encoders. Surprisingly,
LLM prompting with no more than 8 examples allows dual encoders to outperform
heavily engineered models trained on MS MARCO like ColBERT v2 (Santhanam
et al., 2022) by more than 1.2 nDCG on average on 11 retrieval sets. Further
training standard-size re-rankers using the same generated data yields another 5.0
point nDCG improvement. Our studies determine that query generation can be
far more effective than previously observed, especially when a small amount of
task-speciﬁc knowledge is given.
1 I NTRODUCTION
Recently, major progress has been made on neural retrieval models such as dual encoders, which can
retrieve knowledge from a large collection of documents containing millions to billions of passages
(Yih et al., 2011; Lee et al., 2019; Karpukhin et al., 2020). However, Thakur et al. (2021) recently
proposed the BEIR heterogeneous retrieval benchmark, and showed that it is still difﬁcult for neural
retrievers to perform well on a wide variety of retrieval tasks that lack dedicated training data. Thus,
previous approaches focus on transferring knowledge from question answering (QA) datasets such
as MS MARCO (Nguyen et al., 2016). To best transfer from QA datasets, expressive retrievers
are developed that allow ﬁne-grained token-level interaction such as ColBERT (Khattab & Zaharia,
2020; Santhanam et al., 2022) and SPLADE (Formal et al., 2021) but with higher inference cost.
Data augmentation via synthetic question generation has previously been explored (Ma et al., 2021;
Shakeri et al., 2020), but these question generators are typically only trained on popular QA datasets.
We argue that it is hard to expect models based on one or two QA datasets to perform well across
different retrieval tasks. First, different retrieval tasks have very different search intents; in other
words, different deﬁnitions of “relevance”. For example, as illustrated in Figure 1(a), both Dbpedia-
Entity (Hasibi et al., 2017) and FEVER (Thorne et al., 2018) are tasks to retrieve documents from
Wikipedia. Dbpedia-Entity is a task to retrieve entities that are mentioned in the query, while
FEVER is a task to ﬁnd evidence that either supports or refutes a given statement. Which document
is relevant to the query can be very different from one task to another task even if they share the same
1
arXiv:2209.11755v1  [cs.CL]  23 Sep 2022



Source: data\tc16_2312.10997v5\referenced_papers\[20]_2303.08518.pdf (Page 8):

Method 0-SHOT UPRISE FEW-SHOT UPRISE-REMAIN-TARGET UPRISE-ALL-TARGET
# Demos 0 3 3 3 3
Training Data - Remaining Task Types - Remaining Task Types All Task Types
Prompt Pool - Remaining Task Types Target Task Target Task Target Task
Read. 31.6 40.1 37.4 48.8 47.4
Close-QA 19.2 23.3 25.1 28.1 28.9
Paraphrase 47.0 61.6 59.1 61.9 73.4
NLI 38.3 43.0 43.4 52.1 72.4
Sentiment 62.7 61.8 72.7 68.7 82.9
Table 4: Comparative results with few-shot prompting. # Demos is the number of demonstrations prepended to the
input instruction, FEW-SHOT is vanilla few-shot prompting where the demonstrations are randomly sampled from
the training demonstrations of the target task (Brown et al., 2020).
Senti.NLIPara.Closed.Read.
✓✓✓✓-Read.
✓✓✓-✓Closed.
✓✘-✓✘Para.
✘-✓✓✓NLI
-✘✓✓✓Senti.
GENERALIZABLE TO
TRAINING TASK
Figure 8: Generablizability of each task type, ✓ means
the performance of prompt retrieval is better than 0-
SHOT .
Generalizability of each task type. We then
reduce the number of trained tasks to only one to
test its generalizability. Specifically, for each task
type, we train a retriever on this type alone and
then evaluate on the remaining task types. For ex-
ample, if the retriever trained on A outperforms
0-SHOT when testing on B, we regard task type
A is generalizable to task type B . The results in
Figure 8 demonstrate that tasks with diverse ques-
tion/answer types, such as Reading Comprehension
and Closed-book QA, tend to be more generaliz-
able and can serve as representative choices for
training a universal retriever.
9 Exploration of Few-Shot Learning
We compare UPRISE with vanilla few-shot prompt-
ing and apply UPRISE to few-shot prompt retrieval
in Table 4: (1) Comparing UPRISE with FEW-
SHOT , UPRISE approaches and even outperforms
vanilla few-shot prompting on most task types;
(2) UPRISE -REMAIN -TARGET , using the retriever
trained on remaining tasks to retrieve in the target
task pool, outperforms vanilla few-shot prompt-
ing. (3) Substantial improvements are then ob-
served with UPRISE -ALL -TARGET , a unified re-
triever trained on all task types. These findings
emphasize UPRISE ’s effectiveness as a comprehen-
sive method for both zero-shot and few-shot prompt
retrieval.
10 Related Work
Our work is related to prompt engineering meth-
ods including prompt design, prompt tuning, and
prompt search. Here we discuss prompt search
that relates most closely to our work and describe
prompt design and prompt tuning in Appendix E.
Prompt search involves searching for prompts
from pre-training corpora or downstream tasks to
construct the input text (Gao et al., 2021; Liu et al.,
2022; Rubin et al., 2022; Ye et al., 2023, 2022).
To retrieve prompts for the test examples, retriev-
ers such as the sparse retriever BM25 (Robertson
and Zaragoza, 2009) and the dense retriever based
on SBERT (Reimers and Gurevych, 2019) are em-
ployed. Furthermore, methods like EPR (Rubin
et al., 2022) and CEIL (Ye et al., 2023) use the
LLM itself to score the searched prompts, thereby
eliminating the need for manual prompt engineer-
ing and ensuring prompting performance.
11 Conclusion
This paper explores training a lightweight and ver-
satile prompt retriever to improve the zero-shot per-
formance of LLMs. We investigate the retriever’s
ability to generalize from the trained task types to
unseen task types, and from a small LLM to differ-
ent LLMs of much larger scales. We hope our paper
will spur further research on developing a universal
assistant for the ever-expanding landscape of tasks
and large language models.



Source: data\tc16_2312.10997v5\referenced_papers\[21]_2209.11755.pdf (Page 2):

2 F EW-SHOT RETRIEVAL TASK
In this section, we ﬁrst introduce the deﬁnition of a retrieval task and the differences among different
retrieval tasks. We then propose a new Few-Shot Retrieval setting for the BEIR benchmark.
2.1 R ETRIEVAL TASK
Given a large corpus, a retrieval model is responsible to ﬁnd documents that are most relevant to a
provided query q according to a pre-deﬁned relevancy. Formally, we deﬁne a retrieval task as:
T = {D, Q, I},
where D= {d1, d2, ..., dn}is a large corpus of documents for retrieval, Qis a query distribution,
and Iis the underlying search intent for the task. Depending on the task, Dcan be any document
collection, such as web or Wikipedia. Q also varies across tasks, e.g., short keyword search queries,
questions, arguments, etc. If I(q, d) = 1, it means search intent of q has been satisﬁed by the
document d. For example, in QA tasks such as Natural Questions (NQ) the search intent is to ﬁnd
passages that provide the answer to the question, meaning INQ(q, d) = 1if d answers q. Importantly,
for the same pair of (q, d), their relevance can be completely different under different search intents.
For example, some argument retrieval tasks look for supporting arguments, while other tasks need to
retrieve counter arguments.
In this work, we target the scenario where a target retrieval corpus DT is available, but the amount of
annotated query-document pairs for the new task is limited. Most prior of research efforts were put
into adapting retrievers to new corpus DT, but the divergence in queries QT and intents IT remains
under-explored. Next, we explore how search intent can be expressed with a short description and
very few number of examples.
2.2 F EW-SHOT BEIR S ETTING
In this paper, we argue that it is important to let retrievers be aware of task-speciﬁc query distribution
and search intent, as opposed to merely focusing on the domain adaptation of D. Prior belief is that it
is expensive to collect enough in-distribution queries and relevance labels to train a neural retriever,
but intuitively, a person can understand a retrieval task by reading a short instruction and going over a
few examples. In this work, we ask if a few (8 or fewer) examples are sufﬁcient for the machines
to learn a task-speciﬁc retriever. To facilitate our study and future research of few-shot retrieval,
we deﬁne a new few-shot retrieval evaluation setting built upon the BEIR heterogeneous retrieval
benchmark (Thakur et al., 2021).
BEIR has 18 information retrieval datasets across 9 domains, including Bio-Medical, Finance, News,
Twitter, Wikipedia, StackExchange, Quora, Scientiﬁc, and Misc. These datasets also cover a diverse
range of search intents: QA retrieval (question-to-document), duplicate question discovery (question-
to-question), fact checking (claim-to-document), etc. Following Santhanam et al. (2022) and Formal
et al. (2021), we narrow our focus to the publicly-available datasets in BEIR. The original BEIR
evaluation used a zero-shot set up, where no queries or relevant query-document pairs from the
evaluation datasets can be used to train the retrievers.
We extend BEIR to the few-shot setting by randomly taking a few (2 to 8) in-domain relevant query-
document examples as the task-speciﬁc supervision. The examples are sampled from the development
set when it is available. For the BEIR tasks which only have a test set, we use samples from the test
data as few-shot examples. To make the evaluation fair, when evaluating few-shot retriever models,
these test-set examples should be treated as ‘failed to retrieve‘ even if the model successfully retrieves
them. The prompt and few-shot examples will be released to the public.
3 P ROMPTAGATOR
To approach the goal of creating retrievers from few-shot examples, we propose Prompt-base Query
Generation for Retriever (PROMPTAGATOR ). The key idea of PROMPTAGATOR is to transform the few
examples into many more examples by prompting a LLM, instead of using them to train a retriever
directly.
3



Source: data\tc16_2312.10997v5\referenced_papers\[20]_2303.08518.pdf (Page 1):

seen task types during inference. In addition,
we demonstrate that the cross-task capabilities
can generalize well from a small LLM to dif-
ferent LLMs of much larger scales: we use
GPT-Neo-2.7B (Black et al., 2021) to guide
the tuning of the retriever and evaluate the re-
triever’s performance on BLOOM-7.1B (Scao
et al., 2022), OPT-66B (Zhang et al., 2022),
and GPT3-175B (Brown et al., 2020). The
cross-model and cross-task generalization of UP-
RISE makes it a promising and practical solution
for real-world applications.
Furthermore, our approach demonstrates the po-
tential for enhancing even the most powerful LLMs,
as shown in our experiments with ChatGPT. De-
spite its impressive abilities, ChatGPT has been
found to struggle with serious hallucination prob-
lems, leading to responses that are factually inac-
curate (Bang et al., 2023). However, UPRISE is
able to address this issue on fact-checking tasks
by prompting the model to draw correct inferences
from its built-in knowledge.
In summary, our contributions include:
• We introduceUPRISE , a lightweight and versatile
approach to improve zero-shot performance of
LLMs in the cross-task and cross-model scenario.
• UPRISE is tuned with GPT-Neo-2.7B, but
can also benefit different LLMs of much larger
scales, such as BLOOM-7.1B, OPT-66B, and
GPT3-175B.
• Our exploration on ChatGPT demonstrates the
potential of UPRISE in improving performances
of even the strongest LLMs.
2 Problem Definition
We aim to improve zero-shot performance of LLMs
by training a prompt retriever to retrieve prompts1
for any given task input. Specifically, UPRISE de-
composes the prompting process into two steps:
retrieve then predict. Given an input x, we first
retrieve a set of positive prompts P+ from a pre-
constructed pool P:
P+ = R(x, P). (1)
Then we concatenate P+ with x to form an input
sequence for a frozen LLM, which generates a pre-
dicted output:
yP+
= LM

yP+
|P+ ⊕ x

. (2)
1"Prompt" sometimes refers to a natural language template
filled by an input example, but here it denotes the sequence
prepended to the task input.
Engineered Natural Language Prompt
🧊Language Model
TaskInput
🧊Summarizethe…
Prompt Design
Tunable SoftPrompt
🔥0.10,0.05,…
Prompt TuningRetrieved NaturalLanguage Prompt
Prompt Retrieval
🧊Language Model
🧊Language Model
🧊Answerthis…
🔥Prompt RetrieverTunable Prompt Retrieverbackward
Tune
TaskInput
TaskInput
Figure 2: Typical prompt engineering methods and
prompt retrieval. Prompt retrieval prepends a natural
language prompt to the task input and uses a frozen
LLM to evaluate the prompt’s performance. The ob-
tained evaluation is then used to tune the retriever in a
reverse manner.
Our objective is to optimize performance of yP+
to match the target y by updating the retriever R.
Figure 2 compares prompt retrieval with typical
prompt engineering methods: prompt design adds
an engineered natural language prompt (Brown
et al., 2020; Wei et al., 2022b) and prompt tun-
ing tunes a soft prompt (Liu et al., 2021; Lester
et al., 2021). In contrast, prompt retrieval tunes
a retriever to retrieve natural language prompts,
which is both interpretable and flexible. It uses
the language model itself to label each prompt in
the pool as positive/negative, and then tunes a re-
triever from this signal (Rubin et al., 2022). Such
fine-tuned prompt retrieval has demonstrated effec-
tiveness in the task-specific scenario (Rubin et al.,
2022; Ye et al., 2023): a prompt retriever is tuned
on one or multiple specific tasks using the train-
ing sets as the prompt pool. The retriever is then
evaluated on the corresponding testing sets.
Our work is to achieve universality of the prompt
retriever, which means the fine-tuned retriever can
be directly used to retrieve prompts for unseen tasks
and various inference LLMs, without the need for
further tuning. We define the universality from two
perspectives: cross-task retrieval and cross-model
retrieval.
Cross-task retrieval. Considering the diversity
of tasks in real-world applications, we propose
cross-task retrieval to retrieve for task types on
which the prompt retriever has not been trained.
We simulate this setting by evaluating the prompt
retriever on unseen task types: various tasks are



### Claim 11/179

#### Claim Text
Innovations such as the Rewrite-Retrieve-Read [7]model leverage the LLM’s capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 0):

Query Rewriting for Retrieval-Augmented Large Language Models
Xinbei Ma1,2,∗ , Yeyun Gong3, #, †, Pengcheng He4, #, Hai Zhao1,2,†, Nan Duan3
1Department of Computer Science and Engineering, Shanghai Jiao Tong University
2Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University
3Microsoft Research Asia 4Microsoft Azure AI
sjtumaxb@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn,
{yegong, nanduan}@microsoft.com, Herbert.he@gmail.com
Abstract
Large Language Models (LLMs) play pow-
erful, black-box readers in the retrieve-then-
read pipeline, making remarkable progress
in knowledge-intensive tasks. This work in-
troduces a new framework, Rewrite-Retrieve-
Read instead of the previous retrieve-then-read
for the retrieval-augmented LLMs from the per-
spective of the query rewriting. Unlike prior
studies focusing on adapting either the retriever
or the reader, our approach pays attention to
the adaptation of the search query itself, for
there is inevitably a gap between the input text
and the needed knowledge in retrieval. We
first prompt an LLM to generate the query,
then use a web search engine to retrieve con-
texts. Furthermore, to better align the query
to the frozen modules, we propose a trainable
scheme for our pipeline. A small language
model is adopted as a trainable rewriter to cater
to the black-box LLM reader. The rewriter is
trained using the feedback of the LLM reader
by reinforcement learning. Evaluation is con-
ducted on downstream tasks, open-domain QA
and multiple-choice QA. Experiments results
show consistent performance improvement, in-
dicating that our framework is proven effective
and scalable, and brings a new framework for
retrieval-augmented LLM 1.
1 Introduction
Large Language Models (LLMs) have shown re-
markable abilities for human language processing
and extraordinary scalability and adaptability in
few- or zero-shot settings.(Ouyang et al., 2022;
Brown et al., 2020; Chowdhery et al., 2022). How-
ever, the training process depends on large-scale
high-quality corpora but without the perception
∗ Work done during an internship at 3Microsoft Research
Asia. # Equal contribution. †Corresponding author.
This paper was partially supported by Joint Research
Project of Yangtze River Delta Science and Technology Inno-
vation Community (No. 2022CSJGG1400).
1https://github.com/xbmxb/RAG-query-rewriting
of the real world. Thus, LLMs still have to face
the issue of hallucination (Yao et al., 2023; Bang
et al., 2023) and temporal misalignment (Röttger
and Pierrehumbert, 2021; Luu et al., 2022; Jang
et al., 2022). This affects the reliability of LLMs
and hinders wider practical application, because
the consistency between the LLM responses with
the real world needs further validation. Exist-
ing work has proved that incorporating external
knowledge (i.e., non-parametric knowledge) with
internal knowledge (i.e., parametric knowledge)
can effectively alleviate hallucination, especially
for knowledge-intensive tasks. In fact, retrieval-
augmented LLMs have been shown so effective
that they have been regarded as a standard solu-
tion to alleviate the factuality drawbacks in naive
LLM generations. Retrieval augmentation is ap-
plied to select relative passages as external contexts
for the language model, which isretrieve-then-read
framework (Lewis et al., 2020b; Karpukhin et al.,
2020; Izacard et al., 2022). Take the open-domain
Question-Answering task (open-domain QA) as
an example, a retriever first searches for related
documents for a question. Then the LLM receives
the question and the documents, then predicts an
answer.
As most LLMs are only accessible through infer-
ence APIs, they play the part of black-box frozen
readers in the pipeline. This makes previous re-
trieval augmentation methods that require complete
access (Lewis et al., 2020b; Guu et al., 2020; Izac-
ard et al., 2022) no longer feasible. Recent studies
on retrieval-augmented language models lean more
on the LLM-oriented adaptation. An idea is to train
a dense retrieval model to cater to the frozen lan-
guage model (Shi et al., 2023). By using feedback
from the LLM as a training objective, the retrieval
model is tuned for better LLM input contexts. An-
other research line focuses on the design of inter-
actions between the retriever and the reader (Yao
et al., 2023; Khattab et al., 2022), where both the
arXiv:2305.14283v3  [cs.CL]  23 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 1):

retriever and the reader are usually frozen. The idea
is to trigger the emergent ability through carefully
crafted prompts or a sophisticated prompt pipeline.
Multiple interactions with external knowledge al-
low the LLM to approach the correct answer step
by step.
However, there are still problems remaining to
be solved. Existing approaches overlook the adap-
tation of the query, i.e., the input of the retrieve-
then-read pipeline. The retrieval query is either
original from datasets or directly determined by the
black-box generation, thus is always fixed. How-
ever, there is inevitably a gap between the input
text and the knowledge that is really needed to
query. This limits performance and places a burden
on retrieval capability enhancement and prompt
engineering.
In consideration of this issue, this paper pro-
poses Rewrite-Retrieve-Read, a new framework for
retrieval augmentation, which can be further tuned
for adapting to LLMs. In front of the retriever, a
step of rewriting the input is added, filling the gap
between the given input and retrieval need, as is
shown in Figure 1. We adopt the off-the-shelf tool,
an internet search engine, as the retriever, which
avoids the maintenance of the search index and
can access up-to-date knowledge (Lazaridou et al.,
2022). Different from previous studies (Khattab
et al., 2022; Yao et al., 2023) that require the mem-
ory of multiple interaction rounds between the re-
triever and the LLM for each sample, the motiva-
tion of our rewriting step is to clarify the retrieval
need from the input text.
We also propose a trainable scheme for our
rewrite-retrieve-read framework (Figure 1 (c)).
The black-box retriever and the reader form a
frozen system. To further smooth the steps of
our pipeline, we apply a small, trainable language
model to perform the rewriting step, denoted as the
rewriter. The rewriter is trained by reinforcement
learning using the LLM performance as a reward,
learning to adapt the retrieval query to improve the
reader on downstream tasks.
Our proposed methods are evaluated on
knowledge-intensive downstream tasks including
open-domain QA (HotpoQA (Yang et al., 2018),
AmbigNQ (Min et al., 2020), PopQA (Mallen
et al., 2022)) and multiple choice QA (MMLU
(Hendrycks et al., 2021)). The experiments are
implemented on T5-large (Raffel et al., 2020) as
the rewriter, ChatGPT (Ouyang et al., 2022) and
Vicuna-13B (Chiang et al., 2023) as the LLM
reader. The results show that query rewriting con-
sistently improves the retrieve-augmented LLM
performance. The results also indicate that the
smaller language model can be competent for query
rewriting.
To sum up, our proposed novel retrieval-
augmentation method, rewrite-retrieve-read is the
first framework where the input text is adapted for
the frozen retriever and LLM reader. We introduce
a tuneable scheme with a small, trainable model,
achieving performance gains with less resource
consumption.
2 Related Work
2.1 Retrieval Augmentation
Language models require external knowledge to al-
leviate the factuality drawbacks. Retrieval augmen-
tation has been regarded as the standard effective
solution. With a retrieval module, related passages
are provided to the language model as the context
of the original input. Thus factual information like
common sense or real-time news helps with output
prediction through contextualized reading compre-
hension.
Earlier studies use sparse retriever (Chen et al.,
2017) or dense retriever (Karpukhin et al., 2020)
in front of a pre-trained language model (PrLM).
The neural retriever and reader are both PrLMs
of trainable size like BERT (Devlin et al., 2019)
or BART (Lewis et al., 2020a). Hence, the whole
retrieve-then-reader framework is a tuneable end-
to-end system, where the retrieved contexts can
be regarded as the intermediate results (Karpukhin
et al., 2020; Lewis et al., 2020b). Approaches to
smooth the two-step framework are proposed to op-
timize the retrieval and the reading comprehension
(Sachan et al., 2021; Lee et al., 2022; Jiang et al.,
2022). More recently, retrieval remains a powerful
enhancement as the size of models and data scales
rapidly (Mallen et al., 2022; Shi et al., 2023; Brown
et al., 2020). On the other hand, retrieval enhance-
ment can compensate for the shortfall in parameter
size, compared to large-scale language models. For
example, by jointly training the retriever and the
reader, Atlas (Izacard et al., 2022) shows few-shot
performance on par with 540B PalM (Chowdhery
et al., 2022) but be of 50× smaller size.
The Internet as a knowledge baseMore related
to our work, the search engine can assume the role
of the retriever and use the Internet as the source of



Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 3):

constructing the search query.
3 Methodology
We present Rewrite-Retrieve-Read, a pipeline that
improves the retrieval-augmented LLM from the
perspective of query rewriting. Figure 1 shows an
overview. This section first introduces the pipeline
framework in section 3.1, then the trainable scheme
in section 3.2.
3.1 Rewrite-Retrieve-Read
A task with retrieval augmentation can be de-
noted as follows. Given a dataset of a knowledge-
intensive task (e.g., open-domain QA), D =
{(x, y)i}, i= 0, 1, 2, . . . , N, x (e.g., a question)
is the input to the pipeline, y is the expected output
(e.g., the correct answer). Our pipeline consists of
three steps. (i) Query rewrite: generate a query ˜x
for required knowledge based on the original input
x. (ii) Retrieve: search for related context,doc. (iii)
Read: comprehend the input along with contexts
[doc, x] and predict the output ˆy.
A straightforward but effective method is to ask
an LLM to rewrite queries to search for informa-
tion that is potentially needed. We use a few-shot
prompt to encourage the LLM to think, and the
output can be none, one or more queries to search.
3.2 Trainable Scheme
Besides, total reliance on a frozen LLM has shown
some drawbacks. Reasoning errors or invalid
search hinders the performance (Yao et al., 2023;
BehnamGhader et al., 2022). On the other hand,
retrieved knowledge may sometimes mislead and
compromise the language model (Mallen et al.,
2022). To better align to the frozen modules, it is
feasible to add a trainable model and adapt it by
taking the LLM reader feedback as a reward.
Based on our framework, we further propose to
utilize a trainable small language model to take
over the rewriting step, as is shown in the right
part of Figure 1. The trainable model is initial-
ized with the pre-trained T5-large (770M) (Raffel
et al., 2020), denoted astrainable rewriter, Gθ. The
rewriter is first trained on pseudo data to warm up
(§3.2.1), then continually trained by reinforcement
learning (§3.2.2).
3.2.1 Rewriter Warm-up
The task, query rewriting, is quite different from
the pre-training objective of sequence-to-sequence
generative models like T5. First, we construct a
pseudo dataset for the query rewriting task. In-
spired by recent distillation methods (Hsieh et al.,
2023; Ho et al., 2022), we prompt the LLM to
rewrite the original questions x in the training set
and collect the generated queries ˜x as pseudo la-
bels. The collected samples are then filtered: Those
that get correct predictions from the LLM reader
are selected into the warm-up dataset, denoted as
DT rain= {(x, ˜x)|ˆy = y}. The rewriter Gθ is fine-
tuned on DT rainwith the standard log-likelihood
as the training objective, denoted as
Lwarm = −
X
t
logpθ( ˆ˜xt | ˜x<t, x ). (1)
The rewriter model after warm-up shows mod-
est performance, which depends on the pseudo
data quality and rewriter capability. Highly relying
on the human-written prompt line, ˜x can be sub-
optimal. The relatively small scale of the rewriter
size is also a limitation of the performance after the
warm-up. Then we turn to reinforcement learning
to align the rewriter to the following retriever and
LLM reader.
3.2.2 Reinforcement Learning
To further fine-tune the rewriter to cater to the LLM
reader, we adopt a policy gradient reinforcement
learning framework.
Task Formulation In the context of reinforce-
ment learning, the rewriter optimization is for-
mulated as a Markov Decision Process 5-tuple
⟨S, A, P, R, γ⟩. (i) The state space S is a finite set
limited by the vocabulary and the sequence length.
(ii) The action space A is equals to the vocabulary.
(iii) The transition probability P is determined by
the policy network, which is the rewriter model
Gθ. (iv) The reward function R gives a reward
value that depends on the current state. The pol-
icy gradient is derived from rewards, used as the
training objective. (v) γ denotes the discount fac-
tor. More specifically, the rewriter Gθ after the
warm-up is the initial policy model π0. At each
step t, the action at is to generate the next token
ˆ˜xt based on the observation of the present state,
st = [x, ˆ˜x<t]. When the generation is stopped by
the End-Of-Sentence token, one episode is ended.
After finishing the retrieval and reading, a reward
is computed by evaluating the final output, i.e., a
score for the LLM reader prediction.
Policy Optimization We adopt Proximal Policy
Optimization (PPO) (Schulman et al., 2017), fol-
lowing (Ramamurthy et al., 2022). Maximization



Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 2):

Input
Retriever
Output
Documents
Input
Web Search
Documents
Black-box LLM
Query
Input
Documents
Query
Output
 Output
Reward
Input:
What profession does Nicholas Ray and 
Elia Kazan have in common?
Query: Nicholas Ray profession
Nicholas Ray American author and 
director, original name Raymond 
Nicholas Kienzle, born August 7, 
1911, Galesville, Wisconsin, U.S......
director
Rewriter
Retriever
Black-box LLM
Reader
Black-box LLM
Reader
(a) Retrieve-then-read    (b)Rewrite-retrieve-read                  (c) Trainable rewrite-retrieve-read    
Black-box LLM
Reader
Web Search
Retriever
Rewriter
Small PrLM
Example
Query: Elia Kazan profession
Elia Kazan was an American film and 
theatre director, producer, 
screenwriter and actor, described  ......
Correct (reader      )
Hit (retriever      )
✅
✅
Figure 1: Overview of our proposed pipeline. From left to right, we show (a) standard retrieve-then-read method,
(b) LLM as a query rewriter for our rewrite-retrieve-read pipeline, and (c) our pipeline with a trainable rewriter.
external knowledge. Komeili et al. (2022) use an
internet search for relevant information based on
the dialogue history to perform dialogue response
generation. SeeKeR (Shuster et al., 2022) use a
single Transformer to iteratively perform search
query generation, then knowledge extraction for
dialogue generation and sentence completion. For
large-scale models, web search still shows effec-
tive for knowledge augmentation (Lazaridou et al.,
2022), fact-checking (Menick et al., 2022), and
LLM agent enhancement (Yao et al., 2023).
2.2 Cooperation with Black-box LLMs
Large Language Models, such as ChatGPT
(Ouyang et al., 2022), Codex (Chen et al., 2021),
PaLM (Chowdhery et al., 2022), emerge impres-
sive natural language processing ability as well as
remarkable scalability. This leads to a tendency
to embrace LLMs on a wide range of NLP tasks.
However, LLMs are only accessible as a black box
in most cases, which is because (i) Some like Chat-
GPT are not open-source and kept private; (ii) The
large parameter scale requires computational re-
sources that are not always affordable to users. This
constraint means nothing is available except input
and output texts.
Existing studies have proved that LLMs’ abili-
ties can be better leveraged by carefully designed
interaction methods. GenRead (Yu et al., 2023)
prompts an LLM to generate context instead of
deploying a retriever, showing that LLMs can re-
trieve internal knowledge by prompting. ReAct
(Yao et al., 2023) and Self-Ask (Press et al., 2022)
combines the Chain-of-Thought (CoT) (Wei et al.,
2022; Wang et al., 2022) and inter-actions with web
APIs. Only relying on prompt construction, Re-
Act provides novel baselines for interactive tasks.
Demonstrate–Search–Predict (DSP) (Khattab et al.,
2022) defines a sophisticated pipeline between an
LLM and a retriever. Unlike ReAct, DSP integrates
prompts for demonstration bootstrap besides multi-
hop breakdown and retrieval.
Despite the promising performance in the zero or
few-shot setting, the behavior of LLMs sometimes
needs adjustments. A feasible approach is to ap-
pend trainable small models in front of or after the
LLM. The small models, as a part of the parameters
of the system, can be fine-tuned for optimization.
RePlug (Shi et al., 2023) is proposed to fine-tune a
dense retriever for the frozen LLM in the retrieve-
then-read pipeline. The retriever is trained under
the LLM’s supervision to retrieve documents that
are suitable for the LLM. With the same purpose,
Directional Stimulus Prompting (Li et al., 2023)
deploys a small model to provide the LLM with
stimulus (e.g., keywords for summarization, or di-
alogue actions for response generation), which is
updated according to the LLM reward.
Different from the inspiring work mentioned
above, our proposed pipeline contains a query
rewriting step in front of the retrieve-then-read
module. We further propose a trainable scheme
with a small rewriting model, which is a novel
enhancement for retrieval-augmented LLM by re-



Source: data\tc16_2312.10997v5\referenced_papers\[72]_2301.12652.pdf (Page 0):

REPLUG : Retrieval-Augmented Black-Box Language Models
Weijia Shi,1 * Sewon Min,1 Michihiro Yasunaga,2 Minjoon Seo,3 Rich James,4 Mike Lewis,4
Luke Zettlemoyer1 4 Wen-tau Yih4
Abstract
We introduce REPLUG , a retrieval-augmented lan-
guage modeling framework that treats the lan-
guage model (LM) as a black box and augments
it with a tuneable retrieval model. Unlike prior
retrieval-augmented LMs that train language mod-
els with special cross attention mechanisms to en-
code the retrieved text, REPLUG simply prepends
retrieved documents to the input for the frozen
black-box LM. This simple design can be eas-
ily applied to any existing retrieval and language
models. Furthermore, we show that the LM can
be used to supervise the retrieval model, which
can then find documents that help the LM make
better predictions. Our experiments demonstrate
that REPLUG with the tuned retriever significantly
improves the performance of GPT-3 (175B) on
language modeling by 6.3%, as well as the perfor-
mance of Codex on five-shot MMLU by 5.1%.
1. Introduction
Large language models (LLMs) such as GPT-3 (Brown et al.,
2020a) and Codex (Chen et al., 2021a), have demonstrated
impressive performance on a wide range of language tasks.
These models are typically trained on very large datasets and
store a substantial amount of world or domain knowledge
implicitly in their parameters. However, they are also prone
to hallucination and cannot represent the full long tail of
knowledge from the training corpus. Retrieval-augmented
language models (Khandelwal et al., 2020; Borgeaud et al.,
2022; Izacard et al., 2022b; Yasunaga et al., 2022), in con-
trast, can retrieve knowledge from an external datastore
when needed, potentially reducing hallucination and increas-
ing coverage. Previous approaches of retrieval-augmented
language models require access to the internal LM repre-
sentations (e.g., to train the model (Borgeaud et al., 2022;
1University of Washington2Stanford University3KAIST 4Meta
AI.
* Work done while the first author was interning at Meta AI.
Correspondence to: Weijia Shi <swj0419@uw.edu>.
Figure 1. Different from previous retrieval-augmented ap-
proaches (Borgeaud et al., 2022) that enhance a language model
with retrieval by updating the LM’s parameters, REPLUG treats
the language model as a black box and augments it with a frozen
or tunable retriever. This black-box assumption makes REPLUG
applicable to large LMs (i.e., >100B parameters), which are often
served via APIs.
Izacard et al., 2022b) or to index the datastore (Khandelwal
et al., 2020)), and are thus difficult to be applied to very
large LMs. In addition, many best-in-class LLMs can only
be accessed through APIs. Internal representations of such
models are not exposed and fine-tuning is not supported.
In this work, we introduce REPLUG (Retrieve and Plug),
a new retrieval-augmented LM framework where the lan-
guage model is viewed as a black box and the retrieval
component is added as a tuneable plug-and-play module.
Given an input context, REPLUG first retrieves relevant
documents from an external corpus using an off-the-shelf
retrieval model. The retrieved documents are prepended to
the input context and fed into the black-box LM to make
the final prediction. Because the LM context length limits
the number of documents that can be prepended, we also
introduce a new ensemble scheme that encodes the retrieved
documents in parallel with the same black-box LM, allow-
ing us to easily trade compute for accuracy. As shown in
arXiv:2301.12652v4  [cs.CL]  24 May 2023



### Claim 12/179

#### Claim Text
Similarly, approaches like Generate-Read [13] replace traditional retrieval with LLM-generated content, while ReciteRead [22] emphasizes retrieval from model weights, enhancing the model’s ability to handle knowledge-intensive tasks.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 1):

output shows what might be needed to fulfill the
task, and thus can serve as an informative context to
retrieve more relevant knowledge, i.e., generation-
augmented retrieval. The newly retrieved knowl-
edge can benefit another iteration of retrieval-
augmented generation. We can also leverage model
generations to adapt retrieval, by distilling knowl-
edge from a re-ranker with access to model genera-
tions to a dense retriever with access to task inputs
only, which may be beneficial in scenarios where
user inputs can be easily collected, but relevant
knowledge or desirable outputs are not annotated.
We evaluate our method on three tasks, includ-
ing multi-hop question answering, fact verification,
and commonsense reasoning. Our method prompts
an LLM to produce a chain of reasoning steps fol-
lowed by the final answer under a few-shot set-
ting. For in-context demonstrations, we focus on
problem-solving and follow Wei et al. (2022) to
annotate chains of thoughts, without explicitly con-
sidering how generation-augmented retrieval might
be affected, which makes it conceptually simple
and easy to implement. Our method achieves up
to 8.6% absolute gains over previous state-of-the-
art retrieval-augmented methods on four out of six
datasets while being competitive on the remaining
two. According to our experiments, generation gen-
erally benefits from more iterations, with two itera-
tions giving the most performance gains. One may
customize the performance-cost tradeoffs by choos-
ing an appropriate number of iterations. We can
further improve performance and also reduce itera-
tions via the aforementioned generation-augmented
retrieval adaptation.
We summarize our findings as follows:
• Automatic metrics such as exact match can
significantly underestimate the performance
of LLMs in question answering tasks. More-
over, improvements in exact match do not
always reflect improvements in generations.
Evaluation using LLMs may be more reliable.
• ITER -RETGEN is superior to or competi-
tive with state-of-the-art retrieval-augmented
methods, while being simpler and causing
fewer overheads of retrieval and generation.
With generation-augmented retrieval adapta-
tion, we can further improve performance
and also reduce overheads (by reducing itera-
tions).
• It is desirable for an LLM to leverage both
parametric knowledge and non-parametric
knowledge effectively. I TER -RETGEN con-
sistently outperforms Self-Ask on question
answering tasks, regardless of whether in-
context non-parametric knowledge mentions
the answers or not.
2 Related Work
In recent months, there has been a surge in LLM-
powered applications, such as ChatGPT, Bing Chat,
and CoPilot (Chen et al., 2021). While showing an
unprecedented level of performance, LLMs are sub-
ject to the following limitations: (1) Due to a high
demand for compute and data, it remains an open
research question to continually update LLMs both
efficiently and effectively (Scialom et al., 2022);
(2) LLMs also tend to hallucinate (OpenAI, 2023),
i.e., generating plausible but non-factual texts. To
alleviate these issues, there is a growing trend of
augmenting LLMs with tools (Mialon et al., 2023;
Gou et al., 2023), e.g., a code interpreter (Gao
et al., 2022b; Shao et al., 2023) or a search engine
(Nakano et al., 2021), in an attempt to offload sub-
tasks to more qualified experts, or to enrich the
input context for LLMs by providing more relevant
information.
Retrieval augmentation is a mainstream direc-
tion to connect LLMs to the external world. Previ-
ous retrieval-augmented LMs (Izacard and Grave,
2021; Shao and Huang, 2022) typically receive re-
trieved knowledge in a passive way: knowledge
is retrieved based on the task inputs without LMs’
intervention. As it is difficult for a retriever to cap-
ture relevance, especially in the zero-shot setting,
recent work shows a shift towards having LLMs
actively involved in retrieval to improve relevance
modeling, e.g., to provide a specific context for
retrieval with model generations (e.g., generated
search queries (Nakano et al., 2021; Press et al.,
2022; Yao et al., 2022), partial generation (Trivedi
et al., 2022a), or forward-looking sentences (Jiang
et al., 2023)). Khattab et al. (2022) proposed a
DSP programming framework that supports vari-
ous retrieval-augmented methods.
Recent work interleaves retrieval with generation
when completing a single output. Such a structured
workflow may reduce the flexibility in generation
(Yao et al., 2022). ITER -RETGEN avoids interrupt-
ing generation with retrieval, but iterates retrieval
and generation, i.e., to leverage the complete gen-
eration from the previous iteration to retrieve more



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 4):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:5
Table 1. Relate Work.
Method Dataset Scale Evaluation Metrics Evaluation Method Application Field Ref. Lang.
[27] Based on LangChain Pythondocumentation QA dataset86 Accuracy of answer, Faithfulness ofresponse to the retrieved documentEvaluating retrieval andgeneration consistencyGeneral QA scenarios(Read) Yes EN
[27] PDF documents containingtables and charts5 Accuracy of answer, Faithfulness ofresponse to the retrieved documentEvaluating retrieval andgeneration consistency
Semi-structureddata scenarios(Read) Yes EN
[32] Query and responses(with citations)1450 Fluency, Perceived utility,Citation recall and precisionHuman evaluation Citation(Read) Yes EN
[17, 22] Questions, answersand contexts(with citations)- Fluency, Correctness,Citation quality Self-devised metrics,Human evaluation Citation(Read) Yes EN
[54] Questions, answersand contexts(with citations)1948 Location citation recall, Location precision,The coefficient of variation of citation locationsSelf-devised metricsCitation(Read) Yes EN
[29] Questions, answersand contexts(with citations)3422 Accuracy of factual and non-factual,AUC-PR, and so on Self-devised metrics,Common metrics Citation(Read) Yes EN
[61] Statement-citation Pairs12681 Correlation, Classification performance,Retrieval effectiveness, Faithfulness
Self-devised metrics,Common metrics,Human evaluation
Citation(Read) Yes EN
[7] Paragraphs(with citations)10000 Citation density,The coverage of reference factsSelf-devised metricsCitation(Read) Yes EN
[39] Questions, answersand contexts 200
Categorization ability,Logical/Mathematical reasoning,Complex question solving,Summarization ability
Accuracy Financial services,Legal, Business(Read,Delete) Yes EN
[8] LLM-generated dataset 1000
Noise robustness,Negative rejection,Information integration,Counterfactual robustness
Self-devised metricsGeneral, especiallynews domain(Read,Update) Yes CNEN
[14] — — Context relevance, Groundedness,Answer relevance Analyzing the RAG triadGeneral(Create,Read) No —
[13] — — Faithfulness, Answer relevance,Context relevance Automated evaluationusing LLM prompts General(Create,Read) No —
[44] LLM-generated dataset 150Context relevance, Answer faithfulness,Answer relevance Generating custom LLM judges foreach component of a RAG systemGeneral(Create,Read) No EN
Ours LLM-generated dataset 36166ROUGE, BLEU,bertScore, RAGQuestEvalEvaluating retrieval andgeneration consistency
General(Create,ReadUpdate,Delete) Yes CN
optimization, and post-retrieval processing [20]. Pre-retrieval processing encompasses data trans-
former to enhance text standardization, ensuring factual accuracy, optimizing index structures,
adjusting block sizes, and rewriting query [ 4, 16, 50, 52]. Retrieval model optimization entails
the fine-tuning of domain-specific embedding models and the application of dynamic embedding
techniques [11, 60]. Post-retrieval processing minimizes context length through reranking and
compression operations, aiming to emphasize critical information, diminish noise interference, and
enhance integration and utilization by the generator [37, 53, 55].
Furthermore, to enhance the precision and efficiency of the generator when handling retrieval
content, scholars have undertaken a series of optimization measures. As an illustration, researchers
have devised methods such as Chain-of-Note (CON) for the generator [58]. CON generates contin-
uous reading notes to comprehensively evaluate the relevance of retrieved documents to the posed
question, integrating this information to produce precise final responses. This approach further
enhances the capability of RAG in managing retrieval information, guaranteeing the production of
responses that are simultaneously accurate and pertinent. In specific domains, such as medical and
legal, models undergo fine-tuning to enhance the generator’s performance within those particular
fields [10, 24, 56]. Through the implementation of these methods, the generator can more effectively
process retrieved information and furnish responses that are more accurate and relevant.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[24]_2305.06983.pdf (Page 0):

Active Retrieval Augmented Generation
Zhengbao Jiang1∗ Frank F. Xu1∗ Luyu Gao1∗ Zhiqing Sun1∗ Qian Liu2
Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1
1Language Technologies Institute, Carnegie Mellon University
2Sea AI Lab 3FAIR, Meta
{zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu
Abstract
Despite the remarkable ability of large lan-
guage models (LMs) to comprehend and gen-
erate language, they have a tendency to hal-
lucinate and create factually inaccurate out-
put. Augmenting LMs by retrieving informa-
tion from external knowledge resources is one
promising solution. Most existing retrieval aug-
mented LMs employ a retrieve-and-generate
setup that only retrieves information once based
on the input. This is limiting, however, in
more general scenarios involving generation
of long texts, where continually gathering in-
formation throughout generation is essential. In
this work, we provide a generalized view of ac-
tive retrieval augmented generation, methods
that actively decide when and what to retrieve
across the course of the generation. We propose
Forward-Looking Active REtrieval augmented
generation (FLARE), a generic method which
iteratively uses a prediction of the upcoming
sentence to anticipate future content, which is
then utilized as a query to retrieve relevant doc-
uments to regenerate the sentence if it contains
low-confidence tokens. We test FLARE along
with baselines comprehensively over 4 long-
form knowledge-intensive generation tasks/-
datasets. FLARE achieves superior or compet-
itive performance on all tasks, demonstrating
the effectiveness of our method.1
1 Introduction
Generative language models (LMs) (Brown et al.,
2020; Ouyang et al., 2022; OpenAI, 2023; Chowd-
hery et al., 2022; Zhang et al., 2022; Touvron et al.,
2023; Zhao et al., 2023) have become a founda-
tional component in natural language processing
(NLP) systems with their remarkable abilities. Al-
though LMs have memorized some world knowl-
edge during training (Petroni et al., 2019; Roberts
et al., 2020; Jiang et al., 2020), they still tend to
∗Lead contributors.
1Code and datasets are available athttps://github.com/
jzbjyb/FLARE.
hallucinate and create imaginary content (Maynez
et al., 2020; Zhou et al., 2021). Augmenting LMs
with retrieval components that look up relevant in-
formation from external knowledge resources is a
promising direction to address hallucination (Khan-
delwal et al., 2020; Izacard et al., 2022).
Retrieval augmented LMs commonly use a
retrieve-and-generate setup where they retrieve doc-
uments based on the user’s input, and then generate
a complete answer conditioning on the retrieved
documents (Chen et al., 2017; Guu et al., 2020;
Lewis et al., 2020; Izacard and Grave, 2021; Sachan
et al., 2021; Lee et al., 2021; Jiang et al., 2022;
Izacard et al., 2022; Nakano et al., 2021; Qian
et al., 2023; Lazaridou et al., 2022; Shi et al., 2023).
These single-time retrieval augmented LMs outper-
form purely parametric LMs, particularly for short-
form knowledge-intensive generation tasks such
as factoid question answering (QA) (Kwiatkowski
et al., 2019; Joshi et al., 2017), where the informa-
tion needs are clear in the user’s input, and it is
sufficient to retrieve relevant knowledge once solely
based on the input.
Increasingly powerful large LMs have also
demonstrated abilities in more complex tasks that
involve generating long-form output, such as long-
form QA (Fan et al., 2019; Stelmakh et al., 2022),
open-domain summarization (Cohen et al., 2021;
Hayashi et al., 2021; Giorgi et al., 2022), and
(chain-of-thought; CoT) reasoning (Wei et al.,
2022; Ho et al., 2020; Geva et al., 2021; Hendrycks
et al., 2020). In contrast to short-form generation,
long-form generation presents complex informa-
tion needs that are not always evident from the in-
put alone. Similar to how humans gradually gather
information as we create content such as papers,
essays, or books, long-form generation with LMs
would require gathering multiple pieces of knowl-
edge throughout the generation process . For ex-
ample, to generate a summary about a particular
topic, the initial retrieval based on the topic name
arXiv:2305.06983v2  [cs.CL]  22 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 1):

Preprint.
Step 1: Retrieve K documents
California was named after a ﬁctional 
island in a Spanish book. 
Prompt How did US states get their names? 
US states got their names from a variety of 
sources. Eleven states are named after an 
individual person  (e.g, California was named 
after Christopher Columbus) . Some states 
including Texas and Utah, are named after 
Native American tribe.
Retrieval-Augmented Generation (RAG) Ours: Self-reﬂective Retrieval-Augmented Generation (Self-RAG) 
Popular names by states. In Texas, 
Emma is a popular baby name. 
Of the ﬁfty states, eleven are named 
after an individual person. 
Prompt How did US states get their names? + 
Step 2: Prompt LM with K docs and generate
Retriever
LM
Prompt How did US states get their names? 
US states got their names from a variety of sources. Retrieve
Step 1: Retrieve on demand  
Prompt +  
11 of 50 state namesRelevant
Step 2: Generate segment in parallel 
come from persons. Supported
Irrelevant Texas is named
after a Native American tribe. 
Step 3: Critique outputs and select best segment
origins in a 16th-century novel 
Las Sergas de Esplandián. 
California's name has itsRelevant
Partially
US states got their names from a variety of sources. 11 of 50 
states names are come from persons.    26 states are named 
after Native Americans, including Utah. 
Prompt: Write an essay of your best summer vacation
 Prompt: Write an essay of your best summer vacation
No Retrieval My best summer vacation is when my family and I embarked on a road trip along …My best… 
>
Repeat.…
No information in passagesContradictory
>
Prompt +  
 Prompt +  
Retrieve
Figure 1: Overview of SELF -RAG. SELF -RAG learns to retrieve, critique, and generate text passages
to enhance overall generation quality, factuality, and verifiability.
consistently retrieves a fixed number of documents for generation regardless of the retrieval necessity
(e.g., the bottom figure example does not require factual knowledge) and never second visits the
generation quality. Moreover, SELF -RAG provides citations for each segment with its self-assessment
of whether the output is supported by the passage, leading to easier fact verification.
SELF -RAG trains an arbitrary LM to generate text with reflection tokens by unifying them as the
next token prediction from the expanded model vocabulary. We train our generator LM on a diverse
collection of text interleaved with reflection tokens and retrieved passages. Reflection tokens, inspired
by reward models used in reinforcement learning (Ziegler et al., 2019; Ouyang et al., 2022), are
inserted offline into the original corpus by a trained critic model. This eliminates the need to host a
critic model during training, reducing overhead. The critic model, in part, is supervised on a dataset
of input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e.,
GPT-4; OpenAI 2023). While we draw inspiration from studies that use control tokens to start and
guide text generation (Lu et al., 2022; Keskar et al., 2019), our trained LM uses critique tokens to
assess its own predictions after each generated segment as an integral part of the generation output.
SELF -RAG further enables a customizable decoding algorithm to satisfy hard or soft constraints,
which are defined by reflection token predictions. In particular, our inference-time algorithm enables
us to (1) flexibly adjust retrieval frequency for different downstream applications and (2) customize
models’ behaviors to user preferences by leveraging reflection tokens through segment-level beam
search using the weighted linear sum of the reflection token probabilities as segment score.
Empirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF -
RAG significantly outperforms pre-trained and instruction-tuned LLMs that have more parameters and
widely adopted RAG approaches with higher citation accuracy. In particular, SELF -RAG outperforms
retrieval-augmented ChatGPT on four tasks, Llama2-chat (Touvron et al., 2023) and Alpaca (Dubois
et al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with
reflection tokens for overall performance improvements as well as test-time model customizations
(e.g., balancing the trade-off between citation previsions and completeness).
2 R ELATED WORK
Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) augments the input
space of LMs with retrieved text passages (Guu et al., 2020; Lewis et al., 2020), leading to large
improvements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram
et al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number
2



Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 2):

Input
Retriever
Output
Documents
Input
Web Search
Documents
Black-box LLM
Query
Input
Documents
Query
Output
 Output
Reward
Input:
What profession does Nicholas Ray and 
Elia Kazan have in common?
Query: Nicholas Ray profession
Nicholas Ray American author and 
director, original name Raymond 
Nicholas Kienzle, born August 7, 
1911, Galesville, Wisconsin, U.S......
director
Rewriter
Retriever
Black-box LLM
Reader
Black-box LLM
Reader
(a) Retrieve-then-read    (b)Rewrite-retrieve-read                  (c) Trainable rewrite-retrieve-read    
Black-box LLM
Reader
Web Search
Retriever
Rewriter
Small PrLM
Example
Query: Elia Kazan profession
Elia Kazan was an American film and 
theatre director, producer, 
screenwriter and actor, described  ......
Correct (reader      )
Hit (retriever      )
✅
✅
Figure 1: Overview of our proposed pipeline. From left to right, we show (a) standard retrieve-then-read method,
(b) LLM as a query rewriter for our rewrite-retrieve-read pipeline, and (c) our pipeline with a trainable rewriter.
external knowledge. Komeili et al. (2022) use an
internet search for relevant information based on
the dialogue history to perform dialogue response
generation. SeeKeR (Shuster et al., 2022) use a
single Transformer to iteratively perform search
query generation, then knowledge extraction for
dialogue generation and sentence completion. For
large-scale models, web search still shows effec-
tive for knowledge augmentation (Lazaridou et al.,
2022), fact-checking (Menick et al., 2022), and
LLM agent enhancement (Yao et al., 2023).
2.2 Cooperation with Black-box LLMs
Large Language Models, such as ChatGPT
(Ouyang et al., 2022), Codex (Chen et al., 2021),
PaLM (Chowdhery et al., 2022), emerge impres-
sive natural language processing ability as well as
remarkable scalability. This leads to a tendency
to embrace LLMs on a wide range of NLP tasks.
However, LLMs are only accessible as a black box
in most cases, which is because (i) Some like Chat-
GPT are not open-source and kept private; (ii) The
large parameter scale requires computational re-
sources that are not always affordable to users. This
constraint means nothing is available except input
and output texts.
Existing studies have proved that LLMs’ abili-
ties can be better leveraged by carefully designed
interaction methods. GenRead (Yu et al., 2023)
prompts an LLM to generate context instead of
deploying a retriever, showing that LLMs can re-
trieve internal knowledge by prompting. ReAct
(Yao et al., 2023) and Self-Ask (Press et al., 2022)
combines the Chain-of-Thought (CoT) (Wei et al.,
2022; Wang et al., 2022) and inter-actions with web
APIs. Only relying on prompt construction, Re-
Act provides novel baselines for interactive tasks.
Demonstrate–Search–Predict (DSP) (Khattab et al.,
2022) defines a sophisticated pipeline between an
LLM and a retriever. Unlike ReAct, DSP integrates
prompts for demonstration bootstrap besides multi-
hop breakdown and retrieval.
Despite the promising performance in the zero or
few-shot setting, the behavior of LLMs sometimes
needs adjustments. A feasible approach is to ap-
pend trainable small models in front of or after the
LLM. The small models, as a part of the parameters
of the system, can be fine-tuned for optimization.
RePlug (Shi et al., 2023) is proposed to fine-tune a
dense retriever for the frozen LLM in the retrieve-
then-read pipeline. The retriever is trained under
the LLM’s supervision to retrieve documents that
are suitable for the LLM. With the same purpose,
Directional Stimulus Prompting (Li et al., 2023)
deploys a small model to provide the LLM with
stimulus (e.g., keywords for summarization, or di-
alogue actions for response generation), which is
updated according to the LLM reward.
Different from the inspiring work mentioned
above, our proposed pipeline contains a query
rewriting step in front of the retrieve-then-read
module. We further propose a trainable scheme
with a small rewriting model, which is a novel
enhancement for retrieval-augmented LLM by re-



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 1):

retriever and the reader are usually frozen. The idea
is to trigger the emergent ability through carefully
crafted prompts or a sophisticated prompt pipeline.
Multiple interactions with external knowledge al-
low the LLM to approach the correct answer step
by step.
However, there are still problems remaining to
be solved. Existing approaches overlook the adap-
tation of the query, i.e., the input of the retrieve-
then-read pipeline. The retrieval query is either
original from datasets or directly determined by the
black-box generation, thus is always fixed. How-
ever, there is inevitably a gap between the input
text and the knowledge that is really needed to
query. This limits performance and places a burden
on retrieval capability enhancement and prompt
engineering.
In consideration of this issue, this paper pro-
poses Rewrite-Retrieve-Read, a new framework for
retrieval augmentation, which can be further tuned
for adapting to LLMs. In front of the retriever, a
step of rewriting the input is added, filling the gap
between the given input and retrieval need, as is
shown in Figure 1. We adopt the off-the-shelf tool,
an internet search engine, as the retriever, which
avoids the maintenance of the search index and
can access up-to-date knowledge (Lazaridou et al.,
2022). Different from previous studies (Khattab
et al., 2022; Yao et al., 2023) that require the mem-
ory of multiple interaction rounds between the re-
triever and the LLM for each sample, the motiva-
tion of our rewriting step is to clarify the retrieval
need from the input text.
We also propose a trainable scheme for our
rewrite-retrieve-read framework (Figure 1 (c)).
The black-box retriever and the reader form a
frozen system. To further smooth the steps of
our pipeline, we apply a small, trainable language
model to perform the rewriting step, denoted as the
rewriter. The rewriter is trained by reinforcement
learning using the LLM performance as a reward,
learning to adapt the retrieval query to improve the
reader on downstream tasks.
Our proposed methods are evaluated on
knowledge-intensive downstream tasks including
open-domain QA (HotpoQA (Yang et al., 2018),
AmbigNQ (Min et al., 2020), PopQA (Mallen
et al., 2022)) and multiple choice QA (MMLU
(Hendrycks et al., 2021)). The experiments are
implemented on T5-large (Raffel et al., 2020) as
the rewriter, ChatGPT (Ouyang et al., 2022) and
Vicuna-13B (Chiang et al., 2023) as the LLM
reader. The results show that query rewriting con-
sistently improves the retrieve-augmented LLM
performance. The results also indicate that the
smaller language model can be competent for query
rewriting.
To sum up, our proposed novel retrieval-
augmentation method, rewrite-retrieve-read is the
first framework where the input text is adapted for
the frozen retriever and LLM reader. We introduce
a tuneable scheme with a small, trainable model,
achieving performance gains with less resource
consumption.
2 Related Work
2.1 Retrieval Augmentation
Language models require external knowledge to al-
leviate the factuality drawbacks. Retrieval augmen-
tation has been regarded as the standard effective
solution. With a retrieval module, related passages
are provided to the language model as the context
of the original input. Thus factual information like
common sense or real-time news helps with output
prediction through contextualized reading compre-
hension.
Earlier studies use sparse retriever (Chen et al.,
2017) or dense retriever (Karpukhin et al., 2020)
in front of a pre-trained language model (PrLM).
The neural retriever and reader are both PrLMs
of trainable size like BERT (Devlin et al., 2019)
or BART (Lewis et al., 2020a). Hence, the whole
retrieve-then-reader framework is a tuneable end-
to-end system, where the retrieved contexts can
be regarded as the intermediate results (Karpukhin
et al., 2020; Lewis et al., 2020b). Approaches to
smooth the two-step framework are proposed to op-
timize the retrieval and the reading comprehension
(Sachan et al., 2021; Lee et al., 2022; Jiang et al.,
2022). More recently, retrieval remains a powerful
enhancement as the size of models and data scales
rapidly (Mallen et al., 2022; Shi et al., 2023; Brown
et al., 2020). On the other hand, retrieval enhance-
ment can compensate for the shortfall in parameter
size, compared to large-scale language models. For
example, by jointly training the retriever and the
reader, Atlas (Izacard et al., 2022) shows few-shot
performance on par with 540B PalM (Chowdhery
et al., 2022) but be of 50× smaller size.
The Internet as a knowledge baseMore related
to our work, the search engine can assume the role
of the retriever and use the Internet as the source of



Source: data\tc16_2312.10997v5\referenced_papers\[22]_2210.01296.pdf (Page 8):

Published as a conference paper at ICLR 2023
Table 3: Natural Questions (NQ) results with different context passages.
UL2-20B(5) Codex-002(5)
EM / F1 EM / F1
No passage 10.16 / 20.17 31.45 / 44.75
Ground-truth passage 41.02 / 55.73 49.32 / 64.32
BM25-Retrieval (Top-1) 16.31 / 27.66 33.20 / 47.45
LM-Recitation(5) (20-path) 14.16 / 23.13 35.84 / 49.12
Table 4: Per-question error analysis on TriviaQA.
UL2-20B(5) OPT-30B(5)
Hits@Majority 53.42% 49.02%
Not Recit. 21.09% 22.27%
Hits@20-Recit. 5.66% 8.01%
Hits@20-Path 19.82% 20.07%
Table 5: Per-path error analysis on TriviaQA.
Recit. Ans. UL2-20B (5) OPT-30B(5)
  33.60% 30.06%
  7.87% 9.79%
  12.10% 12.57%
  46.44% 47.58%
stronger models (i.e., Codex) tend to beneﬁt more from the the model’s own recitation than BM25
retrieved context.
4.3.4 E RROR ANALYSIS
We perform an error analysis on the 1024 evaluation examples in the TriviaQA dataset. We classify
the errors into three categories: 1) Not Recit., i.e., the correct answer is not recited in any of the
20 recited passages in self-consistency. 2) Hits@20-Recit., i.e., the correct answer can be found in
one of the recited passage, but does not appear in the QA module’s outputs. 3) Hits@20-Path, i.e.,
the correct answer is one of the ﬁnal outputs of the 20 self-consistency paths, but it does not have
the majority votes. The correct ﬁnal answer is marked as Hits@Majority (i.e., Exact Matching). An
algorithmic description is given in Algo. 1. We report the results of UL2-20B and OPT-30B in Tab. 4.
We can see that “No Recit” and “Hits@20-Path” account for the majority of the errors, meaning that
the QA module performs quite well (if the correct answer appears in one of the recitation passages,
it will be extracted by the QA module in most of the cases), and the main bottleneck still lies in the
recitation quality and answer aggregation strategies.
We also perform a per-path error analysis, i.e., how many questions can be answered correctly
(or not) when the recitation exactly contains (or not) the answer tokens. The results are shown in
Tab. 5. We can see that around 7% ∼10% questions have the correct recitation but cannot produce
the correct answer, while around 12% questions do not have the correction recitation but can be
answered correctly anyway.
5 C ONCLUSION & DISCUSSION
In this paper, we propose a novel recitation-augmented generation framework to improve lan-
guage models’ performance in the closed-book question-answering setting. We hypothesize that
for knowledge-intensive NLP tasks, encouraging the model to explicitly recite a speciﬁc knowledge
source would be helpful in augmenting its memory. In addition, we found that diversifying the
recitation process can be beneﬁcial as well since usually there exists multiple knowledge sources
that could be used to answer the same question. We show promising results over three large lan-
guage models and across three different closed-book QA datasets, demonstrating the effectiveness
of our proposed recite-and-answer approach.
One limitation of our method is that updating time-sensitive knowledge for a pure LLM-based
method requires training or ﬁne-tuning the LLMs on the new corpus, which can be costly. For
future work, we plan to further validate the effectiveness of recitation-augmented generation for
other knowledge-intensive NLP tasks in the closed-book setting, such as fact checking.
9



Source: data\tc16_2312.10997v5\referenced_papers\[22]_2210.01296.pdf (Page 1):

Published as a conference paper at ICLR 2023
This paper proposes a new paradigm to help LLMs generate more accurate factual knowledge
without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE),
wherein we tackle knowledge-intensive NLP tasks by ﬁrst reciting relevant information and then
generating the outputs. Such a two-step paradigm decomposes the original knowledge-intensive
task into two sub-tasks: knowledge-recitation and task-execution, where the former can be regarded
as a form of intermediate knowledge retrieval step (from the model weights), while the latter is the
execution step that produces the ﬁnal outputs.
The motivation of introducing an additional knowledge-recitation step comes from our observation
that while few-shot prompting can help LLMs execute speciﬁc NLP tasks, these tasks are usually
not in a similar form as the original causal language modeling pre-training objective. This hinders
LLMs from effectively reciting knowledge from their memory (Carlini et al., 2021). Consider a
student taking a closed-book exam that contains knowledge-intensive questions, for example,“what
is the tenth decimal ofπ?”. They typically cannot directly answer this question because in studying
stage (in analogy to the language modeling pre-training stage for LLMs), it is highly unlikely that
they would read “the tenth decimal of π is 5”. However, there can be some sentences like “the
ﬁrst N digits of πare 3.14159 26535...” existing in the textbook that can be recited by the student.
Therefore, a student can possibly answer this question in a recite-and-answer scheme:“The ﬁrst 10
digits of π are 3.14159 26535. So the answer is 5”. Here, the knowledge-recitation step can serve
as an intermediate step that mimics the language modeling pre-training task, and thus better helps
the LLM to generate factual knowledge.
We verify the effectiveness of our recitation-augmented generation on few-shot Closed-Book Ques-
tion Answering (CBQA) tasks (referred as recite-and-answer in the CBQA context), as illustrated
in Figure 1. CBQA is an attractive open-domain QA task in that a fully parameterized LM can
generate answers directly without an external corpus or separate retrieval models (Roberts et al.,
2020). We show that the proposed recite-and-answer scheme is an effective method for CBQA and
compatible with other techniques for boosting few-shot performance of LLMs. We also show that,
in addition to improving the few-shot in-context learning performance of RECITE-enhanced LLM,
ﬁne-tuning the pre-trained LLMs on synthetic generated question-passage pairs can further improve
the recitation performance and lead to a better downstream QA accuracy.
Experiments on four large language models (PaLM (Chowdhery et al., 2022), UL2 (Tay et al.,
2022a), OPT (Zhang et al., 2022)), and Codex (Chen et al., 2021) show that a recite-and-answer
scheme can improve performance on various types of CBQA tasks, including Wikipedia-based
single-hop QA (Natural Questions, Kwiatkowski et al. 2019), trivia questions (TriviaQA, Joshi et al.
2017), and Wikipedia-based multi-hop QA (HotpotQA, Yang et al. 2018).
2 R ELATED WORK
2.1 O PEN -DOMAIN QUESTION ANSWERING
Open-domain question answering (Prager et al., 2007) refers to the task of generating answers for
arbitrary context-free questions. In the open-book setting, it is typically assumed that the QA model
can ﬁnd the answer in an external corpus, e.g., Wikipedia (Chen et al., 2017; Izacard & Grave,
2021) or web pages (Lazaridou et al., 2022). This is in analogy as taking an open-book exam where
students can search over an external knowledge corpus. The standard pipeline (Chen et al., 2017;
Izacard & Grave, 2021; 2020) usually consists of a learnable or non-learnable document retriever
module and a learnable neural network-based reader module.
In the closed-book setting, the QA model is not allowed to access any external knowledge, and needs
to store all the knowledge in its parameters. It has been recently observed that large-scale pre-trained
language models (Devlin et al., 2019; Radford et al., a; Yang et al., 2019b) can internalize a sort of
implicit “knowledge base” after pre-training (Petroni et al., 2019; Jiang et al., 2020; Talmor et al.,
2020). Roberts et al. (2020) show that after ﬁne-tuning on open-book question-answer pairs, T5
(Raffel et al., 2020) can answer a large portion of knowledge-intensive questions. This is similar as
taking a closed-book exam. However, Lewis et al. (2021) found that the high performance is mainly
due to training set question memorization. Wang et al. (2021) also found that it is still challenging
for relatively small-scale pre-trained language models like RoBERTa (Liu et al., 2019) or GPT-2
(Radford et al., b) to answer closed-book questions.
2



Source: data\tc16_2312.10997v5\referenced_papers\[22]_2210.01296.pdf (Page 2):

Published as a conference paper at ICLR 2023
In this work, we focus on evaluating the CBQA performance of large language models (LLMs) in
the few-shot setting, which ideally minimizes the bias of train-test overlapping (Liu et al., 2021). We
propose a recite-and-answer scheme, which is similar to a student ﬁrst recite the factoid knowledge
about the question, and then answer the question.
2.2 I N-CONTEXT FEW -SHOT LEARNING
Large language models (LLMs) such as GPT-3 (Brown et al., 2020) have the surprising ability
to do in-context learning, where the model learns to do new tasks simply by being prompted a few
exemplars. The LLMs learn from these exemplars without being explicitly pre-trained for in-context
learning and without any gradient updates or ﬁne-tuning. Recent study showed that such ability
improves with the scaling of both model size (Brown et al., 2020; Rae et al., 2021; Chowdhery et al.,
2022) and number of tokens for training (Hoffmann et al., 2022). When evaluated on knowledge-
intensive question answering tasks, these models are usually evaluated in the closed-book setting,
where the factoid knowledge are completely stored in the model parameters of dense LLMs.
Recently, Atlas (Izacard et al., 2022) shows that for knowledge-intensive NLP tasks, a relatively
lite model with retrieval augmentations can achieve similar or even better performance through few-
shot ﬁne-tuning, which proves that memorization can be decoupled from generalization in LLMs.
In contrast, we show that still a large underestimated amount of knowledge can be retrieved from
LLMs’ model weights through better-designed prompting.
2.3 R ATIONALE -AUGMENTED REASONING
Ling et al. (2017) pioneer the work of solving math word problems by generating step-by-step
human-readable solutions described by natural language and math equations before the ﬁnal answer.
That is fundamentally different from other works which directly generate the ﬁnal answers or use
formal languages. e.g. equations only, to illustrate the intermediate solving steps (Roy et al., 2016;
Amini et al., 2019; Chen et al., 2019). Cobbe et al. (2021) extend (Ling et al., 2017) by constructing a
much larger dataset to ﬁnetune a pre-trained large language model to solve math word problems and
a parameterized ranker is trained to rank candidate solutions to improve the solving rate. Wei et al.
(2022) propose chain-of-thought prompting which combines the idea of natural language rationales
(Ling et al., 2017; Cobbe et al., 2021) with few-shot prompting (Brown et al., 2020).
In this work, instead of generating a chain of thought for multi-step reasoning questions, we de-
compose the process of answering a knowledge-intensive question into two steps: recite the relevant
knowledge stored in the model parameters, and then answer the question.
2.4 M EMORIZATION IN LARGE LANGUAGE MODELS
Recent study shows that large language models can memorize its training data, and generate texts
from training data given certain prompts (Carlini et al., 2021; 2022; Zhang et al., 2021; Kharitonov
et al., 2021; Thakkar et al., 2020; Carlini et al., 2019; Tirumala et al., 2022). Most related to our
work, Carlini et al. (2022) found that the memorization ability of LLMs signiﬁcantly grows as the
model capacity increases, the number of times an example has been duplicated, and the number of
tokens of context used to prompt the model. While these works mainly analyze the fundamental
properties of memorization in the exact setting, where exactly N tokens are used as the prompt to
reproduce the sufﬁx of the prompt, our work relies on “fuzzy memorizaiton”, where the prompts
tend to not be exactly the same as the training data, but still improve the memorization accuracy.
The proposed recitation-augmented generation idea is also related to the line of work on utilizing
Transformer memory as an information retrieval model (Tay et al., 2022b) and self-talk models for
commonsense reasoning (Shwartz et al., 2020; Liu et al., 2022). Zhuang et al. (2022); Wang et al.
(2022c); Zhou et al. (2022) proposed to augment documents at indexing time with a number of
generated queries. Bevilacqua et al. (2022) proposed to directly generate n-grams grounded in one
or multiple documents with constrained decoding.
3



Source: data\tc16_2312.10997v5\referenced_papers\[173]_2403.10131.pdf (Page 8):

Preprint, Under Review
2 4 6 8 10
# T est Documents (T op-k)
0.22
0.24
0.26
0.28
0.30
0.32Final Accuracy
Natural Questions
Train D*
Train D* + 1D
Train D* + 2D
Train D* + 3D
2 4 6 8 10
# T est Documents (T op-k)
0.125
0.150
0.175
0.200
0.225
0.250Final Accuracy
Hotpot QA
Train D*
Train D* + 1D
Train D* + 2D
Train D* + 3D
Figure 6: Test-Time Documents Varying: To analyze how robust RAFT is to varying number
of test-time documents, we study three domains – NQ, Trivia QA and HotPot QA. In NQ,
we find that training with 4 documents leads to optimal performance, and this changes to 3
and 2 for for Trivia QA and HotPot QA respectively. However, we see that training with
only golden documents leads to poor performance.
training with D∗ + 3D and it is D∗ + 1D documents with Hotpot QA. This insight has been
particularly beneficial for our algorithm, RAFT . In our experiments, we consistently employ
a training setup consisting of one golden document alongside four distractor documents.
Generalization to a variable number of test-time documents. We extended our research
to examine the impact of different quantities of test-time documents on the model’s per-
formance. Specifically, our experiments focused on assessing how models, trained with
varying numbers of distractor documents, respond to changes in the number of documents
presented at test time. The results, illustrated in Fig. 6, confirm that the inclusion of distrac-
tor documents during training indeed makes the model more resilient to fluctuations in the
number of documents encountered during testing. This ability to maintain consistent perfor-
mance despite variations in test-time document numbers further validates the robustness of
our approach, RAFT . This finding underscores the importance of a well-calibrated training
environment to prepare the model for a range of scenarios it may encounter in real-world.
6 Related Works
Retrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)
enhance LLMs by integrating a retrieval module that sources relevant information from
external knowledge bases, significantly improving performance across various NLP tasks,
including language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,
2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;
Wang et al., 2023) and open-domain question answering (Izacard et al., 2023; Lewis et al.,
2020). For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever,
treating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the
decoder-only architecture to include retrieved texts and conducts pre-training from scratch.
kNN-LM (Khandelwal et al., 2019) interpolates between the LM’s next token distribution
and distributions computed from retrieved tokens at inference. (Shi et al., 2023d; Ram
et al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or
fine-tuned retriever.
Memorization A key question around large neural language models is whether they truly
“understand” text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern
memorization (Carlini et al., 2019; Tänzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;
2022) develop methodologies to quantify the extent of memorization in neural models.
(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization
impacts the models’ generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)
demonstrated the ability of language models to memorize and regurgitate training data,
raising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).
Finetuning for RAG More recently, several papers have been exploring the idea of fine-
tuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu
9



### Claim 13/179

#### Claim Text
Additionally, employing sub-queries and hypothetical document embeddings (HyDE) [11] seeks to improve retrieval relevance by focusing on embedding similarities between generated answers and real documents.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[11]_2212.10496.pdf (Page 2):

instruction following generative LLM, as described
above.
Zero-Shot Dense Retrieval The tasks of zero-
shot (dense) retrieval are arguably empirically de-
ﬁned by Thakur et al. (2021) for the neural re-
trieval community. Their BEIR benchmark con-
sists of diverse retrieval tasks. The paper and
many follow-up research generally consider the
Transfer Learning setup where the dense re-
triever is ﬁrst learned using a diverse and richly
supervised corpus and query collection, namely
MS-MARCO (Thakur et al., 2021; Wang et al.,
2022; Yu et al., 2022).
However, as stated by Izacard et al. (2021), such
a large collection can rarely be assumed. In this
paper, therefore, we study the problem of building
effective dense retrieval systems without relevance
labels. Similar to Izacard et al. (2021), we also
do not assume access to the test time corpora for
training. This is a more realistic setup and prevents
over-engineering on the test corpora.
By the deﬁnition in Sachan et al. (2022), our
setup can be roughly considered as “unsuper-
vised”. Strictly, as with Sachan et al. (2022), the
only supervision resides in the LLM, in the pro-
cessing of learning to follow instructions.
Generative Retrieval Generative search is a new
class of retrieval methods that use neural generative
models as search indices (Metzler et al., 2021; Tay
et al., 2022; Bevilacqua et al., 2022; Lee et al.,
2022). These models use (constrained) decoding
to generate document identiﬁers, such as id and
sub-string, which map directly to real documents.
They have to go through special training procedures
over relevance data; effective search may also need
to use novel forms of search indices (Bevilacqua
et al., 2022; Lee et al., 2022). In comparison, our
method uses the standard MIPS index and requires
no training or training data. Our generative model
produces an intermediate hypothetical document
to be fed into a dense encoder, instead of a real
document.
3 Methodology
In this section, we ﬁrst formally deﬁne the prob-
lem of (zero-shot) dense retrieval. Then we will
introduce how HyDE is designed to solve it.
3.1 Preliminaries
Dense retrieval models similarity between query
and document with inner product similarity. Given
a query q and document d, it uses two encoder
function encq and encd to map them into d dimen-
sion vectors vq, vd, whose inner product is used
as similarity measurement.
sim(q, d) =⟨encq(q), encd(d)⟩ = ⟨vq, vd⟩ (1)
For zero-shot retrieval, we consider L query sets
Q1, Q2, ..., QL and their corresponding search cor-
pus, document sets D1, D2, ..., DL. Denote the
j-th query from i-th set query set Qi as qij. We
need to fully deﬁne mapping functions encq and
encd without access to any query set Qi, document
set Di, or any relevance judgment rij.
The difﬁculty of zero-shot dense retrieval lies
precisely in Equation 1: it requires learning of two
embedding functions (for query and document re-
spectively) into the same embedding space where
inner product captures relevance. Without rele-
vance judgments/scores to ﬁt, learning becomes
intractable.
3.2 HyDE
HyDE circumvents the aforementioned learning
problem by performing search in document-
only embedding space that captures document-
document similarity. This can be easily learned
using unsupervised contrastive learning (Izacard
et al., 2021; Gao et al., 2021; Gao and Callan,
2022). We set document encoder encd directly as a
contrastive encoder enccon.
f = encd = enccon (2)
This function is also denoted as f for simplic-
ity. This unsupervised contrastive encoder will
be shared by all incoming document corpus.
vd = f(d) ∀d ∈ D1 ∪ D2 ∪ ... ∪ DL (3)
To build the query vector, we consider in addition
an instruction following LM, InstructLM. It takes a
query q and a textual instruction INST and follows
them to perform the task speciﬁed by INST . For
simplicity, denote,
g(q, INST ) =InstructLM(q, INST ) (4)
Now we can use g to map queries to "hypotheti-
cal" documents by sampling from g, setting INST



Source: data\tc16_2312.10997v5\referenced_papers\[11]_2212.10496.pdf (Page 3):

to be “write a paragraph that answers the
question”. The generated document is not real,
can and is likely to be ungrounded factually (Brown
et al., 2020; Thoppilan et al., 2022). We only re-
quire it to capture relevance pattern. This is done
by generating documents, i.e. providing exam-
ples. Critically, here we ofﬂoad relevance mod-
eling from representation learning model to an
NLG model that generalizes signiﬁcantly more eas-
ily, naturally, and effectively (Brown et al., 2020;
Ouyang et al., 2022). Generating examples also
replaces explicit modeling of relevance scores.
We can now encode the generated document using
the document encoder f. Write,
E[vqij ] =E[f(g(qij, INST i))] (5)
Formally, g deﬁnes a probability distribution based
on the chain rule. In this paper, we simply consider
the expectation value, assuming the distribution of
vqij is uni-modal, i.e. the query is not ambiguous.
The study of ambiguous queries and diversity is
left to future work. We estimate Equation 5 by
sampling N documents from g, [ ˆd1, ˆd2, ...,ˆdN ].
ˆvqij = 1
N
∑
ˆdk∼g(qij ,INST i)
f(dk) (6)
= 1
N
N∑
k=1
f( ˆdk) (7)
We also consider the query as a possible hypothesis,
ˆvqij = 1
N + 1[
N∑
k=1
f( ˆdk) +f(qij)] (8)
Inner product is computed between ˆvqij and the
set of all document vectors {f(d)|d ∈ Di}. The
most similar documents are retrieved. Here the
encoder function f serves as a lossy compressor
that outputs dense vectors, where the extra details
are ﬁltered and left out from the vector. It further
grounds the hypothetical vector to the actual corpus
and the real documents. The full HyDE system is
illustrated in Figure 1.
4 Experiments
4.1 Setup
Implementation We implement HyDE using
InstructGPT, a GPT-3 model from the instruct
series (text-davinci-003; Ouyang et al. (2022))
and Contrievermodels (Izacard et al., 2021). We
sample from InstructGPT using the OpenAI play-
ground default temperature of 0.7 for open-ended
generations. We use the English-only Contriever
model for English retrieval tasks and multilingual
mContrieverfor non-English tasks. We conducted
retrieval experiments with the Pyserini toolkit (Lin
et al., 2021a).
Datasets We consider web search query sets
TREC DL19 (Craswell et al., 2020a) and
DL20 (Craswell et al., 2020b); they are based on
the MS-MARCO dataset (Bajaj et al., 2016). We
also use a diverse collection of 6 low-resource
datasets from the BEIR dataset (Thakur et al.,
2021). For non-English retrieval, we consider
Swahili, Korean, Japanese, and Bengali from the
Mr.Tydi dataset (Zhang et al., 2021).
We use different instructions for each dataset.
They share a similar structure but have different
quantiﬁers to control the exact form of the gener-
ated hypothetical documents. These instructions
can be found in subsection A.1.
Compared Systems Contriever models,
Contrieverand mContriever, serve as our major
baseline. They are trained using unsupervised
contrastive learning. HyDE retrievers share the
exact same embedding spaces with them. The
only difference is how the query vector is built.
These comparisons allow us to easily examine
the effect of HyDE. The classical heuristic-based
lexical retriever BM25 is also included.
Several systems that involve ﬁne-tuning on mas-
sive relevance data are also included as refer-
ences. We consider models ﬁne-tuned on MS-
MARCO and transferred, DPR and ANCE, from
the BEIR paper. For multilingual, we include
the mDPR model from Mr.Tydi paper and MS-
MARCO ﬁne-tuned mBERT and XLM-R from
the Contriever paper. We also include the state-of-
the-art transfer learning models: Contrieverand
mContrieverﬁne-tuned on MS-MARCO, denoted
ContrieverFT and mContrieverFT. These mod-
els have run through the state-of-the-art retrieval
model training pipeline that involves second-stage
retrieval-speciﬁc pre-training (Lee et al., 2019) and
a few rounds of ﬁne-tuning (Qu et al., 2021); they
should be considered empirical upper bounds.
4.2 Web Search
In Table 1, we show retrieval results on TREC
DL19 and TREC DL20. We see HyDEbring sizable
improvements to Contriever across the board for



Source: data\tc16_2312.10997v5\referenced_papers\[11]_2212.10496.pdf (Page 1):

HyDE 
GPT
Contriever
how long does it take to remove
wisdom tooth It usually takes between 30
minutes and two hours to
remove a wisdom tooth...
How wisdom teeth are removed... 
Some ... a few minutes, whereas
others can take 20 minutes or
longer....
How has the COVID-19 pandemic impacted
mental health?
...depression and anxiety had
increased by 20% since the
start of the pandemic...
... two studies investigating
COVID-19 patients ... significantly
higher level of depressive ...
write a passage to answer the question
write a scientific paper passage to answer
the question
인간은  언제  불을  사용했는가 ?
write a passage in Korean to answer the
question in detail
인간이  불을  사용한  기록은  약 
800 만년  전부터  나타난다 ... ... 불을  처음  사용한  시기는  호모 
에렉투스가  살았던  142 만  년  전으 
로  거슬러간다 ...
instruction query generated document real document
Figure 1: An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries
without changing the underlying GPT-3 and Contriever/mContriever models.
to human intent to follow instructions.
With these ingredients, we propose to
pivot through Hypothetical Document
Embeddings ( HyDE), and decompose dense
retrieval into two tasks, a generative task per-
formed by an instruction-following language
model and a document-document similarity task
performed by a contrastive encoder (Figure 1).
First, we feed the query to the generative model
and instruct it to "write a document that answers
the question", i.e. a hypothetical document.
We expect the generative process to capture
"relevance" by giving an example; the generated
document is not real, can contain factual errors but
is like a relevant document. In the second step,
we use an unsupervised contrastive encoder to
encode this document into an embedding vector.
Here, we expect the encoder’s dense bottleneck
to serve a lossy compressor, where the extra
(hallucinated) details are ﬁltered out from the
embedding. We use this vector to search against
the corpus embeddings. The most similar real
documents are retrieved and returned. The retrieval
leverages document-document similarity encoded
in the inner-product during contrastive training.
Note that, interestingly, with HyDE factorization,
the query-document similarity score is no longer
explicitly modeled nor computed. Instead, the
retrieval task is cast into two NLU and NLG tasks.
HyDEappears unsupervised. No model is trained
in HyDE: both the generative model and the con-
trastive encoder remain intact. Supervision signals
were only involved in instruction learning of our
backbone LLM.
In our experiments, we showHyDEusing Instruct-
GPT (Ouyang et al., 2022) and Contriever (Izacard
et al., 2021) as backbone models signiﬁcantly out-
performs the previous state-of-the-art Contriever-
only zero-shot no-relevance system on 11 queries
sets, covering tasks like Web Search, Question
Answering, Fact Veriﬁcation and languages like
Swahili, Korean, Japanese.
2 Related Works
Dense Retrieval (Lee et al., 2019; Karpukhin
et al., 2020) has been extensively studied after the
emergence of pre-trained Transformer language
models (Devlin et al., 2019). Researchers stud-
ied the metric learning problems, such as training
loss (Karpukhin et al., 2020) and negative sam-
pling (Xiong et al., 2021; Qu et al., 2021), and also
introduced distillation (Qu et al., 2021; Lin et al.,
2021b; Hofstätter et al., 2021). Later works studied
the second stage pre-training of language model
speciﬁcally for retrieval (Izacard et al., 2021; Gao
and Callan, 2021; Lu et al., 2021; Gao and Callan,
2022; Liu and Shao, 2022).
The popularity of dense retrieval can be partially
attributed to the rich and successful research in very
efﬁcient minimum inner product search (MIPS) at
very large (billion) scales (Johnson et al., 2017).
Instructions-Following Language Models
Soon after the emergence of LLMs, several groups
of researchers discover that LLMs trained on data
consisting of instructions and their execution can
zero-shot generalize to perform new tasks with new
instructions (Ouyang et al., 2022; Sanh et al., 2022;
Min et al., 2022; Wei et al., 2022). This can be
done by standard supervised sequence-to-sequence
learning or more effectively with reinforcement
learning (Ouyang et al., 2022).
Concurrent to us, Asai et al. (2022) studied
“Task-aware Retrieval with Instructions”. They
ﬁne-tuned dense encoders that can also encode
task-speciﬁc instruction prepended to query. In
comparison, we use an unsupervised encoder and
handle different tasks and their instruction with an



Source: data\tc16_2312.10997v5\referenced_papers\[11]_2212.10496.pdf (Page 0):

Precise Zero-Shot Dense Retrieval without Relevance Labels
Luyu Gao∗† Xueguang Ma∗‡ Jimmy Lin‡ Jamie Callan†
†Language Technologies Institute, Carnegie Mellon University
‡David R. Cheriton School of Computer Science, University of Waterloo
{luyug, callan}@cs.cmu.edu, {x93ma, jimmylin}@uwaterloo.ca
Abstract
While dense retrieval has been shown effec-
tive and efﬁcient across tasks and languages,
it remains difﬁcult to create effective fully
zero-shot dense retrieval systems when no rel-
evance label is available. In this paper, we
recognize the difﬁculty of zero-shot learning
and encoding relevance. Instead, we pro-
pose to pivot through Hypothetical Document
Embeddings (HyDE). Given a query, HyDE ﬁrst
zero-shot instructs an instruction-following
language model (e.g. InstructGPT) to gen-
erate a hypothetical document. The docu-
ment captures relevance patterns but is unreal
and may contain false details. Then, an un-
supervised contrastively learned encoder (e.g.
Contriever) encodes the document into an
embedding vector. This vector identiﬁes a
neighborhood in the corpus embedding space,
where similar real documents are retrieved
based on vector similarity. This second step
ground the generated document to the actual
corpus, with the encoder’s dense bottleneck
ﬁltering out the incorrect details. Our exper-
iments show that HyDE signiﬁcantly outper-
forms the state-of-the-art unsupervised dense
retriever Contriever and shows strong per-
formance comparable to ﬁne-tuned retrievers,
across various tasks (e.g. web search, QA, fact
veriﬁcation) and languages (e.g. sw, ko, ja).1
1 Introduction
Dense retrieval (Lee et al., 2019; Karpukhin et al.,
2020), the method of retrieving documents using
semantic embedding similarities, has been shown
successful across tasks like web search, question
answering, and fact veriﬁcation. A variety of meth-
ods such as negative mining (Xiong et al., 2021; Qu
et al., 2021), distillation (Qu et al., 2021; Lin et al.,
2021b; Hofstätter et al., 2021) and task-speciﬁc
∗ Equal contribution.
1No models were trained or ﬁne-tuned in making this pre-
print. Our open source code is available at https://github.
com/texttron/hyde.
pre-training (Izacard et al., 2021; Gao and Callan,
2021; Lu et al., 2021; Gao and Callan, 2022; Liu
and Shao, 2022) have been proposed to improve the
effectiveness of supervised dense retrieval models.
On the other hand, zero-shot dense retrieval still
remains difﬁcult. Many recent works consider the
alternative transfer learning setup, where the dense
retrievers are trained on a high-resource dataset and
then evaluated on queries from new tasks. The MS-
MARCO collection (Bajaj et al., 2016), a massive
judged dataset with a large number of judged query-
document pairs, is arguably the most commonly
used. As argued by Izacard et al. (2021), in prac-
tice, however, the existence of such a large dataset
cannot always be assumed. Even MS-MARCO re-
stricts commercial use and cannot be adopted in a
variety of real-world search scenarios.
In this paper, we aim to build effective fully
zero-shot dense retrieval systems that require no
relevance supervision, work out-of-box and gener-
alize across tasks. As supervision is not available,
we start by examining self-supervised representa-
tion learning methods. Modern deep learning en-
ables two distinct learning algorithms. At the token
level, generative large language models (LLM) pre-
trained on large corpus have demonstrated strong
natural language understanding (NLU) and gen-
eration (NLG) capabilities (Brown et al., 2020;
Chen et al., 2021; Rae et al., 2021; Hoffmann
et al., 2022; Thoppilan et al., 2022; Chowdhery
et al., 2022). At the document level, text (chunk)
encoders pre-trained with contrastive objectives
learn to encode document-document similarity into
inner-product (Izacard et al., 2021; Gao and Callan,
2022). On top of these, one extra insight into LLM
is borrowed: the LLMs further trained to follow
instructions can zero-shot generalize to diverse un-
seen instructions (Ouyang et al., 2022; Sanh et al.,
2022; Min et al., 2022; Wei et al., 2022). Ouyang
et al. (2022) show that with a small amount of data,
GPT-3 (Brown et al., 2020) models can be aligned
arXiv:2212.10496v1  [cs.IR]  20 Dec 2022



Source: data\tc16_2312.10997v5\referenced_papers\[11]_2212.10496.pdf (Page 5):

Swahili Korean Japanese Bengali
w/o relevance judgement
BM25 38.9 28.5 21.2 41.8
mContriever 38.3 22.3 19.5 35.3
HyDE 41.7 30.6 30.7 41.3
w/ relevance judgement
mDPR 7.3 21.9 18.1 25.8
mBERT 37.4 28.1 27.1 35.1
XLM-R 35.1 32.2 24.8 41.7
mContrieverFT 51.2 34.2 32.4 42.3
Table 3: MRR@100 on Mr.Tydi. Best performing w/o
relevance and overall system(s) are marked bold.
under-speciﬁcation of the instruction; more elabo-
rative instructions may help.
4.4 Multilingual Retrieval
Multilingual setup poses several additional chal-
lenges to HyDE. The small-sized contrastive en-
coder gets saturated as the number of languages
scales (Conneau et al., 2020; Izacard et al., 2021).
Meanwhile, our generative LLM faces an opposite
issue: with languages of not as high resource as
English or French, the high capacity LLM can get
under-trained (Hoffmann et al., 2022).
Nevertheless, in Table 3, we still ﬁnd HyDE
able to improve the mContriever model. It can
outperform non-Contriever models ﬁne-tuned on
and transferred from MS-MARCO. On the other
hand, we do observe some margins between HyDE
and ﬁne-tuned mContrieverFT. Since HyDE and
mContrieverFT use similar contrastive encoders,
we hypothesize this is because the non-English lan-
guages we considered are under-trained in both
pre-training and instruction learning stages.
5 Analysis
The generative LLM and contrastive encoder make
up the backbone of HyDE. In this section, we study
the effect of changing their realizations. In partic-
ular, we consider smaller language models (LM)
and ﬁne-tuned encoders. We conduct our studies
on TREC DL19/20.
5.1 Effect of Different Generative Models
In Table 4, we show HyDE using other
instruction-following language models. In
particular, we consider a 52-billion Cohere
model ( command-xlarge-20221108) and a
11-billion FLAN model ( FLAN-T5-xxl; Wei
et al. (2022)). 2 Generally, we observe that all
2Model sizes are from https://crfm.stanford.edu/
helm/v1.0/?models.
Model DL19 DL20
Contriever 44.5 42.1
ContrieverFT 62.1 63.2
HyDE
w/ Contriever
w/ Flan-T5 (11b) 48.9 52.9
w/ Cohere (52b) 53.8 53.8
w/ GPT (175b) 61.3 57.9
w/ ContrieverFT
w/ Flan-T5 (11b) 60.2 62.1
w/ Cohere (52b) 61.4 63.1
w/ GPT (175b) 67.4 63.5
Table 4: NDCG@10 on TREC DL19/20. Effect
of changing different instruction LMs and using ﬁne-
tuned encoder. Best w/o relevance and overall models
are marked bold.
models bring improvement to the unsupervised
Contriever, with larger models bringing larger
improvements. At the time when this paper is
written, the Cohere model is still experimental
without much detail disclosed. We can only
tentatively hypothesize that training techniques
may have also played some role in the performance
difference.
5.2 HyDE with Fine-tuned Encoder
To begin with, HyDE with ﬁne-tuned encoder is
not the intended usage: HyDE is more powerful
and irreplaceable when few relevance labels are
present. Here we are interested to ﬁnd out if
and how HyDE embedding can affect ﬁne-tuned en-
coders. In Table 4, we see that less powerful instruc-
tion LMs can negatively impact the overall perfor-
mance of the ﬁne-tuned retriever. (To remind our
readers, ContrieverFT is in-domain supervisedly
ﬁne-tuned for TREC DL19/20). The performance
degradations remain small. On the other hand, we
also observe the InstructGPT model able to fur-
ther bring up the performance, especially on DL19.
This suggests that there may still exist certain fac-
tors not captured by the ﬁne-tuned encoder but only
by the generative model.
6 Conclusion
At the end of the paper, we encourage the readers
to take a moment and reﬂect on the HyDE model.
Compare it to some of the other recently seen re-
trievers or re-ranker. These other models probably
differ in their architecture, training method, and/or
task, but probably all of them involve modeling
relevance scores between a pair of query and docu-



### Claim 14/179

#### Claim Text
Adjustments in module arrangement and interaction, such as the Demonstrate-Search-Predict (DSP) [23] framework and the iterative Retrieve-Read-Retrieve-Read flow of ITERRETGEN [14], showcase the dynamic use of module outputs to bolster another module’s functionality, illustrating a sophisticated understanding of enhancing module synergy.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 2):

Question:
Retrieval:
Retrieval -Augmented Generation: 
What is the height of the player who won the 2015 AFL 
Rising Star award?
Title: 2015 AFL Rising Star Context: The NAB AFL Rising 
Star award is given annually to a stand out young player in 
the Australian Football League. The award was won by Jesse 
Hogan of Melbourne
The 2015 AFL Rising Star award was won by Jesse Hogan of 
Melbourne. Jesse Hogan is a professional Australian rules 
footballer. He is 198 cm tall. So the answer is 198 cm
Title: Jesse Hogan Context: Jesse Hogan … playing for the 
Melbourne Football Club. A key forward, Hogan is 1.95 m 
tall … made his AFL debut in the 2015 season and won the 
Ron Evans Medal as the AFL Rising Star
The 2015 AFL Rising Star award was won by Jesse Hogan of 
Melbourne. Jesse Hogan is 1.95 m tall.
So the answer is 1.95 m
Iteration 1 Iteration 2
Generation-Augmented Retrieval:
Retrieval -Augmented Generation: 
Question:
What is the height of the player who won the 2015 AFL 
Rising Star award?
Figure 1: ITER -RETGEN iterates retrieval and generation. In each iteration, I TER -RETGEN leverages the model
output from the previous iteration as a specific context to help retrieve more relevant knowledge, which may help
improve model generation (e.g., correcting the height of Hesse Hogan in this figure). We only show two iterations
in this figure for brevity. Solid arrows connect queries to the retrieved knowledge, and dashed arrows denote
retrieval-augmented generation.
relevant information which helps improve genera-
tion in the next iteration. ITER -RETGEN also has
the advantage of processing all retrieved knowl-
edge as a whole during the generation process, and
is conceptually simpler and easier-to-implement,
while being empirically strong in multi-hop ques-
tion answering, fact verification, and commonsense
reasoning.
A closely related work called GAR (Mao et al.,
2021) augments queries with generated background
information. HyDE (Gao et al., 2022a) also shares
a similar spirit, but focuses on zero-shot informa-
tion retrieval, and proposes to first prompt an LLM
to produce “hypothetical” paragraphs that cover the
information needed to answer a given question, and
then use the generated paragraphs to retrieve the
real ones. RepoCoder (Zhang et al., 2023) focuses
on repository-level code completion, and proposes
a 2-iteration retrieval-generation paradigm where
the second iteration leverages the intermediate code
completion for retrieval. By contrast, we propose
to synergize retrieval and generation with I TER -
RETGEN on various natural language tasks, and
explore how we can further adapt retrieval with
model generations.
3 Iterative Retrieval-Generation Synergy
3.1 Overview
Given a question q and a retrieval corpus D =
{d} where d is a paragraph, ITER -RETGEN repeats
retrieval-generation for T iterations; in iteration
t, we (1) leverage the generation yt−1 from the
previous iteration, concatenated with q, to retrieve
top-k paragraphs, and then (2) prompt an LLM M
to produce an output yt, with both the retrieved
paragraphs (denoted as Dyt−1||q) and q integrated
into the prompt. Therefore, each iteration can be
formulated as follows:
yt = M(yt|prompt(Dyt−1||q, q)), ∀1 ≤ t ≤ T (1)
The last output yT will be produced as the final
response.
3.2 Generation-Augmented Retrieval
There are many natural language tasks with com-
plex information needs. For example, in open-
domain multi-hop question answering, specific in-
formation needs may manifest themselves only
after correctly answering some prerequisite sub-
questions. In other words, there may exist semantic
gaps between the original question q and its sup-
porting knowledge, which can not be effectively
addressed by a retriever with a representation bot-
tleneck. In the first iteration, we can retrieve knowl-
edge with only the question q. In later iterations,
the LLM output from the previous iteration, though
having no guarantee of correctness, shows what
might be needed to answer the question, and thus
can be leveraged to bridge the semantic gaps; with
improved retrieval, an LLM can potentially pro-
duce a better output.
3.3 Retrieval-Augmented Generation
In each iteration, we generate an output using
Chain-of-Thought prompting except that we also
prepend retrieved knowledge to the question q.
Though there may exist more advanced prompting



Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 8):

of whether the in-context non-parametric knowl-
edge mentions the answers or not. This indicates
that when the in-context non-parametric knowledge
is irrelevant or incomplete, ITER -RETGEN exploits
parametric knowledge better than Self-Ask.
4.7 Error Analysis
On HotPotQA, we manually analyzed 20 random
cases where ITER -RETGEN (T = 2) fails. 25% of
predictions are false negatives. On 10% of cases,
ITER -RETGEN retrieves all necessary information
but fails to perform correct reasoning. The remain-
ing 65% of error cases are related with retrieval, on
76.9% of which, retrieval is misled by completely
wrong reasoning from the first iteration, while on
the other cases, reasoning in the first iteration is
partially correct, but the retriever fails to retrieve
the missing pieces in the second iteration. We also
observed that, in the first iteration, reasoning can be
negatively affected by noisy and possibly distrac-
tive knowledge retrieved using only the questions
as the queries.
5 Case Study
Table 7 demonstrates retrieval-generation synergy
with two examples from HotPotQA and Strate-
gyQA, respectively. In the first iteration, as both
questions need multi-hop reasoning, the retriever
fails to retrieve all supporting knowledge using only
the questions. Despite being affected by distrac-
tive retrieved knowledge (the capacity of a different
arena in the example from HotPotQA) and show-
ing imperfect parametric knowledge (the generated
statement that Raclette is unlikely to be found in
Paris in the example from StrategyQA) in the first
iteration, the LLM generates phrases that help re-
trieve relevant knowledge in the second iteration,
and successfully corrects its outputs.
6 Conclusion
We demonstrate the effectiveness of ITER -RETGEN
in answering questions with complex information
needs. Despite simple, I TER -RETGEN outper-
forms retrieval-augmented methods that have a
more complex workflow, which we believe could
serve as a strong baseline for future research on
retrieval-augmented generation. We also show that
generation-augmented retrieval adaptation can fur-
ther improve the performance of I TER -RETGEN
while also reducing overheads.
Limitations
In this work, we propose to enhance retrieval-
augmented large language models with I TER -
RETGEN which synergizes retrieval and generation
in an iterative manner, and demonstrates strong
performance compared to more structured prompt-
ing techniques such as Self-Ask. However, it’s
worth noting that our experiments utilized a fixed
black-box large language model, which may not
have been equally optimized for various forms of
prompting. It would be intriguing to investigate the
potential of prompting-specific (gradient-based) op-
timization in pushing the limits further. This could
involve enabling a large language model to leverage
parametric and non-parametric knowledge more
flexibly and effectively. By exploring this avenue,
we may uncover new insights and advancements
in the field. Furthermore, our experiments did not
cover long-form generation which would probably
benefit from more fine-grained retrieval than ITER -
RETGEN does in this work. We acknowledge that
this area warrants further exploration, and we leave
it for future work.
Acknowledgements
Zhihong Shao and Minlie Huang were supported by
the National Science Foundation for Distinguished
Young Scholars (with No. 62125604) and the
NSFC projects (Key project with No. 61936010).
They were also supported by the Guoqiang In-
stitute of Tsinghua University, with Grant No.
2020GQG0005.



Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 0):

Enhancing Retrieval-Augmented Large Language Models with Iterative
Retrieval-Generation Synergy
Zhihong Shao1, Yeyun Gong2, yelong shen3, Minlie Huang1∗, Nan Duan2, Weizhu Chen3
1 The CoAI Group, DCST, Institute for Artificial Intelligence,
1 State Key Lab of Intelligent Technology and Systems,
1 Beijing National Research Center for Information Science and Technology,
1 Tsinghua University, Beijing 100084, China
2 Microsoft Research Asia 3 Microsoft Azure AI
szh19@mails.tsinghua.edu.cn aihuang@tsinghua.edu.cn
Abstract
Retrieval-augmented generation has raise exten-
sive attention as it is promising to address the
limitations of large language models including
outdated knowledge and hallucinations. How-
ever, retrievers struggle to capture relevance,
especially for queries with complex informa-
tion needs. Recent work has proposed to im-
prove relevance modeling by having large lan-
guage models actively involved in retrieval, i.e.,
to guide retrieval with generation. In this pa-
per, we show that strong performance can be
achieved by a method we call ITER -RETGEN,
which synergizes retrieval and generation in an
iterative manner: a model’s response to a task
input shows what might be needed to finish
the task, and thus can serve as an informative
context for retrieving more relevant knowledge
which in turn helps generate a better response
in another iteration. Compared with recent
work which interleaves retrieval with gener-
ation when completing a single output, I TER -
RETGEN processes all retrieved knowledge as
a whole and largely preserves the flexibility in
generation without structural constraints. We
evaluate ITER -RETGEN on multi-hop question
answering, fact verification, and commonsense
reasoning, and show that it can flexibly lever-
age parametric knowledge and non-parametric
knowledge, and is superior to or competitive
with state-of-the-art retrieval-augmented base-
lines while causing fewer overheads of retrieval
and generation. We can further improve per-
formance via generation-augmented retrieval
adaptation.
1 Introduction
Generative Large Language Models (LLMs)
have powered numerous applications, with well-
perceived utility. Despite being powerful, LLMs
lack knowledge that is under-represented in their
training data, and are prone to hallucinations, es-
pecially in open-domain settings (OpenAI, 2023).
∗*Corresponding author: Minlie Huang.
Retrieval-augmented LLMs, therefore, have raised
widespread attention as LLM outputs can be poten-
tially grounded on external knowledge.
Previous retrieval-augmented LMs (Izacard
et al., 2022b; Shi et al., 2023) typically adopted
one-time retrieval, i.e., to retrieve knowledge us-
ing only the task input (e.g., a user question for
open-domain question answering). One-time re-
trieval should suffice to fulfill the information needs
if they are clearly stated in the original input,
which is applicable to factoid question answering
(Kwiatkowski et al., 2019) and single-hop fact ver-
ification (Thorne et al., 2018), but not to tasks with
complex information needs, e.g., multi-hop rea-
soning (Yang et al., 2018) and long-form question
answering (Fan et al., 2019).
To fulfill complex information needs, recent
work proposes to gather required knowledge multi-
ple times throughout the generation process, using
partial generation (Trivedi et al., 2022a; Press et al.,
2022)) or forward-looking sentence(s) (Jiang et al.,
2023) as search queries. However, such structured
workflows of interleaving retrieval with generation
have the following limitations: (1) as intermediate
generation is conditioned on knowledge retrieved
before, with no awareness of knowledge retrieved
afterwards, they fail to process all retrieved knowl-
edge as a whole during the generation process; (2)
they require multi-round retrieval to gather a com-
prehensive set of knowledge, and may frequently
change the prompts by updating newly retrieved
knowledge, thus increasing the overheads of both
retrieval and generation.
In this paper, we find it simple but effective to
enhance retrieval-augmented LLMs through itera-
tive retrieval-generation synergy (ITER -RETGEN,
Fig 1). ITER -RETGEN iterates retrieval-augmented
generation and generation-augmented retrieval:
Retrieval-augmented generation outputs a response
to a task input based on all retrieved knowledge
(initially using the task input as the query). This
arXiv:2305.15294v2  [cs.CL]  23 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[59]_2310.05149.pdf (Page 1):

2. ITERATIVE RETRIEV AL-GENERATION
SYNERGY
In this section, we first introduce the overall framework, and
then introduce the retrieval-generation collaboration frame-
work in detail, including generation augmented retrieval and
retrieval augmented generation.
2.1. Overview
We show the framework of ITRG in Figure 2. Given a user
question q and a document corpus D = {di}|D|
i=1 (i.e, di is a
Wikipedia paragraph.), ITRG repeats generation augmented
retrieval (GAR) and retrieval augmented generation (RAG)
for T iterations. In the GAR process of iteration t, we con-
catenate the output yt−1 of the last iteration and question q to
form a new query, and then use a dense retriever to retrieve
top-k paragraphs. In the first iteration, we only use the ques-
tion as the query. In the RAG process of iteration t, based on
the question q and the retrieved top-k paragraphs, we exploit
large language models to generate new paragraphs to answer
questions. Specifically, we propose two methods to generate
new paragraphs, which will be introduced in detail in §2.3.
2.2. Generation Augmented Retrieval
Knowledge-intensive tasks (e.g., open-domain question an-
swering) often require access to additional documents. A
common approach is to directly employ the question as the
query, and then equip a sparse or dense retriever to retrieve
relevant documents. In practice, we find that in some cases
using the question directly as the query fails to retrieve rel-
evant documents because there may exist semantic gaps be-
tween them. To alleviate this problem, we propose a simple
query expansion method. At the first iteration (t = 1), we use
the original question q as the query. At iteration t (t >1), we
concatenate the original question q and the document gener-
ated yt−1 in the last iteration as the new query qt = [q; yt−1].
Then, we utilize a pre-trained dense retriever to retrieve top-k
documents, which are denoted as Rt = {d}.
Given an input question q, the retriever aims to retrieve
a small set of documents from a corpus D = {di}|D|
i=1 that
are relevant to q. Following prior work [14], we use a dense
retriever based on the dual encoder architecture, where an en-
coder is used to encode both the input contextq and the docu-
ment d. Specifically, the encoder maps each document d ∈ D
to an embedding E(d) by taking the mean pooling of the last
hidden representation over the tokens in d. At query time,
the same encoder is applied to the input context q to obtain a
query embedding E(q). The similarity between the query em-
bedding and the document embedding is computed by their
cosine similarity: s(d, q) = cos(E(d), E(q)). The top-k doc-
uments that have the highest similarity scores are retrieved.
Question: What is the date of birth of Emilie HeghArntzen'smother?Retrieval:
Generation:
Retrieval:
Generation:
Retrieval:
Generation:
Iteration 1
Iteration 2
Iteration 3
infoboxname: Emilie HeghArntzen; caption: HeghArntzenin 2018 ; birth_date: January 1, 1994 ; birth_place: Skien, Norway ; nationality: Norwegian ; 
Emilie HeghArntzenwas born on January 1, 1994 in Skien, Norway. Her mother is unknown.
Camilla Marie Gjersemwas born together with a twin sister, Anne Line, on 6 January 1994 in Hønefoss, Norway. Their mother, PerlinaBangug, is a Filipina from Ilagan, Isabela, and their father, PetterGjersem, a Norwegian from Raufoss. Camilla Gjersemis a law student at the University of Oslo.
Hanne Hegh(born 19 January 1960) is a Norwegian handball player. She played 220 matches for the Norwegian national handball team between 1978 and 1992. She is the mother of Emilie HeghArntzen.
infoboxname: Hanne Hegh; caption: Hanne Hegh2008 ; nationality: Norwegian ; birth_date: April 27, 1960; birth_place: Oslo, Norway ; 
Hanne Heghwas born on April 27, 1960 in Oslo, Norway. She is the mother of Emilie HeghArntzen, who was born on January 1, 1994 in Skien, Norway.
Fig. 2: Iterative retrieval-generation synergy framework con-
tains two steps in each iteration: (1) generation augmented
retrieval (GAR): utilize the output of the previous iteration to
expand the query to help retrieve more relevant documents;
(2) retrieval augmented generation (RAG): utilize retrieved
documents to generate new documents to answer questions.
We only show three iterations in this figure for brevity. Solid
arrows indicate RAG within an iteration, and dashed arrows
indicate GAR between iterations. Purple represents correct
and useful information, and red represents wrong or invalid
information.
2.3. Retrieval Augmented Generation
Following previous work [13], for a given question q, we
could directly prompt large language models to generate re-
lated documents without retrieving them from an external cor-
pus. However, we find that if only the parametric knowledge
learned by the large model in the pre-training stage is used,
the generated documents may be incomplete. Retrieval aug-
mented generation (RAG) aims to comprehensively under-
stand the retrieved non-parametric knowledge and the para-
metric knowledge inside large language models to generate
more accurate factual knowledge. Specifically, we propose
two strategies, which will be described in detail below.
2.3.1. Refine
An intuitive idea is to refine the previously generated docu-
ment yt−1 based on the original question q and the retrieved
top-k documents at the current iteration step Rt to obtain a
new document yt. We call this method refine. Considering
that the document retrieved in the last iterationRt−1 has been
used to generate the last document yt−1, we refine the previ-
ous output yt−1 with updated documents Rupdate.
Rupdate = Rt − Rt−1, (1)
yt = M(prompt (yt−1, q, Rupdate)) , (2)



Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 12):

A Experiments Using Llama-2
To demonstrate the effectiveness of ITER -RETGEN
on open-source models, we replaced the generation
model text-davinci-003 in Table 2 with Llama-
2 models (Touvron et al., 2023), and re-ran the
evaluation. As shown in Table 8, I TER -RETGEN
consistently outperforms all baselines significantly.
B Few-Shot Prompts
In this section, we present all few-shot prompts
used in our experiments. We replace retrieved
paragraphs with the placeholder {Knowledge} for
brevity. CoT prompting shares the same in-context
demonstrations with ITER -RETGEN, except that it
is not augmented with retrieval.
B.1 HotPotQA
Prompts for Direct Prompting, ReAct, Self-Ask,
and ITER -RETGEN are presented in Table 9, Table
10, Table 11, and Table 12, respectively.
B.2 2WikiMultiHopQA
Prompts for Direct Prompting, ReAct, Self-Ask,
and ITER -RETGEN are presented in Table 13, Table
14, Table 15, and Table 16, respectively.
B.3 MuSiQue
Prompts for Direct Prompting, ReAct, Self-Ask,
and ITER -RETGEN are presented in Table 17, Table
18, Table 19, and Table 20, respectively.
B.4 Bamboogle
Prompts for Direct Prompting, ReAct, Self-Ask,
and ITER -RETGEN are presented in Table 21, Table
22, Table 23, and Table 24, respectively.
B.5 Feverous
Prompts for Direct Prompting, ReAct, Self-Ask,
and ITER -RETGEN are presented in Table 25, Table
26, Table 27, and Table 28, respectively.
B.6 StrategyQA
Prompts for Direct Prompting, ReAct, Self-Ask,
and ITER -RETGEN are presented in Table 29, Table
30, Table 31, and Table 32, respectively.



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[23]_2212.14024.pdf (Page 1):

DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
QHow many storeys are in...Q In which city did Akeem Ellis play in 2017?A Ellesmere PortQ When was the discoverer of Palomar 4 born?A 1889TrainDemonstratedefdemonstrate(x:Example) -> Example:x.demos = annotate(x.train, attempt)returnxdefattempt(d:Example):d= search(d)d= predict(d)if d.pred ==d.answer: returnd1QHow many storeys are in the castle...Q When was the discoverer of Palomar 4 born?A 1889Hop1Who discovered Palomar 4?Psg1Edwin Hubble discovered Palomar 4...Hop2When was Edwin Powell born?Psg2Edwin Powell Hubble (1889–1953) was...Pred1889x : ExampleQ In which city did Akeem Ellis play...A Ellesmere Port... ...PredWaterloo❌Demos“How many storeys are in the castle David Gregory inherited?”QHow many storeys are in the...Demos. . .Hop1Which castle did David Gregory inherit?Psg1David Gregory inherited Kinnairdy Castle...Hop2How many storeys are in Kinnairdy Castle?Psg2Kinnairdy Castle […] having five storeys...QHow many storeys does the.... . .. . .PredFive storeysSearchdefsearch(x:Example) -> Example:x.hop1 =generate(hop_template)(x).predx.psg1 =retrieve(x.hop1, k=1)[0]x.hop2 =generate(hop_template)(x).predx.psg2 =retrieve(x.hop2, k=1)[0]returnx2 Predictdefpredict(x:Example) -> Example:x.context = [x.psg1, x.psg2]x.pred=generate(qa_template)(x).predreturnx3“Five storeys”
Figure 2.A toy example of a DSP program for multi-hop question answering. Given an input question and a 2-shot training set, the
DEMONSTRATE stage programmatically annotates intermediate transformations on the training examples using a form of weak supervision.
Learning from a resulting demonstration, the SEARCH stage decomposes the complex input question and retrieves supporting information
over two retrieval hops. Finally, the PREDICT stage uses the demonstration and retrieved passages to answer the question.
retrieve-then-read strategy fails because the RM cannot ﬁnd
passages that directly answer the question.
We introduce the DEMONSTRATE –SEARCH –PREDICT
(DSP ) framework for in-context learning, which relies en-
tirely on passing natural language text (and scores) be-
tween a frozen RM and LM. DSP introduces a num-
ber of composable functions that bootstrap training exam-
ples (DEMONSTRATE ), gather information from a knowl-
edge corpus ( SEARCH ), and generate grounded outputs
(PREDICT ), using them to systematically unify techniques
from the retrieval-augmented NLP and the in-context learn-
ing literatures (Lee et al., 2019; Khattab et al., 2021a; Anan-
tha et al., 2020; Gao et al., 2022; Izacard et al., 2022; Dohan
et al., 2022; Zelikman et al., 2022; Zhang et al., 2022).
We use DSP to suggest powerful strategies for knowledge-
intensive tasks with compositions of these techniques. This
reveals new conceptual possibilities for in-context learning
in general (§2), and it allows us to present rich programs
that set new state-of-the-art results (§3).
Figure 1 shows the path that a DSP program might take to
arrive at an answer, and Figure 2 illustrates how a deliberate
program achieves this. Instead of asking the LM to answer
this complex question, the program’sSEARCH stage uses the
LM to generate a query “Which castle did David Gregory
inherit?” The RM retrieves a passage saying Gregory inher-
ited the Kinnairdy Castle. After a second search “hop” ﬁnds
the castle’s number of storeys, thePREDICT stage queries
the LM with these passages to answer the original question.
Although this program implements behaviors such as query
generation, it requires no hand-labeled examples of these
intermediate transformations (i.e., of the queries and pas-
sages of both retrieval hops). Instead, the DEMONSTRATE
stage uses labeled question–answer pairs to implement a
form of weak supervision that programmatically annotates
the transformations invoked within SEARCH and PREDICT .
We evaluate severalDSP programs on answering questions
in open-domain, multi-hop, and conversational settings. In
them, we implement novel and reusable transformations
such as bootstrapping annotations for all of our pipelines
with weak supervision (§2.3), reliably rewriting questions to
resolve conversational dependencies and iteratively decom-
pose complex queries with summarization of intermediate
hops (§2.4), and generating grounded responses from mul-
tiple passages with self-consistency (§2.5). We report pre-
liminary results on Open-SQuAD, HotPotQA, and QReCC
using the frozen LM GPT-3.5 and RM ColBERTv2 (Khat-
tab & Zaharia, 2020; Santhanam et al., 2022b) with no
ﬁne-tuning. Our DSP programs deliver 37–120%, 8–39%,
and 80–290% relative gains against corresponding vanilla
LMs, a standard retrieve-then-read pipeline, and a contem-
poraneous self-ask pipeline (Press et al., 2022), respectively.
Future versions of this report will include additional test
tasks and LM choices.
In summary, this work makes the following contributions.
First, we argue that simple task-agnostic pipelines for in-
context learning should give way to deliberate, task-aware
strategies. Second, we show that this shift need not be a
burden: with DSP , such strategies can be easily expressed
as short programs using composable operators. Third, this
composability spawns powerful capacities, like automati-
cally annotating demonstrations for complex pipelines from
end-task labels. Fourth, for three knowledge-intensive tasks,
we implement rich programs that establish state-of-the-art
results for in-context learning.



Source: data\tc16_2312.10997v5\referenced_papers\[23]_2212.14024.pdf (Page 2):

DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
2. DEMONSTRATE –SEARCH –PREDICT
We now introduce the DSP framework and show its expres-
sive power by suggesting a number of strategies in which
the LM and RM can come together to tackle complex prob-
lems effectively. We show in §3 that such strategies out-
perform existing in-context learning methods. We begin by
discussing the LM and RM foundation modules on which
DSP is built (§2.1) and then the datatypes and control ﬂow
within DSP (§2.2). Subsequently, we discuss each of the
three inference stages: DEMONSTRATE (§2.3), SEARCH
(§2.4), and PREDICT (§2.5).
2.1. Pretrained Modules: LM and RM
A DSP program deﬁnes the communication between the
language model LM and the retrieval model RM.
Language Model We invoke a frozen language model
LM to conditionally generate (or score) text. For each
invocation, the program prepares a prompt that adapts the
LM to a speciﬁc function (e.g., answering questions or
generating queries). A prompt often includes instructions,
a few demonstrations of the desired behavior, and an input
query to be answered.
As in Figure 2, the LM generates not only: (i) the ﬁnal
answer to the input question (in thePREDICT stage), but also
(ii) intermediate “hop” queries to ﬁnd useful information
for the input question (SEARCH ) as well as (iii) exemplar
queries that illustrate how to produce queries for questions
in the training set (DEMONSTRATE ). This systematic use of
the LM is a hallmark of DSP programs.
Retrieval Model DSP programs also invoke a frozen re-
trieval model RM to retrieve the top-k most “relevant”
text sequences for a given query. The RM can index a
massive set of pre-deﬁned passages for scalable search, and
those passages can be updated without changing the retrieval
parameters. The RM accepts free-form textual inputs and
specializes in estimating the relevance (or similarity) of a
text sequence to a query.
As in Figure 2, the RM is responsible for retrieving (i)
passages for each query generated by the LM (during the
SEARCH stage), but also (ii) passages that are used within
demonstrations (DEMONSTRATE ). In the latter case, the
RM’s contributions are less about providing directly rel-
evant information to the input question and more about
helping the LM adapt to the domain and task.
Though not utilized in this example, the RM is also used in
DSP for functions like retrieving “nearest-neighbor” demon-
strations from task training data (DEMONSTRATE ) and se-
lecting well-grounded generated sequences from the LM
(PREDICT ).
2.2. Datatypes and Control Flow
We have implemented the DSP framework in Python. The
present section introduces the core data types and compos-
able functions provided by the framework. We use illustra-
tive code snippets to ground the examples, and to convey
the power that comes from being able to express complex
interactions between the LM and RM in simple programs.
The Example Datatype To conduct a task, a DSP pro-
gram manipulates one or more instances of the Example
datatype. An Example behaves like a Python dictionary
with multiple ﬁelds. The program is typically provided with
a few training examples. The code snippet below illustrates
this for multi-hop question answering.
1 from dsp import Example
2
3 train = [ Example ( question =" When was the discoverer
of Palomar 4 born ?", answer =" 1889 "),
4 Example ( question ="In which city did Akeem
Ellis play in 2017? ", answer =" Ellesmere Port ")]
This snippet contains two labeled examples, each with a
multi-hop question (e.g., “In which city did Akeem Ellis
play in 2017?”) and its short answer (“Ellesmere Port”).
Arbitrary keys and values are allowed within an Example,
though typical values are strings or lists of strings.
In this task, we are unlikely to ﬁnd an individual passage
that provides the answer to any question. For example, the
ﬁrst training example can probably be resolved only by ﬁrst
answering the question of who discovered Palomar (“Edwin
Hubble”) and then addressing the question of Hubble’s birth
date using different evidence passages. We typically assume
that the human-labeled training data do not include labels
for intermediate transformations (e.g., queries for individual
hops) that would be useful for following these steps, and so
it is the job of the DSP program to discover these strategies
via in-context learning.
A DSP Program The following code snippet is a com-
plete program for resolving multi-hop questions like those
in Figure 1, with help from train examples like those above.
1 def multihop_program ( question : str ) -> str :
2 x = Example ( question = question , train = train )
3 x = multihop_demonstrate (x)
4 x = multihop_search (x)
5 x = multihop_predict (x)
6 return x. answer
7
8 multihop_program (" How many storeys does the castle
David Gregory inherited have ?")
9 # => " five storeys "
The program takes the input (here, a question) and outputs
the system output (its short answer). It starts by creating
an Example for the input question and assigning the train
ﬁeld to the training set from the previous snippet. Programs



Source: data\tc16_2312.10997v5\referenced_papers\[23]_2212.14024.pdf (Page 5):

DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
versational search (Del Tredici et al., 2021; Raposo et al.,
2022) pipelines have received much attention. These sys-
tems are typically ﬁne-tuned with many hand-labeled query
“rewrites” (Anantha et al., 2020), “decompositions” (Geva
et al., 2021; Min et al., 2019), or target hops (Yang et al.,
2018; Jiang et al., 2020). Supported with automatic anno-
tations from DEMONSTRATE , the SEARCH stage allows us
to simulate many such strategies and many others in terms
of passing queries, passages, and demonstrations between
the RM and LM. More importantly, SEARCH facilitates our
vision of advanced strategies in which the LM and RM co-
operate to incrementally plan a research path for which the
RM gathers information and the LM identiﬁes next steps.
Case Study Let us build on our running multi-hop exam-
ple as a case study. We can deﬁne multihop_search_v2
(Line 4 in our core program), a slightly more advanced ver-
sion of the SEARCH transformation from Figure 2. This
transformation simulates the iterative retrieval component
of ﬁne-tuned retrieval-augmented systems like IRRR (Qi
et al., 2020), which reads a retrieved passage in every hop
and generates a search query (or a termination condition to
stop hopping), and Baleen (Khattab et al., 2021a), which
summarizes the information from many passages in each
hop for inclusion in subsequent hops.
1 from dsp import generate
2
3 def multihop_search_v2 (x, max_hops =3) :
4 x. hops = []
5
6 for hop in range ( max_hops ):
7 summary , query = generate ( hop_template )(x)
8 x. hops . append (( summary , query ))
9
10 if query == /quotesingle.VarN/A /quotesingle.Var: break
11
12 passages = retrieve ( query , k =5)
13 x. context = [ summary ] + passages
14
15 return x
In multihop_search_v2, Line 7 calls the generate prim-
itive, which invokes the LM to produce a query for each
retrieval hop. The LM is conditioned on a prompt that is
prepared using the hop_template template. (We discuss
prompt templates and thegenerate primitive in §2.5.) Here,
this template may be designed to generate a prompt that has
the following format (e.g., for the second hop).
1 My task is to write a simple query that gathers
information for answering a complex question . I
write N/A if the context contains all
information required .
2
3 { Task demonstrations from x. demos , if any }
4
5 Context : {x. context }
6 Question : {x. question }
7 Summary : Let /quotesingle.Vars summarize the above context .
__{ summary }__
8 Search Query : __{ query }__
As shown, the LM is instructed to read the context re-
trieved in earlier hops and a complex question. It is then
prompted to write: (i) a summary of the supplied con-
text and (ii) a search query that gathers information for
answering that question. The generated text will be ex-
tracted and assigned to the summary and query variables in
(multihop_search_v2; Line 7). On Line 10, we terminate
the hops if the query is “N/A”. Otherwise, Line 12 retrieves
k = 5 passages using the query and Line 13 assigns the
context for the subsequent hop (or for PREDICT ), setting
that to include the summary of all previous hops as well as
the passages retrieved in the ﬁnal hop so far.
Comparison with self-ask It may be instructive to con-
trast this multi-hop DSP program with the recent “self-
ask” (Press et al., 2022) prompting technique, which we
compare against in §3. Self-ask can be thought of as a sim-
ple instantiation of DSP’sSEARCH stage. In it, the LM asks
one or more “follow-up questions”, which are intercepted
and sent to a search engine. The search engine’s answers
are concatenated into the prompt and are used to answer
the question. This is essentially a simpliﬁed simulation of
IRRR (Qi et al., 2020).
As a general framework,DSP can express ideas like self-ask
and many other, more sophisticated pipelines as we discuss
in the present section. More importantly, DSP offers a num-
ber of intrinsic advantages that lead to large empirical gains:
80%–290% over self-ask. For instance, DSP programs are
deeply modular, which among other things means that DSP
programs will annotate and construct their own demonstra-
tions. Thus, they can be developed without labeling any
of the intermediate transformations (e.g., the queries gener-
ated). In addition, the LM prompts constructed by DSP get
automatically updated to align with the training data and re-
trieval corpus provided. In contrast, approaches like self-ask
rely on a hand-written prompt with hard-coded examples.
Moreover, DSP assigns the control ﬂow to an explicit pro-
gram and facilitates design patterns that invoke the LM (or
RM) to conduct small transformations. This allows us to
build steps that are dedicated to generating one or more re-
trieval queries, summarizing multiple passages per hop, and
answering questions. These steps are individually simpler
than the self-ask prompt, yet our multi-hop DSP program
deliberately composes them to build richer pipelines that are
thus more reliable. In contrast, self-ask delegates the con-
trol ﬂow to the LM completions, maintaining state within
the prompt itself and intercepting follow-up questions to
conduct search. We ﬁnd that this paradigm leads to a “self-
distraction” problem (§3.5) that DSP programs avoid.
Fusing Retrieval Results For improved recall and robust-
ness, we can also fuse the retrieval across multiple gen-
erated queries. Fusion has a long history in information



Source: data\tc16_2312.10997v5\referenced_papers\[23]_2212.14024.pdf (Page 6):

DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
retrieval (Fox & Shaw, 1994; Xue & Croft, 2013; Kur-
land & Culpepper, 2018) and sequentially processing multi-
ple queries was explored recently by Gao et al. (2022) for
retroactively attributing text generated by LMs to citations.
Inspired by these, we include afused_retrieval primitive
to DSP to offer a versatile mechanism for interacting with
frozen retrievers. It accepts an optional fusion function that
maps multiple retrieval lists into one. By default, DSP uses
a variant of CombSUM (Fox & Shaw, 1994), assigning each
passage the sum of its probabilities across retrieval lists.
To illustrate, the modiﬁcation below generates n = 10
queries for the transformation multihop_search_v2.
c = generate ( hop_template , n =10) (x)
passages = fused_retrieval (c. queries , k =5)
summary = c. summaries [0] # highest - scoring summary
Compositions & Extensions To illustrate a simple com-
position, we can equip a chatbot with the capacity for con-
versational multi-hop search by combining a query rewriting
step, which produces a query that encompasses all of the
relevant conversational context, with the multi-hop transfor-
mation, as follows.
1 def conversational_multihop_search (x):
2 x. question = generate ( conv_rewriting_template )(x)
3 return multihop_search_v2 (x)
Similar approaches can be used for correcting spelling mis-
takes or implementing pseudo-relevance feedback (Cao
et al., 2008; Wang et al., 2022a), in which retrieved passages
are used to inform a better search query, though this has not
been attempted with pretrained LMs to our knowledge.
2.5. PREDICT
The PREDICT stage generates the system output using
demonstrations (e.g., in x.demos) and passages (e.g., in
x.context). PREDICT tackles the challenges of reliably
solving the downstream task, which integrates much of the
work on in-context learning in general. Within DSP, it also
has the more specialized function of systematically aggre-
gating information across a large number of demonstrations,
passages, and candidate predictions.
Generating Candidates Generally, PREDICT has to pro-
duce one or more candidate predictions for the end-task.
To this end, the basic primitive in PREDICT is generate,
which accepts a Template and (via currying) an Example
and queries the LM to produce one or more completions,
as explored earlier in §2.4. A corresponding primitive that
uses the RM in this stage is rank, which accepts a query
and one or more passages and returns their relevance scores.
1 Template # template : an object that can produce
prompts and parse completions
2
3 generate ( template : Template )
4 -> fn( example : Example )
5 -> Completions # object with keys to access
extracted preds and scores
6
7 rank ( query : str , passages : List [ str ])
8 -> List [ float ] # object with keys to access
passage texts and scores
A Template is an object that can produce prompts, that is,
map an Example to a string, and extract ﬁelds out of com-
pletions. For instance, we can map an example x that has a
question and retrieved passages to the following prompt:
1 My task is to answer questions using Web documents .
2
3 { Task demonstrations from x. demos , if any }
4
5 Context : {x. passage }
6 Question : {x. question }
7 Rationale : Let /quotesingle.Vars think step by step . __{ rationale }__
8 Answer : __{ answer }__
As this illustrates, the LM will be asked to generate a chain-
of-thought rationale (CoT; Wei et al. 2022; Kojima et al.
2022) and an answer, and the generated text will be ex-
tracted back into the rationale and answer keys of each
completion.
Each invocation to the LM can sample multiple candidate
predictions. Selecting a “best” prediction is the subject of
much work on decoding (Wiher et al., 2022; Li et al., 2022),
but a frozen and general-purpose LM may not support cus-
tom modiﬁcations to decoding. Within these constraints, we
present several high-level strategies for selecting predictions
and aggregating information in DSP via the LM and RM.
Selecting Predictions Among multiple candidates, we
can simply extract the most popular prediction. When a CoT
is used to arrive at the answer, this is the self-consistency
method of Wang et al. (2022c), which seeks to identify
predictions at which multiple distinct rationales arrive.
1 from dsp import generate , majority
2
3 def multihop_predict (x):
4 candidates = generate ( template_qa )(x)
5 return x. copy ( answer = majority ( candidates ). answer )
DSP generalizes this in two ways. First, we can sample
multiple “pipelines of transformations” (PoT) within the pro-
gram, rather than locally with “chains of thought” (CoT) in
one transformation. These chains may even invoke different
paths in the program, as illustrated below.



Source: data\tc16_2312.10997v5\referenced_papers\[23]_2212.14024.pdf (Page 3):

DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
invoke and compose DSP primitives (i.e., built-in functions)
to build the DEMONSTRATE , SEARCH , and PREDICT trans-
formations that deﬁne the program.
Transformations A transformation is a function that
takes an Example as input and returns an Example, pop-
ulating new ﬁelds (or modifying existing ﬁelds) in it. This
program invokes three developer-deﬁned transformations,
namely, multihop_demonstrate, multihop_search, and
multihop_predict. Transformations may themselves in-
voke other transformations, and they act analogously to
layers in standard deep neural network (DNN) program-
ming frameworks such as PyTorch, except that they pass
text data instead of tensors between each other and do not
involve backpropagation.
We categorize transformations according to their behavior
(or purpose) under one of the DEMONSTRATE , SEARCH ,
and PREDICT stages. That said, DSP does not impose this
categorization and allows us to deﬁne functions that may
blend these stages. We will discuss each of the three stages
next.
2.3. DEMONSTRATE
It is known that including examples of the desired behavior
from the LM in its prompt typically leads to better perfor-
mance (Brown et al., 2020). In DSP , a demonstration is a
training example that has been prepared to illustrate speciﬁc
desired behaviors from the LM. A DEMONSTRATE transfor-
mation takes as input x of type Example and prepares a list
of demonstrations in x.demos, typically by selecting a sub-
set of the training examples in x.train and bootstrapping
new ﬁelds in them.
Bootstrapping Demonstrations Examples in the train-
ing set typically consist of the input text and the target
output of the task. The DEMONSTRATE stage can aug-
ment a training example by programmatically bootstrapping
annotations for intermediate transformations. In our run-
ning “multi-hop” example, the demonstrations illustrate
three LM-based transformations: (i) how to break down the
input question in order to gather information for answer-
ing it (i.e., ﬁrst-hop retrieval), (ii) how to use information
gathered in an earlier “hop” to ask follow-up questions, and
(iii) how to use the information gathered to answer complex
questions.
1 Examples = list [ Example ]
2 Transformation = Callable [[ Example ],
3 Optional [ Example ]]
4
5 annotate ( train : Examples , fn: Transformation )
6 -> Examples
Akin to a specialized map, the annotate primitive accepts
a user-deﬁned transformation fn and applies it over a list
of training examples. Whenever fn returns an example
(rather than None), annotate caches the intermediate pre-
dictions (i.e., the generated queries and retrieved passages).
These predictions serve as successful demonstrations for the
pipeline transformations. In simple uses, fn may attempt
to answer the example “zero-shot” one or more times. This
is typically done by invoking the SEARCH and PREDICT
stages of the program. When an answer is produced, if
fn assesses it as correct, it returns a populated example in
which the intermediate predictions are present.
Case Study The snippet below deﬁnes the func-
tion multihop_demonstrate, called in Line 3 of
multihop_program, and illustrates the usage of annotate.
1 from dsp import sample , annotate
2
3 def attempt_example (d: Example ):
4 d = d. copy ( demos =[])
5 d = multihop_search (d)
6 d = multihop_predict (d)
7 return d if d. pred == d. answer else None
8
9 def multihop_demonstrate (x: Example ):
10 demos = annotate (x. train , attempt_example )
11 return Example (x, demos = demos )
In Line 10, multihop_demonstrate invokes annotate,
which bootstraps missing ﬁelds in training examples by
caching annotations from attempt_example. The transfor-
mation attempt_example takes a training example d and
attempts to answer it in a zero-shot fashion: it creates a copy
of d with no demonstrations (Line 4; i.e., zero-shot) and
invokes the multi-hop search and predict pipeline (Lines 5
and 6). Each transformation returns an updated version of
d with additional ﬁelds populated. If the pipeline answers
correctly (Line 7), the updated d is returned.
Figure 2 illustrates this behavior. DEMONSTRATE trans-
forms a training question–answer pair to a fully-populated
demonstration, including ﬁelds such as hop1 and hop2 (i.e.,
queries for multi-hop search) as well as psg1 and psg2.
When the LM is later invoked to conduct a transformation,
say, generating a “second-hop” query duringSEARCH , the
psg1 ﬁeld serves as context and the hop2 ﬁeld serves as a
label for this particular training example.
Discussion This simple case study illustrates the power of
composition in the DSP abstraction. Because the pipeline
is a well-deﬁned program in which transformations com-
municate by passing text attached to Examples, a simple
map-and-ﬁlter strategy can leverage the LM and RM to
bootstrap annotations for a full pipeline from end-task la-
bels. This is an extensible strategy, but even in its simplest
form it generalizes the approaches explored recently by Ze-
likman et al. (2022), Wei et al. (2022), Zhang et al. (2022),
and Huang et al. (2022) in which an LM self-generates
chain-of-thought rationales for an individual prompt.



### Claim 15/179

#### Claim Text
The flexible orchestration of Modular RAG Flow showcases the benefits of adaptive retrieval through techniques such as FLARE [24] and Self-RAG [25].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[24]_2305.06983.pdf (Page 2):

sidering the impressive performance achieved by
GPT-3.5 (Ouyang et al., 2022) on a variety of
tasks, we examine the effectiveness of our meth-
ods on text-davinci-003. We evaluate FLARE
on 4 diverse tasks/datasets involving generating
long outputs, including multihop QA (2WikiMul-
tihopQA), commonsense reasoning (StrategyQA),
long-form QA (ASQA), and open-domain summa-
rization (WikiAsp) (Ho et al., 2020; Geva et al.,
2021; Stelmakh et al., 2022; Hayashi et al., 2021).
Over all tasks, FLARE achieves superior or com-
petitive performance compared to single-time and
multi-time retrieval baselines, demonstrating the
effectiveness and generalizability of our method.
2 Retrieval Augmented Generation
We formally define single-time retrieval augmented
generation and propose the framework of active
retrieval augmented generation.
2.1 Notations and Definitions
Given a user input x and a document corpus D =
{di}|D|
i=1 (such as all Wikipedia articles), the goal of
retrieval augmented LMs is to generate the answer
y = [s1, s2, ...,sm] = [w1, w2, ..., wn] containing
m sentences or n tokens leveraging information
retrieved from the corpus.
In retrieval augmented LM, the LM typically
pairs with a retriever that can retrieve a list of
documents Dq = ret(q) for a query q; the LM
conditions on both the user input x and retrieved
documents Dq to generate the answer. Since we
focus on examining various methods of determin-
ing when and what to retrieve, we follow exist-
ing methods (Ram et al., 2023; Trivedi et al.,
2022) to prepend the retrieved documents before
the user input to aid future generation for both
baselines and our method for fair comparisons:
y = LM([Dq, x]), where [·, ·] is concatenation fol-
lowing the specified order.
2.2 Single-time Retrieval Augmented
Generation
The most common choice is to directly use the user
input as the query for retrieval and generate the
complete answer at once y = LM([Dx, x]).
2.3 Active Retrieval Augmented Generation
To aid long-form generation with retrieval, we pro-
pose active retrieval augmented generation. It is a
generic framework that actively decides when and
what to retrieve through the generation process,
resulting in the interleaving of retrieval and genera-
tion. Formally, at step t(t ≥ 1), the retrieval query
qt is formulated based on both the user input x and
previously generated output y<t = [y0, ...,yt−1]:
qt = qry(x, y<t),
where qry(·) is the query formulation function. At
the beginning (t = 1), the previous generation is
empty (y<1 = ∅), and the user input is used as the
initial query (q1 = x). Given retrieved documents
Dqt, LMs continually generate the answer until the
next retrieval is triggered or reaches the end:
yt = LM([Dqt, x, y<t]),
where yt represents the generated tokens at the cur-
rent step t, and the input to LMs is the concatena-
tion of the retrieved documents Dqt, the user input
x, and the previous generation y<t. We discard
previously retrieved documents ∪t′<tDqt′ and only
use the retrieved documents from the current step
to condition the next generation to prevent reaching
the input length limit of LMs.
3 FLARE: Forward-Looking Active
REtrieval Augmented Generation
Our intuition is that (1) LMs should only retrieve
information when they do not have the necessary
knowledge to avoid unnecessary or inappropriate
retrieval, and (2) the retrieval queries should reflect
the intents of future generations. We propose two
forward-looking active retrieval augmented gener-
ation (FLARE) methods to implement the active
retrieval augmented generation framework. The
first method prompts the LM to generate retrieval
queries when necessary while generating the an-
swer using retrieval-encouraging instructions, de-
noted as FLAREinstruct. The second method directly
uses the LM’s generation as search queries, denoted
as FLAREdirect, which iteratively generates the next
sentence to gain insight into the future topic, and
if uncertain tokens are present, retrieves relevant
documents to regenerate the next sentence.
3.1 FLARE with Retrieval Instructions
Inspired by Toolformer (Schick et al., 2023), a
straightforward way of expressing information
needs for retrieval is to generate “[Search(query)]”
when additional information is needed (Schick
et al., 2023), e.g., “The colors on the flag of
Ghana have the following meanings. Red is for
[Search(Ghana flag red meaning)] the blood of mar-
tyrs, ...” When working with GPT-3.5 models that



Source: data\tc16_2312.10997v5\referenced_papers\[24]_2305.06983.pdf (Page 8):

β EM F 1 Prec. Rec.
0.0 0.488 0.576 0.571 0.605
0.2 0.498 0.588 0.582 0.616
0.4 0.510 0.597 0.591 0.627
0.6 0.506 0.593 0.586 0.622
Table 5: Performance of FLARE with respect to the
masking threshold β on 2WikiMultihopQA.
ASQA-hint WikiAsp
EM D-F 1 R-L DR UniEval E-F 1 R-L
Implicit 45.7 36.9 37.7 37.3 53.4 18.8 27.7
Explicit 46.2 36.7 37.7 37.2 53.4 18.9 27.6
Table 6: A comparison between implicit and explicit
query formulation methods in FLARE.
thresholds β. Retrieving directly with the complete
sentence (β = 0) is worse than masking tokens
with low probabilities, confirming our hypothesis
that low-confidence erroneous tokens can distract
retrievers. We compare implicit and explicit query
formulation methods in Table 6. Performances of
both methods are similar, indicating that both meth-
ods can effectively reflect information needs.
7 Related Work
We refer to subsection 2.2 and section 4 for ex-
tensively discussion on single-time and multi-time
retrieval augmented LMs, which is the most rele-
vant area to this paper.
Iterative and adaptive retrieval Iterative re-
trieval and refinement has been studied in both
text and code generation tasks (Peng et al., 2023;
Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu
et al., 2023). FLARE differs from these methods in
the granularity of generation and retrieval strategies.
Adaptive retrieval has been studied in single-time
retrieval scenarios based on either question pop-
ularity or generation probabilities (Mallen et al.,
2022; Li et al., 2023), while we focus on long-form
generation requiring active information access.
Browser-enhanced LMs WebGPT (Nakano
et al., 2021) and WebCPM (Qin et al., 2023) train
LMs to interact with browser to enhance factuality
using reinforcement learning or supervised train-
ing where multiple queries can be triggered before
generation. FLARE is built on text-based retrievers
but can be combined with a browser to potentially
improve retrieval quality.
8 Conclusion
To aid long-form generation with retrieval aug-
mentation, we propose an active retrieval aug-
mented generation framework that decides when
and what to retrieve during generation. We imple-
ment this framework with forward-looking active
retrieval that iteratively uses the upcoming sentence
to retrieve relevant information if it contains low-
confidence tokens and regenerates the next sen-
tence. Experimental results on 4 tasks/datasets
demonstrate the effectiveness of our methods. Fu-
ture directions include better strategies for active
retrieval and developing efficient LM architectures
for active information integration.
9 Limitations
We also conduct experiments on Wizard of
Wikipedia (Dinan et al., 2019) and ELI5 (Fan et al.,
2019), and found that FLARE did not provide sig-
nificant gains. Wizard of Wikipedia is a knowledge-
intensive dialogue generation dataset where the out-
put is relatively short (∼20 tokens on average) so
retrieving multiple disparate pieces of information
might not be necessary. ELI5 (Fan et al., 2019)
is a long-form QA dataset requiring in-depth an-
swers to open-ended questions. Due to issues men-
tioned in Krishna et al. (2021) such as difficulties
of grounding generation in retrieval and evalua-
tion, both single-time retrieval and FLARE did not
provide significant gains over not using retrieval.
From an engineering perspective, interleaving gen-
eration and retrieval with a naive implementation
increases both overheads and the cost of generation.
LMs need to be activated multiple times (once for
each retrieval) and a caching-free implementation
also requires recomputing the previous activation
each time after retrieval. This issue can be poten-
tially alleviated with special architectural designs
that encode the retrieved documents Dqt and the
input/generation (x/y<t) independently.
Acknowledgements
This work was supported in part by a grant from
the Singapore Defence Science and Technology
Agency and the IBM PhD Fellowship. We thank
Chunting Zhou, Amanda Bertsch, Uri Alon, Hi-
roaki Hayashi, Harsh Trivedi, Patrick Lewis, Timo
Schick, Kaixin Ma, Shuyan Zhou, and Songwei Ge
for their insightful discussions and help with the
experiments.



Source: data\tc16_2312.10997v5\referenced_papers\[24]_2305.06983.pdf (Page 14):

gon) including the following aspects: academics,
history.”, the output we aim to generate is “# Aca-
demics. In 2008, 91% of the school’s seniors re-
ceived their high school diploma... # History. The
class of 2008 was the 100th class in the school’s
history.” where # is used to indicate aspects. We
manually annotate 4 exemplars (Prompt D.10), and
use the Bing search engine to retrieve 5 documents
from the open web. To avoid leaking, we exclude
several Wikipedia-related domains listed in Table 8
from Bing’s search results.
C Hyperparameters
Hyperparameters of FLARE on different datasets
are listed in Table 9.
D Prompts and Few-shot exemplars
The prompt used to linearize multiple documents
is shown in Prompt D.1. The prompt used in self-
ask (Press et al., 2022) is shown in Prompt D.2.
Prompts and exemplars of different tasks/datasets
are shown in Prompt D.3, D.4, D.5, D.6, D.8, and
D.10, respectively.
Prompt D.1: document formatting
Search results:
[1] Document 1
[2] Document 2
...
The user input x
Prompt D.2: multihop QA with self-ask
Question: Who lived longer, Theodor Haecker or Harry
Vaughan Watkins?
Are follow up questions needed here: Yes.
Follow up: How old was Theodor Haecker when he died?
Intermediate answer: Theodor Haecker was 65 years old
when he died.
Follow up: How old was Harry Vaughan Watkins when he
died?
Intermediate answer: Harry Vaughan Watkins was 69 years
old when he died.
So the final answer is: Harry Vaughan Watkins.



Source: data\tc16_2312.10997v5\referenced_papers\[24]_2305.06983.pdf (Page 6):

0.0
20.0
40.0
60.0
80.0
2WikiMultihopQA StrategyQA ASQA ASQA-hint WikiAsp
No ret. Single-time ret. Previous-window ret. Forward-Looking Active REtrieval augmented generation (FLARE)
Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for
each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp.
following aspects: academics, history.” Experimen-
tal setting details are included in Appendix B.
Metrics include ROUGE, named entity-based F1,
and UniEval (Zhong et al., 2022) which measures
factual consistency.
6 Experimental Results
We first report overall results across 4 tasks/datasets
and compare the performance of FLARE with all
the baselines introduced in section 4. We then
run ablation experiments to study the efficacy of
various design choices of our method.
6.1 Comparison with Baselines
Overall results. The overall performance of
FLARE and baseline across all tasks/datasets are
reported in Figure 4. FLARE outperforms all base-
line on all tasks/datasets, indicating that FLARE
is a generic method that can effectively retrieve
additional information throughout the generation.
Among various tasks, multihop QA shows the
most significant improvement. This is largely due
to the task’s clear definition and specific objective
of producing the final answer through a 2-hop rea-
soning process, which makes it easier for LMs to
generate on-topic output. In contrast, ASQA and
WikiAsp are more open-ended, which increases the
difficulty of both generation and evaluation. The
improvement on ASQA-hint is larger than that of
ASQA because identifying ambiguous aspects is
challenging even for humans in many cases, and
providing a generic hint helps LMs to stay on topic.
Thorough comparisons with baselines. The per-
formance of all baselines on 2WikiMultihopQA
are reported in Table 1. FLARE outperforms all
baselines by a large margin, which confirms that
forward-looking active retrieval is highly effective.
Most multi-time retrieval augmented approaches
outperform single-time retrieval but with different
Methods EM F 1 Prec. Rec.
No retrieval 28.2 36.8 36.5 38.6
Single-time retrieval 39.4 48.8 48.6 51.5
Multi-time retrieval
Previous-window 43.2 52.3 51.7 54.5
Previous-sentence 39.0 49.2 48.9 51.8
Question decomposition 47.8 56.4 56.1 58.6
FLAREinstruct (ours) 42.4 49.8 49.1 52.5
FLAREdirect (ours) 51.0 59.7 59.1 62.6
Table 1: FLARE and baselines on 2WikiMultihopQA.
Previous-window (Borgeaud et al., 2022; Ram et al.,
2023), previous-sentence (Trivedi et al., 2022), and ques-
tion decomposition (Press et al., 2022; Yao et al., 2022)
methods are reimplemented for fair comparisons.
margins. The improvement of retrieving using the
previous sentence is relatively small which we hy-
pothesize is mainly because the previous sentence
often describes entities or relations different from
those in the next sentence in 2WikiMultihopQA.
While the previous-window approach might use
the first half of a sentence to retrieve information
potentially helpful for generating the second half.
Among all baselines, the question decomposition
approach (Press et al., 2022) achieves the best per-
formance. which is not surprising since the in-
context exemplars manually annotated with decom-
posed sub-questions (Prompt D.2) guide LMs to
generate sub-questions that align with the topic/in-
tent of future generations. FLARE outperforms
this baseline, indicating that manual exemplar an-
notation is not necessary for effective future-aware
retrieval. The gap between FLAREinstruct and ques-
tion decomposition is large, indicating that teaching
LMs to generate search queries using task-generic
retrieval instructions and exemplars is challenging.
We report all metrics for the other datasets in
Table 2. FLARE outperforms baselines with re-
spect to all metrics. Retrieval using the previ-



Source: data\tc16_2312.10997v5\referenced_papers\[24]_2305.06983.pdf (Page 13):

A FLARE Implementation Details
FLAREinstruct implementation details We
found that LMs can effectively combine retrieval
and downstream task-related skills and generate
meaningful search queries while performing the
task. However, there are two issues: (1) LMs tend
to generate fewer search queries than necessary.
(2) Generating excessive search queries can
disrupt answer generation and adversely affect
performance. We address these issues using two
methods respectively. First, we increase the logit
of the token “[” by 2.0 to improve the chances
of LMs generating “[Search(query)]”. Second,
whenever LMs generate a search query, we use it
to retrieve relevant information, promptly remove
it from the generation, and generate the next few
tokens while forbidding “[” by adding a large
negative value to the logit of “[”.
The initial query of FLARE. FLARE starts
with the user input x as the initial query to re-
trieve documents to generate the first sentence
ˆs1 = LM([Dx, x]) to bootstrap the iterative gener-
ation process. For the following steps, the tempo-
rary forward-looking sentence is generated without
retrieved documents.
Sentence tokenization. For each step t, we gen-
erate 64 tokens which are longer than most sen-
tences, and use NLTK sentence tokenizer 5 to ex-
tract the first sentence and discard the rest.
Efficiency As shown in subsection 6.2, on aver-
age retrieval is triggered for 30% ∼ 60% of sen-
tences depending on downstream tasks. In compar-
ision, KNN-LM (Khandelwal et al., 2020) retrieves
every token, RETRO or IC-RALM (Borgeaud et al.,
2022; Ram et al., 2023) retrievers every 4∼32 to-
kens, and IRCoT (Trivedi et al., 2022) retrieves
every sentence. Compared to single-time retrieval,
however, interleaving retrieval and generation with
a naive implementation indeed increases overheads,
which we discuss in the limitation section (sec-
tion 9).
B Datasets and Settings
Datasets, metrics, and experimental settings are
summarized in Table 7.
5https://www.nltk.org/api/nltk.tokenize.
PunktSentenceTokenizer.html
Multihop QA For “Why did the founder of Ver-
sus die?”, the output we aim to generate is “The
founder of Versus was Gianni Versace. Gianni Ver-
sace was shot and killed on the steps of his Miami
Beach mansion on July 15, 1997. So the answer
is shot.” We use 8 exemplars from Trivedi et al.
(2022) listed in Prompt D.4 for in-context learn-
ing, BM25 as the retriever, and Wikipedia articles
as the retrieval corpus. Similar to the observation
in Trivedi et al. (2022), we found incorporating
retrieval results for exemplars improves the per-
formance, we use the input x of each exemplar to
retrieve several documents and then add them using
the format in Prompt D.1. We found increasing the
number of retrieval documents often increases per-
formance. Therefore, we use the maximum number
of documents that can fit within the input length
limit of text-davinci-003, which is 2 for 2Wiki-
MultihopQA.
Commonsense Reasoning For “Would a pear
sink in water?”, the output we aim to generate is
“The density of a pear is about 0.6g/cm3, which is
less than water. Objects less dense than water float.
Thus, a pear would float. So the final answer is no.”
We use 6 exemplars from Wei et al. (2022) listed in
Prompt D.5, BM25 on the Wikipedia corpus, and 3
retrieved documents to run experiments.
Long-form QA For “Where do the Philadelphia
Eagles play their home games?”, the output we
aim to generate is “We need to consider the dif-
ferent possible locations or venues that could be
considered the home field of the Philadelphia Ea-
gles. These include the city, the sports complex,
or the stadium. Therefore, this question has 3 in-
terpretations and the answers are: (1) The city is
Philadelphia. (2) The sports complex is the South
Philadelphia Sports Complex. (3) The stadium is
the Lincoln Financial Field stadium.” For both the
original setting (ASQA) and the setting with hints
(ASQA-hint), we manually annotate 8 exemplars
(Prompt D.6 and D.8), use BM25 on the Wikipedia
corpus, and 3 retrieved documents to run experi-
ments.
Open-domain Summarization The original
WikiAsp dataset is designed for multi-document
summarization and provides a list of references to
systems. We converted it into the open-domain
setting by removing the associated references and
instead gathering information from the open web.
For “Generate a summary about Echo School (Ore-



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 15):

Preprint.
APPENDIX
A S ELF -RAG Details 17
A.1 Reflection Tokens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.2 S ELF -RAG Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.3 S ELF -RAG Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B Experimental Details 19
B.1 More Details of Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.2 More Details of Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C Results 20
C.1 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C.2 Human Evaluation Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.3 Qualitative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
D Full List of Instructions and Demonstrations for GPT-4 21
16



Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 0):

Preprint.
SELF -RAG : L EARNING TO RETRIEVE , GENERATE , AND
CRITIQUE THROUGH SELF -REFLECTION
Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§
†University of Washington §Allen Institute for AI ‡IBM Research AI
{akari,zeqiuwu,yizhongw,hannaneh}@cs.washington.edu, avi@us.ibm.com
ABSTRACT
Despite their remarkable capabilities, large language models (LLMs) often produce
responses containing factual inaccuracies due to their sole reliance on the paramet-
ric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad
hoc approach that augments LMs with retrieval of relevant knowledge, decreases
such issues. However, indiscriminately retrieving and incorporating a fixed number
of retrieved passages, regardless of whether retrieval is necessary, or passages are
relevant, diminishes LM versatility or can lead to unhelpful response generation.
We introduce a new framework called Self-Reflective Retrieval-Augmented Gen-
eration (SELF -RAG) that enhances an LM’s quality and factuality through retrieval
and self-reflection. Our framework trains a single arbitrary LM that adaptively
retrieves passages on-demand, and generates and reflects on retrieved passages
and its own generations using special tokens, called reflection tokens. Generating
reflection tokens makes the LM controllable during the inference phase, enabling it
to tailor its behavior to diverse task requirements. Experiments show that SELF -
RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs
and retrieval-augmented models on a diverse set of tasks. Specifically, SELF -RAG
outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,
reasoning and fact verification tasks, and it shows significant gains in improving
factuality and citation accuracy for long-form generations relative to these models.1
1 I NTRODUCTION
State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)
despite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation
(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs
with relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,
2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce
unnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they
retrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,
the output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since
the models are not explicitly trained to leverage and follow facts from provided passages. This
work introduces Self-Reflective Retrieval-augmented Generation ( SELF -RAG) to improve an
LLM’s generation quality, including its factual accuracy without hurting its versatility, via on-demand
retrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on
its own generation process given a task input by generating both task output and intermittent special
tokens (i.e., reflection tokens). Reflection tokens are categorized into retrieval and critique tokens to
indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,
given an input prompt and preceding generations, SELF -RAG first determines if augmenting the
continued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that
calls a retriever model on demand (Step 1). Subsequently,SELF -RAG concurrently processes multiple
retrieved passages, evaluating their relevance and thengenerating corresponding task outputs (Step
2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms
of factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which
1Our code and trained models are available at https://selfrag.github.io/.
1
arXiv:2310.11511v1  [cs.CL]  17 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 9):

Preprint.
0 50 100 150
Num of training (k)
35
40
45
50
55Perfomance
(a) PopQA
0 100
Num of training (k)
71
72
73
 (b) PubHealth
0 100
Num of training (k)
40
60
 (c) ASQA (prec)
Pop Bio.
S & P 92.5 70.0
ISREL 95.0 90.0
ISSUP 90.0 85.0
(d) Human evaluation on PopQA
and Bio generation.
Figure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect
of the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)
Human analysis on SELF -RAG outputs as well as reflection tokens.
contrary, a larger weight results in lower MAUVE scores: when generation gets longer and more
fluent, there are often more claims that are not fully supported by citations, consistent with findings
by Liu et al. (2023a). Our framework lets practitioners choose and customize models’ behaviors at
test time by adjusting such parameters without requiring additional training.
Efficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval
occurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects
overall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers
of threshold δ (larger δ results in less retrieval) on PubHealth and PopQA. Figure 3c shows that
the model’s retrieval frequencies dramatically change on both datasets. as δ varies. On one hand,
performance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.
Effects of training data size. We conduct an analysis of how the data scale affects the model’s
performance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original
150k training instances, and fine-tune four SELF -RAG 7B variants on those subsets. Then, we compare
the model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF -
RAG trained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models’
performance trained on different amount of data. Across all datasets, increasing data size often shows
upward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do
not observed such significant improvements on Llama2-FT7B when increasing the training data from
50k to 150k. These results also indicate that further expanding the training data of SELF -RAG may
lead to further improvements, although in this work we limit our training data size to 150k.
Human evaluations. We conduct small human evaluations on SELF -RAG outputs, as well as the
reliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio
results. Following Menick et al. (2022), human annotators evaluate S&P, which indicates whether
the model output is plausible (i.e., the output is a reasonable and on-topic response to the question
as if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to
verify the validity of the answer). For S&P, we do not consider the instances where SELF -RAG
predicts irrelevant or no support. We then ask our annotators whether the model-predicted
reflection tokens about ISREL and ISSUP match their inspections (e.g., whether the fully supported
output is supported by the cited evidence). Human annotators find SELF -RAG answers are often
plausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is
consistent with Menick et al. (2022). Human annotators also find ISREL and ISSUP reflection token
predictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated
examples and explanations on assessments.
6 C ONCLUSION
This work introduces SELF -RAG, a new framework to enhance the quality and factuality of LLMs
through retrieval on demand and self-reflection. SELF -RAG trains an LM to learn to retrieve, generate,
and critique text passages and its own generation by predicting the next tokens from its original
vocabulary as well as newly added special tokens, called reflection tokens.SELF -RAG further enables
the tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on
six tasks using multiple metrics demonstrate that SELF -RAG significantly outperforms LLMs with
more parameters or with conventional retrieval-augmented generation approaches.
10



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 5):

111:6 Lyu, et al.
2.2 RAG Benchmarks
When investigating the development and optimization of RAG, the effective evaluation of their
performance becomes a fundamental concern. Table 1 shows some commonly used benchmarks
for evaluating RAG. LangChain provides benchmark tasks, such as LangChain Docs Q&A and
Semi-structured Reports [27], designed to assess various RAG architectures. These datasets are
constructed from snapshots of Python documentation and PDFs containing tables and charts.
They emphasize the model’s capability to handle structured and semi-structured data. Evaluation
standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing
large models for question-answering generation has emerged as a prevalent approach in building
evaluation datasets. For instance, RGB [8] creates its evaluation dataset by gathering recent news
reports and employing LLM to generate relevant events, questions, and answers. Conversely,
ARES [44]. relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
These methods not only showcase the RAG system’s proficiency in handling real-time data but also
illustrate the utility of automation and synthetic data in the evaluation process. For evaluating the
capabilities of models across various professional domains, the Instruct-Benchmark-Tester dataset
encompasses a range of question types, with a particular focus on financial services, legal, and
intricate business scenarios [39].
Depending on whether the evaluation phase incorporates ground truth, metrics of existing
evaluation methods can be categorized into those necessitating reference and those not requiring
it. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by
contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-
Benchmark-Tester [39] employs accuracy score as an evaluation metric, a widely acknowledged
measure of model performance that assesses the extent to which model-generated answers align
with reference answers. The primary objective of RGB [8] is to evaluate whether large models can
effectively utilize external documents to acquire knowledge and generate accurate answers. Its
evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.
Reference-free evaluation methods, including TruLens-Eval [14], RAGAS [13], and ARES [44],
provide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning
context relevance, answer faithfulness, and answer relevance. TruLens-Eval [14] introduces the
RAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,
encompassing context relevance, groundedness, and answer relevance. RAGAS [13], serving as
a reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity
to identify pertinent and concentrated context passages, along with the LLMs’ proficiency in
faithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a
predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each
aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy
when compared to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-
powered inference to offer statistical assurances for its scoring, generating confidence intervals.
ARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer
relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts
and producing both faithful and relevant answers. Regarding evaluation methods, [32] places an
emphasis on assessing the credibility and accuracy of responses generated by generative search
engines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including
high costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,
exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely
approximates manual evaluation outcomes.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 1):

111:2 Lyu, et al.
Additional Key Words and Phrases: Retrieval-Augmented Generation, Large Language Models, Evaluation
ACM Reference Format:
Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong
Xu, and Enhong Chen. 2018. CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented
Generation of Large Language Models. J. ACM 37, 4, Article 111 (August 2018), 31 pages. https://doi.org/
XXXXXXX.XXXXXXX
1 INTRODUCTION
Retrieval-augmented generation (RAG) is an advanced technique that leverages external knowledge
sources to enhance the text generation capabilities of large language models(LLMs). It retrieves
relevant paragraphs from a corpus based on the input, and feeds them to the LLMs along with the
input. With the help of external knowledge, LLMs can generate more accurate and credible responses
and effectively address challenges such as outdated knowledge [19], hallucinations [3, 9, 35, 62],
and lack of domain expertise [30, 46]. Therefore, RAG technology is attracting increasing attention.
Although the effectiveness of retrieval-augmented strategies has been proven through extensive
practice, their implementation still requires a significant amount of tuning. The overall performance
of the RAG system is affected by multiple factors, such as the retrieval model, construction of the
external knowledge base, and language model. Therefore, automatic evaluation of RAG systems
is crucial. Currently, there are only a few existing benchmarks for evaluating RAG performance,
as creating high-quality datasets and experimenting with them entail significant costs. These
benchmarks can be classified into two types: reference-required and reference-free evaluation.
Reference-free evaluation frameworks, such as RAGAS [13] and ARES [44], use LLM-generated
data to evaluate RAG systems on contextual relevance, faithfulness, and informativeness. These
frameworks do not depend on ground truth references, but only assess the coherence of the
generated text with the retrieved context. This approach may be unreliable if the retrieved external
information is low-quality.
Consequently, reference-required evaluations remain the predominant method for assessing RAG
systems. Existing benchmarks for reference-required evaluations, such as RGB [8] and NQ [26].
do have their limitations. First, they all rely on question answering tasks to measure the perfor-
mance of RAG systems. Question answering is not the only RAG application scenario, and an
optimization strategy that works well for question answering may not be generalized to other
scenarios. Thus, these benchmarks may not capture the full potential of RAG systems. Second, in
the experiments, current evaluations usually focus on evaluating the LLM part of the RAG pipeline,
or focus on retriever performance in the knowledge-intensive scenario [40], while ignoring the
retrieval methods in non-knowledge-intensive scenarios and external knowledge base construction.
These components are also crucial for RAG systems. Therefore, a comprehensive evaluation of the
RAG system may not be obtained using any existing benchmarks.
To evaluate the performance of RAG in different application scenarios, we need a comprehensive
benchmark that covers more than just the question-answering task. Lewis et al. [28] argue that the
core of RAG systems is their interactive way of combining LLMs with external knowledge sources.
And following [25], we can group any interaction with external knowledge sources into four basic
actions: create, read, update, and delete, which are also known as CRUD actions [48]. Therefore,
we can use the CRUD framework to classify the RAG systems’ application scenarios. As shown in
Figure 1, each CRUD category demonstrates different capabilities of the RAG system:
•In "CREATE", the system improves the input text by adding relevant information from
external sources, making creative outputs such as poetry, stories, or code.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



### Claim 16/179

#### Claim Text
Another benefit of a flexible architecture is that the RAG system can more easily integrate with other technologies (such as fine-tuning or reinforcement learning) [26].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 5):

111:6 Lyu, et al.
2.2 RAG Benchmarks
When investigating the development and optimization of RAG, the effective evaluation of their
performance becomes a fundamental concern. Table 1 shows some commonly used benchmarks
for evaluating RAG. LangChain provides benchmark tasks, such as LangChain Docs Q&A and
Semi-structured Reports [27], designed to assess various RAG architectures. These datasets are
constructed from snapshots of Python documentation and PDFs containing tables and charts.
They emphasize the model’s capability to handle structured and semi-structured data. Evaluation
standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing
large models for question-answering generation has emerged as a prevalent approach in building
evaluation datasets. For instance, RGB [8] creates its evaluation dataset by gathering recent news
reports and employing LLM to generate relevant events, questions, and answers. Conversely,
ARES [44]. relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
These methods not only showcase the RAG system’s proficiency in handling real-time data but also
illustrate the utility of automation and synthetic data in the evaluation process. For evaluating the
capabilities of models across various professional domains, the Instruct-Benchmark-Tester dataset
encompasses a range of question types, with a particular focus on financial services, legal, and
intricate business scenarios [39].
Depending on whether the evaluation phase incorporates ground truth, metrics of existing
evaluation methods can be categorized into those necessitating reference and those not requiring
it. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by
contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-
Benchmark-Tester [39] employs accuracy score as an evaluation metric, a widely acknowledged
measure of model performance that assesses the extent to which model-generated answers align
with reference answers. The primary objective of RGB [8] is to evaluate whether large models can
effectively utilize external documents to acquire knowledge and generate accurate answers. Its
evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.
Reference-free evaluation methods, including TruLens-Eval [14], RAGAS [13], and ARES [44],
provide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning
context relevance, answer faithfulness, and answer relevance. TruLens-Eval [14] introduces the
RAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,
encompassing context relevance, groundedness, and answer relevance. RAGAS [13], serving as
a reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity
to identify pertinent and concentrated context passages, along with the LLMs’ proficiency in
faithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a
predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each
aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy
when compared to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-
powered inference to offer statistical assurances for its scoring, generating confidence intervals.
ARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer
relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts
and producing both faithful and relevant answers. Regarding evaluation methods, [32] places an
emphasis on assessing the credibility and accuracy of responses generated by generative search
engines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including
high costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,
exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely
approximates manual evaluation outcomes.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[26]_2401.06954.pdf (Page 2):

Figure 2: Differing from previous RAGs that update
LLMs, retrievers, or both, BGM connects a frozen LLM
and a frozen retriever through a lightweight bridge
model which adapts the retrieved information to the
LLM’s preference. This makes BGM applicable to
“large” LMs and off-the-shelf retrievers.
from various knowledge sources is proven effective
across numerous NLP tasks, including language
modeling (Borgeaud et al., 2022; Khandelwal et al.,
2020; Shi et al., 2023b), question answering (Lewis
et al., 2020; Izacard et al., 2022; de Jong et al.,
2023; De Jong et al., 2023; Shi et al., 2023b; Guu
et al., 2020; Izacard and Grave, 2020; Xu et al.,
2023), fact versification (Lewis et al., 2020) and
text generation (Lewis et al., 2020). Specifically,
RAG utilizes input as a query and comprises two
main components: (1) a retriever retrieves a set
of items from a side corpus. Particular items may
vary across different tasks, including documents,
passages, or even tokens. In this study, we focus on
retrieving passages; and (2) a LLM incorporates
the retrieved items, as additional information in the
input context, and makes final predictions.
A fundamental question in this process arises
regarding the disparate preferences between LLMs
and retrievers, as LLMs performing optimally only
when their preferences are satisfied. Bridging the
preference gap is crucial. Depending on which
components are subject to updates, this challenge
can be categorized into three families.
Finetuning retrievers and LLMs jointly. This
is the most widely used setting of RAG (Izacard
et al., 2022; Khandelwal et al., 2020; Wu et al.,
2022; Guu et al., 2020; Lewis et al., 2020). How-
ever, most prior work is based on relative small
LMs (< 1B). For example, Altas (Izacard et al.,
2022) finetunes LLM (T5 (Raffel et al., 2020a))
and retriever (Contriever (Izacard et al., 2021))
jointly by leveraging the LLM to provide super-
visory signal to train the retriever. RAG (Lewis
et al., 2020) uses a tunnable query encoder and
DPR (Karpukhin et al., 2020) as retriever, BART
as LLM, and design an end-to-end schema to train
the query encoder and the LLM.
Finetuning LLMs only. Updating retrievers is
not always desirable as it is costly and requires
the document index to be periodically updated. To
bridge the preference gap, it is also possible to only
update the LLMs. FiD (Izacard and Grave, 2020)
takes the retrieved documents and query as input,
finetunes the LLM to adapt to the external infor-
mation. Similarly, Lummen (De Jong et al., 2023)
and Glimmer (de Jong et al., 2023) improve FiD
via adding reranker and pre-encoding memeory.
Finetuning retrievers only. Although above
systems have shown improves results, they are not
always applicable in practice. Many LLMs (espe-
cially larger ones) are only available as black-box
APIs. Given this constraint, a natural approach
is to only update the retrievers to ensure they can
retrieve passages that are more compatible with
LLMs. REPLUG (Shi et al., 2023b) adapts a sim-
ilar idea as Atlas but fix the LM. RECOMP (Xu
et al., 2023) trains a compressors to summarize
the retrieved document from retriever. However,
this family of models is incapable of performing
any sample-level selection and can only choose top
passages by setting a fixed threshold.
Unlike the existing approaches, BGM does not
fine-tune the LLM or the retriever and instead
employs a bridge model in between (Fig. 2).
RL for information retrieval. Before the LLM
era, RL has been used in information retrieval (IR)
(Xia et al., 2017; Wei et al., 2017; Zeng et al., 2018;
Xu et al., 2020). The core approach was to frame
the IR problem as a Markov Decision Process and
apply an RL algorithm to solve it. Typically, an
IR task would be structured to determine which
document to select for a ranking position, using
ranking metrics such as DCG as the reward. None
of these existing studies explore the application of
RL in the context of RAG.
In the LLM era, RL has been used in query
rewriting for retrieval (Wu et al., 2021; Nogueira
and Cho, 2017; Adolphs et al., 2021), where a
black-box retriever is assumed. This is a different
problem from RAG. Bacciu et al. (2023) suggest us-
ing RL to fine-tune retriever in the RAG context in
their opinion paper, not supported by experiments.
Their work does not recognize the importance of
bridging the gap between retrievers and LLMs, nor



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 1):

RAG system finds relevant contexts and generates
answers that are both faithful and relevant.
Many existing RAG evaluation frameworks re-
quire substantial human annotations for scoring.
ARES significantly improves data efficiency dur-
ing evaluation by only requiring three inputs: an in-
domain passage set, a human preference validation
set of approximately 150 annotated datapoints or
more, and few-shot examples of in-domain queries
and answers (e.g. five examples or more), which
are used for prompting LLMs in synthetic data gen-
eration.
Given the corpus of in-domain passages, ARES
proceeds in three stages. First, it leverages an LM
to construct a synthetic dataset of question–answer
pairs, derived from the passages in the corpus. Sec-
ond, it defines three separate judge models to per-
form three classification tasks (context relevance,
answer faithfulness, and answer relevance). These
judges are lightweight models fine-tuned against a
contrastive learning objective. Third, ARES scores
the different RAG systems being assessed using
prediction-powered inference (PPI; Angelopoulos
et al. 2023) to improve model-based evaluation ac-
curacy and provide statistical confidence intervals
for RAG scoring. PPI utilizes a small set of human
annotated datapoints for computing its confidence
intervals; we designate this annotated set as our hu-
man preference validation set, which is composed
of approximately 150 annotated datapoints or more
that designate both positive and negative examples
for context relevance, answer faithfulness, and an-
swer relevance.
We conduct extensive empirical evaluations,
demonstrating that ARES accurately scores
RAG systems across the six knowledge-intensive
datasets in KILT and SuperGLUE, beating exist-
ing automated evaluation approaches like RAGAS
by 59.3 and 14.4 percentage points on average
across context relevance and answer relevance eval-
uation accuracy, respectively. Additionally, ARES
accurately calculates answer hallucination occur-
rences in the AIS attribution dataset (Rashkin et al.,
2022), predicting within 2.5 percentage points of
the ground truth average for answer hallucinations.
Compared to annotation-based evaluation methods,
ARES is substantially more accurate and efficient,
requiring 78% less annotations than the baseline
approach. We also find that ARES consistently
distinguishes competitive RAG systems that are
only a few points apart in ground-truth metrics.
This precision enables ARES to guide the develop-
ment and comparison of competitive approaches
and configurations.
We make the ARES code and datasets publicly
available on Github.
2 Related Work
RAG (Guu et al., 2020; Lewis et al., 2020; Khat-
tab et al., 2021; Izacard et al., 2022)) is now a
common strategy for bolstering LLMs by combin-
ing them with retrieval systems. Through retrieval,
RAG helps LM systems gather domain-specific
knowledge, ground generations in factual informa-
tion (Shuster et al., 2021; Huo et al., 2023), and
offer a degree of transparency or interpretability
via citing sources (Mialon et al., 2023).
Multiple LLM-based evaluation techniques have
emerged for gauging LLM systems. This is essen-
tial for rapid deployment in new settings, where it
is difficult to build a traditional benchmark dataset
from scratch. Early attempts at this use LLMs
out of the box, as in MT-Bench and Chatbot
Arena (Zheng et al., 2023). AutoCalibrate (Liu
et al., 2023b) seeks to align an LLM-judge with
human preferences, leveraging a self-refinement
prompt to iteratively improve the LLM judge. How-
ever, AutoCalibrate does not offer any statistical
guarantees for the accuracy of its predictions. Other
work has used LLM prompting to evaluate system
quality across natural language generation tasks,
such as translation, summarization, and dialogue
(Kocmi and Federmann, 2023; Fu et al., 2023; Liu
et al., 2023a; Wang et al., 2023).
In the context of knowledge-intensive NLP tasks,
LLMs have been explored for assessing attribution
and factuality in LLMs (Min et al., 2023; Gekhman
et al., 2023; Yue et al., 2023). New guidelines
like LongEval (Krishna et al., 2023) and datasets
like Hagrid and ALCE (Kamalloo et al., 2023;
Gao et al., 2023) provide resources for analyzing
knowledge-intensive LLM pipelines.
The two most-closely related projects to ARES
are EXAM (Sander and Dietz, 2021) and RA-
GAS (James and Es, 2023). To evaluate RAG sys-
tems, the EXAM metric estimates how many exam
questions a reader (simulated as a QA system) can
answer correctly based on the generated response.
This requires a set of queries with several asso-
ciated sub-questions each, which adds a burden
that ARES does not bring. RAGAS is based on a
handful of heuristic hand-written prompts. These
offer little adaptability to new RAG evaluation set-



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 0):

ARES: An Automated Evaluation Framework for Retrieval-Augmented
Generation Systems
Jon Saad-Falcon
Stanford University ∗
jonsaadfalcon@stanford.edu
Omar Khattab
Stanford University
okhattab@stanford.edu
Christopher Potts
Stanford University
cgpotts@stanford.edu
Matei Zaharia
Databricks and UC Berkeley
matei@databricks.com
Abstract
Evaluating retrieval-augmented generation
(RAG) systems traditionally relies on hand
annotations for input queries, passages to re-
trieve, and responses to generate. We intro-
duce ARES, an Automated RAG Evaluation
System, for evaluating RAG systems along
the dimensions of context relevance, answer
faithfulness, and answer relevance. By cre-
ating its own synthetic training data, ARES
finetunes lightweight LM judges to assess the
quality of individual RAG components. To
mitigate potential prediction errors, ARES uti-
lizes a small set of human-annotated datapoints
for prediction-powered inference (PPI). Across
eight different knowledge-intensive tasks in
KILT, SuperGLUE, and AIS, ARES accurately
evaluates RAG systems while using only a few
hundred human annotations during evaluation.
Furthermore, ARES judges remain effective
across domain shifts, proving accurate even
after changing the type of queries and/or docu-
ments used in the evaluated RAG systems. We
make our code and datasets publicly available
on Github.
1 Introduction
Retrieval-augmented generation (RAG) has be-
come a prominent approach for building user-
facing NLP applications, such as systems for ques-
tion answering (QA), fact-checking, and customer
support (Petroni et al., 2021; Wang et al., 2019).
Typically, a RAG system consists of a retriever and
a downstream language model (LM). Given a user
question, the retriever finds relevant passages from
a corpus and the LM uses these passages to gener-
ate a response. This formulation admits a multitude
of choices: what retrieval model to use, how to di-
vide the documents into retrieval chunks, and how
to prompt or finetune the LM to use the retrieved
information, to name only a few of the simplest
design decisions.
∗Project started during research internship at Databricks
The best design for a RAG system is not neces-
sarily universal across data domains, corpus sizes,
and cost/latency budgets. To tune their own RAG
systems, practitioners traditionally need hand an-
notations for test questions, passages to retrieve
(to assess the retriever), and responses to generate,
labeled specifically for their target domain. Alter-
natively, they may evaluate different approaches in
production by collecting human preferences that
compare the candidate systems. Unfortunately,
both of these strategies demand high expertise and
impose considerable annotation costs.
Model-based evaluation is an inexpensive strat-
egy to test generative output quality (Zheng et al.,
2023). For instance, the open-source RAGAS
framework (James and Es, 2023) prompts an LM
for evaluating the relevance of retrieved informa-
tion and the faithfulness and accuracy of generated
responses. Unfortunately, such strategies currently
rely for evaluation on a fixed set of heuristically
hand-written prompts, offering little adaptability
to various evaluation contexts and no guarantees
about quality.
To evaluate RAG systems rapidly and accu-
rately, we propose ARES, the Automated RAG
Evaluation System. ARES is the first automated
RAG evaluation system to generate tailored LLM
judges for each component of a RAG pipeline, lead-
ing to substantial boosts in evaluation precision and
accuracy compared to existing approaches like RA-
GAS. Furthermore, unlike existing RAG evaluation
systems, ARES provides confidence intervals for
its scoring by leveraging prediction-powered in-
ference (PPI; Angelopoulos et al. 2023). Given a
corpus of documents and a RAG system, ARES
reports three evaluation scores: context relevance
(is the retrieved information pertinent to the test
question), answer faithfulness (is the response gen-
erated by the language model properly grounded
in the retrieved context), and answer relevance (is
the response also relevant to the question). A good
arXiv:2311.09476v2  [cs.CL]  31 Mar 2024



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 10):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:11
Researchers at West Virginia University in the United States 
have observed synthetic DNA at the atomic level and learned 
how to alter its structure to enhance its scissor-like functions.
Researchers at West Virginia University have successfully observed
 the structure of synthetic DNA at the atomic level and discovered
 that altering its structure can enhance its scissor-like functions. 
Their research findings were published in the Nature sub-journal, 
Communications Chemistry. The study found that these DNA 
molecules can fold into complex shapes capable of performing ...
Science and Technology Daily, Beijing, July 27 (Reporter Zhang 
Mengran) - Researchers at West Virginia University have achieved 
the observation of synthetic DNA at the atomic level, gaining 
insights into how to alter its structure to enhance its scissor-like 
functions. Understanding these synthetic DNA reactions in greater 
detail could be key to unlocking new medical technologies in the 
future. The research findings were published in the recent issue of 
the Nature sub-journal, Communications Chemistry. Atomic-level ...
The synthetic DNA used in this study, also known as DNAzymes, 
differs from human DNA in that it is created in a laboratory, has low 
production costs, and can catalyze chemical reactions. Researchers 
noted that DNA is typically regarded as inert when it comes to 
storing human genetic information; however, some types of DNA 
evolved in the laboratory defy traditional rules. These DNA ...
Fig. 4. The dataset construction pipeline for text continuation and multi-document summarization task.
our news corpus, removing the 10,000 articles 𝑑 simultaneously. The new news corpus 𝐷−𝑑+𝐸
serves as our retrieval corpus, and we expect the model to use the events and relevant information
from the retrieval corpus to generate a summary of the articles 𝑑.
3.3 Text continuation: RAG Application in "Create"
RAG is useful not only for "Delete", where it retrieves and summarizes key information from massive
texts, but also for "Create". In this scenario, RAG systems show strong creativity by expanding
existing texts, and we take the text continuation task as an evaluation. The text continuation task
aims to automatically produce coherent and relevant subsequent content based on the beginning
of the text, making the text more complete and vivid.
To construct the continuation task dataset, we follow the same method as the summary task
dataset. Figure 4 shows the construction process of text continuation. Specifically, we select a news
article from a high-quality corpus and use a specialized Chinese word segmentation tool, to split it
into sentences. Then, we divide the article into two equal parts: the first half serves as the input,
and the second half as the output of the continuation dataset. We expect the model to use RAG
technology to retrieve relevant information from the document library and generate a continuation
that is coherent, informative, and consistent with the input and output.
To ensure that the retrieval database covers the real continuation text, we use the Baidu search
engine to find external documents and add them to the database. The continuation text differs from
the event text in that it consists of multiple sentences. Therefore, we split the continuation text
into paragraphs by sentences and retrieve relevant documents for each paragraph using the search
engine. This way, we guarantee that the retrieval database contains most of the information to
reconstruct the continuation text.
3.4 Question Answering: RAG Application in "Read"
Another application scenario of RAG is to use external knowledge bases to enhance the question-
answering capabilities of LLMs, which can be applied to various knowledge-intensive tasks. Cur-
rently, there are many evaluation benchmarks to measure the performance of RAG in this scenario,
and multiple question answering datasets have been created.
However, the existing question answering datasets also have some limitations. On the one hand,
some datasets (such as NQ and WEBQA) are outdated, and may have been covered by LLMs in
the pre-training stage, which reduces the advantage of RAG systems. On the other hand, some
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



### Claim 17/179

#### Claim Text
For example, this can involve fine-tuning the retriever for better retrieval results, fine-tuning the generator for more personalized outputs, or engaging in collaborative fine-tuning [27].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[9]_2311.03758.pdf (Page 3):

WWW ’24 Companion, May 13–17, 2024, Singapore, Singapore. Peng, et al.
Mix௦௙௧OtherTask 3௠௦௙௧Rejection SamplingQQ𝑥 QQ𝑦OtherTask 2OtherTask 1Multi-instruction Supervised Fine TuningLLMQueriesRewritesInputNext Token PredictionMulti-instruction SFT Data 𝑥  𝑦ଵ 𝑦ଶ 𝑦௞Beam SearchQueriesInputTop kSFT Model
Offline SystemInputInput as Reference 𝒚𝟏≻ 𝒚𝟐… ≻ 𝒚𝒌…Partial OrderOffline FeedbackObjective Alignment 𝑦ଵ 𝑦ଶ 𝑦௞𝑥𝑥𝑥…InputNext Token Prediction 𝜋(𝑦ଵ|𝑥) 𝜋(𝑦ଶ|𝑥) 𝜋(𝑦௞|𝑥)QueriesGeneration Probability≻≻+ Alignment LossProbability CalibrationPartial OrderOptimization ObjectiveSFT Model…
Figure 2: Framework of BEQUE.
D𝑠𝑓𝑡 = {(𝑐𝑜𝑛𝑐𝑎𝑡(𝑥𝑖,E𝑥),𝑦𝑖)|𝑁𝑠𝑓 𝑡
𝑖=1
such that 𝑥𝑖,𝑦𝑖 ∈D𝑟,𝑖𝑛𝑐𝑟 (𝑥𝑖,𝑦𝑖)> 𝜏𝑖𝑛𝑐𝑟},
(3)
where, E𝑥 is the interacted product title list of query 𝑥, 𝑖𝑛𝑐𝑟(·)
and 𝜏𝑖𝑛𝑐𝑟 denote the increment method and its threshold of query-
rewrite pair, respectively. The detail of the function𝑖𝑛𝑐𝑟(·)is dis-
cussed in the Section 3.3.
Auxiliary Task Datasets: In order to further enhance LLMs’ ability
of comprehending long-tail queries, we have gathered three high-
related task datasets in the context of query rewriting. These tasks
encompass quality classification, product title prediction, and Chain-
of-thought. 1) To tackle the quality classification task, our approach
began with the extraction of query pairs from online logs. These
query pairs were then subjected to human annotation to determine
if they met the data requirements specified for SFT. 2) For the
product title prediction task, we chose the most recent interacted
product under the query as the reference, forming <query, product
title> pairs. 3) As for the CoT task, we employed the original online
queries to construct prompts for human evaluators. It’s noteworthy
that these evaluators were not only tasked with providing query
rewrites aimed at improving the quality of query retrieval but were
also expected to articulate their thought processes, explaining the
rationale behind their specific revisions. The details prompt design
for above auxiliary tasks are shown in Table 1. The collected data
were later integrated into the rewriting task to form the final dataset,
which was then randomly shuffled for the subsequent SFT stage.
Supervised Fine Tuning : The process of generating text with
a condition language model can be viewed as a constrained auto-
regressive sampling strategy. Given a prompt𝑥and its gold standard
𝑦, The training objective is to maximize the conditional probability
𝑝(𝑦|𝑥). Considering our multi-instruction SFT dataset and assum-
ing that 𝑝(𝑦|𝑥) = Î
𝑖=1 𝑝(𝑦𝑖|𝑦0:𝑖−1,𝑥), the training objective of
rewriting model involves minimizing the negative log likelihood:
LSFT (𝜃)= −E(𝑥,𝑦)∼D𝑚𝑠𝑓 𝑡
∑︁
𝑖=1
log 𝜋(𝑦𝑖 |𝑦0:𝑖−1,𝑥; 𝜃), (4)
where D𝑚𝑠𝑓𝑡 denotes multi-instruction SFT data, which consists of
a mixture of query rewriting datasetD𝑠𝑓𝑡 and a variety of auxiliary
task datasets, 𝜋(·)and 𝜃 denote our query rewriting model and its
parameters. It is worth mentioning that LLMs typically have fixed
prefixes in the prompt𝑥. To avoid introducing noise, we disregarded
the losses coressponding to 𝑥.
3.3 Offline Feedback
Currently, most alignment methods [1, 12, 23, 27] rely on manual
annotation and training-based reward models. However, we argue
that these approaches can be easily influenced by the quality of
annotations and the effectiveness of the reward model training,
which often leads to inaccurate reflection of response scores and
compromises the learning of the generation model. To address this
issue, we propose a feedback system based on the Taobao search
engine, which provides more accurate rewrite scores.



Source: data\tc16_2312.10997v5\referenced_papers\[27]_2310.01352.pdf (Page 3):

Published as a conference paper at ICLR 2024
Table 1: Our intruction tuning datasets. All datasets are downloaded from Hugging Face (Lhoest
et al., 2021), with the exception of those marked with ‡, which are taken from Iyer et al. (2022).
Task HF identifier Dataset name DL DR #Train
Dialogue oasst1 OpenAssistant Conversations Dataset (K ¨opf et al., 2023)✓ ✓ 31,598
Open-Domain
QA
commonsenseqa CommonsenseQA (Talmor et al., 2019) ✓ ✓ 9,741
mathqa MathQA (Amini et al., 2019) ✓ ✓ 29,837
webquestions Web Questions (Berant et al., 2013) ✓ ✓ 3,778
wikiqa Wiki Question Answering (Yang et al., 2015) ✓ ✓ 20,360
yahooanswersqa Yahoo! Answers QA ✓ ✓ 87,362
freebaseqa FreebaseQA (Jiang et al., 2019) ✓ 20,358
msmarco* MS MARCO (Nguyen et al., 2016) ✓ 80,143
Reading Com-
prehension
coqa Conversational Question Answering (Reddy et al., 2019) ✓ 108,647
drop Discrete Reasoning Over Paragraphs (Dua et al., 2019) ✓ 77,400
narrativeqa NarrativeQA (Ko ˇcisk´y et al., 2018) ✓ 32,747
newsqa NewsQA (Trischler et al., 2017) ✓ 74,160
pubmedqa PubMedQA (Jin et al., 2019) ✓ ✓ 1,000
quail QA for Artificial Intelligence (Rogers et al., 2020) ✓ 10,246
quarel QuaRel (Tafjord et al., 2019) ✓ ✓ 1,941
squadv2 SQuAD v2 (Rajpurkar et al., 2018) ✓ 130,319
Summarization cnndailymail CNN / DailyMail (Hermann et al., 2015) ✓ 287,113
Chain-of-
thought
Reasoning
aquarat‡ Algebra QA with Rationales (Ling et al., 2017) ✓ 97,467
ecqa‡ Explanations for CommonsenseQ (Aggarwal et al., 2021)✓ 7,598
gsm8k‡ Grade School Math 8K (Cobbe et al., 2021) ✓ 7,473
compeitionmath‡ MATH (Hendrycks et al., 2021b) ✓ 7,500
strategyqa‡ StrategyQA (Geva et al., 2021) ✓ 2,290
* We only used the question-and-answer pairs in the MS MARCO dataset.
DL, we retrieve the top- ˜k relevant text chunks Ci ⊂ Cbased on xi. Mirroring the inference-time
handling, for each retrieved chunk cij ∈ Ci, we create a separate fine-tuning example by prepending
it to the instructions as a background field, resulting in ˜k independent fine-tuning instances per
original example: {(cij ◦ xi, yi)|j = 1. . .˜k}.4
We fine-tune the language model using the next-token prediction objective and minimize the loss
from tokens in the output segment of each instance (Iyer et al., 2022):
L(DL) =−
X
i
X
j
log pLM (yi|cij ◦ xi). (3)
Integrating in-context retrieval augmentation during fine-tuning gives a twofold benefit. First, it
adapts the LLM to better utilize relevant background knowledge to make a prediction. Secondly,
even state-of-the-art retrievers can falter and return inaccurate results. By training the LLM to make
correct predictions when a wrong retrieved chunk is given, we enable the LLM to ignore misleading
retrieval content and lean into its parametric knowledge in such cases. The efficacy of this fine-
tuning strategy is empirically demonstrated in §5.1.
2.4 R ETRIEVER FINE -TUNING
In addition to fine-tuning the language model with retrieval augmentation, we also fine-tune the
retriever to better align its output with the language model. In particular, we adopt a generalized
version of LSR ( LM-Supervised Retrieval, Shi et al., 2023b) training that leverages the language
model itself to provide supervision for retriever fine-tuning.
For a training sample (x, y) in the retriever fine-tuning dataset DR, we define the LSR score for a
retrieved chunk c as follows:
pLSR(c|x, y) = exp (pLM (y|c ◦ x)/τ)P
c′∈C exp (pLM (y|c′ ◦ x)/τ) ≈ exp (pLM (y|c ◦ x)/τ)P
c′∈C′ exp (pLM (y|c′ ◦ x)/τ), (4)
where τ is a temperature hyperparameter, and C′ ⊂ Cdenotes the top-k retrieved chunks for x. A
higher LSR score indicates that c is more effective at improving the language model’s chance of
4The exceptions are summarization tasks and RC tasks with context dependent questions (e.g. “when was
the writer born?”), where we do not perform retrieval and create the fine-tuning instances using the given
background text instead. For RC tasks with self-contained questions, we use the retrieved chunks in addition to
the given background text to create fine-tuning instances, resulting in ˜k + 1of them per original example.
4



Source: data\tc16_2312.10997v5\referenced_papers\[28]_2312.05934.pdf (Page 2):

Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs
Figure 1.A visualization of the knowledge injection framework.
complex multi-step reasoning tasks (Tan et al., 2023) or
when posed with different questions about the same fact,
resulting in disparate outcomes (Berglund et al., 2023).
We observe that most of these issues arise during the pre-
training phase, with catastrophic forgetting being the notable
exception. Hence, many LLMs will suffer from factual
errors of this kind regardless of any post-training process.
3. Injecting Knowledge to Language Models
Following the background given in Section 2, it is clear
that general pre-training is insufficient for many knowledge-
intensive tasks. To solve this, an additional post-processing
step is essential to augment the knowledge of a pre-trained
model. This step is often reffered to as knowledge injec-
tion (Wang et al., 2020; Chen et al., 2022; Liu et al., 2020;
Lauscher et al., 2020).
In this section, we examine two widely used frameworks
for knowledge injection: fine-tuning (FT) and retrieval aug-
mented generation (RAG). We begin by formulating the
knowledge injection problem, aiming to explain both meth-
ods using consistent terminology.
3.1. Problem formulation
In Equations (1) and (2), we presented a formulation for
knowledge in language models through the lens of question-
answering (Q&A). We now extend this formulation to the
problem of knowledge injection using the same terminology.
Given a set of factual questions, there exists some text cor-
pus containing information that is relevant to these questions.
The central assumption of knowledge injection is that given
full access to this corpus, it could serve as an auxiliary
knowledge base and improve the model’s performance on
this set of questions.
Mathematically, let M be a pre-trained model, and let Q be
a set of factual questions, as before. Now, assume we have
a relevant auxiliary knowledge base BQ. Our objective is to
discover a transformation, denoted as F, that, when applied,
would enhance the knowledge about Q:
M′ := F(M, BQ) s.t. LM′,Q > LM,Q. (3)
In this work, we aim to compare two choices for F: fine-
tuning and RAG to see which option performs better in this
problem.
3.2. Fine-Tuning
Fine-tuning is the process of adjusting a pre-trained model
on a specific, often narrower, dataset or task to enhance
its performance in that particular domain. Here, it is vital
to distinguish between different types of fine-tuning. FT
techniques are commonly classified into supervised, unsu-
3



Source: data\tc16_2312.10997v5\referenced_papers\[27]_2310.01352.pdf (Page 2):

Published as a conference paper at ICLR 2024
2 M ETHOD
2.1 A RCHITECTURE
Language Model We focus on retrieval-augmenting pre-trained auto-regressive language mod-
els (Brown et al., 2020). In particular, we use L LAMA (Touvron et al., 2023a), a family of open-
sourced language models pre-trained on trillions of tokens.
Retriever We adopt a dual-encoder based retriever architecture, since it can be easily fine-tuned
and is efficient at the inference stage (Lewis et al., 2020; Izacard et al., 2022b; Shi et al., 2023b).
Given a corpus C and a query q, the document encoder maps each text chunk c ∈ Cto an embedding
Ed(c) and the query encoder maps q to an embedding Eq(q). The top- k relevant text chunks for q
are retrieved based on the query-document embedding similarity, which is often computed via dot
product:
s(q, c) =Eq(q) · Ed(c). (1)
We initialize the retriever using DRAGON + (Lin et al., 2023), a state-of-the-art dual-encoder model
trained with a contrastive learning objective and large-scale data augmentation.
Parallel In-Context Retrieval-Augmentation Following Shi et al. (2023b), for a given language
model prompt x, we retrieve the top- k relevant text chunks C′ ⊂ C, |C′| = k. To stay within the
context window size limit, each retrieved chunk is prepended to the prompt2, and the language model
predictions from multiple augmented prompts are computed in parallel. The final output probability
is a mixture of the probability from each augmented prompt weighted by the chunk relevance score
pLM (y|x, C′) =
X
c∈C′
pLM (y|c ◦ x) · pR(c|x), (2)
where ◦ denotes sequence concatenation, and pR(c|x) = exp s(x,c)P
c′∈C′ exp s(x,c′) are the retriever scores
re-normalized among top-k relevant chunks.
2.2 F INE -TUNING DATASETS
We choose a set of fine-tuning tasks aimed at boosting the language model’s ability to utilize knowl-
edge effectively and improving its contextual awareness in generating predictions. As shown in
Table 1, our language model fine-tuning datasets (DL) consists of 20 datasets across 5 distinct cat-
egories: dialogue, open-domain QA, reading comprehension3, summarization and chain-of-thought
reasoning. For retriever fine-tuning datasets DR, we opt for the QA datasets in our collection fea-
turing standalone questions, and we additionally include two QA datasets, FreebaseQA (Jiang et al.,
2019) and MS-MARCO (Nguyen et al., 2016). The examples of each dataset are serialized for in-
struction tuning using manually compiled templates (Table 10). For tasks in DL ∩ DR, we use the
same template for both fine-tuning steps. In addition, we observe that supplementing the instruction-
tuning data with unsupervised text leads to additional performance gains for both language model
and retriever fine-tuning, and we detail data mixture used in Appendix B.
2.3 R ETRIEVAL AUGMENTED LANGUAGE MODEL FINE -TUNING
To improve the language model’s ability to utilize retrieved information, we fine-tune it on the
selected datasets DL with in-context retrieval augmentation. Formally, we separate each fine-tuning
sequence into an instruction segment ( x) and an output segment ( y). For each example (xi, yi) ∈
2We use a pair of start (“Background:”) and end (“\n\n”) tokens to demarcate the retrieved segment in the
augmented prompt. The complete set of our instruction-tuning templates are shown in Appendix C.
3Our reading comprehension (RC) fine-tuning datasets include SQuAD 2.0 (Rajpurkar et al., 2018), which
trains the model to determine whether a question can be answered using a given passage, and to provide an
answer only when the passage is relevant (otherwise the response is set to “I don’t know”). As shown in
Appendix F, fine-tuning on this dataset promotes a desirable behavior: the instruction-tuned model tends to
respond with “I don’t know” when the retriever presents an incorrect passage. We leave further exploring this
behavior to improve answer generation as a future work.
3



Source: data\tc16_2312.10997v5\referenced_papers\[27]_2310.01352.pdf (Page 20):

Published as a conference paper at ICLR 2024
Table 10: Instruction template used for our fine-tuning datasets. <inst s>, <inst e> and
<answer s> are special markers denoting the start and the end of a field.
Category Instruction Tuning Template Query Template
Dialogue Background: {retrieved passage}\n\nQ:{turn1}A:{turn2}Q:
{turn3}A:...
{turn1} {turn2} {turn3}...
Open-domain QA Background:{retrieved passage}\n\n<insts> {question}
<inste> <answers>{answer}
{question}
Reading Compre-
hension
Background:{context}\n\n<insts>{question} <inste>
<answers>{answer}
{question}
Summarization Background: {context}\n\nSummarize this article:<inste>
<answers>{summary}
Chain-of-thought
Reasoning
Background:{retrieved passage}\n\n<insts>{instructions}
{reasoning chain}<answers>{answer}
{question}
Table 11: Our evaluation datasets. † indicates the development datasets we used to select fine-tuning
hyperparameters.
Task Dataset name Acronym Metric Score
Open-domain
QA
MMLU (?) MMLU Acc. nll
Natural Questions (Kwiatkowski et al., 2019) NQ EM nll
TriviaQA (Joshi et al., 2017) TQA EM nll
†HotpotQA (Yang et al., 2018) HoPo EM nll
ELI5 (Fan et al., 2019) ELI5 Rouge-L nll token
Fact Checking†FEVER (Thorne et al., 2018) FEV Acc. nll
Entity Linking†AIDA CoNLL-Y AGO (Hoffart et al., 2011) AIDA Acc. nll
Slot Filling
†Zero-Shot RE (Levy et al., 2017) zsRE Acc. nll
†T-REx (Elsahar et al., 2018) T-REx Acc. nll
Dialogue †Wizard of Wikipedia (Dinan et al., 2019) WoW F1 nll token
Commonsense
Reasoning
BoolQ (Clark et al., 2019) BoolQ Acc. nll compl
PIQA (Bisk et al., 2020) PIQA Acc. nll char
SIQA (Sap et al., 2019) SIQA Acc. nll char
HellaSwag (Zellers et al., 2019) HellaSwag Acc. nll char
WinoGrande (Sakaguchi et al., 2019) WinoGrande Acc. nll char
ARC-Easy (Clark et al., 2018) ARC-E Acc. nll char
ARC-Challenge (Clark et al., 2018) ARC-C Acc. nll char
OpenBookQA (Mihaylov et al., 2018) OBQA Acc. nll compl
computational cost. For test set evaluation, we use the full set to ensure fair comparison with pre-
vious work. The language model instruction templates and retriever queries used in our evaluation
are shown in Table 12. We randomly select few-shot examples from the official training splits of the
KILT tasks, except for FEV , NQ and TQA, where we use the 64-shot examples released by Izacard
et al. (2022b). For these three datasets, we also ensure that the 5-shot examples are subsets of the 64
examples. For retrieval augmented models, we use the top-1 relevant chunk to augment the prompt
for each in-context few-shot example.
E A DDITIONAL EXPERIMENTS
E.1 S CALING LAWS OF RETRIEVAL AUGMENTED LANGUAGE MODEL FINE -TUNING
We investigate the impact of the base language model size when retrieval-augmented instruction
tuning is applied, and summarize the results in Figure 2. We combine the fine-tuned models with
the base DRAGON + retriever in this set of experiments.
Overall, all models substantially benefit from retrieval augmentation, with smaller models witness-
ing even bigger improvements. We further note that retrieval augmentation can be an effective
strategy for enhancing the performance of smaller models (hence reducing pre-training and infer-
ence costs), given the 7B model leveraging > 1 retrieved chunks surpassed the performance of the
vanilla 65B model on several tasks. This trend also differs across tasks. For tasks that primarily
21



### Claim 18/179

#### Claim Text
In multiple evaluations of their performance on various knowledge-intensive tasks across different topics, [28] revealed that while unsupervised fine-tuning shows some improvement, RAG consistently outperforms it, for both existing knowledge encountered during training and entirely new knowledge.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[28]_2312.05934.pdf (Page 6):

Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs
Training Setup We trained all of the models using the
unsupervised training procedure described in Section 3.2.
For each dataset, we divided the auxiliary knowledge base
into equal chunks of size 256 by concatenating or splitting
the original chunks based on their length. We also added
two special tokens, <BOS> and <EOS>, to demarcate
the original chunks’ beginnings and ends to preserve the
documents’ structure.
The models were trained using learning rates between
1 × 10−6 and 5 × 10−5, which were found through a hyper-
parameter search. All models were trained on 4 NVIDIA
A-100 GPUs for a maximum of 5 epochs and a batch size
of 64.
Evaluation method All evaluations were done by ap-
pending each of the multiple-choice options to the question,
followed by passing the concatenation through the model
to get a log probability score per option. The highest score
was interpreted as the model’s choice and used for accuracy
calculation. More formally, this means that in Equation (1)
we say that M(qn) =cn if:
cn = arg max
l
{M(qn∥a1
n), . . . ,M(qn∥aL
n)}, (4)
where M(qn∥al
n) = logPM(qn∥al
n).
MMLU Results For each task and model, we compared
four approaches: using just the base model, RAG, FT, and fi-
nally combining FT and RAG by using the fine-tuned model
as the generator. Furthermore, we tested the MMLU tasks
using both 0-shot and 5-shot scenarios. The full results are
shown in Table 1. An aggregation of the relative accuracy
gain, i.e.,
(LM′,Q − LM,Q)/LM,Q, (5)
where M is the base model and M′ is the knowledge-
injected model, is shown in Figure 2.
In all cases, RAG performed significantly better compared
to the base models. Furthermore, using RAG with the base
model as the generator was consistently better than only fine-
tuning. In some cases, using the fine-tuned model instead
of the base model as the generator in the RAG pipeline im-
proved results even further. However, this is not consistent
and thus demonstrates the inherent instability of fine-tuning.
Additionally, we found that the 5-shot approach boosts the
results by a small margin in most cases, with a similar trend
being observed in all of the different approaches.
Current Events Results The evaluation on the current
events task is shown in Table 2. RAG proves particularly
effective due to the one-to-one correspondence between
the questions and the auxiliary dataset (see Section 4.3).
Fine-tuning is not competitive with RAG. However, fine-
tuning with multiple paraphrases still provides a significant
improvement over the baseline. We note that combining
RAG with fine-tuning shows inferior performance compared
to RAG alone.
It is worth noting that although the questions are based on
information the models were not exposed to during training,
the results of the base models surpass 1
L = 0.25. This can
partially be explained by the models using reasoning and/or
pre-existing knowledge when answering questions that are
not independent of the past information. Some examples of
this can be found in Appendix C.
Fine-Tuning vs. RAG: In the results of both the MMLU
and current events tasks, a significant advantage for RAG
over fine-tuning is evident. While fine-tuning improved
results compared to the base model in most cases, it was not
competitive with the RAG approach.
Several factors might contribute to this behavior. Firstly,
RAG not only adds knowledge to a model but also incor-
porates context relevant to the question, a feature lacking
in fine-tuning. Additionally, fine-tuning may impact other
capabilities of the model due to a degree of catastrophic for-
getting. Finally, it’s plausible that unsupervised fine-tuned
models might benefit from further alignment through super-
vised or RL-based fine-tuning, as evidenced by the vastly
improved performance of Orca2 over the base Llama2.
6. The Importance of Repetition
Unlike the other tasks, where the model has been exposed to
aspects related to the topic during pretraining,current events
includes new information. In this case, standard regular
fine-tuning not only did not improve the performance of
Llama2 but also significantly degraded it. To improve the
fine-tuning results, we explored augmentation of the data
using paraphrases.
Figure 3.Training loss over time for Mistral-7B.
Data Augmentation Data augmentation is a well-
established method for enhancing the performance of lan-
guage models and has been surveyed extensively (Shorten
7



Source: data\tc16_2312.10997v5\referenced_papers\[28]_2312.05934.pdf (Page 0):

Fine-Tuning or Retrieval?
Comparing Knowledge Injection in LLMs
Oded Ovadia *†, Menachem Brief†, Moshik Mishaeli, and Oren Elisha
{odedovadia,t-mbrief,mmishaeli,oren.elisha}@microsoft.com
Microsoft, Israel
Abstract
Large language models (LLMs) encapsulate a
vast amount of factual information within their
pre-trained weights, as evidenced by their abil-
ity to answer diverse questions across different
domains. However, this knowledge is inherently
limited, relying heavily on the characteristics of
the training data. Consequently, using external
datasets to incorporate new information or refine
the capabilities of LLMs on previously seen in-
formation poses a significant challenge. In this
study, we compare two common approaches: un-
supervised fine-tuning and retrieval-augmented
generation (RAG). We evaluate both approaches
on a variety of knowledge-intensive tasks across
different topics. Our findings reveal that while un-
supervised fine-tuning offers some improvement,
RAG consistently outperforms it, both for exist-
ing knowledge encountered during training and
entirely new knowledge. Moreover, we find that
LLMs struggle to learn new factual information
through unsupervised fine-tuning, and that expos-
ing them to numerous variations of the same fact
during training could alleviate this problem.
Keywords: LLMs, NLP, Fine-Tuning vs. RAG, Knowledge
and Factuality.
1. Introduction
Large language models (LLMs) are able to capture vast
amounts of factual information (Petroni et al., 2019; Cohen
et al., 2023; Hu et al., 2023). LLMs exhibit a remarkable
level of knowledge in various domains due to their massive
pre-training datasets. However, there are two significant
limitations to this knowledge. First, it is static and does
not update with time. Second, it is non-specific and thus
*Corresponding author.
†Equal contribution.
may lack nuanced expertise in particular domains. While
these are two different problems, they are deeply related
since their solution is the same: enhancing the model’s
knowledge.
Recently, the idea of adapting LLMs to particular domains
and updating their knowledge has become increasingly com-
mon (Yu et al., 2022). Various models have been suggested
to improve factual knowledge and capabilities in diverse
fields such as healthcare (Singhal et al., 2023a;b; Wu et al.,
2023a), finance (Wu et al., 2023b; Yang et al., 2023), and
law (Huang et al., 2023; Nguyen, 2023).
In this work, we focus on the evaluation of a model’s knowl-
edge and its ability to memorize, understand, and retrieve
factual data. We aim to understand the concept of knowl-
edge injection (Wang et al., 2020; Chen et al., 2022; Liu
et al., 2020; Lauscher et al., 2020). Given some knowledge
base in the form of a text corpus, what is the best way to
teach a pre-trained model this knowledge?
One way to add knowledge to a pre-trained model is through
fine-tuning. With fine-tuning, we continue the model’s train-
ing process and adapt it using task-specific data. By expos-
ing the model to a specific knowledge base, we expect the
model weights to adapt accordingly. This process is meant
to optimize the model for targeted applications, enhanc-
ing its performance and contextual relevance in specialized
domains.
Another method to enhance a model’s knowledge base is
through the use of in-context learning (ICL) (Chen et al.,
2021; Radford et al., 2019; Min et al., 2021; Lampinen
et al., 2022). The main idea behind ICL is to improve the
performance of pre-trained LLMs on new tasks by modify-
ing the input query to the model without directly changing
the weights of the model. One form of ICL is retrieval aug-
mented generation (RAG) (Lewis et al., 2020; Neelakantan
et al., 2022). RAG uses information retrieval techniques to
enable LLMs to obtain relevant information from a knowl-
edge source and incorporate it into generated text.
This study aims to evaluate the knowledge injection capa-
1
arXiv:2312.05934v3  [cs.AI]  30 Jan 2024



Source: data\tc16_2312.10997v5\referenced_papers\[28]_2312.05934.pdf (Page 7):

Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs
Figure 4.Model accuracy on the current events task as a function
of the number of paraphrases.
et al., 2021). Using generative models for augmentations
has also been used successfully to improve classification
models in the past (Sharma et al., 2022). An example of
data augmentation using paraphrasing can be found in Ap-
pendix B.
Monotonic ImprovementThis approach resulted in notable
improvements in our results, showcasing a direct correlation
between the number of paraphrases utilized and the mod-
els’ accuracy. Our experimentation revealed a compelling
trend, shown in Figure 4. For all models tested, the accuracy
was a monotonically increasing function of the number of
paraphrases used. This observation strongly suggests the
positive impact of paraphrase augmentation, yielding infor-
mation repetition, on the model’s ability to comprehend and
generalize new knowledge from limited data.
Learning New Information In Figure 3, we can see an in-
teresting phenomenon observed throughout our experiments.
After each epoch, i.e., completing another iteration over the
entire dataset, the training loss drops significantly. This is
consistent with what is known about LLMs memorizing the
data during training and overfitting (Tirumala et al., 2022).
Our hypothesis is as follows:
In order to teach pre-trained LLMs new knowl-
edge, the knowledge must be repeated in numer-
ous ways.
This is well known for LLM pre-training (Kandpal et al.,
2023), and we see in this case that this holds for fine-tuning
as well. The rationale for this hypothesis is that mere mem-
orization of sentences does not entail knowledge of their
content, as was already shown in (Berglund et al., 2023). By
providing the information in numerous forms (like the data
augmentation process we used), the various relationships in
the data (e.g., a =⇒ b, b ̸=⇒ c) stand a higher chance
of appearing naturally. We believe this can potentially both
increase LM,Q in general, as well as ameliorate Berglund
et al.’s Reversal Curse. While promising, this result still
warrants further research.
7. Conclusion and Future Work
Large language models possess vast amounts of knowledge
on various topics. In this work, we tested their capability to
adapt to new knowledge: both specialized and completely
unseen. This is among the first studies to compare two
prominent approaches in this domain, namely fine-tuning
and retrieval augmented generation. While fine-tuning can
be useful for many use-cases, we found that RAG is a more
reliable choice for knowledge injection.
Some aspects of this work still warrant further research. For
example, we focused on unsupervised training as our pri-
mary fine-tuning method, as opposed to instruction-tuning
or RL-based methods. Researching combinations of var-
ious techniques, with diverse auxiliary knowledge bases,
may yield improved results. This approach, combined with
our hypothesis from Section 6, could further enhance our
understanding of knowledge injection via FT.
While we believe that this work further enhances our under-
standing of knowledge in LLMs, there is a lot more work
to be done in this field. Specifically, more research is re-
quired regarding the question of knowledge representation
in LLMs, especially from a theoretical perspective.
Finally, further efforts are needed to measure knowledge
in LLMs. While we employed an empirical approach as
described in Equation (2), it is important to explore other
definitions and perspectives on knowledge as well, and ex-
tend upon this work.
8. Limitations
As in all machine learning applications, the choice of hyper-
parameters significantly impacts the results. We therefore
strongly recommend optimizing all relevant hyperparame-
ters for specific cases.
We have supported our claims by running the experiments
on three different models. However, generalization to other
LLMs should be tested thoroughly. For example, GPT-4
achieves near perfect accuracy for some MMLU tasks (Nori
et al., 2023), and thus further improvement is not applicable.
Finally, while we chose various topics for the knowledge
bases, all of our sources came from Wikipedia. Other
datasets may yield different results, and must be evaluated
carefully.
8



Source: data\tc16_2312.10997v5\referenced_papers\[67]_2401.15884.pdf (Page 5):

significantly helps to mitigate the dependence on
the accuracy of the retrieval evaluator.
4.4 Knowledge Refinement
Given a retrieved relevant document, a decompose-
then-recompose knowledge refinement method
is designed to further extract the most critical
knowledge strips in it. To obtain fine-grained
retrieval results, we segmented the retrieved results
into internal strips. If a retrieved result is as short as
one or two sentences, it is regarded as an individual
strip, otherwise, retrieval documents are required to
be split into smaller units which generally consist
of a few sentences according to the total length.
The scale is assumed to include an independent
piece of information, and the filtering is based on
the segments. Then, the retrieval evaluator fine-
tuned in Section 4.2 is employed to calculate the
relevance score of each knowledge strip. Based
on these scores, irrelevant knowledge strips are
filtered out, while relevant ones are recomposed via
concatenation in order, namely internal knowledge.
4.5 Web Search
It would be more intelligent if a system itself
could determine that its existing knowledge corpus
could not solve the problem well and turn to
additional external knowledge for help. On the
contrary, even if a system knows that the existing
knowledge cannot solve the problem, but still
sticks to the limited knowledge corpus, it will only
give a fabricated fact in the end, which is called
hallucination.. Therefore, it is extremely important
to seek complementary external knowledge if
the retrieved results are all assumed irrelevant,
and we consider a system that knows what it
doesn’t know and what it cannot answer to be
more intelligent than one that clings to limited
knowledge and is incapable of seeking external
knowledge. Since retrieval from static and limited
corpora can only return sub-optimal documents
in terms of scope and diversity, large-scale web
searches (Piktus et al., 2021; Komeili et al., 2022)
are integrated as a strategic extension of RAG.
Specifically, the inputs are rewritten into queries
composed of keywords by ChatGPT to mimic the
daily usage of search engine. The prompt for
rewriting is shown in Appendix A. In CRAG ,
a public and accessible commercial web search
API is adopted to generate a series of URL links
for every query. 3 Considering that knowledge
from large-scale web searches could introduce
biases or unreliable information, authoritative and
regulated web pages like Wikipedia are preferred,
which can significantly help mitigate these issues.
Moreover, we utilize the URL links to navigate
web pages, transcribe their content, and employ the
same knowledge refinement method as Section 4.4
to derive the relevant web knowledge, namely
external knowledge.
5 Experiments
We conducted experiments to extensively demon-
strate CRAG ’s adaptability to RAG-based ap-
proaches and its generalizability across both short-
and long-form generation tasks.
5.1 Tasks, Datasets and Metrics
CRAG was evaluated on four datasets, including
PopQA (Mallen et al., 2023) ( short-form gener-
ation), Biography (Min et al., 2023) ( long-form
generation), PubHealth (Zhang et al., 2023a) (true-
or-false question), and Arc-Challenge (Bhaktha-
vatsalam et al., 2021) ( multiple-choice question).
Following previous work, accuracy was adopted
as the evaluation metric for PopQA, PubHealth,
and Arc-Challenge. FactScore (Min et al., 2023)
was adopted as the evaluation metric for Biography.
Readers can refer to Appendix B.1 for more details.
The same metrics are used because our proposed
method is comparable to previous studies, since
we used the same retrieval results as previous
work. The difference lies in that our motivation
is to improve the retrieval quality by correcting
the retrieval results that the system judges to
be of low quality. This can be analogous to
RAG’s augmentation to standalone parameterized
language models and we further augment RAG
with corrective strategies.
5.2 Baselines
We primarily compared CRAG with both ap-
proaches with and without retrieval, where the
latter can be further split into standard RAG and
latest advanced RAG, including:
Baselines without retrieval. We evaluated some
public LLMs, LLaMA2-7B,13B (Touvron et al.,
2023b), instruction-tuned models, Alpaca-7B,13B
(Dubois et al., 2023), and CoVE65B (Dhuliawala
et al., 2024) which introduces iterative engineering
3In this study, Google Search API is utilized for searching.



Source: data\tc16_2312.10997v5\referenced_papers\[28]_2312.05934.pdf (Page 2):

Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs
Figure 1.A visualization of the knowledge injection framework.
complex multi-step reasoning tasks (Tan et al., 2023) or
when posed with different questions about the same fact,
resulting in disparate outcomes (Berglund et al., 2023).
We observe that most of these issues arise during the pre-
training phase, with catastrophic forgetting being the notable
exception. Hence, many LLMs will suffer from factual
errors of this kind regardless of any post-training process.
3. Injecting Knowledge to Language Models
Following the background given in Section 2, it is clear
that general pre-training is insufficient for many knowledge-
intensive tasks. To solve this, an additional post-processing
step is essential to augment the knowledge of a pre-trained
model. This step is often reffered to as knowledge injec-
tion (Wang et al., 2020; Chen et al., 2022; Liu et al., 2020;
Lauscher et al., 2020).
In this section, we examine two widely used frameworks
for knowledge injection: fine-tuning (FT) and retrieval aug-
mented generation (RAG). We begin by formulating the
knowledge injection problem, aiming to explain both meth-
ods using consistent terminology.
3.1. Problem formulation
In Equations (1) and (2), we presented a formulation for
knowledge in language models through the lens of question-
answering (Q&A). We now extend this formulation to the
problem of knowledge injection using the same terminology.
Given a set of factual questions, there exists some text cor-
pus containing information that is relevant to these questions.
The central assumption of knowledge injection is that given
full access to this corpus, it could serve as an auxiliary
knowledge base and improve the model’s performance on
this set of questions.
Mathematically, let M be a pre-trained model, and let Q be
a set of factual questions, as before. Now, assume we have
a relevant auxiliary knowledge base BQ. Our objective is to
discover a transformation, denoted as F, that, when applied,
would enhance the knowledge about Q:
M′ := F(M, BQ) s.t. LM′,Q > LM,Q. (3)
In this work, we aim to compare two choices for F: fine-
tuning and RAG to see which option performs better in this
problem.
3.2. Fine-Tuning
Fine-tuning is the process of adjusting a pre-trained model
on a specific, often narrower, dataset or task to enhance
its performance in that particular domain. Here, it is vital
to distinguish between different types of fine-tuning. FT
techniques are commonly classified into supervised, unsu-
3



### Claim 19/179

#### Claim Text
In addition to encyclopedic data, common unstructured data includes cross-lingual text [19] and domainspecific data (such as medical [67]and legal domains [29]).

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[19]_2311.06595.pdf (Page 1):

!"#$డ&క(అదృష-వంత12ో#ా5ో!"క(6ె8యదు, అతనుజ=రం#ారణం@ాAాఠCాలక(EFళHలIకAJ యKడ&?(I don't know if my son is lucky or unlucky, he couldn't go to school because of fever.)
Telugu input:
Cross-Lingual Retriever
I tried the new restaurant in town and the food was amazing, but the service was terrible. I don't think I'll be going back anytime soon.
Top k retrievedEnglishsamples:
What isthesentimentforthetext: Negative, Neutral, orPositive?______eni
SelfPrediction
MPLM
Consider the following English examples:Example: I tried the new restaurant in town and the food was amazing, but the service was terrible. I don't think I'll be going back anytime soon.Sentiment: Negative…Example: My best friend surprised me with tickets to my favorite band's concert and we had the best time ever! I can't thank her enough. Sentiment: PositiveWhat is a possible sentiment for theTelugutext:!"#$డ&క(అదృష-వంత12ో#ా5ో!"క(6ె8యదు, అతనుజ=రం#ారణం@ాAాఠCాలక(EFళHలIకAJ యKడ&?Negative,Neutral,orPositive?______
ConsiderthefollowingEnglishexamples:Text:Sentiment:enieni
WhatisthesentimentfortheTelugu text:Negative,Neutral,orPositive?______te
Prompt Engineering
Negative
MPLM
Neutral
Figure 1: Detailed overview of the CREA-ICL pipeline for LRLs: (a) An LRL input is used as a
query for the cross-lingual retriever, which then retrieves the most semantically similar HRL sample
from the HRL corpus. The associated label is either taken directly from the corpus (labeled setting)
or determined by self-prediction (unlabeled setting). (b) Next, this HRL sample, its label, and the
original input are combined to create a retrieval-augmented input for MPLM to make prediction.
forward in multilingual QA models, demonstrating a many-to-many approach that avoids language-
specific data and retrieval modules, which is particularly beneficial for low-resource languages.In
contrast, strategies like PARC [Nie et al., 2023] propose a more comprehensive methodology by
obtaining semantically aligned instructions from high-resource languages.
Building on these methods, our work introduces novel perspectives and aims to bridge gaps. While
MEGA provides task-centric instructions, we integrate deeper semantic understanding. We embrace
a cross-lingual approach akin to PARC, as depicted in Figure 1. In contrast to PARC’s focus on
masked language models like mBERT and XLMR, we explore the potential of larger, decoder-
only multilingual pretrained language models (MPLMs) - BLOOM and BLOOMZ. Our focus is
on addressing both classification and generation tasks in a cohesive generative style, emphasizing
instruction execution [Muennighoff et al., 2023, Scao et al., 2022].
This paper explores the application of cross-lingual retrieval-augmented ICL (CREA-ICL) to a
specific low-resourced case, the Bangla language-covering text classification and generation tasks.
We prioritize the effective execution of instructions. Our main contributions are:
• A comprehensive evaluation of cross-lingual retrieval augmented ICL, highlighting consis-
tent improvements over MPLMs’ zero-shot performance on Bangla classification tasks.
• An in-depth analysis revealing the challenges in Bangla generation task, providing insights
into the performance dynamics in both the classification and generation domains.
• A pioneering exploration to adapt PARC for generative models, BLOOM and BLOOMZ,
providing insights for a unified pipeline of CREA-ICL.
2



Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 9):

Preprint
long-text STS task. Furthermore, we have proposed an LLM-supervised learning method to cope
with the scarcity of domain-supervised data. Extensive experimental results have demonstrated that
AnglE outperforms baselines, indicating that AnglE can handle both short and long-text STS tasks
and work effectively in various scenarios. In future work, we plan to explore the application of
AnglE in real-world scenarios and provide further insights into AnglE.
REFERENCES
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. SemEval-2012 task 6: A pilot
on semantic textual similarity. In *SEM 2012: The First Joint Conference on Lexical and Com-
putational Semantics – Volume 1: Proceedings of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval
2012), pp. 385–393. Association for Computational Linguistics, 2012.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. *SEM 2013 shared
task: Semantic textual similarity. In Second Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic
Textual Similarity, pp. 32–43, Atlanta, Georgia, USA, 2013. Association for Computational Lin-
guistics.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Wei-
wei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. SemEval-2014 task 10: Multilingual
semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Eval-
uation (SemEval 2014), pp. 81–91. Association for Computational Linguistics, 2014.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Wei-
wei Guo, I˜nigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria,
and Janyce Wiebe. SemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot
on interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation
(SemEval 2015), pp. 252–263. Association for Computational Linguistics, 2015.
Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, Ger-
man Rigau, and Janyce Wiebe. SemEval-2016 task 1: Semantic textual similarity, monolingual
and cross-lingual evaluation. In Proceedings of the 10th International Workshop on Semantic
Evaluation (SemEval-2016), pp. 497–511. Association for Computational Linguistics, 2016.
Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and
applications. In Proceedings of the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 6: Tutorial Abstracts), pp. 41–46. Association for Computational Linguistics,
July 2023.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large anno-
tated corpus for learning natural language inference. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing, pp. 632–642. Association for Computational
Linguistics, 2015.
Fredrik Carlsson, Amaru Cuba Gyllensten, Evangelia Gogoulou, Erik Ylip¨a¨a Hellqvist, and Magnus
Sahlgren. Semantic re-tuning with contrastive tension. In International conference on learning
representations, 2020.
Daniel Cer, Mona Diab, Eneko Agirre, I˜nigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task
1: Semantic textual similarity multilingual and crosslingual focused evaluation. InProceedings of
the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 1–14. Association
for Computational Linguistics, August 2017.
Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Con-
stant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Brian Strope, and Ray Kurzweil. Univer-
sal sentence encoder for English. In Proceedings of the 2018 Conference on Empirical Methods
in Natural Language Processing: System Demonstrations, pp. 169–174. Association for Compu-
tational Linguistics, 2018.
10



Source: data\tc16_2312.10997v5\referenced_papers\[179]_2210.08174.pdf (Page 4):

4 Conclusion
In this work, we proposed a simple, fast and effec-
tive data augmentation technique, SpokenVocabfor
ST. This provides an alternative for converting MT
data to ST data with TTS systems which comes
with monetary and computation costs in practice.
Our approach generates synthetic speech on-the-
ﬂy during training, with no cost or footprint. We
have shown that speech stitched from SpokenVo-
cab works as effective as TTS-generated speech,
and unlike TTS system, it could directly be applied
as a data augmentation tool in code-switching ST.
Our approach can be used in other content-driven
speech processing tasks as an uncompromising and
easy-to-use augmentation technique.
Limitations
CS ST exbihit difﬁculties (Huber et al., 2022;
Weller et al., 2022), exposing several limitations
in this study: 1) Bengali and Sanskrit (another mi-
nority language) are treated without difference, as
they originate from the same script and Sanskrit
is not supported by the Google TTS service. 2)
We use a open-source language detection tool to
calculate the oracle hyper-parameters in the dev
set; yet, imperfection of the detector on token-level
prediction and the fact that source sentences are
written in Latin regardless of the language deviate
the scores from true values.
References
Antonios Anastasopoulos and David Chiang. 2018.
Tied multitask learning for neural speech translation.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 82–91.
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed,
and Michael Auli. 2020. wav2vec 2.0: A frame-
work for self-supervised learning of speech represen-
tations. arXiv preprint arXiv:2006.11477.
Sameer Bansal, Herman Kamper, Karen Livescu,
Adam Lopez, and Sharon Goldwater. 2019. Pre-
training on high-resource speech recognition im-
proves low-resource speech-to-text translation. In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 58–68.
Roldano Cattoni, Mattia Antonino Di Gangi, Luisa
Bentivogli, Matteo Negri, and Marco Turchi. 2021.
Must-c: A multilingual corpus for end-to-end
speech translation. Computer Speech & Language,
66:101155.
Gerard I Gállego, Ioannis Tsiamas, Carlos Escolano,
José AR Fonollosa, and Marta R Costa-jussà. 2021.
End-to-end speech translation with pre-trained mod-
els and adapters: Upc at iwslt 2021. In Proceedings
of the 18th International Conference on Spoken Lan-
guage Translation (IWSLT 2021), pages 110–119.
Christian Huber, Enes Yavuz Ugan, and Alexander
Waibel. 2022. Code-switching without switching:
Language agnostic end-to-end speech translation.
arXiv preprint arXiv:2210.01512.
Hirofumi Inaguma, Tatsuya Kawahara, and Shinji
Watanabe. 2021. Source and target bidirectional
knowledge distillation for end-to-end speech trans-
lation. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1872–1881.
Javier Iranzo-Sánchez, Joan Albert Silvestre-Cerda,
Javier Jorge, Nahuel Roselló, Adria Giménez, Al-
bert Sanchis, Jorge Civera, and Alfons Juan. 2020.
Europarl-st: A multilingual corpus for speech trans-
lation of parliamentary debates. In ICASSP 2020-
2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pages
8229–8233. IEEE.
Ye Jia, Melvin Johnson, Wolfgang Macherey, Ron J
Weiss, Yuan Cao, Chung-Cheng Chiu, Naveen Ari,
Stella Laurenzo, and Yonghui Wu. 2019. Lever-
aging weakly supervised data to improve end-to-
end speech-to-text translation. In ICASSP 2019-
2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pages
7180–7184. IEEE.
Tsz Kin Lam, Shigehiko Schamoni, and Stefan Riezler.
2022. Sample, translate, recombine: Leveraging au-
dio alignments for data augmentation in end-to-end
speech translation. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 245–
254.
Xian Li, Changhan Wang, Yun Tang, Chau Tran,
Yuqing Tang, Juan Pino, Alexei Baevski, Alexis
Conneau, and Michael Auli. 2021. Multilingual
speech translation from efﬁcient ﬁnetuning of pre-
trained models. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 827–838.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising
pre-training for neural machine translation. Transac-
tions of the Association for Computational Linguis-
tics, 8:726–742.



Source: data\tc16_2312.10997v5\referenced_papers\[19]_2311.06595.pdf (Page 4):

SentNoB The SentNoB dataset [Islam et al., 2021] is crafted to dissect the sentiment embedded
within Bangla texts. Our prompt template P is articulated as:
• Autoregressive models:
"Text: {text} What is a possible sentiment for the text given the
following options?"
• Mask prediction models: "{text} Sentiment: [MASK]"
with the verbalizer:
v(0) =positive, v(1) =neural, v(2) =negative
For SentNoB, we use the labeled training set of English Sentiment Analysis dataset [Rosenthal et al.,
2017], which consists of tweets annotated for sentiment on 2-, 3-, and 5-point scales with labels
positive, negative, and neutral.
XLSum As a typical representative generation task, XLSum [Hasan et al., 2021], the multilingual
text summarization dataset consisting of 1.35 million pairs of articles and their corresponding
summaries. These pairs have been expertly annotated by the BBC and meticulously extracted
through a series of carefully designed heuristic methods. We evaluate its Bangla subset to assess
the performance of generation. The English training set is used as the retrieval corpus. The prompt
template is defined as follows:
"{text} Generate a concise summary of the above text using the same
language as the original text:"
XQuAD the XQuAD (Cross-lingual Question Answering Dataset) [Artetxe et al., 2019] was used,
which contains topic-diverse, manually-curated question-answer pairs with their respective contexts
in 11 languages. Greek and Romanian language subsets are being evaluated. These pairs have been
carefully translated from the English SQuAD v1.1 dataset to ensure high quality translations. The
prompt template is defined as follows:
"context: {context} question: {question} answer:"
4.2 Models
BLOOM is an autoregressive Large Language Model trained on a diverse corpus to generate text
based on prompts [Scao et al., 2022]. It is capable of generating coherent text in 46 languages.
BLOOMZ takes a novel approach in the MPLM landscape by applying Bloom filters in the context
of language models [Muennighoff et al., 2023]. This allows the model to use high-resource languages
to improve embeddings for low-resource languages, effectively bridging the gap between languages
with different levels of available resources.
mBERT is an early MPLM that extends the original BERT model [Devlin et al., 2018]. It is pre-
trained on a corpus of 104 languages, using shared WordPiece vocabularies and a unified architecture
for all languages.
mT5 or Multilingual T5 [Xue et al., 2021], is an extension of the T5 (Text-to-Text Transfer
Transformer) model [Raffel et al., 2020] specifically designed for multilingual capabilities. Pre-trained
on mC4, a large multilingual dataset, mT5 demonstrates multilingual capabilities by transforming
input text sequences into output sequences.
Cross-Lingual Retriever We followed Nie et al. [2023] to use the multilingual sentence transformer
“paraphrase-multilingual-mpnet-base-v2” [Reimers and Gurevych, 2019b]. This transformer maps
sentences and paragraphs into a 768-dimensional dense vector space. Such a high-dimensional
embedding facilitates tasks such as clustering and semantic search. Retrieval sample settings k are
meticulously set at 1 and 3 for classification, and confined to 1 for summarization, 3 for QA.
5



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 10):

[8] Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation with
monolingual translation memory. In Proc. of ACL, 2021.
[9] Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in
machine translation research. In Proc. of EACL, 2006.
[10] Qian Cao and Deyi Xiong. Encoding gated translation memory into neural machine translation.
In Proc. of EMNLP, 2018.
[11] Eugene Charniak and Mark Johnson. Coarse-to-fine n-best parsing and maxent discriminative
reranking. In Proc. of ACL, 2005.
[12] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer
open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, 2017.
[13] Wei Chen, Yeyun Gong, Song Wang, Bolun Yao, Weizhen Qi, Zhongyu Wei, Xiaowu Hu,
Bartuer Zhou, Yi Mao, Weizhu Chen, Biao Cheng, and Nan Duan. Dialogved: A pre-trained
latent variable encoder-decoder model for dialog response generation. In Proc. of ACL, 2022.
[14] Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, and Xiangliang
Zhang. Target-aware abstractive related work generation with contrastive learning. In Proc. of
SIGIR, 2022.
[15] Xiuying Chen, Mingzhe Li, Shen Gao, Xin Cheng, Qiang Yang, Qishen Zhang, Xin Gao,
and Xiangliang Zhang. A topic-aware summarization framework with different modal side
information. Proc. of SIGIR, 2023.
[16] Xiuying Chen, Mingzhe Li, Xin Gao, and Xiangliang Zhang. Towards improving faithfulness
in abstractive summarization. In Proc. of NeurIPS, 2022.
[17] Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine translation
with contrastive translation memories. In Proc. of EMNLP, 2022.
[18] Xin Cheng, Shen Gao, Yuchi Zhang, Yongliang Wang, Xiuying Chen, Mingzhe Li, Dongyan
Zhao, and Rui Yan. Towards personalized review summarization by modeling historical
reviews from customer and product separately. arXiv preprint arXiv:2301.11682, 2023.
[19] Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, and Rui Yan. Decouple knowledge
from paramters for plug-and-play language modeling. In Findings of the Association for
Computational Linguistics: ACL 2023, pages 14288–14308, Toronto, Canada, July 2023.
Association for Computational Linguistics.
[20] Xin Cheng, Xun Wang, Tao Ge, Si-Qing Chen, Furu Wei, Dongyan Zhao, and Rui Yan. Scale:
Synergized collaboration of asymmetric language translation engines, 2023.
[21] Michael Collins and Terry Koo. Discriminative reranking for natural language parsing.Comput.
Linguistics, 2005.
[22] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,
Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.
Unsupervised cross-lingual representation learning at scale. In Proc. of ACL, 2020.
[23] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc’Aurelio Ranzato. Residual
energy-based models for text generation. In Proc. of ICLR, 2020.
[24] Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, and Luke Zettlemoyer. CORE: A
retrieve-then-edit framework for counterfactual data generation. In Proc. of EMNLP Findings,
2022.
[25] Le Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, and Changyou Chen. Implicit deep latent
variable models for text generation. In Proc. of EMNLP, 2019.
11



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[19]_2311.06595.pdf (Page 1):

!"#$డ&క(అదృష-వంత12ో#ా5ో!"క(6ె8యదు, అతనుజ=రం#ారణం@ాAాఠCాలక(EFళHలIకAJ యKడ&?(I don't know if my son is lucky or unlucky, he couldn't go to school because of fever.)
Telugu input:
Cross-Lingual Retriever
I tried the new restaurant in town and the food was amazing, but the service was terrible. I don't think I'll be going back anytime soon.
Top k retrievedEnglishsamples:
What isthesentimentforthetext: Negative, Neutral, orPositive?______eni
SelfPrediction
MPLM
Consider the following English examples:Example: I tried the new restaurant in town and the food was amazing, but the service was terrible. I don't think I'll be going back anytime soon.Sentiment: Negative…Example: My best friend surprised me with tickets to my favorite band's concert and we had the best time ever! I can't thank her enough. Sentiment: PositiveWhat is a possible sentiment for theTelugutext:!"#$డ&క(అదృష-వంత12ో#ా5ో!"క(6ె8యదు, అతనుజ=రం#ారణం@ాAాఠCాలక(EFళHలIకAJ యKడ&?Negative,Neutral,orPositive?______
ConsiderthefollowingEnglishexamples:Text:Sentiment:enieni
WhatisthesentimentfortheTelugu text:Negative,Neutral,orPositive?______te
Prompt Engineering
Negative
MPLM
Neutral
Figure 1: Detailed overview of the CREA-ICL pipeline for LRLs: (a) An LRL input is used as a
query for the cross-lingual retriever, which then retrieves the most semantically similar HRL sample
from the HRL corpus. The associated label is either taken directly from the corpus (labeled setting)
or determined by self-prediction (unlabeled setting). (b) Next, this HRL sample, its label, and the
original input are combined to create a retrieval-augmented input for MPLM to make prediction.
forward in multilingual QA models, demonstrating a many-to-many approach that avoids language-
specific data and retrieval modules, which is particularly beneficial for low-resource languages.In
contrast, strategies like PARC [Nie et al., 2023] propose a more comprehensive methodology by
obtaining semantically aligned instructions from high-resource languages.
Building on these methods, our work introduces novel perspectives and aims to bridge gaps. While
MEGA provides task-centric instructions, we integrate deeper semantic understanding. We embrace
a cross-lingual approach akin to PARC, as depicted in Figure 1. In contrast to PARC’s focus on
masked language models like mBERT and XLMR, we explore the potential of larger, decoder-
only multilingual pretrained language models (MPLMs) - BLOOM and BLOOMZ. Our focus is
on addressing both classification and generation tasks in a cohesive generative style, emphasizing
instruction execution [Muennighoff et al., 2023, Scao et al., 2022].
This paper explores the application of cross-lingual retrieval-augmented ICL (CREA-ICL) to a
specific low-resourced case, the Bangla language-covering text classification and generation tasks.
We prioritize the effective execution of instructions. Our main contributions are:
• A comprehensive evaluation of cross-lingual retrieval augmented ICL, highlighting consis-
tent improvements over MPLMs’ zero-shot performance on Bangla classification tasks.
• An in-depth analysis revealing the challenges in Bangla generation task, providing insights
into the performance dynamics in both the classification and generation domains.
• A pioneering exploration to adapt PARC for generative models, BLOOM and BLOOMZ,
providing insights for a unified pipeline of CREA-ICL.
2



Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 9):

Preprint
long-text STS task. Furthermore, we have proposed an LLM-supervised learning method to cope
with the scarcity of domain-supervised data. Extensive experimental results have demonstrated that
AnglE outperforms baselines, indicating that AnglE can handle both short and long-text STS tasks
and work effectively in various scenarios. In future work, we plan to explore the application of
AnglE in real-world scenarios and provide further insights into AnglE.
REFERENCES
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. SemEval-2012 task 6: A pilot
on semantic textual similarity. In *SEM 2012: The First Joint Conference on Lexical and Com-
putational Semantics – Volume 1: Proceedings of the main conference and the shared task, and
Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval
2012), pp. 385–393. Association for Computational Linguistics, 2012.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. *SEM 2013 shared
task: Semantic textual similarity. In Second Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic
Textual Similarity, pp. 32–43, Atlanta, Georgia, USA, 2013. Association for Computational Lin-
guistics.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Wei-
wei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. SemEval-2014 task 10: Multilingual
semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Eval-
uation (SemEval 2014), pp. 81–91. Association for Computational Linguistics, 2014.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Wei-
wei Guo, I˜nigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria,
and Janyce Wiebe. SemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot
on interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation
(SemEval 2015), pp. 252–263. Association for Computational Linguistics, 2015.
Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, Ger-
man Rigau, and Janyce Wiebe. SemEval-2016 task 1: Semantic textual similarity, monolingual
and cross-lingual evaluation. In Proceedings of the 10th International Workshop on Semantic
Evaluation (SemEval-2016), pp. 497–511. Association for Computational Linguistics, 2016.
Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and
applications. In Proceedings of the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 6: Tutorial Abstracts), pp. 41–46. Association for Computational Linguistics,
July 2023.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large anno-
tated corpus for learning natural language inference. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing, pp. 632–642. Association for Computational
Linguistics, 2015.
Fredrik Carlsson, Amaru Cuba Gyllensten, Evangelia Gogoulou, Erik Ylip¨a¨a Hellqvist, and Magnus
Sahlgren. Semantic re-tuning with contrastive tension. In International conference on learning
representations, 2020.
Daniel Cer, Mona Diab, Eneko Agirre, I˜nigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task
1: Semantic textual similarity multilingual and crosslingual focused evaluation. InProceedings of
the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 1–14. Association
for Computational Linguistics, August 2017.
Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Con-
stant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Brian Strope, and Ray Kurzweil. Univer-
sal sentence encoder for English. In Proceedings of the 2018 Conference on Empirical Methods
in Natural Language Processing: System Demonstrations, pp. 169–174. Association for Compu-
tational Linguistics, 2018.
10



Source: data\tc16_2312.10997v5\referenced_papers\[179]_2210.08174.pdf (Page 4):

4 Conclusion
In this work, we proposed a simple, fast and effec-
tive data augmentation technique, SpokenVocabfor
ST. This provides an alternative for converting MT
data to ST data with TTS systems which comes
with monetary and computation costs in practice.
Our approach generates synthetic speech on-the-
ﬂy during training, with no cost or footprint. We
have shown that speech stitched from SpokenVo-
cab works as effective as TTS-generated speech,
and unlike TTS system, it could directly be applied
as a data augmentation tool in code-switching ST.
Our approach can be used in other content-driven
speech processing tasks as an uncompromising and
easy-to-use augmentation technique.
Limitations
CS ST exbihit difﬁculties (Huber et al., 2022;
Weller et al., 2022), exposing several limitations
in this study: 1) Bengali and Sanskrit (another mi-
nority language) are treated without difference, as
they originate from the same script and Sanskrit
is not supported by the Google TTS service. 2)
We use a open-source language detection tool to
calculate the oracle hyper-parameters in the dev
set; yet, imperfection of the detector on token-level
prediction and the fact that source sentences are
written in Latin regardless of the language deviate
the scores from true values.
References
Antonios Anastasopoulos and David Chiang. 2018.
Tied multitask learning for neural speech translation.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 82–91.
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed,
and Michael Auli. 2020. wav2vec 2.0: A frame-
work for self-supervised learning of speech represen-
tations. arXiv preprint arXiv:2006.11477.
Sameer Bansal, Herman Kamper, Karen Livescu,
Adam Lopez, and Sharon Goldwater. 2019. Pre-
training on high-resource speech recognition im-
proves low-resource speech-to-text translation. In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 58–68.
Roldano Cattoni, Mattia Antonino Di Gangi, Luisa
Bentivogli, Matteo Negri, and Marco Turchi. 2021.
Must-c: A multilingual corpus for end-to-end
speech translation. Computer Speech & Language,
66:101155.
Gerard I Gállego, Ioannis Tsiamas, Carlos Escolano,
José AR Fonollosa, and Marta R Costa-jussà. 2021.
End-to-end speech translation with pre-trained mod-
els and adapters: Upc at iwslt 2021. In Proceedings
of the 18th International Conference on Spoken Lan-
guage Translation (IWSLT 2021), pages 110–119.
Christian Huber, Enes Yavuz Ugan, and Alexander
Waibel. 2022. Code-switching without switching:
Language agnostic end-to-end speech translation.
arXiv preprint arXiv:2210.01512.
Hirofumi Inaguma, Tatsuya Kawahara, and Shinji
Watanabe. 2021. Source and target bidirectional
knowledge distillation for end-to-end speech trans-
lation. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1872–1881.
Javier Iranzo-Sánchez, Joan Albert Silvestre-Cerda,
Javier Jorge, Nahuel Roselló, Adria Giménez, Al-
bert Sanchis, Jorge Civera, and Alfons Juan. 2020.
Europarl-st: A multilingual corpus for speech trans-
lation of parliamentary debates. In ICASSP 2020-
2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pages
8229–8233. IEEE.
Ye Jia, Melvin Johnson, Wolfgang Macherey, Ron J
Weiss, Yuan Cao, Chung-Cheng Chiu, Naveen Ari,
Stella Laurenzo, and Yonghui Wu. 2019. Lever-
aging weakly supervised data to improve end-to-
end speech-to-text translation. In ICASSP 2019-
2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pages
7180–7184. IEEE.
Tsz Kin Lam, Shigehiko Schamoni, and Stefan Riezler.
2022. Sample, translate, recombine: Leveraging au-
dio alignments for data augmentation in end-to-end
speech translation. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 245–
254.
Xian Li, Changhan Wang, Yun Tang, Chau Tran,
Yuqing Tang, Juan Pino, Alexei Baevski, Alexis
Conneau, and Michael Auli. 2021. Multilingual
speech translation from efﬁcient ﬁnetuning of pre-
trained models. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 827–838.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising
pre-training for neural machine translation. Transac-
tions of the Association for Computational Linguis-
tics, 8:726–742.



Source: data\tc16_2312.10997v5\referenced_papers\[19]_2311.06595.pdf (Page 4):

SentNoB The SentNoB dataset [Islam et al., 2021] is crafted to dissect the sentiment embedded
within Bangla texts. Our prompt template P is articulated as:
• Autoregressive models:
"Text: {text} What is a possible sentiment for the text given the
following options?"
• Mask prediction models: "{text} Sentiment: [MASK]"
with the verbalizer:
v(0) =positive, v(1) =neural, v(2) =negative
For SentNoB, we use the labeled training set of English Sentiment Analysis dataset [Rosenthal et al.,
2017], which consists of tweets annotated for sentiment on 2-, 3-, and 5-point scales with labels
positive, negative, and neutral.
XLSum As a typical representative generation task, XLSum [Hasan et al., 2021], the multilingual
text summarization dataset consisting of 1.35 million pairs of articles and their corresponding
summaries. These pairs have been expertly annotated by the BBC and meticulously extracted
through a series of carefully designed heuristic methods. We evaluate its Bangla subset to assess
the performance of generation. The English training set is used as the retrieval corpus. The prompt
template is defined as follows:
"{text} Generate a concise summary of the above text using the same
language as the original text:"
XQuAD the XQuAD (Cross-lingual Question Answering Dataset) [Artetxe et al., 2019] was used,
which contains topic-diverse, manually-curated question-answer pairs with their respective contexts
in 11 languages. Greek and Romanian language subsets are being evaluated. These pairs have been
carefully translated from the English SQuAD v1.1 dataset to ensure high quality translations. The
prompt template is defined as follows:
"context: {context} question: {question} answer:"
4.2 Models
BLOOM is an autoregressive Large Language Model trained on a diverse corpus to generate text
based on prompts [Scao et al., 2022]. It is capable of generating coherent text in 46 languages.
BLOOMZ takes a novel approach in the MPLM landscape by applying Bloom filters in the context
of language models [Muennighoff et al., 2023]. This allows the model to use high-resource languages
to improve embeddings for low-resource languages, effectively bridging the gap between languages
with different levels of available resources.
mBERT is an early MPLM that extends the original BERT model [Devlin et al., 2018]. It is pre-
trained on a corpus of 104 languages, using shared WordPiece vocabularies and a unified architecture
for all languages.
mT5 or Multilingual T5 [Xue et al., 2021], is an extension of the T5 (Text-to-Text Transfer
Transformer) model [Raffel et al., 2020] specifically designed for multilingual capabilities. Pre-trained
on mC4, a large multilingual dataset, mT5 demonstrates multilingual capabilities by transforming
input text sequences into output sequences.
Cross-Lingual Retriever We followed Nie et al. [2023] to use the multilingual sentence transformer
“paraphrase-multilingual-mpnet-base-v2” [Reimers and Gurevych, 2019b]. This transformer maps
sentences and paragraphs into a 768-dimensional dense vector space. Such a high-dimensional
embedding facilitates tasks such as clustering and semantic search. Retrieval sample settings k are
meticulously set at 1 and 3 for classification, and confined to 1 for summarization, 3 for QA.
5



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 10):

[8] Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation with
monolingual translation memory. In Proc. of ACL, 2021.
[9] Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in
machine translation research. In Proc. of EACL, 2006.
[10] Qian Cao and Deyi Xiong. Encoding gated translation memory into neural machine translation.
In Proc. of EMNLP, 2018.
[11] Eugene Charniak and Mark Johnson. Coarse-to-fine n-best parsing and maxent discriminative
reranking. In Proc. of ACL, 2005.
[12] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer
open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, 2017.
[13] Wei Chen, Yeyun Gong, Song Wang, Bolun Yao, Weizhen Qi, Zhongyu Wei, Xiaowu Hu,
Bartuer Zhou, Yi Mao, Weizhu Chen, Biao Cheng, and Nan Duan. Dialogved: A pre-trained
latent variable encoder-decoder model for dialog response generation. In Proc. of ACL, 2022.
[14] Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, and Xiangliang
Zhang. Target-aware abstractive related work generation with contrastive learning. In Proc. of
SIGIR, 2022.
[15] Xiuying Chen, Mingzhe Li, Shen Gao, Xin Cheng, Qiang Yang, Qishen Zhang, Xin Gao,
and Xiangliang Zhang. A topic-aware summarization framework with different modal side
information. Proc. of SIGIR, 2023.
[16] Xiuying Chen, Mingzhe Li, Xin Gao, and Xiangliang Zhang. Towards improving faithfulness
in abstractive summarization. In Proc. of NeurIPS, 2022.
[17] Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine translation
with contrastive translation memories. In Proc. of EMNLP, 2022.
[18] Xin Cheng, Shen Gao, Yuchi Zhang, Yongliang Wang, Xiuying Chen, Mingzhe Li, Dongyan
Zhao, and Rui Yan. Towards personalized review summarization by modeling historical
reviews from customer and product separately. arXiv preprint arXiv:2301.11682, 2023.
[19] Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, and Rui Yan. Decouple knowledge
from paramters for plug-and-play language modeling. In Findings of the Association for
Computational Linguistics: ACL 2023, pages 14288–14308, Toronto, Canada, July 2023.
Association for Computational Linguistics.
[20] Xin Cheng, Xun Wang, Tao Ge, Si-Qing Chen, Furu Wei, Dongyan Zhao, and Rui Yan. Scale:
Synergized collaboration of asymmetric language translation engines, 2023.
[21] Michael Collins and Terry Koo. Discriminative reranking for natural language parsing.Comput.
Linguistics, 2005.
[22] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,
Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.
Unsupervised cross-lingual representation learning at scale. In Proc. of ACL, 2020.
[23] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc’Aurelio Ranzato. Residual
energy-based models for text generation. In Proc. of ICLR, 2020.
[24] Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, and Luke Zettlemoyer. CORE: A
retrieve-then-edit framework for counterfactual data generation. In Proc. of EMNLP Findings,
2022.
[25] Le Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, and Changyou Chen. Implicit deep latent
variable models for text generation. In Proc. of EMNLP, 2019.
11



### Claim 20/179

#### Claim Text
When dealing with semi-structured data, one approach involves leveraging the code capabilities of LLMs to execute Text-2-SQL queries on tables within databases, such as TableGPT [85].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[85]_2307.08674.pdf (Page 7):

Figure 3: Cases of TableGPT.
Figure 4: Cases of TableGPT.
8



Source: data\tc16_2312.10997v5\referenced_papers\[85]_2307.08674.pdf (Page 8):

Figure 5: Cases of TableGPT.
Figure 6: Cases of TableGPT.
9



Source: data\tc16_2312.10997v5\referenced_papers\[85]_2307.08674.pdf (Page 9):

Figure 7: Cases of TableGPT.
Figure 8: Cases of TableGPT.
10



Source: data\tc16_2312.10997v5\referenced_papers\[85]_2307.08674.pdf (Page 2):

TableGPT
Housing_price.csv
Table 
Encoder
LLM111
111GPT
User query: How 
house prices have 
changed by region 
in the last decade?
Answer: According to the 
table, the average house 
price in each region has 
gradually increased over 
the past decade, with the 
largest increase in City A...
{
"type": "commands", // "text" or "commands"
"value": {
"commands": [
"SelectCondition",
"GroupBy“
],
"commands_args": [
{
"columns": ["Year"],
"index": [],
"range": [2013, 2023],
"condition": "range",
"slice": "no",
"type": "column",
"relation": "none"
},
{
"by": ["Region", "Year"],
"aggregate_args": {"Price": ["mean"]}
}
]
}
}
Command System
CorrectorExecutor
Generated Table
{
"type": "text", // "text" or "commands"
"value": "According to the table, the average 
house price in each region has gradually increased 
over the past decade, with the largest increase in 
City A..."
}
Commands Set
• InsertCondition
• DeleteCondition
• SelectCondition
• StatisticAnalysis
• SortCondition
• GroupBy
• UnaryTransform
• BinaryTransform
• Visualization
• Prediction
• ……
Figure 1: An architecture of TableGPT framework.
make this approach scalable and feasible, we have also developed a data processing pipeline
that yields notable improvements with only a small amount of data, hence alleviating the
resource-demanding aspect of training LLMs and supporting private deployment.
From a real-world production standpoint, the unstructured code outputted by NL2SQL poses sig-
nificant challenges for preemptive checks and error corrections. Hence, we advocate for the use
of structured command sequences, simplifying post-processing. Data-Copilot [ 38] also embraces
this command-based approach with self-instruct [31], but its reliance on API-called native LLMs
to comprehend tabular data’s processing and analysis logic directly presents limitations. Given the
intrinsic data variability and task-specificity of tabular data, we believe an effective product should be
custom-built for tabular data while maintaining general applicability to broader downstream tasks.
This conviction underscores the imperative of introducing a LLM specifically pre-trained for tabular
data.
To sum up, this work presents a pioneering TableGPT framework, which is a unified, well-fledged
holistic solution, enabling efficient tabular data processing, analysis and visualization, driven all by
natural languages. We summarize several important advantages of TableGPT as follows:
• Language-driven EDA: TableGPT understands user intent from natural language, dissects
the desired actions, and executes external commands on the table. It subsequently returns
the processed results in both tabular and textual explanations to the user. This novel
approach simplifies the way users engage with table data, bringing an intuitive instantiation
to Exploratory Data Analysis (EDA).
• Unified Cross-modal Framework: Innovatively, we devise a global table encoder for
understanding the whole table. TableGPT is able to fully understand the user query, meta-
knowledge, and whole tabular data, which leads to much more reliable execution commands
for table manipulation.
• Generalization and Privacy: By domain-aware fine-tuning, our TableGPT can better
handle data variability of tables and generalize to different domains. Further, our framework
supports private deployment, offering robust data privacy protections. This aspect is critical
in the modern age where data privacy and protection are just paramount.
3



Source: data\tc16_2312.10997v5\referenced_papers\[85]_2307.08674.pdf (Page 6):

3.3 Case Study
We show some cases in Figure 2 - 8. More examples will be released soon.
4 Conclusion
We present TableGPT, a large language model designed for table analysis, unifying tables, nature
language, and commands. It enables a variety of functions like answering questions, manipulating
data, visualizing information, generating analysis reports, and making predictions. Technically,
TableGPT addresses several major challenges in developing a natural language-driven framework for
table data processing, including comprehensive table understanding, instruction chain generation,
and domain-specific fine-tuning. We believe TableGPT has the potential to reshape the landscape of
tabular data processing, accelerating the efficiency of table modeling and exploratory data analysis
(EDA), and empowering various domains like finance, transportation, scientific research, etc.
Figure 2: Cases of TableGPT.
7



### Claim 21/179

#### Claim Text
Alternatively, tables can be transformed into text format for further analysis using text-based methods [75].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[85]_2307.08674.pdf (Page 0):

TableGPT: Towards Unifying Tables, Nature
Language and Commands into One GPT
Liangyu Zha1,2 Junlin Zhou1,2 Liyao Li1,2 Rui Wang1,2 Qingyi Huang3
Saisai Yang3 Jing Yuan3 Changbao Su3 Xiang Li3 Aofeng Su3 Tao Zhang3
Chen Zhou3 Kaizhe Shou Miao Wang Wufang Zhu Guoshan Lu Chao Ye
Yali Ye Wentao Ye Yiming Zhang Xinglong Deng Jie Xu
Haobo Wang4 Gang Chen4 Junbo Zhao4∗
1directional lead 2joint first author 3equal contribution 4project lead
Zhejiang University
Abstract
Tables are prevalent in real-world databases, requiring significant time and effort
for humans to analyze and manipulate. The advancements in large language models
(LLMs) have made it possible to interact with tables using natural language input,
bringing this capability closer to reality. In this paper, we present TableGPT, a
unified fine-tuned framework that enables LLMs to understand and operate on
tables using external functional commands. It introduces the capability to seam-
lessly interact with tables, enabling a wide range of functionalities such as question
answering, data manipulation (e.g., insert, delete, query, and modify operations),
data visualization, analysis report generation, and automated prediction. TableGPT
aims to provide convenience and accessibility to users by empowering them to
effortlessly leverage tabular data. At the core of TableGPT lies the novel concept
of global tabular representations, which empowers LLMs to gain a comprehensive
understanding of the entire table beyond meta-information. By jointly training
LLMs on both table and text modalities, TableGPT achieves a deep understanding
of tabular data and the ability to perform complex operations on tables through
chain-of-command instructions. Importantly, TableGPT offers the advantage of
being a self-contained system rather than relying on external API interfaces. More-
over, it supports efficient data process flow, query rejection (when appropriate)
and private deployment, enabling faster domain data fine-tuning and ensuring data
privacy, which enhances the framework’s adaptability to specific use cases.
1 Introduction
The vast and intricate world of data is often encapsulated in tables, being a foundation for data-
driven decision-making in a wide spectrum of applications, including financial analysis, supply
chain management, and healthcare analytics. It enables stakeholders to analyze trends, patterns,
and relationships, leading to informed business decisions, process improvements, and resource
optimization. For years, data scientists have struggled to process tables using complicated Excel
formulas or handcrafted programming [ 19, 20]. Consequently, there has been an urgent need to
understand and interpret tabular data in a more efficient fashion.
In the field of natural language processing, Generative Pre-trained Transformers (GPTs) [24, 25, 2, 22,
21] or Large Language Models (LLMs) [4, 36, 27, 37] have revolutionized the paradigm of language
∗Correspondence to j.zhao@zju.edu.cn.
Technical report preprint. Work in progress.
arXiv:2307.08674v3  [cs.AI]  7 Aug 2023



Source: data\tc16_2312.10997v5\referenced_papers\[85]_2307.08674.pdf (Page 1):

Table 1: Comparisons with previous command-using LLMs for tabular data. (See details in Sec 3.2)
Properties
Methods ChatExcel [28] SheetCopilot [17] Data-Copilot [38] TableGPT (ours)
Nature Language Operations ! ! ! !
Generalization to Arbitrary Tables ! ! % !
Visualization % ! ! !
Analysis & Report % % ! !
Prediction % % ! !
Chain-of-command % % ! !
Base Model Unknown API API Fine-tuned
Vague Input Rejection % % % !
Private Deployment % % % !
data mining. Following this line of works, researchers have also explored large models for various
modalities like vision [6, 13], and speech [9]. From a technical standpoint, their ability to generate
human-like text has opened new vistas of possibilities for processing tabular data. Nevertheless, it is
non-trivial to directly employ the vanilla ChatGPT [21] model in the tabular area for two reasons:
(i)-Global Table Understanding: the GPTs are known to suffer from the limited token length and
thus, they can not read a whole large table, making them hard to understand the global tabular
information. (ii)-Generalized to Tabular Domain: Second, their training processes are tailored for
natural languages and thus, they are less generalizable when handling tabular data.
There have been several works [8, 39, 18, 17] developed to integrate natural language for tabular
data analysis. NL2SQL (Nature language to SQL) [ 8, 39, 18] is a long-standing research topic
that converts natural language to SQL commands that manipulate the relational database. Recently,
SheetCopilot [17] explored languages to VBA (Visual Basic for Applications, an embedded script
language for Microsoft Excel) command such that benefit from a rich set of spreadsheet software
functionalities. However, we found that both solutions demonstrate unsatisfactory performance. We
speculate that these forms of programming code, which is fundamentally unstructured, adds another
layer of complexity, making automated post-processing almost insurmountable.
In this work, we develop TableGPT that pushes the boundaries of what is possible in data analysis
empowered by LLM techniques, marking an important step forward in our pursuit of making data
more accessible and understandable. Our TableGPT framework unifies tables, natural language, and
commands into a single GPT model, making data interpretation and manipulation more intuitive and
user-friendly. By rethinking the interaction of tables, natural language, and commands, we integrate
several core components into TableGPT:
• Global Table Representation: We make the first attempt to develop a global representation
learning paradigm for tables that encodes the whole table into one vector. By jointly training
the LLM and a table encoder on vast amounts of text and table data, we equip the encoder
to adequately capture the global information in the input table. This enables the LLM to
perceive and understand the table data effectively, thereby providing a more global and
enhanced comprehension of tables.
• Chain-of-Command: We introduce this concept to emphasize the essential idea of a
structured and hierarchical execution of tasks. Just like a well-coordinated organization
where each directive is cascaded from a higher level to its lower counterpart, TableGPT
follows a similar chain of commands, breaking down complex tasks into simpler ones
and executing them step-by-step. Moreover, it fosters the ability to refuse ambiguous or
inappropriate commands, much like an actual data scientist, instead of blindly following
any potential erroneous instruction, thereby improving the interaction between humans and
LLM systems in the field of data science. Our proposed command set is not only easier
to control but also reduces the uncertainty that often accompanies traditional methods of
handling table data.
• Domain-aware Fine-Tuning: To foster the ability to adapt to specific domains of tables and
corresponding textual materials, domain-aware fine-tuning hinges on customizing training in
a way that the model generates text embodying similar stylistic and logical elements found
in a given domain, thereby augmenting its understanding of specific domain table data. To
2



Source: data\tc16_2312.10997v5\referenced_papers\[85]_2307.08674.pdf (Page 3):

2 TableGPT
2.1 Model Design
The development of TableGPT begins with the foundation provided by pre-trained LLMs. The
advancements in the field of natural language processing have led to the development of a number
of exceptional open-source LLMs, such as LLaMa [ 27], Phoenix [ 4], ChatGLM [ 36], Ziya [ 10],
and Baichuan [12]. In designing TableGPT, we opted to use Phoenix [4] with 7B parameters as our
base model for fine-tuning, owing to its excellent capabilities in handling both Chinese and English
languages. This choice is not, however, exclusive. Our model design supports adaptation with other
LLMs, providing versatility and flexibility in its implementation.
What sets TableGPT apart from its predecessors [28, 17, 38] is the novel approach to its fine-tuning
process. We performed the fine-tuning on a vast corpus, comprising 2T tokens of textual data and
0.3M tables. This corpus offers a diverse landscape for the model to learn from, including but not
limited to user query-command sequence pairs and publicly available domain-specific data for table
analysis reports.
The overall architecture of TableGPT is shown in Figure 1. When a user inputs a table and a query,
these are received by TableGPT, which consists of a table encoder and an LLM. The table encoder
serves to extract vector representations from the input table. These representations, coupled with
the text query, are then fed into the LLM for inference. The LLM discerns the user’s query intent
and generates an output that includes both a command sequence and a textual reply. The command
sequence undergoes error correction in the command system’s corrector before it is fed into the
executor for execution. The final output, provided to the user, includes the manipulated table and
a textual reply. This streamlined process delivers efficient, reliable responses to table data queries,
enhancing user experience and simplifying data analysis.
2.2 Global Representation of Table
The rapid development of large language models (LLMs) has seen them interfacing with a multitude
of modalities such as vision, and audio. For instance, the integration of vision and LLMs has led to
models like CLIP [23] (Contrastive Language–Image Pretraining) from OpenAI that connects images
and text through shared latent space. The combination of audio and LLMs gave rise to models like
Wave2Vec [1] and Tacotron [32] that employ the representation of audio in the form of spectrograms
to generate or understand speech.
Despite these advancements, the exploration of LLMs interfacing with tabular data remains limited.
The question of how to enable LLMs to comprehend and interpret tables is essential. Some studies
have attempted to convert sample rows of table data directly into a sentence-like text description [7],
while others have attempted to artificially define a global representation of table data through the
template-based extraction of column names, industry background, and other metadata schema [38].
However, these approaches only extract partial information from table data for LLMs, consequently
overlooking the global information and industry background inherent in the data.
Notably, for the tables, it is required to embed the whole table into one single vector, instead of
generating sample-wise embedding. This can be non-trivial and challenging because, unlike images,
videos, and audio, table data is inherently a highly abstract structured data type. Furthermore, it
possesses a dual permutation invariance structure where shuffling rows or columns does not affect the
information contained within the table, a distinct contrast to images and audio, which carry inductive
bias in adjacent positions or sequences. Moreover, tables from different domains vary in size and
format, such as having different numbers of discrete and continuous columns, making it challenging
to extract features from diverse tables using a unified neural network architecture [34].
Yet, it remains an open problem to effectively extract global representations from tables for LLMs to
achieve comprehensive table understanding. To this end, we present a Cascaded Table Encoder that
jointly extracts knowledge from metadata and whole numerical entries.
Cascaded Table Encoder. Consider the approach of an experienced data scientist encountering a
table. They typically examine the structure of the table data, such as the table headers and distribution
of feature columns, to understand the meaning of different cells based on their position, without
focusing too much on the numeric information of each cell. Following this biologically plausible
4



Source: data\tc16_2312.10997v5\referenced_papers\[85]_2307.08674.pdf (Page 6):

3.3 Case Study
We show some cases in Figure 2 - 8. More examples will be released soon.
4 Conclusion
We present TableGPT, a large language model designed for table analysis, unifying tables, nature
language, and commands. It enables a variety of functions like answering questions, manipulating
data, visualizing information, generating analysis reports, and making predictions. Technically,
TableGPT addresses several major challenges in developing a natural language-driven framework for
table data processing, including comprehensive table understanding, instruction chain generation,
and domain-specific fine-tuning. We believe TableGPT has the potential to reshape the landscape of
tabular data processing, accelerating the efficiency of table modeling and exploratory data analysis
(EDA), and empowering various domains like finance, transportation, scientific research, etc.
Figure 2: Cases of TableGPT.
7



Source: data\tc16_2312.10997v5\referenced_papers\[85]_2307.08674.pdf (Page 2):

TableGPT
Housing_price.csv
Table 
Encoder
LLM111
111GPT
User query: How 
house prices have 
changed by region 
in the last decade?
Answer: According to the 
table, the average house 
price in each region has 
gradually increased over 
the past decade, with the 
largest increase in City A...
{
"type": "commands", // "text" or "commands"
"value": {
"commands": [
"SelectCondition",
"GroupBy“
],
"commands_args": [
{
"columns": ["Year"],
"index": [],
"range": [2013, 2023],
"condition": "range",
"slice": "no",
"type": "column",
"relation": "none"
},
{
"by": ["Region", "Year"],
"aggregate_args": {"Price": ["mean"]}
}
]
}
}
Command System
CorrectorExecutor
Generated Table
{
"type": "text", // "text" or "commands"
"value": "According to the table, the average 
house price in each region has gradually increased 
over the past decade, with the largest increase in 
City A..."
}
Commands Set
• InsertCondition
• DeleteCondition
• SelectCondition
• StatisticAnalysis
• SortCondition
• GroupBy
• UnaryTransform
• BinaryTransform
• Visualization
• Prediction
• ……
Figure 1: An architecture of TableGPT framework.
make this approach scalable and feasible, we have also developed a data processing pipeline
that yields notable improvements with only a small amount of data, hence alleviating the
resource-demanding aspect of training LLMs and supporting private deployment.
From a real-world production standpoint, the unstructured code outputted by NL2SQL poses sig-
nificant challenges for preemptive checks and error corrections. Hence, we advocate for the use
of structured command sequences, simplifying post-processing. Data-Copilot [ 38] also embraces
this command-based approach with self-instruct [31], but its reliance on API-called native LLMs
to comprehend tabular data’s processing and analysis logic directly presents limitations. Given the
intrinsic data variability and task-specificity of tabular data, we believe an effective product should be
custom-built for tabular data while maintaining general applicability to broader downstream tasks.
This conviction underscores the imperative of introducing a LLM specifically pre-trained for tabular
data.
To sum up, this work presents a pioneering TableGPT framework, which is a unified, well-fledged
holistic solution, enabling efficient tabular data processing, analysis and visualization, driven all by
natural languages. We summarize several important advantages of TableGPT as follows:
• Language-driven EDA: TableGPT understands user intent from natural language, dissects
the desired actions, and executes external commands on the table. It subsequently returns
the processed results in both tabular and textual explanations to the user. This novel
approach simplifies the way users engage with table data, bringing an intuitive instantiation
to Exploratory Data Analysis (EDA).
• Unified Cross-modal Framework: Innovatively, we devise a global table encoder for
understanding the whole table. TableGPT is able to fully understand the user query, meta-
knowledge, and whole tabular data, which leads to much more reliable execution commands
for table manipulation.
• Generalization and Privacy: By domain-aware fine-tuning, our TableGPT can better
handle data variability of tables and generalize to different domains. Further, our framework
supports private deployment, offering robust data privacy protections. This aspect is critical
in the modern age where data privacy and protection are just paramount.
3



### Claim 22/179

#### Claim Text
KnowledGPT [15] generates KB search queries and stores knowledge in a personalized base, enhancing the RAG model’s knowledge richness.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[15]_2308.11761.pdf (Page 3):

Method Ent Desc Triples Ent Aspect Info Multi-Hop Search Zero-Shot Ent Linking External KBs Private KB
Toolformer ✓ ✗ ✗ ✓ ✗ ✗ ✓ ✗
ToolkenGPT ✗ ✓ ✗ ✓ ✗ ✗ ✓ ✗
Graph-Toolformer ✗ ✓ ✗ ✗ ✗ ✗ ✓ ✗
RET-LLM ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✓
LangChain KG Memory✗ ✓ ✗ ✗ ✓ ✓ ✗ ✓
Llama-Index KG Index✗ ✓ ✗ ✗ ✓ ✓ ✗ ✓
KnowledGPT ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Table 1: Comparison between KnowledGPT and existing KB-augmented methods. Ent, Rel, and Desc are
abbreviations for entity, relation and description, respectively.
3 Methods
In this section, we introduce KnowledGPT, a com-
prehensive framework to integrate LLMs with KBs.
We first provide the definition of two tasks of
KnowledGPT, knowledge retrieval and knowledge
storage (Sec 3.1). Then, we elaborate the details in
the retrieval (Sec 3.2) and storage (Sec 3.3) process
of KnowledGPT.
3.1 Task Definition
KnowledGPT supplements LLMs with external
knowledge from various knowledge bases (KBs),
including a personalized KB (PKB) as an writable
symbolic memory. Given a user input in natural
language, KnowledGPT undertakes two primary
tasks, namely knowledge retrieval and knowledge
storage. In the knowledge retrieval task, the model
searches through provided KBs to retrieve relevant
knowledge to answer the user query. In the knowl-
edge storage task, the model extracts knowledge
from the user input and inserts it into the PKB.
3.2 Knowledge Retrieval
KnowledGPT follows a three-step process to an-
swer user queries with knowledge from KBs, as
shown in Fig 3. First, it generates a piece of search
code as a logical form for query-specific KB ac-
cess. Then, the search code is executed to retrieve
related knowledge. Finally, KnowledGPT reads the
retrieved knowledge and answers the query.
We utilize the program of thought (PoT)prompt-
ing approach (Chen et al., 2022) , which adopts
Python code as the search language generated by
LLMs. In this paper, we useGPT-4 (OpenAI, 2023)
as the LLM. The code is encapsulated in a search
function, as is shown in the yellow part of Fig 3 ,
which includes built-in Python functions and three
custom KB functions designed to facilitate the in-
teraction of LLMs with KBs:
1. get_entity_info, which accepts an entity as
input and returns its encyclopedic description.
2. find_entity_or_value, which accepts a query
consisting of an entity and a relation as input,
and outputs a list of the corresponding entity
or value.
3. find_relationship, which accepts two entities
as input, and returns a list of their relationship.
Specially, each entity or relation is represented as a
list of candidate aliases, rather than a single name,
to effectively handle synonyms. Besides the out-
puts stated above, these KB functions also return
a message which logs the function call and result.
Then, the overall output of the search function is
obtained by concatenating the messages from indi-
vidual KB function calls. The prompt is shown in
7.
The search function is then executed to retrieve
the expected knowledge from KBs. The code
would be decorated before execution, e.g. with
a try-except statement and KB-specific accessor
object, as is elaborated in Sec 3.2.1. The search
function is executed for each KB respectively in
parallel, and their results are concatenated.
Finally, the retrieved knowledge is provided to
LLMs, and LLMs are tasked with responding to the
user’s query, supported by the retrieved knowledge.
The prompt is shown in Sec 7. LLMs will ignore
the retrieved information and address the user query
independently in scenarios where LLMs judge the
question does not require external knowledge or the
retrieved knowledge is not enough for the query.
3.2.1 Code Implementation
Next, we introduce the implementation of the KB
functions to execute the generated code. We im-
plement the functions at two levels: a unified level
and a KB-specific level.
Functions at the unified level provide a unified
interface for operations over different KBs. These
include the three KB functions ( get_entity_info,
find_entity_or_value, find_relationship) generated



Source: data\tc16_2312.10997v5\referenced_papers\[15]_2308.11761.pdf (Page 2):

Complex Question
Please recommend three novels written by 
the author of 'Lord of the Mysteries’.
Previous Methods like RET-LLM
one step only
RETRIEVE(‘LotM’, ‘novels by the author of’, )
KnowledGPT
multi-hop searching &  processing via code
CODE:
author = RETRIEVE([‘LotM’], ['written by’, 
‘author’])
novels = RETRIEVE(author, [‘write’])
novels = SAMPLE(novels, min(3, len(novels))
Ambiguation
Who directed the film Titanic?
LLM-based Entity Linking
Knowledge Representations
U.S. Bancorp is an American bank holding 
company based in Minneapolis, Minnesota… 
embedding similarity
cosine(Titanic, Titanic (a ship) ) = 0.95
cosine(Titanic, Titanic (1997 film) ) = 0.92
Link to the entity Titanic (a ship) ❎
CANDIDATE ENTS: 
Titanic (a ship): a famous ship that sank in 1912
Titanic (1997 film): a 1997 film directed by 
James Cameron …
LLM CHOICE: 
Titanic (1997 film) seems related with the film 
Titanic . Hence, I choose Titanic (1997 film). 
Extract triples:
(‘U.S. Bancorp’, ‘based in’, ‘Minnesota’)
‘U.S. Bancorp’: ‘U.S. Bancorp, a bank holding 
company based in Minneapolis, Minnesota …
(‘U.S.Bancorp’, ‘scope of business’, ‘The company 
provides banking, investment, mortgage, trust, 
and payment services products to individuals … ‘)
only triples
⚠️Link to a wrong entity 
+ entity description
⚠️generate contrived relation
✅
++ entity-aspect information
⚠️Represent a limited portion of knowledge
Figure 2: Comparison between KnowledGPT and previous methods towards several challenges in bridging LLMs
with KBs. KnowledGPT handles complex questions through multi-hop searching and processing based on code,
tackles entity ambiguation with LLM-based entity linking (middle), and provides extended forms of knowledge
representations to encapsulate a broader range of knowledge from the provided text.
2020), RAG (Lewis et al., 2020), augment LLMs
with document corpus, which have also been in-
creasingly adopted by recent popular LLMs like
ChatGPT as the memory unit (Liu, 2022) (Chase,
2022). ChatDB (Hu et al., 2023) augments LLMs
with databases as symbolic memory.
Knowledge Bases for LLMs Some recent works
have studied to augment LLMs with knowledge
from external KBs or use KBs as symbolic memo-
ries, usually by making LLMs generate API calls
for KB operations. Toolformer (Schick et al., 2023)
trains LLMs to search Wikipedia for texts of enti-
ties. Graph-Toolformer (Zhang, 2023) empowers
LLMs to reason over knowledge graphs. How-
ever, it skips the entity linking step, so it requires
entity id like /m/053yx as input, instead of their
names. ToolkenGPT (Hao et al., 2023) keeps the
LLMs frozen and trains tool embeddings for rela-
tions in KBs to support relational queries. RET-
LLM (Modarressi et al., 2023), similar to the KG
memory of LangChain (Chase, 2022) and Llama-
Index (Liu, 2022), extracts relational triples from
user inputs and store them in a symbolic KG mem-
ory. Compared with previous efforts, KnowledGPT
supports various knowledge representations and
both public and private KBs, as shown in Table 1.
Knowledge-based Question Answering
(KBQA) is to search for answer entities or
relations given natural language queries specific to
certain KGs. Existing KBQA systems are mainly
based on semantic parsing (Berant et al., 2013) or
information extraction (Yao and Van Durme, 2014),
where language models are increasingly involved.
Semantic parsing methods (Yu et al., 2023; Cao
et al., 2022; Zhang et al., 2022b; Abdelaziz et al.,
2021; Lai et al., 2016) leverage semantic parser to
convert natural language queries into intermediate
logic forms such as SPARQL (Prud’hommeaux,
2011) and program (Liang et al., 2016), which
are executed on KBs to obtain the answers.
However, the generated logic forms are usually
non-executable, thus failing to arrive at the correct
answer (Sun et al., 2020). Pangu (Gu et al., 2022)
trains a language model discriminator to evaluate
probability of candidate plans. Information
extraction methods usually combines retrieval
and reasoning (Zhang et al., 2022a; Shi et al.,
2021; Sun et al., 2019; Jiang et al., 2023; Baek
et al., 2023). These methods show effectiveness
in handling single-hop retrieval. However, they
encounter challenges with multi-hop retrieval
concerning storage and computation costs, where
the number of relations expands exponentially
with each added hop.
KnowledGPT differs from KBQA methods in
two aspects. First, many KBQA methods are de-
signed for special queries about relational triples
in KGs, while KnowledGPT augments LLMs to
respond to various user queries with knowledge in
various forms from KBs. Second, KBQA methods
are typically trained on specific datasets and KGs,
whereas KnowledGPT requires no training and can
easily accommodate different LLMs and KBs.



Source: data\tc16_2312.10997v5\referenced_papers\[15]_2308.11761.pdf (Page 9):

Input Text:
Socrates (470–399 BC) was a Greek philosopher from Athens who is 
credited as the founder of Western philosophy and among the first moral 
philosophers of the ethical tradition of thought. An enigmatic figure, 
Socrates authored no texts and is known mainly through the posthumous 
accounts of classical writers, particularly his students Plato and Xenophon...
Query: 
Why was Socrates accused ?
Answer with Retrieved Knowledge: 
Socrates was accused of impiety and corrupting the youth. 
Retrieved Knowledge from KBs:
[FROM Memory][find_entity_or_value(entity_aliases = ['Socrates'], 
relation_aliases = ['accused for', 'charged with', 'accusation']) -> ] Socrates, 
accused of: impiety and corrupting the youth
Extracted Knowledge: 
Entity Description: {
“Socrates”: “Socrates (c. 470–399 BC) was a Greek philosopher from 
Athens who is credited as the founder of Western philosophy and among 
the first moral philosophers of the ethical tradition of thought …”
}
Relation Triples: {
“Socrates”: [
("Socrates", "was a", "Greek philosopher“), 
("Socrates", "credited as", "founder of Western philosophy“), 
("Socrates", "accused of", "impiety and corrupting the youth“), 
("Socrates", "known for", "proclaiming his total ignorance“) 
…
]}
Entity Aspect Information: {
“Socrates”: [
("Socrates", “philosophy", “…Socrates is known for proclaiming 
his total ignorance; he used to say that the only thing he was aware of was 
his ignorance…“), 
("Socrates", "trial and death", "In 399 BC, he was accused of 
impiety and corrupting the youth. After a trial that lasted a day, he was 
sentenced to death. He spent his last day in prison, refusing offers to help 
him escape. “), 
("Socrates", “influence", "Socrates exerted a strong influence on 
philosophers in later antiquity and has continued to do so in the modern 
era… “), 
…
]}
Figure 6: An example showing knowledge extraction
and retrieval of KnowledGPT on the personalized KB.
born?”. Each question is paired with 10 documents,
and KnowledGPT extracts knowledge from these
documents to build the PKB.
The results are shown in Table 4. KnowledGPT
successfully answer all comparison questions, and
15 out of 20 bridge questions. Among the failed
cases, two bridge questions failed to extract the
knowledge needed to answer the question and one
bridge question failed in the entity linking stage.
Overall, this experiment suggests the promising
future of utilizing PKBs as symbolic memory of
LLMs.
Comparison Bridge All
KnowledGPT 5/5 15/20 20/25
Table 4: Number of successful answers on 25 questions
selected from HotPotQA. The results are evaluated by
human annotators.
We further investigate the knowledge extraction
coverage of KnowledGPT on 100 documents from
HotpotQA (Yang et al., 2018), considering various
LLMs and diverse combination of applied knowl-
edge representations. To quantify the coverage, we
employ the word recall rate, calculated as
|Wextracted ∩ Wdoc|
|Wdoc| , (1)
where | · |indicates the cardinality of a set.
Wextracted and Wdoc denote the set of words in
the extracted knowledge and the document respec-
tively, after preprocessing including the removal of
stop words and lemmatization, utilizing the NLTK
toolkit (Bird and Klein, 2009).
The results are shown in Table 5, from which
we have the following observations: (1) When re-
stricting knowledge representation solely to triples,
the extraction coverage stands at 0.53, which indi-
cates that only a limited portion of knowledge can
be represented as triples. Therefore, a PKB sup-
porting triples alone falls short of adequately en-
compassing the knowledge provided by real users.
(2) With additional knowledge representations, i.e.,
entity description and entity-aspect information,
we observe a marked improvement in knowledge
extraction coverage, suggesting that incorporating
entity description and entity-aspect information en-
ables KnowledGPT to populate the PKB with a
broader spectrum of knowledge. (3) ChatGPT and
GPT-4 achieve similar proficiency for knowledge
extraction. GPT-4 outperforms ChatGPT only when
entity-aspect info is included, which probably is
attributed to GPT-4’s enhanced capability at follow-
ing complex instructions.
w/ triples only + entity desc ++ entity aspect info
ChatGPT 0.53 0.66 0.81
GPT-4 0.53 0.62 0.86
Table 5: Knowledge extraction coverage of Knowl-
edGPT on 100 documents from HotpotQA with dif-
ferent LLMs and various combination of applied knowl-
edge representations.
5 Limitations
While KnowledGPT enables LLMs to effectively
perform KB operations on external knowledge
bases, there remain several limitations in its current
form. First, the retrieval process entails a single-
round of code generation and execution for effi-
ciency concerns. However, a multi-round mecha-



Source: data\tc16_2312.10997v5\referenced_papers\[15]_2308.11761.pdf (Page 8):

posed of 100 samples from the test set of NLPCC
2016 KBQA dataset (Duan, 2016), while NLPCC-
MH-59 consists of 59 samples from the test set of
NLPCC-MH (Wang and Zhang, 2019), a multi-hop
KBQA dataset. NLPCC-MH is automatically con-
structed by expanding the NLPCC 2016 dataset,
which leads to certain inherent noise. From the
NLPCC-MH dataset, we manually select 59 sam-
ples, ensuring their quality and the presence of
supporting fact chains in the KB. Furthermore, we
rectify any noise present in these samples. For both
NLPCC-100 and NLPCC-MH-59, we use exclu-
sively the full NLPCC 2016 KBQA Knowledge
Base in this experiment.
We make several modification to KnowledGPT
towards this dataset and KB. First, in the provided
KB, the tail entities of triples may contain multiple
entities separated by special symbols, so we adjust
the prompt for search code generation to request
LLMs to include a splitting mechanism in the gen-
erated code. Second, for better entity linking, as the
provided KB does not contain entity description,
we modify the implementation of _get_entity_in-
formation to return 10 triples related to the entity,
sorted by the jaccard similarity between the query
relation and the relation in triples. Third, we also
modify the entity linking prompt, requiring LLMs
to also adjust the query relation alias to better align
with relations in the KB.
We compare KnowledGPT with the following
baseline methods :
1. Retrieval via embedding similarity. Each
triple is treated as a document and embedded
using the CoSENT model (Ming, 2022). A
single document is retrieved based on embed-
ding similarity for each search. For multi-hop
questions, the result from the first retrieval
is added to the query to facilitate the second
retrieval.
2. Retrieval via BM25 (Robertson et al., 1995).
For each entity, we group all its triples as a
document. The most relevant document is
retrieved using the BM25 algorithm for each
search query, with stop words removed. We re-
gard it as successful if the retrieved document
contains the corresponding triple. For multi-
hop queries, we pick a triple from the initial
retrieval’s document based on the jaccard
similarity between relations, and integrate this
triple into the subsequent retrieval’s query.
3. SPE (Lai et al., 2016). SPE extracts subject-
predicate pairs from simple questions using
embedding similarity. It obtaied the first place
in the contest of NLPCC 2016 KBQA task.
We report averaged F1 on NLPCC-100 and
NLPCC-MH-59. Averaged F1 is a widely adopted
metric for KBQA, designed for tasks with multiple
golden answers and predictions. However, since in
our dataset there is only one answer and one predic-
tion for each sample, so the averaged F1 actually is
equivalent to accuracy.
The results are shown in Table 3, from which we
have the following observation: (1) For single-hop
queries, KnowledGPT significantly outperforms re-
trieval methods via BM25 and embedding similar-
ity, which shows the effectiveness of retrieval from
symbolic KBs compared with document corpus, for
questions related to knowledge in KBs. (2) Zero-
shot KnowledGPT outperforms the SPE method
trained on the full training set of NLPCC-2016
KBQA dataset (0.92 vs 0.85), which shows the
strong zero-shot performance of KnowledGPT. (3)
For multi-hop queries, KnowledGPT also achieves
excellent performance, while the performance of
retrieval methods based on BM25 and embedding
similarity drops significantly.
Dataset BM25 Embedding Similarity SPE KnowledGPT
NLPCC-100 0.71 0.31 0.85 0.92
NLPCC-MH-59 0.44 0.19 - 0.93
Table 3: Averaged F1 of different methods on NLPCC-
100 and NLPCC-MH-59.
4.4 KB as Memory
We conduct experiments to study the effectiveness
of KnowledGPT when paired with a modifiable per-
sonalized KB. KnowledGPT is tasked with building
the PKB by extracting knowledge from provided
documents, and study whether KnowledGPT an-
swer corresponding questions properly with the
PKB. Fig 6 shows an example where text about
Socrates is provided. KnowledGPT extracts knowl-
edge from the text, and answers related questions
by retrieving the extracted knowledge.
We further apply KnowledGPT to Hot-
potQA (Yang et al., 2018), a multi-hop question
answering dataset with provided documents. We
select 25 questions from HotpotQA dev set (distrac-
tor), including 5 comparison questions like “Which
is a shrub, Mimosa or Cryptocoryne?” and 20
bridge questions like “When was Erik Watts’ father



Source: data\tc16_2312.10997v5\referenced_papers\[15]_2308.11761.pdf (Page 1):

edge like relational triples and entity descriptions.
On one hand, various KBs have been constructed
for their practical effectiveness for applications,
and the conciseness, expressiveness, interpretabil-
ity and visibility of their representation. On the
other hand, previous approaches have largely fo-
cused on document corpus, which reveals several
deficiency when applied to KGs, as shown in Fig 1.
Therefore, connecting LLMs with KBs is of signif-
icant importance, yet still remains underexplored.
Recently, several works have attempted to con-
nect LLMs to KBs. Toolformer (Schick et al.,
2023) queries Wikipedia for descriptions of inter-
ested entities to answer related questions. Graph-
Toolformer (Zhang, 2023) and ToolkenGPT (Hao
et al., 2023) make LLMs reason over knowledge
graphs like Freebase. RET-LLM (Modarressi et al.,
2023) builds personalized KG memory with rela-
tional triples extracted from past conversations for
future use, in parallel with practical efforts of KG
Index in LangChain (Chase, 2022) and Llama In-
dex (Liu, 2022).
However, there are still many challenges in this
direction, as shown in Figure 2. Firstly, the pro-
cess by which LLMs navigate through knowledge
bases for intricate and varied questions remains a
problem, especially for multi-hop questions which
requires information across multiple and nested
entries in KBs. Secondly, aligning entities and
relations in knowledge bases with their text men-
tions is a challenging task, as they need to map to
a wide spectrum of natural language expressions
and account for severe ambiguation in the knowl-
edge bases. Thirdly, while the triple-based repre-
sentation in KGs is neat and interpretable, it only
covers limited information compared with natural
language, which suggests the need for new repre-
sentation form in KBs for LLMs.
In this paper, we propose a comprehensive frame-
work, KnowledGPT, to connect LLMs to various
knowledge bases effectively, with an improved ca-
pability in dealing with complex questions, am-
biguation and knowledge representation. Knowl-
edGPT implements a unified accessor interface
for operations on different KBs, including widely-
used public KBs and personalized KB memories.
KnowledGPT accesses entity-oriented knowledge,
including both entity descriptions and relational
triples. For a given query, KnowledGPT searches
KBs with three steps: search code generation,
search execution, and answer generation. Inspired
by (Chen et al., 2022), KnowledGPT adopts pro-
gram of thoughts (PoT)prompting, interacting with
KBs by generating Python code which delegates
searching steps and executing it. This code encap-
sulates functions for assessing KBs such as entity_-
linking. Afterwards, KnowledGPT integrates the
retrieved knowledge to generate the response. If
KnowledGPT judges that the question does not ne-
cessitate knowledge from KBs, or if the retrieved
knowledge is inadequate or absent, the question
will be directly answered by the LLM. Besides,
KnowledGPT can also extract knowledge from un-
structured texts represented in various forms to
enrich the personalized KB.
Overall, our contributions are summarized as
follows:
1. We propose KnowledGPT, a comprehensive
framework to enable LLMs to retrieve knowl-
edge from knowledge bases. It significantly
advances the collaboration between LLMs
and KBs towards vital practical challenges
like complex searching and ambiguation.
2. We propose the use of personalized knowl-
edge bases as symbolic memory for LLMs,
encapsulating entity-oriented knowledge in
three forms of representations. This expands
the scope of knowledge in symbolic memories
compared with triples-only KBs.
3. We demonstrate the efficacy of our proposed
methods with experiments. The results under-
score the utility and potential of using KBs as
symbolic memory for LLMs.
2 Related Works
External Knowledge and Memory for LLMs
Large language models (LLMs), such as GPT-
4 (OpenAI, 2023) and LLaMA (Touvron et al.,
2023), have demonstrated impressive performance
across various applications. However, they still
struggle with knowledge considering completeness,
timeliness, faithfulness and adaptability. Hence,
many recent efforts have been devoted to equip-
ping LLMs with external knowledge. Internet-
augmented language models (Komeili et al.,
2022) (Lazaridou et al., 2022), as well as New
Bing and ChatGPT “Browse with Bing” plugin,
allow LLMs to access up-to-date information
with search engines or web browsers. Retrieval-
augmented methods like REALM (Guu et al.,



### Claim 23/179

#### Claim Text
In response to the limitations of LLMs in understanding and answering questions about textual graphs, G-Retriever [84] integrates Graph Neural Networks 4https://hotpotqa.github.io/wiki-readme.html 5https://github.com/facebookresearch/DPR (GNNs), LLMs and RAG, enhancing graph comprehension and question-answering capabilities through soft prompting of the LLM, and employs the Prize-Collecting Steiner Tree (PCST) optimization problem for targeted graph retrieval.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[84]_2402.07630.pdf (Page 5):

Graph Encoder
Question: What is the name
of justin bieber brother?
Step 1: Indexing
Storage
Step 2: Retrieval Step 3: Subgraph Construction
LLM
(Self Attention Layers)
LLM
(Text Embedder)
Step 4: Generation
bieberjaxon
Projection
justin
bieber
jeremy
bieberparent
jaxon
bieber
children
sibling
m.0gxnnwp
sibling
all bad
album
record
producer
profession
canada
nationality
male
gender
LM
node attributes
justin bieber, this is justin bieber, jeremy bieber, 
justin bieber fan club, justin ...
sibling, sibling_s, hangout, friendship, friend ...
LM
LM
justin
bieber
jeremy
bieberparent
jaxon
bieber
children
siblingm.0gxnnwp
sibling
node_id, node_attr
15, justin bieber
294, jaxon bieber
356, jeremy bieber
551, m.0gxnnwp
src, edge_attr, dst
294, parents, 356
356, children, 15
551, sibling, 294
551, sibling, 15
Question: What is the name
of justin bieber brother?
Answer:
edge attributes
frozen
trainable
Figure 3: Overview of the proposed G-Retriever: 1) Indexing: Graphs are indexed for efficient query
processing; 2) Retrieval: The most semantically relevant nodes and edges are retrieved, conditioned
on the query; 3) Subgraph Construction: A connected subgraph is extracted, covering as many
relevant nodes and edges as possible while maintaining a manageable graph size; 4) Generation: An
answer is generated using a ‘graph prompt’, a textualized graph, and the query.
require multi-hop reasoning. Given the possibility of multiple answers for the same question, the
hit@1 metric is used to assess the precision of the top returned answer.
5 G-Retriever
In this section, we introduce G-Retriever, a new architecture tailored for GraphQA, which integrates
the strengths of GNNs, LLMs, and RAG. To allow efficient fine-tuning while preserving the LLM’s
pretrained language capabilities, we freeze the LLM and use a soft prompting approach on the output
of the GNN. Our RAG-based design mitigates hallucinations through direct retrieval of the graph,
while allowing our approach to scale to graphs exceeding the LLM’s context window size. To adapt
RAG to graphs, we formulate subgraph retrieval as a PCST optimization problem. This approach
also allows us to enhance explainability by returning the retrieved subgraph.
G-Retriever comprises four main steps: indexing, retrieval, subgraph construction and generation, as
depicted in Figure 3. The implementation details of each step are elaborated in the following sections.
5.1 Indexing
We initiate the RAG approach by generating node and graph embeddings using a pre-trained LM.
These embeddings are then stored in a nearest neighbor data structure.
To elaborate, consider xn ∈ DLn as the text attributes of node n. Utilizing a pre-trained LM, such as
SentenceBert [34], we apply the LM to xn, yielding the representation zn:
zn = LM(xn) ∈ Rd, (3)
where d denotes the dimension of the output vector. Similar preprocessing steps are applied to edges.
Refer to Figure 3, Step 1 for an illustrative representation.
5.2 Retrieval
For retrieval, we employ the same encoding strategy to the query xq, to ensure consistent treatment
of textual information:
zq = LM(xq) ∈ Rd. (4)
Next, to identify the most relevant nodes and edges for the current query, we use a k-nearest neighbors
retrieval approach. This method yields a set of ‘relevant nodes/edges’ based on the similarity between
the query and each node or edge. The retrieval operation is defined as:
Vk = argtopkn∈V cos(zq, zn)
Ek = argtopke∈E cos(zq, ze), (5)
6



Source: data\tc16_2312.10997v5\referenced_papers\[84]_2402.07630.pdf (Page 0):

G-Retriever: Retrieval-Augmented Generation for
Textual Graph Understanding and
Question Answering
Xiaoxin He1 Yijun Tian2 Yifei Sun1 Nitesh V . Chawla2 Thomas Laurent3
Yann LeCun4,5 Xavier Bresson1 Bryan Hooi1
{xiaoxin, yifeisun, xaviercs, bhooi}@comp.nus.edu.sg
{yijun.tian, nchawla}@nd.edu, tlaurent@lmu.edu, yann@cs.nyu.edu
1National University of Singapore 2University of Notre Dame 3Loyola Marymount University
4New York University 5Meta AI
Abstract
Given a graph with textual attributes, we enable users to ‘chat with their graph’:
that is, to ask questions about the graph using a conversational interface. In re-
sponse to a user’s questions, our method provides textual replies and highlights the
relevant parts of the graph. While existing works integrate large language models
(LLMs) and graph neural networks (GNNs) in various ways, they mostly focus
on either conventional graph tasks (such as node, edge, and graph classification),
or on answering simple graph queries on small or synthetic graphs. In contrast,
we develop a flexible question-answering framework targeting real-world textual
graphs, applicable to multiple applications including scene graph understanding,
common sense reasoning, and knowledge graph reasoning. Toward this goal, we
first develop a Graph Question Answering (GraphQA) benchmark with data col-
lected from different tasks. Then, we propose our G-Retriever method, introducing
the first retrieval-augmented generation (RAG) approach for general textual graphs,
which can be fine-tuned to enhance graph understanding via soft prompting. To
resist hallucination and to allow for textual graphs that greatly exceed the LLM’s
context window size, G-Retriever performs RAG over a graph by formulating this
task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations
show that our method outperforms baselines on textual graph tasks from multiple
domains, scales well with larger graph sizes, and mitigates hallucination. 1
1 Introduction
Graphs and Large Language Models (LLMs). The advent of LLMs has significantly shaped the
artificial intelligence landscape. As these models are applied to increasingly diverse tasks, their ability
to process complex structured data will be increasingly vital. In particular, in our interconnected
world, a significant portion of real-world data inherently possesses a graph structure, such as the
Web, e-commerce, recommendation systems, knowledge graphs, and many others. Moreover, many
of these involve graphs with textual attributes ( i.e., textual graphs), making them well-suited for
LLM-centric methods. This has spurred interest in combining graph-based technologies, particularly
graph neural networks (GNNs), with LLMs to enhance their reasoning on graphs [44, 15, 24].
The Present Work: Enabling ‘Chat With Your Graph’. While existing works integrate LLMs and
GNNs in various ways, they mostly focus on conventional graph tasks such as node, edge and graph
1Our codes and datasets are available at: https://github.com/XiaoxinHe/G-Retriever
arXiv:2402.07630v3  [cs.LG]  27 May 2024



Source: data\tc16_2312.10997v5\referenced_papers\[84]_2402.07630.pdf (Page 18):

E Discussion on the Complexity
E.1 The integration of GNNs, LLMs and GraphRAG
G-Retriever is framework integrate the strengths of GNNs, LLMs and GraphRAG. The LLM+X
framework, which involves enriching LLMs with multi-modal capabilities by integrating an LLM
with an encoder from another modality, is a widely adopted approach. Notable examples include
Llava, MiniGPT-4, and Flamingo, among others. They are not complex in terms of understanding
or implementation. Regarding the integration of GraphRAG, it does not require training and can
be implemented during the preprocessing stage or on the fly. This approach does not significantly
increase time complexity or computational complexity. On the contrary, it can substantially reduce
the size of the graph (e.g., eliminating 99% of nodes in the WebQSP dataset), which in turn speeds
up the overall running time (e.g., reducing it from 18.7 min/epoch to 6.2 min/epoch on the WebQSP
dataset).
E.2 Computational Resources
Utilizing two A100 GPUs, each with 80GB of memory, we conducted tests on Llama2-7b and
WebQSP datasets. Our experiments had a training batch size of 16 and an evaluation batch size of 32,
yielding the following results.
Table 12: Performance and Efficiency of Various Methods on theWebQSP dataset.
Settting Method Hit@1 Time
Inference-only Question only 61.16 31 min
Textual graph and question 41.06 40 min
Frozen LLM w/ PT Prompt Tuning 48.34 18.7 min/epoch
G-Retriever 70.49 6.2 min/epoch
Tuned LLM LoRA 66.03 19 min/epoch
G-Retriever w/ LoRA 73.79 6.9 min/epoch
These results highlight efficiency improvements via graph RAG, which significantly reduces graph
size (e.g., eliminating 99% of nodes in the WebQSP dataset) and speeds up running time.
F Hallucination in Graph LLMs
In this section, we present quantitative results regarding hallucinations in the SceneGraphs dataset.
Baseline. For our baseline, we adapted MiniGPT-4 [57] to graph contexts. This approach involves a
frozen LLM interacting with a trainable GNN that encodes graph data as a soft prompt, denoted as
LLM+Graph Prompt Tuning. We focus on graph prompt tuning as the baseline, instead of converting
the graph into text, since the textual representation of the graph is large and consistently exceeds the
input token limits of LLMs.
Experiment Design. We instructed the LLM to answer graph-related questions and to list nodes or
edges in the explanation graph that support its answers. Since standard answers for these questions
do not exist, allowing the LLM to respond flexibly, it becomes challenging to evaluate its responses.
To address this, we manually examined 100 responses generated by our method and the LLM with
graph prompt tuning, verifying whether the nodes and edges referenced in the LLM’s output actually
exist in the graph.
Evaluation Metrics. We assessed the model’s faithfulness using three metrics: the fraction of valid
nodes (denoted as Valid Nodes), the fraction of valid edges (denoted as Valid Edges), and the fraction
of times the entire set of nodes and edges cited was valid (denoted as Fully Valid Graphs).
Results. The results, as depicted in Table 5, illustrate the comparative effectiveness of theG-Retriever
over the baseline LLM+Graph Prompt Tuning method in reducing hallucinations. The LLM+Graph
Prompt Tuning approach demonstrated a significantly lower accuracy in referencing graph elements,
19



Source: data\tc16_2312.10997v5\referenced_papers\[84]_2402.07630.pdf (Page 1):

New Benchmark
GraphQA
New Architecture
G-Retriever
Techniques
Retrieval Augmented Generation
Graph Prompt Tuning
Graph Textualization
Sure! Here's an example of a 500-word advertisement
for the place based on the scene graph:
"Experience the breathtaking beauty of our place,
where the rolling hills meet the sparkling water of the
swimming pool. Take a leisurely stroll through the
lush green  elds, and let the gentle breeze caress
your face as you gaze out at the picturesque view. The
sheep graze peacefully in the distance, their white
coats glistening in the sunlight. The sky is a brilliant
blue, with  u y white clouds drifting lazily across.
Come and bask in the serene ambiance of our place,
where the natural beauty will leave you in awe."
Unified Conversational Interface
grazing on
above
eating
grass
water
moutains
covered
Scene Graph
sky
blue, clear, cloudly
sheep
white,  u y
Knowledge Graph
benjamin franklin
lighting rod
glass harmonica
franklin
stoveinventions
inventors
bifocals
inventions
inventions
women change
appearance
addiction
stop
positive
human
searching for
perfectioncapable of
created by
is not a
not capable of
cosmetic
surgery
receive action
Explanation Graph
used for
part of
The explanation graph provides a detailed analysis of
the reasons why women undergo cosmetic surgery.
According to the graph, the main reasons include the
desire to change one's appearance, the in uence of
societal beauty standards, and the addiction to
searching for perfection. These reasons are further
supported by the various nodes and edges in the
graph, which provide additional insights into the
motivations and consequences of cosmetic surgery.
Firstly, the node labeled "human" highlights the
fundamental human desire to feel attractive and
con dent in one's appearance...
Write a 500-word argument essay based on the
information provided in the explanation graph.
Write an 500-word advertisement for this place in the
scene graph that would make people want to visit it.
Great! Based on the provided knowledge graph,
here's an educational tour that highlights interesting
facts and connections related to Benjamin Franklin's
inventions and other notable achievements:
1. Franklin's Lightning Rod: Start by exploring
Franklin's most famous invention - the lightning rod.
Discover how Franklin's invention protected buildings
from lightning strikes and how it changed the way
people understood electricity...
What else did ben franklin invent? 
Surrounding this question, develop an educational
tour through the knowledge graph, highlighting
interesting facts and connections in an engaging way.
Figure 1: We develop a flexible question-answering framework targeting real-world textual graph ap-
plications via a unified conversational interface. Presented here are examples showcasing the model’s
adeptness in handling generative and creative queries in practical graph-related tasks: common sense
reasoning, scene understanding, and knowledge graph reasoning, respectively.
classification [8], or answering simple questions on small or synthetic graphs [44, 31]. In contrast,
we develop a flexible question-answering framework targeting complex and real-world graphs. This
framework enables users to ‘chat with their graph’ via a unified conversational interface, representing
a leap towards intuitive interaction with graph data, as demonstrated in Figure 1.
The Need for a Comprehensive GraphQA Benchmark. Question answering (QA) is a fundamen-
tally important task in natural language processing, serving as a key benchmark for assessing LLMs
and providing a unified interface for various capabilities. Despite extensive research in QA, a com-
prehensive benchmark specifically tailored for the graph modality is lacking. In contrast to existing
benchmarks that focus on basic graph-based reasoning tasks such as node degree, edge existence, and
shortest path [6, 44], our benchmark addresses complex and real-world graph applications including
common sense reasoning, scene understanding, and knowledge graph reasoning (refer to Figure 2).
This is vital for measuring progress toward a model capable of answering a wide range of questions
about graphs from diverse applications.
New Architecture for GraphQA. To enable effective and efficient graph QA, even on large graphs,
we propose G-Retriever, a new framework combining the strengths of GNNs, LLMs, and RAG
(Figure 3). Next, we will discuss the motivation, strengths, and details of our model.
Tackling Hallucination in Graph LLMs.LLMs are prone to hallucination, a phenomenon where the
generated content is factually inaccurate or nonsensical [12]. We validate the presence of this issue in
graph settings. In particular, we employ a baseline method that adapts MiniGPT-4 [57] to graphs,
where a frozen LLM interacts with a trainable GNN that encodes graph data as a soft prompt, as in
GraphToken [31]. Our findings, shown in Table 1, indicate that hallucination, an important problem
in text-based LLMs, is also prevalent in Graph LLMs. This may be attributed to the baseline’s
inability to recall the entire graph structure from a single graph embedding, leading to the generation
of incorrect nodes or edges during the QA task. In contrast, by employing RAG for direct information
retrieval from the actual graph, our G-Retriever mitigates this issue, as substantiated by Table 1.
Enhancing Scalability and Efficiency in Graph LLMs. Recent research endeavors have explored
translating graphs into natural language, such as by flattening nodes and edges into a text sequence,
enabling their processing by LLMs for graph-based tasks [56, 6]. However, this method faces critical
scalability issues. Converting a graph with thousands of nodes and edges into a text sequence results
in an excessive number of tokens, surpassing the input capacity of many LLMs. An alternative of
truncating the graph text sequence to fit the LLM’s input token limit leads to loss of information and
response quality. G-Retriever overcomes these issues with its RAG component, which allows for
effective scaling to larger graphs by selectively retrieving only relevant parts of the graph.
Tailoring the RAG Approach to Graphs. Existing RAG methodologies are primarily designed
for simpler data types or knowledge graphs, where information is retrieved in a manner isolated
from the graph structure [7, 1, 36, 16]. Hence, we introduce a new retrieval approach for general
textual graphs. Notably, we formulate subgraph retrieval as a Prize-Collecting Steiner Tree (PCST)
2



Source: data\tc16_2312.10997v5\referenced_papers\[84]_2402.07630.pdf (Page 2):

Table 1: Observation and mitigation of hallucination in graph LLMs.
User
elephants
grass
eating bushes
in
riveron the side of
standing by
elephant
What kind of animal is in the bushes?
Answer the question and list the nodes or edges from the graph that support your answer.
LLM w/ Graph Prompt Tuning
[Incorrect response with hallucinated nodes and edges]
The animal in the bushes is a deer.
Nodes:
* Deer (node 1)
* Bushes (node 2)
Edges:
* Deer → Bushes (edge 1)
* Deer → Grass (edge 2)
* Bushes → Grass (edge 3)
G-Retriever (Ours)
[Correct response with accurate node and edge references]
Based on the information provided in the graph, the animal in the bushes is an elephant.
The nodes that support this answer are:
* Node 1: The elephant is mentioned in the first line of the graph.
The edges that support this answer are:
* The edge connects the elephant to the bushes, indicating that the elephant is in the bushes.
Therefore, the answer to the question is: The animal in the bushes is an elephant.
optimization problem, which takes the neighborhood information into account during retrieval. This
also allows the return of a subgraph most relevant to a query, thereby improving explainability.
The contributions of this paper are outlined as follows:
• Pioneering the integration of Graph RAG. We present the first retrieval approach for
general textual graph tasks, which greatly enhances scalability and efficiency.
• Enabling ‘Chat with Your Graph’. We develop a flexible question answering framework
to handle complex and real-world textual graphs through a unified conversational interface.
• Introducing A Novel GraphQA Benchmark. We introduce a diverse benchmark targeted
at real-world graph question answering, filling a crucial research gap.
• Empirical Findings. We demonstrate the efficiency and effectiveness of G-Retriever in
multiple domains and present the significant finding of hallucination in graph LLMs.
2 Related Work
Graphs and Large Language Models. A significant body of research has emerged at the intersection
of graph-based techniques and LLMs [30, 24, 15, 44, 54]. This exploration spans diverse aspects,
ranging from the design of general graph models [47, 25, 51, 19, 40, 31], and multi-modal architec-
tures [23, 49] to practical applications. Noteworthy applications include fundamental graph reason-
ing [52, 3, 56], node classification [8, 11, 39, 5, 50, 4, 33], graph classification/regression [32, 55],
and leveraging LLMs for knowledge graph-related tasks [41, 14, 29].
Retrieval-Augmented Generation (RAG).The concept of Retrieval-Augmented Generation, initially
proposed by Lewis et al. [21], has gained increased attention for its ability to mitigate the issue of
hallucination within LLMs and enhance trustworthiness and explainability [7]. Despite its success in
language-related tasks, the application of retrieval-based approaches to general graph tasks remains
largely unexplored. Most existing work focuses primarily on the knowledge graph [38, 1, 36, 16].
Our research is the first to apply a retrieval-based approach to general graph tasks, marking a novel
advancement in the field and demonstrating the versatility of RAG beyond language processing.
Parameter-Efficient Fine-Tuning (PEFT).The field of LLMs has witnessed significant advance-
ments through various parameter-efficient fine-tuning techniques. These methodologies have played
a crucial role in refining LLMs, boosting their performance while minimizing the need for extensive
parameter training. Notable among these techniques are prompt tuning, as introduced by Lester
et al. [20], and prefix tuning, proposed by Li and Liang [22]. Furthermore, methods like LoRA [10],
3



### Claim 24/179

#### Claim Text
SKR [58] classifies questions as known or unknown, applying retrieval enhancement selectively.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[58]_2310.05002.pdf (Page 2):

question
known? unknown?
retriever
LLM itself /
small trainable models
LLM knownsLLM unknowns
v .s. 
Collecting Self-KnowledgeEliciting Self-KnowledgeUsing Self-Knowledge
known unknown
questiontraining question
Figure 2: The overall pipeline of our SKR method. We first collect self-knowledge from training questions according
to the performance with or without external information (§ 3.1). Then we use the LLMs themselves or explicit small
trainable models to elicit self-knowledge of a question qt by referring to the collected self-knowledge from training
questions (§ 3.2). Finally, we use the self-knowledge to the new question and adaptively call a retriever (§ 3.3).
where ◦ denotes concatenation and {qj ◦ aj}d
j=1
are d demonstrations.
The above generated answers ˆa(M, qi) reflects
the internal knowledge to question qi in M. Mean-
while, we can possibly find passages from external
resources that may be related to qi, such passages
can be used as additional information for the model
input. Formally, for each question, we first use a
pre-trained retriever R to find the possibly related
information from corpus C :
pi = {pi1, pi2, ..., pik} = R(qi, C), (2)
where pi = {pi1, pi2, ..., pik} are the top- k re-
trieved passages for qi. In practice, we set R as
dense passage retriever (Karpukhin et al., 2020)
and C as passage chunks from Wikipedia. Then,
we use M again to generate the answer with re-
trieval augmentation:
ˆaR(M, qi) =M(q1◦p1◦a1, ..., qd◦pd◦ad, qi◦pi).
(3)
Given the answers ˆa(M, qi), ˆaR(M, qi), and
the ground-truth answer ai, we categorize each
question into positive subset D+ and negative sub-
set D− based on the differences between results:
qi ∈
(
D+, if E[ˆa(M, qi)] ≥ E[ˆaR(M, qi)];
D−, otherwise,
(4)
where E is an evaluation metric such as accuracy
and exact match score, we discard the question qi
if both the ˆa(M, qi) and ˆaR(M, qi) are incorrect.
Finally, the training set can be split into subset
D+ = {q+
1 , ..., q+
m} which includes questions that
M can directly give correct answers without ex-
ternal information (LLM knowns) and the subset
D− = {q−
1 , ..., q−
n } where the external information
can lead to more accurate results (LLM unknowns).
3.2 Eliciting Self-Knowledge of LLMs
Four different strategies are proposed to detect the
self-knowledge of target questions, including direct
prompting, in-context learning, training a classifier,
and nearest neighbor search. We use the LLM itself
in the former two methods and explicit smaller
modes in the latter two methods.
Direct Prompting Given a question qt, a straight-
forward way to detect whether LLMs are capable
of solving it is to ask them directly:
Direct Prompting
(prompt)
{qt} Q: Do you need additional information to
answer this question?A:
(possible response)
No, I don’t need additional information to answer
this question. / Yes, I need additional information to
answer this question.
Here we use the prompt “ Do you need addi-
tional information to answer this question?” as a
template and detect self-knowledge according to
the possible response. We thought LLM is capable
(or not capable) of solving the question well when
they “don’t need (or need) additional information”.
Direct prompting may intuitively work, but it tests
each question independently and does not make use
of the collected training questions in Section 3.1.
To remedy this issue, we further leverage the col-
lected self-knowledge from training questions in
the next three strategies.
In-Context Learning LLMs have shown a strong
capability to learn from demonstrations and infer



Source: data\tc16_2312.10997v5\referenced_papers\[58]_2310.05002.pdf (Page 3):

through few-shot in-context learning (Brown et al.,
2020). We select few training questions from both
D+ and D− as demonstrations to elicit the self-
knowledge to the question qt:
In-Context Learning
(prompt)
{q+
1 } Q: Do you need additional information to
answer this question?A: No, I don’t need additional
information to answer this question.
{q−
1 } Q: Do you need additional information to
answer this question? A: Yes, I need additional
information to answer this question.
......
{qt} Q: Do you need additional information to
answer this question?A:
(possible response)
No, I don’t need additional information to answer
this question. / Yes, I need additional information to
answer this question.
Here we use the answer templates “No, I don’t
need...” or “Yes, I need...” in demonstrations based
on whether the corresponding question comes from
positive set D+ or negative set D−, respectively.
The proposed direct prompting and in-context
learning methods can elicit self-knowledge of
LLMs to some extent. However, they have sev-
eral limitations. First, both methods require design-
ing prompts and calling the LLMs for each new
question, which makes it impractical. Second, in-
context learning could also be unstable due to con-
textual bias and sensitivity (Zhao et al., 2021; Lu
et al., 2022) and it is more difficult to address such
an issue for close-source LLMs. Third, they cannot
make use of all questions due to the constraints of
maximum tokens. To make our method more prac-
tical and avoid the above issues, we further leverage
smaller models to help elicit self-knowledge.
Training a Classifier Given D+ and D−, we can
take them as a two-way classification problem (e.g.,
setting qi in D+ with a positive label and qi in D−
with a negative label) and use all the samples to
train a classifier such as BERT-base (Devlin et al.,
2019) explicitly:
ˆyi = softmax(Whcls(qi) +b), (5)
where qi ∈ D+ ∪D− is a training question, hcls(qi)
is the sentence-level representation from BERT-
base, W and b are parameters of the classification
head. The parameters can be optimized by minimiz-
ing the cross-entropy loss between the predicted
label distribution ˆyi and the ground-truth label of
qi. Once the training is complete, we can infer the
label of question qt similar to Eq. 5.
Encoder 
Pos. / Neg. 
Training Questions
Question
Figure 3: Illustration of k-nearest-neighbor search to
elicit the self-knowledge to the question qt.
Nearest Neighbor Search Instead of training
an explicit smaller model, we can infer the la-
bel of questions through k-nearest-neighbor (kNN)
search by using a pre-trained fixed encoder, as
shown in Figure 3. kNN (Fix and Hodges, 1989)
is a widely used algorithm and benefit for a range
of NLP tasks (Khandelwal et al., 2020, 2021; Shi
et al., 2022; Xu et al., 2023a). Our motivation is
similar in that if two questions are close in the se-
mantically embedded space, then the LLMs would
show similar self-knowledge for both of them.
Formally, we encode each question into embed-
dings and compute the semantic similarity through
cosine distance sim(qt, qi) = e(qt)·e(qi)
||e(qt)||·||e(qi)||, where
qi ∈ {q+
1 , ..., q+
m, q−
1 , ..., q−
n }, e(·) is the represen-
tations of a sentence encoder such as SimCSE (Gao
et al., 2021). Then we search the top- k nearest
neighbors with the highest similarity. If the top-k
nearest neighbors include ℓ positive ones and k − ℓ
negative ones, we label the questionqt as positive if
ℓ
k−ℓ ≥ m
n or negative if ℓ
k−ℓ < m
n (m and n are the
numbers of questions in D+ and D−, respectively).
3.3 Using Self-Knowledge for Adaptive
Retrieval Augmentation
The self-knowledge given by the responses from
LLMs (via direct prompting or in-context learning)
or the predicted labels (via the trained classifier
or k-nearest-neighbor search) reflects the neces-
sity for external knowledge towards the question
qt. Therefore, we can adaptively call the retriever
instead of using them for every new question:
Adaptive Retrieval Augmentation
(for LLM known)
{q1 ◦ a1}, ...,{qd ◦ ad}, {qt}
A: (LLM directly answers without retrieval)
(for LLM unknown)
{q1 ◦ p1 ◦ a1}, ...,{qd ◦ pd ◦ ad}, {qt}
Here are some passages: {pt}
A: (LLM answers with retrieval augmentation)



Source: data\tc16_2312.10997v5\referenced_papers\[58]_2310.05002.pdf (Page 7):

Question Q: In a Roman Osteria is a 19th-century work of art. True or False?
Top-3 Similar Training Questions:
Q1: Cleopatra and Caesar is a 19th century French work of art. True or False?
Q2: Boulevard des Capucines is a 19th century European work of art. True or False?
Q3: Bulb Fields is a 19th century European work of art. True or False?
Manual-CoT Manual-CoT-IR
A1: Cleopatra and Caesar are a 17th-century Italian paint-
ing by Francesco Barberini. So the answer is false. (✗)
A1: Cleopatra and Caesar is an oil on canvas painting by the
French Academic artist Jean-Léon Gérôme, completed in 1866.
So the answer is true. (✓)
A2: Boulevard des Capucines is a street in Paris, France.
[...] Thus, Boulevard des Capucines is not a 19th-century
European work of art. So the answer is false. (✗)
A2: Boulevard des Capucines is an oil painting [...] by French
Impressionist artist Claude Monet created in 1873. So the
answer is true. (✓)
A3: Bulb Fields is a 1990 painting by Mark Rothko. So
the answer is false. (✗)
A3: Bulb Fields is an oil painting created by Vincent van Gogh
in early 1883. So the answer is true. (✓)
A: An osteria is an Italian type of eating establishment. It
is not a 19th-century artwork. So the answer is false.(✗)
A: In a Roman Osteria is a painting by the Danish painter Carl
Bloch. It was painted in 1866. So the answer is true.(✓)
Table 3: Responses by InstructGPT for the top-3 similar questions from the training set. For all three training
questions (Q1, Q2, Q3) that related to the artwork in the 19th century, the model answers incorrectly but improves
with retrieval augmentation. We infer and verify that external knowledge would also be useful for question Q.
Temporal Commonsense Tabular Strategy Truthful Average
48
56
64
72
80F1 / Accuracy
MSMARCO
SciFact
Wikipedia
Figure 6: The performance on each dataset by using
different corpus for retrieval augmentation.
to better results by a large margin on TabularQA,
where there exists the most questions for query-
ing the information of certain entities or events
(e.g., John Adams was born in the Spring of 1735.
True or False?). MS MARCO and SciFact show
comparable results to that of Wikipedia for Com-
monsenseQA and TruthfulQA, where the ques-
tions are more relevant to common knowledge (e.g.,
What happens to you if you eat watermelon seeds?
Choices:...). Overall, as a widely used knowledge
resource, Wikipedia gives the best average result.
5.5 Case Study
Table 3 illustrates an example showing the different
responses with or without retrieval augmentation
to similar questions and how self-knowledge is
deduced by using nearest-neighbor search.
Given the question “ In a Roman Osteria is a
19th-century work of art. True or False?”, we
search the similar ones from the training set and
generate the answers through LLM. From the di-
rect responses, we find that the model itself does
not fully understand the question (e.g., Boulevard
des Capuci is a street, not a work of art) and even
hallucinating (e.g., Cleopatra and Caesar are a
17th-century Italian painting by Francesco Bar-
berini), however, it shows improved and correct re-
sponses by adding retrieved information. Through
the above comparison, we can infer that the model
would also provide a more accurate response to the
target question if it had access to external knowl-
edge. The results in the last row further validate our
hypothesis. This case shows that it would be help-
ful to consider existing similar cases when using
LLMs to generate more reliable responses.
6 Conclusion
In this paper, we propose a Self-Knowledge guided
Retrieval augmentation (SKR) method, which in-
vestigates eliciting the ability of LLMs to recog-
nize what they know or do not know (i.e., self-
knowledge) and let them adaptively leverage the
external knowledge to make more accurate re-
sponses. Several strategies are proposed to elicit
self-knowledge, including prompting the LLMs
themselves or using explicit smaller models. Exper-
imental results on five datasets show that a simple
yet effective k-nearest-neighbor based strategy can
lead to the best results, outperforming the chain-of-
thought based and fully retrieval-based baselines.



Source: data\tc16_2312.10997v5\referenced_papers\[58]_2310.05002.pdf (Page 0):

Self-Knowledge Guided Retrieval Augmentation
for Large Language Models
Yile Wang1, Peng Li∗1,4, Maosong Sun2,3, Yang Liu∗1,2,3,4
1Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China
2Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China
3Beijing National Research Center for Information Science and Technology
4Shanghai Artificial Intelligence Laboratory, Shanghai, China
{wangyile,lipeng}@air.tsinghua.edu.cn, {sms,liuyang2011}@tsinghua.edu.cn
Abstract
Large language models (LLMs) have shown su-
perior performance without task-specific fine-
tuning. Despite the success, the knowledge
stored in the parameters of LLMs could still
be incomplete and difficult to update due
to the computational costs. As complemen-
tary, retrieval-based methods can offer non-
parametric world knowledge and improve the
performance on tasks such as question answer-
ing. However, we find that the retrieved knowl-
edge does not always help and even has a
negative impact on original responses occa-
sionally. To better make use of both inter-
nal knowledge and external world knowledge,
we investigate eliciting the model’s ability to
recognize what they know and do not know
(which is also called “self-knowledge”) and
propose Self-Knowledge guided Retrieval aug-
mentation (SKR), a simple yet effective method
which can let LLMs refer to the questions
they have previously encountered and adap-
tively call for external resources when deal-
ing with new questions. We evaluate SKR
on multiple datasets and demonstrate that it
outperforms chain-of-thought based and fully
retrieval-based methods by using either In-
structGPT or ChatGPT.
1 Introduction
Large language models (LLMs, Brown et al., 2020;
Chowdhery et al., 2022; Ouyang et al., 2022)
have achieved remarkable performance without
much task-specific fine-tuning. However, the
full-parametric knowledge stored in LLMs could
still be incomplete and difficult to update due to
the computational costs. Alternatively, retrieval-
augmented methods (Guu et al., 2020; Lewis et al.,
2020b; Borgeaud et al., 2022; Izacard et al., 2022;
Shi et al., 2023) can utilize external resources
such as Wikipedia and offer complementary non-
parametric knowledge to enrich the contextualized
∗ Corresponding authors.
Would a German Shepherd
be welcome in an airport?
Yes. German Shepherds are
often used as seeing-eye dogs.
No. Airports have very strict
regulations regarding animals.
Old German Shepherds Dog
is a controversial name for...
Question (Answer: Yes)
Retrieved Passages
Figure 1: Comparison between two responses given
by InstructGPT. The retrieved passages are relevant but
not particularly helpful for solving the question, which
influences the model’s judgment and leads to incorrect
answers.
information, thus helping the model generate more
reliable answers.
Retrieval augmentation has shown to be very
effective for models such as BERT (Devlin et al.,
2019), BART (Lewis et al., 2020a), and T5 (Raf-
fel et al., 2020) in various tasks (Karpukhin et al.,
2020; Khandelwal et al., 2020, 2021; Izacard and
Grave, 2021; Wang et al., 2022; Guo et al., 2023).
As LLMs become more and more “knowledgable”,
recent studies show that the benefit brought from
retrieval augmentation is reducing (Mallen et al.,
2022; Yoran et al., 2023). Moreover, we find that
the retrieved passages could even negatively af-
fect what LLMs originally know. As illustrated
in Figure 1, the model can directly give reason-
able answers “German Shepherds are often used
as seeing-eye dogs”, however, it is distracted and
gives incorrect ones by adding retrieved passages.
The above findings show that one should be more
careful when applying the retrieval-based method
since it is difficult to know in advance whether the
retrieved results are better than what LLMs already
captured. To this end, a key issue is to figure out
what LLMs do well (e.g., they can answer correctly
without assistance) and what they cannot do well
(e.g., they answer incorrectly and external informa-
tion can lead to improved results).
Unfortunately, LLMs themselves have a limited
ability to recognize what they know and do not
arXiv:2310.05002v1  [cs.CL]  8 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[58]_2310.05002.pdf (Page 1):

know, which is also called “self-knowledge” (Yin
et al., 2023). However, such an ability is crucial
for generating truthful responses (Kadavath et al.,
2022) and could be helpful for LLMs themselves
to “decide when and when not to use tools” such
as a retriever (Mialon et al., 2023).
In this paper, we investigate eliciting the self-
knowledge of LLMs and propose a simple yet ef-
fective Self-Knowledge guided Retrieval augmen-
tation (SKR) method to flexibly call the retriever
for making better use of both internal and external
knowledge. In particular, different from existing
studies that evaluate the ability through specifically-
designed metrics or datasets, we collect the self-
knowledge of training questions by comparing the
performance with or without retrieval augmenta-
tion. Then, we propose several strategies to de-
tect the self-knowledge corresponding to a ques-
tion by referring to the existing collected training
questions, including using the LLMs themselves
through prompting or explicitly training a small
model. Finally, we leverage such elicited self-
knowledge to better solve the question through
adaptive retrieval augmentation.
We evaluate SKR on five datasets by us-
ing InstructGPT ( text-davinci-003) and Chat-
GPT ( gpt-3.5-turbo-0301). Experimental re-
sults show that SKR outperforms chain-of-thought
based (Wei et al., 2022) and fully retrieval-based
methods by 4.08%/2.91% (for InstructGPT) and
4.02%/4.20% (for ChatGPT), respectively.
2 Related Work
Retrieval-Augmented LLMs Recent studies show
that retrieval-augmented methods can enhance the
reasoning ability of LLMs (Trivedi et al., 2022; He
et al., 2022; Yu et al., 2023; Shao et al., 2023; Jiang
et al., 2023) and make the responses more credible
and traceable (Xu et al., 2023b; Qian et al., 2023).
For example, Trivedi et al. (2022) uses the chain-of-
thought (Wei et al., 2022) reasoning steps as queries
and uses the results to guide further reasoning and
retrieval. He et al. (2022) uses an external natural
language inference model to select the most sup-
ported reasoning path via retrieved evidence. Yu
et al. (2023) propose using the retrieval feedback
to refine the output of LLMs to be more reliable
and accurate. Xu et al. (2023b) propose search-in-
chain and make LLMs interact with retrievers to
improve accuracy and credibility. These methods
aim at integrating sufficient external knowledge for
a better reasoning process, while we propose to bet-
ter utilize both the internal and external knowledge
through eliciting the self-knowledge of LLMs.
Another line of work tries to teach LLMs to use
external tools including retriever, calculator, other
foundation models, etc. (Schick et al., 2023; Shen
et al., 2023; Qin et al., 2023). These works focus
more on leveraging the language understanding
capabilities of LLMs to deploy suitable tools in
different scenarios, while our work investigates the
self-knowledge of LLMs and tries to integrate them
with retrievers in a more flexible manner.
Self-Knowledge in LLMs “Self-knowledge” in
LLMs is originally mentioned in Kadavath et al.
(2022), which is used to measure the LLMs’ confi-
dence in their own knowledge and reasoning. Such
ability is further defined as “the ability to under-
stand limitations on the unknowns” and evaluated
by Yin et al. (2023), where they find a consider-
able gap exists between self-knowledge in mod-
els and humans. To explore the LLMs capabilities
more extensively, unanswerable and more challeng-
ing datasets are also proposed (Rajpurkar et al.,
2018; Srivastava et al., 2022; Suzgun et al., 2022).
Our work is also related to detecting what LLMs
know and do not know, while we do not design
new evaluation metrics or challenging datasets to
test the ability. By explicitly introducing the ex-
ternal resources, we detect the knowledge bound-
ary of LLMs through the performance changes.
Moreover, instead of evaluating each question in-
dependently, we propose several ways to elicit self-
knowledge by referring to existing cases.
3 Method
Our method is depicted under the question-
answering settings, which has been a popular way
to interact with and assess LLMs. The overall
pipeline is shown in Figure 2, which includes
collecting, eliciting, and using self-knowledge of
LLMs. We introduce each of them as follows.
3.1 Collecting Self-Knowledge of LLMs from
Training Samples
Given a dataset D with training question-answer
pairs {qj, aj}|D|
j=1, we can use the LLM M to gen-
erate the answers for each question qi via few-shot
in-context learning (Brown et al., 2020):
ˆa(M, qi) =M(q1 ◦ a1, ..., qd ◦ ad, qi), (1)



### Claim 25/179

#### Claim Text
GenRead [13] replaces the retriever with an LLM generator, finding that LLM-generated contexts often contain more accurate answers due to better alignment with the pre-training objectives of causal language modeling.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 0):

Query Rewriting for Retrieval-Augmented Large Language Models
Xinbei Ma1,2,∗ , Yeyun Gong3, #, †, Pengcheng He4, #, Hai Zhao1,2,†, Nan Duan3
1Department of Computer Science and Engineering, Shanghai Jiao Tong University
2Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University
3Microsoft Research Asia 4Microsoft Azure AI
sjtumaxb@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn,
{yegong, nanduan}@microsoft.com, Herbert.he@gmail.com
Abstract
Large Language Models (LLMs) play pow-
erful, black-box readers in the retrieve-then-
read pipeline, making remarkable progress
in knowledge-intensive tasks. This work in-
troduces a new framework, Rewrite-Retrieve-
Read instead of the previous retrieve-then-read
for the retrieval-augmented LLMs from the per-
spective of the query rewriting. Unlike prior
studies focusing on adapting either the retriever
or the reader, our approach pays attention to
the adaptation of the search query itself, for
there is inevitably a gap between the input text
and the needed knowledge in retrieval. We
first prompt an LLM to generate the query,
then use a web search engine to retrieve con-
texts. Furthermore, to better align the query
to the frozen modules, we propose a trainable
scheme for our pipeline. A small language
model is adopted as a trainable rewriter to cater
to the black-box LLM reader. The rewriter is
trained using the feedback of the LLM reader
by reinforcement learning. Evaluation is con-
ducted on downstream tasks, open-domain QA
and multiple-choice QA. Experiments results
show consistent performance improvement, in-
dicating that our framework is proven effective
and scalable, and brings a new framework for
retrieval-augmented LLM 1.
1 Introduction
Large Language Models (LLMs) have shown re-
markable abilities for human language processing
and extraordinary scalability and adaptability in
few- or zero-shot settings.(Ouyang et al., 2022;
Brown et al., 2020; Chowdhery et al., 2022). How-
ever, the training process depends on large-scale
high-quality corpora but without the perception
∗ Work done during an internship at 3Microsoft Research
Asia. # Equal contribution. †Corresponding author.
This paper was partially supported by Joint Research
Project of Yangtze River Delta Science and Technology Inno-
vation Community (No. 2022CSJGG1400).
1https://github.com/xbmxb/RAG-query-rewriting
of the real world. Thus, LLMs still have to face
the issue of hallucination (Yao et al., 2023; Bang
et al., 2023) and temporal misalignment (Röttger
and Pierrehumbert, 2021; Luu et al., 2022; Jang
et al., 2022). This affects the reliability of LLMs
and hinders wider practical application, because
the consistency between the LLM responses with
the real world needs further validation. Exist-
ing work has proved that incorporating external
knowledge (i.e., non-parametric knowledge) with
internal knowledge (i.e., parametric knowledge)
can effectively alleviate hallucination, especially
for knowledge-intensive tasks. In fact, retrieval-
augmented LLMs have been shown so effective
that they have been regarded as a standard solu-
tion to alleviate the factuality drawbacks in naive
LLM generations. Retrieval augmentation is ap-
plied to select relative passages as external contexts
for the language model, which isretrieve-then-read
framework (Lewis et al., 2020b; Karpukhin et al.,
2020; Izacard et al., 2022). Take the open-domain
Question-Answering task (open-domain QA) as
an example, a retriever first searches for related
documents for a question. Then the LLM receives
the question and the documents, then predicts an
answer.
As most LLMs are only accessible through infer-
ence APIs, they play the part of black-box frozen
readers in the pipeline. This makes previous re-
trieval augmentation methods that require complete
access (Lewis et al., 2020b; Guu et al., 2020; Izac-
ard et al., 2022) no longer feasible. Recent studies
on retrieval-augmented language models lean more
on the LLM-oriented adaptation. An idea is to train
a dense retrieval model to cater to the frozen lan-
guage model (Shi et al., 2023). By using feedback
from the LLM as a training objective, the retrieval
model is tuned for better LLM input contexts. An-
other research line focuses on the design of inter-
actions between the retriever and the reader (Yao
et al., 2023; Khattab et al., 2022), where both the
arXiv:2305.14283v3  [cs.CL]  23 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[64]_2302.00083.pdf (Page 2):

methods that are specialized for the LM task. These
in turn can be used to improve both In-Context
RALM and other more elaborate RALM methods
that currently leverage general purpose retrievers.
Second, due to its compatibility with off-the-shelf
LMs, In-Context RALM can help drive wider de-
ployment of RALM systems.
2 Related Work
RALM approaches can be roughly divided into two
families of models: (i) nearest-neighbor language
models (also called kNN-LM), and (ii) retrieve
and read models. Our work belongs to the second
family, but is distinct in that it involves no further
training of the LM.
Nearest Neighbor Language ModelsThe kNN-
LM approach was first introduced in Khandel-
wal et al. (2020). The authors suggest a simple
inference-time model that interpolates between two
next-token distributions: one induced by the LM
itself, and one induced by the k neighbors from the
retrieval corpus that are closest to the query token in
the LM embedding space. Zhong et al. (2022) sug-
gest a framework for training these models. While
they showed significant gains from kNN-LM, the
approach requires storing the representations for
each token in the corpus, an expensive requirement
even for a small corpus like Wikipedia. Although
numerous approaches have been suggested for al-
leviating this issue (He et al., 2021; Alon et al.,
2022), scaling any of them to large corpora remains
an open challenge.
Retrieve and Read Models This family of
RALMs creates a clear division between document
selection and document reading components. All
prior work involves training the LM. We begin by
describing works that use this approach for tack-
ling downstream tasks, and then mention works ori-
ented towards RALM. Lewis et al. (2020) and Izac-
ard and Grave (2021) fine tuned encoder–decoder
architectures for downstream knowledge-intensive
tasks. Izacard et al. (2022b) explored different
ways of pretraining such models, while Levine
et al. (2022c) pretrained an autoregressive LM on
clusters of nearest neighbors in sentence embed-
ding space. Levine et al. (2022a) showed competi-
tive open domain question-answering performance
by prompt-tuning a frozen LM as a reader. Guu
et al. (2020) pretrained REALM, a retrieval aug-
mented bidirectional, masked LM, later fine-tuned
for open-domain question answering. The work
closest to this paper—with a focus on the language
modeling task—is RETRO (Borgeaud et al., 2022),
which modifies an autoregressive LM to attend to
relevant documents via chunked cross-attention,
thus introducing new parameters to the model. Our
In-Context RALM differs from prior work in this
family of models in two key aspects:
• We use off-the-shelf LMs for document read-
ing without any further training of the LM.
• We focus on how to choose documents for
improved LM performance.
3 Our Framework
3.1 In-Context RALM
Language models define probability distributions
over sequences of tokens. Given such a sequence
x1, ..., xn, the standard way to model its probabil-
ity is via next-token prediction: p(x1, ..., xn) =Qn
i=1 p(xi|x<i), where x<i := x1, ..., xi−1 is the
sequence of tokens preceding xi, also referred to
as its prefix. This autoregressive model is usu-
ally implemented via a learned transformer net-
work (Vaswani et al., 2017) parameterized by the
set of parameters θ:
p(x1, ..., xn) =
nY
i=1
pθ(xi|x<i), (1)
where the conditional probabilities are modeled
by employing a causal self-attention mask (Rad-
ford et al., 2018). Notably, leading LMs such
as GPT-2 (Radford et al., 2019), GPT-3 (Brown
et al., 2020), OPT (Zhang et al., 2022) or Jurassic-
1 (Lieber et al., 2021) follow this simple parame-
terization.
Retrieval augmented language models (RALMs)
add an operation that retrieves one or more docu-
ments from an external corpus C, and condition the
above LM predictions on these documents. Specifi-
cally, for predicting xi, the retrieval operation from
C depends on its prefix: RC(x<i), so the most
general RALM decomposition is: p(x1, ..., xn) =Qn
i=1 p(xi|x<i, RC(x<i)). In order to condition
the LM generation on the retrieved document, pre-
vious RALM approaches used specialized architec-
tures or algorithms (see §2). Inspired by the suc-
cess of In-Context Learning (Brown et al., 2020;
Dong et al., 2023), In-Context RALM refers to the
following specific, simple method of concatenating



Source: data\tc16_2312.10997v5\referenced_papers\[64]_2302.00083.pdf (Page 0):

In-Context Retrieval-Augmented Language Models
Ori Ram∗ Yoav Levine∗ Itay Dalmedigos Dor Muhlgay
Amnon Shashua Kevin Leyton-Brown Yoav Shoham
AI21 Labs
{orir,yoavl,itayd,dorm,amnons,kevinlb,yoavs}@ai21.com
Abstract
Retrieval-Augmented Language Modeling
(RALM) methods, which condition a lan-
guage model (LM) on relevant documents
from a grounding corpus during generation,
were shown to significantly improve lan-
guage modeling performance. In addition,
they can mitigate the problem of factually
inaccurate text generation and provide natu-
ral source attribution mechanism. Existing
RALM approaches focus on modifying the
LM architecture in order to facilitate the in-
corporation of external information, signifi-
cantly complicating deployment. This paper
considers a simple alternative, which we dub
In-Context RALM: leaving the LM architec-
ture unchanged and prepending grounding
documents to the input, without any further
training of the LM. We show that In-Context
RALM that builds on off-the-shelf general
purpose retrievers provides surprisingly large
LM gains across model sizes and diverse cor-
pora. We also demonstrate that the document
retrieval and ranking mechanism can be spe-
cialized to the RALM setting to further boost
performance. We conclude that In-Context
RALM has considerable potential to increase
the prevalence of LM grounding, particularly
in settings where a pretrained LM must be
used without modification or even via API
access.1
1 Introduction
Recent advances in language modeling (LM) have
dramatically increased the usefulness of machine-
generated text across a wide range of use-cases
and domains (Brown et al., 2020). However, the
mainstream paradigm of generating text with LMs
bears inherent limitations in access to external
knowledge. First, LMs are not coupled with any
∗Equal contribution.
1Our code is available at https://github.com/
AI21Labs/in-context-ralm
Perplexity
10.0
15.0
20.0
25.0
30.0
GPT-2 345M (M) GPT-2 1.5B (XL)
No Retrieval In-Context RALM (BM25)
In-Context RALM (Predictive Reranking)
Figure 1: Our framework, dubbed In-Context
RALM, provides large language modeling gains on
the test set of WikiText-103, without modifying the
LM. Adapting the use of a BM25 retriever (Robert-
son and Zaragoza, 2009) to the LM task (§5) yields
significant gains, and choosing the grounding doc-
uments via our new class of Predictive Rerankers
(§6) provides a further boost. See Table 1 for the
full results on five diverse corpora.
source attribution, and must be trained in order
to incorporate up-to-date information that was not
seen during training. More importantly, they tend
to produce factual inaccuracies and errors (Lin
et al., 2022; Maynez et al., 2020; Huang et al.,
2020). This problem is present in any LM gen-
eration scenario, and is exacerbated when gener-
ation is made in uncommon domains or private
data. A promising approach for addressing the
above is Retrieval-Augmented Language Modeling
(RALM), grounding the LM during generation by
conditioning on relevant documents retrieved from
an external knowledge source. RALM systems in-
clude two high level components: (i) document se-
lection, selecting the set of documents upon which
to condition; and (ii) document reading, determin-
ing how to incorporate the selected documents into
the LM generation process.
Leading RALM systems introduced recently
arXiv:2302.00083v3  [cs.CL]  1 Aug 2023



Source: data\tc16_2312.10997v5\referenced_papers\[26]_2401.06954.pdf (Page 0):

Bridging the Preference Gap between Retrievers and LLMs
Zixuan Ke2∗, Weize Kong 1, Cheng Li 1, Mingyang Zhang 1, Qiaozhu Mei 3† and Michael Bendersky1
1Google Research
2University of Illinois at Chicago
3University of Michigan
1{weize,chgli,mingyang,bemike}@google.com
2zke4@uic.edu
3qmei@umich.edu
Abstract
Large Language Models (LLMs) have demon-
strated superior results across a wide range
of tasks, and Retrieval-augmented Generation
(RAG) is an effective way to enhance the per-
formance by locating relevant information and
placing it into the context window of the LLM.
However, the relationship between retrievers
and LLMs in a RAG is still under-investigated.
Most existing work treats the retriever and the
LLM as independent components and leaves a
gap between retrieving human-“friendly” infor-
mation and assembling a LLM-“friendly” con-
text. In this work, we examine a novel bridge
mechanism. We validate the ranking and se-
lection assumptions of retrievers in the context
of RAG and propose a framework that chains
together supervised and reinforcement learn-
ing to train a bridge model that optimizes the
connection between the retriever and the LLM.
Empirical results demonstrate the effectiveness
of our method in both question-answering and
personalized generation tasks.
1 Introduction
Large language models (LLMs) such as GPT-4
(?) and PaLM 2 (Anil et al., 2023), have demon-
strated impressive performance on a wide variety
of tasks. Retrieval-augmented generation (RAG),
which retrieves knowledge items from an external
data source and puts it into the context window
of LLMs, has produced significantly enhanced re-
sults in many NLP tasks (Khandelwal et al., 2020;
Borgeaud et al., 2022; Izacard et al., 2022; Ya-
sunaga et al., 2023).
However, most works on RAG study retrievers
and LLMs separately. On one hand, most retriev-
ers are designed to be human-friendly, usually
based on the general belief in classic information
∗ The work was done during internship at Google Re-
search.
† The work was done as a visiting researcher at Google
Research.
Figure 1: We observe a preference gap when alternating
the ranking and selection of information in RAG. Ex-
periments are conducted with retrieving passages using
GTR (Ni et al., 2021) and using top K of them as ad-
ditional context for a frozen Palm2-S LLM. Different
colors indicate different datasets (detailed in Sec. 5.1)
and the Y-axis shows the relative percentage. Alternat-
ing the selection (Top-1) of information significantly
affects (either positively or negatively) the LLM’s per-
formance, while randomizing the ranking of multiple
selected items (Top-5) does not have a comparable im-
pact (the metrics are detailed in Sec. 5.2). Note the
impact on NQ is even too small to be visible.
retrieval literature that ranking is paramount, as
humans typically read from top to bottom (?). On
the other hand, LLMs exhibit preferences different
from humans and yield accurate results only when
the information in the prompt aligns with these
preferences. This discrepancy leads to sub-optimal
design in current RAG systems, a phenomenon we
term preference gap. This gap manifests in various
aspects. For example, the general belief in ranking
may not align with LLM’s preferences due to the
self-attention mechanism of Transformers, which
can focus on any token regardless of its position.
Another aspect is selection; while humans can eas-
ily disregard irrelevant information in a context, it
has been shown that LLMs are highly sensitive to
irrelevant content (Shi et al., 2023a). There likely
exist more aspects that further diverge the LLM’s
preference from that of humans, e.g., repetition.
arXiv:2401.06954v2  [cs.CL]  20 Feb 2024



Source: data\tc16_2312.10997v5\referenced_papers\[69]_2310.18347.pdf (Page 0):

PRCA: Fitting Black-Box Large Language Models for Retrieval Question
Answering via Pluggable Reward-Driven Contextual Adapter
Haoyan Yang1,2†, Zhitao Li1, Yong Zhang1, Jianzong Wang1∗,
Ning Cheng1, Ming Li1,3, Jing Xiao1
1Ping An Technology (Shenzhen) Co., Ltd., China
2New York University 3University of Maryland
jzwang@188.com
Abstract
The Retrieval Question Answering (ReQA)
task employs the retrieval-augmented frame-
work, composed of a retriever and generator.
The generator formulates the answer based on
the documents retrieved by the retriever. Incor-
porating Large Language Models (LLMs) as
generators is beneficial due to their advanced
QA capabilities, but they are typically too large
to be fine-tuned with budget constraints while
some of them are only accessible via APIs. To
tackle this issue and further improve ReQA per-
formance, we propose a trainable Pluggable
Reward-Driven Contextual Adapter (PRCA),
keeping the generator as a black box. Po-
sitioned between the retriever and generator
in a Pluggable manner, PRCA refines the re-
trieved information by operating in a token-
autoregressive strategy via maximizing rewards
of the reinforcement learning phase. Our ex-
periments validate PRCA’s effectiveness in en-
hancing ReQA performance on three datasets
by up to 20% improvement to fit black-box
LLMs into existing frameworks, demonstrating
its considerable potential in the LLMs era.
1 Introduction
Retrieval Question Answering (ReQA) tasks in-
volve generating appropriate answers to given ques-
tions, utilizing relevant contextual documents. To
achieve this, retrieval augmentation is employed
(Chen et al., 2017; Pan et al., 2019; Izacard and
Grave, 2021), and comprised of two key compo-
nents: a retriever and a generator. The retriever’s
role is to retrieve relevant documents from a large
corpus in response to the question, while the gener-
ator uses this contextual information to formulate
accurate answers. Such systems alleviate the prob-
lem of hallucinations (Shuster et al., 2021), thereby
enhancing the overall accuracy of the output.
† The work was done when the first author was doing
internship at Ping An Technology (Shenzhen) Co., Ltd., China.
∗Corresponding author: Jianzong Wang.
Current Paradigm 
PRCA 
PRCA-based Paradigm 
Query 
Retriever 
Top K 
Documents 
Query 
+
Generator 
Black-Box 
Model 
Corpus 
Generator 
White-Box 
Model 
Query 
Retriever Corpus 
Top K 
Documents 
Query 
+
Figure 1: A comparison between two paradigms for
information retrieval and generation. The upper section
showcases the traditional method where a query is pro-
cessed by a retriever that scans a corpus to fetch the
Top-K documents and then fed to a white-box genera-
tor. The lower section introduces our proposed PRCA
method, which processes extracted Top-K documents
from the retriever before feeding them to black-box
generator to achieve better performance for in-domain
tasks.
Recent advances in Large Language Models
(LLMs) such as the generative pre-trained trans-
former (GPT) series (Brown et al., 2020; Ouyang
et al., 2022; OpenAI, 2023) have demonstrated
remarkable potential, notably in their zero-shot
and few-shot abilities within the realm of QA
tasks. Owing to these capabilities, LLMs are ex-
cellent choices as generators within the retrieval-
augmented framework. However, due to the vast
parameters of LLMs, fine-tuning them becomes
exceedingly difficult within a limited computation
budget. Furthermore, certain LLMs such as GPT-4
(OpenAI, 2023) are closed-source, making it im-
possible to fine-tune them. To achieve optimal
results on specific datasets, fine-tuning retrieval-
augmented models becomes necessary (Guu et al.,
2020; Lewis et al., 2020b; An et al., 2021). Previ-
ous attempts to integrate LLMs into the retrieval-
augmented framework have met with partial suc-
arXiv:2310.18347v1  [cs.CL]  23 Oct 2023



### Claim 26/179

#### Claim Text
Selfmem [17] iteratively creates an unbounded memory pool with a retrieval-enhanced generator, using a memory selector to choose outputs that serve as dual problems to the original question, thus self-enhancing the generative model.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 1):

polysemy, morphology, and coreference). We define this as the primal problem: better memory
prompts better generation. Consequently, numerous studies have focused on how to retrieve better
memory, ranging from sparse retrieval to dense retrieval [10, 63], from a fixed retriever to a learnable
retriever [41, 8], and from sentence-level memory to more fine-grained token-level memory [36, 35].
0.0 0.2 0.4 0.6 0.8 1.0
Memory Similarity
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9Hypothesis BLEU
Figure 1: Relation between memory and hy-
pothesis on JRC-Acquis En→De dataset.
The hypothesis is generated by a retrieval-
augmented translator whose memory is re-
trieved from the training set. The X-axis
represents the similarity between memory
and the reference.
However, a fundamental limitation exists in all previous
works: the memory is retrieved from a fixed corpus
and is constrained by the corpus’s quality. Due to the
finite retrieval space, bounded memory significantly
restricts the potential of memory-augmented generation
models [97]. In this paper, we explore the duality of the
primal problem, which posits that better generation
also prompts better memory. We propose a novel
framework called Selfmem, which iteratively employs
a retrieval-augmented generator to create an unbounded
memory pool and uses a memory selector to choose one
output as memory for the subsequent generation round.
By combining the primal and dual problem, a retrieval-
augmented generation model can elevate itself using
its own output, referred to as self-memory. The key
insight behind Selfmem is that the text more closely
resembling the data distribution during inference is not
the training data [87], but the model’s own output.
Selfmem consists of two complementary components:
a retrieval-augmented generator and a memory selector. The generator operates under two distinct
paradigms: fine-tuning a small model or few-shot prompting an LLM. For the former, we train the
generator with labeled data and retrieved memory, while for the latter, we employ a fixed black-box
LLM exclusively for inference alongside retrieved in-context learning samples. We then use the
generator’s output to train a memory selector based on a specific performance metric. By simply
replacing the retrieved memory with unbounded generated memory, we achieve higher-quality
generation output (primal problem), which subsequently serves as memory for the next round after
being refined by the memory selector (dual problem).
To evaluate the efficacy of theSelfmem, we carry out comprehensive experiments in three distinct text
generation tasks: neural machine translation, abstractive text summarization, and dialogue generation.
We witness substantial enhancements over robust baselines, attaining state-of-the-art outcomes in
JRC-Acquis (four directions), XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1). To gain
deeper insights into the Selfmem, we meticulously investigate each crucial component and pinpoint
the existing system bottleneck to guide future research endeavors.
2 Related Work
2.1 Retrieval-augmented Text Generation
Since the world is not a snapshot once the training corpus is collected, we can never expect an
ever-large model to capture everything in its parameters, even for LLMs like GPT-4 [62]. Therefore,
it is crucial to equip these models with an external memory bank to store additional knowledge or
useful demonstration examples for solving various NLP tasks[41, 78, 95].
In the translation domain, retrieval techniques have long been employed by the localization industry
to enhance human translators’ productivity and consistency even before the advent of machine
translation [94]. Early works on machine translation primarily focused on utilizing memory for
statistical machine translation (SMT) systems [ 80, 50]. For neural machine translation (NMT),
[28] were the first to use search engines to retrieve memory from the training set and incorporate
it with an external memory network. Subsequent research explored various aspects of retrieval-
augmented NMT, such as memory encoding methods [92, 93, 31], joint training of retrievers and
generators with monolingual data [ 8], memory granularity [ 35], and memory diversity [ 17]. For
few-shot LLM generation, strategies for in-context example selection have been proposed to improve
translation quality [2]. Furthermore, in-context machine translation has been shown to be effective
for on-the-fly adaptation [79]. For dialogue response generation tasks, employing exemplar/template
2



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 0):

Lift Yourself Up: Retrieval-augmented Text
Generation with Self-Memory
Xin Cheng1 Di Luo2 Xiuying Chen3 Lemao Liu4 Dongyan Zhao1 Rui Yan2
1 Peking University 2 Renmin University of China
3 KAUST 4 Tencent AI Lab
chengxin1998@stu.pku.edu.cn
Abstract
With direct access to human-written reference as memory, retrieval-augmented
generation has achieved much progress in a wide range of text generation tasks.
Since better memory would typically prompt better generation (we define this as
primal problem). The traditional approach for memory retrieval involves selecting
memory that exhibits the highest similarity to the input. However, this method
is constrained by the quality of the fixed corpus from which memory is retrieved.
In this paper, by exploring the duality of the primal problem: better generation
also prompts better memory, we propose a novel framework, Selfmem, which
addresses this limitation by iteratively employing a retrieval-augmented generator
to create an unbounded memory pool and using a memory selector to choose one
output as memory for the subsequent generation round. This enables the model
to leverage its own output, referred to as self-memory, for improved generation.
We evaluate the effectiveness ofSelfmem on three distinct text generation tasks:
neural machine translation, abstractive text summarization, and dialogue generation,
under two generation paradigms: fine-tuned small model and few-shot LLM.
Our approach achieves state-of-the-art results in four directions in JRC-Acquis
translation dataset, 50.3 ROUGE-1 in XSum, and 62.9 ROUGE-1 in BigPatent,
demonstrating the potential of self-memory in enhancing retrieval-augmented
generation models. Furthermore, we conduct thorough analyses of each component
in the Selfmem framework to identify current system bottlenecks and provide
insights for future research1.
1 Introduction
In recent years, retrieval-augmented text generation has attracted growing interest across various
fields, including neural machine translation[28, 17, 2], dialogue response generation[81, 6, 46], and
language modeling[36, 77, 19]. This innovative generation paradigm initially equips a fine-tuned
small model or a large language model (LLM) with access to an external database (typically the
training corpus) using information retrieval techniques. Subsequently, the generation process is
conducted based on both the input text and the retrieved memory.
In this paradigm, the guiding principle for memory retrieval is to find the memory that exhibits
the highest similarity to the current input [36, 96, 49]. This aligns with the human intuition that a
more similar demonstration sample typically offers more hints. As demonstrated in Figure 1, for a
retrieval-augmented translation model, the memory similarity alone exhibits a strong correlation with
the final translation quality, regardless of other factors that may influence translation quality (e.g.,
1Code and data available at: https://github.com/Hannibal046/SelfMemory
37th Conference on Neural Information Processing Systems (NeurIPS 2023).
arXiv:2305.02437v3  [cs.CL]  23 Dec 2023



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 2):

retrieval as an intermediate step has proven advantageous for generating informative responses [89,
91, 6, 7]. In-context learning example retrieval also aids in controllable dialogue [ 46]. Other
applications include abstractive summarization [64, 14, 18, 15], code generation [30], paraphrase
generation [34, 83], language modeling [36, 105], counterfactual data generation [24], open domain
question answering [12, 33] and semantic parsing [99].
2.2 Neural Text Reranking
By alleviating the discrepancy between training and inference (i.e., exposure bias) and directly
optimizing desired metrics, two-stage reranking methods have facilitated significant progress in
various text generation tasks. In machine translation, pioneering works by [75] and [61] introduced
and popularized discriminative reranking for SMT. In the context of NMT, research has focused on
two primary reranking approaches: generative reranking [56, 32, 88] and discriminative reranking [39,
71, 23]. For syntactic parsing, [21] were the first to employ a two-stage reranking method to select
outputs from a base parser, while [11] introduced a maximum entropy reranker. In text summarization,
RefSum [53] proposed a second-stage summarization framework to address train-test distribution
mismatches. SimCLS [ 54] used pairwise Learning To Rank (LTR) to select candidates with the
highest matching scores. SummaReranker [68] adopted a multi-task mixture-of-experts framework
to leverage different metrics capturing various aspects of generated candidates. BRIO [55] reused
the base model for a second round of fine-tuning with both cross-entropy loss and a candidate-level
ranking loss. JGR [76] employed an alternate training paradigm to train the generator and reranker.
A key limitation of these reranking methods is that they only represent a one-way process, wherein the
selected candidates become the system’s final output. In contrast, our framework innovatively utilizes
the chosen candidates as memory for the subsequent generation round of a retrieval-augmented
generator, which can produce better candidates with enhanced memory.
3 Methods
In this section, we begin with a motivating experiment on generation as memory(§ 3.1). Then, we
introduce Selfmem, a framework comprising a retrieval-augmented generator(§ 3.2) and a memory
selector (§ 3.3). The complete framework and algorithm are illustrated in Figure 2 and Algorithm 1.
3.1 Generation as Memory
The primary motivation behind our framework stems from the observation that the memory, which is
more similar in distribution to the data during inference, is not the training data (38.89 BLEU, as
shown in the first row of Table 1). Instead, it is the model’s own output (58.58 BLEU) within the
unbounded generation space. One interesting exploration involves directly utilizing the generated
output as memory in relation to the primal problem: better memory prompts better generation.
Table 1: Experiments on the relation between mem-
ory quality and the final hypothesis quality, measured
by the BLEU score with ground truth translation. The
retrieval-augmented translator keeps fixed while the
memory is obtained from different sources.
Memory Source Memory Quality Hypothesis Quality
Retrieval 38.89 58.58
Beam 58.58 58.43
Reference 100 90.43
Random 1.14 49.08
We conduct experiments on the JRC-Acquis
En→De dataset. The first row in Table 1
represents conventional retrieval-augmented
training with retrieved memory and achieves
a 58.58 BLEU score. However, directly in-
corporating beam output of this trained model
as memory (Beam) back into the generation
model does not yield any improvements (row
2), despite its higher similarity to the reference
compared to the retrieved ones. We hypoth-
esize two potential reasons for this: (1) the
retrieval-augmented generator may not gen-
eralize effectively in this context due to the
memory distribution shift (from 38.89 to 58.58), and (2) the beam memory does not offer any
information gain compared to the retrieved one, even it exhibits more overlap with the references.
To investigate the first hypothesis, we conduct experiments under the oracle and random scenarios by
using the reference as memory (Reference) and randomly sampled sentences as memory (Random).
The result is shown in Table 1 and it illustrates that a retrieval-augmented generator (trained with
3



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 4):

And the decoder would incorporate H by attention mechanism and generate tokens in an auto-
regressive manner:
hi = Decoder(CrossAttn(H), y<i) PGξ (·|x, y<i) = Softmax(hi) (3)
Dual-Encoder Instead of treating x and m as a long sequence, this architecture has two encoders,
one for x and the other for m. Their outputs are sequentially attended by the decoder with dual cross
attention as in [17]:
Hx = SourceEncoder(x) Hm = MemoryEncoder(m) (4)
hi = Decoder(CrossAttn(Hx, Hm), y<i) (5)
We use Transformer [84] as the building block for both architectures and optimize Gξ with NLL loss:
Lnll = −
|y|X
t=1
log PGξ (yt|x, m, y<t) (6)
3.3 Memory Selector
The role of memory selector Sθ(x, c), parameterized by θ, is to select one candidate c from the
candidate pool C generated by Gξ based on a specific metric ∆(·, ·). The chosen candidate c is
then utilized as memory m for the subsequent generation round of Gξ. As discussed in §3.1, using
pGξ (y|x) as the metric ∆(·, ·) would result in falling into the confidence region of Gξ, leading to
no information gain. Moreover, a larger value of pGξ (y|x) does not necessarily guarantee improved
generation quality [ 59]. Consequently, we define ∆(·, ·) as model-free metrics that are widely
employed for assessing generation quality, such as BLEU for Neural Machine Translation (NMT)
and ROUGE for Summarization. Our memory selector takes the concatenation of the source x and
candidate ci as input, and produces a multinomial distribution pSθ (·|x) over C.
In this paper, we focus on the role of the memory selector, Sθ(x, c), which is parameterized by θ.
The objective of this selector is to choose a single candidate c from the candidate pool C, generated
by Gξ, based on a specific metric, ∆(·, ·).
pSθ (ci|x) = exp(Sθ(x [SEP] ci))
P|C|
j=1 exp(Sθ(x [SEP] cj))
(7)
In accordance with [39], the training goal for Sθ is to minimize the discrepancy between the Sθ’s
predictions and the scores determined by ∆(·, ·). This divergence is quantified using the Kullback-
Leibler (KL) divergence.
Lkl = −
|C|X
i=1
pM (ci)logpSθ (ci|x) where pM (ci) = exp(∆(ci, y)/τ)
P|C|
j=1 exp(∆(cj, y)/τ)
(8)
τ is the temperature to control the smoothness of the distribution. At inference, the output of the Sθ
is arg max
ci∈C
pSθ (ci|x).
3.4 Combine Generator and Selector
We define two generation modes forGξ. The first mode, referred to as the hypothesis mode, generates
a single output for each input, which is utilized for system evaluation. The second mode, known as
the candidate mode, produces N outputs for a given input, and is employed for training Sθ as well as
memory selection. By integrating two modes together, we present the complete framework of our
proposed model, Selfmem, as illustrated in Algorithm 1.
4 Experimental Setup
4.1 Dataset
We assess the performance of Selfmem on three generation tasks, utilizing a total of seven datasets.
Translation. We evaluate our framework on JRC-Acquis datasets [82], a collection of parallel
5



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 3):

Target Distribution
 Frozen LLM / Trainable LM
NLL Loss
KL Loss
Y N 
Y 1 
... 
Y 
X 
 Y 
X 
candidates 
source 
target training 
memory 
... ... 
... ... 
(a) Retrieval-augmented Generator (b) Memory Selector
Retrieval
Predicted Distribution
M 
Primal
Dual
Figure 2: Overall framework. There are two components in Selfmem, a retrieval-augmented genera-
tor (a) and a memory selector (b). For the primal problem, (a) takes source and memory as input to
generate candidates for (b). For the dual problem, (b) takes as input source and generated candidates
to select memory for (a).
retrieved memory) has already learned to discriminate between different memories in both oracle and
random scenarios, without updating the model weights.
To evaluate the second conjecture, we first define the token sets of the reference, retrieved memory,
and beam memory as R, M, and B, respectively. The overlap token set, denoted by O, is defined
as the tokens that overlap with the references in the beam memory but not in the retrieved memory,
which is represented as R ∩ B − R ∩ M. O is considered as the additional information provided
by the beam memory. Inspired by the confidence analysis of NMT model [58], we compute the set
confidence score, ψ (·), as follows:
ψ (·) = 1
| · |
X
y i∈·
p (y i |x, y <i ) (1)
where p (y i |x, y <i ) is defined by the generation model. ψ (·) measures the confidence with which the
generation model generates the tokens. The value of ψ (R) is 0.58, while that of O is 0.76, indicating
that the generator is relatively confident in generating tokens in O, and therefore does not need to
resort to external memory [38]. Beam search ranks generated candidates based on p (y |x ), where the
selected memory falls within the confidence region of the generator and consequently provides no
information gain. This observation motivates us to select memory according to metrics other than
p (y |x ) in the memory selector (§3.3).
3.2 Retrieval-augmented Generator
Given a text pair (x, y ), where x = {x1, ..., x|x |} is the source, y = {y1, ..., y|y |} is the target. They
could be (document, summary) in summarization, (context, response) in dialogue generation or
(source, target) in machine translation. The retrieval-augmented generation would first use x to
retrieve memory m from datastore D. Then the generator G ξ (x, m ), parameterized by ξ , would take
both x and m as input to generate the target sentence y . In this paper, following standard practice,
we choose the training set as D = {(x i , y i )}|D|
i =1. For LLM as G ξ , we use the standard in-context
learning format to give (x, y ) as demonstration example. For tunable generator G ξ , we only keep the
target side of top-1 retrieval results as memory and we consider two commonly used architectures:
Joint-Encoder [29, 87, 41] and Dual-Encoder [92, 8, 17].
Joint-Encoder This architecture is the standard encoder-decoder-based model [3, 84]. The input is
the concatenation of x and m . The encoder would first map the input into the hidden states H :
H = Encoder(x [SEP] m ) (2)
4



### Claim 27/179

#### Claim Text
Coarse-grained retrieval units theoretically can provide more relevant information for the problem, but they may also contain redundant content, which could distract the retriever and language models in downstream tasks [50], [87].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[30]_2312.06648.pdf (Page 0):

EMNLP 2024 Main Conference
Dense XRetrieval: What Retrieval Granularity Should We Use?
Tong Chen♣* Hongwei Wang♢ Sihao Chen♡ Wenhao Yu♢
Kaixin Ma♢ Xinran Zhao♠ Hongming Zhang♢ Dong Yu♢
♣University of Washington ♢Tencent AI Lab
♡University of Pennsylvania ♠Carnegie Mellon University
Abstract
Dense retrieval has become a prominent
method to obtain relevant context or world
knowledge in open-domain NLP tasks. When
we use a learned dense retriever on a retrieval
corpus at inference time, an often-overlooked
design choice is the retrieval unit in which the
corpus is indexed, e.g. document, passage, or
sentence. We discover that the retrieval unit
choice significantly impacts the performance of
both retrieval and downstream tasks. Distinct
from the typical approach of using passages or
sentences, we introduce a novel retrieval unit,
proposition, for dense retrieval. Propositions
are defined as atomic expressions within text,
each encapsulating a distinct factoid and pre-
sented in a concise, self-contained natural lan-
guage format. We conduct an empirical com-
parison of different retrieval granularity. Our
experiments reveal that indexing a corpus by
fine-grained units such as propositions signif-
icantly outperforms passage-level units in re-
trieval tasks. Moreover, constructing prompts
with fine-grained retrieved units for retrieval-
augmented language models improves the per-
formance of downstream QA tasks given a spe-
cific computation budget.
1 Introduction
Dense retrievers are a popular class of techniques
for accessing external information sources for open-
domain NLP tasks (Karpukhin et al., 2020). Before
we use a learned dense retriever to retrieve from a
corpus, an imperative design decision we have to
make is the retrieval unit – i.e. the granularity at
which we segment and index the retrieval corpus
for inference. In practice, the choice of retrieval
units, e.g. documents, fixed-length passage chunks
or sentences, etc, is usually pre-determined based
* Work was done during internship at Tencent AI Lab,
Bellevue.
/githubhttps://github.com/chentong0/
factoid-wiki
Question: What is the angle of the Tower of Pisa?
Passage
Retrieval
Prior to restoration work performed be-
tween 1990 and 2001, the tower leaned at
an angle of 5.5 degrees, but the tower now
leans at about 3.99 degrees. This means
the top of the Leaning Tower of Pisa is dis-
placed horizontally 3.9 meters (12 ft 10 in)
from the center.
Sentence
Retrieval
Prior to restoration work performed be-
tween 1990 and 2001, the tower leaned at
an angle of 5.5 degrees, but the tower now
leans at about 3.99 degrees.
Proposition
Retrieval
The Leaning Tower of Pisa now leans at
about 3.99 degrees.
Contriever GTR
35
45
55
65
75Recall@5 (%)43.0
65.2
47.3
66.7
52.7
68.0
Passage Retrieval
Contriever GTR
25
30
35
40
45EM@500 (%)
34.1
38.5
36.2
40.1
37.3
41.3
Open-domain QA
Passage Sentence Proposition
Figure 1: ( Top) An example of three granularities of
retrieval units of Wikipedia text when using dense re-
trieval. (Bottom) We observe that retrieving by proposi-
tions yields the best retrieval performance in both pas-
sage retrieval task and downstream open-domain QA
task, e.g. with Contriever (Izacard et al., 2022) or GTR
(Ni et al., 2022) as the backbone retriever. Highlight in-
dicates the part that contains the answer to the question.
on how the dense retrieval model is instantiated
or trained (Lewis et al., 2020; Lee et al., 2021a;
Santhanam et al., 2022; Ni et al., 2022).
In this paper, we investigate an overlooked re-
search question with dense retrieval inference – at
what retrieval granularity should we segment and
index the retrieval corpus? We aim to investigate
this question in two aspects.
• First, we examine how the granularity of the
index affects passage retrieval performance.
• Second, we investigate whether fine-grained units
1
arXiv:2312.06648v3  [cs.CL]  4 Oct 2024



Source: data\tc16_2312.10997v5\referenced_papers\[30]_2312.06648.pdf (Page 6):

stream QA performance, we extract the answer
from the retrieved passage by a QA reader, Fusion-
in-decoder. The results are shown in Table 4.
Retrieval by proposition-level index achieves the
highest average exact match (EM) on all four re-
triever models. Apart from limited exceptions, the
proposition-level index achieves the highest EM
for most retrieval tasks and on most datasets. We
observe that the trend of downstream QA perfor-
mance is highly consistent with passage retrieval
recall, suggesting higher passage recall implies bet-
ter downstream QA performance.
6 How Does Granularity Influence
Retrieval-Augmented LMs?
In this section, we study how the choice of different
granularity used in the prompts affects the retrieval-
augmented generation across open-domain QA
tasks. To fairly compare different granularity with
the same computation budget, we limit the num-
ber of retrieved tokens for input to the language
model at l = 100or 500 tokens. Our results sug-
gest that retrieval by finer-grained units enables a
higher density of question-related information in
the prompts, leading to better performance.
6.1 Open-domain QA Performance
Table 5 shows the evaluation results with LLaMA-
2-7B as the language model. Across different re-
trievers, we observe higher QA performance in
terms of the EM@l metric on average when using
propositions as the retrieval unit.
Using propositions rather than passages in the
prompts, the four dense retrievers—SimCSE, Con-
Retriever, DPR, and GTR—improve by +4.1, +3.2,
+2.7, and +2.8 in the EM@500 score. The improve-
ments for using sentences over passages for the
four retrieval models are +2.4, +2.1, +2, and +1.6,
respectively. It is interesting to note that in the
LLaMA-2-7B model, the QA accuracy on TQA
and WebQ is not sensitive to retrieval type. The
highest improvements over the closed-book setting
are only +4.9 and +3.2, achieved by GTR with
propositions. Nevertheless, we observe that using
sentences and propositions in the prompts results
in higher performance than using passages for all
retrieval models on these two datasets. The results
suggest that using finer-grained units in the prompts
is beneficial to retrieval-augmented generation.
6.2 Finer-grained Granularity ⇒ Higher
Density of Question-Related Information
Intuitively, compared to sentences or passages as
retrieval units, the advantage of propositions is that
the retrieved propositions have a higher density
of relevant information to the query. With finer-
grained retrieval units, the correct answer to the
query would more likely appear in the top- l re-
trieved words by a dense retriever.
We illustrate this phenomenon by an analysis
shown in Figure 4. Here, we investigate the posi-
tion at which the ground truth answer appears in
the top-l retrieved words. Specifically, we calcu-
late the recall of the gold answer within the initial l
retrieved words with GTR working with Wikipedia
indexed in three different granularities.
We show the results in Figure 4 and 7 with l
ranging from 0 to 500 across all five datasets. For
a fixed word retrieval budget, proposition retrieval
shows a higher success rate than sentence and pas-
sage retrieval methods. The largest improvement of
proposition retrieval over passage retrieval occurs
within the range of 100-200 words, which corre-
sponds to roughly 10 propositions, 5 sentences, or
2 passages. As word count increases, the recall rate
of the three granularities converges, encompassing
all relevant information.
7 Related Work
Recent works on dense retrievers typically adopt
a dual-encoder architecture (Yih et al., 2011;
Reimers and Gurevych, 2019; Karpukhin et al.,
2020; Ni et al., 2022). With dual-encoders,
each query and document is encoded into a low-
dimensional feature vector respectively, and their
relevance is measured by a non-parametric similar-
ity function between the embedding vectors (Muss-
mann and Ermon, 2016). Due to the limited expres-
sivity from the similarity function, dual encoder
models often generalize poorly to new tasks with
scarce training data (Thakur et al., 2021). Previous
studies use techniques such as data augmentation
(Wang et al., 2022; Yu et al., 2023a; Izacard et al.,
2022; Gao and Callan, 2022; Lin et al., 2023; Dai
et al., 2023), continual pre-training (Chang et al.,
2020; Sachan et al., 2021; Oguz et al., 2022), task-
aware training (Xin et al., 2022; Cheng et al., 2023),
hybrid sparse-dense retrieval (Luan et al., 2021;
Chen et al., 2022), or mixed strategy retrieval (Ma
et al., 2022, 2023) and so on to improve cross-task
generalization performance of dense retrievers.
7



Source: data\tc16_2312.10997v5\referenced_papers\[30]_2312.06648.pdf (Page 7):

Retriever Granularity
NQ TQA WebQ SQuAD EQ Avg.
EM EM EM EM EM EM
@100 @500@100 @500@100 @500@100 @500@100 @500@100 @500
Closed-book 23.4 57.4 25.9 13.0 23.2 28.6
Unsupervised Dense Retrievers
SimCSE Passage 20.5 22.9 49.7 52.9 24.5 24.6 13.7 16.6 20.7 25.5 25.8 28.5
Sentence 21.1 24.3 52.1 54.2 24.2 26.1 17.7 21.5 22.9 28.3 27.6 30.9
Proposition22.0 26.0 51.0 53.9 23.5 27.0 18.6 22.7 25.9 33.6 28.2 32.6
Contriever Passage 24.5 28.7 54.7 57.9 25.7 26.9 17.7 24.2 25.6 32.5 29.6 34.1
Sentence 25.0 30.2 56.3 59.2 26.8 29.2 22.5 28.1 26.1 34.1 31.3 36.2
Proposition25.8 30.3 56.8 60.0 26.8 29.9 24.8 29.7 27.1 36.5 32.3 37.3
Supervised Dense Retrievers
DPR Passage 30.6 33.7 56.5 60.3 25.0 26.8 14.2 18.9 26.4 31.6 30.6 34.3
Sentence 32.5 34.1 58.3 61.7 25.4 28.0 17.6 22.1 29.8 35.6 32.7 36.3
Proposition31.5 33.8 57.6 60.6 27.1 28.2 18.2 22.6 32.9 39.7 33.5 37.0
GTR Passage 30.0 33.9 56.9 60.0 24.5 25.9 21.5 27.4 42.2 45.3 35.0 38.5
Sentence 30.9 34.0 58.9 61.9 24.5 27.0 29.8 31.7 42.9 45.9 37.4 40.1
Proposition32.1 33.8 58.8 62.3 25.7 29.1 32.5 33.1 43.0 48.1 38.4 41.3
Table 5: Open-domain QA performance (EM = Exact Match) with LLaMA-2-7B model (Touvron et al., 2023). The
context in the prompts is constructed by passage, sentence, or propositions limiting at l = 100or 500 tokens. We
prompt the LLaMA-2-7B model with four-shot demonstrations for each test case.
0 200 400
#Words
50
60
70Recall (%)
GTR / NQ
0 200 400
#Words
60
70Recall (%)
GTR / TQA
0 200 400
#Words
50
60
70Recall (%)
GTR / WebQ
0 200 400
#Words
40
50
60Recall (%)
GTR / SQuAD
0 200 400
#Words
60
70
80Recall (%)
GTR / EQ
Passage Sentence Proposition
Figure 4: Recall of the gold answer in the retrieved text limited to first k words for the GTR retriever. Finer-grained
retrieval has a higher recall across all numbers of words.
The motivation of our work echoes in part with
multi-vector retrieval, e.g. ColBERT (Khattab and
Zaharia, 2020), DensePhrase (Lee et al., 2021a,b),
ME-BERT (Luan et al., 2021), and MVR (Zhang
et al., 2022), where the retrieval model learns to
encode a candidate retrieval unit into multiple vec-
tors to increase model expressivity and improve
retrieval granularity (Seo et al., 2019; Humeau
et al., 2019). Our work instead focuses on the
setting where we do not update the dense retriever
model or its parameters. We show that indexing
the retrieval corpus by different granularity can be
a simple and orthogonal strategy for improving the
generalization of dense retrievers at inference time.
In line with generating retrieval units from the
original corpus, Sarthi et al. (2024) propose using
generative summaries as additional retrieval units
alongside the original text, enhancing queries with
document-level understanding. In contrast, our
work generates propositions to improve queries
related to long-tailed entities. These approaches are
complementary, as they address different aspects
of retrieval enhancement.
The use of propositions as a unit of text rep-
resentation dates back to the Pyramid method in
summarization evaluation (Nenkova and Passon-
neau, 2004), where a model-generated summary
is evaluated by each proposition. Proposition ex-
traction from text has been a long-standing task,
with earlier formulations focusing on a structured
representation of propositions (Etzioni et al., 2008;
Gildea and Jurafsky, 2000). More recent studies
have found success in extracting free-text propo-
sitions via few-shot prompting with LLMs (Min
et al., 2023; Kamoi et al., 2023), or fine-tuning
compact-sized models (Chen et al., 2023b).
Retrieve-then-read, or more broadly retrieval
augmented generation, has recently emerged as
a popular paradigm for open-domain question an-
swering (Lewis et al., 2021; Jiang et al., 2023; Asai
et al., 2023). While earlier works provide up to
the top 100 retrieved passages for the downstream
8



Source: data\tc16_2312.10997v5\referenced_papers\[42]_2208.03299.pdf (Page 6):

updated, as the true top-k documents may not be retrieved in the top-L results from the stale index. In
practice, it is possible to track the positions of the top-K re-ranked documents in the top-L, and estimate
when the index needs to be updated.
Query-side ﬁne-tuning. Finally, the last strategy is to decouple the encoding of the queries and documents.
In this case, we ﬁx the parameters corresponding to the document encoder, and only train the parameters
corresponding to the query encoder. Thus, the embeddings of documents are ﬁxed, and we do not need to
refresh the index, and thus there is no computational overhead. As we will see in practice, the impact of
ﬁxing the documents encoder varies greatly for diﬀerent tasks when a large training dataset is available. For
most of the few-shot settings that we consider, query-side ﬁnetuning does not have large performance impact,
and sometimes even slightly improves performance.
3 Related work
3.1 Retrieval in natural language processing
Retrieval for knowledge intensive tasks.Previous work has shown that retrieval improves performance
across a variety of tasks such as question answering (Voorhees et al., 1999; Chen et al., 2017; Kwiatkowski et al.,
2019), fact checking (Thorne et al., 2018), dialogue (Dinan et al., 2019) or citation recommendation (Petroni
et al., 2022). Historically, this information retrieval step was implemented using term-matching methods, such
as TF-IDF or BM25 (Jones, 1972; Robertson et al., 1995). For open-domain question answering (Voorhees
et al., 1999), documents are often retrieved from Wikipedia (Chen et al., 2017). Recently, dense retrievers
based on neural networks have become popular. These usually follow a dual-encoder architecture (Yih et al.,
2011; Huang et al., 2013; Shen et al., 2014), where queries and passages are encoded independently as vectors,
and relevance is computed using the inner product or Euclidean distance. Popular supervised retrievers
include DPR (Karpukhin et al., 2020), which is trained to discriminate the relevant passage among negative
passages, and extensions such as ANCE (Xiong et al., 2020) which improved the hard negatives mining
process. We refer the reader to Yates et al. (2021) for a survey of dense retrieval techniques.
After retrieval, the relevant documents are processed to produce the ﬁnal output. In open-domain QA, models
can extract a span of text from retrieved documents as the answer (Chen et al., 2017; Clark & Gardner, 2018;
Wang et al., 2019; Karpukhin et al., 2020), a method inspired by reading comprehension (Richardson, 2013;
Rajpurkar et al., 2016). Recently, generating the answer as free-form text, using a seq2seq model conditioned
on retrieved documents have become prevalent (Lewis et al., 2020; Izacard & Grave, 2020; Min et al., 2020).
These architectures have also been shown to reduce hallucination in dialogue agents (Shuster et al., 2021).
Retriever training. The need for expensive query-document annotations for training the retriever can be
bypassed, by leveraging signals from the language model, or using unsupervised learning. REALM (Guu et al.,
2020) and RAG (Lewis et al., 2020) jointly train the retriever and language model by modelling documents as
latent variable, and minimizing the objective with gradient descent. REALM pre-trains end-to-end with an
MLM approach but uses an extractive BERT-style model (Devlin et al., 2019). Guu et al. (2020) also explore
a query-side ﬁnetuning at ﬁnetuning time to avoid index refreshes, which is also explored in the context of
phrase-based retrieval by Lee et al. (2021b). Izacard & Grave (2020) proposed to use cross-attention scores
as supervision with knowledge distillation. Sachan et al. (2021) perform joint training of the reader and the
retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee
et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention
scores from the language model. Theinverse cloze taskwas proposed by Lee et al. (2019) to pre-train dense
retrievers in an unsupervised way. Paranjape et al. (2021) propose a method to train retrieval-augmented
generators using a second “informed” retriever with access to the output, which the test-time retriever can be
distilled from, and Hofstätter et al. (2022) recently proposed a training set ﬁltering/weighting approach to
train stronger retrieval-augmented generators. Izacard et al. (2022) explored diﬀerent contrastive learning
methods to train retrievers, while Ram et al. (2022) used recurring spans within a document to create
pseudo-positive query-document pairs.
7



Source: data\tc16_2312.10997v5\referenced_papers\[30]_2312.06648.pdf (Page 4):

Retriever Granularity NQ TQA WebQ SQuAD EQ Avg.
R@5 R@20 R@5 R@20 R@5 R@20 R@5 R@20 R@5 R@20 R@5 R@20
Unsupervised Dense Retrievers
SimCSE Passage 28.8 44.3 44.9 59.4 39.8 56.0 29.5 45.5 28.4 40.3 34.3 49.1
Sentence 35.5 53.1 50.5 64.3 45.3 64.1 37.1 52.3 36.3 50.1 40.9 56.8
Proposition41.1 58.9 52.4 66.5 50.0 66.8 38.7 53.9 49.5 62.2 46.3 61.7
Contriever Passage 42.5 63.8 58.1 73.7 37.1 60.6 40.8 59.8 36.3 56.3 43.0 62.8
Sentence 46.4 66.8 60.6 75.7 41.7 63.1 45.1 63.5 42.7 61.3 47.3 66.1
Proposition50.1 70.0 65.1 77.9 45.9 66.8 50.7 67.7 51.7 70.1 52.7 70.5
Supervised Dense Retrievers
DPR Passage 66.0 78.0 71.6 80.2 62.9 74.9 38.3 53.9 47.5 60.4 57.3 69.5
Sentence 66.0 78.0 71.8 80.5 64.1 74.4 40.3 55.9 53.7 66.0 59.2 71.0
Proposition65.4 77.7 70.7 79.6 62.8 75.1 41.4 57.2 59.4 71.3 59.9 72.2
GTR Passage 66.3 78.4 70.1 79.4 63.3 76.5 54.4 68.1 71.7 80.5 65.2 76.6
Sentence 66.4 79.4 71.6 80.9 62.2 76.8 60.9 73.4 72.5 81.3 66.7 78.4
Proposition66.5 79.6 72.2 80.9 63.2 77.4 63.3 75.0 74.9 83.0 68.0 79.2
Table 3: Passage retrieval performance (Recall@ k = 5, 20) on five different open-domain QA datasets when
pre-trained dense retrievers work with the three different granularity from the retrieval corpus. Underline denotes
cases where the training split of the target dataset was included in the training data of the dense retriever.
prompts under the same computation budget, we
set a token length limit for retrieved units.
For this reason, we follow an evaluation setup
where the maximum number of retrieved tokens
is capped at l = 100 or 500, i.e. only the top
l tokens from passage, sentence, or proposition
level retrieval are fed into the language model as
input. We evaluate the percentage of questions for
which the predicted answer exactly matches (EM)
the ground truth. We denote our metric as EM @
l tokens. We use LLaMA-2-7B (Touvron et al.,
2023) in our evaluation. To ensure the model’s out-
put aligns with the format of each dataset, we em-
ploy in-context learning, incorporating four-shot
demonstrations as illustrated in Figure 9.
5 How Does Granularity Influence
Passage Retrieval?
In this section, we report and discuss how index-
ing the corpus at various granularity influences the
passage retrieval performance. Surprisingly, de-
spite all of the dense retrieval models being trained
on only passage-level documents, all the models
demonstrate on-par or superior performance when
the corpus is indexed at the proposition level. Our
results suggest that indexing the corpus at the finer-
grained units improves the cross-task generaliza-
tion on passage retrieval.
5.1 Passage Retrieval Performance
We report our evaluation results in Table 3. We
observe that retrieval by propositions outperforms
retrieval by sentences or passages on most tasks for
both unsupervised and supervised retrievers.
With all dense retrievers tested, proposition-
level retrieval consistently outperforms sentence
and passage-level retrieval on average across the
five datasets. With the unsupervised retrievers, i.e.
SimCSE and Contriever, we see an averaged Re-
call@5 improvement of +12.0 and +9.3 (35.0%
and 22.5% relative improvement) on five datasets.
With the supervised retrievers, proposition-level
retrieval still shows an advantage on average, yet
the sizes of improvements are smaller. We hypothe-
size that this is due to these retrievers being trained
on query-passage pairs. For instance, with DPR,
which have been trained on NQ, TQA, WebQ, and
SQuAD, we observe that proposition and sentence
level retrieval perform slightly worse compared to
passage level on three out of the four datasets, with
the exception of SQuAD. As shown in Table 3, all
supervised retrievers demonstrate comparable per-
formance across three levels of retrieval granularity
in NQ, TQA, and WebQ.
However, on datasets that the retriever model has
not seen during training, we observe that retrieval
by proposition demonstrates a clear advantage. For
instance, most notably on SQuAD or EntityQues-
tions, we observe that proposition-based retrieval
significantly outperforms the other two granulari-
ties. We see 25% Recall@5 relative improvement
on EntityQuestions with relatively weak retrievers
like DPR. Furthermore, the Recall@5 of retrieval
by proposition on SQuAD improved most on GTR,
5



### Claim 28/179

#### Claim Text
Among them, DenseX [30]proposed the concept of using propositions as retrieval units.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[30]_2312.06648.pdf (Page 7):

Retriever Granularity
NQ TQA WebQ SQuAD EQ Avg.
EM EM EM EM EM EM
@100 @500@100 @500@100 @500@100 @500@100 @500@100 @500
Closed-book 23.4 57.4 25.9 13.0 23.2 28.6
Unsupervised Dense Retrievers
SimCSE Passage 20.5 22.9 49.7 52.9 24.5 24.6 13.7 16.6 20.7 25.5 25.8 28.5
Sentence 21.1 24.3 52.1 54.2 24.2 26.1 17.7 21.5 22.9 28.3 27.6 30.9
Proposition22.0 26.0 51.0 53.9 23.5 27.0 18.6 22.7 25.9 33.6 28.2 32.6
Contriever Passage 24.5 28.7 54.7 57.9 25.7 26.9 17.7 24.2 25.6 32.5 29.6 34.1
Sentence 25.0 30.2 56.3 59.2 26.8 29.2 22.5 28.1 26.1 34.1 31.3 36.2
Proposition25.8 30.3 56.8 60.0 26.8 29.9 24.8 29.7 27.1 36.5 32.3 37.3
Supervised Dense Retrievers
DPR Passage 30.6 33.7 56.5 60.3 25.0 26.8 14.2 18.9 26.4 31.6 30.6 34.3
Sentence 32.5 34.1 58.3 61.7 25.4 28.0 17.6 22.1 29.8 35.6 32.7 36.3
Proposition31.5 33.8 57.6 60.6 27.1 28.2 18.2 22.6 32.9 39.7 33.5 37.0
GTR Passage 30.0 33.9 56.9 60.0 24.5 25.9 21.5 27.4 42.2 45.3 35.0 38.5
Sentence 30.9 34.0 58.9 61.9 24.5 27.0 29.8 31.7 42.9 45.9 37.4 40.1
Proposition32.1 33.8 58.8 62.3 25.7 29.1 32.5 33.1 43.0 48.1 38.4 41.3
Table 5: Open-domain QA performance (EM = Exact Match) with LLaMA-2-7B model (Touvron et al., 2023). The
context in the prompts is constructed by passage, sentence, or propositions limiting at l = 100or 500 tokens. We
prompt the LLaMA-2-7B model with four-shot demonstrations for each test case.
0 200 400
#Words
50
60
70Recall (%)
GTR / NQ
0 200 400
#Words
60
70Recall (%)
GTR / TQA
0 200 400
#Words
50
60
70Recall (%)
GTR / WebQ
0 200 400
#Words
40
50
60Recall (%)
GTR / SQuAD
0 200 400
#Words
60
70
80Recall (%)
GTR / EQ
Passage Sentence Proposition
Figure 4: Recall of the gold answer in the retrieved text limited to first k words for the GTR retriever. Finer-grained
retrieval has a higher recall across all numbers of words.
The motivation of our work echoes in part with
multi-vector retrieval, e.g. ColBERT (Khattab and
Zaharia, 2020), DensePhrase (Lee et al., 2021a,b),
ME-BERT (Luan et al., 2021), and MVR (Zhang
et al., 2022), where the retrieval model learns to
encode a candidate retrieval unit into multiple vec-
tors to increase model expressivity and improve
retrieval granularity (Seo et al., 2019; Humeau
et al., 2019). Our work instead focuses on the
setting where we do not update the dense retriever
model or its parameters. We show that indexing
the retrieval corpus by different granularity can be
a simple and orthogonal strategy for improving the
generalization of dense retrievers at inference time.
In line with generating retrieval units from the
original corpus, Sarthi et al. (2024) propose using
generative summaries as additional retrieval units
alongside the original text, enhancing queries with
document-level understanding. In contrast, our
work generates propositions to improve queries
related to long-tailed entities. These approaches are
complementary, as they address different aspects
of retrieval enhancement.
The use of propositions as a unit of text rep-
resentation dates back to the Pyramid method in
summarization evaluation (Nenkova and Passon-
neau, 2004), where a model-generated summary
is evaluated by each proposition. Proposition ex-
traction from text has been a long-standing task,
with earlier formulations focusing on a structured
representation of propositions (Etzioni et al., 2008;
Gildea and Jurafsky, 2000). More recent studies
have found success in extracting free-text propo-
sitions via few-shot prompting with LLMs (Min
et al., 2023; Kamoi et al., 2023), or fine-tuning
compact-sized models (Chen et al., 2023b).
Retrieve-then-read, or more broadly retrieval
augmented generation, has recently emerged as
a popular paradigm for open-domain question an-
swering (Lewis et al., 2021; Jiang et al., 2023; Asai
et al., 2023). While earlier works provide up to
the top 100 retrieved passages for the downstream
8



Source: data\tc16_2312.10997v5\referenced_papers\[30]_2312.06648.pdf (Page 0):

EMNLP 2024 Main Conference
Dense XRetrieval: What Retrieval Granularity Should We Use?
Tong Chen♣* Hongwei Wang♢ Sihao Chen♡ Wenhao Yu♢
Kaixin Ma♢ Xinran Zhao♠ Hongming Zhang♢ Dong Yu♢
♣University of Washington ♢Tencent AI Lab
♡University of Pennsylvania ♠Carnegie Mellon University
Abstract
Dense retrieval has become a prominent
method to obtain relevant context or world
knowledge in open-domain NLP tasks. When
we use a learned dense retriever on a retrieval
corpus at inference time, an often-overlooked
design choice is the retrieval unit in which the
corpus is indexed, e.g. document, passage, or
sentence. We discover that the retrieval unit
choice significantly impacts the performance of
both retrieval and downstream tasks. Distinct
from the typical approach of using passages or
sentences, we introduce a novel retrieval unit,
proposition, for dense retrieval. Propositions
are defined as atomic expressions within text,
each encapsulating a distinct factoid and pre-
sented in a concise, self-contained natural lan-
guage format. We conduct an empirical com-
parison of different retrieval granularity. Our
experiments reveal that indexing a corpus by
fine-grained units such as propositions signif-
icantly outperforms passage-level units in re-
trieval tasks. Moreover, constructing prompts
with fine-grained retrieved units for retrieval-
augmented language models improves the per-
formance of downstream QA tasks given a spe-
cific computation budget.
1 Introduction
Dense retrievers are a popular class of techniques
for accessing external information sources for open-
domain NLP tasks (Karpukhin et al., 2020). Before
we use a learned dense retriever to retrieve from a
corpus, an imperative design decision we have to
make is the retrieval unit – i.e. the granularity at
which we segment and index the retrieval corpus
for inference. In practice, the choice of retrieval
units, e.g. documents, fixed-length passage chunks
or sentences, etc, is usually pre-determined based
* Work was done during internship at Tencent AI Lab,
Bellevue.
/githubhttps://github.com/chentong0/
factoid-wiki
Question: What is the angle of the Tower of Pisa?
Passage
Retrieval
Prior to restoration work performed be-
tween 1990 and 2001, the tower leaned at
an angle of 5.5 degrees, but the tower now
leans at about 3.99 degrees. This means
the top of the Leaning Tower of Pisa is dis-
placed horizontally 3.9 meters (12 ft 10 in)
from the center.
Sentence
Retrieval
Prior to restoration work performed be-
tween 1990 and 2001, the tower leaned at
an angle of 5.5 degrees, but the tower now
leans at about 3.99 degrees.
Proposition
Retrieval
The Leaning Tower of Pisa now leans at
about 3.99 degrees.
Contriever GTR
35
45
55
65
75Recall@5 (%)43.0
65.2
47.3
66.7
52.7
68.0
Passage Retrieval
Contriever GTR
25
30
35
40
45EM@500 (%)
34.1
38.5
36.2
40.1
37.3
41.3
Open-domain QA
Passage Sentence Proposition
Figure 1: ( Top) An example of three granularities of
retrieval units of Wikipedia text when using dense re-
trieval. (Bottom) We observe that retrieving by proposi-
tions yields the best retrieval performance in both pas-
sage retrieval task and downstream open-domain QA
task, e.g. with Contriever (Izacard et al., 2022) or GTR
(Ni et al., 2022) as the backbone retriever. Highlight in-
dicates the part that contains the answer to the question.
on how the dense retrieval model is instantiated
or trained (Lewis et al., 2020; Lee et al., 2021a;
Santhanam et al., 2022; Ni et al., 2022).
In this paper, we investigate an overlooked re-
search question with dense retrieval inference – at
what retrieval granularity should we segment and
index the retrieval corpus? We aim to investigate
this question in two aspects.
• First, we examine how the granularity of the
index affects passage retrieval performance.
• Second, we investigate whether fine-grained units
1
arXiv:2312.06648v3  [cs.CL]  4 Oct 2024



Source: data\tc16_2312.10997v5\referenced_papers\[30]_2312.06648.pdf (Page 1):

1. Prior to restoration work performed 
between 1990 and 2001, the Leaning Tower 
of Pisa leaned at an angle of 5.5 degrees.
2. The Leaning Tower of Pisa now leans at 
about 3.99 degrees.
3. The top of the Leaning Tower of Pisa is 
displaced horizontally 3.9 meters (12 ft 10 in) 
from the center.
Prior to restoration work performed between 
1990 and 2001, the tower leaned at an angle of 
5.5 degrees , // but the tower now leans at 
about 3.99 degrees. // This means the top of the 
Learning Tower of Pisa is displaced horizontally 
3.9 meters (12 ft 10 in) from the center. WIkipedia
FactoidWiki
?
 ✓
AnswerQuery
QA Model
Retrieval Units
Retrieval 
Units
Passage Retrieval
B C
D
Propositionizer
?
Query
Corpus
Passages
Sentences
Propositions
Retrieval 
Units
Retriever
A
Figure 2: We discover that segmenting and indexing a retrieval corpus on the proposition level can be a simple yet
effective strategy to increase dense retrievers’ generalization performance at inference time (A, B). We empirically
compare the retrieval and downstream open-domain QA task performance when dense retrievers work with
Wikipedia indexed at the level of 100-word passages, sentences, or propositions (C, D).
can replace passages in downstream QA tasks.
Based on our empirical experiments, we discover
that selecting the proper retrieval granularity at in-
ference time can be a simple yet effective strategy
for improving dense retrievers’ retrieval and down-
stream QA performance. We illustrate our intuition
with an example of open-domain QA in Table 1.
The example shows retrieved text by the same re-
triever at three different granularities. The pas-
sage, which represents a coarser retrieval unit with
a longer context, is theoretically able to provide
more relevant information for the question. How-
ever, a passage often includes extraneous details
(e.g., restoration period and horizontal displace-
ment in the example of Table 1) that could poten-
tially distract both the retriever and the language
model in downstream tasks (Shi et al., 2023; Yu
et al., 2023b). On the other hand, sentence-level in-
dexing provides a finer-grained approach but does
not entirely address the issue (Akkalyoncu Yilmaz
et al., 2019; Yang et al., 2020). This is because
sentences can still be complex and compounded,
and they are often not self-contained, lacking nec-
essary contextual information (e.g., in the example
of Table 1, “the tower” is the coreference of “Pisa
Tower”) for judging the query-document relevance.
To address these shortcomings of typical re-
trieval units such as passages or sentences, we pro-
pose using proposition as a novel retrieval unit for
dense retrieval. Propositions are defined as atomic
expressions within text, where each encapsulates
a distinct factoid and is presented in a concise,
self-contained natural language format. We show
an example proposition in Table 1. The proposi-
tion describes the information regarding the Tower
of Pisa’s current leaning angle in a self-contained
way and precisely responds to what the question
is querying. We provide a more detailed definition
and description of proposition in §2. To validate
the efficacy of using proposition as a retrieval unit
for dense retrievers inference, we first process and
index an English Wikipedia dump with all docu-
ments segmented into propositions, which we refer
to as FACTOID WIKI .
We conduct experiments on five different open-
domain QA datasets and empirically compare the
performance of four dual-encoder retrievers when
Wikipedia is indexed by passages, sentences, and
our proposed propositions. Notably, our findings in-
dicate that proposition-based retrieval outperforms
sentence and passage-based retrieval, especially in
terms of generalization, as discussed in §5. This
suggests that propositions, being both compact and
rich in context, enable dense retrievers to access
precise information while maintaining adequate
context. The average improvement over passage-
based retrieval of Recall@20 is +10.1 on unsuper-
vised dense retrievers and +2.7 on supervised re-
trievers, even though these retrievers were directly
trained on passage-level retrieval. Furthermore,
we observe a distinct advantage of proposition-
based retrieval in downstream QA performance
when using retrieval-augmented language models,
as elaborated in §6. Retrieval by finer-grained units
inherently provides a higher density of question-
relevant information. This finding implies using
finer-grained units in the prompts achieves the same
performance with a shorter input length, and hence,
a faster inference time.
Our main contributions are:
2



Source: data\tc16_2312.10997v5\referenced_papers\[30]_2312.06648.pdf (Page 6):

stream QA performance, we extract the answer
from the retrieved passage by a QA reader, Fusion-
in-decoder. The results are shown in Table 4.
Retrieval by proposition-level index achieves the
highest average exact match (EM) on all four re-
triever models. Apart from limited exceptions, the
proposition-level index achieves the highest EM
for most retrieval tasks and on most datasets. We
observe that the trend of downstream QA perfor-
mance is highly consistent with passage retrieval
recall, suggesting higher passage recall implies bet-
ter downstream QA performance.
6 How Does Granularity Influence
Retrieval-Augmented LMs?
In this section, we study how the choice of different
granularity used in the prompts affects the retrieval-
augmented generation across open-domain QA
tasks. To fairly compare different granularity with
the same computation budget, we limit the num-
ber of retrieved tokens for input to the language
model at l = 100or 500 tokens. Our results sug-
gest that retrieval by finer-grained units enables a
higher density of question-related information in
the prompts, leading to better performance.
6.1 Open-domain QA Performance
Table 5 shows the evaluation results with LLaMA-
2-7B as the language model. Across different re-
trievers, we observe higher QA performance in
terms of the EM@l metric on average when using
propositions as the retrieval unit.
Using propositions rather than passages in the
prompts, the four dense retrievers—SimCSE, Con-
Retriever, DPR, and GTR—improve by +4.1, +3.2,
+2.7, and +2.8 in the EM@500 score. The improve-
ments for using sentences over passages for the
four retrieval models are +2.4, +2.1, +2, and +1.6,
respectively. It is interesting to note that in the
LLaMA-2-7B model, the QA accuracy on TQA
and WebQ is not sensitive to retrieval type. The
highest improvements over the closed-book setting
are only +4.9 and +3.2, achieved by GTR with
propositions. Nevertheless, we observe that using
sentences and propositions in the prompts results
in higher performance than using passages for all
retrieval models on these two datasets. The results
suggest that using finer-grained units in the prompts
is beneficial to retrieval-augmented generation.
6.2 Finer-grained Granularity ⇒ Higher
Density of Question-Related Information
Intuitively, compared to sentences or passages as
retrieval units, the advantage of propositions is that
the retrieved propositions have a higher density
of relevant information to the query. With finer-
grained retrieval units, the correct answer to the
query would more likely appear in the top- l re-
trieved words by a dense retriever.
We illustrate this phenomenon by an analysis
shown in Figure 4. Here, we investigate the posi-
tion at which the ground truth answer appears in
the top-l retrieved words. Specifically, we calcu-
late the recall of the gold answer within the initial l
retrieved words with GTR working with Wikipedia
indexed in three different granularities.
We show the results in Figure 4 and 7 with l
ranging from 0 to 500 across all five datasets. For
a fixed word retrieval budget, proposition retrieval
shows a higher success rate than sentence and pas-
sage retrieval methods. The largest improvement of
proposition retrieval over passage retrieval occurs
within the range of 100-200 words, which corre-
sponds to roughly 10 propositions, 5 sentences, or
2 passages. As word count increases, the recall rate
of the three granularities converges, encompassing
all relevant information.
7 Related Work
Recent works on dense retrievers typically adopt
a dual-encoder architecture (Yih et al., 2011;
Reimers and Gurevych, 2019; Karpukhin et al.,
2020; Ni et al., 2022). With dual-encoders,
each query and document is encoded into a low-
dimensional feature vector respectively, and their
relevance is measured by a non-parametric similar-
ity function between the embedding vectors (Muss-
mann and Ermon, 2016). Due to the limited expres-
sivity from the similarity function, dual encoder
models often generalize poorly to new tasks with
scarce training data (Thakur et al., 2021). Previous
studies use techniques such as data augmentation
(Wang et al., 2022; Yu et al., 2023a; Izacard et al.,
2022; Gao and Callan, 2022; Lin et al., 2023; Dai
et al., 2023), continual pre-training (Chang et al.,
2020; Sachan et al., 2021; Oguz et al., 2022), task-
aware training (Xin et al., 2022; Cheng et al., 2023),
hybrid sparse-dense retrieval (Luan et al., 2021;
Chen et al., 2022), or mixed strategy retrieval (Ma
et al., 2022, 2023) and so on to improve cross-task
generalization performance of dense retrievers.
7



Source: data\tc16_2312.10997v5\referenced_papers\[30]_2312.06648.pdf (Page 14):

multiset-base), GTR (sentence-trans
formers/gtr-t5-base).
We use T5-large size Fusion-in-decoder
model ( nq_reader_large ) released by
the authors in https://github.com/
facebookresearch/FiD. We use Hugging-
Face checkpoint (meta-llama/Llama-2-7b)
for LLaMA-2-7B.
F Additional Results
In Section 5.2, we demonstrated the advantage
of retrieval by proposition over retrieval by sen-
tence, particularly as the population of the entity
decreases in EQ. We used the occurrence in the
top-1000 paragraphs retrieved by BM25 as a proxy
for frequency, rather than counting the number of
hyperlinks to the entity used in Sciavolino et al.,
2021. Therefore, the trend in the performance ver-
sus frequency plot shows some differences (Fig-
ure 6) between our results and those in Sciavolino
et al., 2021. For example, some entities are am-
biguous (e.g., 1992, a TV series). In such cases,
the occurrence of the surface form of the entity is
large. Simultaneously, questions related to ambigu-
ous entities are challenging to answer, leading to
lower recall.
In Section 6.2, we discussed the recall of an-
swers in the retrieved text with respect to the con-
text length. We further illustrate the performance
trends of six dense retrievers, as detailed in Fig-
ure 7. The results indicate that the recall rate of
propositions consistently outperforms that of sen-
tences and passages. Our findings lead to the con-
clusion that question-related density is greater in
proposition units compared to sentences and pas-
sages.
G Error Case Study
To understand the source of errors from each type
of retrieval granularity, we present and discuss four
typical examples of mistakes in Table 6 and Table 7.
With each example, we show the question and its
corresponding top-1 retrieved text unit by the GTR
retriever across the three granularities.
We observe that with passage-level retrieval, the
ambiguity of an entity or its references presents a
challenge for dense retrievers, which echoes find-
ings from (Min et al., 2020). For instance, in exam-
ple Q1, the question asks for “Super Bowl 50”, but
the retrieved passage and sentence refers to “Super
Bowl 5”. In Example Q2, passage retrieval fails
to identify the part referring to the correct “atomic
number”. Instead, the top-1 retrieved passage men-
tions “atomic number” in a different and irrelevant
context to the question. Retrieval by sentences can
also have a similar problem as retrieval by passages
like Example Q1. Also, retrieval by sentences faces
another challenge of lacking context. In Example
Q3 (shown in Table 7), sentence-based retrieval
fails as the correct sentence in the retrieved passage
uses “it” to refer to the pericardial sac.
Retrieval by propositions tackles the aforemen-
tioned problems by ensuring each retrieval unit
contains one piece of fact only and necessary con-
text is incorporated in the propositions. However,
proposition-based retrieval faces challenges with
questions that involve multi-hop reasoning over
long-range textual analysis. In Example Q4 (shown
in Table 7), the retrieved passage separately de-
scribes the actor’s name and the character they por-
tray. There is not a single proposition that entails
both the question and the answer.
15



### Claim 29/179

#### Claim Text
The granularity of retrieval can also be adapted to downstream tasks, such as retrieving Item IDs [40]in recommendation tasks and Sentence pairs [38].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[178]_2210.03765.pdf (Page 14):

Context: A leaf blower is shown blowing a large pile of leaves across a green lawn in front of residential houses. The leaves…
StoryEndGen is her hands . the woman
GPT2 (no ﬁnetune) are then blown by a small wind turbine.
GPT2 (text-only ﬁnetune) are then shown in a large circle and the roof is shown in a close up.
GPT2 + iNLG are placed on the ground and the man is shown sitting on the back.
Generated Image:
Context: Men are standing in the edge of a trampoline preparing to make a jump into a pool in a roofed pool. People…
StoryEndGen then then the camera and then the camera and the camera and the camera
GPT2 (no ﬁnetune) are standing in the edge of a trampoline preparing to amke a jump into a 
pool in a roofed pool.
GPT2 (text-only ﬁnetune) are standing in the middle of the pool preparing to jump into a pool in a 
roofed pool.
GPT2 + iNLG are swimming in the pool and throwing a ball.
Generated Image:
Context: They mix the eggs around a bowl and place butter and milk into another bowl and mix them all together. They…
StoryEndGen the cake ups and the cake and then the cake and then the cake and the 
cake and then the cake and then the cake and
GPT2 (no ﬁnetune) will be very soft and ﬂuﬀy.
GPT2 (text-only ﬁnetune) are ready to use.
GPT2 + iNLG then put a bowl on the end of the sink and put the mixture in the sink.
Generated Image:
(a)
Context: A leaf blower is shown blowing a large pile of leaves across a green lawn in front of residential houses. The leaves…
StoryEndGen is her hands . the woman
GPT2 (no ﬁnetune) are then blown by a small wind turbine.
GPT2 (text-only ﬁnetune) are then shown in a large circle and the roof is shown in a close up.
GPT2 + iNLG are placed on the ground and the man is shown sitting on the back.
Generated Image:
Context: Men are standing in the edge of a trampoline preparing to make a jump into a pool in a roofed pool. People…
StoryEndGen then then the camera and then the camera and the camera and the camera
GPT2 (no ﬁnetune) are standing in the edge of a trampoline preparing to amke a jump into a 
pool in a roofed pool.
GPT2 (text-only ﬁnetune) are standing in the middle of the pool preparing to jump into a pool in a 
roofed pool.
GPT2 + iNLG are swimming in the pool and throwing a ball.
Generated Image:
Context: They mix the eggs around a bowl and place butter and milk into another bowl and mix them all together. They…
StoryEndGen the cake ups and the cake and then the cake and then the cake and the 
cake and then the cake and then the cake and
GPT2 (no ﬁnetune) will be very soft and ﬂuﬀy.
GPT2 (text-only ﬁnetune) are ready to use.
GPT2 + iNLG then put a bowl on the end of the sink and put the mixture in the sink.
Generated Image:
(b)
Context: A leaf blower is shown blowing a large pile of leaves across a green lawn in front of residential houses. The leaves…
StoryEndGen is her hands . the woman
GPT2 (no ﬁnetune) are then blown by a small wind turbine.
GPT2 (text-only ﬁnetune) are then shown in a large circle and the roof is shown in a close up.
GPT2 + iNLG are placed on the ground and the man is shown sitting on the back.
Generated Image:
Context: Men are standing in the edge of a trampoline preparing to make a jump into a pool in a roofed pool. People…
StoryEndGen then then the camera and then the camera and the camera and the camera
GPT2 (no ﬁnetune) are standing in the edge of a trampoline preparing to amke a jump into a 
pool in a roofed pool.
GPT2 (text-only ﬁnetune) are standing in the middle of the pool preparing to jump into a pool in a 
roofed pool.
GPT2 + iNLG are swimming in the pool and throwing a ball.
Generated Image:
Context: They mix the eggs around a bowl and place butter and milk into another bowl and mix them all together. They…
StoryEndGen the cake ups and the cake and then the cake and then the cake and the 
cake and then the cake and then the cake and
GPT2 (no ﬁnetune) will be very soft and ﬂuﬀy.
GPT2 (text-only ﬁnetune) are ready to use.
GPT2 + iNLG then put a bowl on the end of the sink and put the mixture in the sink.
Generated Image:
(c)
Figure 9: Comparisons on few-shot sentence completion performance on ActivityNet.
Context: Sushi. Jerry loves to eat food from other nations.
Action-Plan She was to to to a . He . He was to to to the . He . She was to to to . He . She was to the the .
Plan-and-Write
the man was a lot of her . good he had to go good good good good good good good good good 
good good good good good i decided good his new phone good good good good good good good 
good good good good friends # wanted # made # found # happy good good # home good good 
good # time good good good # bought # time 
SimCTG Tom was hungry. He went to the sushi place. There was a lot of sushi. Tom was hungry. He went to 
the sushi place
GPT2 (no ﬁnetune) He's a big fan of the Japanese. He's a big fan of the Japanese. He's a big fan of the Japanese. He's 
a big fan of the Japanese.
GPT2 (text-only ﬁnetune) He goes to the country to buy the freshest seafood. He buys a lot of seafood. He buys more seafood 
and buys more seafood. Jerry buys more seafood and eats better.
GPT2 + iNLG He was unable to ﬁnd a slice he wanted. He grabbed a slice of sushi at a fast food restaurant. He 
was able to eat it and it was the perfect meal.
Generated Image:
Edward:/data2/wanrongzhu/inlg/data/story_generation/roc_stories/generated_images/test_42_0.png
(a)
Context: Hole in the House. Tim was putting away some paperwork in the attic.
Action-Plan He was to to to the . He . He was to to the . . He was to to to the . . He was to the the .
Plan-and-Write
# i was a very hot . </s> he decided good to the store good good good good good good good good 
good good good good <EOT> friends # wanted # made # found # happy <EOL> # my friend went 
good with good good good good good good good good good good good good , she had good and 
looked in good good good good good good good good good good good # time # found # home
SimCTG My neighbor's house was burglarized. I went to investigate. My neighbor's house was burglarized. 
My neighbor's house was burglarized. My neighbor's house was burglarized
GPT2 (no ﬁnetune)
He was trying to ﬁgure out how to get the house to be ready for the next day. He was trying to ﬁgure 
out how to get the house to be ready for the next day. “I'm not going to be able to do that," he said. 
"I'm not going to be able to do that.
GPT2 (text-only ﬁnetune) He was trying to write a letter to his boss. He was trying to get his boss to write a letter to him. Tim 
was frustrated.
GPT2 + iNLG He saw a map of the area. He went to the bathroom to check. There was nothing there. He was 
surprised to see it was a loophole.
Generated Image:
Edward:/data2/wanrongzhu/inlg/data/story_generation/roc_stories/generated_images/test_202_0.png
(b)
Figure 10: Comparisons on few-shot story generation performance on ROCStories.



Source: data\tc16_2312.10997v5\referenced_papers\[33]_2305.17653.pdf (Page 14):

Template: SST-2/CR/MR/MPQA
/* Example */
Does the following sentence have a positive or negative sentiment?
one long string of cliches .
The answer is negative.
/* Test data */
Does the following sentence have a positive or negative sentiment?
the performances take the movie to a higher level .
The answer is
Template: SST-5
/* Example */
What sentiment does this sentence have? terrible, bad, okay, good or great "with a romantic comedy plotline straight from the
ages, this cinderella story doesn’t have a single surprise up its sleeve ."
The answer is bad
/* Test data */
What sentiment does this sentence have? terrible, bad, okay, good or great
hardly a film that comes along every day.
The answer is
Template: CoLA
/* Example */
The following sentence is either "acceptable", meaning it is grammatically correct and makes sense, or "unacceptable". Which
is it?
I ordered if John drink his beer.
The answer is unacceptable
/* Test data */
The following sentence is either "acceptable", meaning it is grammatically correct and makes sense, or "unacceptable". Which
is it?
Angela characterized Shelly as a lifesaver.
The answer is
Template: Subj
/* Example */
Is this a subjective or objective description?
when the skittish emma finds blood on her pillow why does she still stay behind?
The answer is objective
/* Test data */
Is this a subjective or objective description?
"at the end of the worst day of his life, bruce angrily ridicules and rages against god and god responds ."
The answer is
Template: TREC
/* Example */
Which category best describes the following question:
How far is it from Denver to Aspen.
Choose from the following list: Description, Entity, Abbreviation, Person, Quantity, Location.
The answer is Quantity.
/* Test data */
Which category best describes the following question:
What were Ottoman objectives?
Choose from the following list: Description, Entity, Abbreviation, Person, Quantity, Location.
The answer is
Table 13: The prompt instances of in-context learning in our prompt-guided reranker.



Source: data\tc16_2312.10997v5\referenced_papers\[21]_2209.11755.pdf (Page 20):

SCIDOCS
a 8.4an 2.8the 2.7learning1.3deep 1.0on 1.0Others 82.8
Gold queries
a 18.9
an 2.7
automatic1.0
new 0.9
what 0.9
how 0.8
Others 74.8
Few-shot examples
what 34.9
the 16.0
how 10.5
who 7.0
where 6.9
when 6.8
Others 17.8
NQ-QGen
sentiment 20.0
a 20.0
20 20.0
statistics 20
parasitic 20.0
Prompts (3 examples)
NFCORPUS
how 2.7is 2.3the 2.1are 1.2what1.1preventing0.8Others 89.9
Gold queries
what 15.8how 5.5the 3.7dietary2.4can1.6does1.5Others 69.6
Few-shot examples
what 36.3the 12.5where 11.7when 10.8how 9.1who 5.2Others 14.4
NQ-QGen
preventing 33.3
1. 33.3
platelet 33.3
Prompts (3 examples)
DBPedia-Entity
how 15.4what 12.4why 6.5is 6.4can 4.3should2.5Others 52.5
DB Gold queries
what 36.8who 25.0when 14.4how 3.8the 2.9events1.7Others 15.4
Few-shot examples
who 26.6what 23.6when 22.0where 17.7how 4.5the 1.2Others 4.5
NQ-QGen
who 12.5which 12.5martin 12.5magazines 12.508 12.5james 12.5edward 12.5south 12.5
Prompts (8 examples)
Figure 4: Top ﬁrst word distribution on queries generated from different models in all other BEIR
datasets.
Filter Pre-training with C4 
independent cropping
Argument: {doc1} Counter-argument: {query1} X Argument    
{doc2} Counter-argument: {query2} X Argument: {document} X
Zero-shot 
Prompt
Synthetic data
(query, document) pairs
Training
Target domain 
documents
Generate
{document} Read the passage and generate a query
 Positive
Negative mining
Hard 
negatives
Large
LM
Small 
Dual Encoder
Small 
Cross Attention 
Reranker
 Negative synthetic data
(query, document) pairs
Figure 5: PROMPTAGATOR ++ Training pipeline.
21



Source: data\tc16_2312.10997v5\referenced_papers\[66]_2210.00185.pdf (Page 15):

Mixture Task Template Name
Semi-T0 Evaluation
openbookqa/main choose_an_answer_with_options
which_correct
pick_using_id
choices
only_options
which_correct_inverse
pick_answer_with_options
piqa what_is_the_correct_ending
pick_correct_choice_with_choice_given_before_goal
pick_correct_choice_index
finish_sentence_with_correct_choice
choose the most appropriate solution
rotten_tomatoes Reviewer Opinion bad good choices
Sentiment with choices
super_glue/cb can we infer
based on the previous passage
claim true/false/inconclusive
does it follow that
justified in saying
always/sometimes/never
GPT-3 style
consider always/sometimes/never
guaranteed true
must be true
guaranteed/possible/impossible
does this imply
MNLI crowdsource
should assume
take the following as truth
super_glue/copa exercise
. . . What could happen next, C1 or C2?
i_am_hesitating
plausible_alternatives
C1 or C2? premise, so/because. . .
. . . As a result, C1 or C2?
best_option
. . . which may be caused by
more likely
cause_effect
. . . why? C1 or C2
choose
super_glue/wic question-context-meaning-with-label
grammar_homework
affirmation_true_or_false
same_sense
GPT-3-prompt-with-label
polysemous
hellaswag complete_first_then
Randomized prompts template
Predict ending with hint
if_begins_how_continues
Table 5: PromptSource template names used for each task (Part2).



Source: data\tc16_2312.10997v5\referenced_papers\[92]_2205.10625.pdf (Page 27):

Published as a conference paper at ICLR 2023
leads to “darl”. So, “provoked, columbia, sir, principal” outputs “darl”.
Q: “provoked, columbia, sir, principal, seven”
A: “provoked, columbia, sir, principal” outputs “darl”. The last letter of “seven” is “n”. Concatenating:
“darl”, “n” leads to “darln”. So, “provoked, columbia, sir, principal, seven” outputs “darln”.
Q: “provoked, columbia, sir, principal, seven, extreme”
A: “provoked, columbia, sir, principal, seven” outputs “darln”. The last letter of “extreme” is “e”.
Concatenating: “darln”, “e” leads to “darle”. So, “provoked, columbia, sir, principal, seven, extreme”
outputs “darle”.
Q: “provoked, columbia, sir, principal, seven, extreme, timothy”
A: “provoked, columbia, sir, principal, seven, extreme” outputs “darle”. The last letter of “timothy”
is “y”. Concatenating: “darle”, “y” leads to “darley”. So, “provoked, columbia, sir, principal, seven,
extreme, timothy” outputs “darley”.
Q: “provoked, columbia, sir, principal, seven, extreme, timothy, submitted”
A: “provoked, columbia, sir, principal, seven, extreme, timothy” outputs “darley”. The last letter of
“submitted” is “d”. Concatenating: “darley”, “d” leads to “darleyd”. So, “provoked, columbia, sir,
principal, seven, extreme, timothy, submitted” outputs “darleyd”.
Q: “provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered”
A: “provoked, columbia, sir, principal, seven, extreme, timothy, submitted” outputs “darleyd”. The
last letter of “considered” is “d”. Concatenating: “darleyd”, “d” leads to “darleydd”. So, “provoked,
columbia, sir, principal, seven, extreme, timothy, submitted, considered” outputs “darleydd”.
Q: “provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered, spies”
A: “provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered” outputs
“darleydd”. The last letter of “spies” is “s”. Concatenating: “darleydd”, “s” leads to “darleydds”. So,
“provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered, spies” outputs
“darleydds”.
Q: “provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered, spies,
dost”
A: “provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered, spies” outputs
“darleydds”. The last letter of “dost” is “t”. Concatenating: “darleydds”, “t” leads to “darleyddst”.
So, “provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered, spies, dost”
outputs “darleyddst”.
Q: “provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered, spies,
dost, treasury”
A: “provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered, spies, dost”
outputs “darleyddst”. The last letter of “treasury” is “y”. Concatenating: “darleyddst”, “y” leads to
“darleyddsty”. So, “provoked, columbia, sir, principal, seven, extreme, timothy, submitted, considered,
spies, dost, treasury” outputs “darleyddsty”.
8 SCAN
8.1 P ROMPT CONTEXTS
In this section we present the prompt contexts used for the SCAN benchmark in Section 3.2. It
includes one context for each of standard prompting, least-to-most prompting, and chain-of-thought
prompting.
28



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[39]_2305.05065.pdf (Page 16):

Table 10: Testing scalability by generating the Semantic IDs on the combined dataset vs generating
the Semantic IDs on only the Beauty dataset.
Recall@5 NDCG@5 Recall@10 NDCG@10
Semantic ID [Combined datasets] 0.04355 0.3047 0.06314 0.03676
Semantic ID [Amazon Beauty] 0.0454 0.0321 0.0648 0.0384
transformer model. Each Semantic ID consists of a tuple of 4 integers, each of which are stored in 8
bits, hence totalling to 32 bits per item. Each item is represented by an Item ID, stored as a 32 bit
integer. Thus, the size of each lookup table will be of the order of 64N bits, where N is the number
of items in the dataset.
Memory cost of embedding tables . TIGER uses much smaller embedding tables compared to
traditional recommender systems. This is because where traditional recommender systems store an
embedding for each item, TIGER only stores an embedding for each semantic codeword. In our
experiments, we used 4 codewords each of cardinality 256 for Semantic ID representation, resulting
in 1024 (256×4) embeddings. For traditional recommender systems, the number of embeddings is
N, where N is the number of items in the dataset. In our experiments, N ranged from 10K to 20K
depending on the dataset. Hence, the memory cost of TIGER’s embedding table is 1024d, where d is
the dimension of the embedding, whereas the memory cost for embedding lookup tables in traditional
recommendation systems is Nd.
17



Source: data\tc16_2312.10997v5\referenced_papers\[39]_2305.05065.pdf (Page 15):

Table 8: The effect of providing user information to the recommender system
Recall@5 NDCG@5 Recall@10 NDCG@10
No user information 0.04458 0.0302 0.06479 0.0367
With user id (reported in the paper) 0.0454 0.0321 0.0648 0.0384
Table 9: The mean and stand error of the metrics for different dataset (computed using 3 runs with
different random seeds)
Datasets Recall@5 NDCG@5 Recall@10 NDCG@10
Beauty 0.0441 ± 0.00069 0.0309 ± 0.00062 0.0642 ± 0.00092 0.0374 ± 0.00061
Sports and Outdoors 0.0278 ± 0.00069 0.0189 ± 0.00043 0.0419 ± 0.0010 0.0234 ± 0.00048
Toys and Games 0.0518 ± 0.00064 0.0375 ± 0.00039 0.0698 ± 0.0013 0.0433 ± 0.00047
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
K
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35Invalid IDs (%)
(a) Sports and Outdoors
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
K
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00Invalid IDs (%) (b) Beauty
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
K
0
1
2
3
4
5
6Invalid IDs (%) (c) Toys and Games
Figure 6: Percentage of invalid IDs when generating Semantic IDs using Beam search for various
values of K. As shown, ∼ 0.3% − 6% of the IDs are invalid when retrieving the top-20 items.
E Discussion
Effects of Semantic ID length and codebook size. We tried varying the Semantic ID length and
codebook size, such as having an ID consisting of 6 codewords each from a codebook of size 64.
We noticed that the recommendation metrics for TIGER were robust to these changes. However,
note that the input sequence length increases with longer IDs (i.e., more codewords per item ID),
which makes the computation more expensive for our transformer-based sequence-to-sequence model.
Scalability. To test the scalability of Semantic IDs, we ran the following experiment: We combined
all the three datasets and generated Semantic IDs for the entire set of items from the three datasets.
Then, we used these Semantic IDs for the recommendation task on the Beauty dataset. We compare
the results from this experiment with the original experiment where the Semantic IDs are generated
only from the Beauty dataset. The results are provided in Table 10. We see that there is only a small
decrease in performance here.
Inference cost. Despite the remarkable success of our model on the sequential recommendation
task, we note that our model can be more computationally expensive than ANN-based models during
inference due to the use of beam search for autoregressive decoding. We emphasize that optimizing
the computational efficiency of TIGER was not the main objective of this work. Instead, our work
opens up a new area of research: Recommender Systems based on Generative Retrieval. As part of
future work, we will consider ways to make the model smaller or explore other ways of improving
the inference efficiency.
Memory cost of lookup tables. We maintain two lookup hash tables for TIGER: an Item ID to
Semantic ID table and a Semantic ID to Item ID table. Note that both of these tables are generated
only once and then frozen: they are generated after the training of the RQ-V AE-based Semantic
ID generation model, and after that, they are frozen for the training of the sequence-to-sequence
5https://github.com/jeykigung/P5
6https://github.com/jeykigung/P5/blob/0aaa3fd8366bb6e708c8b70708291f2b0ae90c82/preprocess/data_preprocess_amazon.ipynb
16



Source: data\tc16_2312.10997v5\referenced_papers\[107]_2306.02224.pdf (Page 7):

A.1 APPENDIX 1. Webshop: Evidence of GPT-4 considering additional opinion.
A.1.1 Example 1. Webshop: AutoGPT(GPT-4) disagreed with the Additional Opinion.
[breaklines]
Command search returned: =Observation=
Instruction:
i want a pink niuta 2 pack hair towel wrap for dry hair, and price lower than 20.00 dollars
[button] Back to Search [button_]
Page 1 (Total results: 50)
[button] Next > [button_]
[button] B08G14B779 [button_]
NIUTA 2 Pack Hair Towel Wrap, Microfiber Quick Drying Hair Towels, Super Absorbent Quick Dry Hair Towel,
Wrapped Bath Cap (Pink+Light Blue)
$4.99
[button] B092QTHX2Z [button_]
Microfiber Hair Towel Wrap POPCHOSE Ultra Absorbent, Fast Drying Hair Turban Soft, No Frizz Hair Wrap Towels for Women Wet Hair,
Curly, Longer, Thicker Hair. . .
$8.99
[button] B08NX5S6S2 [button_]
Microfiber Hair Towel Wrap for Women Super Absorbent Quick Dry Hair Towels with Button Design for Drying Curly&Thick Hair 2
Pack(Purple+Camel)
$8.99
[button] B09KKCQ7FK [button_]
VOLO Hero Microfiber Hair Towel, Super Absorbent, Ultra-Soft, Fast Drying | Reduce Dry Time by 50%, Large, Premium Wrap Towel,
Sustainable Packaging, Anti-Frizz | Luna Gray (2 Pack)
$70.0
[button] B083FPQ77W [button_]
Microfiber Hair Towels Super Absorbent Hair Drying Towel Turban for Women and Girls Quick Magic Hair Dry Hat Hair Towel Wrap Bathing
Wrapped Cap 2 Packs (Pink+Grey)
$13.99
[button] B09KK4GZW1 [button_]
VOLO Hero Microfiber Hair Towel | Super Absorbent, Ultra-Soft, Fast Drying | Reduce Dry Time by 50% | Large, Premium Wrap Towel for
All Hair Types | Anti-Frizz, Anti-Breakage | Luna Gray (2 Pack)
$78.0
[button] B074T227KV [button_]
Microfiber Hair Towel, 2 Pack Dry Hair Towel Twist Wrap Absorbent Quickly Dry Hair Towel for Kids and Women (Pink+Purple)
$10.97
[button] B08CZBMHDR [button_]
Hair Towel Wrap, [2 Pack] Microfiber Quick Dry Hair Turban Wrap - Super Absorbent,Microfiber Hair Towel with Button,Quick Dry
Hair Towel for Women,Hair Turbans for Wet Hair,Wrapped Bath Cap(Pink Blue)
$10.99
[button] B09FB25N5V [button_]
SYMGILA Microfiber Hair Towel Wrap, Hair Turbans for Wet Hair, Drying Hair Wrap Towels for Curly Hair Women Anti Frizz 2 Pack
(Blue/Beige)
$100.0
[button] B08YMYXBK8 [button_]
VOLO Hero Microfiber Hair Towel, Super Absorbent, Ultra-Soft, Fast Drying | Reduce Dry Time by 50%, Large, Premium Wrap Towel,
Sustainable Packaging, Anti-Frizz, Anti-Breakage, Hands-Free (Luna Gray)
$34.99
loop number:2
Here's one suggestion for the command: click with 'back to search'.
Please use this suggestion as a reference and make your own judgement. Determine which next command to use, and respond
using the JSON format specified above:
{
"thoughts": {
"text": "I found the matching item with a price lower than $20.",
"reasoning": "The first item (B08G14B779) matches the description and has a price of $4.99, which is lower than $20.",
"plan": "- click on the matching item (B08G14B779)\n- review the item details\n- make the purchase",
"criticism": "Clicking 'back to search' is not necessary since I already found the matching item."
},
"command": {
"name": "click",
"args": {
"tool_input": "B08G14B779"
}
}
}



Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 11):

Language Models as Semantic Indexers
Table 7.Dataset Statistics
Dataset # Items # Users # Rec history (train/dev/test) # Search query (train/dev/test) # Search labels (train/dev/test)
Amazon-Beauty 12,101 22,363 111,815 / 22,363 / 22,363 1,049 / 150 / 338 1,907 / 268 / 582
Amazon-Sports 18,357 35,598 177,990 / 35,598 / 35,598 1,299 / 186 / 443 2,209 / 311 / 764
Amazon-Toys 11,924 19,412 97,060 / 19,412 / 19,412 1,010 / 145 / 351 1,653 / 250 / 594
Table 8.Dataset Statistics
Dataset # Documents # Query (train/test) # Search labels (train/test)
NQ320k 109,739 307,373 / 7,830 307,373 / 7,830
MACRO 1M 1,000,000 502,939 / 6,980 532,751 / 7437
TREC-DL 1M 1,000,000 502,939 / 93 532,751 / 1,069
Algorithm 1 Self-supervised ID Learning Procedure of
LMI NDEXER
1: Input: The document corpus tdu.
2: Output: The semantic IDs tcduof the documents tdu.
A semantic indexer SemIndexerp¨qwhich contains a
semantic encoder SemEncθp¨qand codebooks tEtut.
A reconstruction model Reconϕp¨q.
3: Begin
4: // initialize semantic encoder
5: SemEncθp¨qÐ T5-base;
6: // reconstruction warm up
7: minϕ L0
recon “´ ř
d
ř
wPdzd0
h
log Preconpw|d0
h q;
8: for t “1, . . . , Tdo
9: // semantic encoder & codebook warm up
10: ht ÐSemEncθpd, cdq;
11: zw ÐReconϕpq “tct
d, ht
du, k“dt
hq;
12: minϕ,ϕc Lt “Lt
recon `Lt
contrastive `Lt
commitment;
13: ht
d ÐSemEncθpd, ct
dq;
14: Et ÐKMeanspht
dq;
15: // whole framework training
16: zw ÐReconϕpq “tct
d, ct
du, k“dh, v“dhq;
17: minϕ,ϕc Lt “Lt
recon `Lt
contrastive `Lt
commitment;
18: ct
d Ðargmaxjpspct
d “j|cd, dq;
19: end for
20: Return tcdu, SemIndexerp¨q;
21: End
rate in {1e-3, 2e-3, 5e-3}. The training epochs are set to be
30, 10, and 5 for Amazon datasets, NQ, and MS MACRO
respectively. The hyper-parameter configuration for self-
supervised semantic indexer training can be found in Table
9.
In the downstream recommendation task, for generative
recommendation methods with semantic IDs (rq-V AE in-
dexer, hierarchical clustering indexer, and LMI NDEXER ),
we concatenate the textual information (title & description)
of the user’s previously interacted items, serve it as the input
text into the generative language model and ask the model
to generate the ID for next item. The baselines are using
the same T5-base checkpoint. We train all the compared
generative recommendation methods for 10,000 steps with
the learning rate searched in {1e-2, 1e-3, 1e-4}. The batch
size is set to be 32, the maximum input text length is set
to be 1024 and all experiments are run on an 8 A100 40G
machine. The number of beams for beam search is set to
20. The hyper-parameter configuration for generative rec-
ommendation training can be found in Table 10.
In the downstream product search task, for generative re-
trieval methods with semantic IDs (rq-V AE indexer, hierar-
chical clustering indexer, and LMI NDEXER ), we serve the
query as the input text into the generative language model
and ask the model to generate the ID for the relevant items.
All baselines initially load the same T5-base checkpoint.
We train all the compared generative retrieval methods for
10,000 steps with the learning rate searched in {1e-2, 1e-
3, 1e-4}. The batch size is set to 32, the maximum input
text length is set to be 1024 and all experiments are run on
an 8 A100 40G machine. The number of beams for beam
search is set to 20. The hyper-parameter configuration for
generative product search training can be found in Table 11.
In the downstream document retrieval task, for generative
retrieval methods with semantic IDs (rq-V AE indexer, hier-
archical clustering indexer, and LMI NDEXER ), we serve the
query as the input text into the semantic indexer and ask the
model to generate the ID for the relevant documents. Fol-
lowing (Wang et al., 2022), we use docT5query (Nogueira
12



Source: data\tc16_2312.10997v5\referenced_papers\[39]_2305.05065.pdf (Page 2):

(a) Semantic ID generation for items using
quantization of content embeddings.
Bidirectional Transformer Encoder
User_5
t_u5
Item 515
Sem. ID = (5, 25, 78)
Item 233
Sem. ID = (5, 23, 55)
t_23 t_55t_5Tokens
Transformer Decoder
<BOS>
Item 64
Sem. ID = (5, 25, 55)
Item Interaction History of User 5
t_25 t_78t_5
Encoded 
Context
t_25t_5
t_5
<EOS>t_55
t_25 t_55
Next Item
Neurips Version
(b) Transformer based encoder-decoder setup for building the
sequence-to-sequence model used for generative retrieval.
Figure 2: An overview of the modeling approach used in TIGER.
AttRec [41] proposed by Zhang et al. used self-attention mechanism to model the user’s intent
in the current session, and personalization is ensured by modeling user-item affinity with metric
learning. Concurrently, Kang et al. also proposed SASRec [17], which used self-attention similar to
decoder-only transformer models. Inspired by the success of masked language modeling in language
tasks, BERT4Rec [32] and Transformers4Rec [6] utilize transformer models with masking strate-
gies for sequential recommendation tasks. S3-Rec [44] goes beyond just masking by pre-training
on four self-supervised tasks to improve data representation. The models described above learn a
high-dimensional embedding for each item and perform an ANN in a Maximum Inner Product Search
(MIPS) space to predict the next item. In contrast, our proposed technique, TIGER, uses Generative
Retrieval to directly predict the Semantic ID of the next item.
P5 [8] fine-tunes a pre-trained large language models for multi-task recommender systems. The P5
model relies on the LLM tokenizer (SentencePiece tokenizer [29]) to generate tokens from randomly-
assigned item IDs. Whereas, we use Semantic ID representation of items thay are learned based on the
content information of the items. In our experiments (Table 2), we demonstrate that recommendation
systems based on Semantic ID representation of items yield much better results than using random
codes.
Semantic IDs. Hou et al.proposed VQ-Rec [12] to generate “codes” (analogous to Semantic IDs)
using content information for item representation. However, their focus is on building transferable
recommender systems, and do not use the codes in a generative manner for retrieval. While they
also use product quantization [15] to generate the codes, we use RQ-V AE to generate Semantic IDs,
which leads to hierarchical representation of items (Section 4.2). In a concurrent work to us, Singh et
al. [31] show that hierarchical Semantic IDs can be used to replace item IDs for ranking models in
large scale recommender systems improves model generalization.
Generative Retrieval. While techniques for learning search indices have been proposed in the past
[20], generative retrieval is a recently developed approach for document retrieval, where the task is to
return a set of relevant documents from a database. Some examples include GENRE [5], DSI [34],
NCI [37], and CGR [22]. A more detailed coverage of the related work is in Appendix A. To the best
of our knowledge, we are the first to propose generative retrieval for recommendation systems using
Semantic ID representation of items.
3 Proposed Framework
Our proposed framework consists of two stages:
1. Semantic ID generation using content features.This involves encoding the item content features
to embedding vectors and quantizing the embedding into a tuple of semantic codewords. The
resulting tuple of codewords is referred to as the item’s Semantic ID.
2. Training a generative recommender system on Semantic IDs.A Transformer model is trained on
the sequential recommendation task using sequences of Semantic IDs.
3



### Claim 30/179

#### Claim Text
To capture the logical relationship between document content and structure, KGP [91] proposed a method of building an index between multiple documents using KG.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[91]_2308.11730.pdf (Page 11):

document, we need to maintain a pointer from the document
to its sentence nodes. For adding sentence nodes of one doc-
ument, we need to first apply the KG construction method to
compute the lexical/semantic similarity between each of the
newly added sentence nodes and the existing nodes in KG,
and then add corresponding edges connecting them, which
is also linear to the size of the current graph.
For space complexity, it takesO(|V|(α + β)) to maintain
the constructed KG on the fly where α is the average space
for saving the passage embedding vector while β is the av-
erage space for saving the textual information of that pas-
sage. Although our constructed KG treats passages as nodes,
which cannot scale very well when the graph is extremely
large, the total number of documents a user maintains in a
folder is typically around 10-100, which is still affordable.
8.4 Markdown-Formatted Table
Figure 8 demonstrates that by sending Tables in the mark-
down format, ChatGPT can successfully understand their
content and perform information retrieval based on the given
questions. However, we do observe that such a markdown-
formatted solution is not feasible for the long table due to
the input token limitation of ChatGPT, we plan to explore
the solution using SQL as the prompt content or modeling
the Table as the grid graph to solve the issue in the future.
Figure 8: An example demonstrating that ChatGPT can un-
derstand table in the markdown format.
8.5 Knowledge Graph Construction Comparison
Table 5 compares different knowledge graph construction
methods and their pros and cons.
• TAGME: TAGME (Ferragina and Scaiella 2010) is very
effective in extracting Wikipedia Entities from a passage
despite the low efficiency. In our graph construction, it
usually takes more than 8 hours to extract entities of all
passages for even just 12 Wikipedia documents. Even af-
ter we apply parallel processing, it still takes more than 2
hours. In addition, it can only handle entities mentioned
in the existing Wikipedia system and hence cannot gen-
eralize to documents from other domains.
• TF-IDF and KNN-ST: Although there is no domain lim-
itation, it is hard to guarantee the extracted keywords or
the embedding semantic similarity can precisely encode
the relationships that are desired for answering the given
question between any two passages. We empirically find
TF-IDF is more likely to extract meaningless keywords
even after removing supporting verbs and articles.
• KNN-MDR: Since KNN-MDR pre-trains the sentence
encoder by predicting the next supporting passage given
already-retrieved passages, the embedding similarity be-
tween two passages is more likely to encode necessary
logical associations required for MD-QA. However, the
main bottleneck here is how to obtain the logically or-
dered supporting facts that can progressively reach the
answer. Obtaining these sequential data is non-trivial and
usually requires a large number of human resources for
well-curated annotation.
• Existing Knowledge Base: One common approach in
the literature is to use existing knowledge bases or ex-
tract subgraphs from them for specific tasks (Yasunaga
et al. 2022; Dong et al. 2023; Yasunaga et al. 2021). Be-
cause the factual information is characterized as a triplet
consisting of two entity nodes and their relationship, it
is very powerful in encoding factual information/com-
monsense knowledge and also avoids the scalability is-
sue (since two different passages might share the same
entity). Despite its potency and ease of use, construct-
ing this type of KGs demands meticulously designed
relation extractors, which is still deemed a challenging
task in the literature. Recent research has explored using
LLMs for relation extraction. However, with increasing
document numbers, using non-open-sourced LLMs can
become prohibitively expensive. A potential solution is
fine-tuning an open-sourced LLM specifically for rela-
tion extraction. Detailed discussion on this is beyond the
scope of this study and is thus omitted.
To put it in a nutshell, there’s no one-size-fits-all method
for KG construction. Our paper offers an in-depth analysis of
the proposed KG construction methods alongside other ex-
isting ones. The best approach often depends on the specific
use case. For broad domains containing general factual in-
formation, tools like ’TAGME’ or ’Knowledge Base’ might
be apt. However, for more niche or sensitive areas, meth-
ods like TF-IDF/KNN-ST are more appropriate. In certain
situations, gathering domain-specific data and pre-training
encoders is the most effective way to build the KG.



Source: data\tc16_2312.10997v5\referenced_papers\[91]_2308.11730.pdf (Page 2):

Figure 3: Knowledge Graph Construction. We split each document in the document collection into passages. For each passage,
we either directly obtain their embeddings via pre-trained encoders or extract their keywords to build bag-of-word (BOW)
features. Then we connect two passages based on their embedding similarity or whether they share common keywords. Addi-
tionally, we extract tables/pages via Extract-PDF API and add them as structural nodes to the KG. If pages include passages
and tables, we add a directed edge to denote the belonging relations. The table nodes include the markdown formatted content
of that table as Figure 8 in Supplementary has empirically shown that LLMs are able to understand tables in this format.
3 Knowledge Graph Construction
Despite numerous well-established KGs (Hoffart et al. 2013;
Tian et al. 2023b), they treat nodes/edges as entities/rela-
tions, which necessitates sophisticated relational extraction
techniques and thereby limits their applicability in general
domains (Huang et al. 2021). Additionally, their primary fo-
cus on the Wikipedia domain also restricts their usage for
answering non-Wikipedia questions such as ones over legal
or financial documents. To remedy this issue, we propose
generally applicable KG construction methods.
We first analyze two representative questions in Fig-
ure 2(a)-(b) to motivate our KG construction. Answering
these two questions necessitates the deduction of logical as-
sociations among different passages. These associations are
encoded either through 1) lexical similarity: common key-
words shared among different passages, e.g., ‘Alf Clausen’
bridges passage S1 and passage S2 in Figure 2(a), or 2) se-
mantic similarity: syntactic elements that convey semantic
relations, e.g., ‘nationality’ and ‘American director’ in Fig-
ure 2(b). This motivates us to construct the graph by mod-
eling passages as nodes and their lexical/semantic similarity
as edges. More specifically in Figure 3, we split each docu-
ment into individual passages, and for each passage Si, we
add a node vi to the KG with its feature being the text of that
passage Xi. Then we add edges by checking the lexical/se-
mantic similarity between pairs of passage nodes.
TF-IDF KG Construction For adding edges according to
lexical similarity, we first apply TF-IDF keyword extrac-
tion (Ramos et al. 2003) over each document to filter out
meaningless words such as supporting verbs and articles,
which also reduces the dimension of bag-of-word (BOW)
features, sparsifies the constructed graph and increases the
graph traversal efficiency. In addition, we add the document
title into the extracted keyword set since some questions fo-
cus on title entities. We collect the extracted keywords from
all documents to form the keyword space W and then con-
nect two passages if they share any common keyword inW.
KNN-ST/MDR KG Construction For adding edges ac-
cording to semantic similarity, we can readily employ pre-
existing models such as sentence transformers to gener-
ate passage embedding Xi for each node vi and subse-
quently compute pairwise similarity matrix to construct the
K-nearest neighbor (KNN) graph. However, these off-the-
shelf models, typically trained on tasks not so-related to
MD-QA, may not adequately encapsulate necessary logical
associations in their embedding similarity demanded by the
question. To overcome this problem, we follow the train-
ing strategy of MDR (Xiong et al. 2020) and train a sen-
tence encoder by predicting the subsequent supporting facts
based on previously supporting facts, thereby endowing the
encoder with reasoning capability. Consequently, the em-
bedding similarity and the corresponding constructed KNN
graph fundamentally encapsulate the necessary logical asso-
ciations between different passages.
TAGME Moreover, we employ TAGME (Min et al. 2019)
to extract Wikipedia entities from each passage and con-
struct the graph based on whether two passage nodes share
common Wikipedia entities.
In addition to passage nodes, we further add structural
nodes into the graph by extracting document structures via
Extract-PDF 3. In this paper, we only consider adding pages
and tables but the constructed KG can include more differ-
ent types of document structures. The feature of table nodes
is the markdown since LLMs can understand this as demon-
strated in Figure 8 in Supplementary. The feature of page
nodes is the page number and we add directed edges from
it to sentence/table nodes in that page. Note that we do not
aim to propose a one-size-fits-all KG construction method.
Instead, we seek to compare the merits and limitations of
various methods in Table 5, offering guidance on which KGs
are best suited for specific scenarios.
3https://developer.adobe.com/document-services/docs/
overview/pdf-extract-api/



Source: data\tc16_2312.10997v5\referenced_papers\[91]_2308.11730.pdf (Page 1):

Figure 2: Three popular questions that require reasoning and retrieving over passages/pages/tables from multiple documents.
(a) Bridging questionsrely on sequential reasoning while (b) Comparing questionsrely on parallel reasoning over different
passages. (c) Structural questionsrely on fetching contents in the corresponding document structures.
compilation of multi-modality structured data (e.g., pages,
sections, paragraphs, tables, and figures) and some ques-
tions may specifically ask for the content in certain struc-
tures, which necessitates a comprehensive grasp of these
complex document structures. For example, the question in
Figure 2(c) asks about the difference between Page 1 and Ta-
ble 2, which is unanswerable if leveraging heuristic methods
like BM25 or deep-learning ones like DPR (Karpukhin et al.
2020). Building on top of previous challenges, the advent of
LLMs introduces new complexities.
For the challenge of alternatively retrieving and reasoning
knowledge across different documents, although previous
works train a multi-hop retriever (Xiong et al. 2020; Yavuz
et al. 2022) to imitate such process by sequentially fetch-
ing the next passage based on the already-retrieved ones,
none of them explore the potential of engaging LLMs into
this process. More recent works design different prompt-
ing strategies such as Chain/Tree/Graph-of-thought (Trivedi
et al. 2022a; Wei et al. 2022; Yao et al. 2023; Yao, Li, and
Zhao 2023) to guide LLMs approaching answers progres-
sively. However, prompting non-open-sourced LLMs back
and forth incurs forbiddable latency as well as unaffordable
consumption. In addition, how to integrate different docu-
ment structures into the prompt design so that LLMs can
understand them is still an open-ended question.
Given the above challenges, we propose a knowledge
graph prompting (KGP) method for enhancing LLMs in
MD-QA. Specifically, we construct a KG over the given doc-
uments with nodes symbolizing passages or document struc-
tures and edges denoting their lexical/semantic similarity be-
tween passages or intra-document structural relations. Then
for the first challenge of alternative reasoning and retrieving
knowledge across different documents, we design an LLM-
based KG traversal agent, which can alternatively generate
the next evidence to approach the question, i.e., reasoning,
and select the most promising neighbor to visit from the con-
structed KG based on the generated evidence, i.e., retrieval.
Moreover, we apply the instruction fine-tuning strategy to
augment the reasoning capability of the LLM-based KG
traversal agent and hence refrain from repeatedly prompting
non-open-sourced LLMs for evidence generation. For the
multi-modality challenge, we add different types of nodes
to the KG characterizing different document structures and
hence enabling content retrieval within those specific struc-
tures. We highlight our contributions as follows:
• Generally-applicable KG Construction. We propose
three KG construction methods over documents, with pas-
sages or document structures as nodes and their lexical/se-
mantical similarity or structural relations as edges. Then
we empirically evaluate the quality of the constructed
KGs in MD-QA by checking the level of overlap between
the neighborhood and the supporting facts for each ques-
tion (Figure 5). Additionally, we provide a comprehensive
summary of both our proposed and existing KG construc-
tion methods in Table 5 in Supplementary.
• Engaging KG for Prompt Formulation.We design a
Knowledge Graph Prompting (KGP) method, which lever-
ages the LLM-based KG traversal agent to retrieve the
question-relevant contexts by traversing the constructed
KG. Moreover, we fine-tune this agent to adaptively tra-
verse the most promising neighbors for approaching the
question based on the visited nodes (retrieved passages).
• Case Studies Verifying MD-QA Framework.We com-
pare the performance of MD-QA when using different
types of LLM agents in graph traversal (Table 2) on the
KGs constructed over different numbers of documents
(Figure 7(c)). We conduct case studies on visualizing KGP
for MD-QA in Section 8.7 in Supplementary.
2 Notations
Following (Tian et al. 2023a), let G = (V, E) be a knowl-
edge graph constructed from a set of documents D, where
the node set V = {vi}n
i=1 representing document structures
(e.g., passages/pages/tables, etc.) and the edge setE ⊂ V×V
representing the connections among different nodes (e.g.,
semantic/lexical similarity and belonging relations among
document structures, etc.). Let X = {Xi}n
i be node fea-
tures and Xi corresponds to the feature of node vi, the form
of which could be the text for the passage, the markdown for
the table and the page number for the page.



Source: data\tc16_2312.10997v5\referenced_papers\[91]_2308.11730.pdf (Page 6):

Figure 7: (a)-(b): Performance first increases and then decreases as the branching factor increases. The results are averaged
across 100 sampled questions on 2WikiMQA and MuSiQue. (c): Performance/Efficiency increases/decreases as the number of
documents increases on MuSiQue. KGP-T5 achieves higher performance/efficiency than DPR.
5.3 Impact of Graph Traversal Agent
Here we study the influence of using different LLM agents to
traverse over TAGME-constructed KG on MD-QA. Specif-
ically, we compare agents that select the next neighbor to
visit randomly or intelligently via guidance from ChatGPT,
LLaMA, T5, and MDR in Table 2. Because the random
agent only blindly traverses the KG without any guidance
from LLM, it unavoidably collects irrelevant passages and
hence achieves the worst performance than others under
LLMs’ guidance. This aligns with our previous observation
on the low precision in Figure 5 and further demonstrates the
necessity of using LLMs to guide the graph traversal. Inter-
estingly, we find that KGP-T5 performs better than LLaMA
even though the parameters of LLaMA-7B are more than the
ones with T5-0.7B. We hypothesize this is because LLaMA-
7B requires more data to fine-tune than T5-0.7B.
5.4 Sensitivity Analysis
Here we perform the sensitivity analysis of the branching
factor (the number of nodes selected from candidate neigh-
bors to visit next). In Figure 7(a)-(b), the performance first
increases as the branching factor increases because more
passage nodes selected from the candidate neighbors lead to
more reasoning paths to reach the final answer. However, as
we fix the context budget to ensure fair comparison (i.e., the
total number of passages we are allowed to retrieve for each
question is the same across all baselines), the performance
declines as the branching factor increases because the num-
ber of initial seeding nodes diminishes, leading to reduced
coverage of the KG. Furthermore, we compare the efficiency
of KGP when the constructed KG includes different num-
bers of documents in Figure 7(c). KGP consistently achieves
higher performance than other baselines and higher effi-
ciency than embedding-based DPR. TF-IDF is slightly faster
than KGP because it is a purely heuristic-based method.
6 Related Work
Question answering Question Answering (QA) aims to
provide answers to users’ questions in natural language (Zhu
et al. 2021; Pandya and Bhatt 2021), and most QA sys-
tems are composed of information retrieval (IR) and an-
swer extraction (AE) (Mao et al. 2021; Ju et al. 2022;
Liu and Qin 2022). In IR, the system searches for
query-relevant factual passages using heuristic methods
(BM25) (Robertson, Zaragoza et al. 2009) or neural-ranking
ones (DPR) (Karpukhin et al. 2020). In AE, the final an-
swer is extracted usually as a textual span from related pas-
sages. Although this framework has been broadly applied
in O-QA (Mao et al. 2021) and D-QA (Xu et al. 2020;
Mathew, Karatzas, and Jawahar 2021), no previous work fo-
cus on MD-QA, which demands alternatively reasoning and
retrieving knowledge from multiple documents. To tackle
this issue, we construct KGs to encode logical associations
among different passages across documents and design an
LLM-based graph traversal agent to alternatively generate
the reason and visit the most matching passage node.
Pre-train, Prompt, and Predict with LLMs With the
emergence of LLMs, the paradigm of ‘pre-train, prompt,
predict’ has gained magnificent popularity in handling a
wide spectrum of tasks (Gururangan et al. 2020; Liu et al.
2023; Yu et al. 2023). This approach begins with pre-training
LLMs by pretext tasks to encode world knowledge into
tremendous parameters (Wu et al. 2023) followed by a
prompting function to extract pertinent knowledge for down-
stream tasks (Yang et al. 2023). Recent advancements ex-
plore different prompting strategies to enhance LLMs’ rea-
soning capabilities (Wei et al. 2022; Jin et al. 2023). In con-
trast to that, our work offers a novel perspective by trans-
forming the prompt formulation into the KG traversal.
7 Conclusion
Answering multi-document questions demands knowledge
reasoning and retrieving from different documents across
various modalities, presenting challenges for applying the
paradigm of ‘pre-train, prompt and predict’ with LLMs.
Recognizing the logical associations among passages and
structural relations within documents, we propose a Knowl-
edge Graph Prompting method (KGP) for aiding LLMs in
MD-QA. The KGP constructs KGs from documents with
nodes as sentences or document structures, and edges as
their lexical/semantic similarity/structural relations. Since
constructed KGs may contain irrelevant neighbor informa-
tion, we further design an LLM-based graph traversal agent
that selectively visits the most promising node in approach-
ing the question. In the future, we plan to investigate the
capability of LLMs in understanding graph topology and ex-
plore the potential of fine-tuning/prompting LLMs to encode
complex topological signals hidden in the graph.



Source: data\tc16_2312.10997v5\referenced_papers\[91]_2308.11730.pdf (Page 4):

Given a questionq asking about the document content, the
LLM-based graph traversal agent reasons over previously
visited nodes/retrieved passages{sk}j
k=0 and then generates
the next passage sj+1 as follows:
sj+1 = arg max
v∈Nj
ϕ(g(Xv), f(||j
k=0Xk)), (1)
where ||j
k=0Xk concatenates the textual information of pre-
viously retrieved passages/visited nodes. For the choice of
f, one way is to employ encoder-only models like Roberta-
base (Asai et al. 2019; Xiong et al. 2020; Yavuz et al. 2022)
and correspondingly g would be another encoder model with
ϕ(·) being the inner product measuring the embedding sim-
ilarity. Another way is to employ encoder-decoder models
such as T5 (Brown et al. 2020; Touvron et al. 2023) and
correspondingly g would be an identity function with ϕ(·)
measuring the textual similarity. To mitigate the hallucina-
tion issue and enhance the reasoning capability (Wei et al.
2022; Ji et al. 2023) of the LLM traversal agent, we fur-
ther instruction fine-tunef (Chung et al. 2022) by predicting
the next supporting facts based on previous supporting facts,
thereby integrating commonsense knowledge encoded orig-
inally in their pre-trained parameters with the enhanced rea-
soning capability inherited from the instruction fine-tuning.
After visiting the top-scoring nodes selected from the candi-
date neighbor queue by Eq (1), the candidate neighbor queue
is updated by adding neighbors of these newly visited nodes.
We iteratively apply this process until hit the preset budget.
Next, we illustrate the above process with an example in Fig-
ure 4 and present the algorithm thereafter.
Figure 4 presents the content-based question asking ‘In
what year was the creator of the current arrangement of
Simpson’s Theme born?’. We use TF-IDF search to initial-
ize the seeding passage Node 1, which reads: ‘Alf Heiberg
Clausen (born March 28, 1941) is an American film com-
poser’. Subsequently, we prefix the currently retrieved-
context (Node 1) with the question and prompt the LLM to
generate the next evidence required to approach the question
one step closer. Because we augment the reasoning capabil-
ity of the LLM by instruction fine-tuning, it is expected to
recognize the logical association between the question and
the currently retrieved context. Consequently, it can predict
the subsequent passage thatmaintains logical coherence, al-
beit may contain factual mistakes , i.e., ‘Alf Clausen (born
April 16, 1941) is an American composer of film and tele-
vision scores.’ To rectify this potential factual mistake, we
select nodes from the candidate neighbors that match the
most with the LLM-generated passage, in this case, Node
4 ‘Alf Heiberg Clausen (born March 28, 1941) is an Ameri-
can film composer’. Since this passage sources directly from
documents, it inherently ensures the validity of the informa-
tion. Then we prompt LLMs along with the retrieved context
Node 1 and 4 for the answer.
Additionally, for questions asking about document struc-
tures, we extract the document structure names and locate
their corresponding structural nodes in the KG. For the table
node, we retrieve its markdown formatted content while for
the page node, we traverse its one-hop neighbor and obtain
passages belonging to that page.
Algorithm 1: LLM-based KG Traversal Algorithm to Re-
trieve Relevant Context for Content-based Question.
Input: A question q over a set of documents D, the
constructed KG G = {V, E, X} over D, the
fine-tuned LLM-guided graph traversal fGT, the
preset context budget K, the TF-IDF search
function g.
1 Initialize seed passages Vs = g(V, X, q)
2 Initialize the retrieved passage queue P = [{vi}|vi ∈ Vs]
3 Initialize the candidate neighbor queue C = [Ni|vi ∈ Vs]
4 Initialize the retrieved passage counter k = P
Pi∈P |Pi|
5 while queue P and queue C are not empty do
6 Pi ← P.dequeue(), Ci ← C.dequeue()
7 V′
i = Graph Traversal({q} ∪ Pi, Ci, k) by Eq (1)
8 for v ∈ V′
i do
9 P.enqueue(Pi ∪ {v}), C.enqueue(Nv)
10 k ← k + 1
11 if k > Kthen
12 Terminate
13 return Retrieved Passage Queue P
Here we present the algorithm for our proposed KGP
method for MD-QA. Given a question, we first apply LLM
to classify whether the question is asking about the docu-
ment structure or content. If the question focuses on the doc-
ument structure, we extract the structural keywords such as
Page or Table, and retrieve the content in the correspond-
ing structural nodes in KG. If the question focuses on the
document content, we follow the step according to Algo-
rithm 1. Specifically, we first initialize seeding passages Vs
and the reasoning path queue P by TF-IDF search. Then
for each seeding passage vi ∈ Vs, we add its neighbor-
ing passage nodes Ni into the candidate neighbor queue C
(lines 1-4). After that, we iteratively dequeue the earliest en-
queued reasoning path/candidate neighborhood Pi/Ci from
P/C and employ the fine-tuned LLM-based graph traver-
sal agent to rank the dequeued neighbors in Ci by Eq. (1)
(lines 5-7). Last, we select top-k passage nodesV′
i from Ci to
visit next based on their rank and correspondingly update the
candidate neighbor queue and reasoning path queue (lines 8-
13). The above process terminates when either the candidate
neighbor queue becomes empty or the prefixed budgetK for
the retrieved passages is met. The time and space complexity
are thoroughly analyzed in Section 8.3 in Supplementary.
5 Experiment
In this section, we conduct experiments to verify the pro-
posed knowledge graph prompting method (KGP) for MD-
QA. In particular, we answer the following questions:
• Q1 - Section 5.1: How well does KGP perform MD-QA
compared with existing baselines?
• Q2 - Section 5.2-5.3: How do the quality of the con-
structed KG and the LLM-based graph traversal agent
impact the MD-QA performance?
Due to the space limitation, we comprehensively introduce
our experimental setting, including dataset collection, base-
lines, and evaluation criteria, in Supplementary 8.1-8.2.



### Claim 31/179

#### Claim Text
Specifically, a complex question can be decomposed into a series of simpler sub-questions using the least-to-most prompting method [92].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[92]_2205.10625.pdf (Page 1):

Published as a conference paper at ICLR 2023
subproblems. Both stages are implemented by few-shot prompting, so that there is no training or
ﬁnetuning in either stage. An example usage of least-to-most prompting is illustrated in Figure 1.
The term least-to-most prompting is borrowed from educational psychology (Libby et al., 2008),
where it is used to denote the technique of using a progressive sequence of prompts to help a student
to learn a new skill. Here we apply this technique for teaching humans to teach language models.
Empirical results on symbolic manipulation, compositional generalization, and math reasoning show
that least-to-most prompting can indeed generalize to problems harder than those demonstrated.
Figure 1: Least-to-most prompting solving a math word problem in two stages: (1) query the lan-
guage model to decompose the problem into subproblems; (2) query the language model to sequen-
tially solve the subproblems. The answer to the second subproblem is built on the answer to the ﬁrst
subproblem. The demonstration examples for each stage’s prompt are omitted in this illustration.
2 L EAST -TO-MOST PROMPTING
Least-to-most prompting teaches language models how to solve a complex problem by decomposing
it to a series of simpler subproblems. It consists of two sequential stages:
1. Decomposition. The prompt in this stage contains constant examples that demonstrate the
decomposition, followed by the speciﬁc question to be decomposed.
2. Subproblem solving. The prompt in this stage consists of three parts: (1) constant exam-
ples demonstrating how subproblems are solved; (2) a potentially empty list of previously
answered subquestions and generated solutions, and (3) the question to be answered next.
In the example shown in Figure 1, the language model is ﬁrst asked to decompose the original
problem into subproblems. The prompt that is passed to the model consists of examples that illustrate
how to decompose complex problems (which are not shown in the ﬁgure), followed by the speciﬁc
problem to be decomposed (as shown in the ﬁgure). The language model ﬁgures out that the original
problem can be solved via solving an intermediate problem “How long does each trip take?”.
2



Source: data\tc16_2312.10997v5\referenced_papers\[92]_2205.10625.pdf (Page 55):

Published as a conference paper at ICLR 2023
Q: How many births were there for Asians?
A: “177 births to Asians”. So the answer is 177.
Q: How many births were there for Native Americans?
A: “22 births to American Indians”. So the answer is 22.
Q: How many births were there for Hispanics?
A: “219 births to Hispanics”. So the answer is 219.
Q: How many total births were there for Asians, Native Americans and Hispanics?
A: We know that there were 177 births for Asians. We also know that there were 22 births for Native
Americans. We also know that there were 219 births for Hispanics. So 177 + 22 + 219 = 418 total
births for Asians, Native Americans and Hispanics. So the answer is 418.
10 GSM8K
10.1 E XPERIMENT RESULTS : O NE-SHOT PROMPTS
We compare here the effectiveness on compositional generalization of least-to-most prompting vs.
chain-of-thought prompting by constructing for each prompting method a simple prompt context that
contains a single example that is solvable with just 2 reasoning steps. We then evaluate accuracy
on examples that may involve larger numbers of reasoning steps. The same example is used for
both prompting methods. For the least-to-most prompting prompt, we adopt a simpliﬁed approach
in which the problem decomposition and solution stages are merged into a single pass, with just one
follow-up request being made to the language model to solicit the ﬁnal answer.
The accuracy (%) of the two prompting methods with the GPT-3 code-davinci-002 model,
with breakdown by number of steps in the expected solution, are listed in Table 18.
Accuracy numbers for all prompting methods are calculated after applying the same post-processing
as described in Section 3.3 for DROP.
While the least-to-most prompting accuracy is overall only moderately higher than that of chain-of-
thought prompting, the accuracy breakdown by number of steps shows that least-to-most prompting
signiﬁcantly outperforms chain-of-thought prompting as the number of reasoning steps increases
beyond what was illustrated in the prompt.
Accuracy by Steps All 2 3 4 5+
Least-to-Most (1-shot): aL 62.39 74.53 68.91 59.73 45.23
Chain-of-Thought (1-shot): aC 60.87 76.68 67.29 59.39 39.07
Accuracy change: (aL/aC) −1 +2.49 -2.80 +2.40 +0.58 +15.77
Table 18: Accuracy (%) of a simple 1-shot least-to-most prompt with the GPT-3
code-davinci-002 model on GSM8K, compared to that of a corresponding chain-of-thought
prompt, broken down by number of reasoning steps required in the expected solution. Examples
with 3 or more reasoning steps would require generalizing to more steps than were shown in the
demonstration example in the prompt (which contains just 2 steps).
10.2 E XPERIMENT RESULTS : E NGINEERED PROMPTS
We compare here the overall accuracy of the above-reported “Chain-of-Thought (1-shot)” and
“Least-to-Most (1-shot)” methods with alternative existing prompting methods, as well as with vari-
ants of chain-of-thought and least-to-most prompting in which the prompts were engineered using
multiple in-domain examples taken from the GSM8K train set.
The evaluated prompting methods are as follows (see Appendices 10.3 and 10.4 for the exact prompt
contexts):
• Zero-Shot: Simple zero-shot prompting.
• Standard prompting: Standard few-shot prompting, using the same 4 examples as in the
“problem solving” prompt context of “Least-to-Most (best)”.
56



Source: data\tc16_2312.10997v5\referenced_papers\[92]_2205.10625.pdf (Page 6):

Published as a conference paper at ICLR 2023
3.3 M ATH REASONING
In this section, we apply least-to-most prompting to solve math word problems in GSM8K (Cobbe
et al., 2021) and DROP (Dua et al., 2019). We are particularly interested at seeing if large language
models combined with least-to-most prompting can solve problems more difﬁcult than those seen in
the prompts. Here, we simply measure the difﬁculty by the number of solving steps.
Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together?
A: Let’s break down this problem: 1. How many apples does Anna have? 2. How many apples do
they have together?
1. Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 apples.
2. Elsa and Anna have 5 + 7 = 12 apples together.
The answer is: 12.
Table 9: The least-to-most prompt for solving GSM8K. The demonstration problem is solved in
only two steps, but the prompt can handle problems that require multiple steps to be solved.
Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together?
A: Anna has 2 more apples than Elsa. So Anna has 2 + 5 = 7 apples. So Elsa and Anna have 5 + 7
= 12 apples together.
The answer is: 12.
Table 10: The chain-of-thought prompt for solving GSM8K. It is derived from the least-to-most
prompt in Table 9 by removing the decomposition part.
The prompt that we design to solve GSM8K is shown in Table 9. The demonstration exemplar
consists of two parts. The ﬁrst part (starting from “Let’s break down this problem . . . ”) shows how
the original problem can be decomposed into simpler subproblems, and the the second part shows
how the subproblems are solved in sequence. Note that this prompt combines decomposition and
subproblem solving into a single pass. One may instead design two different prompts respectively
for decomposition and subproblem solving, as the least-to-most prompts in the previous sections, to
further improve performance. Here, we focus on investigating how this simple least-to-most prompt
generalizes from a simple 2-step problem to more complex multi-step problems.
We also construct a chain-of-thought prompt (Table 10) as our baseline. It is derived from the least-
to-most prompt (Table 9) by removing the decomposition part. The results are shown in Table 11.
Overall, least-to-most prompting only slightly improves chain-of-thought prompting: from 60.97%
to 62.39%. However, least-to-most prompting essentially improves chain-of-thought prompting in
solving problems which need at least 5 steps to be solved: from 39.07% to 45.23% (Table 12).
We ﬁnd that almost every problem in GSM8K that least-to-most prompting fails to solve can be
eventually solved by using a manually crafted decomposition. This should not be surprising. For
our humans, as long as we know how to decompose a complex problem into simpler subproblems,
we actually have solved it. For the DROP benchmark, least-to-most prompting outperforms chain-
of-thought prompting by a large margin (Table 11). That is probably because most problems in
DROP can be trivially decomposed.
Method Non-football (DROP) Football (DROP) GSM8K
Zero-Shot 43.86 51.77 16.38
Standard prompting 58.78 62.73 17.06
Chain-of-Thought 74.77 59.56 60.87
Least-to-Most 82.45 73.42 62.39
Table 11: Accuracies (%) of different prompting methods on GSM8K and DROP (only the subset
containing numerical problems). The base language model is code-davinci-002.
7



Source: data\tc16_2312.10997v5\referenced_papers\[92]_2205.10625.pdf (Page 2):

Published as a conference paper at ICLR 2023
In the next phase, we ask the language model to sequentially solve the subproblems from the problem
decomposition stage. The original problem is appended as the ﬁnal subproblem. The solving starts
from passing to the language model a prompt that consists of examples that illustrate how problems
are solved (not shown in the ﬁgure), followed by the ﬁrst subproblem “How long does each trip
take?”. We then take the answer generated by the language model (“... each trip takes 5 minutes.”)
and construct the next prompt by appending the generated answer to the previous prompt, followed
by the next subproblem, which happens to be the original problem in this example. The new prompt
is then passed back to the language model, which returns the ﬁnal answer.
Least-to-most prompting can be combined with other prompting techniques like chain-of-thought
(Wei et al., 2022) and self-consistency (Wang et al., 2022b), but does not need to be. Also, for some
tasks, the two stages in least-to-most prompting can be merged to form a single-pass prompt.
3 R ESULTS
We present least-to-most prompting results for symbolic manipulation, compositional generaliza-
tion, and math reasoning tasks, and compare it with chain-of-thought prompting.
3.1 S YMBOLIC MANIPULATION
We take the last-letter-concatenation task (Wei et al., 2022). In this task, each input is a list of
words, and the corresponding output is the concatenation of the last letters of the words in the list.
For example, “thinking, machine” outputs “ge”, since the last letter of “thinking” is “g” and the
last letter of “machine” is “e”. Chain-of-thought prompting does a perfect job when the testing lists
have the same length as the lists in the prompt exemplars. However, it performs poorly when the
testing lists are much longer than the lists in the prompt exemplars. We show that least-to-most
prompting overcomes this limitation and signiﬁcantly outperforms chain-of-thought prompting on
length generalization.
Q: “think, machine, learning”
A: “think”, “think, machine”, “think, machine, learning”
Table 1: Least-to-most prompt context (decomposition) for the last-letter-concatenation task. It can
decompose arbitrary long lists into sequential subsists with an accuracy of 100%.
Q: “think, machine”
A: The last letter of “think” is “k”. The last letter of “machine” is “e”. Concatenating “k”, “e” leads
to “ke”. So, “think, machine” outputs “ke”.
Q: “think, machine, learning”
A: “think, machine” outputs “ke”. The last letter of “learning” is “g”. Concatenating “ke”, “g”
leads to “keg”. So, “think, machine, learning” outputs “keg”.
Table 2: Least-to-most prompt context (solution) for the last-letter-concatenation task. The two
exemplars in this prompt actually demonstrate a base case and a recursive step.
Least-to-most prompting. The least-to-most prompt contexts for the last-letter-concatenation task
are shown in Tables 1 and 2. The exemplar in Table 1 demonstrates how to decompose a list into
a sequence of sublists. The exemplar in Table 2 demonstrates how to map an input to the desired
output. Given a new list, we ﬁrst append it to the exemplar in Table 1 to construct the decomposition
prompt, which is sent to the language model to obtain the list’s decomposition. Then, we construct
for each sublist S a solution prompt, which consists of the exemplars in Table 2, followed by the
previous sublist/response pairs (if any), followed by S. We sequentially issue these prompts to the
language model and use the last response as the ﬁnal solution.
It is worth a closer look at the exemplars in Table 2. Essentially, they teach language models how to
build answers to new problems using the answers to previously solved problems: (1) the list in the
3



Source: data\tc16_2312.10997v5\referenced_papers\[92]_2205.10625.pdf (Page 0):

Published as a conference paper at ICLR 2023
LEAST -TO-MOST PROMPTING ENABLES COMPLEX
REASONING IN LARGE LANGUAGE MODELS
Denny Zhou†∗ Nathanael Sch¨arli† Le Hou† Jason Wei† Nathan Scales† Xuezhi Wang†
Dale Schuurmans† Claire Cui† Olivier Bousquet† Quoc Le† Ed Chi†
†Google Research, Brain Team
ABSTRACT
Chain-of-thought prompting has demonstrated remarkable performance on vari-
ous natural language reasoning tasks. However, it tends to perform poorly on
tasks which requires solving problems harder than the exemplars shown in the
prompts. To overcome this challenge of easy-to-hard generalization, we propose
a novel prompting strategy, least-to-most prompting. The key idea in this strat-
egy is to break down a complex problem into a series of simpler subproblems
and then solve them in sequence. Solving each subproblem is facilitated by the
answers to previously solved subproblems. Our experimental results on tasks re-
lated to symbolic manipulation, compositional generalization, and math reason-
ing reveal that least-to-most prompting is capable of generalizing to more difﬁcult
problems than those seen in the prompts. A notable ﬁnding is that when the GPT-3
code-davinci-002 model is used with least-to-most prompting, it can solve
the compositional generalization benchmark SCAN in any split (including length
split) with an accuracy of at least 99% using just 14 exemplars, compared to only
16% accuracy with chain-of-thought prompting. This is particularly noteworthy
because neural-symbolic models in the literature that specialize in solving SCAN
are trained on the entire training set containing over 15,000 examples. We have
included prompts for all the tasks in the Appendix.
1 I NTRODUCTION
Despite the great success of deep learning in the past decade, there still remain huge differences
between human intelligence and machine learning: (1) Given a new task, humans usually can learn
to accomplish it from only a few demonstration examples, while machine learning requires a large
amount of labeled data for model training; (2) Humans can clearly explain the underlying rationale
for their predictions or decisions, while machine learning is essentially a black box; (3) Humans can
solve problems more difﬁcult than any they have seen before, while for machine learning, examples
in training and testing are typically at the same level of difﬁculty.
The recently proposed chain-of-thought prompting approach (Wei et al., 2022; Chowdhery et al.,
2022) has taken a signiﬁcant step for narrowing the gap between human intelligence and machine in-
telligence. It combines the idea of natural language rationales (Ling et al., 2017; Cobbe et al., 2021)
with few-shot prompting (Brown et al., 2020). When further integrated with self-consistency decod-
ing (Wang et al., 2022b) rather than using the typical greedy decoding, few-shot chain-of-thought
prompting largely outperforms the state-of-the-art results in the literature on many challenging natu-
ral language processing tasks obtained from specially designed neural models trained with hundreds
of times more annotated examples, while being fully interpretable.
However, chain-of-thought prompting has a key limitation—it often performs poorly on tasks that
require generalization of solving problems harder than the demonstration examples, such as com-
positional generalization (Lake & Baroni, 2018; Keysers et al., 2020). To tackle such easy-to-hard
generalization issues, we propose least-to-most prompting. It consists of two stages: ﬁrst decom-
posing a complex problem into a list of easier subproblems, and then sequentially solving these
subproblems, whereby solving a given subproblem is facilitated by the answers to previously solved
∗Corresponding to: dennyzhou@google.com
1
arXiv:2205.10625v3  [cs.AI]  16 Apr 2023



### Claim 32/179

#### Claim Text
Validated expanded queries typically exhibit higher reliability [93]. 9 2) Query Transformation: The core concept is to retrieve chunks based on a transformed query instead of the user’s original query.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[62]_2307.11019.pdf (Page 15):

Sparse ChatGPT Dense0
10
20
30
40
50
60
70
80Recall Rate
40.60
59.20
72.80
(a) Recall Rate
Sparse ChatGPT Dense0
5
10
15
20
25
30
35
40Positive Percentage
17.08
32.22
37.32 (b) Positive Percentage
Sparse ChatGPT Dense0.0
0.5
1.0
1.5
2.0
2.5
3.0Answers per 1k T okens
0.80
2.77
1.59 (c) Answers per 1K Tokens
Sparse ChatGPT Dense0
10
20
30
40
50EM
GPT-4
ChatGPT
LLaMA2-Chat
Mistral
(d) Exact Match
Sparse ChatGPT Dense0
10
20
30
40
50
60
70Give up
GPT-4
ChatGPT
LLaMA2-Chat
Mistral (e) Give-up Rates
Sparse ChatGPT Dense0
20
40
60
80
100Eval-Acc
GPT-4
ChatGPT
LLaMA2-Chat
Mistral (f) Eval-Acc
Figure 4: Metrics related to retrieval, QA, and judgement using different retrievers.
from the fact that documents generated by
ChatGPT are more concise and the documents are
closely related to the given questions. Although
Wikipedia documents from the dense retriever
have a higher recall rate and proportion of positive
documents, they are relatively longer and contain
more irrelevant information. As discussed earlier,
this could be one of the reasons limiting the upper
bound of retrieval augmentation. Furthermore,
it could potentially contribute to the heightened
confidence levels observed in LLMs when utilizing
ChatGPT-generated documents as supporting
documents, as evidenced by their low Give-up
rates. In addition, the impact of different retrieval
sources on Eval-Acc is not substantial.
B.2 Analysis on Query Types
Retrieval augmentation can change the query
category preference of LLMs in QA and fac-
tual knowledge boundary perception. In order to
investigate the propensity of LLMs to handle ques-
tions with varied characteristics, we separately cal-
culate the answer accuracy of LLMs across differ-
ent question categories. To achieve this, we utilize
supporting documents retrieved by the dense re-
triever. As shown in Figure 5, we can see that Chat-
GPT and LLaMA2 achieve the highest EM when
dealing with questions in the “ who” and “which”
category, indicating these types of questions may
be the strong suit of the two LLMs. On the other
hand, ChatGPT and LLaMA2 may not suffice for
the question type of “how” in knowledge-intensive
scenarios. When retrieval augmentation is incor-
porated, we observe that the preference of LLMs
changes. The overall answer accuracies of LLMs
are improved, and the accuracies in most categories
increase proportionately. Specially, both ChatGPT
and LLaMA2 perform best on the question type
“who”. We found thatEM for certain question types
does not increase with the overall EM increase
(e.g., “how” and “yesno” for ChatGPT, "which" for
LLaMA2), indicating that retrieval augmentation
cannot effectively enhance the QA performance
for all question types. Figure 6 shows the Eval-
Acc metric in each query type for ChatGPT and
LLaMA2. We can see that most types of ques-
tions achieve better posteriori judgement accura-
cies, except “how” and “yesno” for LLaMA2 and
“yesno” for ChatGPT. Similarly, the result indicates



Source: data\tc16_2312.10997v5\referenced_papers\[62]_2307.11019.pdf (Page 7):

LLMs Supporting Doc EM F1 Give-up Right/G Right/ ¬G Eval-Right Eval-Acc
Davinci-003
None 27.20 36.20 29.20% 13.70% 32.77% 72.80% 45.01%
Golden 50.60 62.93 15.80% 15.19% 57.24% 52.00% 71.08%
Retrieved 39.00 51.27 12.80% 14.06% 42.66% 46.40% 71.43%
High-related 10.20 20.66 18.00% 8.89% 10.49% 28.40% 57.89%
Weak-related 11.80 19.69 41.40% 10.63% 12.63% 20.80% 61.71%
Random 23.00 30.82 88.40% 20.59% 41.38% 19.40% 66.26%
ChatGPT
None 33.40 45.32 57.40% 26.48% 42.72% 84.40% 43.40%
Golden 50.00 64.28 22.60% 23.01% 57.88% 75.20% 53.24%
Retrieved 39.40 52.65 26.60% 18.05% 47.14% 68.80% 53.56%
High-related 16.20 28.20 42.00% 13.81% 17.93% 56.20% 47.82%
Weak-related 18.40 29.86 60.20% 16.61% 21.11% 49.80% 46.21%
Random 24.80 35.35 91.00% 23.30% 40.00% 29.80% 48.80%
GPT-4
None 34.60 48.72 15.20% 9.21% 39.15% 90.20% 38.87%
Golden 53.60 67.36 15.60% 20.51% 59.72% 73.00% 53.58%
Retrieved 43.60 56.36 12.60% 15.87% 47.60% 66.40% 50.86%
High-related 21.60 35.13 39.20% 24.49% 19.74% 62.40% 47.21%
Weak-related 24.40 34.83 61.20% 24.18% 24.74% 60.00% 44.96%
Random 34.40 45.43 71.40% 25.49% 56.64% 54.00% 42.50%
LLaMA2
None 16.60 24.26 6.60% 3.03% 17.56% 58.40% 46.74%
Golden 48.60 61.33 18.40% 29.35% 52.94% 7.60% 72.12%
Retrieved 33.40 45.39 24.80% 20.16% 37.77% 5.20% 73.08%
High-related 9.40 19.07 30.60% 8.50% 9.80% 4.80% 67.86%
Weak-related 8.80 16.00 50.20% 9.16% 8.43% 6.20% 59.09%
Random 13.20 19.34 93.40% 13.28% 12.12% 5.80% 58.97%
Mistral
None 11.20 19.30 69.80% 8.02% 18.54% 78.20% 31.65%
Golden 47.80 60.93 39.60% 28.28% 60.60% 50.20% 58.67%
Retrieved 35.20 45.82 40.00% 21.50% 44.33% 50.20% 56.39%
High-related 5.60 14.23 58.40% 4.11% 7.69% 47.60% 53.35%
Weak-related 5.40 11.21 76.80% 3.91% 10.34% 46.60% 53.48%
Random 12.40 18.40 98.40% 11.79% 50.00% 64.80% 33.57%
Table 3: Evaluation results of retrieval-augmented LLMs with supporting documents of various qualities on NQ,
where the supporting documents are obtained from the dense retriever. We place different settings according to the
relevance between the documents and the question from high to low.
We employ Wikipedia as the document corpus, and
sample ten documents per question from the re-
trieval results acquired by the dense retriever for
each sampling strategy.
3.3.2 Main Findings
LLMs demonstrate enhanced capabilities in QA
abilities and perception of knowledge bound-
aries when provided with higher quality sup-
porting documents. We employ the sampling
strategy in Section 3.3.1 to obtain supporting doc-
uments for each question. Table 3 presents the
results. We can see that using golden (high-quality)
documents as supporting documents yields better
performance compared to using retrieval results.
However, if incorrect (low-quality) documents are
utilized, including high-related, weak-related, and
randomly selected incorrect documents, the perfor-
mance of LLMs may be compromised, resulting
in inferior performance compared to employing re-
trieval results as supporting documents or respond-
ing without supporting documents. In addition,
the give-up rates of LLMs decrease as the qual-
ity of supporting documents improves, indicating
that LLMs exhibit higher confidence when fortified
with high-quality supporting documents. More-
over, with higher quality supporting documents,
the Eval-Acc rates of LLMs increase, showing that
LLMs demonstrate higher accuracy in perceiving
their factual knowledge boundaries.
LLMs cannot handle the conflicts between inter-
nal and external knowledge well. Based on the
above observation, when LLMs generate responses
with low-quality supporting documents, the per-
formance is inferior to generating responses based
on internal knowledge (without retrieval augmen-
tation). This phenomenon indicates that LLMs
heavily rely on the given supporting documents
during the generation process. Note that we give
LLMs the option in the prompt to decide whether
to use the supporting documents for a question.
However, LLMs still tend to rely on supporting



Source: data\tc16_2312.10997v5\referenced_papers\[62]_2307.11019.pdf (Page 4):

Datasets LLMs Retrieval Source
QA Priori Judgement Posteriori Judgement
EM F1 Give-up Right/G Right/ ¬G Eval-Right Eval-Acc
NQ
Davinci003
Sparse 27.80 38.29 21.20% 12.26% 31.98% 39.40% 67.94%
Dense 39.00 51.27 12.80% 14.06% 42.66% 46.40% 71.43%
ChatGPT 34.00 47.36 6.20% 6.45% 35.82% 46.00% 71.54%
ChatGPT
Sparse 28.40 41.10 42.40% 17.92% 36.11% 67.00% 48.77%
Dense 39.40 52.65 26.60% 18.05% 47.14% 68.80% 53.56%
ChatGPT 32.20 47.37 7.40% 2.70% 34.56% 78.80% 49.90%
GPT-4
Sparse 34.20 45.81 28.20% 14.18% 42.06% 57.20% 48.48%
Dense 43.60 56.36 12.60% 15.87% 47.60% 66.40% 50.86%
ChatGPT 34.40 48.56 4.20% 4.76% 35.70% 69.80% 48.69%
LLaMA2
Sparse 23.00 34.14 32.80% 15.85% 26.49% 6.00% 75.00%
Dense 33.40 45.39 24.80% 20.16% 37.77% 5.20% 73.08%
ChatGPT 33.40 48.19 5.20% 15.38% 34.39% 5.00% 87.88%
Mistral
Sparse 23.20 33.21 59.00% 13.22% 37.56% 48.60% 54.71%
Dense 35.20 45.82 40.00% 21.50% 44.33% 50.20% 56.39%
ChatGPT 32.60 47.49 14.40% 16.67% 35.28% 41.00% 64.24%
TriviaQA
Davinci003
Sparse 64.60 70.19 15.60% 19.23% 72.99% 69.00% 77.15%
Dense 69.60 75.31 10.00% 30.00% 74.00% 74.40% 81.49%
ChatGPT 67.40 75.43 2.00% 10.00% 68.57% 72.20% 81.00%
ChatGPT
Sparse 62.60 69.98 23.00% 34.78% 70.91% 79.80% 73.29%
Dense 66.20 74.75 18.20% 39.56% 72.13% 82.40% 75.73%
ChatGPT 65.00 74.44 3.00% 13.33% 66.60% 90.40% 74.34%
GPT-4
Sparse 66.20 75.99 12.40% 35.48% 70.55% 83.40% 76.79%
Dense 69.00 78.01 7.20% 30.56% 71.98% 85.80% 76.51%
ChatGPT 66.40 76.33 2.60% 15.38% 67.76% 83.40% 73.39%
LLaMA2
Sparse 51.00 59.51 35.60% 40.45% 56.83% 13.20% 70.19%
Dense 58.60 66.57 33.40% 40.72% 67.57% 11.40% 75.00%
ChatGPT 63.00 71.76 2.80% 28.57% 63.99% 18.20% 79.35%
Mistral
Sparse 52.20 59.55 30.40% 20.39% 66.09% 59.20% 68.75%
Dense 57.40 65.59 24.20% 26.45% 67.28% 59.80% 72.53%
ChatGPT 62.20 71.72 3.60% 16.67% 63.90% 55.20% 77.76%
HotpotQA
Davinci003
Sparse 31.20 40.95 27.20% 14.71% 37.36% 31.20% 76.84%
Dense 26.80 35.89 37.00% 13.51% 34.60% 35.20% 76.89%
ChatGPT 28.20 39.34 8.20% 12.20% 29.63% 33.40% 77.37%
ChatGPT
Sparse 29.60 41.28 50.60% 17.39% 42.11% 51.80% 54.90%
Dense 26.40 35.75 58.40% 14.38% 43.27% 48.20% 56.10%
ChatGPT 26.40 38.30 11.20% 7.14% 28.83% 68.20% 48.24%
GPT-4
Sparse 36.00 47.71 25.60% 14.84% 43.28% 43.40% 64.90%
Dense 31.80 43.92 37.00% 17.30% 40.32% 46.00% 60.34%
ChatGPT 29.80 41.67 8.80% 6.82% 32.02% 48.40% 68.55%
LLaMA2
Sparse 24.00 33.45 45.60% 16.67% 30.15% 8.60% 58.89%
Dense 21.60 31.17 57.00% 13.68% 32.09% 7.60% 66.99%
ChatGPT 25.80 37.56 11.80% 22.03% 26.30% 7.20% 82.53%
Mistral
Sparse 25.00 35.49 52.40% 13.74% 37.39% 42.40% 62.42%
Dense 23.60 32.70 59.80% 13.38% 38.81% 45.60% 59.75%
ChatGPT 26.20 37.93 14.00% 12.86% 28.37% 37.60% 70.04%
Table 2: Results of retrieval-augmented LLMs, the abbreviations are explained in Section 2.3.1.
certain extent. Moreover, it can be observed that
closed source LLMs overall exhibit better QA per-
formance than publicly available LLMs we used,
and GPT-4 obtains the best QA performance. Over-
all, closed source LLMs show a higher accuracy in
percepting their factual knowledge boundary.
3.2 What Impact Does Retrieval
Augmentation Have on LLMs?
Following the analysis under the closed-book set-
ting, we next study the retrieval augmentation set-
ting. Specifically, with the supporting documents
from retrievers, we employ the priori judgement to
determine whether to give up answering, and the
posteriori judgement to assess the correctness of
answers generated by LLMs. Additionally, we em-
ploy QA prompting to guide LLMs in answering
the questions.
3.2.1 Main Findings
We conduct our analysis from three perspectives:
knowledge utilization, recognition of knowledge
boundaries, and the impact of documents from vari-
ous sources. After thorough retrieval augmentation
experiments under different settings in Table 2, we
arrive at the following findings.
LLMs cannot sufficiently utilize their internal
knowledge, while retrieval augmentation can



Source: data\tc16_2312.10997v5\referenced_papers\[30]_2312.06648.pdf (Page 5):

101 102 103
Frequency
20
40
60Recall@5
SimCSE
101 102 103
Frequency
20
40
60Recall@5
Contriever
101 102 103
Frequency
40
50
60Recall@5
DPR
101 102 103
Frequency
50
60
70
80Recall@5
GTR
Passage Sentence Proposition
Figure 3: Document retrieval recall vs. the frequency of the target entity in each question from the Entity Questions
dataset. The frequency of each entity (i.e. smaller value ⇒ less common entities, and vice versa) is estimated by
the frequency of the entity in its top-1000 passage retrieved by BM25. On queries with less common entities, we
observe that retrieving by proposition shows a larger advantage over retrieval by proposition.
Retriever Granularity NQ TQA WebQ SQuAD EQ Avg.
top-5 top-20 top-5 top-20 top-5 top-20 top-5 top-20 top-5 top-20 top-5 top-20
Unsupervised Dense Retrievers
SimCSE Passage 16.6 23.6 32.3 40.8 15.5 19.1 14.6 20.7 16.1 20.3 19.0 24.9
Sentence 20.7 28.1 36.0 44.5 18.5 21.9 19.6 25.8 19.9 25.1 23.0 29.1
Proposition24.5 33.1 37.5 46.2 19.7 23.0 21.4 27.6 26.8 32.0 26.0 32.4
Contriever Passage 23.2 35.1 40.8 50.8 16.3 22.1 23.9 32.7 20.2 27.9 24.9 33.7
Sentence 26.0 36.8 43.4 52.9 18.4 23.9 26.7 34.7 23.7 30.3 27.6 35.7
Proposition28.9 39.2 47.2 55.6 19.5 25.2 30.8 37.6 28.8 35.8 31.1 38.7
Supervised Dense Retrievers
DPR Passage 41.1 45.6 50.6 57.0 23.7 25.5 18.8 25.4 25.3 29.7 31.9 36.6
Sentence 40.3 45.6 51.7 57.6 24.0 26.9 21.1 27.4 28.6 32.9 33.1 38.1
Proposition39.7 45.2 51.0 56.8 24.3 27.5 22.2 28.3 32.0 36.0 33.9 38.8
GTR Passage 39.8 46.1 49.7 55.9 23.0 25.9 29.9 35.1 37.8 39.6 36.0 40.5
Sentence 39.4 45.9 51.7 58.0 23.2 26.1 35.7 39.1 38.0 39.9 37.6 41.8
Proposition40.0 46.9 52.5 58.4 24.2 26.5 37.8 40.4 39.2 41.0 38.7 42.6
Table 4: Open-domain QA performance (Exact Match) using Fusion-in-Decoder model (Izacard and Grave, 2021)
to extract answer from top-5 and top-20 passages retrieved on the index of passages, sentences, and propositions.
with 16% relative improvements.
5.2 Retrieval on Finer-grained Index ⇒
Better Cross-Task Generalization
Our results show the advantage of retrieval on
proposition-level index in cross-task generalization
settings. We observe that on SQuAD and Entity
Questions, retrieval on the proposition-level index
brings more performance gain over the passage-
level index and sentence-level index.
To better understand where the improvements
can be attributed, we conduct an additional analysis
on Entity Questions. As Entity Questions features
questions targeting the properties of longer-tail enti-
ties, we study how the retrieval performance under
three different granularities is affected by the occu-
rance of the target entity in question, i.e. whether
the entity appears frequently in Wikipedia or not.
We estimate the frequency of each entity with the
following method. Given the surface form of an en-
tity, we use BM25 to retrieve the top 1000 relevant
passages from Wikipedia. We use the number of
occurrences of the entity in its relevant passages as
an estimate of its frequency. With the 20,000 test
queries, around 25% of the target entities have an
frequency value of less or equal to 3.
Figure 3 shows the passage retrieval perfor-
mance vs. the frequency of the target entity in
each question. Across all four dense retrievers,
we observe that retrieving by proposition shows a
much larger advantage over retrieving by passages
with questions targeting less common entities. As
the frequency of entities increases, the performance
gap decreases. Our findings indicate that the per-
formance gain from retrieval by proposition can
mostly be attributed to queries for long-tailed infor-
mation. This echoes our observation that retrieval
on proposition-level index improves the cross-task
generalization performance of dense retrievers.
5.3 Higher Passage Recall ⇒ Higher
Downstream QA Accuracy
To further understand whether the passage retrieval
on a finer-grained index achieves higher down-
6



Source: data\tc16_2312.10997v5\referenced_papers\[82]_2310.12836.pdf (Page 6):

Table 1: Results on Natural Questions and HotpotQA for open-domain question answering and WebQSP and Mintaka
for knowledge graph question answering, with FLAN of different sizes as the LM. We emphasize the best results in bold.
Base (250M) Large (780M) XL (3B)
Datasets Methods F1 EM Acc F1 EM Acc F1 EM Acc
Natural Questions
w/ Wikipedia
Naive Language Models 7.53 3.24 4.57 11.09 6.29 7.81 16.89 11.16 12.94
Knowledge-Augmented LMs 18.06 12.30 15.26 18.61 13.74 16.40 19.03 14.13 16.90
Adaptive Retrieval w/ Confidence 16.70 11.02 14.07 18.16 13.07 15.60 20.89 15.76 18.28
LLM-Augmenter w/ Knowledge F1 19.58 13.56 16.81 28.53 21.22 25.32 31.00 23.06 27.59
LLM-Augmenter w/ Confidence 19.91 14.14 17.19 20.19 14.97 18.29 22.88 17.17 20.49
KALMV (Ours) 52.98 42.36 50.43 56.80 46.13 53.57 67.43 58.06 63.17
HotpotQA
w/ Wikipedia
Naive Language Models 14.25 9.68 10.36 16.80 11.78 12.41 21.97 15.06 16.22
Knowledge-Augmented LMs 31.20 22.77 25.13 33.46 25.29 27.37 35.47 27.08 29.14
Adaptive Retrieval w/ Confidence 26.82 19.10 21.11 26.80 19.65 21.23 29.41 21.55 23.54
LLM-Augmenter w/ Knowledge F1 32.89 23.24 26.12 39.40 28.55 31.60 46.97 34.54 37.72
LLM-Augmenter w/ Confidence 34.75 25.67 28.20 35.78 27.29 29.38 40.57 31.35 33.71
KALMV (Ours) 64.06 52.31 55.84 63.74 52.39 55.98 67.21 54.99 58.07
WebQSP
w/ Wikidata
Naive Language Models 32.53 21.35 25.78 40.33 30.08 32.74 46.20 36.43 40.11
Knowledge-Augmented LMs 53.57 43.25 53.68 42.37 26.13 62.28 49.45 36.02 59.28
Adaptive Retrieval w/ Entity 49.13 37.79 46.32 47.81 35.68 49.32 51.99 41.54 51.16
Adaptive Retrieval w/ Confidence 46.76 36.49 43.66 48.32 36.56 51.98 53.17 43.32 53.89
LLM-Augmenter w/ Knowledge F1 56.42 45.95 56.26 44.41 27.79 64.56 51.95 38.12 61.96
LLM-Augmenter w/ Confidence 56.62 47.33 56.36 44.35 28.79 64.47 50.63 36.62 60.67
KALMV (Ours) 74.31 63.92 77.78 54.79 45.46 82.71 67.10 50.81 83.21
Mintaka
w/ Wikidata
Naive Language Models 16.16 8.53 10.59 20.90 12.83 14.46 26.99 19.08 21.22
Knowledge-Augmented LMs 24.28 15.46 19.15 24.57 15.39 23.77 27.74 18.23 22.92
Adaptive Retrieval w/ Entity 23.66 14.68 17.87 25.96 16.45 22.92 30.34 21.36 24.20
Adaptive Retrieval w/ Confidence 21.46 13.15 16.06 25.34 16.28 22.07 29.00 20.68 23.70
LLM-Augmenter w/ Knowledge F1 27.99 18.18 22.14 28.19 18.07 27.15 34.23 22.77 28.05
LLM-Augmenter w/ Confidence 28.16 18.74 22.26 28.46 18.88 27.42 33.24 22.55 27.31
KALMV (Ours) 59.29 51.52 59.13 53.15 42.30 62.87 58.15 48.44 59.11
Table 2: Results on WebQSP and Mintaka, where we use
Wikipedia as the knowledge source and report results with F1.
Datasets Methods Base Large XL
WebQSP
Naive Language Models 32.53 40.33 46.20
Knowledge-Augmented LMs 27.96 27.39 26.40
Adaptive Retrievalw/ Confidence 36.15 41.68 44.89
LLM-Augmenterw/ Knowledge F128.35 38.14 41.21
LLM-Augmenterw/ Confidence 30.01 28.75 29.70
KALMV (Ours) 56.70 60.63 63.75
Mintaka
Naive Language Models 16.16 20.90 26.99
Knowledge-Augmented LMs 27.10 26.25 28.32
Adaptive Retrievalw/ Confidence 24.74 26.20 28.87
LLM-Augmenterw/ Knowledge F129.84 40.30 43.87
LLM-Augmenterw/ Confidence 28.81 27.64 30.91
KALMV (Ours) 65.49 66.48 70.83
of error-rectifying steps in the range of {1, 2, 3},
and filter out answers that are determined to have
errors by our verifier after the maximum step. Fur-
ther, for the ensemble, we use 5 different outputs,
which have the probabilities of three choices (Sec-
tion 3.2), from 5 different instructions, and average
probabilities to select one option for verification.
5 Experimental Results and Analyses
Main Results We conduct experiments on two
question answering tasks: open-domain QA with
Wikipedia and knowledge graph QA with Wikidata.
As shown in Table 1, our proposed KALMV sig-
nificantly improves the performance of knowledge-
augmented LMs on all datasets across different LM
sizes by effectively verifying errors in the knowl-
Ratio Ret. Gro. Cor.
0
20
40
60
80
100
Verification Accuracy
33%
14%
53%
WebQSP
Ratio Ret. Gro. Cor.
76%
11%
13%
Mintaka
Ratio Ret. Gro. Cor.
76%
8%
15%
Natural Questions
Ratio Ret. Gro. Cor.
65%
10%
25%
HotpotQA
Retrieval Verification Groundness Verification Correctness Verification
Figure 2: Ratios of verification types and verification accu-
racies on them, on each dataset with the FLAN Base as LMs.
edge retrieval and answer generation steps. In addi-
tion, for knowledge graph QA, we also validate our
KALMV on the setting where LMs are augmented
with the documents from Wikipedia in Table 2, on
which it also outperforms baselines substantially.
Note that LLM-Augmenter, which verifies whether
the generated answers are grounded in the retrieved
knowledge, shows decent performance compared
to other baselines. However, KALMV outperforms
it by large margins, which suggests the importance
of verifying the retrieval error and training the sepa-
rate LM compared to using the heuristic measure to
verify only the groundedness in answer generation.
Analyses on Verification To understand how the
proposed verifier works, we analyze it in multiple
aspects. In the first bar of each subplot in Fig-
ure 2, we report the percentages of the knowledge
retrieval error, the knowledge grounding error, and
the correct generation, and we can see that the most
common errors come from the incorrect knowledge



### Claim 33/179

#### Claim Text
In addition to using LLM for query rewriting, specialized smaller language models, such as RRR (Rewrite-retrieve-read) [7].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 0):

Query Rewriting for Retrieval-Augmented Large Language Models
Xinbei Ma1,2,∗ , Yeyun Gong3, #, †, Pengcheng He4, #, Hai Zhao1,2,†, Nan Duan3
1Department of Computer Science and Engineering, Shanghai Jiao Tong University
2Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University
3Microsoft Research Asia 4Microsoft Azure AI
sjtumaxb@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn,
{yegong, nanduan}@microsoft.com, Herbert.he@gmail.com
Abstract
Large Language Models (LLMs) play pow-
erful, black-box readers in the retrieve-then-
read pipeline, making remarkable progress
in knowledge-intensive tasks. This work in-
troduces a new framework, Rewrite-Retrieve-
Read instead of the previous retrieve-then-read
for the retrieval-augmented LLMs from the per-
spective of the query rewriting. Unlike prior
studies focusing on adapting either the retriever
or the reader, our approach pays attention to
the adaptation of the search query itself, for
there is inevitably a gap between the input text
and the needed knowledge in retrieval. We
first prompt an LLM to generate the query,
then use a web search engine to retrieve con-
texts. Furthermore, to better align the query
to the frozen modules, we propose a trainable
scheme for our pipeline. A small language
model is adopted as a trainable rewriter to cater
to the black-box LLM reader. The rewriter is
trained using the feedback of the LLM reader
by reinforcement learning. Evaluation is con-
ducted on downstream tasks, open-domain QA
and multiple-choice QA. Experiments results
show consistent performance improvement, in-
dicating that our framework is proven effective
and scalable, and brings a new framework for
retrieval-augmented LLM 1.
1 Introduction
Large Language Models (LLMs) have shown re-
markable abilities for human language processing
and extraordinary scalability and adaptability in
few- or zero-shot settings.(Ouyang et al., 2022;
Brown et al., 2020; Chowdhery et al., 2022). How-
ever, the training process depends on large-scale
high-quality corpora but without the perception
∗ Work done during an internship at 3Microsoft Research
Asia. # Equal contribution. †Corresponding author.
This paper was partially supported by Joint Research
Project of Yangtze River Delta Science and Technology Inno-
vation Community (No. 2022CSJGG1400).
1https://github.com/xbmxb/RAG-query-rewriting
of the real world. Thus, LLMs still have to face
the issue of hallucination (Yao et al., 2023; Bang
et al., 2023) and temporal misalignment (Röttger
and Pierrehumbert, 2021; Luu et al., 2022; Jang
et al., 2022). This affects the reliability of LLMs
and hinders wider practical application, because
the consistency between the LLM responses with
the real world needs further validation. Exist-
ing work has proved that incorporating external
knowledge (i.e., non-parametric knowledge) with
internal knowledge (i.e., parametric knowledge)
can effectively alleviate hallucination, especially
for knowledge-intensive tasks. In fact, retrieval-
augmented LLMs have been shown so effective
that they have been regarded as a standard solu-
tion to alleviate the factuality drawbacks in naive
LLM generations. Retrieval augmentation is ap-
plied to select relative passages as external contexts
for the language model, which isretrieve-then-read
framework (Lewis et al., 2020b; Karpukhin et al.,
2020; Izacard et al., 2022). Take the open-domain
Question-Answering task (open-domain QA) as
an example, a retriever first searches for related
documents for a question. Then the LLM receives
the question and the documents, then predicts an
answer.
As most LLMs are only accessible through infer-
ence APIs, they play the part of black-box frozen
readers in the pipeline. This makes previous re-
trieval augmentation methods that require complete
access (Lewis et al., 2020b; Guu et al., 2020; Izac-
ard et al., 2022) no longer feasible. Recent studies
on retrieval-augmented language models lean more
on the LLM-oriented adaptation. An idea is to train
a dense retrieval model to cater to the frozen lan-
guage model (Shi et al., 2023). By using feedback
from the LLM as a training objective, the retrieval
model is tuned for better LLM input contexts. An-
other research line focuses on the design of inter-
actions between the retriever and the reader (Yao
et al., 2023; Khattab et al., 2022), where both the
arXiv:2305.14283v3  [cs.CL]  23 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 1):

retriever and the reader are usually frozen. The idea
is to trigger the emergent ability through carefully
crafted prompts or a sophisticated prompt pipeline.
Multiple interactions with external knowledge al-
low the LLM to approach the correct answer step
by step.
However, there are still problems remaining to
be solved. Existing approaches overlook the adap-
tation of the query, i.e., the input of the retrieve-
then-read pipeline. The retrieval query is either
original from datasets or directly determined by the
black-box generation, thus is always fixed. How-
ever, there is inevitably a gap between the input
text and the knowledge that is really needed to
query. This limits performance and places a burden
on retrieval capability enhancement and prompt
engineering.
In consideration of this issue, this paper pro-
poses Rewrite-Retrieve-Read, a new framework for
retrieval augmentation, which can be further tuned
for adapting to LLMs. In front of the retriever, a
step of rewriting the input is added, filling the gap
between the given input and retrieval need, as is
shown in Figure 1. We adopt the off-the-shelf tool,
an internet search engine, as the retriever, which
avoids the maintenance of the search index and
can access up-to-date knowledge (Lazaridou et al.,
2022). Different from previous studies (Khattab
et al., 2022; Yao et al., 2023) that require the mem-
ory of multiple interaction rounds between the re-
triever and the LLM for each sample, the motiva-
tion of our rewriting step is to clarify the retrieval
need from the input text.
We also propose a trainable scheme for our
rewrite-retrieve-read framework (Figure 1 (c)).
The black-box retriever and the reader form a
frozen system. To further smooth the steps of
our pipeline, we apply a small, trainable language
model to perform the rewriting step, denoted as the
rewriter. The rewriter is trained by reinforcement
learning using the LLM performance as a reward,
learning to adapt the retrieval query to improve the
reader on downstream tasks.
Our proposed methods are evaluated on
knowledge-intensive downstream tasks including
open-domain QA (HotpoQA (Yang et al., 2018),
AmbigNQ (Min et al., 2020), PopQA (Mallen
et al., 2022)) and multiple choice QA (MMLU
(Hendrycks et al., 2021)). The experiments are
implemented on T5-large (Raffel et al., 2020) as
the rewriter, ChatGPT (Ouyang et al., 2022) and
Vicuna-13B (Chiang et al., 2023) as the LLM
reader. The results show that query rewriting con-
sistently improves the retrieve-augmented LLM
performance. The results also indicate that the
smaller language model can be competent for query
rewriting.
To sum up, our proposed novel retrieval-
augmentation method, rewrite-retrieve-read is the
first framework where the input text is adapted for
the frozen retriever and LLM reader. We introduce
a tuneable scheme with a small, trainable model,
achieving performance gains with less resource
consumption.
2 Related Work
2.1 Retrieval Augmentation
Language models require external knowledge to al-
leviate the factuality drawbacks. Retrieval augmen-
tation has been regarded as the standard effective
solution. With a retrieval module, related passages
are provided to the language model as the context
of the original input. Thus factual information like
common sense or real-time news helps with output
prediction through contextualized reading compre-
hension.
Earlier studies use sparse retriever (Chen et al.,
2017) or dense retriever (Karpukhin et al., 2020)
in front of a pre-trained language model (PrLM).
The neural retriever and reader are both PrLMs
of trainable size like BERT (Devlin et al., 2019)
or BART (Lewis et al., 2020a). Hence, the whole
retrieve-then-reader framework is a tuneable end-
to-end system, where the retrieved contexts can
be regarded as the intermediate results (Karpukhin
et al., 2020; Lewis et al., 2020b). Approaches to
smooth the two-step framework are proposed to op-
timize the retrieval and the reading comprehension
(Sachan et al., 2021; Lee et al., 2022; Jiang et al.,
2022). More recently, retrieval remains a powerful
enhancement as the size of models and data scales
rapidly (Mallen et al., 2022; Shi et al., 2023; Brown
et al., 2020). On the other hand, retrieval enhance-
ment can compensate for the shortfall in parameter
size, compared to large-scale language models. For
example, by jointly training the retriever and the
reader, Atlas (Izacard et al., 2022) shows few-shot
performance on par with 540B PalM (Chowdhery
et al., 2022) but be of 50× smaller size.
The Internet as a knowledge baseMore related
to our work, the search engine can assume the role
of the retriever and use the Internet as the source of



Source: data\tc16_2312.10997v5\referenced_papers\[173]_2403.10131.pdf (Page 8):

Preprint, Under Review
2 4 6 8 10
# T est Documents (T op-k)
0.22
0.24
0.26
0.28
0.30
0.32Final Accuracy
Natural Questions
Train D*
Train D* + 1D
Train D* + 2D
Train D* + 3D
2 4 6 8 10
# T est Documents (T op-k)
0.125
0.150
0.175
0.200
0.225
0.250Final Accuracy
Hotpot QA
Train D*
Train D* + 1D
Train D* + 2D
Train D* + 3D
Figure 6: Test-Time Documents Varying: To analyze how robust RAFT is to varying number
of test-time documents, we study three domains – NQ, Trivia QA and HotPot QA. In NQ,
we find that training with 4 documents leads to optimal performance, and this changes to 3
and 2 for for Trivia QA and HotPot QA respectively. However, we see that training with
only golden documents leads to poor performance.
training with D∗ + 3D and it is D∗ + 1D documents with Hotpot QA. This insight has been
particularly beneficial for our algorithm, RAFT . In our experiments, we consistently employ
a training setup consisting of one golden document alongside four distractor documents.
Generalization to a variable number of test-time documents. We extended our research
to examine the impact of different quantities of test-time documents on the model’s per-
formance. Specifically, our experiments focused on assessing how models, trained with
varying numbers of distractor documents, respond to changes in the number of documents
presented at test time. The results, illustrated in Fig. 6, confirm that the inclusion of distrac-
tor documents during training indeed makes the model more resilient to fluctuations in the
number of documents encountered during testing. This ability to maintain consistent perfor-
mance despite variations in test-time document numbers further validates the robustness of
our approach, RAFT . This finding underscores the importance of a well-calibrated training
environment to prepare the model for a range of scenarios it may encounter in real-world.
6 Related Works
Retrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)
enhance LLMs by integrating a retrieval module that sources relevant information from
external knowledge bases, significantly improving performance across various NLP tasks,
including language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,
2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;
Wang et al., 2023) and open-domain question answering (Izacard et al., 2023; Lewis et al.,
2020). For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever,
treating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the
decoder-only architecture to include retrieved texts and conducts pre-training from scratch.
kNN-LM (Khandelwal et al., 2019) interpolates between the LM’s next token distribution
and distributions computed from retrieved tokens at inference. (Shi et al., 2023d; Ram
et al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or
fine-tuned retriever.
Memorization A key question around large neural language models is whether they truly
“understand” text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern
memorization (Carlini et al., 2019; Tänzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;
2022) develop methodologies to quantify the extent of memorization in neural models.
(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization
impacts the models’ generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)
demonstrated the ability of language models to memorize and regurgitate training data,
raising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).
Finetuning for RAG More recently, several papers have been exploring the idea of fine-
tuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu
9



Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 2):

Input
Retriever
Output
Documents
Input
Web Search
Documents
Black-box LLM
Query
Input
Documents
Query
Output
 Output
Reward
Input:
What profession does Nicholas Ray and 
Elia Kazan have in common?
Query: Nicholas Ray profession
Nicholas Ray American author and 
director, original name Raymond 
Nicholas Kienzle, born August 7, 
1911, Galesville, Wisconsin, U.S......
director
Rewriter
Retriever
Black-box LLM
Reader
Black-box LLM
Reader
(a) Retrieve-then-read    (b)Rewrite-retrieve-read                  (c) Trainable rewrite-retrieve-read    
Black-box LLM
Reader
Web Search
Retriever
Rewriter
Small PrLM
Example
Query: Elia Kazan profession
Elia Kazan was an American film and 
theatre director, producer, 
screenwriter and actor, described  ......
Correct (reader      )
Hit (retriever      )
✅
✅
Figure 1: Overview of our proposed pipeline. From left to right, we show (a) standard retrieve-then-read method,
(b) LLM as a query rewriter for our rewrite-retrieve-read pipeline, and (c) our pipeline with a trainable rewriter.
external knowledge. Komeili et al. (2022) use an
internet search for relevant information based on
the dialogue history to perform dialogue response
generation. SeeKeR (Shuster et al., 2022) use a
single Transformer to iteratively perform search
query generation, then knowledge extraction for
dialogue generation and sentence completion. For
large-scale models, web search still shows effec-
tive for knowledge augmentation (Lazaridou et al.,
2022), fact-checking (Menick et al., 2022), and
LLM agent enhancement (Yao et al., 2023).
2.2 Cooperation with Black-box LLMs
Large Language Models, such as ChatGPT
(Ouyang et al., 2022), Codex (Chen et al., 2021),
PaLM (Chowdhery et al., 2022), emerge impres-
sive natural language processing ability as well as
remarkable scalability. This leads to a tendency
to embrace LLMs on a wide range of NLP tasks.
However, LLMs are only accessible as a black box
in most cases, which is because (i) Some like Chat-
GPT are not open-source and kept private; (ii) The
large parameter scale requires computational re-
sources that are not always affordable to users. This
constraint means nothing is available except input
and output texts.
Existing studies have proved that LLMs’ abili-
ties can be better leveraged by carefully designed
interaction methods. GenRead (Yu et al., 2023)
prompts an LLM to generate context instead of
deploying a retriever, showing that LLMs can re-
trieve internal knowledge by prompting. ReAct
(Yao et al., 2023) and Self-Ask (Press et al., 2022)
combines the Chain-of-Thought (CoT) (Wei et al.,
2022; Wang et al., 2022) and inter-actions with web
APIs. Only relying on prompt construction, Re-
Act provides novel baselines for interactive tasks.
Demonstrate–Search–Predict (DSP) (Khattab et al.,
2022) defines a sophisticated pipeline between an
LLM and a retriever. Unlike ReAct, DSP integrates
prompts for demonstration bootstrap besides multi-
hop breakdown and retrieval.
Despite the promising performance in the zero or
few-shot setting, the behavior of LLMs sometimes
needs adjustments. A feasible approach is to ap-
pend trainable small models in front of or after the
LLM. The small models, as a part of the parameters
of the system, can be fine-tuned for optimization.
RePlug (Shi et al., 2023) is proposed to fine-tune a
dense retriever for the frozen LLM in the retrieve-
then-read pipeline. The retriever is trained under
the LLM’s supervision to retrieve documents that
are suitable for the LLM. With the same purpose,
Directional Stimulus Prompting (Li et al., 2023)
deploys a small model to provide the LLM with
stimulus (e.g., keywords for summarization, or di-
alogue actions for response generation), which is
updated according to the LLM reward.
Different from the inspiring work mentioned
above, our proposed pipeline contains a query
rewriting step in front of the retrieve-then-read
module. We further propose a trainable scheme
with a small rewriting model, which is a novel
enhancement for retrieval-augmented LLM by re-



Source: data\tc16_2312.10997v5\referenced_papers\[27]_2310.01352.pdf (Page 21):

Published as a conference paper at ICLR 2024
Table 12: Language model prompts and retriever query templates used for our evaluation datasets.
We did not perform retrieval for commonsense reasoning tasks evaluation.
Task LLM Prompt Template Query Template
Knowledge-Intensive Tasks
MMLU Background: {retrieved passage}\n\nQuestion:{question}\nA.
{choice}\nB.{choice}\nC.{choice}\nD.{choice}\nA:{answer}
{question}\nA.
{choice}\nB.
{choice}\nC.
{choice}\nD.{choice}
NQ, TQA, ELI5,
HoPo, zsRE
Background:{retrieved passage}\n\nQ:{question}\nA:{answer} { question}
AIDA Background: {retrieved passage}\n\n{context}\nOutput the
Wikipedia page title of the entity mentioned between [STARTENT]
and [ENDENT] in the given text\nA:{answer}
{context} tokens be-
tween [STARTENT] and
[ENDENT]
FEV Background: {retrieved passage }\n\nIs this statement true?
{statement} {answer}
{statement}
T-REx Background: {retrieved passage}\n\n{entity1} [SEP]{relation}
\nA:{answer}
{entity1}[SEP]{relation}
WoW Background: {retrieved passage}\n\nQ: {turn1}\nA: {turn2}\nQ:
{turn3}...\nA:{answer}
{turn1} {turn2} {turn3}...
Commonsense Reasoning Tasks
ARC-E, ARC-C Question:{question}\nAnswer:{answer}
BoolQ {context}\nQuestion:{question}\nAnswer:{answer}
HellaSwag {context} {ending}
OpenbookQA {question} {answer}
PIQA Question: {question}\nAnswer:{answer}
SIQA {context}Q:{question}A:{answer}
WinoGrande {prefix} {answer} {suffix}
Figure 2: RA-IT model performance (combined with D RAGON +) across sizes 7B, 13B and 65B on
our development tasks. 0-shot performance: dashed lines; 5-shot performance: solid lines.
measure one-hop fact look-up abilities (such as Zero-Shot RE and T-REx), retrieval augmentation
provides significant improvements across all model sizes and can bring the performance of smaller
models closer to that of their larger counterparts. For more complex tasks (such as HotpotQA and
WoW), the advantage of using a larger LLM remains prominent.
22



### Claim 34/179

#### Claim Text
The implementation of the query rewrite method in the Taobao, known as BEQUE [9] has notably enhanced recall effectiveness for long-tail queries, resulting in a rise in GMV .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[9]_2311.03758.pdf (Page 7):

WWW ’24 Companion, May 13–17, 2024, Singapore, Singapore. Peng, et al.
Table 5: Overall performance of BEQUE with multiple baselines. The best results are in bold, and the second-best results are
underlined. “Top Queries” are defined as queries with retrieval results containing more than 70% related products; “Torso
Queries” are defined as queries with retrieval results containing 10%-70% related products; “Tail Queries” are defined as queries
with retrieval results containing less than 10% related products. All the metrics are the larger the better.
Method Top Queries Torso Queries Tail Queries All Queries
rele incr hitrate rele incr hitrate rele incr hitrate rele incr hitrate
CLE-QR 73.4 90.0 13.16 24.7 36.4 15.77 10.0 17.2 12.36 69.6 90.0 12.95
BART 67.5 106.0 13.26 27.9 130.0 17.70 8.4 27.5 13.59 62.2 100.0 13.56
Q2D (ChatGPT) 71.7 47.3 12.96 35.3 66.7 16.86 12.8 15.9 17.21 66.7 45.5 14.73
Qwen (SFT) 67.1 117.4 14.18 25.5 56.4 17.49 7.2 34.9 14.91 61.4 109.6 14.58
RL (rele) 74.7 48.6 12.66 39.4 2.1 15.60 14.5 8.1 11.36 70.0 45.1 12.42
RL (incr) 46.9 76.1 13.33 17.7 134.1 18.00 4.4 60.9 15.20 42.5 75.9 14.22
RL (hitrate) 56.0 176.8 15.10 6.2 117.3 20.16 2.2 51.0 18.22 49.6 162.8 15.75
BEQUE (rele) 69.3 174.5 15.04 27.3 160.9 19.29 4.1 66.4 18.27 62.3 164.0 16.43
BEQUE (incr) 64.6 212.3 15.45 20.5 207.7 21.45 2.4 71.7 19.62 57.7 198.7 17.27
BEQUE (hitrate) 69.3 177.0 15.39 18.4 204.0 19.78 5.0 62.5 18.22 62.1 167.0 16.64
Table 6: Online A/B test of BEQUE on Mobile Taobao Search.
“all queries”: every query in the test bucket counts, regardless
of whether it has been rewritten or not. “covered queries”:
only rewritten queries count. “long-tail queries”: rewritten
and long-tail queries count. “few-recall queries”: rewritten
and few-recall queries count.
Online Traffic GMV #Trans UV
all queries +0.40% +0.34% +0.33%
covered queries +2.96% +1.36% +1.22%
long-tail queries +1.57% +2.52% +2.32%
“few-recall” queries +18.66% +5.90% +6.25%
4.5 Online Experiments
To assess the actual online performance of BEQUE, we deployed it
on Taobao search for a 14-day online test, during which we recorded
the three key metrics in the Taobao search scene: GMV, #Trans, and
UV. Table 6 reveals that BEQUE surpassed the previous-generation
rewriting model CLE-QR by 0.4%, 0.34%, and 0.33% in terms of GMV,
#Trans, and UV, respectively. This implies that BEQUE contributes
millions of GMV to Taobao search. It’s important to note that the
overall performance mentioned here refers to all queries in the
testing buckets. Since we inference offline, there are about 70% of
online queries that do not hit our rewriting table. Even in these
cases, our model still delivers remarkable enhancements. Addition-
ally, for the queries covered (rewritten) by BEQUE (approximately
27% of total PV), there were noteworthy increases of 2.96%, 1.36%,
and 1.22% in GMV, #Trans, and UV, respectively. These findings
indicate that BEQUE effectively rewrites queries and addresses po-
tential semantic gaps in the semantic matching process. Moreover,
BEQUE significantly improved online #Trans and UV for long-tail
queries and “few-recall” queries, although we disregarded the GMV
fluctuation for this subset due to its low proportion. This improve-
ment can be attributed to our specialized optimization for long-tail
queries. During the first-stage SFT of BEQUE, rejection sampling
and auxiliary task data enhanced the model’s performance in terms
of retrieval increment and relevance, and also deepened its under-
standing of long-tail queries. The alignment process in the second
and third stages effectively compelled the model to align with online
objectives of Taobao search.
5 CONCLUSION
In this paper, we introduce BEQUE, a framework specifically de-
signed for e-commerce query rewriting. The main objective of
BEQUE is to address the semantic gap that occurs during the se-
mantic matching process, particularly for long-tail queries. Initially,
we improve the quality of the rewriting dataset by employing re-
jection sampling and auxiliary task mixing. We then train a LLM
using this refined dataset, which enhances the model’s query un-
derstanding and enables effective rewriting of e-commerce queries.
Utilizing the well-trained LLM, we generate multiple candidate
rewrites for each sampled query. To establish a partial order among
these candidates, we create an offline feedback system based on
online Taobao search. This feedback system accurately evaluates
the retrieval quality of the candidate rewrites from various per-
spectives, such as relevance, increment, and hitrate. Finally, by
incorporating the partial order of rewriting retrieval quality, we
introduce PRO, which aligns the model’s objectives with those of
Taobao. This ensures that our approach generates rewriting results
that yield high-quality retrieval outcomes. Through multiple exper-
iments, we have demonstrated the effectiveness of our approach
in improving offline metrics. Additionally, online A/B experiments
have substantiated a significant increase in Taobao Search’s GMV,
#Trans, and UV, particularly for long-tail queries.
6 ACKNOWLEDGMENTS.
This work was supported in part by the grants from National Natu-
ral Science Foundation of China (No.62222213, U22B2059, 62072423),
and the USTC Research Funds of the Double First-Class Initiative
(No.YD2150002009).



Source: data\tc16_2312.10997v5\referenced_papers\[9]_2311.03758.pdf (Page 6):

Large Language Model based Long-tail Query Rewriting in Taobao Search WWW ’24 Companion, May 13–17, 2024, Singapore, Singapore.
Table 3: Comparison of Qwen with/without auxiliary tasks.
MI means multi-instruction
Method rele (%) incr (%) hitrate (%)
Qwen w/o MI 61.4 109.6 14.58
Qwen 62.6 133.2 15.63
Table 4: Comparison of BEQUE with different number of
contrast candidates.
Number rele (%) incr (%) hitrate (%)
2 53.3 215.6 17.66
3 56.6 205.4 17.36
4 57.7 198.7 17.27
5 58.8 190.3 17.21
4.4.3 Results of Different Contrast Number. In Table 4, we present
the impact of varying contrastive numbers of candidates on model
performance during the objective alignment stage. It can be ob-
served that, as the number of candidates increases, the relevance
shows a consistent improvement, while the increment and hitrate
decrease. This outcome can be attributed to our modified PRO,
where all candidates are treated as gold standard and SFT Loss is
calculated for each of them. Consequently, a larger candidate pool
implies an increase in the SFT loss weights and a decrease in the
partial order learning weights, improving relevance of generated
rewrites. Furthermore, there exists a trade-off between relevance
and increment, where an increase in one metric necessitates sac-
rificing the other, leading to a negative impact on the increment
metric. In addition, it is important to mention that hitrate can be
regarded as the increment with weak relevance constraint, which
explains its decrease in this context. Considering the balance of the
three metrics, we select the model with contrast number of 4 to be
best checkpoint for overall comparison.
4.4.4 Main Results. We compare BEQUE with multiple baselines,
including CLE-QR, query2doc (Q2D), BART, Qwen, and RL-based
LLM. CLE-QR [18] is the previous-generation query rewriter of
Taobao search that generates semantic representations and re-
trieve related rewrites for each query based on contrastive learning.
BART [17] is a powerful pre-trained generation model based on the
encoder-decoder structure. We fine-tune it with query pairs from
online logs to enhance its ability to rewrite e-commerce queries.
Qwen [32] is a large-scale language model based on the decoder-
only structure that contains 7 Billion parameters. Similarly, we
fine-tune it with query pairs from online logs to enhance its ability
to rewrite e-commerce queries. Furthermore, following the settings
of [1], we introduce an RL-based LLM and utilize relevance, incre-
ment, and hitrate as rewards to encourage the RL model to align
with the Taobao offline metrics, respectively. From analyzing the
data presented in Table 5, the following conclusions can be drawn:
Generative models outperform discriminative models when
rewriting “Torso” and “Tail” queries. For instance, when consid-
ering CLE-QR and BART, both models exhibit similar performance
on “Top Queries” across three metrics. However, BART significantly
outperforms CLE-QR in terms of hitrate and increment on “Torso
Queries” and “Tail Queries” while maintaining relevance. This dis-
crepancy arises because discriminative models like CLE-QR rely on
existing queries in the search system as rewrite candidates, which
are often biased towards top queries. As a result, torso and tail
queries, which lack semantically similar top rewrites, do not re-
ceive related search candidates from CLE-QR. In contrast, BART’s
rewriting process is not restricted by the semantic scope of online
queries, enabling it to generate rewrites that are not present in
the search system’s history. This allows BART to overcome the
limitations of discriminative models and optimize torso and tail
query rewriting problem.
LLMs exhibit superior long-tail semantic understanding
capabilities compared to small models. Qwen and BART serve
as examples, where Qwen, with its extensive parameter size, demon-
strates stronger semantic expansion than BART in terms of hitrate
and increment of “All Queries”. Analyzing individual query slices,
Qwen’s improvement in hitrate and increment primarily occurs
in the “Tail Queries”, further validating the suitability of LLMs for
long-tail query rewriting tasks.
Retrieval augmentation methods demonstrate limited se-
mantic expansion capabilities. Comparing Q2D (ChatGPT) and
BEQUE, Q2D (ChatGPT) maintains good retrieval relevance across
all query slices but lacks sufficient semantic expansion capabilities,
resulting in subpar increment and hitrate performance. Conversely,
our BEQUE, which is specifically optimized for semantic expansion
in rewriting, significantly enhances these two metrics.
The reinforcement learning (RL) may introduce bias and
impact the effectiveness of the rewriting LLMs. Examining RL
and BEQUE, RL process introduces a reward model to guide the
base model’s training. However, calculating the reward requires
offline search system simulation, and the reward model may not
accurately capture the search system’s features, leading to reduced
performance of RL models. In contrast, our BEQUE employs con-
trastive learning to explicitly learn the partial order of candidates,
circumventing potential bias caused by the reward model. Ulti-
mately, while minimizing the adverse impact on retrieval relevance,
BEQUE substantially improves the model’s increment and hitrate.
Different offline metrics work differently as rewards. For
instance, when the BEQUE framework prioritizes relevance as its
training objective, it demonstrates a more cautious approach to
bridging the semantic gap. The improvements in both increment
and hitrate tend to be challenging to achieve in this context. How-
ever, when the primary objective shifts to maximizing increment,
the model demonstrates a significant capacity to enhance both the
increment and hitrate of retrieval, effectively addressing the issue
of “few-recall”. In such cases, a marginal decrease in relevance be-
comes an acceptable trade-off. When hitrate becomes the target,
the model also can effectively enhance both the increment and
hitrate. Nevertheless, owing to the intricacies of the hitrate compu-
tation process, the model encounters difficulties in capturing the
partial order among candidates. Consequently, the model’s ability
to expand semantic is diminished in comparison to the BEQUE that
focuses on increment.



Source: data\tc16_2312.10997v5\referenced_papers\[9]_2311.03758.pdf (Page 0):

Large Language Model based Long-tail Query Rewriting in
Taobao Search
Wenjun Peng∗
pengwj@mail.ustc.edu.cn
University of Science and Technology
of China & State Key Laboratory of
Cognitive Intelligence
Hefei, Anhui, China
Guiyang Li
liguiyang.lgy@taobao.com
Taobao and Tmall Group
Hangzhou, Zhejiang, China
Yue Jiang
jy270069@alibaba-inc.com
Taobao and Tmall Group
Hangzhou, Zhejiang, China
Zilong Wang
huanshi.wzl@taobao.com
Taobao and Tmall Group
Hangzhou, Zhejiang, China
Dan Ou†
oudan.od@taobao.com
Taobao and Tmall Group
Hangzhou, Zhejiang, China
Xiaoyi Zeng
yuanhan@taobao.com
Taobao and Tmall Group
Hangzhou, Zhejiang, China
Derong Xu
derongxu@mail.ustc.edu.cn
University of Science and Technology
of China & State Key Laboratory of
Cognitive Intelligence
Hefei, Anhui, China
Tong Xu‡
tongxu@ustc.edu.cn
University of Science and Technology
of China & State Key Laboratory of
Cognitive Intelligence
Hefei, Anhui, China
Enhong Chen
cheneh@ustc.edu.cn
University of Science and Technology
of China & State Key Laboratory of
Cognitive Intelligence
Hefei, Anhui, China
ABSTRACT
In the realm of e-commerce search, the significance of semantic
matching cannot be overstated, as it directly impacts both user
experience and company revenue. Along this line, query rewrit-
ing, serving as an important technique to bridge the semantic gaps
inherent in the semantic matching process, has attached wide at-
tention from the industry and academia. However, existing query
rewriting methods often struggle to effectively optimize long-tail
queries and alleviate the phenomenon of “few-recall” caused by
semantic gap. In this paper, we present BEQUE, a comprehensive
framework that Bridges the sEmantic gap for long-tail QUEries. In
detail, BEQUE comprises three stages: multi-instruction supervised
fine tuning (SFT), offline feedback, and objective alignment. We
first construct a rewriting dataset based on rejection sampling and
auxiliary tasks mixing to fine-tune our large language model (LLM)
in a supervised fashion. Subsequently, with the well-trained LLM,
we employ beam search to generate multiple candidate rewrites,
and feed them into Taobao offline system to obtain the partial order.
Leveraging the partial order of rewrites, we introduce a contrastive
learning method to highlight the distinctions between rewrites, and
∗This work was done when the first author was an intern at Taobao Main Search.
†Corresponding author 1
‡Corresponding author 2
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
WWW ’24 Companion, May 13–17, 2024, Singapore, Singapore.
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0172-6/24/05. . . $15.00
https://doi.org/10.1145/3589335.3648298
align the model with the Taobao online objectives. Offline experi-
ments prove the effectiveness of our method in bridging semantic
gap. Online A/B tests reveal that our method can significantly boost
gross merchandise volume (GMV), number of transaction (#Trans)
and unique visitor (UV) for long-tail queries. BEQUE has been de-
ployed on Taobao, one of most popular online shopping platforms
in China, since October 2023.
CCS CONCEPTS
• Information systems →Query reformulation; • Computing
methodologies →Natural language processing .
KEYWORDS
Query reformulation; large language models; semantic matching
ACM Reference Format:
Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng,
Derong Xu, Tong Xu, and Enhong Chen. 2024. Large Language Model based
Long-tail Query Rewriting in Taobao Search. In Companion Proceedings of
the ACM Web Conference 2024 (WWW ’24 Companion), May 13–17, 2024,
Singapore, Singapore. ACM, New York, NY, USA, 9 pages. https://doi.org/10.
1145/3589335.3648298
1 INTRODUCTION
Past decades have witnessed the exceptionally rapid growth of
e-commerce platforms. Leading e-commerce companies, such as
Taobao, JD and Amazon, have amassed hundreds of millions of
users, generating billions of gross merchandise volume (GMV) an-
nually. To facilitate the quick retrieval of related products for these
users, a well-established search paradigm has been proposed, as
illustrated in Figure 1, Specifically, this paradigm involves several
steps, i.e., “semantic understanding - retrieval - rank” . Among them,
semantic understanding serves as the foundation of entire system,
arXiv:2311.03758v3  [cs.IR]  4 Mar 2024



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 20):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:21
Table 6. The experimental results for evaluating different embedding models in our benchmark.
task name embedding name bleu rouge-L bertScore RAGQuestEval length
precision recall
text continuation
m3e-base 3.59 17 .55 83 .76 27 .30 23 .73 350.0
bge-base 3.66 17 .78 83 .99 26 .96 24.68 367.6
stella-base 3.73 17 .67 84.05 28.78 24.65 366.6
gte-base 3.76 17.80 84.03 27 .35 24 .18 362.1
summarization
m3e-base 22.91 33 .23 88 .31 68 .58 46 .02 210.5
bge-base 23.69 33.53 88.49 68 .06 46 .18 205.9
stella-base 23.50 33.50 88 .58 68.22 46.56 205.5
gte-base 22.87 33 .46 88.58 68.10 47.13 211.1
question answering
m3e-base 38.81 56 .49 83 .41 50 .18 69 .72 75.2
1-document
bge-base 39.76 57.24 83 .81 52 .67 70 .82 73.3
stella-base 39.58 57.28 83.91 53.13 71.74 73.9
gte-base 39.58 57 .19 83 .90 52 .39 71.97 76.5
question answering
m3e-base 22.32 36 .81 86 .91 42 .97 55 .67 148.4
2-document
bge-base 22.75 37 .25 87 .16 42 .93 56 .73 149.8
stella-base 23.39 37.75 87.37 44.83 58.00 149.5
gte-base 23.20 37 .59 87.48 43.99 57 .58 151.5
question answering
m3e-base 20.72 34 .78 87 .43 39 .57 50 .88 154.3
3-document
bge-base 21.05 35 .04 87 .81 40 .32 51.37 156.6
stella-base 21.26 35.27 87.81 41.4 1 50 .42 154.4
gte-base 21.15 35.59 87.86 40 .18 51 .11 157.2
hallucination
m3e-base 32.83 53.27 80.78 65.87 81.69 64.5
modification
bge-base 32.35 53 .04 80 .49 65 .07 80 .85 64.8
stella-base 32.34 52 .96 80 .59 65 .74 81 .50 65.2
gte-base 31.69 52 .46 80 .40 65 .35 80 .69 64.5
which is a creative task, BM25 can retrieve content that is highly relevant to the user’s intention,
but may overlook some details.
Open-Domain Multi-Document Summarization: On the overall semantic similarity metric,
the performance of the dense retriever is roughly equivalent to that of BM25. On the QuestEval
metric, BM25 surpasses dense retriever in terms of key information precision, but slightly trails
behind in key information recall. If the retrieved content contains a lot of irrelevant information,
the model-generated summary may have errors or redundancies. BM25 retrieved content usually
matches the user’s intention better, but sometimes may miss some important information. Therefore,
BM25 is weaker than dense retriever in key information recall, but excels in key information
accuracy. Besides, hybrid retrieval algorithms presumably combine the advantages of both, and the
RAG system generates content with suitable precision and recall.
Question Answering: In question answering, we find that dense retriever has a more obvious
advantage over BM25, when dealing with reasoning questions that require synthesizing multiple
documents. In question-answering tasks that require considering three documents, Dense retriever
not only surpasses BM25 in all the overall semantic similarity metrics, but also achieves a significant
improvement in key information precision and recall. This indicates that question-answering
retrieval is more difficult than text continuation and other tasks, especially reasoning question-
answering, which requires a higher level of semantic understanding, and simple keyword retrieval
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[9]_2311.03758.pdf (Page 1):

WWW ’24 Companion, May 13–17, 2024, Singapore, Singapore. Peng, et al.
“DIY blind box”
“Self-building blind box”
Query  Rewriting
Semantic UnderstandingRetrieval (Multi-path)
Item-based CF
Embedding-based  retrieval
PrerankRankRerankDisplay
Ranking System
ThousandsThousandsDozens
Merge
Inverted Index based Exact Matching
Figure 1: Framework of Taobao search engine.
ensuring accurate matching of user intent. However, due to the
variations for how users express their preferences for products,
semantic gaps often exist between their queries and the product
keywords, even worse with long-tail queries where retrieval sys-
tem may fail to provide any relevant products. For instance, a user
with personal expression habits may input a long-tail query like
"self-building blind box" , which will lead to more retrieval results if
with its synonymous query like "DIY blind box" . Unfortunately, tra-
ditional term-matching solutions like inverted index could probably
fail to match the commonly-used “DIY” with non-customary term
“self-building”, which limit the retrieval results and significantly
impair the user experience. Therefore, it is urgently required to
solve the semantic gap challenge for long-tail queries, and address
the problem of “few-recall” in e-commerce platform.
Traditionally, prior arts [13, 19, 34, 44] mainly focus on the “em-
bedding based retrieval” paradigm, which initially map the queries
and products into a common semantic space, and then support the
Top-𝐾 retrieval with approximate nearest neighbor (ANN) meth-
ods. However, the retrieval outcomes might be difficult to interpret,
which severely limit the performance. To enhance the controlla-
bility of retrieval outcomes, some efforts have been made on the
“query rewriting & exact match” paradigm. On the one hand, the
discriminative methods [18, 46] attempt to “rewrite” queries via
finding similar terms from a query reformulation set, and then
utilize them to search for relevant products using sparse retrieval.
Although these approaches could effectively expand the semantic
of hot queries, long-tail queries may not be adequately optimized,
thus no related rewrite can be generated. On the other hand, the
generative methods [26, 38] involve supervised training on <query,
rewrite> pair data to empower the model with rewriting capabilities,
and the alignment process [1, 23] is further incorporated to enhance
the metric preference. Although these methods partially address
the semantic gap problem, they typically rely on small generative
models with limited comprehension of long-tail queries, which
significantly constrained the rewriting capability. Recently, with
the development of LLM techniques, some efforts [ 2, 15, 36, 37]
solely utilize LLMs as retrieval data augmentation generators with-
out additional training to expand query semantics. However, these
methods, even with carefully curated prompts, may still constrain
the ability to specialize for query rewriting task, leading to the poor
alignment with objectives of e-commerce search.
To effectively bridge the semantic gap for long-tail queries, and
solve the above challenges via producing controlled and aligned
outcomes with integrating the knowledge of LLMs, we propose
BEQUE, a novel framework that involves three stages of fine-tuning
LLMs, namely multi-instruction supervised fine-tuning (SFT), offline
system feedback , and objective alignment . Specifically, in the first
stage, we utilize the rejection sampling to collect <query, rewrite>
pairs with desired quality distribution, and then combine these
pairs with data from quality classification, product title prediction
and chain of thought (CoT) tasks to construct the multi-instruction
rewriting dataset for fine-tuning our LLM. Next, with the well-
trained LLM, we employ beam search to generate multiple candidate
rewrites for each query. These candidate rewrites are fed into the
Taobao offline system to retrieve a collection of related products. We
calculate the quality score of retrieved products for the rewrites and
use them as rewards for candidates ranking. To calibrate generation
probability of the candidate rewrites, we introduce a Bradley-Terry
based contrastive learning method that considers the partial order
among the these rewrites. Ultimately, the model training objective
is aligned with the online goal of the Taobao search, ensuring that
the generated rewrites yield the desired search results.
The main contributions of this work are listed as follows:
•We have analyzed long-tail queries in e-commerce search
and identified the semantic gap problem associated with
such queries. To the best of our knowledge, we are the first
to fine-tune LLMs for industrial query rewriting task.
•We propose a three-stage fine-tuned framework called BEQUE
to address the issue of semantic gap in long-tail queries. This
framework is designed to generate rewrites that align with
the objectives of Taobao search.
•The effectiveness of our model is demonstrated through
both offline and online experiments, showcasing its ability
to significantly improve e-commerce revenue.
2 RELATED WORKS
2.1 Query Rewriting
Query rewriting, also known as query expansion or query reformu-
lation, plays a pivotal role in e-commerce search technology and
has a profound impact on the user’s shopping experience and the
revenue of e-commerce platforms. This technique can be broadly
categorized into discriminative and generative methods.
Discriminative methods treat query rewriting as a retrieval
process that expand the semantics of the original query by selecting
appropriate terms from the candidate set. For example, pseudo-
relevance [6, 31, 40] selects the top k documents from the initial
retrieval as semantic extensions. These approaches, however, of-
ten pose challenges in effectively controlling the semantic scope
and ensuring retrieval relevance. To address these challenges, one
potential solution is to utilize a well-built thesaurus [ 5, 22] as a
candidate rewrite set. However, it is important to note that the
effectiveness of these methods highly depends on the quality of
the thesaurus. Inadequate quality may result in query semantic
drift, where the intended meaning of the query is compromised.
Furthermore, alternative approaches [3, 8, 18, 21] involve generat-
ing candidate rewrites based on search logs, incorporating similar
terms from users’ search history as extensions. Unfortunately, due
to the Matthew effect, search logs naturally exhibit a bias towards
popular queries, resulting in that the training data collected through
this approach may not sufficiently meet the optimization needs for
less frequently searched long-tail queries.



### Claim 35/179

#### Claim Text
HyDE [11] construct hypothetical documents (assumed answers to the original query).

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[11]_2212.10496.pdf (Page 2):

instruction following generative LLM, as described
above.
Zero-Shot Dense Retrieval The tasks of zero-
shot (dense) retrieval are arguably empirically de-
ﬁned by Thakur et al. (2021) for the neural re-
trieval community. Their BEIR benchmark con-
sists of diverse retrieval tasks. The paper and
many follow-up research generally consider the
Transfer Learning setup where the dense re-
triever is ﬁrst learned using a diverse and richly
supervised corpus and query collection, namely
MS-MARCO (Thakur et al., 2021; Wang et al.,
2022; Yu et al., 2022).
However, as stated by Izacard et al. (2021), such
a large collection can rarely be assumed. In this
paper, therefore, we study the problem of building
effective dense retrieval systems without relevance
labels. Similar to Izacard et al. (2021), we also
do not assume access to the test time corpora for
training. This is a more realistic setup and prevents
over-engineering on the test corpora.
By the deﬁnition in Sachan et al. (2022), our
setup can be roughly considered as “unsuper-
vised”. Strictly, as with Sachan et al. (2022), the
only supervision resides in the LLM, in the pro-
cessing of learning to follow instructions.
Generative Retrieval Generative search is a new
class of retrieval methods that use neural generative
models as search indices (Metzler et al., 2021; Tay
et al., 2022; Bevilacqua et al., 2022; Lee et al.,
2022). These models use (constrained) decoding
to generate document identiﬁers, such as id and
sub-string, which map directly to real documents.
They have to go through special training procedures
over relevance data; effective search may also need
to use novel forms of search indices (Bevilacqua
et al., 2022; Lee et al., 2022). In comparison, our
method uses the standard MIPS index and requires
no training or training data. Our generative model
produces an intermediate hypothetical document
to be fed into a dense encoder, instead of a real
document.
3 Methodology
In this section, we ﬁrst formally deﬁne the prob-
lem of (zero-shot) dense retrieval. Then we will
introduce how HyDE is designed to solve it.
3.1 Preliminaries
Dense retrieval models similarity between query
and document with inner product similarity. Given
a query q and document d, it uses two encoder
function encq and encd to map them into d dimen-
sion vectors vq, vd, whose inner product is used
as similarity measurement.
sim(q, d) =⟨encq(q), encd(d)⟩ = ⟨vq, vd⟩ (1)
For zero-shot retrieval, we consider L query sets
Q1, Q2, ..., QL and their corresponding search cor-
pus, document sets D1, D2, ..., DL. Denote the
j-th query from i-th set query set Qi as qij. We
need to fully deﬁne mapping functions encq and
encd without access to any query set Qi, document
set Di, or any relevance judgment rij.
The difﬁculty of zero-shot dense retrieval lies
precisely in Equation 1: it requires learning of two
embedding functions (for query and document re-
spectively) into the same embedding space where
inner product captures relevance. Without rele-
vance judgments/scores to ﬁt, learning becomes
intractable.
3.2 HyDE
HyDE circumvents the aforementioned learning
problem by performing search in document-
only embedding space that captures document-
document similarity. This can be easily learned
using unsupervised contrastive learning (Izacard
et al., 2021; Gao et al., 2021; Gao and Callan,
2022). We set document encoder encd directly as a
contrastive encoder enccon.
f = encd = enccon (2)
This function is also denoted as f for simplic-
ity. This unsupervised contrastive encoder will
be shared by all incoming document corpus.
vd = f(d) ∀d ∈ D1 ∪ D2 ∪ ... ∪ DL (3)
To build the query vector, we consider in addition
an instruction following LM, InstructLM. It takes a
query q and a textual instruction INST and follows
them to perform the task speciﬁed by INST . For
simplicity, denote,
g(q, INST ) =InstructLM(q, INST ) (4)
Now we can use g to map queries to "hypotheti-
cal" documents by sampling from g, setting INST



Source: data\tc16_2312.10997v5\referenced_papers\[11]_2212.10496.pdf (Page 1):

HyDE 
GPT
Contriever
how long does it take to remove
wisdom tooth It usually takes between 30
minutes and two hours to
remove a wisdom tooth...
How wisdom teeth are removed... 
Some ... a few minutes, whereas
others can take 20 minutes or
longer....
How has the COVID-19 pandemic impacted
mental health?
...depression and anxiety had
increased by 20% since the
start of the pandemic...
... two studies investigating
COVID-19 patients ... significantly
higher level of depressive ...
write a passage to answer the question
write a scientific paper passage to answer
the question
인간은  언제  불을  사용했는가 ?
write a passage in Korean to answer the
question in detail
인간이  불을  사용한  기록은  약 
800 만년  전부터  나타난다 ... ... 불을  처음  사용한  시기는  호모 
에렉투스가  살았던  142 만  년  전으 
로  거슬러간다 ...
instruction query generated document real document
Figure 1: An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries
without changing the underlying GPT-3 and Contriever/mContriever models.
to human intent to follow instructions.
With these ingredients, we propose to
pivot through Hypothetical Document
Embeddings ( HyDE), and decompose dense
retrieval into two tasks, a generative task per-
formed by an instruction-following language
model and a document-document similarity task
performed by a contrastive encoder (Figure 1).
First, we feed the query to the generative model
and instruct it to "write a document that answers
the question", i.e. a hypothetical document.
We expect the generative process to capture
"relevance" by giving an example; the generated
document is not real, can contain factual errors but
is like a relevant document. In the second step,
we use an unsupervised contrastive encoder to
encode this document into an embedding vector.
Here, we expect the encoder’s dense bottleneck
to serve a lossy compressor, where the extra
(hallucinated) details are ﬁltered out from the
embedding. We use this vector to search against
the corpus embeddings. The most similar real
documents are retrieved and returned. The retrieval
leverages document-document similarity encoded
in the inner-product during contrastive training.
Note that, interestingly, with HyDE factorization,
the query-document similarity score is no longer
explicitly modeled nor computed. Instead, the
retrieval task is cast into two NLU and NLG tasks.
HyDEappears unsupervised. No model is trained
in HyDE: both the generative model and the con-
trastive encoder remain intact. Supervision signals
were only involved in instruction learning of our
backbone LLM.
In our experiments, we showHyDEusing Instruct-
GPT (Ouyang et al., 2022) and Contriever (Izacard
et al., 2021) as backbone models signiﬁcantly out-
performs the previous state-of-the-art Contriever-
only zero-shot no-relevance system on 11 queries
sets, covering tasks like Web Search, Question
Answering, Fact Veriﬁcation and languages like
Swahili, Korean, Japanese.
2 Related Works
Dense Retrieval (Lee et al., 2019; Karpukhin
et al., 2020) has been extensively studied after the
emergence of pre-trained Transformer language
models (Devlin et al., 2019). Researchers stud-
ied the metric learning problems, such as training
loss (Karpukhin et al., 2020) and negative sam-
pling (Xiong et al., 2021; Qu et al., 2021), and also
introduced distillation (Qu et al., 2021; Lin et al.,
2021b; Hofstätter et al., 2021). Later works studied
the second stage pre-training of language model
speciﬁcally for retrieval (Izacard et al., 2021; Gao
and Callan, 2021; Lu et al., 2021; Gao and Callan,
2022; Liu and Shao, 2022).
The popularity of dense retrieval can be partially
attributed to the rich and successful research in very
efﬁcient minimum inner product search (MIPS) at
very large (billion) scales (Johnson et al., 2017).
Instructions-Following Language Models
Soon after the emergence of LLMs, several groups
of researchers discover that LLMs trained on data
consisting of instructions and their execution can
zero-shot generalize to perform new tasks with new
instructions (Ouyang et al., 2022; Sanh et al., 2022;
Min et al., 2022; Wei et al., 2022). This can be
done by standard supervised sequence-to-sequence
learning or more effectively with reinforcement
learning (Ouyang et al., 2022).
Concurrent to us, Asai et al. (2022) studied
“Task-aware Retrieval with Instructions”. They
ﬁne-tuned dense encoders that can also encode
task-speciﬁc instruction prepended to query. In
comparison, we use an unsupervised encoder and
handle different tasks and their instruction with an



Source: data\tc16_2312.10997v5\referenced_papers\[11]_2212.10496.pdf (Page 3):

to be “write a paragraph that answers the
question”. The generated document is not real,
can and is likely to be ungrounded factually (Brown
et al., 2020; Thoppilan et al., 2022). We only re-
quire it to capture relevance pattern. This is done
by generating documents, i.e. providing exam-
ples. Critically, here we ofﬂoad relevance mod-
eling from representation learning model to an
NLG model that generalizes signiﬁcantly more eas-
ily, naturally, and effectively (Brown et al., 2020;
Ouyang et al., 2022). Generating examples also
replaces explicit modeling of relevance scores.
We can now encode the generated document using
the document encoder f. Write,
E[vqij ] =E[f(g(qij, INST i))] (5)
Formally, g deﬁnes a probability distribution based
on the chain rule. In this paper, we simply consider
the expectation value, assuming the distribution of
vqij is uni-modal, i.e. the query is not ambiguous.
The study of ambiguous queries and diversity is
left to future work. We estimate Equation 5 by
sampling N documents from g, [ ˆd1, ˆd2, ...,ˆdN ].
ˆvqij = 1
N
∑
ˆdk∼g(qij ,INST i)
f(dk) (6)
= 1
N
N∑
k=1
f( ˆdk) (7)
We also consider the query as a possible hypothesis,
ˆvqij = 1
N + 1[
N∑
k=1
f( ˆdk) +f(qij)] (8)
Inner product is computed between ˆvqij and the
set of all document vectors {f(d)|d ∈ Di}. The
most similar documents are retrieved. Here the
encoder function f serves as a lossy compressor
that outputs dense vectors, where the extra details
are ﬁltered and left out from the vector. It further
grounds the hypothetical vector to the actual corpus
and the real documents. The full HyDE system is
illustrated in Figure 1.
4 Experiments
4.1 Setup
Implementation We implement HyDE using
InstructGPT, a GPT-3 model from the instruct
series (text-davinci-003; Ouyang et al. (2022))
and Contrievermodels (Izacard et al., 2021). We
sample from InstructGPT using the OpenAI play-
ground default temperature of 0.7 for open-ended
generations. We use the English-only Contriever
model for English retrieval tasks and multilingual
mContrieverfor non-English tasks. We conducted
retrieval experiments with the Pyserini toolkit (Lin
et al., 2021a).
Datasets We consider web search query sets
TREC DL19 (Craswell et al., 2020a) and
DL20 (Craswell et al., 2020b); they are based on
the MS-MARCO dataset (Bajaj et al., 2016). We
also use a diverse collection of 6 low-resource
datasets from the BEIR dataset (Thakur et al.,
2021). For non-English retrieval, we consider
Swahili, Korean, Japanese, and Bengali from the
Mr.Tydi dataset (Zhang et al., 2021).
We use different instructions for each dataset.
They share a similar structure but have different
quantiﬁers to control the exact form of the gener-
ated hypothetical documents. These instructions
can be found in subsection A.1.
Compared Systems Contriever models,
Contrieverand mContriever, serve as our major
baseline. They are trained using unsupervised
contrastive learning. HyDE retrievers share the
exact same embedding spaces with them. The
only difference is how the query vector is built.
These comparisons allow us to easily examine
the effect of HyDE. The classical heuristic-based
lexical retriever BM25 is also included.
Several systems that involve ﬁne-tuning on mas-
sive relevance data are also included as refer-
ences. We consider models ﬁne-tuned on MS-
MARCO and transferred, DPR and ANCE, from
the BEIR paper. For multilingual, we include
the mDPR model from Mr.Tydi paper and MS-
MARCO ﬁne-tuned mBERT and XLM-R from
the Contriever paper. We also include the state-of-
the-art transfer learning models: Contrieverand
mContrieverﬁne-tuned on MS-MARCO, denoted
ContrieverFT and mContrieverFT. These mod-
els have run through the state-of-the-art retrieval
model training pipeline that involves second-stage
retrieval-speciﬁc pre-training (Lee et al., 2019) and
a few rounds of ﬁne-tuning (Qu et al., 2021); they
should be considered empirical upper bounds.
4.2 Web Search
In Table 1, we show retrieval results on TREC
DL19 and TREC DL20. We see HyDEbring sizable
improvements to Contriever across the board for



Source: data\tc16_2312.10997v5\referenced_papers\[81]_2312.15883.pdf (Page 9):

Conference’17, July 2017, Washington, DC, USA Jiang, Zhang and Xu et al.
Figure 4: Case study. We demonstrate the User Query Q, Hypothesis Output HO, Retrieved Reasoning Chains RCand Pruned
Reasoning chains RCprune of HyKGE using GPT-3.5 Turbo to verify the interpretability and effectiveness of HyKGE. Red shed
signifies that the knowledge or answer is derived from evidence in Q, blue shed indicates that the evidence originates from
HO, and green shed represents a corrected answer with the help of KG, despite initially being false in HO.
### User Query Q:
I feel stomach reflux after eating. What medicine should I take?
### Hypothesis Output HO:
Stomach reflux can be a symptom caused by gastroesophageal reflux . 1. Do you have symptoms related to indigestion , excess stomach acid , gastric ulcers ,
bile reflux , or stomach pain ? ... When experiencing mild gastroesophageal reflux, the following treatment plans can be considered:- Avoid overeating - Avoid spicy,
stimulating foods, greasy foods, and alcohol - Maintain a light diet, eat more vegetables. Under appropriate evidence, consider taking H2 receptor antagonists or
proton pump inhibitors . It is not recommended to take calcium carbonate or aluminum hydroxide medications, as calcium carbonate can lead to kidney
stones, and the aluminum element in aluminum hydroxide is toxic and may cause allergic reactions or discomfort.
### Retrieved Reasoning Chains RC:
Kidney stones →Laboratory tests →Serum calcium ←Laboratory tests ←Gastric ulcer. Kidney stones →Clinical symptoms and signs →Fever ←Complications ←
Esophageal cancer. Excess gastric acid →Affected area →Abdomen ←Affected area ←Stomach pain. Niflumic acid →Adverse reaction →Discomfort ←
Adverse reaction ←Calcium carbonate. Gastrointestinal bleeding →Related diseases →Gastric ulcer. Gastric ulcer →Treatment →Aluminum hydroxide.
Calcium carbonate →Indication →Excess gastric acid. Aluminum hydroxide →Indication →Gastric reflux.
Niflumic acid →Third level classification →Others ←Affected area ←Postprandial food reflux.
Duodenogastric reflux and bile reflux gastritis →Treatment →Calcium carbonate. More else ... [125 reasoning chains in total.]
### Pruned Reasoning chains RCprune:
Gastric ulcer →Treatment →Aluminum hydroxide. Calcium carbonate →Indication →Excess gastric acid.
Aluminum hydroxide →Indication →Gastric reflux. Niflumic acid →Third level classification →Others ←Affected area ←Postprandial food reflux.
Duodenogastric reflux and bile reflux gastritis →Treatment →Calcium carbonate. Stomach reflux →Related disease →Excess gastric acid.
Caved-S →Indication →Excess gastric acid. Weile tablets →Indication →Excess gastric acid.
### Answer:
Stomach reflux can be a symptom caused by gastroesophageal reflux disease (GERD) , and this condition could potentially lead to reflux esophagitis. Consider the
following medical recommendations: Acid-suppressing medications (Proton Pump Inhibitors) : You may consider using omeprazole or esomeprazole to effectively
reduce gastric acid secretion, aiding in relieving symptoms of gastric reflux and mitigating the development of reflux esophagitis. Alternatively, you can use
acid-neutralizing medications (antacids) such as magnesium aluminum carbonate (Rennie), calcium carbonate, and aluminum hydroxide. Another option is the use
of H2 receptor antagonists such as ranitidine or famotidine. Additionally, medications like Caved-S or Weile tablets are also suitable for mild to moderate
symptoms of gastric reflux. Gastric protective medications: For short-term relief of stomach pain and discomfort,
you may also consider using gastric mucosal protectants such as sucralfate. It helps form a protective mucous layer, reducing irritation to the stomach mucosa. Diet
and lifestyle adjustments: First, ... it is advisable to consult a healthcare professional for evaluation and diagnosis as soon as possible.
A case study of HyKGE pipeline
1 2 3 4 5
k
0
10
20
30
40
50
60EM Score (%)
Base(MMCU)
Base(CMB)
w/ HO (CMB)
w/o HO (CMB)
w/ HO (MMCU)
w/o HO (MMCU)
101
102
103
104
105
106
107
Retrieval Chains Count
5 7 10 15 30 50
topK
25
30
35
40
45
50EM Score (%)
Base(MMCU)
Base(CMB)
w/ Fragment (CMB)
w/o Fragment (CMB)
w/ Fragment (MMCU)
w/o Fragment (MMCU)
Figure 5: (Left.) Hyper-parameter study with the KG hop 𝑘on
MMCU-Medical and CMB-Exam with GPT 3.5 turbo, from 1
to 5. (Right.) Hyper-parameter study with the reranker 𝑡𝑜𝑝𝐾
on MMCU-Medical and CMB-Exam with GPT 3.5 turbo, from
5 to 50.
Figure 5 (Right.) depicts EM with different reranking thresholds.
Similar to Figure 5 (Left.), as 𝑡𝑜𝑝𝐾 increases, the trends demon-
strate that overwhelming reasoning chains will hamper LLMs’ abil-
ity for comprehension. Meanwhile, it is obvious that HyKGE w/o
Fragment always underperforms on EM as analyzed in Section 5.3.
5.6 Case Study (RQ2 and RQ3)
This case study presents a representative sample that illustrates the
effectiveness of our HyKGE model using GPT-3.5 Turbo as shown
in Table 4. The color coding within the table is key to understand-
ing the source and validity of the information and we have these
observations: i) Compared to a brief user query, semantic spaces of
HOare more abundant and have a clear direction for answering,
helping us better understand user intention and extract more ef-
fective entity information. Ultimately, HyKGE extracted 23 entities
from HOcompared to only 1 from Q. ii) Comparing the RCwith



Source: data\tc16_2312.10997v5\referenced_papers\[81]_2312.15883.pdf (Page 4):

HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable Medical LLMs Responses Conference’17, July 2017, Washington, DC, USA
Figure 3: The prompt formats of (Up.) Hypothesis Output Module and (Down.) LLM Reader.
### Task Description:
You are a medical expert. Please write a passage to answer [User Query] while adhering to [Answer Requirements].
### Answer Requirements:
1) Please take time to think slowly, understand step by step, and answer questions. Do not skip key steps.
2) Fully analyze the problem through thinking and exploratory analysis.
### {{ User Query }}
The Prompt Format of Hypothesis Output Module (PHO)
### Task Description:
You are a medical expert. Based on relevant medical [Background Knowledge] and your medical knowledge, provide professional medical advice for [User Query]
while adhering to [Answer Requirements].
### Answer Requirements:
1) Take time to think slowly, understand step by step, and answer questions.
2) Clearly state key information in the answer and provide direct and specific answers to user questions.
### {{ Background Knowledge }}
The retrieved knowledge chains are: Kidney stones →Laboratory tests →Serum calcium ←Laboratory tests ←Gastric ulcer... (example)
### {{ User Query }}
The Prompt Format of LLM Reader (PReader)
is built upon the W2NER model [ 40], the state-of-the-art word-
word NER model that effectively addresses three primary types
of NER situations (flat, overlapped, discontinuous). This medical
NER model can wonderfully extract medical entities from complex
medical contexts:
U= [𝑢1,··· ,𝑢|U|]= NER(Q⊕HO) , (2)
where ⊕is the concatenation function and 𝑢𝑖 represents the corre-
sponding extracted entity.
4.2 Knowledge Graph Retrieval Module
4.2.1 Embedding Alignment. Subsequently, we link the potential
entity to KGusing dense retrieval methods. This process involves
employing an encoding model, denoted as enc(·), to encode the po-
tential entity 𝑢𝑖 and entities Ewithin KG. To be specific, we utilize
the GTE embedding model [45] "gte_sentence-embedding"4, which
is currently the top-performing model for text vector embedding
in the retrieval field. GTE Encoder follows a two-stage training
process: initially using a large-scale dataset with weak supervision
from text pairs, followed by fine-tuning with high-quality manually
labeled data using Contrastive Learning [38, 41].
Then, the inner product similarity between the embeddings of
𝑢𝑖 and Eis then computed. The entity with the highest similarity,
surpassing a predefined threshold 𝛿 ∈[0,1], is considered a match.
This linkage process can be formulated as follows:
sim(𝑢𝑖,𝑒𝑗)=



### Claim 36/179

#### Claim Text
Using the Step-back Prompting method [10], the original query is abstracted to generate a high-level concept question (step-back question).

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 15):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
MMLU Physics/Chemistry Final Answer Prompt
You are an expert at Physics/Chemistry. You are given a
Physics/Chemistry problem and a set of principles involved in
solving the problem. Solve the problem step by step by following the
principles. Here are a few examples:
Question: <Question Example1>
Principles: <Principles Example1>
Answer: <Answer Example1>
...
Question: <Question Example5>
Principles: <Principles Example5>
Answer: <Answer Example5>
Question: <Question>
Principles: <Principles>
Answer:
Table 8: Prompt of querying the model for final answer with first principles behind the question in
MMLU high-school Physics and Chemistry.
After extracting the first principles of solving a particular question, we formulate the prompt in
Table 8 to query the model for the final answer.
Tables 9-10 show one demonstration exemplar of Question-Principles-Answer triplets for MMLU
high-school Physics and Chemistry, respectively. For GSM8K, given the simplicity of the principles,
we directly combine the principles and the solution in the demonstration exemplar. Table 11 shows
the exemplar we used in the paper.
D.2 K NOWLEDGE QA
We use the following prompting in Table 12 to demonstrate to the LLM on asking a step-back question
for TimeQA and SituatedQA including up to 5 exemplar demonstrations of pairs of Original Question
and Step-back Question.
Table 13 shows 5 exemplars from the Train split of TimeQA and SituatedQA as demonstrations of
asking step-back questions.
The step-back question is extracted from the model output using the prompt. Using the step-back
question, we do retrieval augmentation. Using both the retrieval augmentations from the original
question and the step-back question, we formulate the final prompt to query the model for the final
answer, as shown in Table 14.
D.3 M ULTI -HOP REASONING
For Multi-Hop Reasoning, we use the same prompting template as in Knowledge QA to ask the
step-back question, and query for the final answer given the retrieval augmentations. Table 15 shows
5 demonstration exemplars for asking step-back questions from the Train split of MuSiQue and
StrategyQA.
D.4 B ASELINE PROMPTS
For standard zero-shot and few-shot prompting of the baseline model, we formulate the prompt using
the template in Table 16 with up to 1 exemplars.
D.5 C HAIN OF THOUGHT (COT)
For zero-shot CoT prompting, we simply append Let’s think step by step. to the question to query the
model.
16



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 18):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Knowledge QA Step-Back Prompt
You are an expert at world knowledge. Your task is to step back and
paraphrase a question to a more generic step-back question, which is
easier to answer. Here are a few examples:
Original Question: <Original Question Example1>
Stepback Question: <Stepback Question Example1>
...
Original Question: <Original Question Example5>
Stepback Question: <Stepback Question Example5>
Original Question: <Original Question>
Stepback Question:
Table 12: Prompt of asking step-back question in Knowledge QA tasks.
dataset Original Question Step-back Question
TimeQA Which position did Knox Cunningham
hold from May 1955 to Apr 1956?
Which positions have Knox Cunning-
ham held in his career?
TimeQA Who was the spouse of Anna Karina
from 1968 to 1974?
Who were the spouses of Anna Karina?
TimeQA Which team did Thierry Audel play for
from 2007 to 2008?
Which teams did Thierry Audel play for
in his career?
TimeQA What was the operator of GCR Class
11E from 1913 to Dec 1922?
What were the operators of GCR Class
11E in history?
TimeQA Which country did Sokolovsko belong
to from 1392 to 1525?
Which countries did Sokolovsko belong
to in history?
SituatedQA when was the last time a team from
canada won the stanley cup as of 2002
which years did a team from canada
won the stanley cup as of 2002
SituatedQA when did england last get to the semi
final in a world cup as of 2019
which years did england get to the semi
final in a world cup as of 2019?
SituatedQA what is the biggest hotel in las vegas nv
as of November 28, 1993
what is the size of the hotels in las vegas
nv as of November 28, 1993
SituatedQA who has scored most runs in t20
matches as of 2017
What are the runs of players in t20
matches as of 2017
SituatedQA who is the highest paid player in the nba
this season as of 2017
what is the salary of the high paid play-
ers in the nba this season as of 2017
Table 13: Few-shot demonstration exemplars for asking step-back questions in TimeQA and Situat-
edQA.
19



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 20):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Baseline few-shot Prompt
You are an expert of world knowledge and physics. Your task is to solve
the following question. Here are a few examples:
Question: <Question Example>
Answer: <Answer Example>
Question: <Question>
Answer:
Table 16: Prompt of querying the baseline model for final answer with few-shot demonstration
exemplars.
For few-shot CoT prompting, we use the same template as the Baseline prompting in Sec. D.4 by
replacing the few-shot examples using CoT responses, as shown in Tables 18, 19, 20, 21, and 22.
D.6 T AKE A DEEP BREATH (TDB)
We study the zero-shot prompting found in Yang et al. (2023): we take Take a deep breath and work
on this problem step-by-step, and prepend it to the question.
E E XAMPLES OF ERROR ANALYSIS AND WINS OF STEP -BACK PROMPTING
E.1 MMLU ERROR ANALYSIS
In Tables 23-27, we show one example for each of the 5 error categories we identified through error
analysis on STEP -BACK PROMPTING .
E.2 E XAMPLE WINS FROM STEP -BACK PROMPTING
Tables 28- 30, 31 32, 33 and 34 illustrate the some successful examples of STEP -BACK PROMPTING
on MMLU-Physics, MMLU-Chemistry, TimeQA, SituatedQA, and StrategyQA respectively.
21



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 2):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
2023) demonstrating the efficacy of STEP -BACK PROMPTING in tackling complex tasks, which are
otherwise challenging due to the amount of details needed for reasoning. Figure 1 shows a summary
of all the key results presented in this paper. Some the tasks are very challenging: both PaLM-2L and
GPT-4 achieve only ∼ 40% accuracy on TimeQA and MuSiQue. Chain-of-Thought prompting leads
to a minor improvement on a few tasks, while STEP -BACK PROMPTING improves the performance
of PaLM-2L across the board: 7% and 11% on MMLU Physics and Chemistry, 27% on TimeQA,
and 7% on MuSiQue.
We conduct a variety of analyses and find that STEP -BACK PROMPTING leads to strong performance
improvements (up to 36%) over chain-of-thought (CoT) prompting (Wei et al., 2022b) and “take-a-
deep-breath” (TDB) prompting (Yang et al., 2023). We perform a qualitative evaluation where we
find that Step-Back fixes a large portion of errors of the base model (up to ∼ 40%) while introducing
a small portion of new errors (max ∼ 12%). We also conduct an error analysis and find that majority
of the errors made by STEP -BACK PROMPTING is attributed to the intrinsic limitations of reasoning
capabilities of LLMs while abstraction skills are relatively easy to demonstrate to LLMs, pointing
out the direction for future improvements of methods alike STEP -BACK PROMPTING .
2 S TEP -BACK PROMPTING
STEP -BACK PROMPTING is motivated by the observation that many tasks contain a lot of details,
and it is hard for LLMs to retrieve relevant facts to tackle the task. As shown in the first example
(top) in Figure 2, for a Physics question of “What happens to the pressure, P , of an ideal gas if the
temperature is increased by a factor of 2 and the volume is increased by a factor of 8 ?”, the LLM can
deviate from the first principle of Ideal Gas Law when reasoning directly on the question. Similarly, a
question of “Estella Leopold went to which school between Aug 1954 and Nov 1954?” is very hard to
address directly given the detailed time range constraint. In both cases, asking a step-back question
helps the model to solve the problem effectively.
We define a step-back question as a derived question from the original question at a higher level of
abstraction. For instance, instead of directly asking “which school Estella Leopold went to during a
specific period”, a step-back question (Figure 2 bottom) would ask about the “education history”,
which is a high-level concept encompasses the original question. Answering the step-back question
of “Estella Leopold’s education history” in this case will provide all the necessary information to
reason about “which school Estella Leopold went to during a specific period”. The premise is that
the step-back question is typically much easier. Grounding the reasoning on top of such abstractions
helps to avoid reasoning errors in the intermediate steps such as the example shown in Figure 2 (left)
from Chain-of-Thought. In short, S TEP -BACK PROMPTING consists two simple steps:
• Abstraction: Instead of addressing the question directly, we first prompt the LLM to ask a generic
step-back question about a higher-level concept or principle, and retrieve relevant facts about the
high-level concept or principle. The step-back question is unique for each task in order to retrieve
the most relevant facts.
• Reasoning: Grounded on the facts regarding the high-level concept or principle, the LLM can
reason about the solution to the original question. We term this asAbstraction-grounded Reasoning.
In the following sections, we present an empirical study of STEP -BACK PROMPTING on a range of
challenging tasks covering STEM, Knowledge QA, and Multi-Hop Reasoning involving complex
reasoning.
3 E XPERIMENTAL SETUP
Here we define the tasks and models we experiment with. We also describe our evaluation metric and
the baselines we consider.
3.1 T ASKS
We experiment with the following diverse tasks: (a) STEM, (b) Knowledge QA, and (c) Multi-Hop
Reasoning. We describe below the datasets we consider (see Appendix B for more details).
3



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 8):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
complex tasks such as knowledge-intensive QA, multi-hop reasoning, and science questions into two
separate steps of Abstraction and Reasoning. We demonstrate through empirical experiments that
Abstraction is an easy skill for the LLMs such as PaLM-2L via sample-efficient in-context learning.
Grounding on the high-level concepts and principles, LLMs can leverage their intrinsic Reasoning
capabilities to derive the solution. This reduces the chance of reasoning failures in the intermediate
steps and is shown to improve the performance on a wide range of complex reasoning tasks. Despite
the success, through error analysis, we find that Reasoning is still one of the hardest skills for LLMs
to acquire: it is still the dominant failure mode even after the large reduction of task complexity by
STEP -BACK PROMPTING .
Nevertheless, Abstraction is neither necessary nor possible in all scenarios. For instance, the task
can be as simple as who was the president of the United States in 2000? , in which case there is
no such need to step back and ask a high-level question as the answer to such questions is readily
available. Questions such as what is the speed of light? point to the first principles themselves. Doing
Abstraction in this case would not make a difference either.
8 R ELATED WORK
8.1 P ROMPTING
Few-shot prompting (Brown et al., 2020; Liu et al., 2023; Mishra et al., 2022a; Wei et al., 2022b)
has significantly improved model performance across a range of tasks without requiring updating
any model parameters. Our work STEP -BACK PROMPTING is in the same category as the chain-of-
thought prompting (Wei et al., 2022b) and scratchpad (Nye et al., 2021) owing to its simplicity and
generic nature. But our approach is focused on the key idea of abstraction which is inspired from the
fact that taking a step back often helps humans in performing complex tasks. Our work is also related
to the recitation-augmented language models (Sun et al., 2022); however in contrast to their work, we
explicitly perform step-back and abstraction, with optional use of retrieval augmentation depending
on the nature of the task at hand.
8.2 D ECOMPOSITION
Decomposing a task into simpler tasks and solving these tasks to complete the original task has
been an effective way (Zhou et al., 2022; Patel et al., 2022; Khot et al., 2022; Press et al., 2022) to
improve model performance on complex tasks. Several prompting methods have been successful in
this regard. Our work STEP -BACK PROMPTING , in contrast, is on making the question more abstract
and high-level, which is different from decomposition that is often a low-level breakdowns of the
original question. For instance, a generic question for which employer did Steve Jobs work for in
1990? could be what is the employment history of Steve Jobs? While decomposition would lead
to sub-questions such as What was Steve Jobs doing in 1990?, Was Steve Jobs employed in 1990?
and If Steve Jobs was employed, who was his employer? Furthermore, abstract questions such as
what is the employment history of Steve Jobs? are often generic in nature to have a many-to-one
mapping since many questions (e.g. which employer did Steve Jobs work for in 1990? and which
employer did Steve Jobs work for in 2000?) can have the same abstract question. This is in contrast
to decomposition where there is often a one-to-many mapping since there are multiple decomposed
sub-problems necessary to solve a given question.
9 C ONCLUSION
We introduce STEP -BACK PROMPTING as a simple yet generic method to elicit deep reasoning via
abstraction in large language models. Experimentation on LLMs across fact-seeking, commonsense
reasoning and domain-specific reasoning benchmarks shows that STEP -BACK PROMPTING signifi-
cantly improves model performance. We hypothesize that abstraction helps models to hallucinate
less and reason better, probably reflecting the true nature of the model which are often hidden
while responding to the original question without abstraction. We hope our work will inspire more
human-inspired approaches to elicit the hidden potential of large language models.
9



### Claim 37/179

#### Claim Text
Recent research has introduced prominent embedding models such as AngIE, V oyage, BGE,etc [94]–[96], which are benefit from multi-task instruct tuning.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[45]_2310.07713.pdf (Page 3):

InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining
sions, number of layers, and attention heads. We adopt the
Sentence Piece tokenizer (Kudo & Richardson, 2018) for
both GPT and Retro. We pretrain all models with 1.1 trillion
tokens of the pretraining corpus. More details of corpus be
found in Appendix §A.1.
Unfreezing decoder at Retro-fitting. As Retro shares its
backbone decoder with the GPT decoder and only adds
around 10% additional parameters for Retro encoder and
cross-attention, we can initialize Retro decoder from pre-
trained GPT models, randomly initialize Retro encoder
and cross-attention, and continue pretraining with retrieval,
which is named as “ Retro-fitting”. Note that, Borgeaud
et al. (2022) freezes the decoder parameters at Retro-fitting.
In contrast, we unfreeze all the decoder parameters and
continue pretraining the entire model. We also conduct an
ablation study of Retro-fitting based on a pretraiend GPT
of 823M parameters and compare the validation perplex-
ity loss when freezing or unfreezing Retro decoder during
pretraining. As shown in Figure 3, given the same training
schedules, unfreezing Retro decoder parameters converges
faster and demonstrates better validation perplexity, which
eventually yields a better Retro decoder to incorporate in-
context retrieved evidence, even without a Retro encoder
as shown in §5.4. We continue pretraining with retrieval
on an additional 100 billion tokens, which is 9% of the pre-
training data used for pretrained GPT models. To have a
fair comparison, we also continue pretraining GPT foun-
dation models on the same 100 billion tokens, which we
name “GPT-fitting”. In terms of overall pretraining cost,
Retro 48B only need 2.58% additional GPU hours than its
counterpart GPT trained on 1.2T tokens. More details of
continued pretraining are in Appendix A.2 and A.3.
Perplexity evaluation. We evaluate the perplexity of GPT
foundation models, GPT-fitting models, and Retro-fitting
models of varying parameter sizes in Figure 2. The val-
idation corpus consists of 1% held-out samples from the
pretraining corpus, which are not used in the pretraining
stage, the continued pretraining stage, and the retrieval
database to ensure that there is no validation data leakage.
From Figure 2, one can see that after continued pretraining
on additional 100 billion tokens, the perplexity of GPT-
fitting slightly improves over original pretrained GPT, while
Retro significantly outperforms both GPT and GPT-fitting
across different parameter sizes in terms of perplexity. Retro
achieves even better perplexity than GPT models with 4×
larger parameter sizes. Notably the improvement is still
significant when the parameter sizes of Retro scale up to
48B, and the gap does not decrease from 8B to 48B. We
present more evaluation results in §5.4.
4. Instruction Tuning
Instruction tuning can significantly improve the ability of
foundation LLMs to follow instructions, thus improving
zero-shot results on downstream tasks (e.g., Wei et al.,
2022a; Chung et al., 2022). In this section, we further
enhance Retro via instruction tuning.
4.1. Datasets Blending
Existing instruction tuning methods mainly leverage super-
vised fine-tuning on a blend of instruction following datasets
(Wei et al., 2022a; Chung et al., 2022; Sanh et al., 2022a;
Wang et al., 2023b).
We use a blend of high-quality instruction tuning datasets to
train LLMs to follow instructions in conversational formats,
which include: i) a high-quality social dialogue dataset
SODA (Kim et al., 2022), ii) a long-form QA dataset ELI5
that requires elaborate answers (Fan et al., 2019), iii) LLM-
generated instructions: Self-Instruct (Wang et al., 2022b)
and Unnatural Instructions (Honovich et al., 2022), iv)
FLAN and Chain-of-thought datasets (Chung et al., 2022;
Wei et al., 2022c; Longpre et al., 2023), v) a private crowd-
sourced conversational dataset and public human-written
conversation datasets OpenAssistant (Köpf et al., 2023) and
Dolly (Conover et al., 2023), and vi) samples from the pre-
training corpus.
The format of all the instruction tuning data is unified in a
conversational way with three roles: “system”, “assistant”,
and “user”. The “system” role sets up the tone and style of
LLM assistants to give helpful, detailed, and polite answers
to the user’s questions. The “user” and “assistant” role
contains the questions and the corresponding answers from
the instruction tuning datasets. We show an example format
of the instruction data in Appendix C.1. In total, we collect
a total of 128K high-quality samples for instruction tuning.
4.2. Training details
For each training sample, we take the multi-turn conversa-
tions between the user and the assistant as context and apply
the loss mask only to the last response from the assistant.
We use the standard language modeling loss with teacher
forcing. Since Wei et al. (2022a) suggests that instruction
tuning is most effective with large language models, we ap-
ply instruction tuning to the GPT-fitting 43B model and the
Retro 48B model, naming them “InstructGPTRAG 43B”2 and
“InstructRetro 48B”, respectively. We finetune the LLMs by
taking the loss only on the answer part with a batch size of
128 and a learning rate of 5e-6 for 1000 steps with a weight
decay of 0.01. We use the Adam optimizer (Kingma & Ba,
2We distinguish “InstructGPTRAG”, which uses supervised fine-
tuning and RAG, from “InstructGPT” (Ouyang et al., 2022), which
leverage RLHF for instructing tuning.
4



Source: data\tc16_2312.10997v5\referenced_papers\[45]_2310.07713.pdf (Page 15):

InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining
C. Qualitative examples
C.1. An example From the instruction tuning data
Table 9 below is a data example from the instruction tuning data.
Table 9.A sample conversation from our instruction tuning dataset. The black part serves as the context, while the black part is masked as
the answer mask, on which the LM loss is computed.
Speaker Conversation
System
This is a chat between a user and an artificial intelligence assistant.
The assistant gives helpful, detailed, and polite answers to the user’s
questions based on the context. The assistant should also indicate when
the answer cannot be found in the context.
User
My bicycle tire has a puncture and while I have a pump with me, I don’t
have a patch repair kit. Is there a way to keep the tire from running out
of air until I can get to a bicycle shop?
Assistant Yes, you could use duct tape to seal the puncture.
User Can you give me a tutorial on how to use the duct tape to seal the
puncture?
Assistant
Sure, here’s a tutorial on how to use duct tape to seal a puncture in a tire:
1. Start by cleaning the area around the puncture with a damp cloth.
2. Cut a piece of duct tape large enough to cover the puncture, plus a
few millimeters of extra.
3. Remove the backing from the duct tape, and press it firmly over the
puncture.
4. Rub the tape firmly with your fingers to ensure a good seal.
5. You can also use a roller or a flat item like a ruler to make sure that
the tape is pressed firmly against the puncture.
6. Trim away any excess tape, and your puncture should now be sealed.
16



Source: data\tc16_2312.10997v5\referenced_papers\[98]_2307.03172.pdf (Page 7):

1st 5th 10th
Position of Document with the Answer
50
55
60
65
70Accuracy
10 T otal Retrieved Documents (~2K tokens)
1st 5th 10th 15th 20th
Position of Document with the Answer
50
55
60
65
70Accuracy
20 T otal Retrieved Documents (~4K tokens)
1st 5th 10th 15th 20th 25th 30th
Position of Document with the Answer
50
55
60
65
70Accuracy
30 T otal Retrieved Documents (~6K tokens)
mpt-30b-instruct longchat-13b-16k flan-t5-xxl flan-ul2
Figure 8: When encoder-decoder models (Flan-UL2 and Flan-T5-XXL) evaluated on sequences that are shorter
than their encoder’s training-time maximum sequence length (2048 and 512 tokens, respectively), they are relatively
robust to changes in the position of relevant information within their input context (left subplot). In contrast, when
these models are evaluated on sequences longer than those seen during training (center and right subplots), we
observe a U-shaped performance curve—performance is higher when relevant information occurs at the beginning
or end of the input context, as opposed to the middle of the input context.
1st 5th 10th 15th 20th
Position of Document with the Answer
50
60
70
80Accuracy
20 T otal Retrieved Documents 
(~4K tokens, query-aware contextualization)
claude-1.3
claude-1.3-100k
gpt-3.5-turbo-0613
gpt-3.5-turbo-16k-0613
mpt-30b-instruct
longchat-13b-16k
Figure 9: Query-aware contextualization (placing the
query before and after the documents) does not sub-
stantially improve robustness of language models to
changing the position of relevant information in multi-
document QA; performance slightly increases when
relevant information occurs at the very beginning, but
otherwise slightly decreases.
of the prompt and decoder-only models can only
attend to prior tokens at each timestep. In contrast,
encoder-decoder models (which seem more robust
to changes in the position of relevant information;
§4.1) use a bidirectional encoder to contextualize
input contexts—can we use this observation to im-
prove decoder-only models by placing the query be-
fore and after the data, enabling query-aware con-
textualization of documents (or key-value pairs)?
We find that query-aware contextualization dra-
matically improves performance on the key-value
retrieval task—all models achieve near-perfect per-
formance on the 75, 140, and 300 key-value pair
settings. For example, GPT-3.5-Turbo (16K) with
query-aware contextualization achieves perfect per-
formance when evaluated with 300 key-value pairs.
In contrast, without query-aware contextualiza-
tion, the worst-case performance is 45.6% (Fig-
ure 7). Despite the significant impact on key-
value retrieval performance, query-aware contextu-
alization minimally affects performance trends in
the multi-document question answering task (Fig-
ure 9); it slightly improves performance when the
relevant information is located at the very begin-
ning of the input context, but slightly decreases
performance in other settings.
4.3 Effect of Instruction Fine-Tuning
The models we evaluated are all instruction fine-
tuned—after their initial pre-training, they undergo
supervised fine-tuning on a dataset of instructions
and responses. The task specification and/or in-
struction is commonly placed at the beginning of
the input context in supervised instruction fine-
tuning data, which might lead instruction fine-
tuned language models to place more weight on
the start of the input context. To better understand
the potential effects of instruction fine-tuning on
how language models use long input contexts, we
compare the multi-document question answering
performance of MPT-30B-Instruct against its base
model (i.e., before instruction fine-tuning) MPT-
30B. We use the same experimental setup as §2.
Figure 10 compares the multi-document QA
performance of MPT-30B and MPT-30B-Instruct
as a function of the position of the relevant in-



Source: data\tc16_2312.10997v5\referenced_papers\[45]_2310.07713.pdf (Page 6):

InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining
Table 1.Zero-shot evaluation on eight short-form QA and reading comprehension datasets. The average relative improvement of decoder-
only InstructRetro 43B across the short-form QA tasks is 7% over InstructGPTRAG. Note that, InstructRetro 43B obtains very comparable
results than original InstructRetro 48B with encoder.
 denotes using retrieval augmentation at both training and generation, while
denotes using retrieval augmentation at inference only.
Task NQ TriviaQA NewsQA SQuAD 2.0 SQuAD 1.1 Quoref NarrativeQA DROP
Metric EM EM F1 F1 / EM F1 / EM F1 F1 F1
GPT-3 175B 14.6 64.3 - 59.5 / 52.6 - - - 23.6(Brown et al.)
PaLM 2 -L 37.5 - - - / - - - - -(Chowdhery et al.)
GLaM 64B 24.7 71.3 - 71.1 / 64.7 - / - - - 57.3(Du et al.)
FLAN-LaMDA 137B 20.7 68.1 - 44.2 / - 80.1 / - - - 22.7(Wei et al.)
Llama 2 RAG 70B
 37.7 65.6 53.4 71.4 / 64.1 73.4 / 66.2 69.7 52.7 57.2(Touvron et al.)
Retro 7.5B
 8.9 36.0 - - / - - - - -(Borgeaud et al.)
Retro++ 9B
 25.8 48.3 - - / - - - - -(Wang et al.)
Atlas 11B
 26.7 56.9 - - / - - / - - - -(Izacard et al.)
Raven 11B
 29.6 65.7 - - / - - / - - - -(Huang et al.)
RA-DIT 65B
 35.2 75.4 - - - / - - - -(Lin et al.)
InstructGPTRAG 43B
 37.0 78.1 52.4 70.7 / 64.3 72.4 / 65.8 71.5 53.9 51.8
InstructRetro 43B
 38.9 78.3 57.4 75.6 / 69.3 77.1 / 70.4 76.2 60.0 54.8
(w/o encoder, Avg: +7%) (+5.14%) (+0.26%) (+9.54%) (+6.93%) (+6.49%) (+6.57%) (+11.32%) (+5.79%)
InstructRetro 48B
 38.6 77.8 57.0 74.8 / 67.7 76.4 / 69.0 76.1 59.8 54.6
(w/ encoder, Avg: +6%) (+4.32%) (-0.38%) (+8.78%) (+5.80%) (+5.52%) (+6.43%) (+10.95%) (+5.41%)
For example, InstructRetro 43B achieves better accuracy
than Llama 2 with RAG on multiple tasks, close to FLAN-
LaMDA 137B, which is 3× the size of InstructRetro 43B.
Impact of Retro encoder for downstream tasks. We also
notice that InstructRetro 48B and 43B perform very com-
parable from Table 1. We enable the Retro encoder for
retrieval-augmented pretraining, while disabling the Retro
encoder due to the lack of retrieved high-quality neighbors
for instruction tuning. Note that we still perform retrieval-
augmented generation for downstream tasks, where the re-
trieved contexts are put into the decoder of both Instruc-
tRetro 48B and 43B as part of the prompts. The only differ-
ence is whether we enable the cross attention gate in Figure
4 to attend the Retro encoder in InstructRetro 48B or disable
it in InstructRetro 43B. When enabling the Retro encoder,
we put the top-2 neighbors in the encoder to align with the
pretraining behavior.
This suggests that although Retro is proficiently trained to
infer both with and without the neighbors in the encoder, it
is more beneficial to align with the instruction tuning pro-
tocols and bypass the Retro encoder to solely serve as a
GPT decoder during evaluation. We think it is an important
and promising future research direction to explore retrieval-
augmented instruction tuning with the Retro encoder ac-
tivated, especially when high-quality retrieval-augmented
instruction tuning data is available.
InstructRetro demonstrates larger improvement on long-
form QA datasets. When comparing the results of In-
structRetro on short-form QA datasets and long-form QA
datasets, we observe InstructRetro 43B demonstrates large
7



Source: data\tc16_2312.10997v5\referenced_papers\[45]_2310.07713.pdf (Page 16):

InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining
C.2. An example From the downstream QA dataset: SQuAD 1.1
We re-format all of the QA tasks in the conversational format. An example from the SQuAD 1.1 dataset with the
conversational format is shown in the Table 10 below.
Table 10.A sample conversation prompt template from the SQuAD 1.1 dataset.
Speaker Conversation
System
System: This is a chat between a user and an artificial intelligence
assistant. The assistant gives helpful, detailed, and polite answers to the
user’s questions based on the context. The assistant should also indicate
when the answer cannot be found in the context.
User
title: , source: The pound-force has a metric counterpart, less commonly
used than the newton: the kilogram-force (kgf) (sometimes kilopond),
is the force exerted by standard gravity on one kilogram of mass. The
kilogram-force leads to an alternate, but rarely used unit of mass: the
metric slug (sometimes mug or hyl) is that mass that accelerates at 1
ms-2 when subjected to a force of 1 kgf. The kilogram-force is not a
part of the modern SI system, and is generally deprecated; however it
still sees use for some purposes as expressing aircraft weight, jet thrust,
bicycle spoke tension, torque wrench settings and engine output torque.
Other arcane units of force include the sthène, which is equivalent to
1000 N, and the kip, which is equivalent to 1000 lbf.
Based on the above article, answer a question. What is the seldom used
force unit equal to one thousand newtons?
Assistant The answer is
17



### Claim 38/179

#### Claim Text
PROMPTAGATOR [21] utilizes the LLM as a few-shot query generator to create task-specific retrievers, addressing challenges in supervised fine-tuning, particularly in data-scarce domains.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[21]_2209.11755.pdf (Page 1):

Retrieval Performance on 11 BEIR Datasets
46.6 45.5
47.8
49.9
52.8
Promptagator++ Zero-shot
Supervised (MS MARCO) *
Promptagator Zero-shot
Promptagator Few-shot
Promptagator++ Few-shot
Figure 1: Few-shot retrieval with PROMPTAGATOR . Left (a): Retrieval tasks from BEIR differ in query
distribution, retrieval corpus, and search intents. Middle (b): Most prior work uses supervised setting
(2) which trains model on a large QA retrieval datasets and transfer to other retrieval tasks. Right
(c): Few-shot PROMPTAGATOR performance. Average nDCG@10 on 11 datasets from BEIR from our
PROMPTAGATOR models and previously MS MARCO-supervised models (SPLADE v2).
domain. Moreover, different tasks have distinct distributions of queries even when their search intents
are similar. For example, in the BEIR benchmark, queries in HotpotQA (Yang et al., 2018) are long
compositional questions, while queries in FiQA (Maia et al., 2018) are short ﬁnancial questions.
In this paper, we advocate to work on the setting of Few-shot Retrieval for diverse retrieval (§2),
where each task comes with a short description and a few annotated examples to clearly illustrate
the search intents. Given that only a few examples are available, we propose Prompt-base Query
Generation for Retriever (PROMPTAGATOR ) (§3) which aims to resolve the data scarcity issue while
retaining the efﬁciency of a small dual encoder, by harnessing the power of large language models
(LLM) such as FLAN (Wei et al., 2022a). PROMPTAGATOR combines prompting with LLMs as a
query generator without ﬁne-tuning (§3.1), and can generate good queries with minimal supervision
– shown in Figure 1(b), it solely relies on a few supervised examples from the target task without
using annotated query-document pairs from Natural Questions (Kwiatkowski et al., 2019) or MS
MARCO (Nguyen et al., 2016) to train the retriever directly. The key insight of PROMPTAGATOR is to
amplify the power of few-shot examples by creating task-speciﬁc prompting, which in turn enables
generating a large set of synthetic queries for training retrievers suited for the task. To ensure the
generated data quality, we develop a ﬁltering technique that ensures round-trip consistency using
generated data only (§3.2). Our ﬁlter is tailored to retrieval, which removes ambiguous, generic, and
low-quality questions, and signiﬁcantly improves retrieval performance.
While PROMPTAGATOR is not the ﬁrst application of LLM for retrieval, prior attempts of using LLMs
often come with higher serving cost. Neelakantan et al. (2022) proposes to use the GPT-3 (Brown
et al., 2020) embeddings in dual encoder models. However, the embedding size is 12k and hence
makes the search index footprint and inference cost high. Sachan et al. (2022) and Bonifacio et al.
(2022) have applied prompting and LLMs for reranking, while leaving the retriever untouched. With
PROMPTAGATOR , we show that LLMs can be used to generate efﬁcient end-to-end retriever with high
accuracy. The contributions of the paper are as follows:
• We analyze the previously overlooked differences across retrieval tasks in their search intents
and query distributions, and propose a Few-Shot Retrieval setting for the BEIR dataset. Our
prompt and fewshot examples will be released to facilitate future research.
• We propose PROMPTAGATOR , a simple recipe for few-shot retrieval by prompting with
a LLM to generate synthetic task-speciﬁc training data. For the ﬁrst time, end-to-end
retrievers solely based on a few supervised examples can be strong and efﬁcient to serve
with PROMPTAGATOR .
• Our experimental results show that, surprisingly, PROMPTAGATOR with two-to-eight ex-
amples produced signiﬁcantly better retrievers compared to recent models trained on MS
MARCO or NQ that have over 500K human annotated examples (Figure 1(c)). PROMPTA -
GATOR outperforms ColBERT v2 and SPLADE v2 on 11 retrieval tasks we tested, while
reranking boosts results by another 5 points on standard retrieval evaluation metric.
2



Source: data\tc16_2312.10997v5\referenced_papers\[21]_2209.11755.pdf (Page 0):

PROMPTAGATOR
 : F EW-SHOT DENSE RETRIEVAL
FROM 8 EXAMPLES
Zhuyun Dai∗†, Vincent Y. Zhao∗†, Ji Ma∗†, Yi Luan∗†, Jianmo Ni, Jing Lu, Anton Bakalov,
Kelvin Guu, Keith B. Hall and Ming-Wei Chang†
Google Research
{zhuyundai, vzhao, maji, luanyi, mingweichang}@google.com
∗equal contributions †corresponding authors
ABSTRACT
Much recent research on information retrieval has focused on how to transfer
from one task (typically with abundant supervised data) to various other tasks
where supervision is limited, with the implicit assumption that it is possible to
generalize from one task to all the rest. However, this overlooks the fact that
there are many diverse and unique retrieval tasks, each targeting different search
intents, queries, and search domains. In this paper, we suggest to work on Few-shot
Dense Retrieval, a setting where each task comes with a short description and
a few examples. To amplify the power of a few examples, we propose Prompt-
base Query Generation for Retriever (PROMPTAGATOR
 ), which leverages large
language models (LLM) as a few-shot query generator, and creates task-speciﬁc
retrievers based on the generated data. Powered by LLM’s generalization ability,
PROMPTAGATOR makes it possible to create task-speciﬁc end-to-end retrievers solely
based on a few examples without using Natural Questions (Kwiatkowski et al.,
2019) or MS MARCO (Nguyen et al., 2016) to train dual encoders. Surprisingly,
LLM prompting with no more than 8 examples allows dual encoders to outperform
heavily engineered models trained on MS MARCO like ColBERT v2 (Santhanam
et al., 2022) by more than 1.2 nDCG on average on 11 retrieval sets. Further
training standard-size re-rankers using the same generated data yields another 5.0
point nDCG improvement. Our studies determine that query generation can be
far more effective than previously observed, especially when a small amount of
task-speciﬁc knowledge is given.
1 I NTRODUCTION
Recently, major progress has been made on neural retrieval models such as dual encoders, which can
retrieve knowledge from a large collection of documents containing millions to billions of passages
(Yih et al., 2011; Lee et al., 2019; Karpukhin et al., 2020). However, Thakur et al. (2021) recently
proposed the BEIR heterogeneous retrieval benchmark, and showed that it is still difﬁcult for neural
retrievers to perform well on a wide variety of retrieval tasks that lack dedicated training data. Thus,
previous approaches focus on transferring knowledge from question answering (QA) datasets such
as MS MARCO (Nguyen et al., 2016). To best transfer from QA datasets, expressive retrievers
are developed that allow ﬁne-grained token-level interaction such as ColBERT (Khattab & Zaharia,
2020; Santhanam et al., 2022) and SPLADE (Formal et al., 2021) but with higher inference cost.
Data augmentation via synthetic question generation has previously been explored (Ma et al., 2021;
Shakeri et al., 2020), but these question generators are typically only trained on popular QA datasets.
We argue that it is hard to expect models based on one or two QA datasets to perform well across
different retrieval tasks. First, different retrieval tasks have very different search intents; in other
words, different deﬁnitions of “relevance”. For example, as illustrated in Figure 1(a), both Dbpedia-
Entity (Hasibi et al., 2017) and FEVER (Thorne et al., 2018) are tasks to retrieve documents from
Wikipedia. Dbpedia-Entity is a task to retrieve entities that are mentioned in the query, while
FEVER is a task to ﬁnd evidence that either supports or refutes a given statement. Which document
is relevant to the query can be very different from one task to another task even if they share the same
1
arXiv:2209.11755v1  [cs.CL]  23 Sep 2022



Source: data\tc16_2312.10997v5\referenced_papers\[21]_2209.11755.pdf (Page 9):

instruction, a few examples and also performs model ﬁne-tuning (Schick & Schütze, 2021a;b;c;
Gao et al., 2021b; Logan IV et al., 2022; Izacard et al., 2022b). Our work adopts the ﬁrst approach.
Usually 10-100 examples are used. For example, 32 examples are used in the few-shot setting in
GPT3. In the context of retrieval, Bonifacio et al. (2022) provides GPT3 three question-document
pairs and uses it as the question generator for training interaction based models.
Prompt-based Query Generation The idea of using prompted LLMs for query generation has
previously been proposed for improving retrieval reranking. In UPR (Sachan et al., 2022), they
proposed to use prompted LLMs to rerank the passages directly. InPars (Bonifacio et al., 2022) is
probably the most closely related work to ours, where they proposed to use few-shot prompting with
GPT-3 to generate synthetic data for training a T5-based reranker applied to a BM25 retriever. In this
paper, we propose few-shot prompted LLMs and show that generated data can produce efﬁcient and
strong end-to-end retrievers. Moreover, we show the quality of generated data can be improved by
task-speciﬁc prompts and consistency ﬁltering.
Retrievers with late interactions While dual encoder models are very efﬁcient at retrieval due to
the MIPS algorithms, their expressivity is limited due to the fact that their score is just a dot-product
between the query vector and the document vector. ColBERT (Santhanam et al., 2022; Khattab &
Zaharia, 2020) and SPLADE (Formal et al., 2021) are the models to increase the interactions between
the query and document by allowing token-level interactions. Because these models are not just dot
product between queries and documents, MIPS algorithms can not be used directly. Hence, these
models usually have much higher serving cost compared to dual encoders.
6 C ONCLUSION AND DISCUSSIONS
In this paper, we have presented PROMPTAGATOR , a novel approach to few-shot retrieval. We showed
that it is possible to create task-speciﬁc, end-to-end retrievers with only a few annotated examples.
The few-shot examples, ampliﬁed by prompt-based LLM query generation, simpliﬁes the complexity
of training neural retrievers for a new tasks and leads to promising retrieval performance gains. It
hopefully inspires future research to further push the limit of few-shot retrieval, towards generalizable
retrieval systems that can seamlessly and efﬁciently adapt to many tasks.
While we demonstrate that query generation can be very effective, many questions remain for the
roles of question generation and large language models. One of the key issue that needs thorough
investigation is on the generated data efﬁciency. We have not yet explored exactly how many query-
document pairs are needed for each task, or how to use these generated examples more efﬁciently.
Another issue that is worthwhile understanding is the sensitivity of ﬁnal retriever’s performance with
respect to the prompt. Finally, we would like to draw a connection fromPROMPTAGATOR to distillation,
as the ﬁnal dual encoders deﬁnitely beneﬁt a lot from the large language model. Analyzing the
headroom and understanding how we can better transfer knowledge from LLMs to retrievers would
be a critical topic for the future.
7 C OMPUTE USAGE AND ENVIRONMENTAL IMPACT
We used the 137B large language model FLAN for query generation. FLAN is based on the
same pretrained model as LaMDA (Thoppilan et al., 2022). LaMDA was pre-trained on a large
corpus consisting of 1.56T words, costing 451 MWh energy and 25.2 tCO2e carbon footprint. In
PROMPTAGATOR , we generated 29.23M queries * 2 prompts = 58.46M queries, for a total of 610M
words. As mentioned in (§6), PROMPTAGATOR can be viewed as distilling LLM to standard-sized
dual encoders via prompt-based query generation. While the distillation process is computationally
expensive, it signiﬁcantly reduces cost for inference.
ACKNOWLEDGEMENTS
We thank Kenton Lee, Tom Kwiatkowski, and Daniel Gillick for technical discussion and providing
feedback on our manuscript. We thank Alex Salcianu for developing a bulk inference pipeline for
large language models.
10



Source: data\tc16_2312.10997v5\referenced_papers\[21]_2209.11755.pdf (Page 4):

RetrievalSupervision Cross-AttnDistillation Retriever Token-levelRetrievalServing ModelSize # RerankingDoc. QGenModel
Contriever NA self 110M 0GTR-XXL MS MARCO(500K) self 6B 0Splade v2 MS MARCO(500K) self  110M 0ColBERT v2MS MARCO(500K) self  110M 0GenQ MS MARCO(500K) self 110M 0 T5 (MS MARCO)GPL MS MARCO(500K) self 110M 0 T5 (MS MARCO)MonoT5 MS MARCO(500K) BM25  3B 1000InPars Few (3) BM25  3B 1000 GPT-3UPR NA Contriever 110M+3B 1000 T0 ∗
PROMPTAGATOR Few (0-8) self 110M 0 FLANPROMPTAGATOR++ Few (0-8) P ROMPTAGATOR 110M+110M 200 FLAN
Table 1: Comparison of settings, resources and model size for different frameworks. Our models are
just a 110M-size dual encoder PROMPTAGATOR and a 110M-size reranker PROMPTAGATOR ++, as good
quality generated data allows simple models/pipeline to achieve strong performance. See text for
more details for UPR’s QGen model1.
Following prior work (Ni et al., 2021), we initialize the dual encoder using the Transformer encoder
from a T5 (Raffel et al., 2020) checkpoint. We then pretrain our retriever on C4 with the independent
cropping task from Contriever (Izacard et al., 2022a), where we treat two random crops from the
same document as positive retrieval pairs and train with a cross-entropy loss over in-batch random
negatives. Next, we ﬁne-tune the dual-encoder on the query-document pairs generated from our
prompt-base QGen, again with cross-entropy loss over in-batch random negatives. After training for
a set number of epochs, we apply round-trip ﬁltering on our synthetic data as described in (§3.2)
using this initial dual encoder, and continue to ﬁne-tune the dual encoder on the ﬁltered data.
We also propose PROMPTAGATOR ++, a reranker trained on the same synthetic data generated from
our prompt-base QGen, which reﬁnes the retrieved candidates using a slower but more accurate
cross-attention model. We train the reranker using a cross-entropy loss with 31 sampled negatives
from top 200 passages retrieved by the PROMPTAGATOR retriever, which approximates the inference
time distribution (reranking top 200 from the retriever).
3.4 Z ERO -SHOT PROMPTAGATOR RETRIEVER
The prompt-based query generation can also run in a zero-shot manner, where we universally apply
the following prompt irrespective of the target task: f’{d} Read the passage and generate
a query.’. Here d denotes the document text. We train retrievers and rerankers on the zero-shot
prompt generated data, leading to zero-shot PROMPTAGATOR and zero-shot PROMPTAGATOR ++.
3.5 D ISCUSSION
Table 1 compares the PROMPTAGATOR recipe to some recently proposed approaches. Our dual encoder
does not rely on hard negative mining or distillation; it uses a standard dual encoder model without
adding the token-level matching inductive biases that ColBERT and SPLADE have. Our reranker
also uses a 110M model instead of larger models. We aim to use this simpliﬁed recipe to highlight the
power of few-shot data, as we will shown in (§4.3). Comparing PROMPTAGATOR to these approaches,
the ability to use a prompt and few-shot examples with a LLM makes PROMPTAGATOR be able to
generate efﬁcient models with high accuracy. While other LLM approaches such as InPars (Bonifacio
et al., 2022) and UPR (Sachan et al., 2022) have focused on reranking, PROMPTAGATOR focuses on
retrieval.
4 E XPERIMENTS
We report quantitative evaluation ofPROMPTAGATOR by measuring its retrieval performance on the
BEIR benchmark. We then dive deeper into the results through ablation studies and qualitative
analysis.
1UPR uses T0 query generation for reranking, instead of for synthetic data augmentation that other QGen
approaches do.
5



Source: data\tc16_2312.10997v5\referenced_papers\[21]_2209.11755.pdf (Page 8):

OTHER 72.4the 15.2a 4.3it 3.7there2.3this1.2if 0.8
(a) Gold queries
OTHERS 63.1the 23.2a 1.7it 4.3there4.1this1.9progressive1.7
(b) Few-shot
OTHERS 10.6what 30.4who 28.4the 9.1when 8.3where 6.8why 6.5
(c) NQ-QGen
while 25.0
it 25.0
new 25.0
the 25.0
(d) Prompts (4 examples)
Figure 3: Top ﬁrst word distribution on queries generated from different models in the ArguAna
dataset. Left (a)(b)(c): Compare gold queries (a) and generated queries (b)(c). Queries generated by
few-shot models has closer distribution to the gold queries, while the NQ-QGen queries are mostly
questions. Right (d): The few shot FLAN can generate diverse queries even though there are only 4
examples in the prompt. Statistics of more datasets are available in the Appendix (Figure 4).
4.4 Q UALITATIVE ANALYSIS
In order to understand the advantages of few-shot PROMPTAGATOR , we analyze the distribution of the
ﬁrst words of the queries generated by different query generation methods for the ArguAna task in
Figure 3. Note that the distribution of few-shot PROMPTAGATOR (Fig. 3b) is much closer to the real
distribution (Fig. 3a) while the NQ-QGen (Fig. 3c) mostly generated questions even when query of
the tasks are arguments. Examples are showcased in Table 5 in the Appendix.
5 R ELATED WORK
Neural retrieval models The success of pre-trained large language models (Devlin et al., 2019;
Raffel et al., 2020; Brown et al., 2020) has fostered a lush growth in the ﬁeld of neural retrieval
models. Neural retrieval models can be grouped into two categories, namely representation based
models and interaction based models.
Representation based models (Palangi et al., 2016; Gillick et al., 2018; Karpukhin et al., 2020) encode
a query and passage independently into a common dense space, and scores their relevance based on
vector dot-product or cosine similarity. Recent research on representation based models has primarily
focused on the following aspects: developing better pre-training tasks (Lee et al., 2019; Chang et al.,
2020; Khattab & Zaharia, 2020; Izacard et al., 2022a; Oguz et al., 2022) or pre-training architectures
(Gao & Callan, 2021; 2022), improving expressiveness using multi-vector representations (Luan
et al., 2021), improving negative contrast (Qu et al., 2021; Xiong et al., 2021; Lu et al., 2021), and
improving generalization across different domains (Thakur et al., 2021; Ren et al., 2022). Different
techniques have been explored to improve the generalization, such as using query generation for data
augmentation (Ma et al., 2021; Wang et al., 2022), using contrastive learning for better pre-training
(Izacard et al., 2022a), using knowledge distillation (Chen et al., 2021; Wang et al., 2022) and scaling
the model size (Ni et al., 2021).
Although encoding the query and document into a single vector enables fast retrieval via approximate
nearest neighbor search (Johnson et al., 2021; Wu et al., 2019), it also constrains the representational
power of these models thus leading to sub-optimal predictions. Interaction based models on the other
hand explicitly model the interaction between query and document terms (Guo et al., 2016; Hui
et al., 2017; Xiong et al., 2017; Dai et al., 2018; McDonald et al., 2018; Nogueira & Cho, 2019),
and therefore make more informed decisions. These models are typically more expensive, and thus
are used for reranking or rescoring. Distilling interaction based models into representation based
models has been shown effective in closing the gap between the two (Hofstätter et al., 2020; Ren
et al., 2021a; Lin et al., 2021; Ren et al., 2021b; Reddi et al., 2021; Zhang et al., 2022). Another
attempt to combine the best of both worlds is by postponing the interaction until the last layer of the
model (Gao et al., 2021a; Khattab & Zaharia, 2020), blurring the boundary between representation
and interaction models.
Few-shot Learning The development of pre-trained large language models also popularize the
few-shot learning paradigm, which utilizes a few examples as context for model inputs (Brown et al.,
2020; Wei et al., 2022b). Two approaches are commonly used. One approach is to provide the LLM
an instruction of the task in natural language with a few examples and do not update any parameter
of LLM (Brown et al., 2020; Bonifacio et al., 2022). The other approach provides the LLM the
9



### Claim 39/179

#### Claim Text
Another approach, LLM-Embedder [97], exploits LLMs to generate reward signals across multiple downstream tasks.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 15):

Conference’17, July 2017, Washington, DC, USA Zhang and Xiao, et al.
Table 12: The impact of LLM-Embedder on different LLMs.
LLM Retriever MMLU PopQA ICL MSC Arxiv
Llama-2-7B-Chat
None 0.4599 0.2061 0.4645 19.3501 3.7647
BGE 0.4896 0.4491 0.5974 14.2943 3.2912
LLM-Embedder 0.4903 0.5052 0.6268 13.4832 3.2322
Aquila-7B-Chat
None 0.4499 0.2028 0.5145 16.0108 3.1204
BGE 0.4832 0.3982 0.5732 14.1843 2.7914
LLM-Embedder 0.4847 0.4405 0.5903 14.1836 2.7351
Qwen-7B-Chat
None 0.5561 0.2393 0.5346 21.0466 2.7888
BGE 0.5787 0.4447 0.6329 16.2064 2.5165
LLM-Embedder 0.5762 0.4782 0.6457 15.4524 2.4824
Baichuan2-7B-Chat
None 0.5226 0.2356 0.4907 18.9711 2.7510
BGE 0.5534 0.4407 0.5960 16.0759 2.4440
LLM-Embedder 0.5511 0.4848 0.6179 15.5890 2.4131
Llama-2-13B-Chat
None 0.5386 0.2886 0.4607 14.7334 3.2357
BGE 0.5603 0.4595 0.6196 11.6875 2.9036
LLM-Embedder 0.5580 0.5026 0.6439 11.5384 2.8540
the evaluation of tool learning and conversational search because
their performances are directly measured by retrieval metrics.
We can observe that our conclusions in Section 3.2.2 still holds.
First of all, retrieval from external world benefits LLM’s perfor-
mance in all four scenarios, since the performance of the plain
LLM (i.e. None) underperforms retrieval-augmented one (BGE and
LLM-Embedder). Besides, our proposed LLM-Embedder is able to
generalize well and maintain its superiority over BGE on most
datasets (PopQA and ICL in particular). An exception is MMLU,
where LLM-Embedder is slightly outperformed by BGE when using
Qwen, Baichuan, and Llama-2-13B. It seems that different LLMs
utilize the same knowledge in different ways, thereby obtaining a
little different results.



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 8):

Retrieve Anything To Augment Large Language Models Conference’17, July 2017, Washington, DC, USA
5 CONCLUSION
In this study, we introduce LLM-Embedder, a novel model designed
to enhance the retrieval augmentation of LLMs in a variety of sce-
narios. Our model integrates four key retrieval capabilities: knowl-
edge, memory, example, and tool, which boost LLMs’ performance
in dealing with knowledge-intensive tasks, long-context modeling,
in-context learning, and tool learning. To optimize LLM-Embedder’s
performance in such diverse scenarios, we’ve refined our train-
ing workflow from multiple aspects, including reward from LLM,
homogeneous negative sampling, instruction based fine-tuning,
and stabilized distillation. Our experiments show LLM-Embedder’s
empirical advantages over both general and task-specific embed-
ding models, which highlights its effectiveness as a foundational
building-block to support the retrieval augmentation of LLMs.



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 2):

Retrieve Anything To Augment Large Language Models Conference’17, July 2017, Washington, DC, USA
the model’s information seeking capability in the conversational
context. 3) Tool Learning. The ToolLLM dataset [ 62] is used to
learn the selection of appropriate tools in the tool-using context.
4) Instruction Tuning: To retrieve useful demonstration examples
for in-context learning, we re-purpose FLAN [86] and UPRISE [18],
which are originally designed for instruction tuning. 5) Genera-
tion. The model is trained to extract valuable historical information
(i.e. memory) based on a long conversation dataset: Multi-Session
Chat [93], as well as long-range language modeling datasets: in-
cluding Books3 [25], ArXiv [25], CodeParrot [79]. These datasets
can be grouped into two types based on the availability of labels.
•Labeled data. The datasets on the first three types of tasks are
composed of pairwise texts, where hard-coded labels are presented.
For question answering datasets (MSMARCO, NQ), each data in-
stance consists of a query and the source passage of answer, denoted
as <query, passage>. For conversational search dataset (QReCC),
each data instance is made up of a conversational query and the
source passage of answer, denoted as <conversation, passage>. For
tool learning dataset (ToolLLM), each data instance includes an
instruction and the description of the needed tool, denoted as <in-
struction, tool desc>.
•Non-labeled data. In contrast, the last two types of datasets
do not have explicit labels. For instruction tuning datasets (FLAN,
UPRISE), each instance consists of human’s instruction and the ex-
pected output: <instruction, output>. For generation datasets, each
instance is a long text sequence partitioned into chunks: [chunk_0,
..., chunk_L]. Books3, ArXiv, and CodeParrot are made up of plain
texts, which are chunked into spans of equal length (128 tokens per
chunk). Multi-Session Chat is composed of conversations, where
each chunk corresponds to a pair of consecutive utterances.
2.2 Training Methodology
2.2.1 Formulation of Training Reward. In our work, we explore
two types of supervision signals for training the LLM-Embedder.
Firstly, we can directly utilize the hard labels provided by the labeled
datasets. Secondly, we aim to optimize the LLM’s final performance
with retrieval augmentation. To achieve this goal, we leverage the
reward produced by LLM for both labeled and unlabeled datasets.
Particularly, given the expected output of the LLM, denoted as𝑂,
and a retrieval candidate, denoted as𝐶, the reward for the candidate,
represented as 𝑟𝐶|𝑂, is derived by the following equation:
𝑟𝐶|𝑂 =
Ö|𝑂|
𝑖=1 LLM(𝑜𝑖|𝐶,𝑂:𝑖−1). (1)
Here, 𝑜𝑖 represents the 𝑖-th token of the expected output, and
LLM(𝑥|𝑦)stands for the LLM’s generation likelihood of producing
𝑥 given the context 𝑦. In other words, a higher reward is assigned
to a retrieval candidate if it results in a higher generation likelihood
for the expected output.
The LLM based reward is applied in the following ways for
each of the tasks in consideration. 1) For Question Answering:
the reward is computed as the generation likelihood of answers
given one single candidate passage. 2) For Instruction Tuning: The
reward is computed as the generation likelihood of the instructed
output given one candidate example. 3) For Generation: the reward
is computed as the generation likelihood of a new content given
one candidate historical chunk. Note that the LLM reward is not
applied to conversational search and tool learning datasets, as there
is no clear expectation of the LLM’s output in these cases.
Given the two sources of supervision signals of LLM-Embedder,
i.e. the native hard labels and the soft reward derived from LLM,
the training is conducted with a composite recipe. The contrastive
learning is applied to capture the semantic relationship reflected
by the hard labels; meanwhile, the knowledge distillation is used
to learn from the soft rewards derived from LLM.
2.2.2 Contrastive Learning. For each pair of hard-labeled texts: 𝑞
and 𝑝 (e.g., query and passage), the loss function of contrastive
learning is formulated in the following way:
min .
∑︁
(𝑞,𝑝)
−log exp(⟨𝒆𝑞,𝒆𝑝⟩/𝜏)Í
𝑝′∈Pexp(⟨𝒆𝑞,𝒆𝑝′⟩/𝜏), (2)
where 𝒆∗stands for the embedding, ⟨·⟩indicates the inner product
operator, Pare the union of positive and negative samples,𝜏 refers
to the temperature. To improve the discriminative power of embed-
dings across diverse application scenarios, we employ a couple of
key designs in our contrastive learning framework.
The first featured design is theInstruction-based Fine-Tuning.
In this approach, each task is assigned with a unique task instruction
denoted as 𝐼𝑡. While generating the query-side embedding, the task
instruction and query content are concatenated and jointly encoded,
resulting in the update of query embedding: 𝒆𝑞 ←encode([𝐼𝑡,𝑞]).
This task-specific instructions plays a pivotal role in initializing the
embedding model with distinct activations, thereby facilitating the
discrimination between different tasks.
The second notable feature is theHomogeneous In-Batch Neg-
ative Sampling. It calls for a considerable amount of negative sam-
ples to guarantee the embedding’s discriminativeness [30, 63, 82].
In our work, this is realized by the joint usage of in-batch negatives
and hard negatives. We also apply cross-device sharing [ 63, 91],
which further expands the scale of negative samples. Consequently,
our method results in 𝐵 ×𝐾 ×𝑁 −1 negative samples in total,
where 𝐵is the batch size, 𝐾 is the number of GPU devices, 𝑁 is the
total number of positive and hard negative samples. However, the
vanilla practice of in-batch negative sampling presents one draw-
back in our multi-task settings. Particularly, the embeddings shared
between different datasets (namely heterogenous negative samples)
are mostly irrelevant, which are less effective for discriminating the
semantic relationships within a specific task scenario. To address
this limitation, we introduce a regularization strategy for the organi-
zation of training data, where the data instances from the same task
are grouped into consecutive mini-batches. The strategy makes the
majority of in-batch negative samples to originate from the same
dataset (i.e. homogeneous negative samples), thus enhancing the
discriminative power of embeddings for each specific task.
2.2.3 Knowledge Distillation. In our training framework, knowl-
edge distillation plays a crucial role in learning from the LLM’s
reward. we employ the KL-divergence to minimize the gap between
the distributions of candidates computed using LLM’s rewards and
those predicted by the embedding model. In particular, for each
query 𝑞and its candidate list P: [𝑝1, ..., 𝑝𝑁], we derive the LLM’s
rewards towards the candidates, denoted as 𝑅: [𝑟1, ..., 𝑟𝑁], using



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 7):

Conference’17, July 2017, Washington, DC, USA Zhang and Xiao, et al.
Table 4: Ablation study for the three influential factors about LLM-Embedder’s training: using soft reward from LLM, stabilized
distillation, instruction based fine-tuning, in-batch negative sampling from the same scenario.
Knowledge ICL Long Tool Conv Search
Method MMLU PopQA Misc. MSC ArXiv ToolLLM QReCC
w.o. LLM Reward 0.4872 0.4794 0.6217 13.9176 3.2495 0.8927 0.4945
w.o. Instruction FT 0.4776 0.5025 0.6211 13.9125 3.2383 0.8192 0.5029
w.o. homo NS 0.4791 0.4520 0.6200 14.0441 3.2558 0.8364 0.4563
w.o. Stablized Distill 0.4815 0.5027 0.6105 13.6090 3.2441 0.7905 0.4865
AAR 0.4826 0.4792 0.5938 14.6999 3.3260 0.4200 0.2877
API-Retriever 0.4625 0.2488 0.5942 14.7834 3.3858 0.8017 0.1137
LLM-R 0.4625 0.2506 0.6262 14.4746 3.3635 0.1321 0.0234
LLM-Embedder 0.4903 0.5052 0.6268 13.4832 3.2322 0.8645 0.5053
the embeddings, because a great portion of the negative samples
will come from different tasks, which are irrelevant with each other.
As we can observe, LLM-Embedder’s performance is decreased due
to such a change, especially for PopQA and Conv Search, where a
massive candidate pool is presented (Wikipedia corpus).
For “w.o. stabilized distill ”, we replace our stabilized distilla-
tion with the conventional KL-divergence based method. As intro-
duced, this operation handles the fluctuated reward from LLM such
that distillation can become more stabilized. We can observe that
LLM-Embedder’s performance is reduced once this step is removed,
especially for ICL where LLM’s reward is the major training signal.
4 RELATED WORKS
The related works are reviewed from two perspectives: retrieval
augmented large language models, and dense retrieval.
•Retrieval Augmented LLMs . Large language models (LLMs)
are praised for their unprecedented capability on language under-
standing and generation. Compared with the conventional methods,
LLMs exhibit overwhelming generality and notable advantages on
typical NLP tasks [17, 19, 78]. Despite such superiority, LLMs still
face a series of severe challenges, such as hallucination, human
alignment, and long-term memory. Many of the existing problems
are caused by the inherent boundaries, which cannot be addressed
by LLMs alone, but to rely on support from the external world. The
retrieval-augmented LLMs are regarded as a go-to option to bridge
LLMs with the external assistance [4, 51]. For the past few years,
they have been widely applied to several critical scenarios. One com-
mon case is the knowledge enhancement. The internal knowledge
of LLMs can be incomplete, static, and limited by the popularity
bias. When dealing with knowledge intensive tasks, the retrieval
augmented LLMs will look for necessary information from an ex-
ternal database, where the generated content can be grounded on
proper knowledge [15, 31, 32, 41]. Besides, the retrieval augmented
LLMs are also used to retrieve historical context to establish long-
term memory [71, 85], retrieve examples to improve the instruction
following capability [18, 83], and retrieve tools to engage with the
physical world [62].
The retrieval augmented LLMs consist of two basic parts: gener-
ator and retriever. According to previous studies [32, 41, 83, 96], the
retrieval augmentation effect is highly influenced by the retrieved
content. In practice, there are two common types of retrievers. One
is to leverage the general purpose retrievers, such as sparse models
like BM25 [69], and dense models, like DPR [37], contriever [30],
E5 [81], BGE [89], OpenAI text embedding [56]. The other option
is develop task-specific retriever, e.g., AAR for knowledge enhance-
ment [96], LLM-R [85] for in-context learning. The general pur-
pose methods are praised for their generality and simplicity for
usage, but may suffer from an inferior retrieval quality. In contrast,
the task-specific ones can better fit one scenario, but fall short in
transferability. Compared with the existing works, LLM-Embedder
unifies the generality and speciality: it comprehensive supports all
major retrieval augmentation needs of LLMs, meanwhile achieving
the leading performance in every application scenario.
•Dense retrieval. Dense retrieval leverages latent representa-
tion of texts, i.e. embeddings, to search for relevant information
from a vector DB. In recent years, it has grown into a major para-
digm of information retrieval. The success of dense retrieval can
attribute to several reasons. The first and foremost driving force is
the development of pre-trained language models[22, 45, 65], where
the textual data can be represented in a highly expressive man-
ner. The general pre-trained models are further improved by the
retrieval-oriented ones [46, 81], which better establish the sentence-
level representation capability during the pre-training stage. The
second factor is the advancement of contrastive learning. On one
hand, there has been a major upgrade of negative sampling, where
massive [30, 37] and sufficiently hard samples [92] are utilized to
help with the embedding’s discriminativeness. On the other hand,
the training objective is improved as well. Instead of simply learning
from hard labels, the embedding models are made to distill knowl-
edge from a more precise ranking model [29, 63, 90]. This notably
facilitates the embedding model to encode fine-grained semantic
relationships. Thirdly, the generality becomes increasingly empha-
sized in these days, where embeddings need to handle a wide variety
of application scenarios. For this purpose, people come up with
many different strategies, e.g., data augmentation [42, 80], domain
adaptation [36, 95], instruction-based fine-tuning [ 5, 76], which
help the model to better handle diverse tasks. These factors are
incorporated and optimized while developing our training recipe,
which results in the empirical competitiveness of LLM-Embedder.



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 3):

Conference’17, July 2017, Washington, DC, USA Zhang and Xiao, et al.
Eq 1. To make the LLM’s rewards suitable for distillation, we trans-
form each reward into a normalized weight:𝑤𝑖 ←softmax𝑅(𝑟𝑖/𝛼),
where 𝛼 represents the temperature. On top of these elements, the
KL divergence is computed by the following equation:
min .
∑︁
P
−𝑤𝑖 ∗log exp(⟨𝒆𝑞,𝒆𝑝⟩/𝜏)Í
𝑝′∈Pexp(⟨𝒆𝑞,𝒆𝑝′⟩/𝜏). (3)
While the above formulation has been successfully employed in
mono-task settings [29, 46, 81], applying it directly to our multi-task
scenario poses unique challenges. Notably, the magnitude of LLM’s
rewards can exhibit high fluctuations due to the diverse training
samples from various tasks. In many cases, the LLM’s rewards
closely distribute, making it challenging to distinguish the quality
of candidates. In contrast, in many other cases, the rewards become
polarized, with candidates receiving either a positive reward or
nearly zero rewards. Both of these scenarios contribute little to the
distillation process and can severely impair the training effect.
•Stabilized Distillation . To address the challenge of fluctu-
ated rewards in our multi-task scenario, we introduce a modified
formulation of the loss function. This adaptation effectively alle-
viates the negative impact resulted from the rewards’ fluctuations.
Particularly, instead of using LLM rewards solely as “soft weights”,
we also leverage them as hard ranking labels. Given LLMs’ rewards
𝑅: [𝑟1, ..., 𝑟𝑁], we re-rank the candidates in a top-down order. This
operation results in a new order for the candidates, denoted as
P: [𝑝1, ..., 𝑝𝑁], where 𝑟𝑖 ≥𝑟𝑖+1. The loss function for knowledge
distillation is accordingly transformed as follows:
min .
∑︁
𝑃 −𝑤𝑖 ∗log exp(⟨e𝑞,e𝑝𝑖 ⟩/𝜏)Í
𝑝′∈P exp(⟨e𝑞,e𝑝′⟩/𝜏).
Here, P comprises two sources: the lower-ranked candidates of 𝑝𝑖:
[𝑝𝑖+1, ..., 𝑝𝑁]; and the the in-batch negative samples.
Our adapted formulation serves to stabilize fluctuated rewards
in two fundamental ways. On one hand, the model is consistently
trained to promote 𝑝𝑖 compared to its lower-ranked counterparts
[𝑝𝑖+1, ...,𝑝𝑁]. This means that the model is always able to learn from
the LLMs’ preferences, regardless of the absolute value of rewards.
This mechanism is particularly effective in handling cases where
LLMs’ rewards are too closely distributed. On the other hand, when
the top-ranked candidate receives a significantly higher reward
compared to the other candidates, the weights will become one-hot.
In this scenario, the distillation process will be reduced to the form
of contrastive learning, with the top-ranked candidate treated as
a positive sample. This mechanism help to address the situations
where polarized rewards are generated by LLMs.
2.3 Retrieval Augmentation of LLMs
The multi-tasking capacity of LLM-Embedder makes it as a versatile
solution. By connecting to the vector DB where any needed external
elements are stored, it may support a wide variety of retrieval
augmentation tasks. In this place, we discuss the typical scenarios
empowered by LLM-Embedder (Figure 2), with focusing on three
key issues: 1) what to store in the vector DB, 2) what is used to
query the vector DB, 3) how to leverage the retrieved data.
•Knowledge Enhancement. When handling knowledge in-
tensive tasks [37, 59], the entire docs from the knowledge corpus
can be encoded and stored in vector DB. In many cases, questions
Long Memory
LLM-Embedder
DocsChunksExamplesTools
Vector DB
Knowledge enhanced In-Context LearningTool Augmented
LLM
Figure 2: Retrieval augmentation with LLM-Embedder.
are explicitly presented, which can be used to query the vector DB.
In other cases, the working context during the generation process
can be used as query [ 27, 34]. The retrieved docs can be directly
applied or refined for more informative segments [44]. Finally, the
query and retrieved docs are concatenated to generate knowledge-
grounded answer, e.g., [knowledge, query] →answer.
•Long-Context Modeling. When dealing with a long context,
the entire history can be chunked, encoded, and off-loaded to the
vector database. The working context during the generation process
can be used to query the vector DB for relevant chunks. In many
cases, both the relevant chunk, e.g., chunk_𝑖, and its subsequent
chunk_𝑖+1 are used for memory augmentation [15], because the
subsequent chunk can be more critical to the future generation. The
retrieved chunks are used to back-fill the current context, where
new content can be generated with remote but important memory,
e.g., [retrieved chunks, current context] →new generation.
•In-context Learning. The demonstration examples, organized
in the form of “(task instruction, expected output)”, can be encoded
and pre-stocked in vector DB. When a new task is given, the task’s
instruction is used to query the vector DB [18, 83]. The retrieved
examples are concatenated with the task’s instruction, based on
which the in-context learning can be conducted, e.g., [retrieved
examples, instruction] →task completion.
•Tool Learning. The tool’s functionality can be verbalized as
a description, and paired with its API: “(description, API)”. In this
way, a massive toolkit can be managed by vector DB based on the
encoded description [62]. Given a user request that involves the
use of tools, the user request can be encoded and used to query
the vector DB. The retrieved tool is executed via its API, where the
execution result is returned for LLM to complete the remaining
generation: [user request, tool’s execution result] →generation.
3 EXPERIMENT
The experimental study is to clarify three basic research questions.
RQ 1. can LLM-Embedder comprehensively support the diverse
scenarios of LLM’s retrieval augmentation. RQ 2. what is LLM-
Embedder’s impact to each specific scenario.RQ 3. what are the key
factors influencing the empirical performance of LLM-Embedder.



### Claim 40/179

#### Claim Text
REPLUG [72] utilizes a retriever and an LLM to calculate the probability distributions of the retrieved documents and then performs supervised training by computing the KL divergence.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[72]_2301.12652.pdf (Page 2):

REPLUG : Retrieval-Augmented Black-Box Language Models
Figure 2.REPLUG at inference (§3). Given an input context, REPLUG first retrieves a small set of relevant documents from an external
corpus using a retriever (§3.1 Document Retrieval). Then it prepends each document separately to the input context and ensembles output
probabilities from different passes (§3.2 Input Reformulation).
and a training scheme to further adapt the retriever to large
LMs.
3. REPLUG
We introduce REPLUG (Retrieve and Plug), a new retrieval-
augmented LM paradigm where the language model is
treated as black box and the retrieval component is added as
a potentially tuneable module.
As shown in Figure 2, given an input context, REPLUG first
retrieves a small set of relevant documents from an external
corpus using a retriever (§3.1). Then we pass the concate-
nation of each retrieved document with the input context
through the LM in parallel, and ensemble the predicted
probabilities (§3.2).
3.1. Document Retrieval
Given an input context x, the retriever aims to retrieve a
small set of documents from a corpus D = {d1...dm} that
are relevant to x. Following prior work (Qu et al., 2021;
Izacard & Grave, 2021b; Ni et al., 2021), we use a dense
retriever based on the dual encoder architecture, where an
encoder is used to encode both the input context x and the
document d. Specifically, the encoder maps each document
d ∈ D to an embedding E(d) by taking the mean pooling of
the last hidden representation over the tokens in d. At query
time, the same encoder is applied to the input context x to
obtain a query embedding E(x). The similarity between the
query embedding and the document embedding is computed
by their cosine similarity:
s(d, x) = cos(E(d), E(x)) (1)
The top-k documents that have the highest similarity scores
when compared with the input x are retrieved in this step.
For efficient retrieval, we precompute the embedding of
each document d ∈ D and construct FAISS index (Johnson
et al., 2019) over these embeddings.
3.2. Input Reformulation
The retrieved top- k documents provide rich information
about the original input context x and can potentially help
the LM to make a better prediction. One simple way to
incorporate the retrieved documents as part of the input to
the LM is to prepend x with all k documents. However, this
simple scheme is fundamentally restricted by the number
of documents (i.e., k) we can include, given the language
model’s context window size. To address this limitation, we
adopt an ensemble strategy described as follows. Assume
D′ ⊂ Dconsists of k most relevant documents to x, ac-
cording to the scoring function in Eq. (1). We prepend each
document d ∈ D′ to x, pass this concatenation to the LM
separately, and then ensemble output probabilities from all
k passes. Formally, given the input context x and its top-k
relevant documents D′, the output probability of the next
token y is computed as a weighted average ensemble:
p(y | x, D′) =
X
d∈D′
p(y | d ◦ x) · λ(d, x),
where ◦ denotes the concatenation of two sequences and the
weight λ(d, x) is based on the similarity score between the
document d and the input context x:
λ(d, x) = es(d,x)
P
d∈D′ es(d,x)
Although our ensemble method requires running the LM
k times, the cross attention is performed between each re-
trieved document and the input context. Therefore, com-
pared with the method of prepending all the retrieved docu-



Source: data\tc16_2312.10997v5\referenced_papers\[72]_2301.12652.pdf (Page 3):

REPLUG : Retrieval-Augmented Black-Box Language Models
ments, our ensemble methods do not incur additional com-
putational cost overhead.
4. REPLUG LSR: Training the Dense
Retriever
Instead of relying only on existing neural dense retrieval
models (Karpukhin et al., 2020a; Izacard et al., 2022a; Su
et al., 2022), we further propose REPLUG LSR (REPLUG
with LM-Supervised Retrieval), which adapts the retriever
in REPLUG by using the LM itself to provide supervision
about which documents should be retrieved.
Inspired by Sachan et al. (2022), our approach can be seen
as adjusting the probabilities of the retrieved documents
to match the probabilities of the output sequence perplexi-
ties of the language model. In other words, we would like
the retriever to find documents that result in lower perplex-
ity scores. As shown in Figure 3, our training algorithm
consists of the four steps: (1) retrieving documents and
computing the retrieval likelihood (§4.1), (2) scoring the
retrieved documents by the language model (§4.2), (3) up-
dating the retrieval model parameters by minimizing the KL
divergence between the retrieval likelihood and the LM’s
score distribution (§4.3), and (4) asynchronous update of
the datastore index (§4.4).
4.1. Computing Retrieval Likelihood
We retrieve k documents D′ ⊂ Dwith the highest simi-
larity scores from a corpus D given an input context x, as
described in §3.1. We then compute the retrieval likelihood
of each retrieved document d:
PR(d | x) = es(d,x)/γ
P
d∈D′ es(d,x)/γ
where γ is a hyperparameter that controls the temerature of
the softmax. Ideally, the retrieval likelihood is computed by
marginalizing over all the documents in the corpusD, which
is intractable in practice. Therefore, we approximate the
retrieval likelihood by only marginalizing over the retrieved
documents D′.
4.2. Computing LM likelihood
We use the LM as a scoring function to measure how much
each document could improve the LM perplexity. Specifi-
cally, we first compute PLM (y | d, x), the LM probability
of the ground truth output y given the input context x and
a document d. The higher the probability, the better the
document di is at improving the LM’s perplexity. We then
compute the LM likelihood of each document d as follows:
Q(d | x, y) = ePLM(y|d,x)/β
P
d∈D′ ePLM(y|d,x)/β
where β is another hyperparameter.
4.3. Loss Function
Given the input context x and the corresponding ground
truth continuation y, we compute the retrieval likelihood
and the language model likelihood. The dense retriever is
trained by minimizing the KL divergence between these two
distributions:
L = 1
|B|
X
x∈B
KL

PR(d | x) ∥ QLM(d | x, y)

,
where B is a set of input contexts. When minimizing the
loss, we can only update the retrieval model parameters. The
LM parameters are fixed due to our black-box assumption.
4.4. Asynchronous Update of the Datastore Index
Because the parameters in the retriever are updated during
the training process, the previously computed document em-
beddings are no longer up to date. Therefore, following Guu
et al. (2020), we recompute the document embeddings and
rebuild the efficient search index using the new embeddings
every T training steps. Then we use the new document
embeddings and index for retrieval, and repeat the training
procedure.
5. Training Setup
In this section, we describe the details of our training proce-
dure. We first describe the model setting in REPLUG (§5.1)
and then describe the procedure for training the retriever in
REPLUG LSR (§5.2).
5.1. REPLUG
In theory, any type of retriever, either dense (Karpukhin
et al., 2020b; Ni et al., 2021) or sparse (Robertson et al.,
2009), could be used for REPLUG . Following prior
work (Izacard et al., 2022b), we use the Contriever (Izacard
et al., 2022a) as the retrieval model for REPLUG , as it has
demonstrated strong performance.
5.2. REPLUG LSR
For REPLUG LSR , we initialize the retriever with the
Contriever model (Izacard et al., 2022a). We use GPT-3
Curie (Brown et al., 2020b) as the supervision LM to com-
pute the LM likelihood.
Training data We use 800K sequences of 256 tokens
each, sampled from the Pile training data (Gao et al., 2020),
as our training queries. Each query is split into two parts:
the first 128 tokens are used as the input context x, and the
last 128 tokens are used as the ground truth continuation
y. For the external corpus D, we sample 36M documents



Source: data\tc16_2312.10997v5\referenced_papers\[72]_2301.12652.pdf (Page 0):

REPLUG : Retrieval-Augmented Black-Box Language Models
Weijia Shi,1 * Sewon Min,1 Michihiro Yasunaga,2 Minjoon Seo,3 Rich James,4 Mike Lewis,4
Luke Zettlemoyer1 4 Wen-tau Yih4
Abstract
We introduce REPLUG , a retrieval-augmented lan-
guage modeling framework that treats the lan-
guage model (LM) as a black box and augments
it with a tuneable retrieval model. Unlike prior
retrieval-augmented LMs that train language mod-
els with special cross attention mechanisms to en-
code the retrieved text, REPLUG simply prepends
retrieved documents to the input for the frozen
black-box LM. This simple design can be eas-
ily applied to any existing retrieval and language
models. Furthermore, we show that the LM can
be used to supervise the retrieval model, which
can then find documents that help the LM make
better predictions. Our experiments demonstrate
that REPLUG with the tuned retriever significantly
improves the performance of GPT-3 (175B) on
language modeling by 6.3%, as well as the perfor-
mance of Codex on five-shot MMLU by 5.1%.
1. Introduction
Large language models (LLMs) such as GPT-3 (Brown et al.,
2020a) and Codex (Chen et al., 2021a), have demonstrated
impressive performance on a wide range of language tasks.
These models are typically trained on very large datasets and
store a substantial amount of world or domain knowledge
implicitly in their parameters. However, they are also prone
to hallucination and cannot represent the full long tail of
knowledge from the training corpus. Retrieval-augmented
language models (Khandelwal et al., 2020; Borgeaud et al.,
2022; Izacard et al., 2022b; Yasunaga et al., 2022), in con-
trast, can retrieve knowledge from an external datastore
when needed, potentially reducing hallucination and increas-
ing coverage. Previous approaches of retrieval-augmented
language models require access to the internal LM repre-
sentations (e.g., to train the model (Borgeaud et al., 2022;
1University of Washington2Stanford University3KAIST 4Meta
AI.
* Work done while the first author was interning at Meta AI.
Correspondence to: Weijia Shi <swj0419@uw.edu>.
Figure 1. Different from previous retrieval-augmented ap-
proaches (Borgeaud et al., 2022) that enhance a language model
with retrieval by updating the LM’s parameters, REPLUG treats
the language model as a black box and augments it with a frozen
or tunable retriever. This black-box assumption makes REPLUG
applicable to large LMs (i.e., >100B parameters), which are often
served via APIs.
Izacard et al., 2022b) or to index the datastore (Khandelwal
et al., 2020)), and are thus difficult to be applied to very
large LMs. In addition, many best-in-class LLMs can only
be accessed through APIs. Internal representations of such
models are not exposed and fine-tuning is not supported.
In this work, we introduce REPLUG (Retrieve and Plug),
a new retrieval-augmented LM framework where the lan-
guage model is viewed as a black box and the retrieval
component is added as a tuneable plug-and-play module.
Given an input context, REPLUG first retrieves relevant
documents from an external corpus using an off-the-shelf
retrieval model. The retrieved documents are prepended to
the input context and fed into the black-box LM to make
the final prediction. Because the LM context length limits
the number of documents that can be prepended, we also
introduce a new ensemble scheme that encodes the retrieved
documents in parallel with the same black-box LM, allow-
ing us to easily trade compute for accuracy. As shown in
arXiv:2301.12652v4  [cs.CL]  24 May 2023



Source: data\tc16_2312.10997v5\referenced_papers\[72]_2301.12652.pdf (Page 4):

REPLUG : Retrieval-Augmented Black-Box Language Models
Figure 3.REPLUG LSR training process (§4). The retriever is trained using the output of a frozen language model as supervision
signals.
of 128 tokens from the Pile training data. To avoid trivial
retrieval, we ensure that the external corpus documents do
not overlap with the documents from which the training
queries are sampled.
Training details To make the training process more ef-
ficient, we pre-compute the document embeddings of the
external corpus D and create a FAISS index (Johnson et al.,
2019) for fast similarity search. Given a queryx, we retrieve
the top 20 documents from the FAISS index and compute
the retrieval likelihood and the LM likelihood with a tem-
perature of 0.1. We train the retriever using the Adam opti-
mizer (Kingma & Ba, 2015) with a learning rate of 2e-5, a
batch size of 64, and a warmup ratio of 0.1. We re-compute
the document embeddings every 3k steps and fine-tune the
retriever for a total of 25k steps.
6. Experiments
We perform evaluations on both language modeling (§6.1)
and downstream tasks such as MMLU (§6.2) and open-
domain QA (§6.3). In all settings, REPLUG ˜improve the
performance of various black-box language models, show-
ing the effectiveness and generality of our approach.
6.1. Language Modeling
Datasets The Pile (Gao et al., 2020) is a language mod-
eling benchmark that consists of text sources from diverse
domains such as web pages, code and academic papers. Fol-
lowing prior work, we report bits per UTF-8 encoded byte
(BPB) as the metric on each subset domain.
Baselines We consider GPT-3 and GPT-2 family language
model as the baselines. The four models from GPT-3
(Davinci, Curie, Baddage and Ada) are black-box models
that are only accessible through API
Our model We add REPLUG and REPLUG LSR to the
baselines. We randomly subsampled Pile training data
(367M documents of 128 tokens) and use them as the re-
trieval corpus for all models. As the Pile dataset has made
efforts to deduplicate documents across train, validation and
test splits (Gao et al., 2020), we did not do additional filter-
ing. For both REPLUG and REPLUG LSR, we use a length
of 128-token context to do retrieval and adopt the ensem-
ble method (Section 3.2) to incorporate top 10 retrieved
documents during inference.
Results Table 1 reports the results of the original base-
lines, baselines augmented with the REPLUG , and baselines
augmented with the REPLUG LSR . We observe that both
REPLUG and REPLUG LSR significantly outperform the
baselines. This demonstrates that simply adding a retrieval
module to a frozen language model (i.e., the black-box set-
ting) is effective at improving the performance of different
sized language models on language modeling tasks. Fur-
thermore, REPLUG LSR consistently performs better than
REPLUG by a large margin. Specifically, REPLUG LSR
results in 7.7% improvement over baselines compared to
4.7% improvement of REPLUG averaged over the 8 models.
This indicates that further adapting the retriever to the target
LM is beneficial.
6.2. MMLU
Datasets Massive Multi-task Language Understanding
(MMLU (Hendrycks et al., 2021)) is a multiple choice QA
dataset that covers exam questions from 57 tasks includ-
ing mathematics, computer science, law, US history and
etc. The 57 tasks are grouped into 4 categories: humani-
ties, STEM, social sciences and other. Following Chung



Source: data\tc16_2312.10997v5\referenced_papers\[72]_2301.12652.pdf (Page 7):

REPLUG : Retrieval-Augmented Black-Box Language Models
Perplexity
14.00
16.60
19.20
21.80
24.40
27.00
Parameters (Million)
100 475 850 1225 1600
Original
 + RE-PLUG
GPT-2
(a)
Perplexity
11.00
13.60
16.20
18.80
21.40
24.00
Parameters (Million)
100 2075 4050 6025 8000
Original
 + RE-PLUG
BLOOM (b)
Perplexity
9.00
13.00
17.00
21.00
25.00
29.00
Parameters (Million)
100 1000 10000 100000 1000000
Original
 + RE-PLUG
OPT (c)
Figure 5. GPT-2, BLOOM and OPT models of varying sizes consistently benefit from REPLUG . The x-axis indicates the size of the
language model and the y-axis is its perplexity on Wikitext-103.
7.2. REPLUG is applicable to diverse language models
Here we further study whether REPLUG could enhance di-
verse language model families that have been pre-trained
using different data and methods. Specifically, we focus on
three groups of language models with varying sizes: GPT-
2 (117M, 345M, 774M, 1.5B parameters) (Brown et al.,
2020a), OPT (125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B,
66B) (Zhang et al., 2022b) and BLOOM (560M, 1.1B, 1.7B,
3B and 7B) (Scao et al., 2022). We evaluate each model on
Wikitext-103 (Stephen et al., 2017) test data and report its
perplexity. For comparison, we augment each model with
REPLUG that adopts the ensemble method to incorporate
top 10 retrieved documents. Following prior work (Khan-
delwal et al., 2020), we use Wikitext-103 training data as
the retrieval corpus.
Figure 5 shows the performance of different-sized language
models with and without REPLUG . We observe that the
performance gain brought by REPLUG stays consistent with
model size. For example, OPT with 125M parameters
achieves 6.9% perplexity improvement, while OPT with
66B parameters achieves 5.6% perplexity improvement. Ad-
ditionally, REPLUG improves the perplexity of all the model
families. This indicates that REPLUG is applicable to di-
verse language models with different sizes.
7.3. Qualitative Analysis: rare entities benefit from
retrieval
To understand why the REPLUG improves language model-
ing performance, we conducted manual analysis of exam-
ples in which the REPLUG results in a decrease in perplexity.
We find that REPLUG is more helpful when texts contain
rare entities. Figure 6 shows a test context and its contin-
uation from the Wikitext-103 test set. For REPLUG , we
use the test context as a query to retrieve a relevant docu-
ment from Wikitext-103 training data. We then compute
the perplexity of the continuation using the original GPT-2
1.5B and its REPLUG enhanced version. After incorporating
the retrieved document, the perplexity of the continuation
improves by 11%. Among all tokens in the continuation, we
found that REPLUG is most helpful for the rare entity name
"Li Bai". This is likely because the original LM does not
have sufficient information about this rare entity name. How-
ever, by incorporating the retrieved document,REPLUG was
able to match the name with the relevant information in the
retrieved document, resulting in better performance.
Figure 6.Rare entities benefit from retrieval. After incorporat-
ing the retrieved document during inference, the entity " Li Bai"
and the token " greatest" in the continuation show the most im-
provement in perplexity (15% for "Li Bai" and 5% for "greatest").
Other tokens’ perplexity changes are within 5%.
8. Conclusion
We introduce REPLUG , a retrieval-augmented language
modeling paradigm that treats the language model as a
black box and augments it with a tuneable retrieval model.
Our evaluation shows that REPLUG can be integrated with
any existing language model to improve their performance



### Claim 41/179

#### Claim Text
To optimize the multi-task capabilities of LLM, UPRISE [20] trained a lightweight prompt retriever that can automatically retrieve prompts from a pre-built prompt pool that are suitable for a given zero-shot task input.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[20]_2303.08518.pdf (Page 0):

UPRISE : Universal Prompt Retrieval for Improving Zero-Shot Evaluation
Daixuan Cheng Shaohan Huang ∗ Junyu Bi Yuefeng Zhan Jianfeng Liu
Yujing Wang Hao Sun Furu Wei Denvy Deng Qi Zhang
Microsoft
daixuancheng6@gmail.com bijunyu21@mails.ucas.ac.cn
{shaohanh, yuefzh, jianfengliu, yujing.wang, hasun, fuwei, dedeng, qizhang}@microsoft.com
Abstract
Large Language Models (LLMs) are popu-
lar for their impressive abilities, but the need
for model-specific fine-tuning or task-specific
prompt engineering can hinder their generaliza-
tion. We propose UPRISE (Universal Prompt
Retrieval for Improving zero-Shot Evaluation),
which tunes a lightweight and versatile re-
triever that automatically retrieves prompts
for a given zero-shot task input. Specifi-
cally, we demonstrate universality in a cross-
task and cross-model scenario: the retriever
is tuned on diverse tasks, but tested on un-
seen task types; we use a small frozen LLM,
GPT-Neo-2.7B, for tuning the retriever,
but test the retriever on different LLMs of
much larger scales, such as BLOOM-7.1B,
OPT-66B and GPT3-175B. Additionally, we
show that UPRISE mitigates the hallucination
problem in our experiments with ChatGPT,
suggesting its potential to improve even the
strongest LLMs. Our model and code are avail-
able at https://github.com/microsoft/LMOps.
1 Introduction
Large Language Models (LLMs) such as
GPT-3 (Brown et al., 2020), OPT (Zhang et al.,
2022), and BLOOM (Scao et al., 2022) have
shown impressive capabilities across a wide range
of tasks. Recent research proposes two main
approaches to further improve their performance:
fine-tuning LLMs to follow prompts (Hu et al.,
2022; Houlsby et al., 2019; Zaken et al., 2022; Wei
et al., 2022a; Sanh et al., 2022) and developing
prompt engineering techniques to guide the
LLMs (Brown et al., 2020; Wei et al., 2022b; Liu
et al., 2021; Lester et al., 2021).
Fine-tuning LLMs adjusts their weights to fit spe-
cific prompts. However, this can be constrained by
computational limitations or the unavailability of
model weights (Hu et al., 2022). Multi-task tuning
∗Corresponding author
BLOOM-7.1B
GPT3-175BOPT-66B
🧊
🔥Prompt RetrieverTune
GPT-Neo-2.7B
Read. Compre.Close. QAParaphrase
Cross-Model
NLI SentimentFact-Check.Cross-Task
InferenceTrain
Figure 1: UPRISE tunes a prompt retriever on multi-
ple tasks with a small LLM, but conducts inference on
unseen task types with a different larger LLM.
provides an alternative approach to improve zero-
shot task generalization (Wei et al., 2022a; Sanh
et al., 2022), which partially justifies the tuning
cost. Yet, the constant evolution of LLMs creates
the need for tuning new models, making the cumu-
lative fine-tuning cost a big concern.
Prompt engineering constructs prompts to guide
frozen LLMs. Prompt design adds an engineered
natural language prompt to teach the LLM to learn
in context (Brown et al., 2020) or to induce the
LLM to reason (Wei et al., 2022b). Prompt tun-
ing adds a soft prompt represented by continuous
parameters, and optimizes it through gradient prop-
agation (Liu et al., 2021; Li and Liang, 2021; Lester
et al., 2021). While these methods can improve per-
formance for specific tasks, it is uncertain whether
the prompts designed for one task can generalize
to unseen task types, as prompt designers are blind
in strict zero-shot settings (van de Kar et al., 2022).
In this paper, we propose UPRISE (Universal
Prompt Retrieval for Improving Zero- Shot
Evaluation), which tunes a lightweight and ver-
satile retriever that automatically retrieves prompts
from a pre-constructed pool, given a zero-shot
task input. As illustrated in Figure 1, the re-
triever is trained to retrieve prompts for mul-
tiple tasks, enabling it to generalize to un-
arXiv:2303.08518v4  [cs.CL]  16 Dec 2023



Source: data\tc16_2312.10997v5\referenced_papers\[20]_2303.08518.pdf (Page 8):

Method 0-SHOT UPRISE FEW-SHOT UPRISE-REMAIN-TARGET UPRISE-ALL-TARGET
# Demos 0 3 3 3 3
Training Data - Remaining Task Types - Remaining Task Types All Task Types
Prompt Pool - Remaining Task Types Target Task Target Task Target Task
Read. 31.6 40.1 37.4 48.8 47.4
Close-QA 19.2 23.3 25.1 28.1 28.9
Paraphrase 47.0 61.6 59.1 61.9 73.4
NLI 38.3 43.0 43.4 52.1 72.4
Sentiment 62.7 61.8 72.7 68.7 82.9
Table 4: Comparative results with few-shot prompting. # Demos is the number of demonstrations prepended to the
input instruction, FEW-SHOT is vanilla few-shot prompting where the demonstrations are randomly sampled from
the training demonstrations of the target task (Brown et al., 2020).
Senti.NLIPara.Closed.Read.
✓✓✓✓-Read.
✓✓✓-✓Closed.
✓✘-✓✘Para.
✘-✓✓✓NLI
-✘✓✓✓Senti.
GENERALIZABLE TO
TRAINING TASK
Figure 8: Generablizability of each task type, ✓ means
the performance of prompt retrieval is better than 0-
SHOT .
Generalizability of each task type. We then
reduce the number of trained tasks to only one to
test its generalizability. Specifically, for each task
type, we train a retriever on this type alone and
then evaluate on the remaining task types. For ex-
ample, if the retriever trained on A outperforms
0-SHOT when testing on B, we regard task type
A is generalizable to task type B . The results in
Figure 8 demonstrate that tasks with diverse ques-
tion/answer types, such as Reading Comprehension
and Closed-book QA, tend to be more generaliz-
able and can serve as representative choices for
training a universal retriever.
9 Exploration of Few-Shot Learning
We compare UPRISE with vanilla few-shot prompt-
ing and apply UPRISE to few-shot prompt retrieval
in Table 4: (1) Comparing UPRISE with FEW-
SHOT , UPRISE approaches and even outperforms
vanilla few-shot prompting on most task types;
(2) UPRISE -REMAIN -TARGET , using the retriever
trained on remaining tasks to retrieve in the target
task pool, outperforms vanilla few-shot prompt-
ing. (3) Substantial improvements are then ob-
served with UPRISE -ALL -TARGET , a unified re-
triever trained on all task types. These findings
emphasize UPRISE ’s effectiveness as a comprehen-
sive method for both zero-shot and few-shot prompt
retrieval.
10 Related Work
Our work is related to prompt engineering meth-
ods including prompt design, prompt tuning, and
prompt search. Here we discuss prompt search
that relates most closely to our work and describe
prompt design and prompt tuning in Appendix E.
Prompt search involves searching for prompts
from pre-training corpora or downstream tasks to
construct the input text (Gao et al., 2021; Liu et al.,
2022; Rubin et al., 2022; Ye et al., 2023, 2022).
To retrieve prompts for the test examples, retriev-
ers such as the sparse retriever BM25 (Robertson
and Zaragoza, 2009) and the dense retriever based
on SBERT (Reimers and Gurevych, 2019) are em-
ployed. Furthermore, methods like EPR (Rubin
et al., 2022) and CEIL (Ye et al., 2023) use the
LLM itself to score the searched prompts, thereby
eliminating the need for manual prompt engineer-
ing and ensuring prompting performance.
11 Conclusion
This paper explores training a lightweight and ver-
satile prompt retriever to improve the zero-shot per-
formance of LLMs. We investigate the retriever’s
ability to generalize from the trained task types to
unseen task types, and from a small LLM to differ-
ent LLMs of much larger scales. We hope our paper
will spur further research on developing a universal
assistant for the ever-expanding landscape of tasks
and large language models.



Source: data\tc16_2312.10997v5\referenced_papers\[20]_2303.08518.pdf (Page 1):

seen task types during inference. In addition,
we demonstrate that the cross-task capabilities
can generalize well from a small LLM to dif-
ferent LLMs of much larger scales: we use
GPT-Neo-2.7B (Black et al., 2021) to guide
the tuning of the retriever and evaluate the re-
triever’s performance on BLOOM-7.1B (Scao
et al., 2022), OPT-66B (Zhang et al., 2022),
and GPT3-175B (Brown et al., 2020). The
cross-model and cross-task generalization of UP-
RISE makes it a promising and practical solution
for real-world applications.
Furthermore, our approach demonstrates the po-
tential for enhancing even the most powerful LLMs,
as shown in our experiments with ChatGPT. De-
spite its impressive abilities, ChatGPT has been
found to struggle with serious hallucination prob-
lems, leading to responses that are factually inac-
curate (Bang et al., 2023). However, UPRISE is
able to address this issue on fact-checking tasks
by prompting the model to draw correct inferences
from its built-in knowledge.
In summary, our contributions include:
• We introduceUPRISE , a lightweight and versatile
approach to improve zero-shot performance of
LLMs in the cross-task and cross-model scenario.
• UPRISE is tuned with GPT-Neo-2.7B, but
can also benefit different LLMs of much larger
scales, such as BLOOM-7.1B, OPT-66B, and
GPT3-175B.
• Our exploration on ChatGPT demonstrates the
potential of UPRISE in improving performances
of even the strongest LLMs.
2 Problem Definition
We aim to improve zero-shot performance of LLMs
by training a prompt retriever to retrieve prompts1
for any given task input. Specifically, UPRISE de-
composes the prompting process into two steps:
retrieve then predict. Given an input x, we first
retrieve a set of positive prompts P+ from a pre-
constructed pool P:
P+ = R(x, P). (1)
Then we concatenate P+ with x to form an input
sequence for a frozen LLM, which generates a pre-
dicted output:
yP+
= LM

yP+
|P+ ⊕ x

. (2)
1"Prompt" sometimes refers to a natural language template
filled by an input example, but here it denotes the sequence
prepended to the task input.
Engineered Natural Language Prompt
🧊Language Model
TaskInput
🧊Summarizethe…
Prompt Design
Tunable SoftPrompt
🔥0.10,0.05,…
Prompt TuningRetrieved NaturalLanguage Prompt
Prompt Retrieval
🧊Language Model
🧊Language Model
🧊Answerthis…
🔥Prompt RetrieverTunable Prompt Retrieverbackward
Tune
TaskInput
TaskInput
Figure 2: Typical prompt engineering methods and
prompt retrieval. Prompt retrieval prepends a natural
language prompt to the task input and uses a frozen
LLM to evaluate the prompt’s performance. The ob-
tained evaluation is then used to tune the retriever in a
reverse manner.
Our objective is to optimize performance of yP+
to match the target y by updating the retriever R.
Figure 2 compares prompt retrieval with typical
prompt engineering methods: prompt design adds
an engineered natural language prompt (Brown
et al., 2020; Wei et al., 2022b) and prompt tun-
ing tunes a soft prompt (Liu et al., 2021; Lester
et al., 2021). In contrast, prompt retrieval tunes
a retriever to retrieve natural language prompts,
which is both interpretable and flexible. It uses
the language model itself to label each prompt in
the pool as positive/negative, and then tunes a re-
triever from this signal (Rubin et al., 2022). Such
fine-tuned prompt retrieval has demonstrated effec-
tiveness in the task-specific scenario (Rubin et al.,
2022; Ye et al., 2023): a prompt retriever is tuned
on one or multiple specific tasks using the train-
ing sets as the prompt pool. The retriever is then
evaluated on the corresponding testing sets.
Our work is to achieve universality of the prompt
retriever, which means the fine-tuned retriever can
be directly used to retrieve prompts for unseen tasks
and various inference LLMs, without the need for
further tuning. We define the universality from two
perspectives: cross-task retrieval and cross-model
retrieval.
Cross-task retrieval. Considering the diversity
of tasks in real-world applications, we propose
cross-task retrieval to retrieve for task types on
which the prompt retriever has not been trained.
We simulate this setting by evaluating the prompt
retriever on unseen task types: various tasks are



Source: data\tc16_2312.10997v5\referenced_papers\[20]_2303.08518.pdf (Page 4):

tween input xi and prompt p using inner prod-
ucts (Rubin et al., 2022).
3.4 Inference
After fine-tuning the prompt encoder, we use it
to encode the entire prompt pool with EP (·). At
inference time, for a testing task input xtest, we
compute its encoding EX(xtest) and then use maxi-
mum inner-product search over the prompt pool
to retrieve K most similar prompts, sorted by
their inner product in descending order, denoted
as P+ = (p1, ..., pK). We then concatenate the
prompts with the task input, resulting in the con-
catenation pK ⊕...⊕p1 ⊕xtest (Rubin et al., 2022).
To evaluate the inference results, we use the
same method described in Section 3.2 to generate
predictions, and then use each task’s corresponding
evaluation metric to compute the scores.
4 Experiment Settings
Task clustering. We group the tasks used in our
method into clusters, including Reading Compre-
hension, Closed-book QA, Paraphrase Detection,
Natural Language Inference, Sentiment Analysis,
Commonsense Reasoning, Coreference Resolution,
Structure to Text, and Summarization. The datasets
used in each cluster are listed in Appendix A.
Data sampling. To prevent the retriever tuning
from being dominated by large datasets, we ran-
domly sample up to 10k data examples from each
task’s training set, while also maintaining class
balance in classification tasks3. The prompt pool
consists of the sampled training data only. On aver-
age, for each testing task cluster, there are approxi-
mately 180k training examples sampled from the
training clusters.
LLMs. We use GPT-Neo-2.7B (Black
et al., 2021) from EleutherAI to tune the
retriever, and evaluate the performance on
larger LLMs from various sources during in-
ference, including BLOOM-7.1B (Scao et al.,
2022) from BigScience, OPT-66B (Zhang
et al., 2022) from Meta, and Davinci and
text-davinci-001 from OpenAI, both be-
longing to the GPT3-175B (Brown et al., 2020)
series. Greedy search is used to obtain predictions
from all the LLMs.
Prompt scoring. We set the size of the randomly
sampled subset to L = 50and the number of (hard)
3For instance, in a four-classification task, we sample a
maximum of 2.5k data examples from each class.
negatives to B = 20. For difficult questions, we
repeat the re-sampling process up to seven rounds,
as we found that this is sufficient to identify a posi-
tive prompt for 90% of the training examples. If no
sampled prompt yields a score greater than 0, we
filter out the corresponding training example.
Tuning. We initialize the two independent en-
coders of the retriever with BERTBASE (Devlin
et al., 2019). Each retriever is fine-tuned for three
epochs, and the best checkpoint is chosen based
on retrieval accuracy using the validation set. For
detailed tuning hyperparameters, Please refer to
Appendix B.
Inference. During inference, we set the number
K of concatenated prompts to a relatively small
value of 3, to balance between prompting perfor-
mance and inference efficiency. For each dataset,
we report metric scores on the test set when avail-
able, falling back to the validation set otherwise.
5 Main Results
We evaluate our prompt retriever on natural lan-
guage understanding tasks where generative LLMs
are known to need improvement (Liu et al., 2021).
Table 1 compares the performance of UPRISE to
vanilla zero-shot prompting.
5.1 Cross-Task Prompt Retrieval
Based on the results of GPT-Neo-2.7B, we can
assess our ability of generalizing across different
task types. UPRISE has positive impacts on most
of the testing clusters. Specifically, we achieve
absolute gains of 8.5% and 14.6% in Reading Com-
prehension and Paraphrase Detection tasks, respec-
tively. We also find that UPRISE shows consistent
improvements across all tasks in Closed-book QA
and Natural Language Inference clusters.
However, UPRISE has negative impacts on Com-
monsense Reasoning and Coreference Resolution
tasks. We conduct analyses in Appendix D to un-
derstand the reasons, revealing that Coreference
Resolution hardly benefits from demonstrations
and Commonsense Reasoning is harmed by differ-
ent demonstration formats.
5.2 Cross-Model Prompt Retrieval
In addition to evaluating cross-task generalization,
we can explore the cross-model ability by exam-
ining the results of BLOOM, OPT, Davinci and
text-davinci-001. UPRISE continues to im-
prove performance on Reading Comprehension,



Source: data\tc16_2312.10997v5\referenced_papers\[20]_2303.08518.pdf (Page 2):

grouped into different clusters based on their task
types, and we hold out each task cluster for evalu-
ation while training the retriever on all remaining
clusters (Wei et al., 2022a).
Cross-model retrieval. Due to the high cost of
tuning a prompt retriever with a large-scale LLM,
we propose evaluating the capability to generalize
from a small LLM to a large LLM. Specifically, we
use a relatively small LLM for tuning the retriever,
while using a much larger LLM for inference. Fur-
thermore, we suggest exploring the transferability
between different LLM sources, as there are LLMs
developed by different companies or institutions.
3 Method
As shown in Figure 3, UPRISE uses a frozen LLM
to supervise the fine-tuning of a prompt retriever
on diverse tasks, and then uses this trained retriever
to retrieve prompts for unseen task types with dif-
ferent LLMs during inference. In this section, we
elaborate on our data construction, prompt scoring,
retriever tuning and inference pipeline.
3.1 Data Construction
Task Data. We use instruction templates from
FLAN (Wei et al., 2022a) to convert task datasets
into natural language instructions 2. Each task
dataset corresponds to approximately seven tem-
plates. For each data example (xi, yi), we ran-
domly select one of the seven templates to convert
xi into a task input and yi into a label completion.
The option suffices and new line characters “ \n”
are automatically removed from the task input, to
make the text format more similar to that of the
pre-training corpus, improving prompting perfor-
mance (van de Kar et al., 2022).
Prompt pool. For each testing cluster, the
prompt pool used for retrieval is made up of train-
ing demonstrations of the remaining task clusters
(i.e., the clusters for training the retriever). This is
inspired by in-context learning (Brown et al., 2020),
which presents a few training demonstrations be-
fore the task input to improve model performance.
Each demonstration is a concatenation of the task
input and the label completion. Our motivation
is that the testing input may benefit from similar
question types, topics, or reasoning chains in the
retrieved demonstrations, despite that the testing
2We exclude templates that “turn the task around”, such
as asking a sentiment classification task to generate a movie
review.
input and the demonstrations are of different task
types.
3.2 Prompt Scoring
For each training example (xi, yi) in the training
clusters, we collect a set of positive and nega-
tive prompts from the prompt pool P = {pj}NP
j=1,
where the positive prompt indicates that the frozen
LLM achieves good task scores conditioned on the
prompt-input concatenation. We use these posi-
tive and negative labels to supervise the contrastive
learning of the retriever.
We categorize all tasks into two question types:
text completion and multiple choice (Brown et al.,
2020), and use different methods to score the
prompts for each training example.
Text completion is the question to do free-form
completion. We calculate score of the prompt using
the following equation:
score (pj, xi) = metric
 
yi, ypj
i

, (3)
where ypj
i = LM
 
ypj
i |pj ⊕ xi

is the model pre-
diction based on the input concatenation pj ⊕ xi,
and ⊕ is a text delimiter “ \n”. metric (·) is the
function used to calculate the task metric score
(e.g., F1 or ROUGE).
Multiple choice is the question to choose one
correct completion from several options. Suppose
there are M options in a multiple choice question
xi, yi, {om}M
m=1

, where {om}M
m=1 is the option
set and oyi is the gold option. We feed the concate-
nation pj ⊕ xi to the LLM and calculate per-token
likelihood of each option: LH (om). The option
with the highest likelihood is considered as the
model prediction ypj
i (Brown et al., 2020).
Accuracy of the prediction acc
 
yi, ypj
i

is a
common metric for multiple-choice questions, but
it only produces 0 or 1 for each example, making it
hard to compare prompt effectiveness. To address
this, we multiply the accuracy by the per-token like-
lihood of the gold option, which is normalized by
the sum of the per-token likelihood of all options,
to achieve a fine-grained comparison. The final
score is formulated as:
score (pj, xi) = acc
 
yi, ypj
i

· LH (oyi)PM
m=1 LH (om)
.
(4)
Prompt filtering. Intuitively, to collect the posi-
tive and negative prompts for each training exam-
ple, we need to score every prompt in the prompt



### Claim 42/179

#### Claim Text
AAR (Augmentation-Adapted Retriver) [47] introduces a universal adapter designed to accommodate multiple downstream tasks.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[47]_2305.17331.pdf (Page 0):

Augmentation-Adapted Retriever Improves Generalization of Language
Models as Generic Plug-In
Zichun Yu1 Chenyan Xiong2 Shi Yu1 Zhiyuan Liu13
1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China
2Microsoft Research, Redmond, USA
3Beijing National Research Center for Information Science and Technology, Beijing, China
{yuzc19, yus21}@mails.tsinghua.edu.cn; chenyan.xiong@microsoft.com
liuzy@tsinghua.edu.cn
Abstract
Retrieval augmentation can aid language
models (LMs) in knowledge-intensive tasks
by supplying them with external information.
Prior works on retrieval augmentation usually
jointly fine-tune the retriever and the LM,
making them closely coupled. In this paper, we
explore the scheme of generic retrieval plug-in:
the retriever is to assist target LMs that may
not be known beforehand or are unable to
be fine-tuned together. To retrieve useful
documents for unseen target LMs, we propose
augmentation-adapted retriever (AAR), which
learns LM’s preferences obtained from a
known source LM. Experiments on the MMLU
and PopQA datasets demonstrate that our AAR
trained with a small source LM is able to signif-
icantly improve the zero-shot generalization of
larger target LMs ranging from 250M Flan-T5
to 175B InstructGPT. Further analysis indicates
that the preferences of different LMs overlap,
enabling AAR trained with a single source
LM to serve as a generic plug-in for various
target LMs. Our code is open-sourced at
https://github.com/OpenMatch/Augmentation-
Adapted-Retriever.
1 Introduction
Large language models (LMs) that possess bil-
lions of parameters are able to capture a signif-
icant amount of human knowledge, leading to
consistent improvements on various downstream
tasks (Brown et al., 2020; Kaplan et al., 2020;
Roberts et al., 2020). However, the undeniable
drawback of large LMs lies in their high compu-
tational cost, which negatively impacts their effi-
ciency (Strubell et al., 2019; Bender et al., 2021).
Furthermore, the knowledge memorized from pre-
training and the implicit reasoning process of LMs
can be inaccurate and intractable sometimes, hin-
dering their applications on knowledge-intensive
tasks (Guu et al., 2020; Lewis et al., 2020; Mallen
et al., 2022; Wei et al., 2022).
Flan-T5Base
(250M)
Flan-T5Large
(780M)
Flan-T5XL
(3B)
InstructGPT
(175B)
# Parameters
35
40
45
50
55
60
65MMLU Accuracy
Standalone LM
LM w/ Few-Shot Prompting
LM w/ Adaptive Retrieval
LM w/ AAR (Ours)
Figure 1: Performance of LM w/ AAR (Ours).
Instead of leveraging the knowledge and rea-
soning abilities embedded within the parameters
of the LMs, retrieval augmentation (Guu et al.,
2020; Lewis et al., 2020; Borgeaud et al., 2022)
enhances the LM with a retriever that can retrieve
knowledge from an external corpus. On the other
hand, prior retrieval augmentation methods (Izac-
ard and Grave, 2021a; Izacard et al., 2022) necessi-
tate fine-tuning the backbone LM to adjust to the
retriever and tackle specific downstream tasks. This
kind of fine-tuning can be expensive when more
and more unique demands emerge (Maronikolakis
and Schütze, 2021). More importantly, many top-
tier LMs can only be accessed through black-box
APIs (Ouyang et al., 2022; OpenAI, 2023). These
APIs allow users to submit queries and receive re-
sponses but typically do not support fine-tuning.
In this paper, we introduce Augmentation-
Adapted Retriever (AAR) to assist black-box LMs
with downstream tasks as generic plug-in. To re-
trieve valuable documents for many unseen LMs,
we propose to leverage a small source LMto pro-
vide LM-preferred signals for retriever’s training.
The retriever after training (i.e., AAR) can be di-
rectly utilized to assist a large target LMby plug-
ging in the retrieved documents.
Specifically, we choose a small encoder-decoder
LM as the source LM and utilize its fusion-
arXiv:2305.17331v1  [cs.CL]  27 May 2023



Source: data\tc16_2312.10997v5\referenced_papers\[47]_2305.17331.pdf (Page 2):

relevant documents from a corpus using a retriever,
and then utilizes the documents to enhance its lan-
guage generation capabilities.
The objective of the retriever is to find an aug-
mentation document set Da from a corpus C that
helps the LM handle a given query q. Previous
researches (Karpukhin et al., 2020; Xiong et al.,
2021) concentrate primarily on the dense retrieval
system that searches in the dense vector space since
dense retrieval usually performs more accurately
and efficiently than sparse one.
A dense retrieval model first represents q and
the document d into an embedding space using a
pre-trained encoder g,
q = g(q); d = g(d), d∈ C, (1)
and match their embeddings by dot product func-
tion f, which supports fast approximate nearest
neighbor search (ANN) (André et al., 2016; John-
son et al., 2021). We then define Da that contains
top-N retrieved documents as:
Da = {da
1 . . . da
N } = ANNN
f(q,◦). (2)
For the LM backbones, the decoder-only and
the encoder-decoder models are the two primary
choices of the retrieval-augmented LMs (Izacard
and Grave, 2021b; Yu et al., 2023).
Given a decoder-only LM like GPT-3 (Brown
et al., 2020), the LM input can be a simple concate-
nation of the query and all the augmentation docu-
ments {da
1 . . . da
N }. Then, the LM will generate the
answer based on the inputs auto-regressively.
For an encoder-decoder LM like T5 (Raffel et al.,
2020), taking simple concatenation as the encoder
input may still be effective. However, this method
may not scale to a large volume of documents due
to the quadratic self-attention computation associ-
ated with the number of documents. To aggregate
multiple documents more efficiently, Izacard and
Grave (2021b) propose the fusion-in-decoder (FiD)
mechanism, which soon becomes the mainstream
in the development of encoder-decoder retrieval-
augmented LMs. It first encodes each concatena-
tion of the (da
i , q) pair separately and then lets the
decoder attend to all parts:
FiD(q) =Dec(Enc(da
1 ⊕q) . . .Enc(da
N ⊕q)). (3)
In this way, the encoder computes self-attention
over one document at a time so that the compu-
tational cost can grow linearly with the number
of documents. Furthermore, FiD cross-attention
is found effective in estimating the relative im-
portance of the augmentation documents from
Negatives  
ANCE Sampling Positives  
Ground Truth  
Top-K FiDAtt 
  
  
 
Pre-Trained Retriever
Q + D1 ...
Enc Enc
Dec
Enc...
Source LMFusion-in-Decoder
Retrieve 
N Docs 
Source Task Q + D2Q + DN
Augmentation-Adapted Retriever
Target LMsTarget TasksGeneric  
Plug-In 
Figure 2: Illustration of augmentation-adapted retriever.
the LM’s perspective (Izacard and Grave, 2021a).
Therefore, soft FiD distillation (Izacard and Grave,
2021a; Izacard et al., 2022; Shi et al., 2023), which
minimizes the KL-divergence between retrieval
likelihood and LM likelihood, is often used to train
the retriever and the LM end-to-end.
3.2 Augmentation-adapted Retriever
Due to the emerging real-world demands and
the limitations of black-box APIs, fine-tuning
retrieval-augmented LM for each possible down-
stream task can be infeasible. Hence, we intro-
duce Augmentation-Adapted Retriever (AAR) as a
generic plug-in for black-box LMs. As illustrated
in Figure 2, AAR can learn the preferences of LMs
without the need for fine-tuning them.
Specifically, we utilize an encoder-decoder LM
as source LM (Ls) to provide LM-preferred signals
on a source task (Ts) for fine-tuning a pre-trained
retriever. Then, we plug the fine-tuned retriever
into unseen target LM (Lt) on a set of target tasks
(Tt) non-intersecting with Ts.
Our training method starts from a source task Ts,
where we aggregate the source LM Ls’s average
FiD cross-attention (FiDAtt) scores Sa
i correspond-
ing to document da
i from the first decoder token
over all the layers, all the heads and all the input
tokens t of da
i ⊕ q:
Sa
i = 1
ln ∗ hn ∗ tn
X
layers
X
heads
X
t∈da
i ⊕q
FiDAtt(FiD(q)). (4)
where ln, hn, tn are the numbers of the layers, the
heads and the input tokens.
To make the training process more robust, we uti-
lize the FiDAtt scores to annotate the LM-preferred
positive documents in a discrete way:
Da+ = Dh+ ∪ Top-KSa
i ,Da, (5)



Source: data\tc16_2312.10997v5\referenced_papers\[47]_2305.17331.pdf (Page 1):

in-decoder attention scores (Izacard and Grave,
2021a) to annotate LM-preferred documents. The
LM-preferred documents are then combined with
human-preferred documents to form the positive
document set. Negative documents are mined by
the retriever itself using the ANCE (Xiong et al.,
2021) technique. After fine-tuning the retriever
with LM’s preferences, it can directly assist unseen
target LMs in the zero-shot task generalization.
We evaluate AAR on a multi-task language
understanding dataset MMLU (Hendrycks et al.,
2021) and an entity-centric question answering
dataset PopQA (Mallen et al., 2022). For the tar-
get LMs, we choose Flan-T5 (Chung et al., 2022)
series as our backbone for encoder-decoder LMs
and InstructGPT (Ouyang et al., 2022) as our back-
bone for decoder-only LMs. Figure 1 shows that
assisted with a generic AAR, LMs of different sizes
and architectures can consistently outperform the
standalone LMs; the performance of smaller LMs
can sometimes surpass the standalone counterparts
of significantly larger sizes (e.g., Flan-T5 Large w/
AAR outperforms standalone Flan-T5XL by 0.6%).
AAR also demonstrates advantages over other aug-
mentation approaches such as few-shot prompting
and adaptive retrieval (Mallen et al., 2022).
Further analysis reveals that the preferences ob-
tained from different-sized source LMs are similar,
and LMs with near capacities tend to yield closer
preferred document sets. As a result, our AAR
model trained from a small source LM can be con-
sidered as a generic plug-in to enhance the zero-
shot generalization of a significantly larger target
LM. We also discover that the documents preferred
by LMs can provide assistance to the model from
alternative perspectives, rather than relying solely
on the full information favored by search users.
2 Related Work
Retrieval Augmentation.Augmenting LMs with
retrieved information from external memories has
shown effective on diverse knowledge-intensive
tasks (Guu et al., 2020). Prior works explore
novel ways to train the whole retriever-LM sys-
tem in an end-to-end fashion, using retrieval-
augmented sequence log-likelihood (Lewis et al.,
2020; Borgeaud et al., 2022), fusion-in-decoder
attention distillation (Izacard and Grave, 2021a;
Izacard et al., 2022), or knowledge graph (Ju et al.,
2022). To decouple the retriever from LM, Rubin
et al. (2022) train an independent prompt retriever
for in-context learning, and Lin et al. (2022) only
fine-tune the LM via the retrieved data that is simi-
lar to few-shot unsupervised samples.
Recent researches adopt zero-shot retrieval aug-
mentation that does not fine-tune the LM on In-
structGPT (Ouyang et al., 2022). It can benefit
entity-centric question answering (Mallen et al.,
2022), chain-of-thought reasoning (He et al., 2022),
and multi-hop question answering (Khattab et al.,
2022). Parallel work (Shi et al., 2023) uses LM
likelihood to train the retriever for satisfying black-
box LM’s preferences, and they adopt GPT-3
Curie (Brown et al., 2020) to provide the super-
vision signals. In this work, we devise the retriever
that can be used as a generic plug-in to assist a
variety of unseen LMs.
Zero-shot Learning and Reasoning. Large-
scale unsupervised pre-trained LMs like GPT-
3 (Brown et al., 2020), GPT-4 (OpenAI, 2023),
and PaLM (Chowdhery et al., 2022) are able to
perform zero-shot learning on many downstream
tasks with a task description provided at inference
time. Instruction-finetuned LMs (Sanh et al., 2022;
Chung et al., 2022; Ouyang et al., 2022), which
are pre-trained on multiple supervised tasks using
human instructions, also also exhibit robust zero-
shot learning capabilities. Yu et al. (2023) pro-
pose a new scheme of zero-shot reasoning, which
first prompts large LMs to generate relevant docu-
ments and then perform reading comprehension
on the generated contents. Recently, there has
been a growing trend of utilizing plug-and-play
knowledge injection to enhance the zero-shot per-
formance of LMs, which is achieved through map-
ping network (Zhang et al., 2023) or document
encoding (Xiao et al., 2023). Our work improves
the zero-shot generalization of LMs by utilizing the
retrieved information. We demonstrate that identi-
fying LMs’ preferences to train the retriever can in
turn bring additional evidence texts for LMs.
3 Method
In this section, we first introduce the preliminaries
of the dense retrieval and the retrieval-augmented
LM (§ 3.1), then propose our augmentation-
adapted retriever (§ 3.2).
3.1 Preliminaries
Retrieval-augmented LM (Guu et al., 2020; Lewis
et al., 2020) is a type of LM that leverages external
information to improve its performance. It retrieves



Source: data\tc16_2312.10997v5\referenced_papers\[47]_2305.17331.pdf (Page 7):

Corpora MMLU PopQA
All Hum. Soc. Sci. STEM OtherAll
MS MARCO44.8 42.2 46.4 39.0 53.2 13.6
KILT-Wikipedia42.6 42.5 45.9 34.3 50.5 37.7
Standalone LM36.1 40.4 39.8 27.0 40.6 8.8
Table 3: Ablation of the retrieval corpus, with Flan-
T5Base as LM and AARANCE as retriever.
human-preferred documents), LM-preferred docu-
ments provide helpful information from alternative
perspectives. Therefore, adapting retrievers with
LM-preferred documents can in turn make retrieval-
augmented LM perform better.
5.4 Multi-task Training of AAR
In this section, we explore if the multi-task training
of AAR can endow the retriever with better gener-
alization to the target task. Specifically, we choose
KILT (Petroni et al.) as our multi-task data source,
which consists of 5 categories (Fact Checking, En-
tity Linking, Slot Filling, Open Domain QA, and
Dialogue). We take one representative subtask per
category to form a mixture of multiple source tasks.
Figure 7 illustrates that ANCE trained with
multi-task KILT can consistently outperform the
single-task MSMARCO QA, proving the bet-
ter generalization ability brought by multi-task
augmentation-adapted training. It is possible that
LMs may vary slightly in preferred documents for
different tasks and AAR can switch more smoothly
to the target task with the help of multi-task train-
ing. Contriever does not benefit greatly from multi-
task training. We conjecture that this is because
Contriever has been pre-trained with multiple for-
mats of data augmentations and thus generalizes
better to new data distribution than ANCE. Inter-
estingly, multi-task instruction-finetuned retriever
TART (Asai et al., 2022) has an overall worse per-
formance compared to AAR, highlighting the ben-
efits of having LM-preferred documents during the
multi-task training. A more detailed analysis about
the selection of source tasks is in Appendix B.
5.5 Effect of Retrieval Corpus
Table 3 demonstrates that regardless of the retrieval
corpus, AAR results in consistent and substantial
performance gains over the standalone LM.
On MMLU, using MS MARCO as the retrieval
corpus improves the LM more compared to KILT-
Wikipedia. We hypothesize that the retriever has
been trained with MS MARCO corpus and thus
holds better retrieval performance on it.
Settings Methods MMLUPopQA
All All
Few-shotOPT (Zhang et al., 2022) 26.0 12.3
GPT-neo (Black et al., 2021)28.7 11.3
Zero-shot
OPT 22.7 12.0
GPT-neo 25.3 9.9
OPT GenRead 22.3 12.2
GPT-neo GenRead 24.4 11.9
OPT w/ AARContriever(Ours) 23.2 29.1
GPT-neo w/ AARContriever(Ours) 25.2 27.8
OPT w/ AARANCE(Ours) 23.7 32.9
GPT-neo w/ AARANCE(Ours) 26.6 30.1
Table 4: Results of OPT and GPT-neo. We use their
1.3B version. The score marked as bold means the best
performance in the zero-shot setting.
On PopQA, model performance will drop by
large margins if we use MS MARCO as the re-
trieval corpus instead of KILT-Wikipedia. The pri-
mary reason is that the PopQA dataset is sampled
from Wikidata and designed for long-tail questions.
Partial long-tail knowledge can be only found in
KILT-Wikipedia (Mallen et al., 2022) while MS
MARCO lacks the indispensable evidence that
should be utilized for answer prediction. For in-
stance, given the question “Who is the mother
of Melissa Benn?”, there is no document in MS
MARCO containing the answer “Caroline Benn”.
Under such circumstances, aligning the retrieval
corpus with the data source can be necessary to
leverage AAR’s ability.
5.6 Application Scenarios of AAR
To examine if AAR works for unseen LMs that
lack zero-shot generalization ability, we also report
the results of OPT (Zhang et al., 2022) and GPT-
neo (Black et al., 2021). These models may have
poor zero-shot performance due to the lack of multi-
task instruction tuning.
From Table 4, we find that our AAR improves
both LMs marginally on MMLU while achieving
significant gains on PopQA. We conjecture that
LMs can benefit more easily from retrieval augmen-
tation on the knowledge-probing task like PopQA,
where the answer span can be directly acquired
from the retrieved documents. MMLU requires the
LM to not only comprehend the retrieved pieces of
evidence but also perform knowledge-based reason-
ing over them. OPT and GPT-neo may not possess
such abilities in zero-shot scenarios.
In summary, although AAR perfectly fits the
multi-task instruction-finetuned LMs such as the
Flan-T5 series and InstructGPT, it may not bring
significant gains for LMs whose zero-shot perfor-



Source: data\tc16_2312.10997v5\referenced_papers\[26]_2401.06954.pdf (Page 2):

Figure 2: Differing from previous RAGs that update
LLMs, retrievers, or both, BGM connects a frozen LLM
and a frozen retriever through a lightweight bridge
model which adapts the retrieved information to the
LLM’s preference. This makes BGM applicable to
“large” LMs and off-the-shelf retrievers.
from various knowledge sources is proven effective
across numerous NLP tasks, including language
modeling (Borgeaud et al., 2022; Khandelwal et al.,
2020; Shi et al., 2023b), question answering (Lewis
et al., 2020; Izacard et al., 2022; de Jong et al.,
2023; De Jong et al., 2023; Shi et al., 2023b; Guu
et al., 2020; Izacard and Grave, 2020; Xu et al.,
2023), fact versification (Lewis et al., 2020) and
text generation (Lewis et al., 2020). Specifically,
RAG utilizes input as a query and comprises two
main components: (1) a retriever retrieves a set
of items from a side corpus. Particular items may
vary across different tasks, including documents,
passages, or even tokens. In this study, we focus on
retrieving passages; and (2) a LLM incorporates
the retrieved items, as additional information in the
input context, and makes final predictions.
A fundamental question in this process arises
regarding the disparate preferences between LLMs
and retrievers, as LLMs performing optimally only
when their preferences are satisfied. Bridging the
preference gap is crucial. Depending on which
components are subject to updates, this challenge
can be categorized into three families.
Finetuning retrievers and LLMs jointly. This
is the most widely used setting of RAG (Izacard
et al., 2022; Khandelwal et al., 2020; Wu et al.,
2022; Guu et al., 2020; Lewis et al., 2020). How-
ever, most prior work is based on relative small
LMs (< 1B). For example, Altas (Izacard et al.,
2022) finetunes LLM (T5 (Raffel et al., 2020a))
and retriever (Contriever (Izacard et al., 2021))
jointly by leveraging the LLM to provide super-
visory signal to train the retriever. RAG (Lewis
et al., 2020) uses a tunnable query encoder and
DPR (Karpukhin et al., 2020) as retriever, BART
as LLM, and design an end-to-end schema to train
the query encoder and the LLM.
Finetuning LLMs only. Updating retrievers is
not always desirable as it is costly and requires
the document index to be periodically updated. To
bridge the preference gap, it is also possible to only
update the LLMs. FiD (Izacard and Grave, 2020)
takes the retrieved documents and query as input,
finetunes the LLM to adapt to the external infor-
mation. Similarly, Lummen (De Jong et al., 2023)
and Glimmer (de Jong et al., 2023) improve FiD
via adding reranker and pre-encoding memeory.
Finetuning retrievers only. Although above
systems have shown improves results, they are not
always applicable in practice. Many LLMs (espe-
cially larger ones) are only available as black-box
APIs. Given this constraint, a natural approach
is to only update the retrievers to ensure they can
retrieve passages that are more compatible with
LLMs. REPLUG (Shi et al., 2023b) adapts a sim-
ilar idea as Atlas but fix the LM. RECOMP (Xu
et al., 2023) trains a compressors to summarize
the retrieved document from retriever. However,
this family of models is incapable of performing
any sample-level selection and can only choose top
passages by setting a fixed threshold.
Unlike the existing approaches, BGM does not
fine-tune the LLM or the retriever and instead
employs a bridge model in between (Fig. 2).
RL for information retrieval. Before the LLM
era, RL has been used in information retrieval (IR)
(Xia et al., 2017; Wei et al., 2017; Zeng et al., 2018;
Xu et al., 2020). The core approach was to frame
the IR problem as a Markov Decision Process and
apply an RL algorithm to solve it. Typically, an
IR task would be structured to determine which
document to select for a ranking position, using
ranking metrics such as DCG as the reward. None
of these existing studies explore the application of
RL in the context of RAG.
In the LLM era, RL has been used in query
rewriting for retrieval (Wu et al., 2021; Nogueira
and Cho, 2017; Adolphs et al., 2021), where a
black-box retriever is assumed. This is a different
problem from RAG. Bacciu et al. (2023) suggest us-
ing RL to fine-tune retriever in the RAG context in
their opinion paper, not supported by experiments.
Their work does not recognize the importance of
bridging the gap between retrievers and LLMs, nor



### Claim 43/179

#### Claim Text
While PRCA [69] add a pluggable reward-driven contextual adapter to enhance performance on specific tasks.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[69]_2310.18347.pdf (Page 8):

Case Study without and with PRCA
Question:
Of what form do Mersenne primes take?
Golden Answer: 2p-1
Part of Retrieved Documents: [Golden
Answer Source] Mersenne primes are
prime numbers that are of the form ::::2p-1,
where p is an arbitrary prime. [Pre-
dicted Answer Source] The Sieve of Er-
atosthenes, attributed to Eratosthenes, is a
simple method to compute primes, although
the large primes found today with comput-
ers are not generated this way. are prime.
Prime numbers of this form are known as
:::::::factorial:::::::primes.
Predicted Answer without PRCA: Facto-
rial primes
Context through PRCA: Mersenne primes
are prime numbers that are of the form 2p-1,
where p is an arbitrary prime. The Lu-
cas–Lehmer test is particularly fast for num-
bers of this form, so many of the largest
primes found today are Mersenne primes.
Predicted Answer with PRCA: 2p-1
Note: “–” denotes key information
relevant to the question, “ ~” represents
predicted answers.
5.5 Ablation Study of PRCA
We assessed the impact of PRCA on three datasets
using the configurations from section 5.2, which
showed maximum improvements. The evaluation
is conducted with and without the reward-driven
stage to observe the impact of PRCA on the per-
formance. As illustrated in Figure 7, without the
reward-driven training stage, the effect of PRCA on
the entire configuration becomes adverse because
PRCA merely simplifies the text without discerning
which information is beneficial for the generator
to answer questions, resulting in the omission of
useful text. In contrast, once the training process
incorporates the reward-driven stage, the quality of
the context becomes directly aligned with reward
values, assisting PRCA in more effectively distill-
ing pertinent information. Therefore, the reward-
driven stage is vital, allowing PRCA to retain key
details while simplifying text, enhancing its overall
effect.
−0.05 0.00 0.05 0.10 0.15 0.20
GPT4 Assessment Accuracy
SQuAD
HotpotQA
T op cOCQA
0.09
0.13
0.2
-0.05
-0.08
-0.04
Effect of PRCA  W th the 
Reward-Dr ven T ra n ng Stage
Effect of PRCA  W thout the 
Reward-Dr ven T ra n ng Stage
Figure 7: An illustration showcasing the impact of the
reward-driven stage on PRCA’s performance.
6 Conclusion
In conclusion, this research successfully introduces
a PRCA-based paradigm for ReQA tasks, tack-
ling the inherent challenges of fine-tuning LLMs
in the retrieval-enhancement framework, especially
given their vast parameter size and closed-source
natures. PRCA innovatively distills retrieved docu-
ments via generator rewards, leading to a marked
improvement in the ReQA task’s performance. Ex-
perimental outcomes consistently demonstrate the
robustness and effectiveness of PRCA when paired
with various retrievers and generators, indicating
its potential to be widely deployed as an adapter on
the ReQA task.
Limitations
While PRCA has shown effectiveness in improving
ReQA task performance, it has limitations, includ-
ing dependency on generators, convergence issues,
and limited integration with retrievers. The reward
during reinforcement learning training is derived
from the generator, requiring PRCA retraining with
different generators, which can be time-consuming.
PRCA may also experience difficulties converging
in a single training session, which impacts the sta-
bility and consistency of its performance. Lastly,
PRCA’s operation as a pluggable adapter limits its
ability to train jointly with retrievers, which means
if the retrieval quality is not up to par, PRCA’s
effectiveness could be compromised.
Acknowledgement
Supported by the Key Research and Develop-
ment Program of Guangdong Province (grant No.
2021B0101400003) and Corresponding author is
Jianzong Wang (jzwang@188.com).



Source: data\tc16_2312.10997v5\referenced_papers\[69]_2310.18347.pdf (Page 1):

cess but also come with limitations. (Shi et al.,
2023) utilized the logits from the final layer of the
LLMs when calculating the loss function, which
may not be available to certain powerful LLMs that
served via APIs. (Ma et al., 2023) involved fre-
quently invoking pricy LLMs and overlooked the
impact of the input token length on the accuracy
and effectiveness of the system.
To overcome these hurdles, we propose a train-
able Pluggable Reward-driven Context Adapter
(PRCA) that enables one to fine-tune the adapter
instead of LLMs under the retrieval-augmented
framework on specific datasets and achieve
higher performance. Furthermore, PRCA distills
the retrieved documents information guided by
rewards from the generator through reinforcement
learning. The distillation of retrieval information
through PRCA reduces the length of text input to
the generator and constructs a context of superior
quality, which mitigates the hallucination issues
during the answer generation. As shown in Figure
1, PRCA is placed between the retriever and the
generator, forming a PRCA-based Paradigm where
both the generator and the retriever remain frozen.
In general, the introduction of the PRCA-based
paradigm brings the following advantages:
Black-box LLMs Integration With the use
of PRCA, LLMs can be treated as a black box
integrated into the retrieval-augmented framework,
eliminating the need for resource-intensive fine-
tuning and restrictions on closed-nature models.
Robustness PRCA serves as a pluggable
adapter that is compatible with various retrievers
and generators because PRCA-based paradigm
keeps both the generator and retriever frozen.
Efficiency The PRCA-based paradigm en-
sures the efficiency of the framework by reducing
the text length inputted into the generator and can
adapt to different retrieval corpus.
2 Related Work
2.1 The Potential of LLMs as Black-Box
Models
LLMs have demonstrated remarkable capabilities
in downstream QA tasks, even in scenarios with
limited or no training data (Wei et al., 2022). This
emergence capability enables them to efficiently
tackle such tasks, making them potential candidates
for black-box models in inference. Furthermore,
the non-open-source nature and large parameter
size of these models further contribute to their in-
clination towards being perceived as black boxes.
On one hand, LLMs like GPT-4 (OpenAI, 2023)
and PaLM (Scao et al., 2023) have showcased im-
pressive performance in QA tasks. However, their
closed source nature restricts access to these mod-
els, making API-based utilization the only feasi-
ble option, thereby categorizing them as black-box
models.
On the other hand, training LLMs, exemplified
by models like Bloom (Scao et al., 2022) and GLM-
130B (Zeng et al., 2023), impose substantial com-
putational demands. Specifically, training Bloom
took 3.5 months using 384 NVIDIA A100 80GB
GPUs. Similarly, GLM-130B requires a two-month
training period on a cluster of 96 DGX-A100 GPU
servers. These resource requirements make it ex-
tremely challenging for the majority of researchers
to deploy these models. Moreover, LLMs exhibit
rapid development speeds. For instance, from
LLaMA (Touvron et al., 2023) to Alpaca (Taori
et al., 2023) and now Vicuna (Peng et al., 2023),
the iterations are completed within a month. It
is evident that the speed of training models lags
behind the pace of model iterations. Consequen-
tially, tuning small-size adapters for any sequence-
to-sequence LLMs on downstream tasks could be
a simpler and more efficient approach.
2.2 Retrieval-Augmented Framework
Various retrieval augmented ideas have been pro-
gressively developed and applied to improve the
performance in the ReQA task.
In the initial stage of research, independent
statistical similarity-base retrievers like TF-IDF
(Sparck Jones, 1972) and BM25 (Robertson and
Zaragoza, 2009) were used as fundamental retrieval
engines. They helped in extracting the most rel-
evant documents from the corpus for QA tasks
(Chen et al., 2017; Izacard and Grave, 2021).
The concept of vectorization was subsequently
introduced, where both questions and documents
were represented as vectors, and vector similar-
ity became a critical parameter for retrieval. This
paradigm shift was led by methods such as dense
retrieval, as embodied by DPR (Karpukhin et al.,
2020). Models based on contrastive learning like
SimCSE (Gao et al., 2021) and Contriver (Izac-
ard et al., 2022a), along with sentence-level se-



Source: data\tc16_2312.10997v5\referenced_papers\[69]_2310.18347.pdf (Page 2):

mantic models such as Sentence-BERT (Reimers
and Gurevych, 2019), represented this era. These
methods can be seen as pre-trained retrievers that
boosted the effectiveness of the ReQA task.
Further development led to the fusion of retrieval
and generation components within the ReQA
frameworks. This was implemented in systems
like REALM (Guu et al., 2020) and RAG (Lewis
et al., 2020b), where retrievers were co-trained with
generators, further refining the performance in the
ReQA task.
Recently, advanced approaches like Atlas (Izac-
ard et al., 2022b) and RETRO (Borgeaud et al.,
2022) have been introduced which could achieve
performance comparable to large-scale models like
Palm (Chowdhery et al., 2022) and GPT3 (Brown
et al., 2020) with significantly fewer parameters.
3 Methodology
3.1 Two-Stage Training for PRCA
PRCA is designed to take sequences composed of
the given query and the Top-K relevant documents
retrieved by the retriever. The purpose of PRCA
is to distill this collection of results, presenting
a concise and effective context to the generator,
while keeping both the retriever and the genera-
tor frozen. This PRCA-based paradigm introduces
two challenges: the effectiveness of the retrieval
cannot be directly evaluated due to its heavy depen-
dence on the responses generated by the genera-
tor, and learning the mapping relationship between
the generator’s outputs and the input sequence via
backpropagation is obstructed due to the black-box
generator. To tackle these issues, we propose a two-
stage training strategy for PRCA, as illustrated in
Figure 2. In the contextual stage, supervised learn-
ing is employed to train PRCA, encouraging it to
output context-rich extractions from the input text.
During the reward-driven stage, the generator is
treated as a reward model. The difference between
the generated answer and the ground truth serves as
a reward signal to further train PRCA. This process
effectively optimizes the information distillation
to be more beneficial for the generator to answer
accurately.
3.2 Contextual Extraction Stage
In the contextual extraction stage, we train PRCA
to extract textual information. Given an input
text Sinput, PRCA generates an output sequence
Cextracted, representing the context derived from the
Reward 
Generator 
Reward Model 
PRCA 
Top-K 
Documents 
Query 
Optimal 
Context 
Query 
++
PRCA 
Extracted Context Input Text 
Contextual Extraction Stage 
Reward-Driven Stage 
Figure 2: An illustration of the two-stage sequential
training process for the PRCA. In the first “Contex-
tual Extraction Stage”, PRCA module is pre-trained on
domain abstractive summarization tasks. The second
“Reward-Driven Stage”, demonstrates the interaction be-
tween retrieved Top-K documents and the PRCA. Here,
the PRCA refines the query using both the documents
and the original query, producing an optimal context.
This context is processed by a generator to obtain a
reward, signifying the quality and relevance of the con-
text, with the feedback loop aiding in further refining
the model’s output and performance.
input text. The objective of the training process is
to minimize the discrepancy between Cextracted and
the ground truth context Ctruth and the loss function
is demonstrated as follows:
min
θ
L(θ) =− 1
N
NX
i=1
C(i)
truth log(fPRCA(S(i)
input; θ))
(1)
where θ represents the parameters of PRCA
In the context extraction stage, PRCA is ini-
tialized from a BART-Large model pre-trained on
CNN Daily Mail dataset (Lewis et al., 2020a).
3.3 Reward-Driven Stage
In the reward-driven stage, the objective is to align
the extracted context Cextracted from the previous
stage with the downstream generator, ensuring that
the text distilled by PRCA serves effectively to
guide the generator’s answering. Given the black-
box nature of the generator, a direct update of
PRCA is not feasible. Therefore, we resort to rein-
forcement learning to optimize PRCA’s parameters.
Specifically, the generator offers rewards to guide
the update of PRCA’s parameter, targeting to im-
prove answer quality. The reward is based on the
ROUGE-L score between the generated answer O



Source: data\tc16_2312.10997v5\referenced_papers\[69]_2310.18347.pdf (Page 0):

PRCA: Fitting Black-Box Large Language Models for Retrieval Question
Answering via Pluggable Reward-Driven Contextual Adapter
Haoyan Yang1,2†, Zhitao Li1, Yong Zhang1, Jianzong Wang1∗,
Ning Cheng1, Ming Li1,3, Jing Xiao1
1Ping An Technology (Shenzhen) Co., Ltd., China
2New York University 3University of Maryland
jzwang@188.com
Abstract
The Retrieval Question Answering (ReQA)
task employs the retrieval-augmented frame-
work, composed of a retriever and generator.
The generator formulates the answer based on
the documents retrieved by the retriever. Incor-
porating Large Language Models (LLMs) as
generators is beneficial due to their advanced
QA capabilities, but they are typically too large
to be fine-tuned with budget constraints while
some of them are only accessible via APIs. To
tackle this issue and further improve ReQA per-
formance, we propose a trainable Pluggable
Reward-Driven Contextual Adapter (PRCA),
keeping the generator as a black box. Po-
sitioned between the retriever and generator
in a Pluggable manner, PRCA refines the re-
trieved information by operating in a token-
autoregressive strategy via maximizing rewards
of the reinforcement learning phase. Our ex-
periments validate PRCA’s effectiveness in en-
hancing ReQA performance on three datasets
by up to 20% improvement to fit black-box
LLMs into existing frameworks, demonstrating
its considerable potential in the LLMs era.
1 Introduction
Retrieval Question Answering (ReQA) tasks in-
volve generating appropriate answers to given ques-
tions, utilizing relevant contextual documents. To
achieve this, retrieval augmentation is employed
(Chen et al., 2017; Pan et al., 2019; Izacard and
Grave, 2021), and comprised of two key compo-
nents: a retriever and a generator. The retriever’s
role is to retrieve relevant documents from a large
corpus in response to the question, while the gener-
ator uses this contextual information to formulate
accurate answers. Such systems alleviate the prob-
lem of hallucinations (Shuster et al., 2021), thereby
enhancing the overall accuracy of the output.
† The work was done when the first author was doing
internship at Ping An Technology (Shenzhen) Co., Ltd., China.
∗Corresponding author: Jianzong Wang.
Current Paradigm 
PRCA 
PRCA-based Paradigm 
Query 
Retriever 
Top K 
Documents 
Query 
+
Generator 
Black-Box 
Model 
Corpus 
Generator 
White-Box 
Model 
Query 
Retriever Corpus 
Top K 
Documents 
Query 
+
Figure 1: A comparison between two paradigms for
information retrieval and generation. The upper section
showcases the traditional method where a query is pro-
cessed by a retriever that scans a corpus to fetch the
Top-K documents and then fed to a white-box genera-
tor. The lower section introduces our proposed PRCA
method, which processes extracted Top-K documents
from the retriever before feeding them to black-box
generator to achieve better performance for in-domain
tasks.
Recent advances in Large Language Models
(LLMs) such as the generative pre-trained trans-
former (GPT) series (Brown et al., 2020; Ouyang
et al., 2022; OpenAI, 2023) have demonstrated
remarkable potential, notably in their zero-shot
and few-shot abilities within the realm of QA
tasks. Owing to these capabilities, LLMs are ex-
cellent choices as generators within the retrieval-
augmented framework. However, due to the vast
parameters of LLMs, fine-tuning them becomes
exceedingly difficult within a limited computation
budget. Furthermore, certain LLMs such as GPT-4
(OpenAI, 2023) are closed-source, making it im-
possible to fine-tune them. To achieve optimal
results on specific datasets, fine-tuning retrieval-
augmented models becomes necessary (Guu et al.,
2020; Lewis et al., 2020b; An et al., 2021). Previ-
ous attempts to integrate LLMs into the retrieval-
augmented framework have met with partial suc-
arXiv:2310.18347v1  [cs.CL]  23 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[69]_2310.18347.pdf (Page 6):

Figure 3: Comparison of performance of different gen-
erators (T5, Phoenix, Vicuna, ChatGLM, and GPT-3.5)
on three benchmark datasets: SQuAD, HotpotQA, and
TopicOCQA. The horizontal axis represents the GPT-4
assessment accuracy. Bars depict the performance levels
of each generator, with green and red arrows indicating
the enhanced or diminished effects due to PRCA inte-
gration, respectively.
documents, T5 performs well because its features
fit well in handling this dataset, capable of directly
extracting answers from the context. But under the
effect of PRCA, the structure of the text might be
altered, and T5’s direct answer extraction may lead
to some errors, thereby reducing performance.
While in a few configurations, the characteristics
of PRCA may have negative effects, for the vast
majority of configurations, our experiments vali-
date that under PRCA-based paradigm, PRCA can
effectively enhance the performance in the ReQA
task, demonstrating robustness.
5.2 Efficiency of PRCA
PRCA represents an effective approach for
enhancing the performance of the ReQA task
without significantly increasing computational
demand. Its efficiency is manifested in optimizing
parameters to achieve superior results and in
simplifying input text, thereby aiding generators in
managing complex text.
Parameter Efficiency Figure 4 portrays a
comparative analysis between the generators,
which gain the maximum improvements with
PRCA, and the GPT-3.5 model which operates
without PRCA, across 3 datasets. PRCA boasts
roughly 0.4 billion parameters, the most signif-
icantly improved generators encompass about
7 billion parameters on average, while GPT-3.5
has approximately 1.75 trillion parameters. As
demonstrated in Figure 4, with a marginal
SQuAD HotpotQA T opicOCQA
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
GPT4 Assessment Accuracy
+12.0 %
+27.1%
+64.5%
Baselines
GPT3.5
Positive  Effect 
of PRCA
Figure 4: Performance comparison between PRCA-
enhanced baseline models and GPT-3.5 across SQuAD,
HotpotQA, and TopicOCQA. Light and dark blue bars
represent baseline and GPT-3.5 performance, while
striped green indicates PRCA’s improvement.
Table 4: PRCA inference speed test results.
Dataset Precision GPU Batch Size Inference Speed
(token/s)
PRCA float32 A100 1 126
PRCA float32 A100 2 231
PRCA float32 A100 4 492
parameter increment, the performance of these
generators improved by 12.0%, 27.1%, and 64.5%
respectively. Hence, PRCA has great potential to
be an efficient way to boost the performance of
ReQA task while keeping computational resources
consumption acceptable. During the inference
process, a fully-trained PRCA will perform only
standard forward propagation and hence introduce
limited impact on inference latency. Our inference
latency test on SQUAD was reported in Table 4.
This low latency ensures that the system maintains
a smooth process without significant delays after
integrating PRCA, underscoring the high efficiency
of PRCA in boosting system performance.
Input Simplification As illustrated in Figure 5,
we analyzed the relationship between reward and
token count during reward-driven stage for a QA
pair in the HotpotQA dataset, with and without
PRCA. There’s a discernible difference in the re-
ward trajectories with and without PRCA. Both
reward curves ascend with the increase in token



### Claim 44/179

#### Claim Text
BGM [26] keeps the retriever and LLM fixed,and trains a bridge Seq2Seq model in between.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[26]_2401.06954.pdf (Page 1):

Repeated information is generally considered detri-
mental for retrieval systems (Xia et al., 2017), but it
may be useful to LLMs for weighting the relevance
of context items.
We empirically investigate this preference gap,
focusing specifically on ranking and selection. As
shown in Fig. 1, when we randomize the ordering
of top-5 retrieved items (in our case, passages), the
performance of RAG only varies by around 1% 1.
However, the variation exceeds 5% when the LLM
is only presented with the top-1 passage under each
order (therefore it encounters different selections
of information). This indicates the general belief in
ranking does not always apply to LLMs, and that
the selection of information could be more crucial.
This finding confirms the existence of the prefer-
ence gap between retrievers and LLMs, and it is
critical to bridge this preference gap to enhance
the performance of RAG. To the best of our knowl-
edge, this is a novel insight that may guide future
designs of RAG systems.
Existing work has tried to finetune the LLMs
to align with the retriever or adjust the retriever to
align with the LLM. However, finetuning LLMs,
especially at the scale of GPT-4 or Palm 2, is of-
ten expensive. Similarly, it is difficult to adjust
production-level retrievers such as Google or Bing.
Even when the retriever is adjustable, existing ef-
forts often focus on re-ranking the retrieved re-
sults and fail to address other aspects of preference
such as selection or repetition. Instead, we pro-
pose a novel and practical framework called BGM
(Bridging the Gap between retrievers and LLMs),
which keeps the retriever and LLM fixed and trains
a bridge model in between. The bridge model aims
to transform the retrieved information into a format
that LLMs prefers and can effectively work with.
Without loss of generality, we structure the
bridge model as asequence-to-sequence (seq2seq)
model, which allows dynamically selecting items,
re-ranking them, and potentially broader opera-
tions like repeating some of them in the retrieval-
augmented prompt. Training such a bridge model
is challenging as there are usually no ground truth
labels on ideal item sequences for retrieval aug-
1We note that this finding is different from (Liu et al.,
2023), where a “loss in the middle” phenomenon is observed,
meaning that the LLM is better at using the relevant infor-
mation at the beginning or end of its input context. This is
probably due to different numbers of retrieved items. Liu et al.
(2023) used 20 documents, whereas we use 5 passages. Re-
gardless, both findings indicate that there exists a preference
gap between the retrievers and LLMs, which we aim to bridge.
mentation. Existing work has tried to derive super-
visory signals for ranking from RAG’s downstream
task performance, such as perplexity distillation
(Izacard et al., 2022). Nevertheless, these meth-
ods only provide pointwise supervisory signals for
each item independently. Directly applying the
same idea to obtain sequential supervision is infea-
sible, since it would require feeding all possible
item sequences into the LLM to obtain perplexity.
We developed a greedy search approach to solve
this problem (Sec. 4.1). Moreover, we find sequen-
tial supervision can be too sparse to effectively
train such a seq2seq model (Table 5). To address
this issue, we employed reinforcement learning
(RL) on the SL trained bridge model, regarding the
downstream task performance as the reward and
the bridge model as a policy model. Chaining SL
and RL provides increased supervision from the
downstream task. It also offers the flexibility to ex-
plore more advanced strategies, such as repetition,
in forming the optimal passage sequence.
Our experiments reveal that BGM can enhance
the performance of various downstream tasks, such
as Question Answering (QA) and personalized gen-
eration, across a spectrum of datasets, from public
QA and amazon reviews to private email conver-
sations. Notably, the modified passages retrieved
by BGM surpass the performance of strong retriev-
ers and baseline reranking models. This under-
scores the significance and promise of the “bridge”
approach in the realm of RAG. In summary, our
contributions can be summarized as follows:
• We empirically establish the existence of the
preference gap between retrievers and LLMs,
and introduce BGM to address this gap.
• We propose a seq2seq bridge model to jointly
accomplish reranking and selection, adapting
the retrieved information to be LLM-friendly.
We employ a SL and RL training scheme to
optimize this adaptation process.
• We evaluate BGM with diverse tasks, includ-
ing QA and text generation, with publicly
available and personalized datasets. The eval-
uation underscores the effectiveness of BGM
in bridging the preference gap and improving
RAG performance in downstream tasks.
2 Related Work
Retrieval-augmented Generation (RAG). Aug-
menting LLMs with relevant information retrieved



Source: data\tc16_2312.10997v5\referenced_papers\[26]_2401.06954.pdf (Page 7):

Model NQ HotpotQAEmail Book
Metric EM EM BLEU BLEU
Test on Palm2-S
BGM (in-domain)45.37 35.64 10.42 12.07
BGM
(Trained on NQ) — 33.42 5.66 11.22
BGM
(Trained on Email)35.59 27.98 — 11.38
Test on Palm2-XXS
BGM
(Trained with Palm2-XXS)39.88 28.69 — —
BGM
(Trained with Palm2-S)30.63 24.55 — —
Table 8: Ablation - the generability of BGM across
various datasets and LLM sizes.
is inadequate, highlighting the necessity of incor-
porating RL. Note that removing SL is likely inef-
fective, leading to a poorly initialized policy model
for RL with an excessively large search space.
(4) How does the size of bridge model affect
the final performance? In Table 2, we utilized
Flan-T5-XXL (11B) as the bridge model, which is
already smaller than the PaLM2-S. However, we
are curious about the feasibility of using an even
more lightweight LM as the bridge model. The
ablation study shown in Table 6 presents the out-
comes of employing bridge models of various sizes.
It is evident that all three sizes (large, XL, and
XXL) surpass the performance of the setup without
a bridge model (i.e., GTR), with the largest size
yielding the best results. This demonstrates that a
bridge model is beneficial even at a smaller scale,
and that larger sizes lead to further improvements.
(5) How does the size of LLMs affect the final
performance? In Table 2, Palm2-S was used as
the LLM. We are interested in evaluating the effec-
tiveness of the bridge model with different sizes of
LLMs. We conducted experiments using Palm2-
XXS in Table 7. It is important to note that the
smaller LLM struggles with personalized genera-
tion datasets (i.e., Email and Book), resulting in
BLEU scores lower than 1%. We opted not to re-
port these results as they may not accurately reflect
the trend. However, observations from NQ and Hot-
potQA suggest that BGM significantly outperforms
all baselines by a large margin. This indicates that
BGM is effective even with a smaller LLM.
(6) Can a bridge model generalize to differ-
ent datasets and LLMs? Our experiments above
demonstrate the effectiveness of the bridge model
when trained “in-domain” (i.e., trained and tested
on the same dataset) using PaLM2-S as the LLM.
A more ambitious goal is to extend this perfor-
mance to new datasets or LLMs without extra train-
ing. We conducted ablation experiments to investi-
gate bridge model generalizability: Table 8’s upper
section shows the results across different datasets.
Row 2 shows the performance of the bridge model
when trained exclusively on the NQ dataset and
then tested on three other unseen datasets. Sim-
ilarly, row 3 shows training BGM solely on the
Email dataset and testing it on the other three. In
all cases, performance falls short of BGM’s when
trained and tested on the same datasets. This is
expected, given the lack of techniques for dataset
generalization, a topic we leave for future work.
In the lower section of Table 8, we present the
results of BGM when tested on Palm2-XXS, but
trained on Palm2-S. Comparing with results of
BGM both trained and tested on Palm2-XXS (see
row 1 of the bottom section), the mismatch between
the training and testing LLMs leads to a significant
decline in performance. This suggests that BGM’s
ability to generalize across different LLMs is cur-
rently limited. Addressing this is considered an
important direction for future research.
(7) Case Studies. We provided examples of
GTR, PSR, and BGM in Table 9 in Appendix for
the NQ dataset. For question I, both GTR and PSR
yield the same incorrect output, even though both
include a relevant passage for RAG. Only BGM
provides the correct answer, indicating that addi-
tional irrelevant context can be noisy and detrimen-
tal to RAG’s performance. In question II, none
of the candidate passages contain the answer (they
discuss FaZe Clan and the number of subscribers,
but do not identify who has the most subscribers).
GTR and PSR provide incorrect answers, as their
additional context is unhelpful. In contrast, BGM
opts not to select any passages and answers the
question using its own memory, resulting in the
correct answer. This demonstrates that retrieval-
augmented processes are not always necessary, and
BGM is capable of handling such cases.
6 Conclusion
This paper demonstrates the need to bridge the pref-
erence gap between retrievers and LLMs, which
has various levels of impact on ranking and se-
lection in RAG systems. We propose a bridge
model, BGM, to adapt the output of a frozen re-
triever for frozen LLMs, formatting the task as a
seq2seq problem. BGM chains supervised and re-
inforcement learning, for dense supervisions and
end-to-end training. Our extensive experiments
validate BGM’s effectiveness.



Source: data\tc16_2312.10997v5\referenced_papers\[26]_2401.06954.pdf (Page 3):

does it specify what the bridge model should be.
3 Problem Formulation
Retriever. Given an input x, the retriever aims
to retrieve a ranked list of passages from a cor-
pus D = {di}m
i=1 that are relevant to x. In this
work, we assume a typical scenario of employing
a frozen dense retriever. Typically, a dual encoder
architecture is applied, where an encoder is used
to encode both the input context x and the passage
d. Specifically, the encoder maps each passage to
an embedding E(d). The similarity between in-
put and passage embedding is computed by their
cosine similarity,
s(d, x) =cos(E(d), E(x)). (1)
The top-k passages that have the highest simi-
larity scores when compared with the input x are
retrieved in this step,
(dretr.
j )k
j=1 = Top-K({s(d, x)}m
i=1). (2)
Bridge Model for RAG. The retrieved top-K
passages provide richer information about the orig-
inal input/query x to help the LLM to make a better
prediction on downstream tasks. A bridge model
B adapts the retrieved passages to a sequence of
passages that is LLM-friendly. As mentioned in
the Sec. 1, the bridge model is a seq2seq model. It
takes all the retrieved passages (dretr.
j )k
j=1 as well
as the query x as input, and outputs the adapted
passages (dbdr.
j )n
j=1,
(dbdr.
j )n
j=1 = B

x, (dretr.
j )k
j=1

. (3)
This formulation is general enough as the
seq2seq model automatically considers ranking by
generating the next token based on the preceding
one, selection by placing the end-of-sentence token
in the appropriate position, and repetion by gen-
erating the same passage ID (as explained in the
following paragraph). Note that n may be smaller
or larger than k due to selection and repetition.
Before concatenating the query and passages as
bridge model’s input, we prepend each passage
with a unique sentinel token as its passage ID, e.g.,
[query][id1]dretr.
1 [id2]dretr.
2 . In this way, the model
only needs to generate the passage IDs instead of
the actual passage content, which is much more
efficient and avoids making unfaithful changes to
the retrieved passages. We then convert the ob-
tained passage IDs to the corresponding passages
for downstream processing.
Retrieval-augmented generation with bridge.
We concatenate adapted passages from the bridge
model, (dbdr.
j )n
j=1, with the input x, and fed the
resulting long sequence into the LLM as context to
obtain the output for downstream tasks.
4 Training the Bridge Model
In Eq. 3, we format the bridge model as seq2seq
model. As mentioned in Sec. 1, its training is chal-
lenging due to the lack of supervision for passage
sequences, the infeasibility of directly applying
existing methods that provide only point-wise su-
pervision, and the sparsity of sequential supervi-
sion. For effective training, we propose to chain
supervised learning (SL) and reinforcement learn-
ing (RL), where SL aims to reduce the search space
of RL and provides a reasonably good initial model
that does ranking and selection. RL aims to opti-
mize the policy model, i.e., bridge model, for the
downstream task. Fig. 3 shows an overview.
Algorithm 1: Synthesis SPS
Input: (dretr.
j )n
j=1, R(·)
Output: (dsilv.
j )s
j=1
1 dsilv. ← ();
2 Rsilv. ← R(dsilv.);
3 Rbest ← −∞;
4 while true do
5 for c ← dretr.
1 to dretr.
n where c ̸∈ dsilv. do
6 d′ ← add_to_seq(dsilv., c);
7 Rcur ← R(d′);
8 if Rcur > Rbest then
9 cbest ← c;
10 Rbest ← Rcur;
11 if Rbest > Rsilv. then
12 dsilv. ← add_to_seq(dsilv., cbest);
13 Rsilv ← Rbest;
14 Rbest ← −∞;
15 else
16 break;
4.1 Supervised Learning
To conduct SL, the ground-truth passage sequence
is required for each query. Existing approaches,
which focus on obtaining the relevance score of
each passage (see Section 1), are not applicable as
we need to determine which combination of pas-
sages is most effective for the downstream tasks.
To address this, we propose to synthesis silver pas-
sage sequence (SPS) by selecting only the useful



Source: data\tc16_2312.10997v5\referenced_papers\[26]_2401.06954.pdf (Page 4):

Figure 3: Illustrating the training of BGM: Step 1: First, we prepare the silver passage sequence for supervised
learning (SL) through a greedy search on the retrieved passages. Step 2: These silver passage sequences are then
used for the supervised training of the bridge model. The bridge model is a seq2seq model that takes the query and
passages with prepended passage IDs as input and outputs the passage IDs. Step 3: Finally, the SL-trained bridge
model is treated as a policy model and is further trained using reinforcement learning.
passages. This is done by greedy search that incre-
mentally selects the next passage that can maximize
the downstream task performance.
Synthesising SPS using greedy search. We de-
note the downstream task performance when using
a given passage sequence for RAG as R(·). For
the edge case where the passage sequence is empty,
R(∅) simply denotes the task performance without
retrieval augmentation (i.e., no passage used). We
start from an empty SPS dsilv. = ∅, and iteratively
add the next best candidate passage to the sequence,
measured based on the resulted task performance
R(dsilv.). We stop until no improvement can be
made to R(dsilv.). Algorithm 1 shows the pseudo-
code for synthesising SPS. The training with the
SPS is achieved by applying cross-entropy loss.
4.2 Reinforcement Learning
Although SL can already help training the bridge
model, it is still ineffective – we observe using SL
alone results in mixed performance (see Table 5).
This is attributed to sparse supervision (see Sec. 1)
and the lack of end-to-end training on downstream
results.
To address these issues, we apply RL to con-
tinue the training of the bridge model. In SL, we
only consider permutations or deletions in the SPS,
while RL can accommodate more complex manip-
ulations that an optimal passage sequence might
require, such as repetition. Additionally, RL pro-
vides enhanced supervision beyond the silver se-
quences through the reward from sampled passage
sequences. Using the performance of downstream
task as the reward, the bridge model is trained in
an end-to-end manner. Specifically, our task can
be formulated as an RL problem – Reward is the
performance of the downstream task, usually mea-
sured based on certain ground-truth labels, e.g., Ex-
tract Match or BLEU. Policy model is the bridge
model that need to be trained. Action space is re-
stricted to passage IDs (Sec. 3) as we are interested
in organizing rather than revising the retrieved pas-
sages. In training, the reward objective can be
optimized by any off-the-shelf RL algorithm, e.g.,
proximal policy optimization (PPO).
5 Experiments
5.1 Datasets and Baselines
Datasets. We consider four datasets, ranging from
popular QA datasets to personalized generation
datasets. We also include one dataset that con-
tains private email conversations (Avocado Email),
which is unlikely to be included in the LLM’s pre-
training datasets. This will further help us investi-
gate the effectiveness of our proposed BGM model,
as the LLM will have to rely on the retrieved pas-



Source: data\tc16_2312.10997v5\referenced_papers\[26]_2401.06954.pdf (Page 2):

Figure 2: Differing from previous RAGs that update
LLMs, retrievers, or both, BGM connects a frozen LLM
and a frozen retriever through a lightweight bridge
model which adapts the retrieved information to the
LLM’s preference. This makes BGM applicable to
“large” LMs and off-the-shelf retrievers.
from various knowledge sources is proven effective
across numerous NLP tasks, including language
modeling (Borgeaud et al., 2022; Khandelwal et al.,
2020; Shi et al., 2023b), question answering (Lewis
et al., 2020; Izacard et al., 2022; de Jong et al.,
2023; De Jong et al., 2023; Shi et al., 2023b; Guu
et al., 2020; Izacard and Grave, 2020; Xu et al.,
2023), fact versification (Lewis et al., 2020) and
text generation (Lewis et al., 2020). Specifically,
RAG utilizes input as a query and comprises two
main components: (1) a retriever retrieves a set
of items from a side corpus. Particular items may
vary across different tasks, including documents,
passages, or even tokens. In this study, we focus on
retrieving passages; and (2) a LLM incorporates
the retrieved items, as additional information in the
input context, and makes final predictions.
A fundamental question in this process arises
regarding the disparate preferences between LLMs
and retrievers, as LLMs performing optimally only
when their preferences are satisfied. Bridging the
preference gap is crucial. Depending on which
components are subject to updates, this challenge
can be categorized into three families.
Finetuning retrievers and LLMs jointly. This
is the most widely used setting of RAG (Izacard
et al., 2022; Khandelwal et al., 2020; Wu et al.,
2022; Guu et al., 2020; Lewis et al., 2020). How-
ever, most prior work is based on relative small
LMs (< 1B). For example, Altas (Izacard et al.,
2022) finetunes LLM (T5 (Raffel et al., 2020a))
and retriever (Contriever (Izacard et al., 2021))
jointly by leveraging the LLM to provide super-
visory signal to train the retriever. RAG (Lewis
et al., 2020) uses a tunnable query encoder and
DPR (Karpukhin et al., 2020) as retriever, BART
as LLM, and design an end-to-end schema to train
the query encoder and the LLM.
Finetuning LLMs only. Updating retrievers is
not always desirable as it is costly and requires
the document index to be periodically updated. To
bridge the preference gap, it is also possible to only
update the LLMs. FiD (Izacard and Grave, 2020)
takes the retrieved documents and query as input,
finetunes the LLM to adapt to the external infor-
mation. Similarly, Lummen (De Jong et al., 2023)
and Glimmer (de Jong et al., 2023) improve FiD
via adding reranker and pre-encoding memeory.
Finetuning retrievers only. Although above
systems have shown improves results, they are not
always applicable in practice. Many LLMs (espe-
cially larger ones) are only available as black-box
APIs. Given this constraint, a natural approach
is to only update the retrievers to ensure they can
retrieve passages that are more compatible with
LLMs. REPLUG (Shi et al., 2023b) adapts a sim-
ilar idea as Atlas but fix the LM. RECOMP (Xu
et al., 2023) trains a compressors to summarize
the retrieved document from retriever. However,
this family of models is incapable of performing
any sample-level selection and can only choose top
passages by setting a fixed threshold.
Unlike the existing approaches, BGM does not
fine-tune the LLM or the retriever and instead
employs a bridge model in between (Fig. 2).
RL for information retrieval. Before the LLM
era, RL has been used in information retrieval (IR)
(Xia et al., 2017; Wei et al., 2017; Zeng et al., 2018;
Xu et al., 2020). The core approach was to frame
the IR problem as a Markov Decision Process and
apply an RL algorithm to solve it. Typically, an
IR task would be structured to determine which
document to select for a ranking position, using
ranking metrics such as DCG as the reward. None
of these existing studies explore the application of
RL in the context of RAG.
In the LLM era, RL has been used in query
rewriting for retrieval (Wu et al., 2021; Nogueira
and Cho, 2017; Adolphs et al., 2021), where a
black-box retriever is assumed. This is a different
problem from RAG. Bacciu et al. (2023) suggest us-
ing RL to fine-tune retriever in the RAG context in
their opinion paper, not supported by experiments.
Their work does not recognize the importance of
bridging the gap between retrievers and LLMs, nor



### Claim 45/179

#### Claim Text
Furthermore, PKG 10 introduces an innovative method for integrating knowledge into white-box models via directive fine-tuning [75].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[27]_2310.01352.pdf (Page 3):

Published as a conference paper at ICLR 2024
Table 1: Our intruction tuning datasets. All datasets are downloaded from Hugging Face (Lhoest
et al., 2021), with the exception of those marked with ‡, which are taken from Iyer et al. (2022).
Task HF identifier Dataset name DL DR #Train
Dialogue oasst1 OpenAssistant Conversations Dataset (K ¨opf et al., 2023)✓ ✓ 31,598
Open-Domain
QA
commonsenseqa CommonsenseQA (Talmor et al., 2019) ✓ ✓ 9,741
mathqa MathQA (Amini et al., 2019) ✓ ✓ 29,837
webquestions Web Questions (Berant et al., 2013) ✓ ✓ 3,778
wikiqa Wiki Question Answering (Yang et al., 2015) ✓ ✓ 20,360
yahooanswersqa Yahoo! Answers QA ✓ ✓ 87,362
freebaseqa FreebaseQA (Jiang et al., 2019) ✓ 20,358
msmarco* MS MARCO (Nguyen et al., 2016) ✓ 80,143
Reading Com-
prehension
coqa Conversational Question Answering (Reddy et al., 2019) ✓ 108,647
drop Discrete Reasoning Over Paragraphs (Dua et al., 2019) ✓ 77,400
narrativeqa NarrativeQA (Ko ˇcisk´y et al., 2018) ✓ 32,747
newsqa NewsQA (Trischler et al., 2017) ✓ 74,160
pubmedqa PubMedQA (Jin et al., 2019) ✓ ✓ 1,000
quail QA for Artificial Intelligence (Rogers et al., 2020) ✓ 10,246
quarel QuaRel (Tafjord et al., 2019) ✓ ✓ 1,941
squadv2 SQuAD v2 (Rajpurkar et al., 2018) ✓ 130,319
Summarization cnndailymail CNN / DailyMail (Hermann et al., 2015) ✓ 287,113
Chain-of-
thought
Reasoning
aquarat‡ Algebra QA with Rationales (Ling et al., 2017) ✓ 97,467
ecqa‡ Explanations for CommonsenseQ (Aggarwal et al., 2021)✓ 7,598
gsm8k‡ Grade School Math 8K (Cobbe et al., 2021) ✓ 7,473
compeitionmath‡ MATH (Hendrycks et al., 2021b) ✓ 7,500
strategyqa‡ StrategyQA (Geva et al., 2021) ✓ 2,290
* We only used the question-and-answer pairs in the MS MARCO dataset.
DL, we retrieve the top- ˜k relevant text chunks Ci ⊂ Cbased on xi. Mirroring the inference-time
handling, for each retrieved chunk cij ∈ Ci, we create a separate fine-tuning example by prepending
it to the instructions as a background field, resulting in ˜k independent fine-tuning instances per
original example: {(cij ◦ xi, yi)|j = 1. . .˜k}.4
We fine-tune the language model using the next-token prediction objective and minimize the loss
from tokens in the output segment of each instance (Iyer et al., 2022):
L(DL) =−
X
i
X
j
log pLM (yi|cij ◦ xi). (3)
Integrating in-context retrieval augmentation during fine-tuning gives a twofold benefit. First, it
adapts the LLM to better utilize relevant background knowledge to make a prediction. Secondly,
even state-of-the-art retrievers can falter and return inaccurate results. By training the LLM to make
correct predictions when a wrong retrieved chunk is given, we enable the LLM to ignore misleading
retrieval content and lean into its parametric knowledge in such cases. The efficacy of this fine-
tuning strategy is empirically demonstrated in §5.1.
2.4 R ETRIEVER FINE -TUNING
In addition to fine-tuning the language model with retrieval augmentation, we also fine-tune the
retriever to better align its output with the language model. In particular, we adopt a generalized
version of LSR ( LM-Supervised Retrieval, Shi et al., 2023b) training that leverages the language
model itself to provide supervision for retriever fine-tuning.
For a training sample (x, y) in the retriever fine-tuning dataset DR, we define the LSR score for a
retrieved chunk c as follows:
pLSR(c|x, y) = exp (pLM (y|c ◦ x)/τ)P
c′∈C exp (pLM (y|c′ ◦ x)/τ) ≈ exp (pLM (y|c ◦ x)/τ)P
c′∈C′ exp (pLM (y|c′ ◦ x)/τ), (4)
where τ is a temperature hyperparameter, and C′ ⊂ Cdenotes the top-k retrieved chunks for x. A
higher LSR score indicates that c is more effective at improving the language model’s chance of
4The exceptions are summarization tasks and RC tasks with context dependent questions (e.g. “when was
the writer born?”), where we do not perform retrieval and create the fine-tuning instances using the given
background text instead. For RC tasks with self-contained questions, we use the retrieved chunks in addition to
the given background text to create fine-tuning instances, resulting in ˜k + 1of them per original example.
4



Source: data\tc16_2312.10997v5\referenced_papers\[27]_2310.01352.pdf (Page 18):

Published as a conference paper at ICLR 2024
A R ETRIEVAL CORPUS
We combine the text chunks from the Dec. 20, 2021 Wikipedia dump released by Izacard et al.
(2022b) with additional ones from the 2017-2020 CommonCrawl dumps. The Wikipedia dump
includes lists and infoboxes in addition to regular articles. The articles are split by section, where
long sections are further split into text chunks of equal sizes and contain less than 200 words, leading
to a total of 37M text chunks. We randomly sample a subset of articles from the CommonCrawl
dumps, and split them into equal-sized text chunks that contain less than 100 white-space-separated
words, leading to a total of 362M text chunks.
We use a GPU-based exact k-nearest-neighbor search index implementation 13 released by Izacard
et al. (2022b).
B I MPLEMENTATION DETAILS
Fine-tuning Dataset Selection Prior work (Chung et al., 2022b; Iyer et al., 2022) have demon-
strated that jointly fine-tuning the language model on a diverse collection of instruction-based
datasets leads to improved model generalization for unseen instructions. We adopt a similar strat-
egy by combining five categories of fine-tuning tasks to enhance the language model’s knowledge
utilization (dialogue, open-domain QA, chain-of-thought reasoning) and to improve its contextual
awareness for prediction generation (reading comprehension, summarization). These categories
were selected due to their representativeness of practical knowledge-intensive language tasks.
Retrieval-augmented LM Fine-tuning We use the top-3 retrieved text chunks for a given exam-
ple (i.e. ˜k = 3) to generate the fine-tuning instances. To improve fine-tuning efficiency, we pack
multiple examples up to the language model context window limit (2048 tokens). Each example
is demacrate by a pair of <bos> and <eos> tokens, and we adopt the document attention mask-
ing (Iyer et al., 2022) such that a token only attends to the previous tokens in the same example. We
use a dataset mixture that contains 10% unsupervised text and 5% OASST-1 data. For the remaining
datasets, we establish a cap on the number of examples per dataset at η = 7500based on the model
performance on our development set. 14 We then randomly sample batches in accordance with this
adjusted mixture probability.
We fine-tune the 7B, 13B and 65B LLAMA models using 8, 16 and 64 A100 GPUs, respectively. The
fine-tuning hyperparameters are detailed in Table 8. Similar to Zhou et al. (2023), we found that the
best generalization performance on the dev set can be achieved using a small number of fine-tuning
steps. We evaluate the models every 100 steps, and select the best checkpoint based on the average
dev set performance over the 6 development KILT tasks shown in Table 11 (early stopping).
Table 8: Hyperparameters for retrieval-augmented LM fine-tuning.
Model peak lr end lr lr scheduler warm-up # steps early
stopping batch size model
parallel seq len
RA-DIT 7B 1e-5 1e-7 cosine 200 500 500 64 1 2048
RA-DIT 13B 1e-5 1e-7 cosine 200 500 400 128 2 2048
RA-DIT 65B 1e-5 1e-7 cosine 200 500 300 128 8 2048
64-shot Eval Task Fine-tuning Table 9 summarizes our hyperparameters for 64-shot fine-tuning
on the 9 KILT eval tasks shown in Table 12 except for MMLU. Given the small amount of examples
used (64 ×9 = 576), we fine-tune for a significantly less number of steps at this stage without using
warm-up. We evaluate the model every 50 steps, and select the best checkpoint based on the average
dev set performance over the 6 development KILT tasks shown in Table 11.
Retriever Fine-tuning We employ both unsupervised text and downstream tasks for retriever fine-
tuning. For the corpus data, we randomly sample 900k text chunks from our retrieval corpus to
13https://github.com/facebookresearch/atlas
14We did not thoroughly tune this parameter to avoid overfitting to the development sets.
19



Source: data\tc16_2312.10997v5\referenced_papers\[9]_2311.03758.pdf (Page 3):

WWW ’24 Companion, May 13–17, 2024, Singapore, Singapore. Peng, et al.
Mix௦௙௧OtherTask 3௠௦௙௧Rejection SamplingQQ𝑥 QQ𝑦OtherTask 2OtherTask 1Multi-instruction Supervised Fine TuningLLMQueriesRewritesInputNext Token PredictionMulti-instruction SFT Data 𝑥  𝑦ଵ 𝑦ଶ 𝑦௞Beam SearchQueriesInputTop kSFT Model
Offline SystemInputInput as Reference 𝒚𝟏≻ 𝒚𝟐… ≻ 𝒚𝒌…Partial OrderOffline FeedbackObjective Alignment 𝑦ଵ 𝑦ଶ 𝑦௞𝑥𝑥𝑥…InputNext Token Prediction 𝜋(𝑦ଵ|𝑥) 𝜋(𝑦ଶ|𝑥) 𝜋(𝑦௞|𝑥)QueriesGeneration Probability≻≻+ Alignment LossProbability CalibrationPartial OrderOptimization ObjectiveSFT Model…
Figure 2: Framework of BEQUE.
D𝑠𝑓𝑡 = {(𝑐𝑜𝑛𝑐𝑎𝑡(𝑥𝑖,E𝑥),𝑦𝑖)|𝑁𝑠𝑓 𝑡
𝑖=1
such that 𝑥𝑖,𝑦𝑖 ∈D𝑟,𝑖𝑛𝑐𝑟 (𝑥𝑖,𝑦𝑖)> 𝜏𝑖𝑛𝑐𝑟},
(3)
where, E𝑥 is the interacted product title list of query 𝑥, 𝑖𝑛𝑐𝑟(·)
and 𝜏𝑖𝑛𝑐𝑟 denote the increment method and its threshold of query-
rewrite pair, respectively. The detail of the function𝑖𝑛𝑐𝑟(·)is dis-
cussed in the Section 3.3.
Auxiliary Task Datasets: In order to further enhance LLMs’ ability
of comprehending long-tail queries, we have gathered three high-
related task datasets in the context of query rewriting. These tasks
encompass quality classification, product title prediction, and Chain-
of-thought. 1) To tackle the quality classification task, our approach
began with the extraction of query pairs from online logs. These
query pairs were then subjected to human annotation to determine
if they met the data requirements specified for SFT. 2) For the
product title prediction task, we chose the most recent interacted
product under the query as the reference, forming <query, product
title> pairs. 3) As for the CoT task, we employed the original online
queries to construct prompts for human evaluators. It’s noteworthy
that these evaluators were not only tasked with providing query
rewrites aimed at improving the quality of query retrieval but were
also expected to articulate their thought processes, explaining the
rationale behind their specific revisions. The details prompt design
for above auxiliary tasks are shown in Table 1. The collected data
were later integrated into the rewriting task to form the final dataset,
which was then randomly shuffled for the subsequent SFT stage.
Supervised Fine Tuning : The process of generating text with
a condition language model can be viewed as a constrained auto-
regressive sampling strategy. Given a prompt𝑥and its gold standard
𝑦, The training objective is to maximize the conditional probability
𝑝(𝑦|𝑥). Considering our multi-instruction SFT dataset and assum-
ing that 𝑝(𝑦|𝑥) = Î
𝑖=1 𝑝(𝑦𝑖|𝑦0:𝑖−1,𝑥), the training objective of
rewriting model involves minimizing the negative log likelihood:
LSFT (𝜃)= −E(𝑥,𝑦)∼D𝑚𝑠𝑓 𝑡
∑︁
𝑖=1
log 𝜋(𝑦𝑖 |𝑦0:𝑖−1,𝑥; 𝜃), (4)
where D𝑚𝑠𝑓𝑡 denotes multi-instruction SFT data, which consists of
a mixture of query rewriting datasetD𝑠𝑓𝑡 and a variety of auxiliary
task datasets, 𝜋(·)and 𝜃 denote our query rewriting model and its
parameters. It is worth mentioning that LLMs typically have fixed
prefixes in the prompt𝑥. To avoid introducing noise, we disregarded
the losses coressponding to 𝑥.
3.3 Offline Feedback
Currently, most alignment methods [1, 12, 23, 27] rely on manual
annotation and training-based reward models. However, we argue
that these approaches can be easily influenced by the quality of
annotations and the effectiveness of the reward model training,
which often leads to inaccurate reflection of response scores and
compromises the learning of the generation model. To address this
issue, we propose a feedback system based on the Taobao search
engine, which provides more accurate rewrite scores.



Source: data\tc16_2312.10997v5\referenced_papers\[27]_2310.01352.pdf (Page 20):

Published as a conference paper at ICLR 2024
Table 10: Instruction template used for our fine-tuning datasets. <inst s>, <inst e> and
<answer s> are special markers denoting the start and the end of a field.
Category Instruction Tuning Template Query Template
Dialogue Background: {retrieved passage}\n\nQ:{turn1}A:{turn2}Q:
{turn3}A:...
{turn1} {turn2} {turn3}...
Open-domain QA Background:{retrieved passage}\n\n<insts> {question}
<inste> <answers>{answer}
{question}
Reading Compre-
hension
Background:{context}\n\n<insts>{question} <inste>
<answers>{answer}
{question}
Summarization Background: {context}\n\nSummarize this article:<inste>
<answers>{summary}
Chain-of-thought
Reasoning
Background:{retrieved passage}\n\n<insts>{instructions}
{reasoning chain}<answers>{answer}
{question}
Table 11: Our evaluation datasets. † indicates the development datasets we used to select fine-tuning
hyperparameters.
Task Dataset name Acronym Metric Score
Open-domain
QA
MMLU (?) MMLU Acc. nll
Natural Questions (Kwiatkowski et al., 2019) NQ EM nll
TriviaQA (Joshi et al., 2017) TQA EM nll
†HotpotQA (Yang et al., 2018) HoPo EM nll
ELI5 (Fan et al., 2019) ELI5 Rouge-L nll token
Fact Checking†FEVER (Thorne et al., 2018) FEV Acc. nll
Entity Linking†AIDA CoNLL-Y AGO (Hoffart et al., 2011) AIDA Acc. nll
Slot Filling
†Zero-Shot RE (Levy et al., 2017) zsRE Acc. nll
†T-REx (Elsahar et al., 2018) T-REx Acc. nll
Dialogue †Wizard of Wikipedia (Dinan et al., 2019) WoW F1 nll token
Commonsense
Reasoning
BoolQ (Clark et al., 2019) BoolQ Acc. nll compl
PIQA (Bisk et al., 2020) PIQA Acc. nll char
SIQA (Sap et al., 2019) SIQA Acc. nll char
HellaSwag (Zellers et al., 2019) HellaSwag Acc. nll char
WinoGrande (Sakaguchi et al., 2019) WinoGrande Acc. nll char
ARC-Easy (Clark et al., 2018) ARC-E Acc. nll char
ARC-Challenge (Clark et al., 2018) ARC-C Acc. nll char
OpenBookQA (Mihaylov et al., 2018) OBQA Acc. nll compl
computational cost. For test set evaluation, we use the full set to ensure fair comparison with pre-
vious work. The language model instruction templates and retriever queries used in our evaluation
are shown in Table 12. We randomly select few-shot examples from the official training splits of the
KILT tasks, except for FEV , NQ and TQA, where we use the 64-shot examples released by Izacard
et al. (2022b). For these three datasets, we also ensure that the 5-shot examples are subsets of the 64
examples. For retrieval augmented models, we use the top-1 relevant chunk to augment the prompt
for each in-context few-shot example.
E A DDITIONAL EXPERIMENTS
E.1 S CALING LAWS OF RETRIEVAL AUGMENTED LANGUAGE MODEL FINE -TUNING
We investigate the impact of the base language model size when retrieval-augmented instruction
tuning is applied, and summarize the results in Figure 2. We combine the fine-tuned models with
the base DRAGON + retriever in this set of experiments.
Overall, all models substantially benefit from retrieval augmentation, with smaller models witness-
ing even bigger improvements. We further note that retrieval augmentation can be an effective
strategy for enhancing the performance of smaller models (hence reducing pre-training and infer-
ence costs), given the 7B model leveraging > 1 retrieved chunks surpassed the performance of the
vanilla 65B model on several tasks. This trend also differs across tasks. For tasks that primarily
21



Source: data\tc16_2312.10997v5\referenced_papers\[27]_2310.01352.pdf (Page 1):

Published as a conference paper at ICLR 2024
Figure 1: The RA-DIT approach separately fine-tunes the LLM and the retriever. For a given exam-
ple, the LM-ft component updates the LLM to maximize the likelihood of the correct answer given
the retrieval-augmented instructions (§2.3); the R-ft component updates the retriever to minimize
the KL-Divergence between the retriever score distribution and the LLM preference (§2.4)
In this work, we show lightweight instruction tuning (Chung et al., 2022b; Iyer et al., 2022; Zhou
et al., 2023) alone can significantly boost the performance of RALMs, especially in knowledge
intensive scenarios. We propose Retrieval-Augmented Dual Instruction Tuning (RA-DIT), an ap-
proach that retrofits any LLM with retrieval capabilities via fine-tuning over a set of tasks selected
to cultivate knowledge utilization and contextual awareness in the language model predictions. We
initialize the framework using pre-trained LLAMA (Touvron et al., 2023a) and a state-of-the-art dual-
encoder based dense retriever, DRAGON + (Lin et al., 2023). Following Shi et al. (2023b), we retrieve
relevant text chunks based on the language model prompt. Each retrieved chunk is prepended to the
prompt, and the predictions from multiple chunks are computed in parallel and ensembled to pro-
duce the final output.
We perform instruction-tuning in two separate steps. For language model fine-tuning (LM-ft), we
adopt the label-loss objective (Chung et al., 2022b; Iyer et al., 2022) and augment each fine-tuning
prompt with a retrieved “background” field prepended to the instructions (Figure 1). We also lever-
age the design of existing NLP tasks and populate this field with the ground truth context for tasks
such as reading comprehension and summarization. By incorporating the background text during
fine-tuning, we guide the LLM to optimally utilize the retrieved information and ignore distract-
ing content (Shi et al., 2023a). For retriever fine-tuning (R-ft), we update the query encoder using
a generalized LM-Supervised Retrieval (LSR, Shi et al., 2023b) training objective computed over a
combination of supervised tasks and unsupervised text completion. This way we enable the retriever
to yield more contextually relevant results, aligned with the preferences of the LLM.
We demonstrate that each fine-tuning step offers significant performance gains, and that the fine-
tuned LLM and retriever can be combined to achieve further improvements. Our largest model,
RA-DIT 65B, attains state-of-the-art performance in zero- and few-shot settings on knowledge
intensive benchmarks, notably surpassing the un-tuned in-context RALM approach on datasets
including MMLU (Hendrycks et al., 2021a) (+8.2% 0-shot; +0.7% 5-shot) and Natural Ques-
tions (Kwiatkowski et al., 2019) (+22% 0-shot; +3.8% 5-shot). In addition, RA-DIT 65B also
substantially outperforms A TLAS 11B on 8 knowledge-intensive tasks (+7.2% on average in the
64-shot fine-tuning setting). This suggests that language models and retrievers, when optimized in-
dependently and then fused through instruction-tuning, can compete effectively with RALMs that
have undergone extensive continuous pre-training. We further conduct a comprehensive model anal-
ysis, showing the effectiveness of our approach across LLMs of varying sizes, as well as evaluating
the influence of different fine-tuning strategies and retriever configurations.1
1We release the scripts for indexing Common Crawl data and generating our fine-tuning and inference
prompts at: https://github.com/facebookresearch/RA-DIT.
2



### Claim 46/179

#### Claim Text
Context Curation Redundant information can interfere with the final generation of LLM, and overly long contexts can also lead LLM to the “Lost in the middle” problem [98].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[101]_2310.06839.pdf (Page 0):

LongLLMLingua: Accelerating and Enhancing LLMs in Long Context
Scenarios via Prompt Compression
Huiqiang Jiang, Qianhui Wu, Xufang Luo,
Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu
Microsoft Corporation
{hjiang,qianhuiwu,xufluo,dongsli,cyl,yuqyang,liliqiu}@microsoft.com
Abstract
In long context scenarios, large language mod-
els (LLMs) face three main challenges: higher
computational cost, performance reduction,
and position bias. Research indicates that LLM
performance hinges on the density and posi-
tion of key information in the input prompt. In-
spired by these findings, we propose LongLLM-
Lingua for prompt compression towards im-
proving LLMs’ perception of the key informa-
tion to simultaneously address the three chal-
lenges. Our extensive evaluation across vari-
ous long context scenarios demonstrates that
LongLLMLingua not only enhances perfor-
mance but also significantly reduces costs and
latency. For instance, in the NaturalQuestions
benchmark, LongLLMLingua boosts perfor-
mance by up to 21.4% with around 4x fewer
tokens in GPT-3.5-Turbo, leading to substantial
cost savings. It achieves a 94.0% cost reduction
in the LooGLE benchmark. Moreover, when
compressing prompts of about 10k tokens at ra-
tios of 2x-6x, LongLLMLingua can accelerate
end-to-end latency by 1.4x-2.6x.1
1 Introduction
Large language models (LLMs) have revolution-
ized user-oriented language technologies and are
serving as crucial components in more and more
applications. Carefully designing prompts is nec-
essary to achieve better performance in specific
downstream tasks. The commonly used technolo-
gies such as In-Context Learning (ICL) (Min et al.,
2022; Dong et al., 2023), Retrieval Augment Gener-
ation (RAG) (Lewis et al., 2020; Asai et al., 2024),
and Multi-turn Agent (Shen et al., 2024; Park et al.,
2023; Wu et al., 2023a) are driving prompts to be
increasingly longer, even reaching thousands of to-
kens. Scenarios such as multi-document question
answering, code completion, and document sum-
marization also necessitate the processing of long
contexts.
1Access our code at https://aka.ms/LongLLMLingua.
There are three main challenges when LLMs are
used in long context scenarios: (1) Higher com-
putational costs, encompassing both financial and
latency expenses. (2) Longer prompts introduce
irrelevant and redundant information, which can
weaken LLMs’ performance (Shi et al., 2023), as
illustrated in Figure 1a. (3) LLMs exhibit position
bias (Kamradt, 2023), also known as the "lost in the
middle" issue (Liu et al., 2024), suggesting that the
placement of key information within the prompt
significantly affects LLMs’ performance. This is
demonstrated by the purple curve in Figure 1b.
Inspired by these observations, we propose
LongLLMLingua to address the three challenges.
Specifically, we use LLMLingua (Jiang et al.,
2023a) as the backbone for prompt compression
to address the first challenge, i.e., reduce cost and
latency. However, in the case of long contexts, the
distribution of question-relevant key information
in the prompt is generally dynamic and sparse. Ex-
isting prompt compression methods like LLMLin-
gua (Jiang et al., 2023a) and Selective-Context (Li
et al., 2023c) that often fail to consider question
during compression, resulting in retention of exces-
sive noise and decreased performance. LongLLM-
Lingua aims to improve LLMs’ perception of key
information pertinent to the question, thereby over-
coming the noise and position bias issues in long
contexts, shown in Figure 1b. The underlying prin-
ciple of LongLLMLingua is that small LM are
inherently capable of capturing the distribution of
key information relevant to a given question.
Our main contributions are five-fold: (1) We
propose a question-aware coarse-to-fine compres-
sion method to improve the key information den-
sity in the prompt (Sec. 4.1); (2) We introduce
a document reordering strategy to minimize po-
sition bias in LLMs. (Sec. 4.2); (3) We estab-
lish dynamic compression ratios for precise con-
trol between coarse and fine compression levels
(Sec. 4.3); (4) We propose a post-compression
arXiv:2310.06839v2  [cs.CL]  12 Aug 2024



Source: data\tc16_2312.10997v5\referenced_papers\[172]_2309.17453.pdf (Page 3):

Published as a conference paper at ICLR 2024
Dense Attention Window Attention Sliding Window 
w/ Re-computation StreamingLLM
Dense Attention Window Attention StreamingLLM
Figure 3: Language modeling perplexity on texts with 20K tokens across various LLM. Observations reveal
consistent trends: (1) Dense attention fails once the input length surpasses the pre-training attention window
size. (2) Window attention collapses once the input length exceeds the cache size, i.e., the initial tokens are
evicted. (3) StreamingLLM demonstrates stable performance, with its perplexity nearly matching that of the
sliding window with re-computation baseline.
Improving LLMs’ Utilization of Long Textoptimizes LLMs to better capture and employ the
content within the context rather than merely taking them as inputs. As highlighted by Liu et al.
and Li et al., success in the previously mentioned two directions does not necessarily translate to
competent utilization of lengthy contexts. Addressing this effective usage of prolonged contexts
within LLMs is still a challenge. Our work concentrates on stably harnessing the most recent tokens,
enabling the seamless streaming application of LLMs.
3 S TREAMING LLM
3.1 T HE FAILURE OF WINDOW ATTENTION AND ATTENTION SINKS
While the window attention technique offers efficiency during inference, it results in an exceedingly
high language modeling perplexity. Consequently, the model’s performance is unsuitable for deploy-
ment in streaming applications. In this section, we use the concept of attention sink to explain the
failure of window attention, serving as the inspiration behind StreamingLLM.
Identifying the Point of Perplexity Surge.Figure 3 shows the perplexity of language modeling on
a 20K token text. It is evident that perplexity spikes when the text length surpasses the cache size, led
by the exclusion of initial tokens. This suggests that the initial tokens, regardless of their distance
from the predicted tokens, are crucial for maintaining the stability of LLMs.
Why do LLMs break when removinginitial tokens’ KV? We visualize attention maps from
all layers and heads of the Llama-2-7B and models in Figure 2. We find that, beyond the bottom
two layers, the model consistently focuses on the initial tokens across all layers and heads. The
implication is clear: removing these initial tokens’ KV will remove a considerable portion of the
denominator in the SoftMax function (Equation 1) in attention computation. This alteration leads to a
significant shift in the distribution of attention scores away from what would be expected in normal
inference settings.
SoftMax(x)i = exi
ex1 + PN
j=2 exj
, x 1 ≫ xj, j∈ 2, . . . , N (1)
There are two possible explanations for the importance of the initial tokens in language modeling:
(1) Either their semantics are crucial, or (2) the model learns a bias towards their absolute position.
To distinguish between these possibilities, we conduct experiments (Table 1), wherein the first four
tokens are substituted with the linebreak token “\n". The observations indicate that the model still
significantly emphasizes these initial linebreak tokens. Furthermore, reintroducing them restores
the language modeling perplexity to levels comparable to having the original initial tokens. This
suggests that the absolute position of the starting tokens, rather than their semantic value, holds
greater significance.
LLMs attend to Initial Tokens as Attention Sinks.To explain why the model disproportionately
focuses on initial tokens—regardless of their semantic relevance to language modeling, we introduce
the concept of “attention sink". The nature of the SoftMax function (Equation 1) prevents all attended
tokens from having zero values. This requires aggregating some information from other tokens across
all heads in all layers, even if the current embedding has sufficient self-contained information for its
prediction. Consequently, the model tends to dump unnecessary attention values to specific tokens. A
similar observation has been made in the realm of quantization outliers (Xiao et al., 2023; Bondarenko
et al., 2023), leading to the proposal of SoftMax-Off-by-One (Miller, 2023) as a potential remedy.
4



Source: data\tc16_2312.10997v5\referenced_papers\[170]_2310.03025.pdf (Page 0):

Published as a conference paper at ICLR 2024
RETRIEVAL MEETS LONG CONTEXT LARGE LANGUAGE
MODELS
Peng Xu†, Wei Ping†, Xianchao Wu, Lawrence McAfee
Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina
Mohammad Shoeybi, Bryan Catanzaro
NVIDIA
†{pengx, wping}@nvidia.com
ABSTRACT
Extending the context window of large language models (LLMs) is getting popular
recently, while the solution of augmenting LLMs with retrieval has existed for years.
The natural questions are: i) Retrieval-augmentation versus long context window,
which one is better for downstream tasks? ii) Can both methods be combined to
get the best of both worlds?In this work, we answer these questions by studying
both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B
GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context
window using simple retrieval-augmentation at generation can achieve compa-
rable performance to finetuned LLM with 16K context window via positional
interpolation on long context tasks, while taking much less computation. More
importantly, we demonstrate that retrieval can significantly improve the perfor-
mance of LLMs regardless of their extended context window sizes. Our best model,
retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-
turbo-16k and Davinci003 in terms of average score on nine long context tasks
including question answering, query-based summarization, and in-context few-shot
learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a
margin, while being much faster at generation. Our study provides general insights
on the choice of retrieval-augmentation versus long context extension of LLM for
practitioners.
1 I NTRODUCTION
The long context large language models (LLM) have recently received a lot of attention in produc-
tion (e.g., Anthropic, 2023; OpenAI, 2023b), research community (e.g., Chen et al., 2023; Liu et al.,
2023; Tworkowski et al., 2023), and open source community (e.g., Kaiokendev, 2023). Although
the approximate attention methods have been studied for years (e.g., Tay et al., 2022a) (due to the
quadratic time and memory complexities of self-attention mechanism in sequence length), the recent
advance for long context LLMs with exact attention is mainly driven by the development of faster
GPU with more memory and memory-efficient exact attention (Dao et al., 2022; Dao, 2023).
An alternative and long-standing solution for handling long context is retrieval. Specifically, the
LLMs only read relevant context retrieved from a standalone retriever (e.g., Karpukhin et al., 2020;
Wang et al., 2022; Lin et al., 2023), which is much easier to scale1 and runs orders of magnitudes
faster than LLMs for selecting relevant context. Conceptually, the retrieval-augmented decoder-only
LLM can be viewed as applying the sparse attention over its long context window, where the sparsity
pattern is not predefined as Child et al. (2019) but determined by the standalone retriever. In other
words, unretrieved context is treated as irrelevant and has zero-valued attention weights.
Given the surge of interest in long context LLM research and much more required computation
at inference 2, it is still unclear for practitioners whether extending the context window of LLM
1The dense embedding retriever can easily retrieve context from billions of tokens using the fast similarity
search library (Johnson et al., 2019).
2For example, the price of GPT-4 with 32k context length is twice the 8k context model.
1
arXiv:2310.03025v2  [cs.CL]  23 Jan 2024



Source: data\tc16_2312.10997v5\referenced_papers\[60]_2310.03025.pdf (Page 0):

Published as a conference paper at ICLR 2024
RETRIEVAL MEETS LONG CONTEXT LARGE LANGUAGE
MODELS
Peng Xu†, Wei Ping†, Xianchao Wu, Lawrence McAfee
Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina
Mohammad Shoeybi, Bryan Catanzaro
NVIDIA
†{pengx, wping}@nvidia.com
ABSTRACT
Extending the context window of large language models (LLMs) is getting popular
recently, while the solution of augmenting LLMs with retrieval has existed for years.
The natural questions are: i) Retrieval-augmentation versus long context window,
which one is better for downstream tasks? ii) Can both methods be combined to
get the best of both worlds?In this work, we answer these questions by studying
both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B
GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context
window using simple retrieval-augmentation at generation can achieve compa-
rable performance to finetuned LLM with 16K context window via positional
interpolation on long context tasks, while taking much less computation. More
importantly, we demonstrate that retrieval can significantly improve the perfor-
mance of LLMs regardless of their extended context window sizes. Our best model,
retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-
turbo-16k and Davinci003 in terms of average score on nine long context tasks
including question answering, query-based summarization, and in-context few-shot
learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a
margin, while being much faster at generation. Our study provides general insights
on the choice of retrieval-augmentation versus long context extension of LLM for
practitioners.
1 I NTRODUCTION
The long context large language models (LLM) have recently received a lot of attention in produc-
tion (e.g., Anthropic, 2023; OpenAI, 2023b), research community (e.g., Chen et al., 2023; Liu et al.,
2023; Tworkowski et al., 2023), and open source community (e.g., Kaiokendev, 2023). Although
the approximate attention methods have been studied for years (e.g., Tay et al., 2022a) (due to the
quadratic time and memory complexities of self-attention mechanism in sequence length), the recent
advance for long context LLMs with exact attention is mainly driven by the development of faster
GPU with more memory and memory-efficient exact attention (Dao et al., 2022; Dao, 2023).
An alternative and long-standing solution for handling long context is retrieval. Specifically, the
LLMs only read relevant context retrieved from a standalone retriever (e.g., Karpukhin et al., 2020;
Wang et al., 2022; Lin et al., 2023), which is much easier to scale1 and runs orders of magnitudes
faster than LLMs for selecting relevant context. Conceptually, the retrieval-augmented decoder-only
LLM can be viewed as applying the sparse attention over its long context window, where the sparsity
pattern is not predefined as Child et al. (2019) but determined by the standalone retriever. In other
words, unretrieved context is treated as irrelevant and has zero-valued attention weights.
Given the surge of interest in long context LLM research and much more required computation
at inference 2, it is still unclear for practitioners whether extending the context window of LLM
1The dense embedding retriever can easily retrieve context from billions of tokens using the fast similarity
search library (Johnson et al., 2019).
2For example, the price of GPT-4 with 32k context length is twice the 8k context model.
1
arXiv:2310.03025v2  [cs.CL]  23 Jan 2024



Source: data\tc16_2312.10997v5\referenced_papers\[103]_2303.08559.pdf (Page 19):

rerankers correct two kinds of erroneous predic-
tions made by LLMs. (1) The lack of external
knowledge, such as the first (Triptolemus is a fig-
ure in Greek mythology) and third examples (Mi-
nas Gerais is a state instead of city). (2) Limited
reasoning abilities, such as the second (His wife’s
children are his children) and the fourth (The word
"fought" in this sentence does not involve any phys-
ical violence) examples.
F.2 Easy Samples
As discussed in Section 4.3, we attribute the in-
ferior performance of LLMs on easy samples to
their false-positive predictions for negative samples.
Upon examining the incorrect cases, we speculate
that two main reasons are responsible: (1) LLM
hallucination and (2) span boundary mismatch. We
provide details on them with specific cases below.
LLM Hallucination. The most distinctive mis-
takes made by LLMs. Without specific fine-tuning
on IE tasks, LLMs (with in-context learning ap-
proaches) sometimes hallucinate and provide mean-
ingless, even confusing explanations or answers.
For instance, LLMs might predict a verb, preposi-
tion or even special symbols as an entity.
Case 1 : LLMs predict the word die as entity
person-other (Ground truth: None)
Sentence: Flavius Rumoridus (died 5th cen-
tury AD) was a Roman soldier who was ap-
pointed consul in AD 403 in the Western Ro-
man Empire at the same time as the eastern
emperor Theodosius II served in the same
capacity in the East.
Candidate Choices:
(a) died do/does not belong to any known
entities.
(b) died is a kind of geopolitical entity.
(c) died is a person, but not affiliated with
following professions: actor, artist, ath-
lete, author, director, politician, scholar,
soldier.
Analysis: The sentence provides information
about Flavius Rumoridus, who was a Roman
soldier and appointed consul in AD 403. The
sentence also mentions that he died in the 5th
century AD, which is a statement about his
death. Therefore, the correct answer is (c)
because the sentence does not provide any
information about Flavius Rumoridus being
affiliated with any of the professions listed in
option (c).
Correct Answer:(c)
Case 2 : LLMs predict the word of as entity
organization-other(Ground truth: None)
Sentence: During these years he studied at
the South Kensington Schools of the Royal
College of Art.
Candidate Choices:
(a) of do/does not belong to any known en-
tities.
(b) of is an educational institute.
(c) of pertains to an organization that does
not fall under the categories of com-
pany, educational institution, govern-
ment, media, political party, religion,
sports league, sports team, band or mu-
sical group.
Analysis: In the given sentence,of is a prepo-
sition that indicates the relationship between
South Kensington Schools and Royal College
of Art. None of the candidate choices directly
address the correct relationship between these
entities or state that they are not applicable.
Therefore, the correct answer is (c): of does
not pertain to any known entities mentioned
in the choices provided.
Correct Answer:(c)
Span boundary mismatch . Different from
sentence-level approaches which take one or mul-
tiple sentences as an unit and assign each word’s
label globally, LLMs under our filter-then-rerank



### Claim 47/179

#### Claim Text
Therefore, in the RAG system, we typically need to further process the retrieved content. 1) Reranking: Reranking fundamentally reorders document chunks to highlight the most pertinent results first, effectively reducing the overall document pool, severing a dual purpose in information retrieval, acting as both an enhancer and a filter, delivering refined inputs for more precise language model processing [70].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[3]_2310.20158.pdf (Page 8):

Table 6: Recall@100 on TREC-DL20, TREC-DL19 and TREC-COVID with varying number of
rewrites Nrw.
Dataset Nrw
1 3 5
TREC-DL 19 49.7 52.6 54.0
TREC-DL 20 55.1 57.9 58.6
TREC-COVID 12.5 13.0 13.0
Table 7: Effect of re-ranking (in Step 6 of Algorithm 1). Scores reported are nDCG@10.
Dataset With reranking Without reranking
TREC-DL19 73.9 51.3
TREC-DL20 72.3 42.0
TREC-COVID 86.4 66.8
6.1 R ETRIEVAL AUGMENTED GENERATION (RAG)
RAG paradigm has been applied extensively in open-domain Question-Answering. Brill et al. (2002)
use an N-gram tiling technique as g; Chen et al. (2017) use an answer span prediction model as g,
while the retrieval components in both these cases are black-box. On the other hand, recent systems
employ learnable retrieval components: Lee et al. (2019) use BERT-style encoders for both f and
g and learn them jointly; Guu et al. (2020); Lewis et al. (2020); Singh et al. (2021); Izacard et al.
(2022) extend it to jointly pre-training or fine-tuning LMs asg and Transformer-based encoders asf.
Most recently, Yu et al. (2023) eschew the retriever component altogether, and propose a generate-
then-read paradigm to first synthesize context (i.e., relevant documents) instead of retrieving from a
corpus, and then generate the final answer based on the input query and the context. RAG has also
been applied in code generation, e.g., Parvez et al. (2021).
In contrast to most of the aforementioned approaches, we work at the intersection of the (a) zero-shot
setting with no access to training data from the test domain; and (b) top- k retrieval setting where
the performance is measured via ranking metrics, instead of exact-match or Rouge-L based metrics
typically used with generated answers in the RAG paradigm.
6.2 G ENERATION AUGMENTED RETRIEVAL (GAR)
Query expansion techniques generate additional context or pseudo-documents using g (Wang et al.,
2023; Jagerman et al., 2023; Shen et al., 2023; Feng et al., 2023). Query rewriting or reformulation
techniques, that we incorporate in RRR, attempt to generate alternate versions of query q using
different prompting strategies for g (Gao et al., 2022; Mao et al., 2023). Nogueira et al. (2019b)
expand documents instead with queries relevant to q. Generative-relevance feedback techniques
(Mackie et al., 2023b;a;c) use g to generate long-form text as first-pass retrieved documents z, to
then seed the second-pass retrieval with q and z. Related line of work is learning improved dense
representations for queries using pseudo-relevance feedback (Yu et al., 2021; Wang et al., 2021).
Unlike aforementioned approaches, RRR iteratively improves the rewriting and the retrieving
stages.
6.3 L ANGUAGE MODELS FOR RE-RANKING
A recent line of work leverages pre-trained LMs to re-rank the outputs of baseline retrievers using
(a) novel prompting strategies that elicit pairwise preferences (Qin et al., 2023), or (b) sliding-
window techniques (Sun et al., 2023) that re-rank only a small window of retrieved documents at a
time, bubbling up the entire retrieved set gradually, or (c) the likelihood of reconstructing the query
conditioned on the retrieved documents (Sachan et al., 2022). However, these approaches rely on a
good initial ordering of the retrieved documents to be successful. Our empirical evaluations show
that our algorithm ensures recall as well as a good initial ordering of documents.
9



Source: data\tc16_2312.10997v5\referenced_papers\[64]_2302.00083.pdf (Page 7):

Retrieval Stride ( 𝑠)
Perplexity
10.0
20.0
30.0
40.0
1248163264
GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)
Figure 5: An analysis of perplexity as a function
of s, the retrieval stride, i.e., the number of tokens
between consecutive retrieval operations, on the
development set of WikiText-103. Throughout the
paper, we use s = 4 to balance perplexity and
runtime.
Retrieval Query Length (ℓ)
Perplexity
10.0
15.0
20.0
25.0
30.0
35.0
16 32 64
GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)
Figure 6: An analysis of perplexity as a function
of the number of tokens in the query ℓ for BM25
on the development set of WikiText-103. In the
appendix, we show similar trade-offs for dense
retrievers within WikiText-103. Throughout the
paper, we use a query length of ℓ = 32tokens.
trieved by the BM25 retriever. This permits very
limited semantic understanding of the query, since
BM25 is based only on the bag of words signal.
Moreover, it offers no way to accord different de-
grees of importance to different retrieval query to-
kens, such as recognizing that later query tokens
are more relevant to the generated text.
In this section, we focus on choosing which doc-
ument to present to the model, by reranking the
top-k documents returned by the BM25 retriever.5
We use Figure 7 as motivation: it shows the large
potential for improvement among the top-16 docu-
ments returned by the BM25 retriever. We act upon
5In both §6.1 and §6.2 we use k = 16.
Perplexity
10.0
20.0
30.0
40.0
No Retrieval BM25 (Top-1) Oracle: BM25 (Top-16)
GPT-2 117M (S) GPT-2 345M (M) GPT-2 762M (L) GPT-2 1.5B (XL)
Figure 7: Potential for gains from reranking: per-
plexity improvement (on the development set of
WikiText-103) from an oracle that takes the best
of the top-16 documents retrieved by BM25 rather
than the first.
this motivation by using two rerankers. Specifi-
cally, in §6.1 we show performance gains across
our evaluation suite obtained by using an LM to
perform zero-shot reranking of the top- k BM25
retrieved documents (results in third row for each
of the models in Table 1). Then, in §6.2 we show
that training a specialized bidirectional reranker
of the top-k BM25 retrieved documents in a self-
supervised manner via the LM signal can provide
further LM gains (results in forth row for each of
the models in Table 1).
6.1 LMs as Zero-Shot Rerankers
First, we used off-the-shelf language models as
document rerankers for the In-Context RALM set-
ting. Formally, for a query q consisting of the
last ℓ tokens in the prefix of the LM input x, let
{d1, ..., dk} be the top- k documents returned by
BM25. For retrieval iteration j, let the text for
generation be y := xs·j+1, ..., xs·j+s. Ideally, we
would like to find the document di∗ that maximizes
the probability of the text for generation, i.e.,
i∗ = arg max
i∈[k]
pθ(y|[di; x≤s·j]). (5)
However, at test time we do not have access to
the tokens of y. Instead, we used the last pre-
fix tokens (which are available at test time), de-
noted by y′, for reranking. Formally, let s′ be
a hyper-parameter that determines the number of
the prefix tokens by which to rerank. We define
y′ := xs·j−s′+1, ..., xs·j (i.e., the stride of length s′
that precedes y) and choose the document dˆi such



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 21):

111:22 Lyu, et al.
QA 1-document QA 2-document QA 3-document hallucination
modification
0
10
20
30
40
50
60MRR
49.8
42.3
38.5
49.349.4
42.0
38.0
49.2
47.2
41.8
38.1
51.2
48.3
42.3
38.4
50.6
MRR Values for Different Tasks and Embeddings
BGE GTE M3E STELLA
Fig. 8. Comparison of Mean Reciprocal Rank (MRR) scores for different embedding models in our benchmark.
algorithms may not be sufficient. We also found that the Hybrid+Rerank algorithm, which combines
and re-ranks the results of both algorithms, improves on all evaluation metrics. This suggests that
this is a better retrieval algorithm for question-answering tasks.
Hallucination Modification: Consistent with the conclusion of summarization, the BM25
retriever performs slightly better than or equal to the dense retriever. For RAG tasks such as
hallucination modification, which require precise retrieval of highly relevant content, BM25 shows
good performance. Moreover, BM25 requires less computational resources than dense retrievers.
This indicates that different RAG tasks require different retrieval algorithms.
Retrieval Accuracy Evaluation: To make a more comprehensive evaluation, we evaluated the
retrieval accuracy on question answering and hallucination modification tasks using MRR (mean
reciprocal rank) as a separate metric. This separate evaluation allows for a more accurate assessment
of the retriever’s capabilities. Notably, text continuation and open-domain summarization tasks
were excluded due to their subjective and vague evaluation criteria, lacking clear ground truth.
Additionally, both 2-document and 3-document question answering require multiple documents to
address queries. Therefore, we calculate the MRR for each retrieved document individually and
take the average as the final result. The pure hybrid algorithm was not evaluated separately as it
could alter the order of retrieved content, affecting subsequent processing steps.
Figure 7 shows that the hybrid + reranking method excels in most tasks, outperforming other
methods. This demonstrates the effectiveness of combining multiple retrieval strategies with
reranking. Notably, BM25 and dense retrievers perform comparably in many cases, highlighting
the strengths of both traditional and neural network methods. In question answering, performance
for all methods declines as the number of documents increases, aligning with expectations since
multi-document tasks are more challenging and require stronger information integration. These
results are consistent with our previous end-to-end evaluations, confirming the reliability of the
end-to-end evaluation method.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[42]_2208.03299.pdf (Page 6):

updated, as the true top-k documents may not be retrieved in the top-L results from the stale index. In
practice, it is possible to track the positions of the top-K re-ranked documents in the top-L, and estimate
when the index needs to be updated.
Query-side ﬁne-tuning. Finally, the last strategy is to decouple the encoding of the queries and documents.
In this case, we ﬁx the parameters corresponding to the document encoder, and only train the parameters
corresponding to the query encoder. Thus, the embeddings of documents are ﬁxed, and we do not need to
refresh the index, and thus there is no computational overhead. As we will see in practice, the impact of
ﬁxing the documents encoder varies greatly for diﬀerent tasks when a large training dataset is available. For
most of the few-shot settings that we consider, query-side ﬁnetuning does not have large performance impact,
and sometimes even slightly improves performance.
3 Related work
3.1 Retrieval in natural language processing
Retrieval for knowledge intensive tasks.Previous work has shown that retrieval improves performance
across a variety of tasks such as question answering (Voorhees et al., 1999; Chen et al., 2017; Kwiatkowski et al.,
2019), fact checking (Thorne et al., 2018), dialogue (Dinan et al., 2019) or citation recommendation (Petroni
et al., 2022). Historically, this information retrieval step was implemented using term-matching methods, such
as TF-IDF or BM25 (Jones, 1972; Robertson et al., 1995). For open-domain question answering (Voorhees
et al., 1999), documents are often retrieved from Wikipedia (Chen et al., 2017). Recently, dense retrievers
based on neural networks have become popular. These usually follow a dual-encoder architecture (Yih et al.,
2011; Huang et al., 2013; Shen et al., 2014), where queries and passages are encoded independently as vectors,
and relevance is computed using the inner product or Euclidean distance. Popular supervised retrievers
include DPR (Karpukhin et al., 2020), which is trained to discriminate the relevant passage among negative
passages, and extensions such as ANCE (Xiong et al., 2020) which improved the hard negatives mining
process. We refer the reader to Yates et al. (2021) for a survey of dense retrieval techniques.
After retrieval, the relevant documents are processed to produce the ﬁnal output. In open-domain QA, models
can extract a span of text from retrieved documents as the answer (Chen et al., 2017; Clark & Gardner, 2018;
Wang et al., 2019; Karpukhin et al., 2020), a method inspired by reading comprehension (Richardson, 2013;
Rajpurkar et al., 2016). Recently, generating the answer as free-form text, using a seq2seq model conditioned
on retrieved documents have become prevalent (Lewis et al., 2020; Izacard & Grave, 2020; Min et al., 2020).
These architectures have also been shown to reduce hallucination in dialogue agents (Shuster et al., 2021).
Retriever training. The need for expensive query-document annotations for training the retriever can be
bypassed, by leveraging signals from the language model, or using unsupervised learning. REALM (Guu et al.,
2020) and RAG (Lewis et al., 2020) jointly train the retriever and language model by modelling documents as
latent variable, and minimizing the objective with gradient descent. REALM pre-trains end-to-end with an
MLM approach but uses an extractive BERT-style model (Devlin et al., 2019). Guu et al. (2020) also explore
a query-side ﬁnetuning at ﬁnetuning time to avoid index refreshes, which is also explored in the context of
phrase-based retrieval by Lee et al. (2021b). Izacard & Grave (2020) proposed to use cross-attention scores
as supervision with knowledge distillation. Sachan et al. (2021) perform joint training of the reader and the
retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee
et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention
scores from the language model. Theinverse cloze taskwas proposed by Lee et al. (2019) to pre-train dense
retrievers in an unsupervised way. Paranjape et al. (2021) propose a method to train retrieval-augmented
generators using a second “informed” retriever with access to the output, which the test-time retriever can be
distilled from, and Hofstätter et al. (2022) recently proposed a training set ﬁltering/weighting approach to
train stronger retrieval-augmented generators. Izacard et al. (2022) explored diﬀerent contrastive learning
methods to train retrievers, while Ram et al. (2022) used recurring spans within a document to create
pseudo-positive query-document pairs.
7



Source: data\tc16_2312.10997v5\referenced_papers\[3]_2310.20158.pdf (Page 1):

Figure 1: Proposed RRR method for zero-shot Information Retrieval. We implement the rewrite,
filtering, and re-rank stages (colored boxes) via a pre-trained LLMs (in our evaluations, we use
GPT -3.5-Turboand GPT -4models). For the retrieval stage, we use BM25. Details in Section 3.2.
Two popular paradigms for information retrieval with language models are retrieval-augmented gen-
eration (RAG) and generation-augmented retrieval (GAR).
1. RAG paradigm (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Singh et al., 2021; Izac-
ard et al., 2022) fetches (using a retrieval model) relevant documents from the corpus as context for
the language model and then generates an answer for the input query directly using the language
model.
2. GAR paradigm (Wang et al., 2023; Mackie et al., 2023b) augments the input query using lan-
guage models, and then uses a retrieval model to fetch the relevant documents from the corpus.
A key challenge in these paradigms is obtaining a high-quality retrieval model for fetching docu-
ments during the first stage and a post-hoc re-ranking model to improve the precision of the final
top-k results. Dense retrieval techniques like ANCE (Xiong et al., 2020) (Nogueira & Cho, 2019)
and TAS-B (Hofst¨atter et al., 2021) suffer from poor precision, and fine-tuning the models is infea-
sible in the zero-shot setting. Recent results (Thakur et al., 2021) show, somewhat surprisingly, that
off-the-shelf sparse retrieval models like BM25 (Robertson & Zaragoza, 2009) outperform dense
counterparts when combined with generative language models. More strikingly, for re-ranking, re-
cent studies by Sun et al. (2023); Qin et al. (2023), show promise for designing effective re-ranking
strategies using LLMs like GPT-4 as a black-box. These studies, however, do not consider feedback
between the three stages. For example, a good initial ordering of retrieved documents is crucial
for the re-ranking to be effective and a good rewrite of the input query can improve the quality of
retrieved documents.
In this work, we achieve the best of both the worlds, i.e., of GAR and RAG paradigms. We pro-
pose a novel GAR-meets-RAG formulation for zero-shot IR that incorporates a feedback loop of
rewrite and retrieval stages. We design a simple and effective approach to IR, calledRRR (Rewrite-
Retrieve-Rerank), that leverages pre-trained models to perform document retrieval, refinement, and
query rewrite iteratively (Figure 1). The key design principle is that the rewrite-retrieval stages im-
prove the recall of the system and a final re-ranking stage improves the precision. A key technical
contribution in this work is a novel prompting strategy for the query rewrite stage which allows the
rewriter to be aligned to the type of documents present in the unseen corpus.
Our contributions are summarized below:
(1) We propose a novel GAR-meets-RAG recurrence formulation for the zero-shot IR problem, that
uses a RAG model to produce query rewrite, which feeds into a GAR model for retrieval.
(2) We design a simple, iterative algorithm for the proposed problem called RRR that maximizes
recall via rewrite-retrieve stages and precision via a final re-rank stage.
(3) We perform extensive evaluations and comparisons to SOTA techniques on two popular IR
benchmarks. We establish new state-of-the-art Recall@100 and nDCG@10 metrics on 6 out of
8 datasets in the BEIR benchmark, with up to 17% relative gains over the previous best.
2 B ACKGROUND AND NOTATION
Given a query, our task is to retrieve relevant documents from a large corpus, without using any
training data specific to the domain. For example, queries can be factual such as “ Can antioxidant-
rich spices counteract the effects of a high-fat meal? ” from the NFCorpus dataset or open-ended
such as “Should teachers be given tenure?” from the Touch´e-2020 dataset. The ground-truth rel-
evant documents for these queries are online documents (e.g., scientific journal abstracts, tweets,
2



### Claim 48/179

#### Claim Text
Reranking can be performed using rule-based methods that depend on predefined metrics like Diversity, Relevance, and MRR, or model-based approaches like Encoder-Decoder models from the BERT series (e.g., SpanBERT), specialized reranking models such as Cohere rerank or bge-raranker-large, and general large language models like GPT [12], [99]. 2) Context Selection/Compression: A common misconception in the RAG process is the belief that retrieving as many relevant documents as possible and concatenating them to form a lengthy retrieval prompt is beneficial.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[41]_2312.05708.pdf (Page 4):

ally, the absence of conversation history impedes
the model’s ability to adapt to topic shifts that may
occur throughout a dialogue.
Furthermore, the performance of the planner
model is constrained by the length of the context
window. While employing LLMs with longer con-
text windows can enhance performance, it also in-
creases model size and computational complexity.
To address this limitation, incorporating context
compression techniques could potentially improve
end-to-end performance without incurring signifi-
cant increases in model size.
Due to privacy constraints, we simulated real-
world data by generating synthetic user profiles
and personas that mirrored real-world use cases for
a digital assistant.
Ethics Statement
To safeguard privacy, this study exclusively utilizes
synthetically generated data, eliminating the use of
real user information under ethical considerations.
Acknowledgements
We would like to thank Stephen Pulman, Barry
Theobald and Joel Moniz for their valuable feed-
back.
References
Christopher J.C. Burges. 2010. From ranknet to lamb-
darank to lambdamart: An overview. Microsoft Re-
search Technical Report MSR-TR-2010-82.
Gordon V . Cormack, Charles L. A. Clarke, and Stefan
Buettcher. 2009. Reciprocal rank fusion outperforms
condorcet and individual rank learning methods. In
Proceedings of the 32nd International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval., pages 758–759.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Ming-Wei Chang. 2020. Realm: Retrieval-
augmented language model pre-training.
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and
Igor Mordatch. 2022. Language models as zero-shot
planners: Extracting actionable knowledge for em-
bodied agents.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen tau Yih, Tim Rock-
täschel, Sebastian Riedel, and Douwe Kiela. 2020.
Retrieval-augmented generation for knowledge-
intensive nlp tasks.
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin
Paranjape, Michele Bevilacqua, Fabio Petroni, and
Percy Liang. 2023. Lost in the middle: How lan-
guage models use long contexts. arXiv preprint
arXiv:2307.03172.
Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-
Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-
feng Gao. 2023. Chameleon: Plug-and-play compo-
sitional reasoning with large language models. arXiv
preprint arXiv:2304.09842.
Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,
and Nan Duan. 2023. Query rewriting for retrieval-
augmented large language models. arXiv preprint
arXiv:2305.14283.
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-
tavo Hernández Ábrego, Ji Ma, Vincent Y . Zhao,
Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei
Yang. 2021. Large dual encoders are generalizable
retrievers.
OpenAI. 2023. Gpt-4 technical report.
Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang,
Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Don-
ald Metzler, Xuanhui Wang, and Michael Bender-
sky. 2023. Large language models are effective text
rankers with pairwise ranking prompting. arXiv
preprint arXiv:2306.17563v1.
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,
Amnon Shashua, Kevin Leyton-Brown, and Yoav
Shoham. 2023. In-context retrieval-augmented lan-
guage models. arXiv preprint arXiv:2302.00083.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. 2023. Toolformer:
Language models can teach themselves to use tools.
arXiv preprint arXiv:2302.04761.
Krishna Srinivasan, Karthik Raman, Anupam Samanta,
Lingrui Liao, Luca Bertelli, and Mike Bendersky.
2023. Quill: Query intent with large language mod-
els using retrieval augmentation and multi-stage dis-
tillation. arXiv preprint arXiv:2210.15718v1.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971.
Andrew Trotman, Antti Puurula, and Blake Burgess.
2014. Improvements to bm25 and language models
examined. In Proceedings of the 32nd International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval., pages 58–65.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and
Denny Zhou. 2022. Chain-of-thought prompting
elicits reasoning in large language models. arXiv
preprint arXiv:2201.11903.



Source: data\tc16_2312.10997v5\referenced_papers\[74]_2401.13256.pdf (Page 12):

13
edge,” in AAAI, 2022.
[25] W. Sixing and et al., “More is better: Enhancing open-
domain dialogue generation via multi-source heteroge-
neous knowledge,” inEMNLP, 2021.
[26] ——, “Section-aware commonsense knowledge-grounded
dialogue generation with pre-trained language model,”
in Proceedings of the 29th International Conference on
Computational Linguistics, 2022.
[27] E. Dinan and et al., “Wizard of wikipedia: Knowledge-
powered conversational agents,” 2019.
[28] F. Tingchen and et al., “There are a thousand hamlets in a
thousand people’s eyes: Enhancing knowledge-grounded
dialogue with personal memory,” in ACL 2022, 2022.
[29] H. Minlie and et al., “Challenges in building intelligent
open-domain dialog systems,” ACM Trans. Inf. Syst. ,
2020.
[30] H. Wang and et al., “A survey of the evolution of language
model-based dialogue systems,” 2023.
[31] X. Xinchao and et al., “Long time no see! open-domain
conversation with long-term persona memory,” in Find-
ings. of ACL , 2022.
[32] Z. Zheng and et al., “Memory-augmented dialogue man-
agement for task-oriented dialogue systems,”ACM Trans.
Inf. Syst., 2019.
[33] M. Chuan and et al., “Dukenet: A dual knowledge in-
teraction network for knowledge-grounded conversation,”
in Proceedings of the 43rd International ACM SIGIR
Conference on Research and Development in Information
Retrieval, ser. SIGIR ’20, 2020.
[34] R. Nakano and et al., “Webgpt: Browser-assisted question-
answering with human feedback,” 2022.
[35] H. Wang and et al., “Tpe: Towards better compositional
reasoning over conceptual tools with multi-persona col-
laboration,” 2023.
[36] S. Kurt and et al., “Retrieval augmentation reduces
hallucination in conversation,” in Findings. of EMNLP ,
2021.
[37] C. Wang and et al., “Survey on factuality in large language
models: Knowledge, retrieval and domain-specificity,”
2023.
[38] L. Patrick and et al., “Retrieval-augmented generation
for knowledge-intensive nlp tasks,” in Proceedings of
the 34th International Conference on Neural Information
Processing Systems, ser. NIPS’20, 2020.
[39] Y. Gao and et al., “Retrieval-augmented generation for
large language models: A survey,” 2024.
[40] R. Stephen and et al., “The probabilistic relevance frame-
work: Bm25 and beyond,” Foundations and Trends ® in
Information Retrieval, 2009.
[41] G. Jiafeng and et al., “Semantic models for the first-stage
retrieval: A comprehensive review,”ACM Trans. Inf. Syst.,
2022.
[42] K. Vladimir and et al., “Dense passage retrieval for open-
domain question answering,” in EMNLP, 2020.
[43] L. Hang and et al., “Pseudo relevance feedback with
deep language models and dense retrievers: Successes and
pitfalls,” ACM Trans. Inf. Syst., 2023.
[44] B. Sebastian and et al., “An analysis of fusion functions
for hybrid retrieval,”ACM Trans. Inf. Syst., 2023.
[45] Y. Zhu and et al., “Large language models for information
retrieval: A survey,” 2023.
[46] S. Weiwei and et al., “Is ChatGPT good at search?
investigating large language models as re-ranking agents,”
in EMNLP, 2023.
[47] T. Shen and et al., “Large language models are strong zero-
shot retriever,” 2023.
[48] X. Ma and et al., “Zero-shot listwise document reranking
with a large language model,” 2023.
[49] S. Hao and et al., “Toolkengpt: Augmenting frozen
language models with massive tools via tool embeddings,”
2023.
[50] K. Sparck Jones, “A statistical interpretation of term
specificity and its application in retrieval,” Journal of
documentation, 1972.
[51] Y. Shi and et al., “Few-shot conversational dense retrieval,”
in Proceedings of the 44th International ACM SIGIR
Conference on Research and Development in Information
Retrieval, ser. SIGIR ’21, 2021.
[52] D. Jacob and et al., “BERT: Pre-training of deep bidi-
rectional transformers for language understanding,” in
NAACL-HLT, 2019.
[53] R. Ruiyang and et al., “RocketQAv2: A joint training
method for dense passage retrieval and passage re-
ranking,” in EMNLP, 2021.
[54] J. Yoonna and et al., “Call for customized conversation:
Customized conversation grounding persona and knowl-
edge,” in AAAI, 2022.
[55] D. Zhengxiao and et al., “Glm: General language model
pretraining with autoregressive blank infilling,” in ACL,
2022.
[56] A. Zeng and et al., “GLM-130b: An open bilingual pre-
trained model,” inThe Eleventh International Conference
on Learning Representations (ICLR) , 2023.
[57] E. J. Hu and et al., “Lora: Low-rank adaptation of large
language models,” 2021.
[58] Z. Chujie and et al., “CDConv: A benchmark for contra-
diction detection in Chinese conversations,” in EMNLP,
2022.



Source: data\tc16_2312.10997v5\referenced_papers\[44]_2304.06762.pdf (Page 0):

Shall We Pretrain Autoregressive Language Models with Retrieval?
A Comprehensive Study
Boxin Wang∗ ‡1 Wei Ping∗†2 Peng Xu∗2 Lawrence McAfee2
Zihan Liu2 Mohammad Shoeybi2 Yi Dong2 Oleksii Kuchaiev2
Bo Li1 Chaowei Xiao2,3 Anima Anandkumar2 Bryan Catanzaro2
Abstract
Large decoder-only language models (LMs)
can be largely improved in terms of perplex-
ity by retrieval ( e.g., RETRO ), but its impact
on text generation quality and downstream task
accuracy is unclear. Thus, it is still an open
question: shall we pretrain large autoregres-
sive LMs with retrieval? To answer it, we per-
form a comprehensive study on a scalable pre-
trained retrieval-augmented LM (i.e., RETRO )
compared with standard GPT and retrieval-
augmented GPT incorporated at fine-tuning or
inference stages. We first provide the recipe
to reproduce RETRO up to 9.5B parameters
while retrieving a text corpus with 330B tokens.
Based on that, we have the following novel find-
ings: i) RETRO outperforms GPT on text gen-
eration with much less degeneration (i.e., repe-
tition), moderately higher factual accuracy, and
slightly lower toxicity with a nontoxic retrieval
database. ii) On the LM Evaluation Harness
benchmark, RETRO largely outperforms GPT
on knowledge-intensive tasks, but is on par with
GPT on other tasks. Furthermore, we intro-
duce a simple variant of the model, RETRO ++,
which largely improves open-domain QA re-
sults of original RETRO (e.g., EM score +8.6
on Natural Question) and significantly outper-
forms retrieval-augmented GPT in both fine-
tuning and zero-shot evaluation settings. Our
findings highlight the promising direction of
pretraining autoregressive LMs with retrieval
as future foundation models. We release our
code and model at: https://github.com/N
VIDIA/Megatron-LM/blob/main/tools/re
tro/README.md.
1 Introduction
Large language models (LMs), including masked
LMs (e.g., BERT (Devlin et al., 2018)), autore-
gressive LMs (e.g., GPT (Brown et al., 2020)),
and encoder-decoder LMs (e.g., T5 (Raffel et al.,
∗Equal contribution. ‡Work done during an internship at
NVIDIA. 1UIUC. 2NVIDIA. 3University of Wisconsin, Madi-
son. †Correspondence to: Wei Ping <wping@nvidia.com>
2020), BART (Lewis et al., 2020a)), have ob-
tained state-of-the-art results for various NLP tasks.
Among them, the autoregressive LMs like GPT-
3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023)
demonstrate noticeable in-context learning abil-
ity and excellent long-form text generation results.
Due to its importance, the community has spent
considerable efforts to scale up such autoregres-
sive generative LMs with more data and param-
eters and observed significant breakthroughs in
a variety of real-world applications (e.g., Brown
et al., 2020), including open-ended text genera-
tion and various downstream tasks (e.g., ques-
tion answering). The successful public exam-
ples include GPT-3 (w/ 170B parameters) (Brown
et al., 2020), Gopher (280B) (Rae et al., 2021),
Megatron-Turing (530B) (Smith et al., 2022), and
PaLM (540B) (Chowdhery et al., 2022).
Although large-scale autoregressive LMs have
achieved huge successes, they also suffer from sev-
eral weaknesses. First, it requires a huge number
of model parameters to memorize the world knowl-
edge, which makes it costly for deployment. Sec-
ond, it is difficult to safeguard factual accuracy,
which may provide users with incorrect informa-
tion (Lee et al., 2022). Third, it is expensive to
update the model knowledge learned during pre-
training with up-to-date facts (Meng et al., 2022),
yielding outdated answers (Lewis et al., 2020b).
To mitigate the problems above, one line of
research proposes to improve language models
with retrieval. The retrieval process can be inte-
grated into LMs at: i) fine-tuning stage (Karpukhin
et al., 2020; Lewis et al., 2020b; Guu et al., 2020),
or ii) pretraining stage (Borgeaud et al., 2022;
Izacard et al., 2022). Most previous work aug-
ments BERT or encoder-decoder LMs with re-
trieval at fine-tuning stage, demonstrating suc-
cesses for knowledge-intensive NLP tasks (Guu
et al., 2020; Karpukhin et al., 2020; Lewis et al.,
2020b; Khandelwal et al., 2020). However, it re-
arXiv:2304.06762v3  [cs.CL]  21 Dec 2023



Source: data\tc16_2312.10997v5\referenced_papers\[18]_2203.08773.pdf (Page 1):

or non-parametric methods.
In the experiments, we evaluate our method
on four popular types of NLP tasks: summariza-
tion, language modeling, machine translation, and
question answering. We ﬁnd that i) after inte-
grating REINA, we can achieve signiﬁcantly bet-
ter performance on these tasks, 11 datasets in to-
tal, than models with different pre-trained mod-
els; ii) REINA leads to SOTA performance on
the datasets of XSum, CommonsenseQA (Leader-
board No.1), and BigPatent; iii) REINA can scale
up more easily by leveraging more labeled data
from other datasets via retrieval, outperforming
baselines which is trained on the same set of data.
iv) the results on 3 summarization tasks show that
BART-base with REINA rivals BART-large, which
contains twice more parameters now.
The effectiveness of our approach on summa-
rization tasks provides insights into the core of
supervised learning. Even with hundreds of mil-
lions of parameters, a model cannot memorize all
the patterns in the training data. Thus, recapturing
related training data as a side-by-side reminder can
explicitly provide needed information to enhance
the model’s performance at inference. It also points
out that instead of building models of ever increas-
ing sizes, we can make a decent-size model output
high-quality results by leveraging those training
data that resemble the instance at hand. This can
signiﬁcantly reduce the computational cost while
achieving a similar or better performance of a mega-
sized model.
2 Related Work
Retrieval-based Methods Even a pre-trained
model as large as GPT-3 (Brown et al., 2020) can-
not remember everything, and it is important to
leverage information retrieval to collect external
knowledge to solve different NLP tasks. There are
two types of representations for retriever: bag-of-
word (BOW) based sparse representation (Chen
et al., 2017) and dense representation from neural
networks (Karpukhin et al., 2020).
For the sparse representation, as the method
is based on BOW and usually rule-based score,
such as BM25, is used for ranking, it can be eas-
ily adapted to a general large-scale search. This
method has also been widely explored to solve
open domain question answering (Chen et al., 2017;
Wang et al., 2018; Lin et al., 2018) and Machine
Translation (Gu et al., 2018).
Dense representation based retrieval
(DPR) (Karpukhin et al., 2020) is the most
widely explored area in recent years. Dense
representations come from encoders, such as
Transformer, trained with task-speciﬁc data.
And these methods can achieve better recall
performance than sparse representation on
different tasks, such as open domain question
answering (Karpukhin et al., 2020; Guu et al.,
2020; Yu et al., 2021), knowledge-grounded
generation (Zhang et al., 2021), and machine
translation (Cai et al., 2021). One drawback of
DPR is that it cannot process longer documents,
usually less than 128 tokens (Karpukhin et al.,
2020). Another drawback is that it needs parallel
data for model training on speciﬁc tasks.
Considering the generalization and efﬁciency of
sparse representation, in this paper, we use BM25
score (Robertson and Zaragoza, 2009; Schütze
et al., 2008) to retrieve from the training data, and
our method is more ﬂexible with no requirement of
parallel data for model training. Compared to non-
parametric systems guided by search engine (Gu
et al., 2018; Khandelwal et al., 2020), our proposed
method is based on supervised learning and is more
general. Lewis et al. (2021) is related to our work
by retrieving related questions from pre-built large-
scale question-answer pairs. However, our method
doesn’t need addition data augmentation method,
and we have successfully applied REINA to a wide
range of downstream tasks, including summariza-
tion, question answering, machine translation and
language modeling.
Prompt Engineering With the success of large-
scale language models (Brown et al., 2020) on few-
shot learning, prompt engineering comes to be a
popular research direction. The idea is to prepend
several labeled instances to the input sequence and
then conduct the classiﬁcation or generation. Liu
et al. (2021) proposes to prepend the most related
labeled data as prompt to help fewshot inference.
Li and Liang (2021) optimizes the prompt in con-
tinuous space. Motivated by these works where a
good labeled prompt can help fewshot learning, we
also prepend/append the most similar labeled train-
ing data for all the data in training, validation, and
test set. However, different from prompt learning,
we focus on supervised learning settings.



Source: data\tc16_2312.10997v5\referenced_papers\[71]_2310.04408.pdf (Page 12):

Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and
Yoav Shoham. In-context retrieval-augmented language models. ArXiv, abs/2302.00083, 2023.
Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-
IJCNLP), pp. 3982–3992, Hong Kong, China, November 2019. Association for Computational
Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/D19-1410.
Stephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond.
Found. Trends Inf. Retr., 3:333–389, 2009.
Chantal Shaib, Millicent Li, Sebastian Joseph, Iain Marshall, Junyi Jessy Li, and Byron Wal-
lace. Summarizing, simplifying, and synthesizing medical evidence using GPT-3 (with vary-
ing success). In Proceedings of the 61st Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pp. 1387–1407, Toronto, Canada, July 2023. As-
sociation for Computational Linguistics. doi: 10.18653/v1/2023.acl-short.119. URL https:
//aclanthology.org/2023.acl-short.119.
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Huai hsin Chi,
Nathanael Scharli, and Denny Zhou. Large language models can be easily distracted by ir-
relevant context. In International Conference on Machine Learning, 2023a. URL https:
//api.semanticscholar.org/CorpusID:256459776.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettle-
moyer, and Wen tau Yih. Replug: Retrieval-augmented black-box language models. ArXiv,
abs/2301.12652, 2023b.
Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan L. Boyd-Graber,
and Lijuan Wang. Prompting gpt-3 to be reliable. ArXiv, abs/2210.09150, 2022.
Charles Burton Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context.
ArXiv, abs/2209.15189, 2022. URL https://api.semanticscholar.org/CorpusID:
252668389.
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas
Blecher, Cristian Cant´on Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,
Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S.
Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel M. Kloumann, A. V . Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril,
Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar
Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang,
Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.
ArXiv, abs/2307.09288, 2023. URL https://api.semanticscholar.org/CorpusID:
259950998.
Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.
https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,
Sean Welleck, and Yejin Choi. Symbolic knowledge distillation: from general language models to
commonsense models. In Proceedings of the 2022 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies, pp. 4602–4625,
Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/
2022.naacl-main.341. URL https://aclanthology.org/2022.naacl-main.341.
13



### Claim 49/179

#### Claim Text
However, excessive context can introduce more noise, diminishing the LLM’s perception of key information . (Long) LLMLingua [100], [101] utilize small language models (SLMs) such as GPT-2 Small or LLaMA-7B, to detect and remove unimportant tokens, transforming it into a form that is challenging for humans to comprehend but well understood by LLMs.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[172]_2309.17453.pdf (Page 3):

Published as a conference paper at ICLR 2024
Dense Attention Window Attention Sliding Window 
w/ Re-computation StreamingLLM
Dense Attention Window Attention StreamingLLM
Figure 3: Language modeling perplexity on texts with 20K tokens across various LLM. Observations reveal
consistent trends: (1) Dense attention fails once the input length surpasses the pre-training attention window
size. (2) Window attention collapses once the input length exceeds the cache size, i.e., the initial tokens are
evicted. (3) StreamingLLM demonstrates stable performance, with its perplexity nearly matching that of the
sliding window with re-computation baseline.
Improving LLMs’ Utilization of Long Textoptimizes LLMs to better capture and employ the
content within the context rather than merely taking them as inputs. As highlighted by Liu et al.
and Li et al., success in the previously mentioned two directions does not necessarily translate to
competent utilization of lengthy contexts. Addressing this effective usage of prolonged contexts
within LLMs is still a challenge. Our work concentrates on stably harnessing the most recent tokens,
enabling the seamless streaming application of LLMs.
3 S TREAMING LLM
3.1 T HE FAILURE OF WINDOW ATTENTION AND ATTENTION SINKS
While the window attention technique offers efficiency during inference, it results in an exceedingly
high language modeling perplexity. Consequently, the model’s performance is unsuitable for deploy-
ment in streaming applications. In this section, we use the concept of attention sink to explain the
failure of window attention, serving as the inspiration behind StreamingLLM.
Identifying the Point of Perplexity Surge.Figure 3 shows the perplexity of language modeling on
a 20K token text. It is evident that perplexity spikes when the text length surpasses the cache size, led
by the exclusion of initial tokens. This suggests that the initial tokens, regardless of their distance
from the predicted tokens, are crucial for maintaining the stability of LLMs.
Why do LLMs break when removinginitial tokens’ KV? We visualize attention maps from
all layers and heads of the Llama-2-7B and models in Figure 2. We find that, beyond the bottom
two layers, the model consistently focuses on the initial tokens across all layers and heads. The
implication is clear: removing these initial tokens’ KV will remove a considerable portion of the
denominator in the SoftMax function (Equation 1) in attention computation. This alteration leads to a
significant shift in the distribution of attention scores away from what would be expected in normal
inference settings.
SoftMax(x)i = exi
ex1 + PN
j=2 exj
, x 1 ≫ xj, j∈ 2, . . . , N (1)
There are two possible explanations for the importance of the initial tokens in language modeling:
(1) Either their semantics are crucial, or (2) the model learns a bias towards their absolute position.
To distinguish between these possibilities, we conduct experiments (Table 1), wherein the first four
tokens are substituted with the linebreak token “\n". The observations indicate that the model still
significantly emphasizes these initial linebreak tokens. Furthermore, reintroducing them restores
the language modeling perplexity to levels comparable to having the original initial tokens. This
suggests that the absolute position of the starting tokens, rather than their semantic value, holds
greater significance.
LLMs attend to Initial Tokens as Attention Sinks.To explain why the model disproportionately
focuses on initial tokens—regardless of their semantic relevance to language modeling, we introduce
the concept of “attention sink". The nature of the SoftMax function (Equation 1) prevents all attended
tokens from having zero values. This requires aggregating some information from other tokens across
all heads in all layers, even if the current embedding has sufficient self-contained information for its
prediction. Consequently, the model tends to dump unnecessary attention values to specific tokens. A
similar observation has been made in the realm of quantization outliers (Xiao et al., 2023; Bondarenko
et al., 2023), leading to the proposal of SoftMax-Off-by-One (Miller, 2023) as a potential remedy.
4



Source: data\tc16_2312.10997v5\referenced_papers\[54]_2401.14887.pdf (Page 5):

SIGIR ’24, July 14–18, 2024, Washington, DC, USA Cuconasu and Trappolini, et al.
Table 1: Accuracy results of the LLMs when evaluated with prompts composed of the gold document ⋆and a varying number
of distracting /u♀kdocuments. The table illustrates how the inclusion of an increasing number of distracting documents affects
LLM’s performance. Scenarios where the prompt exceeded the model’s input limit, leading to potential data truncation, are not
included ( - ). All values not marked with an asterisk * denote statistically significant changes from the gold-only document
scenario [I, ⋆, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01). Additionally, the closed-book accuracy scores
for the models are as follows: Llama2 (0.1123), MPT (0.1205), Phi-2 (0.0488), Falcon (0.1083).
Far - [I, ⋆, /u♀k, Q] Mid - [I, /u♀k, ⋆, /u♀k, Q] Near - [I, /u♀k, ⋆, Q]
# /u♀kLlama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon
0 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330
1 0.4586 0.1976 0.3585 0.3469 no-mid no-mid no-mid no-mid 0.4283 0.1791 0.4227 0.3602
2 0.3455 0.1913 0.3430 0.3246 0.3322 0.1802 0.3375 0.2823 0.3974 0.2002 0.3975 0.3111
4 0.2745 0.2209* 0.3019 0.2670 0.2857 0.1775 0.2885 0.2378 0.3795 0.2059* 0.3701 0.2736
6 0.2898 0.2171* 0.2943 0.2392 0.2698 0.1424 0.2625 0.2103 0.3880 0.1892 0.3623 0.2656
8 0.2643 0.2077* 0.2513 0.1878 0.2268 0.1002 0.2360 0.1745 0.3748 0.1944 0.3423 0.2424
10 0.2537 - - - 0.2180 - - - 0.3716 - - -
12 0.2688 - - - 0.2382 - - - 0.3991 - - -
14 0.2583 - - - 0.2280 - - - 0.4118 - - -
16 0.2413 - - - 0.2024 - - - 0.3889 - - -
18 0.2348 - - - 0.1795 - - - 0.3781 - - -
Table 2: Accuracy results of the LLMs when evaluated with prompts composed of the gold document ⋆and a varying number
of random documents. Surprisingly, increasing the number of random documents in the Near setting improves LLM’s
performance. Scenarios where the prompt exceeded the model’s input limit, leading to potential data truncation, are not
included ( - ). All values not marked with an asterisk * denote statistically significant changes from the gold-only document
scenario [I, ⋆, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01). Additionally, the closed-book accuracy scores
for the models are as follows: Llama2 (0.1123), MPT (0.1205), Phi-2 (0.0488), Falcon (0.1083).
Far - [I, ⋆, , Q] Mid - [I, , ⋆, , Q] Near - [I, , ⋆, Q]
# Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon
0 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330
1 0.4733 0.2447 0.4329 0.4035 no-mid no-mid no-mid no-mid 0.4862 0.2125* 0.4587 0.4091
2 0.3776 0.2639 0.4249 0.3805 0.3928 0.2584 0.4293 0.3612 0.5032 0.2660 0.4614 0.3912
4 0.3109 0.2933 0.4091 0.3468 0.3998 0.2577 0.3985 0.3462 0.5221 0.2930 0.4311 0.3949
6 0.3547 0.3036 0.4130 0.3250 0.4138 0.2265 0.3891 0.3196 0.5681* 0.2890 0.4388 0.3908
8 0.3106 0.3039 0.3812 0.2543 0.3734 0.1566 0.3596 0.2767 0.5609* 0.2911 0.4258 0.3704
10 0.3390 - - - 0.3675 - - - 0.5579* - - -
12 0.3736 - - - 0.3641 - - - 0.5836 - - -
14 0.3527 - - - 0.3372 - - - 0.5859 - - -
16 0.3401 - - - 0.3159 - - - 0.5722 - - -
18 0.3466 - - - 0.2982 - - - 0.5588* - - -
impact on the model’s effectiveness. We define the positions of the
gold document as follows:
•Near: placed adjacent to the query in the prompt [I, /u♀k, ⋆,
Q] (as in Figure 2)
•Mid: inserted in the middle of the context [I, /u♀k, ⋆, /u♀k, Q]
•Far: positioned as far as possible from the query in the con-
text [I, ⋆, /u♀k, Q]
Results in these settings partially corroborate evidence from [30].
The accuracy is higher when the gold document is near the query,
lower when the gold document is furthest from it, and lowest when
the gold document is placed in the middle of the context. For in-
stance, Llama2, with 18 distracting documents, reaches an accuracy
of 0.37, 0.23, and 0.17, respectively. These results are consistent
across all models tested in the setting with distracting documents.
5.3 Impact of Noise
We devise an additional experimental setting aimed at evaluating
the robustness of the RAG system against noise. To this effect, we
take the gold document and add to it a certain number of docu-
ments picked at random from the corpus; see an example in Figure
4. Against our expectations, the performance does not deteriorate



Source: data\tc16_2312.10997v5\referenced_papers\[101]_2310.06839.pdf (Page 0):

LongLLMLingua: Accelerating and Enhancing LLMs in Long Context
Scenarios via Prompt Compression
Huiqiang Jiang, Qianhui Wu, Xufang Luo,
Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu
Microsoft Corporation
{hjiang,qianhuiwu,xufluo,dongsli,cyl,yuqyang,liliqiu}@microsoft.com
Abstract
In long context scenarios, large language mod-
els (LLMs) face three main challenges: higher
computational cost, performance reduction,
and position bias. Research indicates that LLM
performance hinges on the density and posi-
tion of key information in the input prompt. In-
spired by these findings, we propose LongLLM-
Lingua for prompt compression towards im-
proving LLMs’ perception of the key informa-
tion to simultaneously address the three chal-
lenges. Our extensive evaluation across vari-
ous long context scenarios demonstrates that
LongLLMLingua not only enhances perfor-
mance but also significantly reduces costs and
latency. For instance, in the NaturalQuestions
benchmark, LongLLMLingua boosts perfor-
mance by up to 21.4% with around 4x fewer
tokens in GPT-3.5-Turbo, leading to substantial
cost savings. It achieves a 94.0% cost reduction
in the LooGLE benchmark. Moreover, when
compressing prompts of about 10k tokens at ra-
tios of 2x-6x, LongLLMLingua can accelerate
end-to-end latency by 1.4x-2.6x.1
1 Introduction
Large language models (LLMs) have revolution-
ized user-oriented language technologies and are
serving as crucial components in more and more
applications. Carefully designing prompts is nec-
essary to achieve better performance in specific
downstream tasks. The commonly used technolo-
gies such as In-Context Learning (ICL) (Min et al.,
2022; Dong et al., 2023), Retrieval Augment Gener-
ation (RAG) (Lewis et al., 2020; Asai et al., 2024),
and Multi-turn Agent (Shen et al., 2024; Park et al.,
2023; Wu et al., 2023a) are driving prompts to be
increasingly longer, even reaching thousands of to-
kens. Scenarios such as multi-document question
answering, code completion, and document sum-
marization also necessitate the processing of long
contexts.
1Access our code at https://aka.ms/LongLLMLingua.
There are three main challenges when LLMs are
used in long context scenarios: (1) Higher com-
putational costs, encompassing both financial and
latency expenses. (2) Longer prompts introduce
irrelevant and redundant information, which can
weaken LLMs’ performance (Shi et al., 2023), as
illustrated in Figure 1a. (3) LLMs exhibit position
bias (Kamradt, 2023), also known as the "lost in the
middle" issue (Liu et al., 2024), suggesting that the
placement of key information within the prompt
significantly affects LLMs’ performance. This is
demonstrated by the purple curve in Figure 1b.
Inspired by these observations, we propose
LongLLMLingua to address the three challenges.
Specifically, we use LLMLingua (Jiang et al.,
2023a) as the backbone for prompt compression
to address the first challenge, i.e., reduce cost and
latency. However, in the case of long contexts, the
distribution of question-relevant key information
in the prompt is generally dynamic and sparse. Ex-
isting prompt compression methods like LLMLin-
gua (Jiang et al., 2023a) and Selective-Context (Li
et al., 2023c) that often fail to consider question
during compression, resulting in retention of exces-
sive noise and decreased performance. LongLLM-
Lingua aims to improve LLMs’ perception of key
information pertinent to the question, thereby over-
coming the noise and position bias issues in long
contexts, shown in Figure 1b. The underlying prin-
ciple of LongLLMLingua is that small LM are
inherently capable of capturing the distribution of
key information relevant to a given question.
Our main contributions are five-fold: (1) We
propose a question-aware coarse-to-fine compres-
sion method to improve the key information den-
sity in the prompt (Sec. 4.1); (2) We introduce
a document reordering strategy to minimize po-
sition bias in LLMs. (Sec. 4.2); (3) We estab-
lish dynamic compression ratios for precise con-
trol between coarse and fine compression levels
(Sec. 4.3); (4) We propose a post-compression
arXiv:2310.06839v2  [cs.CL]  12 Aug 2024



Source: data\tc16_2312.10997v5\referenced_papers\[101]_2310.06839.pdf (Page 3):

1 5 10 15 20
Number of Retained Documents
0
20
40
60
80
100Recall(%)
LLMLingua
BM25
OpenAI
Voyageai
BGE-large-en v1.5
SBERT
Gzip
Cohere-Rerank
BGE-llmembeder
Jina
LongLLMLingua rk 
w/o restrict
BGE-Ranker-large
LongLLMLingua rk
(a) Recall Distribution
1 5 10 15 20
Document Position in the Prompt
0.0
0.2
0.4
0.6
0.8
1.0Document Avg. Perplexity
Perplexity
Contrastive Perplexity (b) Perplexity Distribution (5th)
Figure 3: (a) Comparison of recall on NaturalQuestions Multi-documemnt QA dataset, which increases from top to
bottom in terms of Recall@1. Different colors represent different types of methods. Among them, yellow represents
traditional relevance methods, green signifies embedding-based methods, and red denotes rerank-based methods.
(b) Comparison between perplexities and contrastive perplexities of tokens in the prompt from Multi-documemnt
QA dataset. The document containing the ground-truth information is located in the 5th position. More results on
position can be found in the Appendix C.1.
iterative compression mechanism following LLM-
Lingua and directly calculate token perplexities
to compress xins and xque. In this section, we in-
vestigate how to make the fine-grained token-level
compression over {xdoc
k }K′
k=1 aware of the question
xque, so that the compressed results could contain
more question-relevant key information.
A straightforward solution for the awareness of
xque is to simply concatenate it at the beginning
of the whole context. However, this will result in
low perplexities of relevant tokens in the context
following the condition of question xque, further
reducing their differentiation from other tokens.
In this paper, we propose contrastive perplexity,
i.e., the distribution shift caused by the condition of
the question, to represent the association between
the token and the question. The contrastive perplex-
ity based importance metric si for each token xi in
{xdoc
k }K′
k=1 can be formulated as:
si = perplexity(xi|x<i)−perplexity(xi|xque, x<i).
(3)
Additionally, we provide the derivation of its
mathematical significance in the Appendix A, con-
cluding that it is equivalent to conditional pointwise
mutual information (Church and Hanks, 1989).
Figure 3b illustrates the difference between per-
plexities and contrastive perplexities. The distri-
bution of perplexities appears random, making it
challenging to extract information related to the
question. However, tokens with high contrastive
perplexities tend to cluster near the ground-truth
document, which contains information relevant to
the question. This suggests that the proposed con-
trastive perplexity can better distinguish tokens
relevant to the question, thus improving the key
information density in the compressed results.
4.2 How to reduce information loss in the
middle?
As demonstrated in Figure 1b, LLM achieves the
highest performance when relevant information oc-
curs at the beginning and significantly degrades if
relevant information is located in the middle of long
contexts. After the coarse-grained compression, we
have obtained a set of documents {xdoc
k }K′
k=1 with
their corresponding importance scores {rk}K′
k=1 in-
dicating their association with the question xque.
Therefore, we reorder documents using their impor-
tance scores to better leverage LLMs’ information
perception difference in positions:
(xins, xdoc
1 , ··· , xdoc
K′ ,xque)
rk
−→
(xins,xdoc
r1 , ··· , xdoc
rK′ , xque)
(4)
4.3 How to achieve adaptive granular control
during compression?
In fine-grained compression, LLMLingua applies
the same compression ratio over all documents ob-
tained from budget controller. However, the key
information density of different documents is differ-
ent. The more relevant to the question a document



Source: data\tc16_2312.10997v5\referenced_papers\[54]_2401.14887.pdf (Page 6):

The Power of Noise: Redefining Retrieval for RAG Systems SIGIR ’24, July 14–18, 2024, Washington, DC, USA
Table 3: Accuracy of Llama2-7b in configurations involving random Wikipedia documents and retrieved documents [I, , , Q].
Rows denote the number of random documents added, and columns show the quantity of retrieved documents . The left
section reports results using Contriever, and the right section using BM25. Scenarios where the prompt exceeded the model’s
input limit, leading to potential data truncation, are not included ( - ). Each value not marked with an asterisk * represents a
statistically significant change from the base case of retrieved documents only [I, , Q] (first row), as determined by a Wilcoxon
test (p-value < 0.01).
Contriever BM25
#
# 1 2 3 4 5 8 10 1 2 3 4 5 8 10
0 0.1620 0.1866 0.1876 0.1866 0.1921 0.2198 0.2108 0.2008 0.2208 0.2084 0.2028 0.2243 0.2492 0.2447
1 0.1308 0.1616 0.1717 0.1893* 0.1987* 0.2153* 0.2146* 0.1568 0.1963 0.1921 0.2115 0.2295* 0.2475* 0.2506*
2 0.1315 0.1644 0.1859* 0.2008 0.2174 0.2156* 0.2368 0.1644 0.1973 0.2080* 0.2281 0.2558 0.2495* 0.2596
3 0.1301 0.1727 0.2008 0.2316 0.2201 0.2198 0.2409 0.1568 0.2063 0.2160 0.2520 0.2579 0.2644 0.2707
5 0.1464 0.2056 0.2233 0.2240 0.2150 0.2451 0.2482 0.1772 0.2402 0.2437 0.2520 0.2554 0.2804 0.2866
8 0.1734 0.2066 0.2336 0.2375 0.2454 0.2416 0.2364 0.1994 0.2451 0.2579 0.2769 0.2817 0.2859 0.2777
10 0.1796 0.2174 0.2450 0.2502 0.2499 0.2420 - 0.2108 0.2589 0.2734 0.2835 0.2935 0.2853 -
15 0.2018 0.2354 0.2551 0.2530 - - - 0.2243 0.2686 0.2790 0.2928 - - -
16 0.2032 0.2471 0.2558 - - - - 0.2323 0.2662 0.2838 - - - -
17 0.2039 0.2426 - - - - - 0.2326 0.2693 - - - - -
18 0.2073 - - - - - - 0.2309 - - - - - -
Figure 3: This heatmap depicts the attention distribution
across the context documents from the example shown in
Figure 2, relative to the answer generated by Llama2-7b in
a prompt structured as [I, /u♀k, ⋆, Q]. Cell (i, j) denotes the
mean attention that tokens in the generated answer allocate
to the tokens of the i-th document within the j-th attention
layer. This mean attention for each document is calculated
by averaging the attention scores across all its constituent
tokens.
in the presence of noise, as can be seen in Table 2. Instead, we ob-
serve an improvement in performance under the best-performing
setting (near [I, , ⋆, Q]), with an improvement of 0.08 (+36%) in
LLM Input - Random and Gold ⋆
Task instruction...
Documents:
Document [140](Title: Richard Yates (novelist)) For much
of his life, Yates’s work met almost universal critical ac-
claim, yet not one of his books sold over 12,000 copies in...
Document [242] (Title: Android version history) Code
name Version number Initial release date API level Security
patches (No codename ) 1.0 September 23...
Document [3](Title: Millennium Falcon) Han Solo won
the Millennium Falcon from Lando Calrissian in the card
game sabacc...
Question: who owned the millennium falcon be-
fore han solo
Answer: Lando Calrissian
Figure 4: Example LLM input with a correct output, high-
lighted in green. The context of the prompt is composed of
random documents and the gold near the query. The task
instruction is as in Figure 1.
the case of MPT. Furthermore, we observe that different models
exhibit distinct behaviors. Both Llama2 and Phi-2 showed improve-
ments in this setting when the noise is introduced furthest from
the query. However, when the noise is positioned in the far[I, ⋆,
, Q] and mid [I, , ⋆, , Q] settings, these models exhibit a
decline in performance. Notably, this performance degradation is
much less accentuated when compared to the earlier setting with
distracting documents. This suggests that while Llama2 and Phi-2
can effectively handle noise far from the query, their ability to sift



### Claim 50/179

#### Claim Text
PRCA tackled this issue by training an information extractor [69].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[18]_2203.08773.pdf (Page 2):

Figure 2: Model training with retrieval from the training data ( REINA ). (a) Index on the training data and data
retrieval for 4 different tasks. Box in blue is the query or the input sequence to encode. Box in green is the
retrieved text. (b-e) Leveraging retrieved data for model training with different structures. For language modeling,
we prepend the retrieved data to the query data, and append the retrieved data to the query for all the other tasks.
After concatenation, we will directly feed them into Transformers, either Seq2Seq or Encoder-only frameworks,
for text generation and answering selection. As we focus on the question answering tasks requiring commonsense
reasoning, we have another version of index integrating knowledge graph for more precise retrieval. K: external
knowledge from ConceptNet and Wiktionary, src: source language, tgt: target language.
3 Model
In this section, we will introduce the details of our
proposed method. Brieﬂy, given the input, we ﬁrst
retrieve the most matched instances with labels
from the training data. We then concatenate them
with the input sequence to feed into the model for
generating the output. An overview of the whole
method is shown in Figure 2.
3.1 Retrieval-based Methods
A retrieval-based method collects information most
similar to the input from a corpus and then com-
bines it with the input to feed into the NLP model.
Suppose we index the corpus into a list of key-value
pairs, i.e. C = {(ki, vi)}. Then, given the input x,
the retrieval engine E matches it with all keys and
returns the top K most similar keys to the query
together with their values:
{(ki1 , vi1 ), ...,(kiK , viK )} = E(x|C) (1)
In this work, we build the retrieval engine based on
the widely used BM25 score (Schütze et al., 2008).
We choose BM25 over dense representation mainly
for its faster speed.
Then, these retrieved results are combined with
the input x to feed into the NLP model M to gen-
erate the output O:
O = M(f(x, {(ki1 , vi1 ), ...,(kiK , viK )}) (2)
Here, the combination functionf can be concate-
nation, e.g. f(x, {(ki1 , vi1 ), ...,(kiK , viK )}) =
[x; vi1 ; ...; viK ]. As data in different tasks is orga-
nized in different formats with varying lengths, we
will introduce how we deﬁne different combination
functions f for various tasks in the follows.
3.2 Retrieval from Training Data ( REINA )
As retrieval from a large corpus is computation-
ally costly, we propose to retrieve from the labeled



Source: data\tc16_2312.10997v5\referenced_papers\[18]_2203.08773.pdf (Page 0):

Training Data is More Valuable than You Think:
A Simple and Effective Method by Retrieving from Training Data
Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu,
Chenguang Zhu, Michael Zeng
Microsoft Azure Cognitive Services Research
{shuowa, yicxu, yuwfan, yaliu10, siqisun, ruox, chezhu, nzeng}@microsoft.com
Abstract
Retrieval-based methods have been shown to
be effective in NLP tasks via introducing ex-
ternal knowledge. However, the indexing and
retrieving of large-scale corpora bring consid-
erable computational cost. Surprisingly, we
found that REtrieving from the traINing datA
(REINA) only can lead to signiﬁcant gains on
multiple NLG and NLU tasks. We retrieve the
labeled training instances most similar to the
input text and then concatenate them with the
input to feed into the model to generate the out-
put. Experimental results show that this simple
method can achieve signiﬁcantly better perfor-
mance on a variety of NLU and NLG tasks,
including summarization, machine translation,
language modeling, and question answering
tasks. For instance, our proposed method
achieved state-of-the-art results on XSum, Big-
Patent, and CommonsenseQA. Our code is re-
leased.1
1 Introduction
In natural language processing, retrieval-based
methods work by fetching textual information re-
lated to the input from large corpora. The model
then takes both the input and retrieved results as
input to generate results. This can often improve
the performance as the model is exposed to related
knowledge not present in the input. As a result,
retrieval-based methods have been successfully ap-
plied in many tasks such as open-domain question
answering (Chen et al., 2017), language model-
ing (Guu et al., 2018; Khandelwal et al., 2020)
and machine translation (Khandelwal et al., 2021).
However, these methods require building an index
of large-scale corpus, and the retrieval leads to a
signiﬁcant computational burden. For example, the
kNN-MT model for machine translation has a gen-
eration speed two orders of magnitude slower than
traditional MT models (Khandelwal et al., 2021).
1https://github.com/microsoft/REINA
Figure 1: REINA pipeline of model training/inference
with retrieval from training data. Filter only happens at
training, as the same training sample will be retrieved
from the index. For each instance, we concatenate the
input with the retrieved content, i.e., data and/or labels,
for model training and inference.
On the other hand, in the supervised learning
setting, the text most similar in distribution to the
data in inference is the training data. Thus, we
explore whether retrieving from the training data,
which is usually much smaller than a large-scale
corpus, can help improve the performance. Specif-
ically, we ﬁrst index a task’s labeled training data
as input-label pairs. Then, during both training and
testing, we retrieve the input-label pairs most sim-
ilar to the current input2. Finally, we concatenate
the retrieved training pairs with the input and feed
it into the model. An overview of our method is
shown in Figure 1.
We note that our method is similar to recent
works in prompt learning (Brown et al., 2020; Liu
et al., 2021), where a set of labeled data is carefully
chosen based on the input and then included in the
prompt for few-shot learning. Our method also
bears a resemblance to non-parametric instance-
based learning (Gu et al., 2018). However, a crit-
ical difference is that we focus on the supervised
learning setting, where the model parameters are
ﬁne-tuned to learn from given examples to achieve
much higher performance than few-shot learning
2During training, we exclude the training instance itself
from the retrieval results to avoid data leakage.
arXiv:2203.08773v1  [cs.CL]  16 Mar 2022



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 7):

Conference’17, July 2017, Washington, DC, USA Zhang and Xiao, et al.
Table 4: Ablation study for the three influential factors about LLM-Embedder’s training: using soft reward from LLM, stabilized
distillation, instruction based fine-tuning, in-batch negative sampling from the same scenario.
Knowledge ICL Long Tool Conv Search
Method MMLU PopQA Misc. MSC ArXiv ToolLLM QReCC
w.o. LLM Reward 0.4872 0.4794 0.6217 13.9176 3.2495 0.8927 0.4945
w.o. Instruction FT 0.4776 0.5025 0.6211 13.9125 3.2383 0.8192 0.5029
w.o. homo NS 0.4791 0.4520 0.6200 14.0441 3.2558 0.8364 0.4563
w.o. Stablized Distill 0.4815 0.5027 0.6105 13.6090 3.2441 0.7905 0.4865
AAR 0.4826 0.4792 0.5938 14.6999 3.3260 0.4200 0.2877
API-Retriever 0.4625 0.2488 0.5942 14.7834 3.3858 0.8017 0.1137
LLM-R 0.4625 0.2506 0.6262 14.4746 3.3635 0.1321 0.0234
LLM-Embedder 0.4903 0.5052 0.6268 13.4832 3.2322 0.8645 0.5053
the embeddings, because a great portion of the negative samples
will come from different tasks, which are irrelevant with each other.
As we can observe, LLM-Embedder’s performance is decreased due
to such a change, especially for PopQA and Conv Search, where a
massive candidate pool is presented (Wikipedia corpus).
For “w.o. stabilized distill ”, we replace our stabilized distilla-
tion with the conventional KL-divergence based method. As intro-
duced, this operation handles the fluctuated reward from LLM such
that distillation can become more stabilized. We can observe that
LLM-Embedder’s performance is reduced once this step is removed,
especially for ICL where LLM’s reward is the major training signal.
4 RELATED WORKS
The related works are reviewed from two perspectives: retrieval
augmented large language models, and dense retrieval.
•Retrieval Augmented LLMs . Large language models (LLMs)
are praised for their unprecedented capability on language under-
standing and generation. Compared with the conventional methods,
LLMs exhibit overwhelming generality and notable advantages on
typical NLP tasks [17, 19, 78]. Despite such superiority, LLMs still
face a series of severe challenges, such as hallucination, human
alignment, and long-term memory. Many of the existing problems
are caused by the inherent boundaries, which cannot be addressed
by LLMs alone, but to rely on support from the external world. The
retrieval-augmented LLMs are regarded as a go-to option to bridge
LLMs with the external assistance [4, 51]. For the past few years,
they have been widely applied to several critical scenarios. One com-
mon case is the knowledge enhancement. The internal knowledge
of LLMs can be incomplete, static, and limited by the popularity
bias. When dealing with knowledge intensive tasks, the retrieval
augmented LLMs will look for necessary information from an ex-
ternal database, where the generated content can be grounded on
proper knowledge [15, 31, 32, 41]. Besides, the retrieval augmented
LLMs are also used to retrieve historical context to establish long-
term memory [71, 85], retrieve examples to improve the instruction
following capability [18, 83], and retrieve tools to engage with the
physical world [62].
The retrieval augmented LLMs consist of two basic parts: gener-
ator and retriever. According to previous studies [32, 41, 83, 96], the
retrieval augmentation effect is highly influenced by the retrieved
content. In practice, there are two common types of retrievers. One
is to leverage the general purpose retrievers, such as sparse models
like BM25 [69], and dense models, like DPR [37], contriever [30],
E5 [81], BGE [89], OpenAI text embedding [56]. The other option
is develop task-specific retriever, e.g., AAR for knowledge enhance-
ment [96], LLM-R [85] for in-context learning. The general pur-
pose methods are praised for their generality and simplicity for
usage, but may suffer from an inferior retrieval quality. In contrast,
the task-specific ones can better fit one scenario, but fall short in
transferability. Compared with the existing works, LLM-Embedder
unifies the generality and speciality: it comprehensive supports all
major retrieval augmentation needs of LLMs, meanwhile achieving
the leading performance in every application scenario.
•Dense retrieval. Dense retrieval leverages latent representa-
tion of texts, i.e. embeddings, to search for relevant information
from a vector DB. In recent years, it has grown into a major para-
digm of information retrieval. The success of dense retrieval can
attribute to several reasons. The first and foremost driving force is
the development of pre-trained language models[22, 45, 65], where
the textual data can be represented in a highly expressive man-
ner. The general pre-trained models are further improved by the
retrieval-oriented ones [46, 81], which better establish the sentence-
level representation capability during the pre-training stage. The
second factor is the advancement of contrastive learning. On one
hand, there has been a major upgrade of negative sampling, where
massive [30, 37] and sufficiently hard samples [92] are utilized to
help with the embedding’s discriminativeness. On the other hand,
the training objective is improved as well. Instead of simply learning
from hard labels, the embedding models are made to distill knowl-
edge from a more precise ranking model [29, 63, 90]. This notably
facilitates the embedding model to encode fine-grained semantic
relationships. Thirdly, the generality becomes increasingly empha-
sized in these days, where embeddings need to handle a wide variety
of application scenarios. For this purpose, people come up with
many different strategies, e.g., data augmentation [42, 80], domain
adaptation [36, 95], instruction-based fine-tuning [ 5, 76], which
help the model to better handle diverse tasks. These factors are
incorporated and optimized while developing our training recipe,
which results in the empirical competitiveness of LLM-Embedder.



Source: data\tc16_2312.10997v5\referenced_papers\[18]_2203.08773.pdf (Page 3):

training data. In other words, we directly adopt the
training data T = {(x1, y1), ...,(xN , yN )} as the
indexed corpus C, where xi is the input and yi is
the ground-truth label.
Given an input x, the top K retrieved
training instances with labels are combined
with x as input to the model M, i.e.,
M(f(x, {(xi1 , yi1 ), ...,(xiK , yiK )}. Both training
and inference take this retrieve-combine-generate
scheme. Note that during training, as the input x
is already indexed, we ﬁlter it from the retrieval
results to avoid data leakage.
Now, we introduce how we deﬁne the keys, val-
ues, and the combination function for different
NLP tasks.
Summarization is to generate a summary for
a given document. We ﬁrst build an index for
the document-summary pairs in the training data,
where a document is the key and its summary
is the value. Given a document x, we search
for the most similar documents in the index. As
documents are usually quite long, the combination
function only keeps the values (summaries),
i.e., fsumm(x, {(xi1 , yi1 ), ...,(xiK , yiK )}) =
[x; yi1 ; ...; yiK ].
Language Modeling(LM) generates the prob-
ability of a given sequence of words. Typically, a
Left-to-Right language model (Dong et al., 2020)
is trained on chunked sequences with an attention
mask. In this paper, we use Seq2Seq based ap-
proach, i.e., given a context chunk, we predict the
next chunk of text.
In detail, we ﬁrst chunk all the text in the
training data. The IR index is built with one
chunk Ci as the key xi and its next chunk
Ci+1 as the value yi. Given a chunk x, we
look for the most similar keys in the index
and prepend their corresponding next chunks
to x, i,e., fLM (x, {(xi1 , yi1 ), ...,(xiK , yiK )}) =
[yi1 ; ...; yiK ; x].
Machine Translationis to translate text from
the source language S to the target language T .
We deﬁne the key to be the sentence in S and the
value to be its translation in T . To keep the se-
quence short and speed up the training process,
we only concatenate the retrieved text in target
language: fMT (x, {(xi1 , yi1 ), ...,(xiK , yiK )}) =
[x; yi1 ; ...; yiK ].
Question Answering We mainly consider
multiple-choice question answering, where com-
monsense knowledge is also required to reach the
correct answer. For each question xi, there is a
correct choice yi and several distractive candidate
choices. We index the concatenation of the ques-
tion and the corresponding ground-truth choice.
For a new question x, the model is given sev-
eral choices c1, ..., cM . We concatenate x with
each choice ci as the query and retrieve related
training instances: {(xi1 , yi1 ), ...,(xiK , yiK )} =
E(x; ci|C). The combination function f concate-
nates both retrieved question and answers with the
input: fQA((x, ci), {(xi1 , yi1 ), ...,(xiK , yiK )}) =
[x; ci; xi1 ; yi1 ; ...; xiK ; yiK ]. Then, the model pre-
dicts a score representing how likely ci is the cor-
rect choice to x.
As the task requires commonsense knowledge,
we build another version of index integrating com-
monsense knowledge. We follow the strategy from
(Xu et al., 2021) and extract the knowledge from
ConceptNet (Speer et al., 2017) and Wiktionary3
for the concepts in the question and choices. For
each question x and choice c, we use string
match to ﬁnd corresponding entities in Concept-
Net: E(x) = {e(x)
1 , ..., e(x)
nx } appears in the ques-
tion, and E(c) = {e(c)
1 , ..., e(c)
nc } appears in the an-
swer. To ﬁnd the most relevant concept, we choose
the concept with maximum length as the question
and answer concept. We ﬁnd the deﬁnition of the
chosen concepts from Wiktionary. To ﬁnd relations
in ConceptNet, we ﬁnd edges that connects ques-
tion and answer concepts: R = {(e1, r, e2)|e1 ∈
E(x), e2 ∈ E(c), (e1, e2) ∈ KG}. Here KG is Con-
ceptNet and r is a relation (e.g., AtLocation).
We concatenate the Wiktionary deﬁnitions and Con-
ceptNet relations R to form the knowledge, K, for
a question. The knowledge K is included both in
the query and index. Thus, the retrieval process
becomes: {(xi1 , ci1 , Ki1 ), ...,(xiK , yiK , KiK )} =
E(x; ci; K|C). The combination function f
concatenates retrieved questions and answers
with the input: fQAK((x, ci), E(x; ci; K|C)) =
[x; ci; xi1 ; yi1 ; ...; xiK ; yiK ].
3.3 Model Training and Inference
After concatenating the input with the retrieved
data from the training corpus, we feed the new se-
quence into the Seq2Seq framework for generation
tasks and the encoder-only framework for question
answering tasks. During training, as it will also
retrieve the exact golden label, we ﬁlter it directly.
During inference, we will not ﬁlter any retrieved
3https://www.wiktionary.org/



Source: data\tc16_2312.10997v5\referenced_papers\[42]_2208.03299.pdf (Page 6):

updated, as the true top-k documents may not be retrieved in the top-L results from the stale index. In
practice, it is possible to track the positions of the top-K re-ranked documents in the top-L, and estimate
when the index needs to be updated.
Query-side ﬁne-tuning. Finally, the last strategy is to decouple the encoding of the queries and documents.
In this case, we ﬁx the parameters corresponding to the document encoder, and only train the parameters
corresponding to the query encoder. Thus, the embeddings of documents are ﬁxed, and we do not need to
refresh the index, and thus there is no computational overhead. As we will see in practice, the impact of
ﬁxing the documents encoder varies greatly for diﬀerent tasks when a large training dataset is available. For
most of the few-shot settings that we consider, query-side ﬁnetuning does not have large performance impact,
and sometimes even slightly improves performance.
3 Related work
3.1 Retrieval in natural language processing
Retrieval for knowledge intensive tasks.Previous work has shown that retrieval improves performance
across a variety of tasks such as question answering (Voorhees et al., 1999; Chen et al., 2017; Kwiatkowski et al.,
2019), fact checking (Thorne et al., 2018), dialogue (Dinan et al., 2019) or citation recommendation (Petroni
et al., 2022). Historically, this information retrieval step was implemented using term-matching methods, such
as TF-IDF or BM25 (Jones, 1972; Robertson et al., 1995). For open-domain question answering (Voorhees
et al., 1999), documents are often retrieved from Wikipedia (Chen et al., 2017). Recently, dense retrievers
based on neural networks have become popular. These usually follow a dual-encoder architecture (Yih et al.,
2011; Huang et al., 2013; Shen et al., 2014), where queries and passages are encoded independently as vectors,
and relevance is computed using the inner product or Euclidean distance. Popular supervised retrievers
include DPR (Karpukhin et al., 2020), which is trained to discriminate the relevant passage among negative
passages, and extensions such as ANCE (Xiong et al., 2020) which improved the hard negatives mining
process. We refer the reader to Yates et al. (2021) for a survey of dense retrieval techniques.
After retrieval, the relevant documents are processed to produce the ﬁnal output. In open-domain QA, models
can extract a span of text from retrieved documents as the answer (Chen et al., 2017; Clark & Gardner, 2018;
Wang et al., 2019; Karpukhin et al., 2020), a method inspired by reading comprehension (Richardson, 2013;
Rajpurkar et al., 2016). Recently, generating the answer as free-form text, using a seq2seq model conditioned
on retrieved documents have become prevalent (Lewis et al., 2020; Izacard & Grave, 2020; Min et al., 2020).
These architectures have also been shown to reduce hallucination in dialogue agents (Shuster et al., 2021).
Retriever training. The need for expensive query-document annotations for training the retriever can be
bypassed, by leveraging signals from the language model, or using unsupervised learning. REALM (Guu et al.,
2020) and RAG (Lewis et al., 2020) jointly train the retriever and language model by modelling documents as
latent variable, and minimizing the objective with gradient descent. REALM pre-trains end-to-end with an
MLM approach but uses an extractive BERT-style model (Devlin et al., 2019). Guu et al. (2020) also explore
a query-side ﬁnetuning at ﬁnetuning time to avoid index refreshes, which is also explored in the context of
phrase-based retrieval by Lee et al. (2021b). Izacard & Grave (2020) proposed to use cross-attention scores
as supervision with knowledge distillation. Sachan et al. (2021) perform joint training of the reader and the
retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee
et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention
scores from the language model. Theinverse cloze taskwas proposed by Lee et al. (2019) to pre-train dense
retrievers in an unsupervised way. Paranjape et al. (2021) propose a method to train retrieval-augmented
generators using a second “informed” retriever with access to the output, which the test-time retriever can be
distilled from, and Hofstätter et al. (2022) recently proposed a training set ﬁltering/weighting approach to
train stronger retrieval-augmented generators. Izacard et al. (2022) explored diﬀerent contrastive learning
methods to train retrievers, while Ram et al. (2022) used recurring spans within a document to create
pseudo-positive query-document pairs.
7



### Claim 51/179

#### Claim Text
Similarly, RECOMP adopts a comparable approach by training an information condenser using contrastive learning [71].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[18]_2203.08773.pdf (Page 1):

or non-parametric methods.
In the experiments, we evaluate our method
on four popular types of NLP tasks: summariza-
tion, language modeling, machine translation, and
question answering. We ﬁnd that i) after inte-
grating REINA, we can achieve signiﬁcantly bet-
ter performance on these tasks, 11 datasets in to-
tal, than models with different pre-trained mod-
els; ii) REINA leads to SOTA performance on
the datasets of XSum, CommonsenseQA (Leader-
board No.1), and BigPatent; iii) REINA can scale
up more easily by leveraging more labeled data
from other datasets via retrieval, outperforming
baselines which is trained on the same set of data.
iv) the results on 3 summarization tasks show that
BART-base with REINA rivals BART-large, which
contains twice more parameters now.
The effectiveness of our approach on summa-
rization tasks provides insights into the core of
supervised learning. Even with hundreds of mil-
lions of parameters, a model cannot memorize all
the patterns in the training data. Thus, recapturing
related training data as a side-by-side reminder can
explicitly provide needed information to enhance
the model’s performance at inference. It also points
out that instead of building models of ever increas-
ing sizes, we can make a decent-size model output
high-quality results by leveraging those training
data that resemble the instance at hand. This can
signiﬁcantly reduce the computational cost while
achieving a similar or better performance of a mega-
sized model.
2 Related Work
Retrieval-based Methods Even a pre-trained
model as large as GPT-3 (Brown et al., 2020) can-
not remember everything, and it is important to
leverage information retrieval to collect external
knowledge to solve different NLP tasks. There are
two types of representations for retriever: bag-of-
word (BOW) based sparse representation (Chen
et al., 2017) and dense representation from neural
networks (Karpukhin et al., 2020).
For the sparse representation, as the method
is based on BOW and usually rule-based score,
such as BM25, is used for ranking, it can be eas-
ily adapted to a general large-scale search. This
method has also been widely explored to solve
open domain question answering (Chen et al., 2017;
Wang et al., 2018; Lin et al., 2018) and Machine
Translation (Gu et al., 2018).
Dense representation based retrieval
(DPR) (Karpukhin et al., 2020) is the most
widely explored area in recent years. Dense
representations come from encoders, such as
Transformer, trained with task-speciﬁc data.
And these methods can achieve better recall
performance than sparse representation on
different tasks, such as open domain question
answering (Karpukhin et al., 2020; Guu et al.,
2020; Yu et al., 2021), knowledge-grounded
generation (Zhang et al., 2021), and machine
translation (Cai et al., 2021). One drawback of
DPR is that it cannot process longer documents,
usually less than 128 tokens (Karpukhin et al.,
2020). Another drawback is that it needs parallel
data for model training on speciﬁc tasks.
Considering the generalization and efﬁciency of
sparse representation, in this paper, we use BM25
score (Robertson and Zaragoza, 2009; Schütze
et al., 2008) to retrieve from the training data, and
our method is more ﬂexible with no requirement of
parallel data for model training. Compared to non-
parametric systems guided by search engine (Gu
et al., 2018; Khandelwal et al., 2020), our proposed
method is based on supervised learning and is more
general. Lewis et al. (2021) is related to our work
by retrieving related questions from pre-built large-
scale question-answer pairs. However, our method
doesn’t need addition data augmentation method,
and we have successfully applied REINA to a wide
range of downstream tasks, including summariza-
tion, question answering, machine translation and
language modeling.
Prompt Engineering With the success of large-
scale language models (Brown et al., 2020) on few-
shot learning, prompt engineering comes to be a
popular research direction. The idea is to prepend
several labeled instances to the input sequence and
then conduct the classiﬁcation or generation. Liu
et al. (2021) proposes to prepend the most related
labeled data as prompt to help fewshot inference.
Li and Liang (2021) optimizes the prompt in con-
tinuous space. Motivated by these works where a
good labeled prompt can help fewshot learning, we
also prepend/append the most similar labeled train-
ing data for all the data in training, validation, and
test set. However, different from prompt learning,
we focus on supervised learning settings.



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 6):

Retrieve Anything To Augment Large Language Models Conference’17, July 2017, Washington, DC, USA
Table 2: Impact on in-context learning. The performances are measured by Misc. metrics (see Appendix).
In-Context Learning
Method CQA Comm Coref Para NLI RC Sent D2T Summ Avg
None 0.2923 0.7212 0.6578 0.5242 0.4478 0.4892 0.7077 0.1982 0.1447 0.4645
BM25 0.3603 0.7019 0.6029 0.5059 0.4583 0.5396 0.7284 0.3019 0.1555 0.4840
Instructor 0.5003 0.7772 0.5735 0.6312 0.5360 0.6219 0.9148 0.4595 0.4572 0.6036
Contriever 0.4912 0.7723 0.5624 0.6358 0.5466 0.6297 0.9141 0.4380 0.4444 0.6009
RetroMAE-BEIR 0.4594 0.7742 0.5840 0.5755 0.5408 0.6029 0.9286 0.4661 0.4465 0.5939
BGE∗ 0.4718 0.7773 0.5550 0.6171 0.5413 0.5988 0.9281 0.4719 0.4521 0.5974
AAR 0.4809 0.7796 0.5848 0.5890 0.5354 0.6039 0.9210 0.4445 0.4410 0.5938
API-Retriever 0.4765 0.7620 0.5465 0.6266 0.5204 0.6096 0.9245 0.4866 0.4424 0.5945
LLM-R† 0.5165 0.7802 0.5830 0.6567 0.6145 0.6223 0.9059 0.4777 0.4878 0.6262
LLM-Embedder 0.5163 0.7842 0.5927 0.6556 0.6041 0.6318 0.9224 0.4731 0.4742 0.6268
Table 3: Impact on long conversation and language modeling (PPL), tool learning (NDCG), conv search (NDCG).
Conversation Language Modeling Tool C-Search
Method MSC Books3 Arxiv CodeParrot PG19 (o.d.) ToolLLM QReCC
None 19.3501 8.8193 3.7647 2.7663 10.2510 – –
Recency 13.9569 8.7391 3.4158 2.5989 10.2216 – –
BM25 14.6512 8.6576 3.3106 2.4591 10.1960 0.5115 0.4341
Instructor 14.8799 8.6619 3.3546 2.4756 10.2011 0.3882 0.2863
Contriever 14.2129 8.6460 3.2709 2.4437 10.1616 0.4904 0.3563
RetroMAE-BEIR 14.3990 8.6376 3.2903 2.4592 10.1731 0.5205 0.4037
BGE∗ 14.2943 8.6311 3.2912 2.4578 10.1541 0.5761 0.3856
AAR 14.6999 8.6381 3.3260 2.4666 10.1808 0.4200 0.2877
API-Retriever† 14.7834 8.6722 3.3858 2.4919 10.1833 0.8017 0.1137
Conv-ANCE† – – – – – – 0.4560
LLM-R 14.4746 8.6619 3.3635 2.4724 10.2024 0.1321 0.0234
LLM-Embedder 13.4832 8.6080 3.2322 2.4303 10.1185 0.8645 0.5053
a simple yet strong baseline called Recency. Rather than using re-
trieved context, Recency directly leverages the most recent context
immediately preceding the current window. For example, in con-
versation, it considers the last pair of utterances before the current
session; and in language modeling, it introduces the content within
the range of 2049-4096 tokens preceding the latest 2048 tokens.
With the introduction of this new baseline, the impact of retrieval
augmentation becomes more nuanced. On one hand, the LLM-
Embedder continues to exhibit superior performance across various
situations. On the other hand, other retrievers no longer guarantee a
consistent enhancement: although alternative retrieval-augmented
methods yield improved generation quality for language modeling,
a majority of them fall short of Recency’s performance while dealing
with conversation. This observation underscores the challenges
regarding effective memory retrieval in practice.
•Tool Learning and Conversation Search . The experiment
results on tool learning and conversational search are shown in
Table 3. In line with our prior observations, the task-specific ap-
proaches, i.e. the API retriever (Tool) and Conv-ANCE (Conv Search),
consistently deliver higher performances then most of the baselines.
Besides, unlike other cases, BM25 overtakes most of the embedding
models in these two scenarios. However, it’s worth noting that
LLM-Embedder continues to maintain the leading position, which
again highlights its capability in unifying diverse retrieval tasks.
3.2.3 Ablation Studies. The ablation studies are presented to ana-
lyze the influential factors about LLM-Embedder’s training process
(see Table 4): reward from LLM, instruction based fine-tuning, ho-
mogeneous in-batch negative sampling, and stabilized distillation.
For “w.o. LLM reward ”, we replace the soft reward from LLM
by using highest rated candidates as positive samples (i.e. hard la-
bels). By doing so, the knowledge distillation is reduced to contrast
learning. The empirical performance in most of the scenarios are
decreased due to such a change. However, the performances in tool
learning and conversational search are little affect; this is compre-
hensible knowing that LLM-Embedder is purely trained with hard
labels in both scenarios.
For “w.o. instruction FT ”, we remove the task-specific instruc-
tions while fine-tuning LLM-Embedder. Without such a component,
it will become harder for the embedding model to discriminate the
retrieval task in different scenarios. This speculation is consistent
with the observed result, as LLM-Embedder’s performance is de-
creased from such a change.
For “w.o. homo NS ”, the homogeneous in-batch negative sam-
pling is disabled. Such a change could reduce the discrimination of



Source: data\tc16_2312.10997v5\referenced_papers\[18]_2203.08773.pdf (Page 0):

Training Data is More Valuable than You Think:
A Simple and Effective Method by Retrieving from Training Data
Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu,
Chenguang Zhu, Michael Zeng
Microsoft Azure Cognitive Services Research
{shuowa, yicxu, yuwfan, yaliu10, siqisun, ruox, chezhu, nzeng}@microsoft.com
Abstract
Retrieval-based methods have been shown to
be effective in NLP tasks via introducing ex-
ternal knowledge. However, the indexing and
retrieving of large-scale corpora bring consid-
erable computational cost. Surprisingly, we
found that REtrieving from the traINing datA
(REINA) only can lead to signiﬁcant gains on
multiple NLG and NLU tasks. We retrieve the
labeled training instances most similar to the
input text and then concatenate them with the
input to feed into the model to generate the out-
put. Experimental results show that this simple
method can achieve signiﬁcantly better perfor-
mance on a variety of NLU and NLG tasks,
including summarization, machine translation,
language modeling, and question answering
tasks. For instance, our proposed method
achieved state-of-the-art results on XSum, Big-
Patent, and CommonsenseQA. Our code is re-
leased.1
1 Introduction
In natural language processing, retrieval-based
methods work by fetching textual information re-
lated to the input from large corpora. The model
then takes both the input and retrieved results as
input to generate results. This can often improve
the performance as the model is exposed to related
knowledge not present in the input. As a result,
retrieval-based methods have been successfully ap-
plied in many tasks such as open-domain question
answering (Chen et al., 2017), language model-
ing (Guu et al., 2018; Khandelwal et al., 2020)
and machine translation (Khandelwal et al., 2021).
However, these methods require building an index
of large-scale corpus, and the retrieval leads to a
signiﬁcant computational burden. For example, the
kNN-MT model for machine translation has a gen-
eration speed two orders of magnitude slower than
traditional MT models (Khandelwal et al., 2021).
1https://github.com/microsoft/REINA
Figure 1: REINA pipeline of model training/inference
with retrieval from training data. Filter only happens at
training, as the same training sample will be retrieved
from the index. For each instance, we concatenate the
input with the retrieved content, i.e., data and/or labels,
for model training and inference.
On the other hand, in the supervised learning
setting, the text most similar in distribution to the
data in inference is the training data. Thus, we
explore whether retrieving from the training data,
which is usually much smaller than a large-scale
corpus, can help improve the performance. Specif-
ically, we ﬁrst index a task’s labeled training data
as input-label pairs. Then, during both training and
testing, we retrieve the input-label pairs most sim-
ilar to the current input2. Finally, we concatenate
the retrieved training pairs with the input and feed
it into the model. An overview of our method is
shown in Figure 1.
We note that our method is similar to recent
works in prompt learning (Brown et al., 2020; Liu
et al., 2021), where a set of labeled data is carefully
chosen based on the input and then included in the
prompt for few-shot learning. Our method also
bears a resemblance to non-parametric instance-
based learning (Gu et al., 2018). However, a crit-
ical difference is that we focus on the supervised
learning setting, where the model parameters are
ﬁne-tuned to learn from given examples to achieve
much higher performance than few-shot learning
2During training, we exclude the training instance itself
from the retrieval results to avoid data leakage.
arXiv:2203.08773v1  [cs.CL]  16 Mar 2022



Source: data\tc16_2312.10997v5\referenced_papers\[9]_2311.03758.pdf (Page 6):

Large Language Model based Long-tail Query Rewriting in Taobao Search WWW ’24 Companion, May 13–17, 2024, Singapore, Singapore.
Table 3: Comparison of Qwen with/without auxiliary tasks.
MI means multi-instruction
Method rele (%) incr (%) hitrate (%)
Qwen w/o MI 61.4 109.6 14.58
Qwen 62.6 133.2 15.63
Table 4: Comparison of BEQUE with different number of
contrast candidates.
Number rele (%) incr (%) hitrate (%)
2 53.3 215.6 17.66
3 56.6 205.4 17.36
4 57.7 198.7 17.27
5 58.8 190.3 17.21
4.4.3 Results of Different Contrast Number. In Table 4, we present
the impact of varying contrastive numbers of candidates on model
performance during the objective alignment stage. It can be ob-
served that, as the number of candidates increases, the relevance
shows a consistent improvement, while the increment and hitrate
decrease. This outcome can be attributed to our modified PRO,
where all candidates are treated as gold standard and SFT Loss is
calculated for each of them. Consequently, a larger candidate pool
implies an increase in the SFT loss weights and a decrease in the
partial order learning weights, improving relevance of generated
rewrites. Furthermore, there exists a trade-off between relevance
and increment, where an increase in one metric necessitates sac-
rificing the other, leading to a negative impact on the increment
metric. In addition, it is important to mention that hitrate can be
regarded as the increment with weak relevance constraint, which
explains its decrease in this context. Considering the balance of the
three metrics, we select the model with contrast number of 4 to be
best checkpoint for overall comparison.
4.4.4 Main Results. We compare BEQUE with multiple baselines,
including CLE-QR, query2doc (Q2D), BART, Qwen, and RL-based
LLM. CLE-QR [18] is the previous-generation query rewriter of
Taobao search that generates semantic representations and re-
trieve related rewrites for each query based on contrastive learning.
BART [17] is a powerful pre-trained generation model based on the
encoder-decoder structure. We fine-tune it with query pairs from
online logs to enhance its ability to rewrite e-commerce queries.
Qwen [32] is a large-scale language model based on the decoder-
only structure that contains 7 Billion parameters. Similarly, we
fine-tune it with query pairs from online logs to enhance its ability
to rewrite e-commerce queries. Furthermore, following the settings
of [1], we introduce an RL-based LLM and utilize relevance, incre-
ment, and hitrate as rewards to encourage the RL model to align
with the Taobao offline metrics, respectively. From analyzing the
data presented in Table 5, the following conclusions can be drawn:
Generative models outperform discriminative models when
rewriting “Torso” and “Tail” queries. For instance, when consid-
ering CLE-QR and BART, both models exhibit similar performance
on “Top Queries” across three metrics. However, BART significantly
outperforms CLE-QR in terms of hitrate and increment on “Torso
Queries” and “Tail Queries” while maintaining relevance. This dis-
crepancy arises because discriminative models like CLE-QR rely on
existing queries in the search system as rewrite candidates, which
are often biased towards top queries. As a result, torso and tail
queries, which lack semantically similar top rewrites, do not re-
ceive related search candidates from CLE-QR. In contrast, BART’s
rewriting process is not restricted by the semantic scope of online
queries, enabling it to generate rewrites that are not present in
the search system’s history. This allows BART to overcome the
limitations of discriminative models and optimize torso and tail
query rewriting problem.
LLMs exhibit superior long-tail semantic understanding
capabilities compared to small models. Qwen and BART serve
as examples, where Qwen, with its extensive parameter size, demon-
strates stronger semantic expansion than BART in terms of hitrate
and increment of “All Queries”. Analyzing individual query slices,
Qwen’s improvement in hitrate and increment primarily occurs
in the “Tail Queries”, further validating the suitability of LLMs for
long-tail query rewriting tasks.
Retrieval augmentation methods demonstrate limited se-
mantic expansion capabilities. Comparing Q2D (ChatGPT) and
BEQUE, Q2D (ChatGPT) maintains good retrieval relevance across
all query slices but lacks sufficient semantic expansion capabilities,
resulting in subpar increment and hitrate performance. Conversely,
our BEQUE, which is specifically optimized for semantic expansion
in rewriting, significantly enhances these two metrics.
The reinforcement learning (RL) may introduce bias and
impact the effectiveness of the rewriting LLMs. Examining RL
and BEQUE, RL process introduces a reward model to guide the
base model’s training. However, calculating the reward requires
offline search system simulation, and the reward model may not
accurately capture the search system’s features, leading to reduced
performance of RL models. In contrast, our BEQUE employs con-
trastive learning to explicitly learn the partial order of candidates,
circumventing potential bias caused by the reward model. Ulti-
mately, while minimizing the adverse impact on retrieval relevance,
BEQUE substantially improves the model’s increment and hitrate.
Different offline metrics work differently as rewards. For
instance, when the BEQUE framework prioritizes relevance as its
training objective, it demonstrates a more cautious approach to
bridging the semantic gap. The improvements in both increment
and hitrate tend to be challenging to achieve in this context. How-
ever, when the primary objective shifts to maximizing increment,
the model demonstrates a significant capacity to enhance both the
increment and hitrate of retrieval, effectively addressing the issue
of “few-recall”. In such cases, a marginal decrease in relevance be-
comes an acceptable trade-off. When hitrate becomes the target,
the model also can effectively enhance both the increment and
hitrate. Nevertheless, owing to the intricacies of the hitrate compu-
tation process, the model encounters difficulties in capturing the
partial order among candidates. Consequently, the model’s ability
to expand semantic is diminished in comparison to the BEQUE that
focuses on increment.



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 2):

Retrieve Anything To Augment Large Language Models Conference’17, July 2017, Washington, DC, USA
the model’s information seeking capability in the conversational
context. 3) Tool Learning. The ToolLLM dataset [ 62] is used to
learn the selection of appropriate tools in the tool-using context.
4) Instruction Tuning: To retrieve useful demonstration examples
for in-context learning, we re-purpose FLAN [86] and UPRISE [18],
which are originally designed for instruction tuning. 5) Genera-
tion. The model is trained to extract valuable historical information
(i.e. memory) based on a long conversation dataset: Multi-Session
Chat [93], as well as long-range language modeling datasets: in-
cluding Books3 [25], ArXiv [25], CodeParrot [79]. These datasets
can be grouped into two types based on the availability of labels.
•Labeled data. The datasets on the first three types of tasks are
composed of pairwise texts, where hard-coded labels are presented.
For question answering datasets (MSMARCO, NQ), each data in-
stance consists of a query and the source passage of answer, denoted
as <query, passage>. For conversational search dataset (QReCC),
each data instance is made up of a conversational query and the
source passage of answer, denoted as <conversation, passage>. For
tool learning dataset (ToolLLM), each data instance includes an
instruction and the description of the needed tool, denoted as <in-
struction, tool desc>.
•Non-labeled data. In contrast, the last two types of datasets
do not have explicit labels. For instruction tuning datasets (FLAN,
UPRISE), each instance consists of human’s instruction and the ex-
pected output: <instruction, output>. For generation datasets, each
instance is a long text sequence partitioned into chunks: [chunk_0,
..., chunk_L]. Books3, ArXiv, and CodeParrot are made up of plain
texts, which are chunked into spans of equal length (128 tokens per
chunk). Multi-Session Chat is composed of conversations, where
each chunk corresponds to a pair of consecutive utterances.
2.2 Training Methodology
2.2.1 Formulation of Training Reward. In our work, we explore
two types of supervision signals for training the LLM-Embedder.
Firstly, we can directly utilize the hard labels provided by the labeled
datasets. Secondly, we aim to optimize the LLM’s final performance
with retrieval augmentation. To achieve this goal, we leverage the
reward produced by LLM for both labeled and unlabeled datasets.
Particularly, given the expected output of the LLM, denoted as𝑂,
and a retrieval candidate, denoted as𝐶, the reward for the candidate,
represented as 𝑟𝐶|𝑂, is derived by the following equation:
𝑟𝐶|𝑂 =
Ö|𝑂|
𝑖=1 LLM(𝑜𝑖|𝐶,𝑂:𝑖−1). (1)
Here, 𝑜𝑖 represents the 𝑖-th token of the expected output, and
LLM(𝑥|𝑦)stands for the LLM’s generation likelihood of producing
𝑥 given the context 𝑦. In other words, a higher reward is assigned
to a retrieval candidate if it results in a higher generation likelihood
for the expected output.
The LLM based reward is applied in the following ways for
each of the tasks in consideration. 1) For Question Answering:
the reward is computed as the generation likelihood of answers
given one single candidate passage. 2) For Instruction Tuning: The
reward is computed as the generation likelihood of the instructed
output given one candidate example. 3) For Generation: the reward
is computed as the generation likelihood of a new content given
one candidate historical chunk. Note that the LLM reward is not
applied to conversational search and tool learning datasets, as there
is no clear expectation of the LLM’s output in these cases.
Given the two sources of supervision signals of LLM-Embedder,
i.e. the native hard labels and the soft reward derived from LLM,
the training is conducted with a composite recipe. The contrastive
learning is applied to capture the semantic relationship reflected
by the hard labels; meanwhile, the knowledge distillation is used
to learn from the soft rewards derived from LLM.
2.2.2 Contrastive Learning. For each pair of hard-labeled texts: 𝑞
and 𝑝 (e.g., query and passage), the loss function of contrastive
learning is formulated in the following way:
min .
∑︁
(𝑞,𝑝)
−log exp(⟨𝒆𝑞,𝒆𝑝⟩/𝜏)Í
𝑝′∈Pexp(⟨𝒆𝑞,𝒆𝑝′⟩/𝜏), (2)
where 𝒆∗stands for the embedding, ⟨·⟩indicates the inner product
operator, Pare the union of positive and negative samples,𝜏 refers
to the temperature. To improve the discriminative power of embed-
dings across diverse application scenarios, we employ a couple of
key designs in our contrastive learning framework.
The first featured design is theInstruction-based Fine-Tuning.
In this approach, each task is assigned with a unique task instruction
denoted as 𝐼𝑡. While generating the query-side embedding, the task
instruction and query content are concatenated and jointly encoded,
resulting in the update of query embedding: 𝒆𝑞 ←encode([𝐼𝑡,𝑞]).
This task-specific instructions plays a pivotal role in initializing the
embedding model with distinct activations, thereby facilitating the
discrimination between different tasks.
The second notable feature is theHomogeneous In-Batch Neg-
ative Sampling. It calls for a considerable amount of negative sam-
ples to guarantee the embedding’s discriminativeness [30, 63, 82].
In our work, this is realized by the joint usage of in-batch negatives
and hard negatives. We also apply cross-device sharing [ 63, 91],
which further expands the scale of negative samples. Consequently,
our method results in 𝐵 ×𝐾 ×𝑁 −1 negative samples in total,
where 𝐵is the batch size, 𝐾 is the number of GPU devices, 𝑁 is the
total number of positive and hard negative samples. However, the
vanilla practice of in-batch negative sampling presents one draw-
back in our multi-task settings. Particularly, the embeddings shared
between different datasets (namely heterogenous negative samples)
are mostly irrelevant, which are less effective for discriminating the
semantic relationships within a specific task scenario. To address
this limitation, we introduce a regularization strategy for the organi-
zation of training data, where the data instances from the same task
are grouped into consecutive mini-batches. The strategy makes the
majority of in-batch negative samples to originate from the same
dataset (i.e. homogeneous negative samples), thus enhancing the
discriminative power of embeddings for each specific task.
2.2.3 Knowledge Distillation. In our training framework, knowl-
edge distillation plays a crucial role in learning from the LLM’s
reward. we employ the KL-divergence to minimize the gap between
the distributions of candidates computed using LLM’s rewards and
those predicted by the embedding model. In particular, for each
query 𝑞and its candidate list P: [𝑝1, ..., 𝑝𝑁], we derive the LLM’s
rewards towards the candidates, denoted as 𝑅: [𝑟1, ..., 𝑟𝑁], using



### Claim 52/179

#### Claim Text
Each training data point consists of one positive sample and five negative samples, and the encoder undergoes training using contrastive loss throughout this process [102] .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 3):

Language Models as Semantic Indexers
… Codebook𝑬… Codebook𝑬
…
Semantic IDsWordtoken
He onlya rival
Deep TransEncoderDeep TransDecoder
Semantic Indexer
<s>
ShallowTransformer
Masked token 
…He only
is beenrival
ReconstructorSemantic Encoder
is been
Semantic ID representation
Document
 hints
Figure 1.The LMI NDEXER self-supervised ID learning framework overview. The proposed semantic indexer includes a semantic ID
encoder and several codebooks. During self-supervised learning, there is a reconstructor to reconstruct the input document from semantic
ID representations.
query channel input embeddings, and dh (token embeddings
correspond to dh) are fed as key and value channel input
embeddings in the multi-head self-attention. We adopt a
shallow reconstructor which has limited reconstruction capa-
bility based only on the hints in order to force the semantic
indexer to provide high-quality representations. The recon-
struction is conducted as follows:
zw “Reconϕpcd, dhq“
ÿ
t
Transpq “ct
d, k“dh, v“dhq
Preconpw|cd, dhq“ softmaxpW zwq
(5)
where W is the token embedding matrix. However, directly
adopting the reconstruction objective with cd as input to the
reconstructor will not optimize the semantic encoder. Since
the codebook look-up in Eq.(2) is a hard/discrete operation,
the reconstruction objective backpropagation gradients will
flow to the embeddings in the codebook rather than to the
parameters in the semantic encoder. To this end, we propose
to approximate the argmax operation similar to (Jang et al.,
2016) as follows:
ˆct
d “
$
&
%
arg maxet
jPEt ht
d ¨et
j forward pass.
ř
et
jPEt
exppht
d¨et
jqř
et
jPEt exppht
d¨et
jq et
j backward pass. (6)
In the forward pass, we still adopt the argmaxp¨qhard op-
eration; while in the backward pass, the selected semantic
embedding becomes a weighted average of the codebook
embeddings, to enable gradients to flow to ht
d and finally
to the parameters in the semantic encoder. In our imple-
mentation, we achieve this by adopting the stop gradient
operator (Van Den Oord et al., 2017). The reconstruction is
then conducted by
zw “Reconϕpˆct
d, dhq“
ÿ
t
Transpq “ˆct
d, k“dh, v“dhq
(7)
3.2. Training Self-Supervised Semantic Indexer
Progressive Training. To optimize the semantic indexer
and obtain semantic IDs in an auto-regressive way, we adopt
the progressive training scheme similar to (Sun et al., 2023).
The entire learning process consists ofT learning steps, each
corresponding to a specific semantic IDct
d being learned and
optimized at position t within the range of [T]. Additionally,
at each step t, both the ID ct
d and the model parameters
associated with generating ct
d are updated, while previously
generated IDs căt
d remain unchanged. The reconstruction
objective in t-step is shown as:
Lt
recon “´
ÿ
d
ÿ
wPdzdt
h
logPreconpw|cďt
d , dt
hq. (8)
Here dt
h is the hints provided for learning ID on position
t. We will gradually reduce the amounts of hints dt
h as t
increases to inject new knowledge into the new IDs, and
finally contribute to a hierarchical, coarse-to-fine-grained
semantic ID learning.
Contrastive Loss. The reconstruction objective in Eq.(8)
can force the semantic IDs to capture document-level se-
mantics. However, only optimizing the objective can lead to
the case where similar documents sharing căt
d also have the
same ct
d. To alleviate this issue, we propose a contrastive
objective to promote distinction between documents that
previously shared the same prefix, enabling the model to
discern finer-grained hierarchical relationships between doc-
uments:
Lt
contrastive “´
ÿ
d
log exppht
d ¨ht
dq
exppht
d ¨ht
dq` ř
căt
d1 “căt
d
exppht
d ¨ht
d1q.
(9)
The contrastive objective can help push ht
d of documents
sharing the same căt
d away in the t-th latent space and force
them to obtain diverse ct
d, finally contributing to higher
codebook utilization.
Commitment Loss. In addition, when learning the docu-
ment semantic IDs for position t, it is important that the
semantic indexer should remember the IDs that are already
learned before position t. To this end, we add a commitment
loss as:
Lt
commitment “´
ÿ
d
ÿ
jăt
log Pspcj
d|d, căj
d q. (10)
We optimize our model at step t based on a combination of
4



Source: data\tc16_2312.10997v5\referenced_papers\[20]_2303.08518.pdf (Page 3):

🧊
Task 
Input
🔥 Prompt
Encoder
🔥 Input 
Encoder
Tune on many tasks
Prompt
Retriever
Tune 0.37
Contrastive 
Loss 0.19
0.44
0.09Task 
Score 0.13
0.78
Read. Compre.
Read the passage…
Read. Compre.
Read the passage…
NLI
Does love entail…
Close. QA
Please answer…
Read. Compre.
Read the passage…
NLI
Does love entail…
NLI
Premise: today…
Prompt 
Pool
Inference on unseen task types
with a different language model
Task 
Input
Prompt 
Retriever
Task
Output
NLI
Does love entail…
Read. Compre.
Read the passage…
Sentiment
How do you feel…
Sentiment
How do you feel…
Sentiment
Positive
GPT-Neo-2.7B
 BLOOM-7.1B GPT3-175BOPT-66B
Figure 3: Training and inference pipeline. In the training stage, a frozen LLM is used to supervise the tuning of a
prompt retriever, where both the LLM and the retriever take the prompt-input pairs as input, and we use the task
scores given by the LLM to supervise the contrastive learning of the retriever. In the inference stage, for each task
input, the tuned prompt retriever retrieve positive prompt(s) to guide the inference model to predict a task output.
Overall, we follow a cross-task and cross-model paradigm where the task types and LLMs for training could be
different from those for inference.
pool and identify the prompt that yields the best
score as the positive prompt. Conversely, prompts
that lead to the worst scores are labeled as nega-
tive prompts. However, scoring all the prompts can
be computationally expensive (Rubin et al., 2022),
even with a relatively small LLM.
To address this, we only score a subset of L
randomly sampled demonstrations; each demon-
stration is constrained to have the same task as
the training example (xi, yi). This is inspired by
in-context learning where the testing sample and
training demonstrations share the same task, result-
ing in improved task scores. By scoring a subset of
demonstrations, we significantly reduce the com-
putational cost while increasing the likelihood of
identifying positive prompts within the sampled
subset.
Furthermore, in the case of a difficult question,
all L prompt-input concatenation may result in a
score of 0. To address this, we repeat the sampling
process to score another subset of L prompts with
the same task as (xi, yi), until we find at least one
prompt with a score greater than 0.
For all the scored prompts for a training example,
we label the prompt with the highest score as pos-
itive. For negative samples, we randomly sample
B training demonstrations from the prompt pool,
each with a different task from that of (xi, yi). In
addition, we label B demonstrations corresponding
to the lowest B scores in the sampled prompts as
hard negatives, which are of the same task with
(xi, yi) but are less effective.
3.3 Retriever Tuning
After labeling prompts for each training example,
we split the collected data into two sets: 90% for
training and 10% for validation. The prompt re-
triever is a bi-encoder model (Karpukhin et al.,
2020) where the input encoderEX(·) takes the task
input xi as input, and the prompt encoder EP (·)
takes prompt pj as input.
To train the prompt retriever, InfoNCE (van den
Oord et al., 2018) loss is used to maximize the
similarity score between the encoded prompt and
input for positive prompt-input pairs, and minimize
it for (hard) negative prompt-input pairs. For a
single training example (xi, yi), the loss function
for its positive and negative prompts is:
L(xi, p+
i , p−
i,1, . . . p−
i,2B) (5)
= −log esim(xi,p+
i )
esim(xi,p+
i ) + P2B
j=1 esim(xi,p−
i,j) ,
where p+
i is the positive prompt, p−
i,j is one of
the (hard) negative prompts, and sim(xi, p) =
EX(xi)⊤EP (p) calculates the similarity score be-



Source: data\tc16_2312.10997v5\referenced_papers\[102]_2004.04906.pdf (Page 2):

At run-time, DPR applies a different encoderEQ(·)
that maps the input question to a d-dimensional
vector, and retrieves kpassages of which vectors
are the closest to the question vector. We deﬁne
the similarity between the question and the passage
using the dot product of their vectors:
sim(q,p) =EQ(q)⊺EP (p). (1)
Although more expressive model forms for measur-
ing the similarity between a question and a passage
do exist, such as networks consisting of multiple
layers of cross attentions, the similarity function
needs to be decomposable so that the represen-
tations of the collection of passages can be pre-
computed. Most decomposable similarity functions
are some transformations of Euclidean distance
(L2). For instance, cosine is equivalent to inner
product for unit vectors and the Mahalanobis dis-
tance is equivalent to L2 distance in a transformed
space. Inner product search has been widely used
and studied, as well as its connection to cosine
similarity and L2 distance (Mussmann and Ermon,
2016; Ram and Gray, 2012). As our ablation study
ﬁnds other similarity functions perform compara-
bly (Section 5.2; Appendix B), we thus choose
the simpler inner product function and improve the
dense passage retriever by learning better encoders.
Encoders Although in principle the question and
passage encoders can be implemented by any neu-
ral networks, in this work we use two independent
BERT (Devlin et al., 2019) networks (base, un-
cased) and take the representation at the [CLS]
token as the output, so d= 768.
Inference During inference time, we apply the
passage encoder EP to all the passages and index
them using FAISS (Johnson et al., 2017) ofﬂine.
FAISS is an extremely efﬁcient, open-source li-
brary for similarity search and clustering of dense
vectors, which can easily be applied to billions of
vectors. Given a question qat run-time, we derive
its embedding vq = EQ(q) and retrieve the top k
passages with embeddings closest to vq.
3.2 Training
Training the encoders so that the dot-product sim-
ilarity (Eq. (1)) becomes a good ranking function
for retrieval is essentially a metric learning prob-
lem (Kulis, 2013). The goal is to create a vector
space such that relevant pairs of questions and pas-
sages will have smaller distance (i.e., higher simi-
larity) than the irrelevant ones, by learning a better
embedding function.
Let D = {⟨qi,p+
i ,p−
i,1,··· ,p−
i,n⟩}m
i=1 be the
training data that consists of m instances. Each
instance contains one question qi and one relevant
(positive) passage p+
i , along with nirrelevant (neg-
ative) passages p−
i,j. We optimize the loss function
as the negative log likelihood of the positive pas-
sage:
L(qi,p+
i ,p−
i,1,··· ,p−
i,n) (2)
= −log esim(qi,p+
i )
esim(qi,p+
i ) + ∑n
j=1 esim(qi,p−
i,j) .
Positive and negative passages For retrieval
problems, it is often the case that positive examples
are available explicitly, while negative examples
need to be selected from an extremely large pool.
For instance, passages relevant to a question may
be given in a QA dataset, or can be found using the
answer. All other passages in the collection, while
not speciﬁed explicitly, can be viewed as irrelevant
by default. In practice, how to select negative ex-
amples is often overlooked but could be decisive
for learning a high-quality encoder. We consider
three different types of negatives: (1) Random: any
random passage from the corpus; (2) BM25: top
passages returned by BM25 which don’t contain
the answer but match most question tokens; (3)
Gold: positive passages paired with other questions
which appear in the training set. We will discuss the
impact of different types of negative passages and
training schemes in Section 5.2. Our best model
uses gold passages from the same mini-batch and
one BM25 negative passage. In particular, re-using
gold passages from the same batch as negatives
can make the computation efﬁcient while achiev-
ing great performance. We discuss this approach
below.
In-batch negatives Assume that we have B
questions in a mini-batch and each one is asso-
ciated with a relevant passage. Let Q and P be the
(B×d) matrix of question and passage embeddings
in a batch of size B. S = QPT is a (B×B) ma-
trix of similarity scores, where each row of which
corresponds to a question, paired with Bpassages.
In this way, we reuse computation and effectively
train on B2 (qi, pj) question/passage pairs in each
batch. Any (qi, pj) pair is a positive example when
i= j, and negative otherwise. This creates Btrain-
ing instances in each batch, where there are B−1



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 2):

Retrieve Anything To Augment Large Language Models Conference’17, July 2017, Washington, DC, USA
the model’s information seeking capability in the conversational
context. 3) Tool Learning. The ToolLLM dataset [ 62] is used to
learn the selection of appropriate tools in the tool-using context.
4) Instruction Tuning: To retrieve useful demonstration examples
for in-context learning, we re-purpose FLAN [86] and UPRISE [18],
which are originally designed for instruction tuning. 5) Genera-
tion. The model is trained to extract valuable historical information
(i.e. memory) based on a long conversation dataset: Multi-Session
Chat [93], as well as long-range language modeling datasets: in-
cluding Books3 [25], ArXiv [25], CodeParrot [79]. These datasets
can be grouped into two types based on the availability of labels.
•Labeled data. The datasets on the first three types of tasks are
composed of pairwise texts, where hard-coded labels are presented.
For question answering datasets (MSMARCO, NQ), each data in-
stance consists of a query and the source passage of answer, denoted
as <query, passage>. For conversational search dataset (QReCC),
each data instance is made up of a conversational query and the
source passage of answer, denoted as <conversation, passage>. For
tool learning dataset (ToolLLM), each data instance includes an
instruction and the description of the needed tool, denoted as <in-
struction, tool desc>.
•Non-labeled data. In contrast, the last two types of datasets
do not have explicit labels. For instruction tuning datasets (FLAN,
UPRISE), each instance consists of human’s instruction and the ex-
pected output: <instruction, output>. For generation datasets, each
instance is a long text sequence partitioned into chunks: [chunk_0,
..., chunk_L]. Books3, ArXiv, and CodeParrot are made up of plain
texts, which are chunked into spans of equal length (128 tokens per
chunk). Multi-Session Chat is composed of conversations, where
each chunk corresponds to a pair of consecutive utterances.
2.2 Training Methodology
2.2.1 Formulation of Training Reward. In our work, we explore
two types of supervision signals for training the LLM-Embedder.
Firstly, we can directly utilize the hard labels provided by the labeled
datasets. Secondly, we aim to optimize the LLM’s final performance
with retrieval augmentation. To achieve this goal, we leverage the
reward produced by LLM for both labeled and unlabeled datasets.
Particularly, given the expected output of the LLM, denoted as𝑂,
and a retrieval candidate, denoted as𝐶, the reward for the candidate,
represented as 𝑟𝐶|𝑂, is derived by the following equation:
𝑟𝐶|𝑂 =
Ö|𝑂|
𝑖=1 LLM(𝑜𝑖|𝐶,𝑂:𝑖−1). (1)
Here, 𝑜𝑖 represents the 𝑖-th token of the expected output, and
LLM(𝑥|𝑦)stands for the LLM’s generation likelihood of producing
𝑥 given the context 𝑦. In other words, a higher reward is assigned
to a retrieval candidate if it results in a higher generation likelihood
for the expected output.
The LLM based reward is applied in the following ways for
each of the tasks in consideration. 1) For Question Answering:
the reward is computed as the generation likelihood of answers
given one single candidate passage. 2) For Instruction Tuning: The
reward is computed as the generation likelihood of the instructed
output given one candidate example. 3) For Generation: the reward
is computed as the generation likelihood of a new content given
one candidate historical chunk. Note that the LLM reward is not
applied to conversational search and tool learning datasets, as there
is no clear expectation of the LLM’s output in these cases.
Given the two sources of supervision signals of LLM-Embedder,
i.e. the native hard labels and the soft reward derived from LLM,
the training is conducted with a composite recipe. The contrastive
learning is applied to capture the semantic relationship reflected
by the hard labels; meanwhile, the knowledge distillation is used
to learn from the soft rewards derived from LLM.
2.2.2 Contrastive Learning. For each pair of hard-labeled texts: 𝑞
and 𝑝 (e.g., query and passage), the loss function of contrastive
learning is formulated in the following way:
min .
∑︁
(𝑞,𝑝)
−log exp(⟨𝒆𝑞,𝒆𝑝⟩/𝜏)Í
𝑝′∈Pexp(⟨𝒆𝑞,𝒆𝑝′⟩/𝜏), (2)
where 𝒆∗stands for the embedding, ⟨·⟩indicates the inner product
operator, Pare the union of positive and negative samples,𝜏 refers
to the temperature. To improve the discriminative power of embed-
dings across diverse application scenarios, we employ a couple of
key designs in our contrastive learning framework.
The first featured design is theInstruction-based Fine-Tuning.
In this approach, each task is assigned with a unique task instruction
denoted as 𝐼𝑡. While generating the query-side embedding, the task
instruction and query content are concatenated and jointly encoded,
resulting in the update of query embedding: 𝒆𝑞 ←encode([𝐼𝑡,𝑞]).
This task-specific instructions plays a pivotal role in initializing the
embedding model with distinct activations, thereby facilitating the
discrimination between different tasks.
The second notable feature is theHomogeneous In-Batch Neg-
ative Sampling. It calls for a considerable amount of negative sam-
ples to guarantee the embedding’s discriminativeness [30, 63, 82].
In our work, this is realized by the joint usage of in-batch negatives
and hard negatives. We also apply cross-device sharing [ 63, 91],
which further expands the scale of negative samples. Consequently,
our method results in 𝐵 ×𝐾 ×𝑁 −1 negative samples in total,
where 𝐵is the batch size, 𝐾 is the number of GPU devices, 𝑁 is the
total number of positive and hard negative samples. However, the
vanilla practice of in-batch negative sampling presents one draw-
back in our multi-task settings. Particularly, the embeddings shared
between different datasets (namely heterogenous negative samples)
are mostly irrelevant, which are less effective for discriminating the
semantic relationships within a specific task scenario. To address
this limitation, we introduce a regularization strategy for the organi-
zation of training data, where the data instances from the same task
are grouped into consecutive mini-batches. The strategy makes the
majority of in-batch negative samples to originate from the same
dataset (i.e. homogeneous negative samples), thus enhancing the
discriminative power of embeddings for each specific task.
2.2.3 Knowledge Distillation. In our training framework, knowl-
edge distillation plays a crucial role in learning from the LLM’s
reward. we employ the KL-divergence to minimize the gap between
the distributions of candidates computed using LLM’s rewards and
those predicted by the embedding model. In particular, for each
query 𝑞and its candidate list P: [𝑝1, ..., 𝑝𝑁], we derive the LLM’s
rewards towards the candidates, denoted as 𝑅: [𝑟1, ..., 𝑟𝑁], using



Source: data\tc16_2312.10997v5\referenced_papers\[71]_2310.04408.pdf (Page 2):

Extractive Compressor Given n sentences [s1, s2...sn] in the input document set ([d1, d2, ...dN ]),
we train a dual encoder model encθ which embeds sentence si and the input sequence x into fixed-
dimensional embeddings respectively. Their inner product represents how helpful it would be for the
LM M to prepend si to the input x to generate y. The final summary s from the compressor will be a
concatenation of top N sentences ranked by their inner product with the input. As this approach is
extractive, we assume the faithfulness criteria is mostly satisfied.3
Abstractive Compressor We train an encoder-decoder model encdecθ to serve as an abstractive
compressor, which takes the input sequence x and a concatenation of retrieved document set D
[d1; d2; ...dN ]) and output a summary s. Although we do not have human annotations to train this
model, prior work (Goyal et al., 2022; Chen et al., 2023; Potluri et al., 2023) suggests that the extreme-
scale LMs can generate good query-focused summaries when prompted carefully. Yet, using an
extreme-scale model as the compressor is not desirable as we want the compressor to be substantially
smaller than the LMs. Thus, we perform distillation (Hinton et al., 2015) of extreme-scale LMs to
build a lightweight abstractive compressor encdecθ. We do not train specifically for faithfulness, but
later manually evaluate the faithfulness in Section 6.
3 L EARNING THE COMPRESSORS
Our compressor resembles text summarization models in the output should be faithful to the original
input, yet the main goal is different. Instead of capturing salient information for humans readers,
compressors aim to produce a concise text that are useful for a LM on an end task. In this section,
we describe how to train the extractive compressor ( §3.1) and the abstractive compressor ( §3.2)
leveraging end task signals. Further training details can be found in the Appendix A.2.
3.1 E XTRACTIVE COMPRESSION
As we formulate extractive compression as a ranking problem, training extractive compressor re-
sembles training a reranker for the retrieved documents4 with two differences. First, our compressor
considers a different granularity of input (sentence) compared to the initial retrieval unit (paragraph).
Second, the sentence is evaluated based on whether it is useful as input for the LM M on the
downstream task (Shi et al., 2023b; Ram et al., 2023).
Input: Base LM M, Compressor encθ, Training data
{xi, Si, yi}T
1 where xi is input, Si = {sj}n
1 is a set of
candidate sentences from the retrieved documents for xi,
yi is the target answer, and score threshold ϵ.
Output: An updated extractive compressor encoder
encθ
1: T ← ∅
2: for i ∈ {1, . . . , T} do
3: pi ← argMaxsj∈{Si}Score(M, yi, [sj; xi])
4: for j ∈ {1, . . . , n} do
5: L ← ∅
6: if Score(M, yi, [sj; xi]) + ϵ <
Score(M, yi, [pi; xi]) then
7: L ← L ∪si
8: if |L| > 0 then
9: Ni ← argTop5sj∈L(⟨encθ(sj), encθ(xi)⟩)
10: T ← T ∪ {(xi, pi, Ni)}
11: encθ = Finetune(encθ, T )
Figure 2: Learning an extractive compressor for lan-
guage modeling task.
Model We train a dual-encoder model
encθ which encodes the input context x
and the candidate sentence si separately.
We obtain an embedding of x and si by
taking the representation of the [CLS]
token respectively, and compute their sim-
ilarity by calculating the inner product of
the two. We initialize our model with
the contriever checkpoint (Izacard et al.,
2021). This model consists of 110M pa-
rameters, satisfying the efficiency desider-
atum of compressor.
Training Figure 2 presents pseudocode
for training an extractive compressor with
contrastive loss for the language modeling
task. For each input query xi, we iden-
tify positive and negative sentences from
retrieved documents.
For each pair of input sequence xi
and candidate sentences sj, we measure
3Recent work (Zhang et al., 2022) shows that extractive approach does not always preserve faithfulness, but
such cases are still rare compared to abstractive approaches which can easily hallucinate.
4Ram et al. (2023) proposes a document reranker based on a cross-encoder model, which is a similar set-up
to our sentence selector, but less compute efficient.
3



### Claim 53/179

#### Claim Text
Ma et al. [103] propose the “Filter-Reranker” paradigm, which combines the strengths of LLMs and SLMs.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[103]_2303.08559.pdf (Page 6):

wo. LLM reranking          w. LLM reranking
0
25
50
75
100
0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
Confidence Score
Micro−F1
FewNERD (NER)
0
25
50
75
100
0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
Confidence Score
Micro−F1
TACREV (RE)
0
25
50
75
100
0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
Confidence Score
Micro−F1
ACE05 (ED)
Figure 5: Relationship between confidence scores and
performance with/without LLM reranking. We adopt
RoBERTa-large as filter and InstructGPT as reranker.
We conduct experiments to confirm our hypoth-
esis that LLMs excel on hard samples. We group
samples by confidence scores and compare two
methods within each group: (a) SLM-based meth-
ods without LLM reranking, and (b) SLMs as the
filter and LLMs as the reranker. Method (b) dif-
fers from (a) by adding a single LLM to rerank the
top-N SLM predictions, using MCQ prompts.
The results in Figure 5 substantiate our assump-
tion. (1) LLM-based reranking (blue lines) en-
hances performance on hard samples (left areas in
the figure). We provide a detailed analysis of spe-
cific challenging instances where LLM rerankers
prove advantageous in Appendix F.1. These in-
stances demonstrate the efficacy of LLMs in har-
nessing external knowledge and complex reason-
ing to rectify erroneous predictions initially made
by SLMs (red lines). (2) Conversely, LLM-based
reranking impedes performance on easy samples
(right areas), resulting in a significant degradation,
particularly for very easy samples (rightmost areas).
In conclusion, LLMs exhibit greater proficiency in
handling hard samples compared to SLMs, yet they
underperform relative to SLMs on easy samples.
4.3 Why LLMs Fail on Easy Samples
We investigate why LLMs (relatively) fail on easy
samples in this section. As shown in Table 2, we
observe significant higher negative sample ratios
for easy samples across diverse IE tasks. In other
Table 2: Comparative ratios of negative to positive sam-
ples across various datasets and subsets. We set fixed
threshold τ here for simplicity.
FewNERD TACREV ACE05
Overall 5.88 3.03 38.2
Easy samples (τ >0.9) 9.44 3.21 44.0
Hard samples (τ <0.6) 1.28 2.68 1.36
words, most negative samples are easy samples for
SLMs. Here we refer negative samples to those
labeled as None. We speculate that the proficiency
of SLMs with negative samples stems from their
ability to adeptly discern apparent patterns during
the fine-tuning stages. Therefore, SLMs could pre-
dict negative samples with (relatively) high confi-
dence and accuracy. Due to LLMs’ predisposition
to false-positive predictions on negative samples,
however, the performance of LLMs on easy sam-
ples collapses. We attribute such false-positive pre-
dictions to (1) hallucination and (2) span boundary
mismatch. We detail such two kinds of mistakes
with cases in Appendix F.2.
5 Adaptive Filter-then-rerank Paradigm
Above findings can be summarized as: (1) SLMs
generally outperform LLMs, especially with more
training samples and fine-grained labels. (2) SLMs
are much more time- and cost-efficient. (3) LLMs
serve as powerful rerankers on hard samples that
challenge SLMs. Based on them, we propose a
simple, efficient, and effective adaptive reranker
that combines the strengths of SLMs and LLMs.
5.1 Method
Our adaptive filter-then-rerank approach, shown
in Figure 6, uses supervised SLMs as a filter to
make preliminary decisions. Samples with confi-
dence scores exceeding threshold are viewed as
easy samples otherwise hard ones. For easy sam-
ples, we retain SLM predictions as final results. For
hard samples, top-N predictions from SLMs are
reranked via LLMs using ICL. Here LLMs employ
MCQ prompts (Figure 4), containing demos and a
sample to be reranked. The LLMs then generate the
final answer and optionally provide an explanation.
5.2 Experimental Setup
We conduct experiments on FewNERD for NER
task, TACREV for RE task and ACE05 for ED
task. We employ top-performing SLM-based meth-
ods from Section 3 (FSLS or KnowPrompt) as the



Source: data\tc16_2312.10997v5\referenced_papers\[36]_2303.08559.pdf (Page 6):

wo. LLM reranking          w. LLM reranking
0
25
50
75
100
0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
Confidence Score
Micro−F1
FewNERD (NER)
0
25
50
75
100
0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
Confidence Score
Micro−F1
TACREV (RE)
0
25
50
75
100
0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
Confidence Score
Micro−F1
ACE05 (ED)
Figure 5: Relationship between confidence scores and
performance with/without LLM reranking. We adopt
RoBERTa-large as filter and InstructGPT as reranker.
We conduct experiments to confirm our hypoth-
esis that LLMs excel on hard samples. We group
samples by confidence scores and compare two
methods within each group: (a) SLM-based meth-
ods without LLM reranking, and (b) SLMs as the
filter and LLMs as the reranker. Method (b) dif-
fers from (a) by adding a single LLM to rerank the
top-N SLM predictions, using MCQ prompts.
The results in Figure 5 substantiate our assump-
tion. (1) LLM-based reranking (blue lines) en-
hances performance on hard samples (left areas in
the figure). We provide a detailed analysis of spe-
cific challenging instances where LLM rerankers
prove advantageous in Appendix F.1. These in-
stances demonstrate the efficacy of LLMs in har-
nessing external knowledge and complex reason-
ing to rectify erroneous predictions initially made
by SLMs (red lines). (2) Conversely, LLM-based
reranking impedes performance on easy samples
(right areas), resulting in a significant degradation,
particularly for very easy samples (rightmost areas).
In conclusion, LLMs exhibit greater proficiency in
handling hard samples compared to SLMs, yet they
underperform relative to SLMs on easy samples.
4.3 Why LLMs Fail on Easy Samples
We investigate why LLMs (relatively) fail on easy
samples in this section. As shown in Table 2, we
observe significant higher negative sample ratios
for easy samples across diverse IE tasks. In other
Table 2: Comparative ratios of negative to positive sam-
ples across various datasets and subsets. We set fixed
threshold τ here for simplicity.
FewNERD TACREV ACE05
Overall 5.88 3.03 38.2
Easy samples (τ >0.9) 9.44 3.21 44.0
Hard samples (τ <0.6) 1.28 2.68 1.36
words, most negative samples are easy samples for
SLMs. Here we refer negative samples to those
labeled as None. We speculate that the proficiency
of SLMs with negative samples stems from their
ability to adeptly discern apparent patterns during
the fine-tuning stages. Therefore, SLMs could pre-
dict negative samples with (relatively) high confi-
dence and accuracy. Due to LLMs’ predisposition
to false-positive predictions on negative samples,
however, the performance of LLMs on easy sam-
ples collapses. We attribute such false-positive pre-
dictions to (1) hallucination and (2) span boundary
mismatch. We detail such two kinds of mistakes
with cases in Appendix F.2.
5 Adaptive Filter-then-rerank Paradigm
Above findings can be summarized as: (1) SLMs
generally outperform LLMs, especially with more
training samples and fine-grained labels. (2) SLMs
are much more time- and cost-efficient. (3) LLMs
serve as powerful rerankers on hard samples that
challenge SLMs. Based on them, we propose a
simple, efficient, and effective adaptive reranker
that combines the strengths of SLMs and LLMs.
5.1 Method
Our adaptive filter-then-rerank approach, shown
in Figure 6, uses supervised SLMs as a filter to
make preliminary decisions. Samples with confi-
dence scores exceeding threshold are viewed as
easy samples otherwise hard ones. For easy sam-
ples, we retain SLM predictions as final results. For
hard samples, top-N predictions from SLMs are
reranked via LLMs using ICL. Here LLMs employ
MCQ prompts (Figure 4), containing demos and a
sample to be reranked. The LLMs then generate the
final answer and optionally provide an explanation.
5.2 Experimental Setup
We conduct experiments on FewNERD for NER
task, TACREV for RE task and ACE05 for ED
task. We employ top-performing SLM-based meth-
ods from Section 3 (FSLS or KnowPrompt) as the



Source: data\tc16_2312.10997v5\referenced_papers\[103]_2303.08559.pdf (Page 8):

Table 3: Overall results of LLM-based ICL methods, SLM-based supervised methods, and our proposed filter-then-
rerank (SLM+LLM) methods. The best results are in bold face and the second best are underlined. All results
except InstructGPT and GPT-4 are averaged over 5 runs, and sample standard deviations are in the round bracket.
FewNERD (NER) TACREV (RE) ACE (ED)
5-shot 10-shot 20-shot 20-shot 50-shot 100-shot 5-shot 10-shot 20-shot
LLM
CODEX 53.8(0.5) 54.0(1.4) 55.9(0.5) 59.1(1.4) 60.3(2.4) 62.4(2.6) 47.1(1.2) 47.7(2.8) 47.9(0.5)
InstructGPT 53.6(−) 54.6(−) 57.2(−) 60.1(−) 58.3(−) 62.7(−) 52.9(−) 52.1(−) 49.3(−)
GPT-4 - - 57.8(−) - - 59.3(−) - - 52.1(−)
SLM
Previous SoTA 59.4(1.5) 61.4(0.8) 61.9(1.2) 62.4(3.8) 68.5(1.6) 72.6(1.5) 55.1(4.6) 63.9(0.8) 65.8(2.0)
+ Ensemble (S) 59.6(1.7) 61.8(1.2) 62.6(1.0) 64.9(1.5) 71.9(2.2) 74.1(1.7) 56.9(4.7) 64.2(2.1) 66.5(1.7)
+ Rerank (S) 59.4(1.5) 61.0(1.7) 61.5(1.7) 64.2(2.3) 70.8(2.3) 74.3(2.2) 56.1(0.3) 64.0(1.0) 66.7(1.7)
SLM + LLM
Vicuna-13B
+ Rerank (L) 60.0(1.8) 61.9(2.1) 62.2(1.4) 65.2(1.4) 70.8(1.6) 73.8(1.7) 56.9(4.0) 63.5(2.7) 66.0(2.6)
+ Ensemble (S) + Rerank (L) 59.9(0.7) 62.1(0.7) 62.8(1.1) 66.5(0.5) 73.6(1.4) 75.0(1.5) 57.9(5.2) 64.4(1.2) 66.2(2.4)
InstructGPT
+ Rerank (L) 60.6(2.1) 62.7(0.8) 63.3(0.6) 66.8(2.6) 72.3(1.4) 75.4(1.5) 57.8(4.6) 65.3(1.7) 67.3(2.2)
+ Ensemble (S) + Rerank (L) 61.3(1.9) 63.2(0.9) 63.7(1.8) 68.9(1.3) 74.8(1.3) 76.8(1.2) 59.5(3.7) 65.3(1.9) 67.8(2.1)
GPT-4
+ Rerank (L) 60.8(2.3) 62.6 (2.7) 63.0(1.3) 65.9(2.7) 72.3(0.3) 74.5(1.5) 59.6(2.9) 64.9(2.5) 67.1(2.5)
+ Ensemble (S) + Rerank (L) 61.1(2.2) 62.8(0.9) 63.6(1.2) 68.6(1.3) 73.9(1.4) 75.9(2.4) 60.9(3.9) 65.6(1.5) 67.8(1.7)
Table 4: The F1-score differences before and after
reranking on the reranked samples, as well as their pro-
portion of the total samples.
GPT-4 InstructGPT
before after △ ratio before after △ ratio
FewNER 31.9 40 .7 8 .8 3 .2% 31.4 28 .3 −3.1 3.3%
TACREV 25.3 43 .0 17.7 9.1% 33.8 43 .4 9 .6 7 .1%
ACE05 31.1 57 .9 26.8 1.6% 35.6 55 .7 20 .1 0 .5%
We remove all examples, rendering the reranking
a zero-shot problem. (3) LF (label filtering): We
retain all labels as candidate choices for reranking,
instead of only the top- N labels from the SLMs.
(4) AD (adaptive): We feed all samples, not just
hard ones, to the LLMs.
We show their results in Table 5 and see that
(1) Demos with explanations consistently enhance
the reranking ability of LLMs across all datasets.
(2) Demos without explanations also contribute to
performance improvement. (3) Label filtering re-
sults in gains and notably reduces the demo length,
Direct ICL (InstructGPT) Filter−then−rerank Fine−tuning (RoBERTa−large)
0
10
20
30
40
FewNERD TACREV ACE05
Financial cost
dollar($)
0
50
100
150
FewNERD TACREV ACE05
Time cost
second(s)
Figure 7: The financial and time cost over 500 sentences.
InstructGPT as the reranker.
Table 5: Ablation study on three datasets. The filter is
ensembled SLMs and the reranker is GPT-4.
CoT Demo LF AD
FewNERD
(20-shot)
TACREV
(100-shot)
ACE05
(20-shot)
✓ ✓ ✓ ✓ 63.6(1.2) 75.9(2.4) 67.8(1.7)
✗ ✓ ✓ ✓ 63.2(1.2) 75.4(2.4) 67.2(1.7)
✗ ✗ ✓ ✓ 63.0(1.4) 74.9(2.2) 66.6(1.5)
✗ ✗ ✗ ✓ 62.4(2.1) 73.8(2.5) 66.5(1.3)
✗ ✗ ✗ ✗ 12.5(2.7) 59.9(6.0) 5.4(1.1)
Previous SoTA methods 62.6(1.0) 74.1(1.7) 66.5(1.7)
hence cutting inference costs. (4) The performance
collapses without a filter to identify sample diffi-
culty, reiterating the need for an integrated SLM-
LLM system to complement each other.
6 Conclusion
Through an extensive empirical study on nine
datasets spanning four IE tasks, we find that LLMs,
despite their superiority in extreme low-resource
scenarios, are not effective few-shot information
extractors in general. They struggle with IE-related
prompts, have limited demonstration capacity, and
incur high inference costs. However, LLMs signifi-
cantly improve the performance on hard samples
when combined with SLM. Building on these in-
sights, we propose an adaptive filter-then-rerank
paradigm to leverage the strengths of SLMs and
LLMs and mitigate their limitations. This approach
consistently achieves promising results, with an av-
erage 2.4% F1 gain across multiple few-shot IE
tasks, while minimizing latency and budget costs.



Source: data\tc16_2312.10997v5\referenced_papers\[36]_2303.08559.pdf (Page 8):

Table 3: Overall results of LLM-based ICL methods, SLM-based supervised methods, and our proposed filter-then-
rerank (SLM+LLM) methods. The best results are in bold face and the second best are underlined. All results
except InstructGPT and GPT-4 are averaged over 5 runs, and sample standard deviations are in the round bracket.
FewNERD (NER) TACREV (RE) ACE (ED)
5-shot 10-shot 20-shot 20-shot 50-shot 100-shot 5-shot 10-shot 20-shot
LLM
CODEX 53.8(0.5) 54.0(1.4) 55.9(0.5) 59.1(1.4) 60.3(2.4) 62.4(2.6) 47.1(1.2) 47.7(2.8) 47.9(0.5)
InstructGPT 53.6(−) 54.6(−) 57.2(−) 60.1(−) 58.3(−) 62.7(−) 52.9(−) 52.1(−) 49.3(−)
GPT-4 - - 57.8(−) - - 59.3(−) - - 52.1(−)
SLM
Previous SoTA 59.4(1.5) 61.4(0.8) 61.9(1.2) 62.4(3.8) 68.5(1.6) 72.6(1.5) 55.1(4.6) 63.9(0.8) 65.8(2.0)
+ Ensemble (S) 59.6(1.7) 61.8(1.2) 62.6(1.0) 64.9(1.5) 71.9(2.2) 74.1(1.7) 56.9(4.7) 64.2(2.1) 66.5(1.7)
+ Rerank (S) 59.4(1.5) 61.0(1.7) 61.5(1.7) 64.2(2.3) 70.8(2.3) 74.3(2.2) 56.1(0.3) 64.0(1.0) 66.7(1.7)
SLM + LLM
Vicuna-13B
+ Rerank (L) 60.0(1.8) 61.9(2.1) 62.2(1.4) 65.2(1.4) 70.8(1.6) 73.8(1.7) 56.9(4.0) 63.5(2.7) 66.0(2.6)
+ Ensemble (S) + Rerank (L) 59.9(0.7) 62.1(0.7) 62.8(1.1) 66.5(0.5) 73.6(1.4) 75.0(1.5) 57.9(5.2) 64.4(1.2) 66.2(2.4)
InstructGPT
+ Rerank (L) 60.6(2.1) 62.7(0.8) 63.3(0.6) 66.8(2.6) 72.3(1.4) 75.4(1.5) 57.8(4.6) 65.3(1.7) 67.3(2.2)
+ Ensemble (S) + Rerank (L) 61.3(1.9) 63.2(0.9) 63.7(1.8) 68.9(1.3) 74.8(1.3) 76.8(1.2) 59.5(3.7) 65.3(1.9) 67.8(2.1)
GPT-4
+ Rerank (L) 60.8(2.3) 62.6 (2.7) 63.0(1.3) 65.9(2.7) 72.3(0.3) 74.5(1.5) 59.6(2.9) 64.9(2.5) 67.1(2.5)
+ Ensemble (S) + Rerank (L) 61.1(2.2) 62.8(0.9) 63.6(1.2) 68.6(1.3) 73.9(1.4) 75.9(2.4) 60.9(3.9) 65.6(1.5) 67.8(1.7)
Table 4: The F1-score differences before and after
reranking on the reranked samples, as well as their pro-
portion of the total samples.
GPT-4 InstructGPT
before after △ ratio before after △ ratio
FewNER 31.9 40 .7 8 .8 3 .2% 31.4 28 .3 −3.1 3.3%
TACREV 25.3 43 .0 17.7 9.1% 33.8 43 .4 9 .6 7 .1%
ACE05 31.1 57 .9 26.8 1.6% 35.6 55 .7 20 .1 0 .5%
We remove all examples, rendering the reranking
a zero-shot problem. (3) LF (label filtering): We
retain all labels as candidate choices for reranking,
instead of only the top- N labels from the SLMs.
(4) AD (adaptive): We feed all samples, not just
hard ones, to the LLMs.
We show their results in Table 5 and see that
(1) Demos with explanations consistently enhance
the reranking ability of LLMs across all datasets.
(2) Demos without explanations also contribute to
performance improvement. (3) Label filtering re-
sults in gains and notably reduces the demo length,
Direct ICL (InstructGPT) Filter−then−rerank Fine−tuning (RoBERTa−large)
0
10
20
30
40
FewNERD TACREV ACE05
Financial cost
dollar($)
0
50
100
150
FewNERD TACREV ACE05
Time cost
second(s)
Figure 7: The financial and time cost over 500 sentences.
InstructGPT as the reranker.
Table 5: Ablation study on three datasets. The filter is
ensembled SLMs and the reranker is GPT-4.
CoT Demo LF AD
FewNERD
(20-shot)
TACREV
(100-shot)
ACE05
(20-shot)
✓ ✓ ✓ ✓ 63.6(1.2) 75.9(2.4) 67.8(1.7)
✗ ✓ ✓ ✓ 63.2(1.2) 75.4(2.4) 67.2(1.7)
✗ ✗ ✓ ✓ 63.0(1.4) 74.9(2.2) 66.6(1.5)
✗ ✗ ✗ ✓ 62.4(2.1) 73.8(2.5) 66.5(1.3)
✗ ✗ ✗ ✗ 12.5(2.7) 59.9(6.0) 5.4(1.1)
Previous SoTA methods 62.6(1.0) 74.1(1.7) 66.5(1.7)
hence cutting inference costs. (4) The performance
collapses without a filter to identify sample diffi-
culty, reiterating the need for an integrated SLM-
LLM system to complement each other.
6 Conclusion
Through an extensive empirical study on nine
datasets spanning four IE tasks, we find that LLMs,
despite their superiority in extreme low-resource
scenarios, are not effective few-shot information
extractors in general. They struggle with IE-related
prompts, have limited demonstration capacity, and
incur high inference costs. However, LLMs signifi-
cantly improve the performance on hard samples
when combined with SLM. Building on these in-
sights, we propose an adaptive filter-then-rerank
paradigm to leverage the strengths of SLMs and
LLMs and mitigate their limitations. This approach
consistently achieves promising results, with an av-
erage 2.4% F1 gain across multiple few-shot IE
tasks, while minimizing latency and budget costs.



Source: data\tc16_2312.10997v5\referenced_papers\[103]_2303.08559.pdf (Page 5):

(what we used) proves sufficiently effective. Con-
sequently, we believe that there unlikely exists a
lottery prompt that substantially alters our conclu-
sions that LLMs are not good few-shot IE solver.
3.6 Discussion: Why LLMs Fail to Obtain
Satisfactory Performance on IE Tasks?
Underutilized Annotations.We notice that LLMs
appear to benefit less from additional annotations,
i.e., more training samples and label types, than
SLMs. We speculate that LLMs are constrained
by ICL in two ways. (1) More samples: The num-
ber of effective samples for LLMs, those in de-
mos, is limited by maximum input length. More-
over, we also observe LLMs’ performance plateaus
in some tasks before reaching this limit (see Ap-
pendix E.3). Meanwhile, SLMs can continually
learn from more samples through supervised learn-
ing, widening the performance gap as annotated
samples increase. (2) More labels: LLMs struggle
with fine-grained datasets. It suggests a difficulty
in understanding numerous labels and their subtle
interactions merely from the given instruction and
exemplars for LLMs. Also, the examples per label
in demos decrease as label types increase.
Unexplored Task format. As stated in Zhang
et al. (2023a), IE-related tasks are scarce in the
widely-used instruction tuning datasets like Wei
et al. (2022a) and Wang et al. (2022). Furthermore,
the highly-flexible format of NER and ED tasks
impair the ICL abilities 7. Therefore it is likely that
instruction-tuned LLMs are not well-acquainted
with such IE-related task formats.
4 LLMs are Good Few-shot Reranker
4.1 Filter-then-rerank Paradigm
Read following sentences and identify what is the entity type
of “The New Yorker” quoted by <t>.
Sentence:
In 2004 Gourevitch was assigned to cover the 2004 U.S.
presidential election for “<t> The New Yorker <t>”.
Candidate Choices:
(a)The New Yorker does not belong to any known entities.
(b)The New Yorker is a broadcast program.
(c)The New Yorker is a kind of written art.
(d)The New Yorker is a media/newspaper organization.
Analysis:
The New Yorker is a well-known American magazine that has
been published since 1925, and is primarily known for its
long-form journalism, commentary, and satire. It has a
reputation for publishing high-quality writing on a wide
variety of topics, including politics, culture, and the arts.
So The New Yorker is a media/newspaper organization.
Correct Answer: (d)
Figure 4: Multi-choice question (MCQ) prompt.
7These two tasks require unfixed numbers of (label, span)
tuple. Furthermore, the length of each span is also unfixed.
To mitigate LLMs’ drawbacks mentioned above,
we propose a filter-then-rerank paradigm to inte-
grate both SLMs and LLMs within the same system.
This paradigm uses SLMs as filters to select the
top-N candidate labels, then LLMs rerank them
to make final decisions. By using SLM-generated
candidate answers, the focus of LLMs shifts from
sentence-level (i.e., identifying all entities/events
in the sentence) to sample-level (i.e., determin-
ing single entity/event candidate provided). Each
question now corresponds to a single sample, al-
lowing us to reframe prompts as multi-choice ques-
tions (MCQ; shown in Figure 4) problem. Un-
der such format, each candidate label is converted
to a choice by pre-defined templates. We claim
filter-then-rerank paradigm is more likely to elicit
the powers of LLMs and smoothly solve few-shot
IE tasks because: (1) LLMs are more familiar
with MCQ prompts than IE-format prompts (Zhang
et al., 2023a). (2) This paradigm reduces the la-
bel scopes significantly, since N is usually much
smaller than fine-grained label numbers.
4.2 LLMs are Hard Sample Solver
Our filter-then-rerank paradigm, unfortunately,
presents unsatisfactory performance (and even suf-
fers longer latency since LLMs rerank candidates
per sample). Given LLMs’ abilities in memoriza-
tion and reasoning, however, we still believe that
LLMs are potential to solve some, if not most, IE
samples effectively. We hypothesize that LLMs
are more proficient than SLMs on hard samples.
These samples are characterized by their requisite
for external knowledge acquisition or sophisticated
reasoning strategies, areas where LLMs can lever-
age their extensive parametric knowledge bases and
inherent reasoning mechanisms. In contrast, SLMs
often falter with such samples, constrained by their
restricted modeling capacities.
We leverage an unsupervised metric from SLMs
to evaluate thedifficulty of samples. Given a sample
x in the sentence s, we define the highest probabil-
ity across all labels as the confidence score:
conf(x) = max
l∈L
PSLM (l|x; s) (2)
where L denotes the label set andPSLM (l|x; s) the
probability of a span x (in the sentence s) referring
to label l computed by SLMs. We classify sam-
ples with low confidence scores as hard samples.
Otherwise we view them as easy samples.



### Claim 54/179

#### Claim Text
For instance, in Chatlaw [104], the LLM is prompted to self-suggestion on the referenced legal provisions to assess their relevance.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[104]_2306.16092.pdf (Page 4):

that our Chatlaw-MoE not only outperforms specialized legal models but also excels against general-purpose language models,
establishing it as the leading model for legal task performance.
1.3 Chatlaw Utilizes Multi-Agent Collaborative Framework to Enhance Legal Service Reliability
Chatlaw encompasses a LLM-based multi-agent collaboration system and simulates a real lawyer consultation process through
Role Specialization and Agent Workflow.
Explicit role specialization can decompose complex work into smaller and more specific tasks, accelerating agents with
different professional collaborating with each other. We define four roles in our virtual legal firm: Legal Assistant, Legal
Researcher, Legal Compliance, and Legal Editor, and specify the profiles of each role, such as role cognition, constraints, and
corresponding knowledge templates and skills. For example, the Assistant has a set of legal knowledge graphs, the Researcher
can use Internet search tools, and the Senior Lawyer posses document templates.
The agent collaboration workflow follow a sequential SOP. As shown in Figure 1(c): 1) the Legal Assistant analyze the
consultation content and select the corresponding legal knowledge graph, then ask the user questions to fill in the graph nodes
until necessary information is obtained; 2) the Legal Researcher searches the Internet based on key information, finds relevant
legal provisions, and classifies cases to obtain similar cases based on categories; 3) the senior lawyer, analysis relevant cases
and provides legal advice; 4) Legal Editor, summarizes opinions and drafts formal legal documents such as contracts to meet
user needs based on the previous information. We providing details of each agent in the following sections.
1.3.1 Legal Assistant with Knowledge Graph
Under the guidance of legal experts, we abstract the process of legal consultation into the construction of a professional
knowledge graph. Firstly, determining the type of legal consultation question involves acquiring the corresponding predefined
entity set. Subsequently, the process of lawyers guiding the consultee to answer questions is based on this entity set. They ask
questions to users, gather key information, expand corresponding nodes, until forming a complete consultation knowledge
graph, serving as the basis for subsequent processes.
We implement this process using the Legal Assistant agent of the Chatlaw framework. The Legal Assistant first selects
appropriate predefined entity clusters based on the consultation question and initially fills in information nodes. For nodes with
insufficient information, it integrates them into new questions to ask users, guiding them to add more relevant information to the
knowledge graph. As shown in the fig 1(c), we demonstrated a case practice using divorce consultation. The Legal Assistant
selects the predefined entity cluster “marital issues" based on user input and asks the user about “child information," “both
parties" intentions," “financial information," and “historical information" from the four nodes. After the corresponding nodes
are supplemented with complete information, the comprehensive information is passed to the Legal Researcher, who retrieves
corresponding cases based on different node keywords and proceeds with the subsequent steps.
1.3.2 Legal Researcher with Retrieval-Augmented Generation
To mitigate the impact of outdated or erroneous legal information stored in LLMs’ parameters, we construct a Legal Researcher
agent based on Retrieval-Augmented Generation (RAG). This agent accepts the knowledge graph summarized by the Legal
Assistant as input and collects the latest legal statutes and judicial interpretations from external knowledge repositories.
Leveraging the knowledge graph established by the Legal Assistant, we combine relevant nodes in the knowledge graph based
on legal knowledge to obtain keyword pairs. Then, we call upon the retrieval system to gather relevant information from the
internet and domain databases. The retrieved documents are then processed in parallel, and a large language model evaluates
each document’s relevance to the query. Subsequently, a critic model generates critical evaluations to review the generated
content and select the best output. This model may iterate the process, continuously adjusting and optimizing based on the
retrieved information and the content generated. Eventually, the model outputs a self-assessed and optimized response, which is
more likely to be accurate, relevant, and informative.
1.3.3 Lawyer and Legal Editor Agent
During the actual user consultation process, the next two steps involve providing consultation advice and generating consultation
documents, which are implemented in Chatlaw by two agents: Lawyer and Legal Editor. They are responsible for transforming
the consultation content obtained by the Assistant and the relevant cases obtained by the Researcher into the final output.
The Lawyer handles the Consultation stage, using the user’s consultation questions and auxiliary information as inputs
to a large language model to provide consultation advice items. This stage aims to propose as many answers as possible,
covering a wide range of scenarios. Subsequently, a firewall strategy is responsible for identifying potential risks for the user
and addressing them. Meanwhile, the Editor focuses on reviewing the Lawyer’s text and converting it into the final document.
Its text repository contains various document templates, into which the Lawyer’s output is inserted to obtain the final output.
5/11



Source: data\tc16_2312.10997v5\referenced_papers\[104]_2306.16092.pdf (Page 2):

Application
Memorization
Understanding
0
10
20
30
Tasks
Expert 0
0
10
20
30
Expert 1
0
10
20
30
Expert 2
0
10
20
30
Layers
Expert 3
Legal Dataset with Knowledge Graph
Multi-Source Data
Question Answer Dataset
Knowledge Graph
Agent DatasetHuman Finetuned Data
Data Screening Data Construc�on
Mixture-of-Experts Large Language Model
Tokenizer MoEB lock
Mul�-Agent Collabora�ve F ramework
Text-form
Legal Prompt
Multihead Self-Attention
Word Embedding
Text Features Router
Expert:0
Expert:1
Expert:2
Expert:3
Legal Assistant Legal Researcher
Legal Editor Senior Lawyer
User Agent
consult
answer
inquiry
1
2
fill in nodes of 
knowledge-graph
Agent
......
Legal Datasets
Deduplication
Denoising
Legal Entities
Relationship
Substantial Cases
anaylse extract
search
3
4 5
6
Agent
cases
&
provisions
case
study
1
2
3
relevant
relevant
irrelevant...
item:1
item:2
item:n
provide
results
Agent
firewall
strategy
consult
documents
selectingAnswer
Templates
filling
UnderstandingMemorizationApplications
a.
b.
c.
9 8101112
7
Figure 1.Framework of Chatlaw.(a) Chatlaw presents a comprehensive and diverse legal dataset. Multi-source data
undergoes deduplication and denoising, followed by human finetuning to produce high-quality QA datasets, knowledge graphs,
and agent datasets. This process ensures the accuracy and relevance of the legal data used for training. (b) Chatlaw extends
LLM to MoE model. This model utilizes a tokenizer to embed legal prompts using multi-head self-attention mechanisms and
processes text features through multiple experts. Each expert contributes to the final output based on weighted sums, optimizing
the model’s ability to handle various legal tasks efficiently and accurately. (c) Chatlaw employs a multi-agent collaborative
framework. This framework involves several roles, and each agent follow a ‘sense-think-action’ three-step process: The Legal
Assistant interacts with users to gather information and fill in knowledge graph nodes. The Legal Researcher analyzes and
extracts legal entities, relationships, and substantial cases from the legal datasets. The Legal Editor assists users in consulting
documents, selecting templates, and filling documents, while also ensuring a firewall strategy for data security. The Senior
Lawyer conducts case studies, evaluates the relevance of items, and provides comprehensive results. This collaborative
approach ensures efficient and accurate legal consultation, enhancing the quality of legal services and client satisfaction.
3/11



Source: data\tc16_2312.10997v5\referenced_papers\[104]_2306.16092.pdf (Page 0):

Chatlaw: A Multi-Agent Collaborative Legal Assistant
with Knowledge Graph Enhanced Mixture-of-Experts
Large Language Model
Jiaxi Cui1*, Munan Ning1,2,4*, Zongjian Li1*, Bohua Chen1, Yang Yan1, Hao Li1,2,4, Bin
Ling3,4, Yonghong Tian1,2,4, and Li Yuan1,2,4†
1School of Electronic and Computer Engineering, Peking University, Shenzhen, China
2Peng Cheng Laboratory, Shenzhen, China
3Law School, Peking University, Beijing, China
4AI for Science (AI4S)-Preferred Program, Peking University Shenzhen Graduate School, Shenzhen, China
*These authors contributed equally to this work
†Corresponding authors: yuanli-ece@pku.edu.cn
ABSTRACT
AI legal assistants based on Large Language Models (LLMs) can provide accessible legal consulting services, but the
hallucination problem poses potential legal risks. This paper presents Chatlaw, an innovative legal assistant utilizing a
Mixture-of-Experts (MoE) model and a multi-agent system to enhance the reliability and accuracy of AI-driven legal services.
By integrating knowledge graphs with artificial screening, we construct a high-quality legal dataset to train the MoE model.
This model utilizes different experts to address various legal issues, optimizing the accuracy of legal responses. Additionally,
Standardized Operating Procedures (SOP), modeled after real law firm workflows, significantly reduce errors and hallucinations
in legal services. Our MoE model outperforms GPT -4 in the Lawbench and Unified Qualification Exam for Legal Professionals
by 7.73% in accuracy and 11 points, respectively, and also surpasses other models in multiple dimensions during real-case
consultations, demonstrating our robust capability for legal consultation.
Keywords Artificial Intelligence in Law, Mixture of Experts, Large Language Model, Knowledge Graph, Legal Technology
Legal services play a crucial role in protecting individual rights and maintaining social fairness1–3. However, the limited
availability of legal professionals and the high cost of their services often restrict access to these services, particularly in China,
with its vast population and extensive social interactions. Statistics show that Chinese legal aid centers have accepted 800,000
cases over the past seven years, aiding over 6 million people, yet annually, only one-quarter of the 700,000 cases received can
be processed4. This gap in legal service provision deeply impacts justice and equity, especially for those lacking the resources
to effectively navigate the legal system. This raises a crucial question: can we establish an automated legal assistant to address
these challenges?
In recent years, the efficacy of LLMs has been validated across multiple scientific fields, encompassing natural language
processing5, 6, biochemistry7–14, and the medical field15–22. LLMs also offer potential solutions to the challenges in legal services.
Popular models like ChatGPT23 and the LLaMA24 series, along with other general-purpose25–30 or law-specific models,31, 32 can
respond to user inputs based on their internal legal knowledge repositories and provide advisory recommendations. However,
the inherent hallucination issues in LLMs pose potential risks in their application to legal domains 33 since they operate at
the level of word distributions rather than validated facts34. The knowledge generated by these models is often incomplete or
outdated, leading them to produce illusions that, although seemingly relevant, may be misleading or incorrect33, 35.
To address the mentioned issues, this paper designs Chatlaw, a multi-agent virtual legal assistant based on a multi-expert
large language model. Chatlaw effectively mitigates hallucination issues through key aspects of data quality, model optimization,
and consulting processes. Initially, we create a high-quality legal dataset through multiple screenings and integrate similar
advisory knowledge into a knowledge graph to ensure data accuracy. Next, we expand from a single-expert model to an
MoE model, increasing the parameter space to allow different experts to handle specific advisory tasks, thus enhancing the
accuracy of the legal consult. Building on this, we emulate the service workflows of real law firms and develop a set of SOP
for multi-agent collaboration. This SOP includes four independent intelligent agent roles responsible for initial information
gathering, in-depth material research, legal advice, and final consultation report writing. This procedural operation ensures
each step of information processing is efficient and accurate, significantly enhancing the quality of legal services and client
1
arXiv:2306.16092v2  [cs.CL]  30 May 2024



Source: data\tc16_2312.10997v5\referenced_papers\[104]_2306.16092.pdf (Page 6):

1.4 Real-world Legal Consultation Performance
To evaluate the performance of Chatlaw, we conducted a comprehensive assessment based on real judicial consultations, focusing
on criteria such as Completeness, Correctness, Guidance, and Authority. As illustrated in Figure 3, Chatlaw consistently
achieves the highest overall scores across these criteria, particularly excelling in Completeness, Guidance, and Authority.
Compared to other models like GPT-4, GPT-3.5, Baichuan2, ChatGLM2, InternLM2, Qwen, and Fuzi, Chatlaw demonstrates
superior capability in providing high-quality legal consultations. The win rate matrix in Figure 3 shows that Chatlaw has a
higher win rate compared to other models, indicating its overall better performance in cases analysis and law consultation tasks.
These results affirm the effectiveness of our multi-agent collaboration system, highlighting its potential to provide robust and
accurate legal consultation services.
Multi-Agents Collaboration Process 
Legal Assistant: Legal Researcher:
Senior Lawyer:
Hello, I would like to consult about divorce issues.
Do you have any children? If so, what is the current age of the child and who is
taking care of them? Additionally, do you have any joint property that needs to be
divided? And do you have any other divorce claims?
We have one child who is currently 6 years old. I hope to gain custody of the child.
What are your and your spouse's financial situations? Do you have stable income 
sources? Do you have sufficient funds to raise the child? Additionally, do you 
have any other financial claims?
We have stable income sources, with my monthly income around 8000 and hers 
around 6000. 
Legal Consultation Report
I have found the following relevant laws and regulations for you.
1. Article 6 of the "Opinions on Several Specific Issues Concerning the Handling of
Child Custody in Divorce Cases by the People's Courts“
2. Article 1085 of the Civil Code of the People's Republic of China
3. Article 1087 of the Civil Code of the People's Republic of China
4. …
I have provided you with legal advice in the following areas.
1. Child Custody
2. Property Division
3. Domestic Violence and Infidelity
4. Divorce Agreement
Consultation Questions: 
Based on the information you provided, your divorce consultation 
includes the following points:
1. You have a 6-year-old child and hope to gain custody of the
child.
2. Both you and your spouse have stable income sources, with
your monthly income being approximately 8000 yuan and
your spouse's monthly income being approximately 6000
yuan.
3. You wish to divide a jointly owned house, which was
purchased jointly after marriage.
4. There is no domestic violence, infidelity, or similar situations
between you and your spouse.
Consultation Opinions:
1. Child Custody: You have a 6-year-old child and seek custody.
The court considers financial stability, caregiving ability, child's
stability, and preferences. Your stable income and higher
earnings than your spouse may favor your claim.
2. Property Division: You wish to divide a jointly owned house.
According to the Civil Code of China, property bought jointly
after marriage is joint marital property and should be divided
equally. Negotiate with your spouse or request a court
decision if needed.
3. Domestic Violence and Infidelity: There is no domestic
violence or infidelity. The court will base decisions on other
factors.
4. Divorce Agreement: Try to negotiate a divorce agreement with
your spouse, covering child custody and property division. This
can reduce litigation time and costs.
Relevant Cases:
Check out the attachment for more details.
Relevant Laws and Regulations:
1. Article 6 of the "Opinions on Several Specific Issues
Concerning the Handling of Child Custody in Divorce Cases by
the People's Courts“
2. Article 1063, 1085, 1087 of the Civil Code of the People's
Republic of China
3. Article 78 of the Interpretation (I) of the Supreme People's
Court on the Application of the Civil Code of the People's
Republic of China on Marriage and Family
Check out the attachment for more details.
Figure 4.Multi-Agents Collaboration Process and Legal Consultation Report.The diagram illustrates the collaboration
process of multiple agents in providing legal consultation services, with a divorce consultation as an example. The Legal
Assistant initiates the consultation by gathering essential information from the user about their divorce case, including details
about children, financial situations, and property division. The Legal Researcher then identifies relevant laws and regulations,
such as articles from the Civil Code and specific legal opinions. The Senior Lawyer provides comprehensive legal advice based
on the collected information and relevant legal frameworks. The process culminates in the generation of a Legal Consultation
Report, which outlines the key consultation questions and provides detailed opinions on child custody, property division,
domestic violence, infidelity, and divorce agreements. The report also references relevant cases and laws for further guidance,
ensuring that the legal advice is thorough and well-supported.
2 Discussion
Overall, as shown in Fig. 1, we establish a comprehensive legal dataset based on the practical experience of real law firms, train
a MoE model with precise legal consultation capabilities and further expand it into a complete multi-agent framework. This
framework is able to act as a virtual law firm, providing comprehensive legal services to a wide range of users. Evaluations on
two benchmarks and the real-case consultation confirms that our approach surpasses the current powerful LLMs.
One limitation of AI legal assistant is the hallucination issue, where LLMs sometimes create information that does not exist
in the real world. In the legal field, this may manifest as fabricating non-existent legal provisions or introducing outdated and
incorrect laws and cases. To mitigate this issue, we introduced a dedicated legal researcher role that corrects the erroneous
information by retrieving the latest legal provisions and relevant cases from the internet, as shown in Fig. 4.
7/11



Source: data\tc16_2312.10997v5\referenced_papers\[104]_2306.16092.pdf (Page 3):

Understanding
Application
Single Choice
Memorization
Multi Choice
Uncertain Choice
0%
50%
100%
Models
ChatLaw
GPT4
GPT−3.5
Baichuan2
ChatGLM2
InternLM2
Qwen
Fuzi
a c
b d
Legal pr
ovisions
Legal inte
rpretations
RegulationsPrecedentsTypical cases
Cases of pu
blic interest
LAIC
CAIL
Other Competitions
Examination questions
Examination answers
Examination analysis
Lawyer consultationPublic consultationLegal Q&A
Keyword 
extraction
Text classificationText gene
ration
Issue 
Topic Identification
Reading ComprehensionEntity recognition
Relationship extractionIntent recognition
Named Entity RecognitionDispute 
Focus Recognition
Argument Mining
Document proofreading
Document merging
Document splitting
Sub−case segmentation
Clarify detailsLegal document d
rafting
Legal argumentation
Event Detection
Trigger Word Ext
raction
Statute Prediction
Charge Prediction
Sentence PredictionCase Analysis
Statute matching
Sentence calculation
In
ference of charges
Crime Amount Calculation
Public Opinion Summary
25
50
75
100
ChatL
aw
GPT4
GPT−3.5 Baichuan2 ChatGLM2 Inte
rnLM2 Qwen Fuzi
Score
Model
ChatLaw
GPT4
GPT−3.5
Baichuan2
ChatGLM2
InternLM2
Qwen
Fuzi
Model Performance on Lawbench
50
100
ChatL
aw
GPT4
GPT−3.5 Baichuan2 ChatGLM2 Inte
rnLM2 Qwen Fuzi
Score
Model
ChatLaw
GPT4
GPT−3.5
Baichuan2
ChatGLM2
InternLM2
Qwen
Fuzi
Model Performance on Unified Qualification Exam for Legal Professionals
Figure 2.Data Visualization and Performance Comparison of ChatLaw and Other Models.(a) Visualization of the
diverse task range covered by our legal dataset. The dataset encompasses various legal domains, including case classification,
statute prediction, document drafting, and more specialized tasks like public opinion analysis and named entity recognition. (b)
Radar chart illustrating model performance across different categories in the legal dataset. Categories include Understanding,
Memorization and Application are included in Lawbench, while Single Choice, Multi Choice and Uncertain Choice are
different types of Unified Legal Exam. ChatLaw demonstrates superior performance across multiple categories, highlighting its
robustness and versatility. (c) Box plot comparing model performance on Lawbench. The models compared include ChatLaw,
GPT-4, GPT-3.5, Baichuan2, ChatGLM2, InternLM2, Qwen, and Fuzi. ChatLaw consistently outperforms other models,
showcasing its effectiveness in legal cognitive tasks. (d) Box plot comparing model performance on the Unified Qualification
Exam for Legal Professionals. ChatLaw maintains high performance across 5 years from 2018 to 2022, demonstrating its
comprehensive understanding and application of legal knowledge.
1.2.2 Performance on Unified Qualification Exam for Legal Professionals
The other benchmark is the China’s Unified Qualification Exam for Legal Professionals, including single-choice questions,
multiple-choice questions, and uncertain-choice questions. These questions cover various legal fields and can effectively assess
the understanding and application ability of legal concepts, principles, and provisions for LLMs.
As shown in Fig. 2(d), in the unified legal professional exam from 2018 to 2022, our Chatlaw-MoE model consistently
outperformed all other models. With an average score of 115, Chatlaw-MoE significantly surpasses GPT-4’s average score
of 104. Specifically, Chatlaw-MoE achieved scores of 113, 124, 143, 115, and 78 across the five years, consistently demon-
strating superior performance. In comparison, GPT-4’s scores were 102, 108, 82, 82, and 118 respectively. This consistent
outperformance highlights Chatlaw-MoE’s enhanced capability in handling legal examination questions, likely due to the
multi-expert system design which dynamically selects the most suitable experts for processing based on input features. Among
the Legal LLMs, Fuzi-Mingcha had an average score of 34, with its highest performance in 2019 at 40. For General LLMs,
Baichuan2-7B was the strongest, with an average score of 61 and its highest score of 70 in 2019. These results clearly indicate
4/11



### Claim 55/179

#### Claim Text
For example, it can enable LLM to adapt to specific data formats and generate responses in a particular style as instructed [37].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[53]_2312.07559.pdf (Page 15):

Appendix
A P APER QA I MPLEMENTATION DETAILS
A.1 V ERSIONS
The underlying model versions used are GPT-3.5-turbo-0613 and GPT-4-0613. We used
LangChain v0.0.303 and OpenAI-python v0.28.1.
A.2 S YSTEM PROMPT
The system prompt for the LLMs ( ask LLM, summary LLM, and answer LLM) is given below:
Answer in an direct and concise tone, I am in a hurry. Your audience is an expert, so be
highly specific. If there are ambiguous terms or acronyms, first define them.
The system prompt of the agent is
You are a helpful AI assistant.
A.3 T OOL DESCRIPTIONS
search description:
Search for papers to increase the paper count. Input should be a string of keywords. Use
this format: [keyword search], [start year]-[end year]. You may include years as the last
word in the query, e.g. ’machine learning 2020’ or ’machine learning 2010-2020’. The
current year is get_year().
gather evidence description:
Give a specific question to get evidence for it. This will increase evidence and relevant
paper counts.
generate answer description:
Ask a model to propose an answer using evidence from papers. The input is the question to
be answered. The tool may fail, indicating that better or different evidence should be
found.
A.4 A SK LLM P ROMPT
To capture latent knowledge in LLMs, we use the following prompt:
We are collecting background information for the question/task below.
Provide a brief summary of information you know (about 50 words) that could help answer
the question - do not answer it directly and ignore formatting instructions. It is ok to
not answer, if there is nothing to contribute.
Question: question
16



Source: data\tc16_2312.10997v5\referenced_papers\[103]_2303.08559.pdf (Page 2):

T ext 
T ext 
T ext 
Relation ExtractionNamed Entity Recognition 
Event Detection Event Argument Extraction
Figure 1: Examples of prompts used. The green, blue and black parts in the top boxes represent the instruction,
demonstration (demo) and test sentence in the prompt respectively. The red parts represent the outputs from LLMs.
We plot only 1 example for convenience of visualization. The actual demo number is usually much larger than 1.
2015). (4) Event Argument Extraction (EAE):
ACE05, ERE and RAMS (Ebner et al., 2020). With
label numbers ranging from 4 to 168, we assess
LLMs’ performance under different schema com-
plexities. See their details in Appendix A.1.
Few-shot SetWe construct few-shot datasets from
the original datasets above. For training and vali-
dation set, we adopt K-shot sampling strategy, i.e.,
sampling K samples for each label type. See more
details in Appendix A.2. For test set, we down-
sample their original test sets to reduce the cost
of LLMs. We randomly sample 500 sentences for
RE tasks, and 250 sentences for other task. We en-
sure that each label has at least one corresponding
sample to avoid the absence of rare labels.
Evaluation We adopt micro-F1 score in NER, RE
and ED tasks. For EAE task, we follow previous
work (Wang et al., 2023b) and adopt head-F1 score,
which merely considers matching of the head word
rather than the whole content of a text span. We re-
port averaged score w.r.t 5 sampled train/validation
sets unless otherwise stated.
3.2 Small Language Models
We adopt five supervised methods to evaluate the
abilities of SLMs. (1) Vanilla fine-tuning for all
tasks, (2) FSLS (Ma et al., 2022a) for NER and ED
tasks, (3) KnowPrompt (Chen et al., 2022b) for RE
task, (4) PAIE (Ma et al., 2022b) for EAE task, and
(5) UIE (Lu et al., 2022c) for all tasks. See their
details in Appendix B.
3.3 Large Language Models
Detailed in Appendix C, we evaluate the ICL abil-
ities of LLMs. Given labeled sentences D =
{(si, yi)} and a test sentence s, our goal is to pre-
dict structured information y from s using a frozen
LLM L. We feed LLM with prompt PE,I,f (D, s):
PE,I,f (D, s) = [I; f(E(D, s)); f(s)] (1)
We give examples of prompts on four IE tasks
in Figure 1. The prompts consist of three parts: in-
struction I (color in green in Figure 1), demonstra-
tion f(E(D, s)) (demo; color in blue) and the ques-
tion f(x) (color in black). Here E denotes demo
selector and E(D, s) ⊂ D denotes selected sen-
tences as the demo to predict s. Prompt format f 5
refers to the template which converts demoE(D, s)
and sample s to input context for LLMs. Then
LLM generates f(y) (color in red) from which we
could readily parse the extraction results y.
Models L: We explore six LLMs from two
sources. (1) OpenAI models 6: we employ Chat-
5We slightly abuse the notationf to allow s, y and {(s, y)}
as the input for simplicity.
6The versions of model we use are:gpt-3.5-turbo-0301,



Source: data\tc16_2312.10997v5\referenced_papers\[36]_2303.08559.pdf (Page 2):

T ext 
T ext 
T ext 
Relation ExtractionNamed Entity Recognition 
Event Detection Event Argument Extraction
Figure 1: Examples of prompts used. The green, blue and black parts in the top boxes represent the instruction,
demonstration (demo) and test sentence in the prompt respectively. The red parts represent the outputs from LLMs.
We plot only 1 example for convenience of visualization. The actual demo number is usually much larger than 1.
2015). (4) Event Argument Extraction (EAE):
ACE05, ERE and RAMS (Ebner et al., 2020). With
label numbers ranging from 4 to 168, we assess
LLMs’ performance under different schema com-
plexities. See their details in Appendix A.1.
Few-shot SetWe construct few-shot datasets from
the original datasets above. For training and vali-
dation set, we adopt K-shot sampling strategy, i.e.,
sampling K samples for each label type. See more
details in Appendix A.2. For test set, we down-
sample their original test sets to reduce the cost
of LLMs. We randomly sample 500 sentences for
RE tasks, and 250 sentences for other task. We en-
sure that each label has at least one corresponding
sample to avoid the absence of rare labels.
Evaluation We adopt micro-F1 score in NER, RE
and ED tasks. For EAE task, we follow previous
work (Wang et al., 2023b) and adopt head-F1 score,
which merely considers matching of the head word
rather than the whole content of a text span. We re-
port averaged score w.r.t 5 sampled train/validation
sets unless otherwise stated.
3.2 Small Language Models
We adopt five supervised methods to evaluate the
abilities of SLMs. (1) Vanilla fine-tuning for all
tasks, (2) FSLS (Ma et al., 2022a) for NER and ED
tasks, (3) KnowPrompt (Chen et al., 2022b) for RE
task, (4) PAIE (Ma et al., 2022b) for EAE task, and
(5) UIE (Lu et al., 2022c) for all tasks. See their
details in Appendix B.
3.3 Large Language Models
Detailed in Appendix C, we evaluate the ICL abil-
ities of LLMs. Given labeled sentences D =
{(si, yi)} and a test sentence s, our goal is to pre-
dict structured information y from s using a frozen
LLM L. We feed LLM with prompt PE,I,f (D, s):
PE,I,f (D, s) = [I; f(E(D, s)); f(s)] (1)
We give examples of prompts on four IE tasks
in Figure 1. The prompts consist of three parts: in-
struction I (color in green in Figure 1), demonstra-
tion f(E(D, s)) (demo; color in blue) and the ques-
tion f(x) (color in black). Here E denotes demo
selector and E(D, s) ⊂ D denotes selected sen-
tences as the demo to predict s. Prompt format f 5
refers to the template which converts demoE(D, s)
and sample s to input context for LLMs. Then
LLM generates f(y) (color in red) from which we
could readily parse the extraction results y.
Models L: We explore six LLMs from two
sources. (1) OpenAI models 6: we employ Chat-
5We slightly abuse the notationf to allow s, y and {(s, y)}
as the input for simplicity.
6The versions of model we use are:gpt-3.5-turbo-0301,



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 14):

Retrieve Anything To Augment Large Language Models Conference’17, July 2017, Washington, DC, USA
Table 9: Dataset details for training.
Task Dataset #Train Sample Repetition Stablized Distillation Reward Temperature
Question Answering MSMARCO 400870 1 ✓ 1
NQ 58622 1 1
In-Context Learning – 591359 1 ✓ 1
Long Conversation MSC 48925 1 ✓ 0.1
Long-Range Language Modeling
Books3 10000 1
✓
0.1
Arxiv 10000 1 0.1
CodeParrot 10000 1 0.1
Tool Learning ToolBench 87322 2 ✗ n.a.
Conversational Search QReCC 29596 1 ✗ n.a.
Total n.a. 1333911 n.a. n.a. n.a.
Table 10: Instructions for each task.
Task Input Instruction
Question Answering Query Represent this query for retrieving relevant documents:
Key Represent this document for retrieval:
In-Context Learning Query Convert this example into vector to look for useful examples:
Key Convert this example into vector for retrieval:
Long Conversation Query Embed this dialogue to find useful historical dialogues:
Key Embed this historical dialogue for retrieval:
Long-Range Language Modeling Query Embed this text chunk for finding useful historical chunks:
Key Embed this historical text chunk for retrieval:
Tool Learning Query Transform this user request for fetching helpful tool descriptions:
Key Transform this tool description for retrieval:
Conversational Search Query Encode this query and context for searching relevant passages:
Key Encode this passage for retrieval:
Table 11: Hyper parameter settings for training.
#GPU 8×A100 (40G)
#Hard Negative 7
Batch Size Per GPU 100
Optimizer AdamW
Learning Rate 5e-6
Weight Decay 0.01
Scheduler Linear with Warm Up of 0.2
Max Steps 10000
Gradient Checkpointing ✓
B IMPLEMENTATION DETAILS
B.1 Instructions
We use diversified instructions to discriminate different tasks for
the retriever. The instructions used for each task are shown in
Table 10.
B.2 Training Settings
The hyper parameter settings for training LLM-Embedder are re-
ported in Table 11. For evaluation, we use the Flat index from
Faiss [35] when retrieving from an external corpus is required. We
will release our code upon the acceptance of the paper.
C IMPACT OF LLM-EMBEDDER ON
DIFFERENT LLMS
We evaluate the impact of LLM-Embedder different LLMs to val-
idate its generalization ability. Specifically, we utilize Aquila-7B-
Chat [1], Qwen-7B-Chat [6], Baichuan2-7B-Chat [9], and Llama-
2-13B-Chat [78]. The results are shown in Table 12. Specifically,
we compare two baselines: None, where LLM is used individually
without retrieval augmentation; BGE, where LLM is augmented
with retrieved knowledge, examples, and memory (introduced in
Appendix A). We report the average accuracy for MMLU, accuracy
for PopQA, average score for in-context learning, and perplexity for
both Multi-Session Chat and Arxiv. Note that we do not replicate



Source: data\tc16_2312.10997v5\referenced_papers\[160]_2308.10633.pdf (Page 2):

Gradio5-based GUI (Abid et al., 2019) to es-
tablish an inference chain that comprises an
R-LLM. This chain of actions enables users
to design a pipeline for multi-step inference,
such as [retrieve]-[generate], or more intricate
workflows such as [rewrite query]-[retrieve]-
[generate] proposed in Ma et al. (2023). The ver-
satility of this feature is especially beneficial in
creating the chains tailored to specific use cases.
A single-action chain can function as either a
simple retriever that returns the retrieved docu-
ments, or a closed-book QA that leverages the para-
metric knowledge of an LLM to provide answers
without retrieval. In contrast, a chain with multi-
ple actions that include retrieval enables retrieval-
augmented generation or open-book QA, allowing
an LLM to access external documents relevant to a
question. Our default setup for R-LLMs consists
of two actions: retrieve and generate.
2.3 Prompt Engineering
The RALLE framework allows developers to in-
teractively craft customized prompt templates for
LLMs and even for search queries on a per-chain
basis. Each action can be executed independently,
enabling precise control over LLM responses, such
as specifying the desired output format or suppress-
ing undesirable hallucinations. To enhance the ver-
satility of prompt development, RALLE integrates
support for f-strings and eval() function in Python.
2.4 Experiment Tracking
We utilize MLflow (LF Projects, 2023) to track the
experiments, along with their associated configura-
tion files and prompt templates. This allows us to
compare the performance of different experiment
runs objectively, which enables us to develop even
better R-LLMs.
2.5 Chat AI
RALLE also provides support for building a sim-
ple chat interface. This enables users to test out
best practices from the development and evaluation
stages in a practical setting.
3 Experimental Settings
In this section, we evaluate the performance of R-
LLMs constructed with several combinations of
open-source retrievers and LLMs on knowledge-
intensive tasks.
5https://www.gradio.app/
3.1 Tasks and Datasets
We employ KILT (Knowledge Intensive Language
Tasks) benchmark (Petroni et al., 2021), an ex-
tensive benchmark that encompasses 11 datasets
across five knowledge-intensive natural language
processing tasks: fact checking, entity linking, slot
filling, open-domain question answering, and dia-
logue (for further details of KILT, see Petroni et al.
(2021)). We use the training sets for developing
prompts and the development set for evaluation.
As the knowledge source, we utilize the pre-
processed Wikipedia passages provided by KILT.
The passages are derived from English Wikipedia
articles based on the 2019/08/01 Wikipedia dump
data, consisting of a total of 5.9 million articles
and 22.2 million 100-word passages. For both
dense and sparse retrievers, we use the set of 100-
word passages after additional pre-processing that
prepends the title of the article to each passage.
Note that RALLE is dataset-agnostic, allowing
developers to use their own QA datasets and cor-
pora for development and evaluation. See Ap-
pendix A.10 for more information.
3.2 Models
This subsection details the retrievers and LLMs
employed to build R-LLMs in our experiments.
RALLE allows practitioners and researchers to eas-
ily experiment with the most recent models avail-
able in open-source repositories. With the excep-
tion of BM25, all models are available from Hug-
ging Face (Wolf et al., 2020) (see Appendix A.9
for the summary).
3.2.1 LLMs
The LLM used within the R-LLM must compre-
hend instructions provided in a prompt and gener-
ate appropriate responses based on the given infor-
mation. To achieve this, we use instruction-tuned
LLMs with a temperature parameter set to zero for
optimal performance and reproducibility.
Llama-2-chat is tuned with supervised fine-
tuning and reinforcement learning with human
feedback (RLHF) (Christiano et al., 2017; Stiennon
et al., 2020) to align to human preferences for help-
fulness and safety (Touvron et al., 2023b). In our
experiments, we utilize both 13-billion (Llama2-
13B) and 70-billion (Llama2-70B) models.
WizardVicunaLM-13B6 (W-Vicuna-13B)
(Lee, 2023) is formed by combining the concepts
6https://huggingface.co/junelee/
wizard-vicuna-13b



### Claim 56/179

#### Claim Text
For retrieval tasks that engage with structured data, the SANTA framework [76] implements a tripartite training regimen to effectively encapsulate both structural and semantic nuances.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 3):

Language Models as Semantic Indexers
… Codebook𝑬… Codebook𝑬
…
Semantic IDsWordtoken
He onlya rival
Deep TransEncoderDeep TransDecoder
Semantic Indexer
<s>
ShallowTransformer
Masked token 
…He only
is beenrival
ReconstructorSemantic Encoder
is been
Semantic ID representation
Document
 hints
Figure 1.The LMI NDEXER self-supervised ID learning framework overview. The proposed semantic indexer includes a semantic ID
encoder and several codebooks. During self-supervised learning, there is a reconstructor to reconstruct the input document from semantic
ID representations.
query channel input embeddings, and dh (token embeddings
correspond to dh) are fed as key and value channel input
embeddings in the multi-head self-attention. We adopt a
shallow reconstructor which has limited reconstruction capa-
bility based only on the hints in order to force the semantic
indexer to provide high-quality representations. The recon-
struction is conducted as follows:
zw “Reconϕpcd, dhq“
ÿ
t
Transpq “ct
d, k“dh, v“dhq
Preconpw|cd, dhq“ softmaxpW zwq
(5)
where W is the token embedding matrix. However, directly
adopting the reconstruction objective with cd as input to the
reconstructor will not optimize the semantic encoder. Since
the codebook look-up in Eq.(2) is a hard/discrete operation,
the reconstruction objective backpropagation gradients will
flow to the embeddings in the codebook rather than to the
parameters in the semantic encoder. To this end, we propose
to approximate the argmax operation similar to (Jang et al.,
2016) as follows:
ˆct
d “
$
&
%
arg maxet
jPEt ht
d ¨et
j forward pass.
ř
et
jPEt
exppht
d¨et
jqř
et
jPEt exppht
d¨et
jq et
j backward pass. (6)
In the forward pass, we still adopt the argmaxp¨qhard op-
eration; while in the backward pass, the selected semantic
embedding becomes a weighted average of the codebook
embeddings, to enable gradients to flow to ht
d and finally
to the parameters in the semantic encoder. In our imple-
mentation, we achieve this by adopting the stop gradient
operator (Van Den Oord et al., 2017). The reconstruction is
then conducted by
zw “Reconϕpˆct
d, dhq“
ÿ
t
Transpq “ˆct
d, k“dh, v“dhq
(7)
3.2. Training Self-Supervised Semantic Indexer
Progressive Training. To optimize the semantic indexer
and obtain semantic IDs in an auto-regressive way, we adopt
the progressive training scheme similar to (Sun et al., 2023).
The entire learning process consists ofT learning steps, each
corresponding to a specific semantic IDct
d being learned and
optimized at position t within the range of [T]. Additionally,
at each step t, both the ID ct
d and the model parameters
associated with generating ct
d are updated, while previously
generated IDs căt
d remain unchanged. The reconstruction
objective in t-step is shown as:
Lt
recon “´
ÿ
d
ÿ
wPdzdt
h
logPreconpw|cďt
d , dt
hq. (8)
Here dt
h is the hints provided for learning ID on position
t. We will gradually reduce the amounts of hints dt
h as t
increases to inject new knowledge into the new IDs, and
finally contribute to a hierarchical, coarse-to-fine-grained
semantic ID learning.
Contrastive Loss. The reconstruction objective in Eq.(8)
can force the semantic IDs to capture document-level se-
mantics. However, only optimizing the objective can lead to
the case where similar documents sharing căt
d also have the
same ct
d. To alleviate this issue, we propose a contrastive
objective to promote distinction between documents that
previously shared the same prefix, enabling the model to
discern finer-grained hierarchical relationships between doc-
uments:
Lt
contrastive “´
ÿ
d
log exppht
d ¨ht
dq
exppht
d ¨ht
dq` ř
căt
d1 “căt
d
exppht
d ¨ht
d1q.
(9)
The contrastive objective can help push ht
d of documents
sharing the same căt
d away in the t-th latent space and force
them to obtain diverse ct
d, finally contributing to higher
codebook utilization.
Commitment Loss. In addition, when learning the docu-
ment semantic IDs for position t, it is important that the
semantic indexer should remember the IDs that are already
learned before position t. To this end, we add a commitment
loss as:
Lt
commitment “´
ÿ
d
ÿ
jăt
log Pspcj
d|d, căj
d q. (10)
We optimize our model at step t based on a combination of
4



Source: data\tc16_2312.10997v5\referenced_papers\[37]_2211.07067.pdf (Page 3):

the separation tokens in between them) as the ﬁnal
demonstration sequence.
Demonstration dr = Qr [sep] Cr [sep]
The answer is: Ar
We use S-BERT (Reimers and Gurevych, 2019) to
calculate the similarity scores between the current
instance and all demonstrations in ST. S-BERT is
a modiﬁcation of the BERT model (Devlin et al.,
2019) that uses siamese and triplet network struc-
tures to obtain semantically meaningful embed-
dings for word sequences7.
To construct the target (sequence), we ﬁrst de-
termine how much to learn from the demonstration
– if the similarity score is above a threshold (deter-
mined on the development set), and the demonstra-
tion and current instance both have a non-empty an-
swer, then we assign 1 (Yes) to yanalogy, otherwise
0 (No). Then we concatenate all argument spans of
the role with [sep_arg] to construct yseq2seq,
yseq2seq = <s> Argument1
[sep_arg] Argument2 [sep_arg] ...</s>
The ﬁnal y includes yseq2seq and yanalogy.
3.2 Training and Inference
Training After the preparation for S =
{(x(i),y(i))}|S|
i=1, we minimize the joint loss func-
tion during training,
L= Lseq2seq + Lanalogy
Lseq2seq = −
|S|∑
i=1
log p(y(i)
seq2seq|x(i); θ)
= −
|S|∑
i=1
|y(i)
seq2seq|∑
j=1
log p(y(i)
j |x(i),y(i)
<j; θ)
(2)
where Lseq2seq is the cross-entropy loss between
the decoder’s output and the target sequence
yseq2seq. Lanalogy is the binary cross-entropy loss
calculated with the ﬁnal hidden state of the ﬁnal
decoder token.
Inference and Post-processing At test time, we
conduct greedy decoding to obtain the target se-
quence, then we split the decoded sequence with
7The SentenceTransformer library ( https://www.
sbert.net/docs/quickstart.html) supports calcu-
lations in batch.
respect to [seq_arg]. Since it is also required to ob-
tain the offsets of the argument in the input context,
we automatically match the candidate argument’s
span with the input context. Then, if there’s no
matched span, we discard the candidate argument;
if there are multiple matches, we select the one clos-
est to the trigger word. For example, if the input
context is “One of those difﬁcult judges [John M.]
is nominated (Type: nomination) by Adam to be
chief justice in 2000.. [John M.] started ofﬁce on ...”
and there are two appearances of the candidate ar-
gument (in brackets) for the role PERSON , then we
use the ﬁrst candidate’s offsets. Different from our
methods, the template-based generation method
generates a sequence similar to the one in Section 2
– causing the model to (1) not fully exploit the se-
mantic relations of roles across event types; (2)
require more complicated post-processing includ-
ing an additional step to obtain arguments from the
generated template.
3.3 Few-shot Setting and Sampling Strategy
Algorithm 1: Our Strategy for Obtaining Sfew
Input :|S|Unlabeled Examples, Sample Size N
1 k ←# event types (based on ontology);
2 Sfew ←[ ];
// obtain embeddings for all
unlabeled instances
3 for i ←1 to |S|do
4 repi ←[enc(contexti), enc(trigger_texti)];
5 add repi to all_reps;
6 end
7 clusters = k_means(all_reps);
// add instances to samples
8 for i ←1 to k do
9 #instance = length(clusters[i])
|S| ∗N;
10 instances =
sample(clusters[i], #instances);
11 add instances to Sfew ;
12 end
In the few-shot setting, we assume that we have
a budget to obtain annotations for a limited number
of examples’ arguments (5%-20% of all examples)
for training. We denote the set of few training
examples as Sfew . We study (1) how different
sampling strategies affect the Sfew ’s distributions
and models’ performance; (2) how to select the
best set of examples (in zero or one round 8) and
have them annotated for training, to achieve better
performance at test time.
We propose a sampling method calledJointEnc.
It uses k-means clustering upon the embeddings
8One-round active learning setting (Wang et al., 2021).



Source: data\tc16_2312.10997v5\referenced_papers\[18]_2203.08773.pdf (Page 3):

training data. In other words, we directly adopt the
training data T = {(x1, y1), ...,(xN , yN )} as the
indexed corpus C, where xi is the input and yi is
the ground-truth label.
Given an input x, the top K retrieved
training instances with labels are combined
with x as input to the model M, i.e.,
M(f(x, {(xi1 , yi1 ), ...,(xiK , yiK )}. Both training
and inference take this retrieve-combine-generate
scheme. Note that during training, as the input x
is already indexed, we ﬁlter it from the retrieval
results to avoid data leakage.
Now, we introduce how we deﬁne the keys, val-
ues, and the combination function for different
NLP tasks.
Summarization is to generate a summary for
a given document. We ﬁrst build an index for
the document-summary pairs in the training data,
where a document is the key and its summary
is the value. Given a document x, we search
for the most similar documents in the index. As
documents are usually quite long, the combination
function only keeps the values (summaries),
i.e., fsumm(x, {(xi1 , yi1 ), ...,(xiK , yiK )}) =
[x; yi1 ; ...; yiK ].
Language Modeling(LM) generates the prob-
ability of a given sequence of words. Typically, a
Left-to-Right language model (Dong et al., 2020)
is trained on chunked sequences with an attention
mask. In this paper, we use Seq2Seq based ap-
proach, i.e., given a context chunk, we predict the
next chunk of text.
In detail, we ﬁrst chunk all the text in the
training data. The IR index is built with one
chunk Ci as the key xi and its next chunk
Ci+1 as the value yi. Given a chunk x, we
look for the most similar keys in the index
and prepend their corresponding next chunks
to x, i,e., fLM (x, {(xi1 , yi1 ), ...,(xiK , yiK )}) =
[yi1 ; ...; yiK ; x].
Machine Translationis to translate text from
the source language S to the target language T .
We deﬁne the key to be the sentence in S and the
value to be its translation in T . To keep the se-
quence short and speed up the training process,
we only concatenate the retrieved text in target
language: fMT (x, {(xi1 , yi1 ), ...,(xiK , yiK )}) =
[x; yi1 ; ...; yiK ].
Question Answering We mainly consider
multiple-choice question answering, where com-
monsense knowledge is also required to reach the
correct answer. For each question xi, there is a
correct choice yi and several distractive candidate
choices. We index the concatenation of the ques-
tion and the corresponding ground-truth choice.
For a new question x, the model is given sev-
eral choices c1, ..., cM . We concatenate x with
each choice ci as the query and retrieve related
training instances: {(xi1 , yi1 ), ...,(xiK , yiK )} =
E(x; ci|C). The combination function f concate-
nates both retrieved question and answers with the
input: fQA((x, ci), {(xi1 , yi1 ), ...,(xiK , yiK )}) =
[x; ci; xi1 ; yi1 ; ...; xiK ; yiK ]. Then, the model pre-
dicts a score representing how likely ci is the cor-
rect choice to x.
As the task requires commonsense knowledge,
we build another version of index integrating com-
monsense knowledge. We follow the strategy from
(Xu et al., 2021) and extract the knowledge from
ConceptNet (Speer et al., 2017) and Wiktionary3
for the concepts in the question and choices. For
each question x and choice c, we use string
match to ﬁnd corresponding entities in Concept-
Net: E(x) = {e(x)
1 , ..., e(x)
nx } appears in the ques-
tion, and E(c) = {e(c)
1 , ..., e(c)
nc } appears in the an-
swer. To ﬁnd the most relevant concept, we choose
the concept with maximum length as the question
and answer concept. We ﬁnd the deﬁnition of the
chosen concepts from Wiktionary. To ﬁnd relations
in ConceptNet, we ﬁnd edges that connects ques-
tion and answer concepts: R = {(e1, r, e2)|e1 ∈
E(x), e2 ∈ E(c), (e1, e2) ∈ KG}. Here KG is Con-
ceptNet and r is a relation (e.g., AtLocation).
We concatenate the Wiktionary deﬁnitions and Con-
ceptNet relations R to form the knowledge, K, for
a question. The knowledge K is included both in
the query and index. Thus, the retrieval process
becomes: {(xi1 , ci1 , Ki1 ), ...,(xiK , yiK , KiK )} =
E(x; ci; K|C). The combination function f
concatenates retrieved questions and answers
with the input: fQAK((x, ci), E(x; ci; K|C)) =
[x; ci; xi1 ; yi1 ; ...; xiK ; yiK ].
3.3 Model Training and Inference
After concatenating the input with the retrieved
data from the training corpus, we feed the new se-
quence into the Seq2Seq framework for generation
tasks and the encoder-only framework for question
answering tasks. During training, as it will also
retrieve the exact golden label, we ﬁlter it directly.
During inference, we will not ﬁlter any retrieved
3https://www.wiktionary.org/



Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 11):

Language Models as Semantic Indexers
Table 7.Dataset Statistics
Dataset # Items # Users # Rec history (train/dev/test) # Search query (train/dev/test) # Search labels (train/dev/test)
Amazon-Beauty 12,101 22,363 111,815 / 22,363 / 22,363 1,049 / 150 / 338 1,907 / 268 / 582
Amazon-Sports 18,357 35,598 177,990 / 35,598 / 35,598 1,299 / 186 / 443 2,209 / 311 / 764
Amazon-Toys 11,924 19,412 97,060 / 19,412 / 19,412 1,010 / 145 / 351 1,653 / 250 / 594
Table 8.Dataset Statistics
Dataset # Documents # Query (train/test) # Search labels (train/test)
NQ320k 109,739 307,373 / 7,830 307,373 / 7,830
MACRO 1M 1,000,000 502,939 / 6,980 532,751 / 7437
TREC-DL 1M 1,000,000 502,939 / 93 532,751 / 1,069
Algorithm 1 Self-supervised ID Learning Procedure of
LMI NDEXER
1: Input: The document corpus tdu.
2: Output: The semantic IDs tcduof the documents tdu.
A semantic indexer SemIndexerp¨qwhich contains a
semantic encoder SemEncθp¨qand codebooks tEtut.
A reconstruction model Reconϕp¨q.
3: Begin
4: // initialize semantic encoder
5: SemEncθp¨qÐ T5-base;
6: // reconstruction warm up
7: minϕ L0
recon “´ ř
d
ř
wPdzd0
h
log Preconpw|d0
h q;
8: for t “1, . . . , Tdo
9: // semantic encoder & codebook warm up
10: ht ÐSemEncθpd, cdq;
11: zw ÐReconϕpq “tct
d, ht
du, k“dt
hq;
12: minϕ,ϕc Lt “Lt
recon `Lt
contrastive `Lt
commitment;
13: ht
d ÐSemEncθpd, ct
dq;
14: Et ÐKMeanspht
dq;
15: // whole framework training
16: zw ÐReconϕpq “tct
d, ct
du, k“dh, v“dhq;
17: minϕ,ϕc Lt “Lt
recon `Lt
contrastive `Lt
commitment;
18: ct
d Ðargmaxjpspct
d “j|cd, dq;
19: end for
20: Return tcdu, SemIndexerp¨q;
21: End
rate in {1e-3, 2e-3, 5e-3}. The training epochs are set to be
30, 10, and 5 for Amazon datasets, NQ, and MS MACRO
respectively. The hyper-parameter configuration for self-
supervised semantic indexer training can be found in Table
9.
In the downstream recommendation task, for generative
recommendation methods with semantic IDs (rq-V AE in-
dexer, hierarchical clustering indexer, and LMI NDEXER ),
we concatenate the textual information (title & description)
of the user’s previously interacted items, serve it as the input
text into the generative language model and ask the model
to generate the ID for next item. The baselines are using
the same T5-base checkpoint. We train all the compared
generative recommendation methods for 10,000 steps with
the learning rate searched in {1e-2, 1e-3, 1e-4}. The batch
size is set to be 32, the maximum input text length is set
to be 1024 and all experiments are run on an 8 A100 40G
machine. The number of beams for beam search is set to
20. The hyper-parameter configuration for generative rec-
ommendation training can be found in Table 10.
In the downstream product search task, for generative re-
trieval methods with semantic IDs (rq-V AE indexer, hierar-
chical clustering indexer, and LMI NDEXER ), we serve the
query as the input text into the generative language model
and ask the model to generate the ID for the relevant items.
All baselines initially load the same T5-base checkpoint.
We train all the compared generative retrieval methods for
10,000 steps with the learning rate searched in {1e-2, 1e-
3, 1e-4}. The batch size is set to 32, the maximum input
text length is set to be 1024 and all experiments are run on
an 8 A100 40G machine. The number of beams for beam
search is set to 20. The hyper-parameter configuration for
generative product search training can be found in Table 11.
In the downstream document retrieval task, for generative
retrieval methods with semantic IDs (rq-V AE indexer, hier-
archical clustering indexer, and LMI NDEXER ), we serve the
query as the input text into the semantic indexer and ask the
model to generate the ID for the relevant documents. Fol-
lowing (Wang et al., 2022), we use docT5query (Nogueira
12



Source: data\tc16_2312.10997v5\referenced_papers\[39]_2305.05065.pdf (Page 2):

(a) Semantic ID generation for items using
quantization of content embeddings.
Bidirectional Transformer Encoder
User_5
t_u5
Item 515
Sem. ID = (5, 25, 78)
Item 233
Sem. ID = (5, 23, 55)
t_23 t_55t_5Tokens
Transformer Decoder
<BOS>
Item 64
Sem. ID = (5, 25, 55)
Item Interaction History of User 5
t_25 t_78t_5
Encoded 
Context
t_25t_5
t_5
<EOS>t_55
t_25 t_55
Next Item
Neurips Version
(b) Transformer based encoder-decoder setup for building the
sequence-to-sequence model used for generative retrieval.
Figure 2: An overview of the modeling approach used in TIGER.
AttRec [41] proposed by Zhang et al. used self-attention mechanism to model the user’s intent
in the current session, and personalization is ensured by modeling user-item affinity with metric
learning. Concurrently, Kang et al. also proposed SASRec [17], which used self-attention similar to
decoder-only transformer models. Inspired by the success of masked language modeling in language
tasks, BERT4Rec [32] and Transformers4Rec [6] utilize transformer models with masking strate-
gies for sequential recommendation tasks. S3-Rec [44] goes beyond just masking by pre-training
on four self-supervised tasks to improve data representation. The models described above learn a
high-dimensional embedding for each item and perform an ANN in a Maximum Inner Product Search
(MIPS) space to predict the next item. In contrast, our proposed technique, TIGER, uses Generative
Retrieval to directly predict the Semantic ID of the next item.
P5 [8] fine-tunes a pre-trained large language models for multi-task recommender systems. The P5
model relies on the LLM tokenizer (SentencePiece tokenizer [29]) to generate tokens from randomly-
assigned item IDs. Whereas, we use Semantic ID representation of items thay are learned based on the
content information of the items. In our experiments (Table 2), we demonstrate that recommendation
systems based on Semantic ID representation of items yield much better results than using random
codes.
Semantic IDs. Hou et al.proposed VQ-Rec [12] to generate “codes” (analogous to Semantic IDs)
using content information for item representation. However, their focus is on building transferable
recommender systems, and do not use the codes in a generative manner for retrieval. While they
also use product quantization [15] to generate the codes, we use RQ-V AE to generate Semantic IDs,
which leads to hierarchical representation of items (Section 4.2). In a concurrent work to us, Singh et
al. [31] show that hierarchical Semantic IDs can be used to replace item IDs for ranking models in
large scale recommender systems improves model generalization.
Generative Retrieval. While techniques for learning search indices have been proposed in the past
[20], generative retrieval is a recently developed approach for document retrieval, where the task is to
return a set of relevant documents from a database. Some examples include GENRE [5], DSI [34],
NCI [37], and CGR [22]. A more detailed coverage of the related work is in Appendix A. To the best
of our knowledge, we are the first to propose generative retrieval for recommendation systems using
Semantic ID representation of items.
3 Proposed Framework
Our proposed framework consists of two stages:
1. Semantic ID generation using content features.This involves encoding the item content features
to embedding vectors and quantizing the embedding into a tuple of semantic codewords. The
resulting tuple of codewords is referred to as the item’s Semantic ID.
2. Training a generative recommender system on Semantic IDs.A Transformer model is trained on
the sequential recommendation task using sequences of Semantic IDs.
3



### Claim 57/179

#### Claim Text
In addition to aligning with human preferences, it is also possible to align with the preferences of fine-tuned models and retrievers [79].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[42]_2208.03299.pdf (Page 3):

2.2 Training objectives for the retriever
In this section, we discuss four diﬀerent loss functions to train the retriever jointly with the language model.
We consider loss functions that leverage the language model to providesupervisory signalto train the retriever.
In other words, if the language model ﬁnds a document useful when generating the output, the retriever
objective should encourage the retriever to rank said document higher. This allows us to train models
using only query and output pairs from the task of interest, without relying on document annotations. For
example, in the case of fact checking, a model only requires pairs of claims and corresponding verdicts but no
documents containing the evidence to back up the verdict. In practice, we can apply this approach on any
task, including self-supervised pre-training. As shown in the experimental section, pre-training is critical for
obtaining models that exhibit few-shot learning abilities.
Attention Distillation (ADist). The ﬁrst loss that we consider is based on the attention scores of the
language model, and is heavily inspired by Izacard & Grave (2021). The main idea is that the cross-attention
scores between the input documents and the output, can be used as a proxy of the importance of each input
document when generating the output. In particular, Izacard & Grave (2021) showed that these scores can
be aggregated across attention heads, layers and tokens for a given document to obtain a single score for each
document. Then, these scores can be distilled into the retriever by minimizing the KL-divergence with the
probability distributionpretr over the top-K documents{dk}1,...,Kobtained from the retriever:
pretr (d |q) = exp(s(d,q)/θ)∑K
k=1 exp(s(dk,q)/θ)
, (1)
where sis the dot-product between the query and documents vectors andθis a temperature hyper-parameter.
In the original paper, it was proposed to use the pre-softmax scores from the decoder cross-attentions, and
average across heads, layers and tokens. Here, we propose an alternative which gives slightly stronger results,
which relies on the following observation. In the attention mechanism, as deﬁned by
y =
N∑
n=1
αnvn,
the contribution to the outputy of a particular tokenn cannot be evaluated from the attention scoreαn
alone, but should also take the norm of the valuevn into account. Hence, we use the quantityαn∥vn∥2 as the
measure of relevance for tokenn. Following Izacard & Grave (2021), we average these scores over all attention
heads, layers, and tokens to obtain a score for each document. We apply theSoftmax operator over the
resulting scores, to obtain a distributionpattn(dk) over the top-K retrieved documents. We then minimize
the KL-divergence betweenpattn(dk), and the distributionpretr from the retriever deﬁned in Equation 1:
KL(pattn ∥pretr) =
K∑
k=1
pattn(dk) log
(pattn(dk)
pretr(dk)
)
.
Here, this loss is only used to optimize the parameters of the retriever, and not the language model. When
using recent deep learning frameworks, this is achieved by applying aStopGradient operator onpattn.
End-to-end training of Multi-Document Reader and Retriever (EMDR2). Next, we consider the
method introduced by Sachan et al. (2021), which is inspired by the expectation-maximization algorithm,
treating retrieved documents as latent variables. Given a queryq, the corresponding outputa and the set
DK of top-K retrieved documents with the current retriever, the EMDR2 loss to train the retriever is
log
[K∑
k=1
plm(a |q,dk)pretr(dk |q)
]
,
where pretr is again the probability over the top-K documents obtained with the retriever, as deﬁned by
Equation 1. Again, only the parameters of the retriever are updated by applying aStopGradient operator
4



Source: data\tc16_2312.10997v5\referenced_papers\[27]_2310.01352.pdf (Page 2):

Published as a conference paper at ICLR 2024
2 M ETHOD
2.1 A RCHITECTURE
Language Model We focus on retrieval-augmenting pre-trained auto-regressive language mod-
els (Brown et al., 2020). In particular, we use L LAMA (Touvron et al., 2023a), a family of open-
sourced language models pre-trained on trillions of tokens.
Retriever We adopt a dual-encoder based retriever architecture, since it can be easily fine-tuned
and is efficient at the inference stage (Lewis et al., 2020; Izacard et al., 2022b; Shi et al., 2023b).
Given a corpus C and a query q, the document encoder maps each text chunk c ∈ Cto an embedding
Ed(c) and the query encoder maps q to an embedding Eq(q). The top- k relevant text chunks for q
are retrieved based on the query-document embedding similarity, which is often computed via dot
product:
s(q, c) =Eq(q) · Ed(c). (1)
We initialize the retriever using DRAGON + (Lin et al., 2023), a state-of-the-art dual-encoder model
trained with a contrastive learning objective and large-scale data augmentation.
Parallel In-Context Retrieval-Augmentation Following Shi et al. (2023b), for a given language
model prompt x, we retrieve the top- k relevant text chunks C′ ⊂ C, |C′| = k. To stay within the
context window size limit, each retrieved chunk is prepended to the prompt2, and the language model
predictions from multiple augmented prompts are computed in parallel. The final output probability
is a mixture of the probability from each augmented prompt weighted by the chunk relevance score
pLM (y|x, C′) =
X
c∈C′
pLM (y|c ◦ x) · pR(c|x), (2)
where ◦ denotes sequence concatenation, and pR(c|x) = exp s(x,c)P
c′∈C′ exp s(x,c′) are the retriever scores
re-normalized among top-k relevant chunks.
2.2 F INE -TUNING DATASETS
We choose a set of fine-tuning tasks aimed at boosting the language model’s ability to utilize knowl-
edge effectively and improving its contextual awareness in generating predictions. As shown in
Table 1, our language model fine-tuning datasets (DL) consists of 20 datasets across 5 distinct cat-
egories: dialogue, open-domain QA, reading comprehension3, summarization and chain-of-thought
reasoning. For retriever fine-tuning datasets DR, we opt for the QA datasets in our collection fea-
turing standalone questions, and we additionally include two QA datasets, FreebaseQA (Jiang et al.,
2019) and MS-MARCO (Nguyen et al., 2016). The examples of each dataset are serialized for in-
struction tuning using manually compiled templates (Table 10). For tasks in DL ∩ DR, we use the
same template for both fine-tuning steps. In addition, we observe that supplementing the instruction-
tuning data with unsupervised text leads to additional performance gains for both language model
and retriever fine-tuning, and we detail data mixture used in Appendix B.
2.3 R ETRIEVAL AUGMENTED LANGUAGE MODEL FINE -TUNING
To improve the language model’s ability to utilize retrieved information, we fine-tune it on the
selected datasets DL with in-context retrieval augmentation. Formally, we separate each fine-tuning
sequence into an instruction segment ( x) and an output segment ( y). For each example (xi, yi) ∈
2We use a pair of start (“Background:”) and end (“\n\n”) tokens to demarcate the retrieved segment in the
augmented prompt. The complete set of our instruction-tuning templates are shown in Appendix C.
3Our reading comprehension (RC) fine-tuning datasets include SQuAD 2.0 (Rajpurkar et al., 2018), which
trains the model to determine whether a question can be answered using a given passage, and to provide an
answer only when the passage is relevant (otherwise the response is set to “I don’t know”). As shown in
Appendix F, fine-tuning on this dataset promotes a desirable behavior: the instruction-tuned model tends to
respond with “I don’t know” when the retriever presents an incorrect passage. We leave further exploring this
behavior to improve answer generation as a future work.
3



Source: data\tc16_2312.10997v5\referenced_papers\[27]_2310.01352.pdf (Page 3):

Published as a conference paper at ICLR 2024
Table 1: Our intruction tuning datasets. All datasets are downloaded from Hugging Face (Lhoest
et al., 2021), with the exception of those marked with ‡, which are taken from Iyer et al. (2022).
Task HF identifier Dataset name DL DR #Train
Dialogue oasst1 OpenAssistant Conversations Dataset (K ¨opf et al., 2023)✓ ✓ 31,598
Open-Domain
QA
commonsenseqa CommonsenseQA (Talmor et al., 2019) ✓ ✓ 9,741
mathqa MathQA (Amini et al., 2019) ✓ ✓ 29,837
webquestions Web Questions (Berant et al., 2013) ✓ ✓ 3,778
wikiqa Wiki Question Answering (Yang et al., 2015) ✓ ✓ 20,360
yahooanswersqa Yahoo! Answers QA ✓ ✓ 87,362
freebaseqa FreebaseQA (Jiang et al., 2019) ✓ 20,358
msmarco* MS MARCO (Nguyen et al., 2016) ✓ 80,143
Reading Com-
prehension
coqa Conversational Question Answering (Reddy et al., 2019) ✓ 108,647
drop Discrete Reasoning Over Paragraphs (Dua et al., 2019) ✓ 77,400
narrativeqa NarrativeQA (Ko ˇcisk´y et al., 2018) ✓ 32,747
newsqa NewsQA (Trischler et al., 2017) ✓ 74,160
pubmedqa PubMedQA (Jin et al., 2019) ✓ ✓ 1,000
quail QA for Artificial Intelligence (Rogers et al., 2020) ✓ 10,246
quarel QuaRel (Tafjord et al., 2019) ✓ ✓ 1,941
squadv2 SQuAD v2 (Rajpurkar et al., 2018) ✓ 130,319
Summarization cnndailymail CNN / DailyMail (Hermann et al., 2015) ✓ 287,113
Chain-of-
thought
Reasoning
aquarat‡ Algebra QA with Rationales (Ling et al., 2017) ✓ 97,467
ecqa‡ Explanations for CommonsenseQ (Aggarwal et al., 2021)✓ 7,598
gsm8k‡ Grade School Math 8K (Cobbe et al., 2021) ✓ 7,473
compeitionmath‡ MATH (Hendrycks et al., 2021b) ✓ 7,500
strategyqa‡ StrategyQA (Geva et al., 2021) ✓ 2,290
* We only used the question-and-answer pairs in the MS MARCO dataset.
DL, we retrieve the top- ˜k relevant text chunks Ci ⊂ Cbased on xi. Mirroring the inference-time
handling, for each retrieved chunk cij ∈ Ci, we create a separate fine-tuning example by prepending
it to the instructions as a background field, resulting in ˜k independent fine-tuning instances per
original example: {(cij ◦ xi, yi)|j = 1. . .˜k}.4
We fine-tune the language model using the next-token prediction objective and minimize the loss
from tokens in the output segment of each instance (Iyer et al., 2022):
L(DL) =−
X
i
X
j
log pLM (yi|cij ◦ xi). (3)
Integrating in-context retrieval augmentation during fine-tuning gives a twofold benefit. First, it
adapts the LLM to better utilize relevant background knowledge to make a prediction. Secondly,
even state-of-the-art retrievers can falter and return inaccurate results. By training the LLM to make
correct predictions when a wrong retrieved chunk is given, we enable the LLM to ignore misleading
retrieval content and lean into its parametric knowledge in such cases. The efficacy of this fine-
tuning strategy is empirically demonstrated in §5.1.
2.4 R ETRIEVER FINE -TUNING
In addition to fine-tuning the language model with retrieval augmentation, we also fine-tune the
retriever to better align its output with the language model. In particular, we adopt a generalized
version of LSR ( LM-Supervised Retrieval, Shi et al., 2023b) training that leverages the language
model itself to provide supervision for retriever fine-tuning.
For a training sample (x, y) in the retriever fine-tuning dataset DR, we define the LSR score for a
retrieved chunk c as follows:
pLSR(c|x, y) = exp (pLM (y|c ◦ x)/τ)P
c′∈C exp (pLM (y|c′ ◦ x)/τ) ≈ exp (pLM (y|c ◦ x)/τ)P
c′∈C′ exp (pLM (y|c′ ◦ x)/τ), (4)
where τ is a temperature hyperparameter, and C′ ⊂ Cdenotes the top-k retrieved chunks for x. A
higher LSR score indicates that c is more effective at improving the language model’s chance of
4The exceptions are summarization tasks and RC tasks with context dependent questions (e.g. “when was
the writer born?”), where we do not perform retrieval and create the fine-tuning instances using the given
background text instead. For RC tasks with self-contained questions, we use the retrieved chunks in addition to
the given background text to create fine-tuning instances, resulting in ˜k + 1of them per original example.
4



Source: data\tc16_2312.10997v5\referenced_papers\[42]_2208.03299.pdf (Page 6):

updated, as the true top-k documents may not be retrieved in the top-L results from the stale index. In
practice, it is possible to track the positions of the top-K re-ranked documents in the top-L, and estimate
when the index needs to be updated.
Query-side ﬁne-tuning. Finally, the last strategy is to decouple the encoding of the queries and documents.
In this case, we ﬁx the parameters corresponding to the document encoder, and only train the parameters
corresponding to the query encoder. Thus, the embeddings of documents are ﬁxed, and we do not need to
refresh the index, and thus there is no computational overhead. As we will see in practice, the impact of
ﬁxing the documents encoder varies greatly for diﬀerent tasks when a large training dataset is available. For
most of the few-shot settings that we consider, query-side ﬁnetuning does not have large performance impact,
and sometimes even slightly improves performance.
3 Related work
3.1 Retrieval in natural language processing
Retrieval for knowledge intensive tasks.Previous work has shown that retrieval improves performance
across a variety of tasks such as question answering (Voorhees et al., 1999; Chen et al., 2017; Kwiatkowski et al.,
2019), fact checking (Thorne et al., 2018), dialogue (Dinan et al., 2019) or citation recommendation (Petroni
et al., 2022). Historically, this information retrieval step was implemented using term-matching methods, such
as TF-IDF or BM25 (Jones, 1972; Robertson et al., 1995). For open-domain question answering (Voorhees
et al., 1999), documents are often retrieved from Wikipedia (Chen et al., 2017). Recently, dense retrievers
based on neural networks have become popular. These usually follow a dual-encoder architecture (Yih et al.,
2011; Huang et al., 2013; Shen et al., 2014), where queries and passages are encoded independently as vectors,
and relevance is computed using the inner product or Euclidean distance. Popular supervised retrievers
include DPR (Karpukhin et al., 2020), which is trained to discriminate the relevant passage among negative
passages, and extensions such as ANCE (Xiong et al., 2020) which improved the hard negatives mining
process. We refer the reader to Yates et al. (2021) for a survey of dense retrieval techniques.
After retrieval, the relevant documents are processed to produce the ﬁnal output. In open-domain QA, models
can extract a span of text from retrieved documents as the answer (Chen et al., 2017; Clark & Gardner, 2018;
Wang et al., 2019; Karpukhin et al., 2020), a method inspired by reading comprehension (Richardson, 2013;
Rajpurkar et al., 2016). Recently, generating the answer as free-form text, using a seq2seq model conditioned
on retrieved documents have become prevalent (Lewis et al., 2020; Izacard & Grave, 2020; Min et al., 2020).
These architectures have also been shown to reduce hallucination in dialogue agents (Shuster et al., 2021).
Retriever training. The need for expensive query-document annotations for training the retriever can be
bypassed, by leveraging signals from the language model, or using unsupervised learning. REALM (Guu et al.,
2020) and RAG (Lewis et al., 2020) jointly train the retriever and language model by modelling documents as
latent variable, and minimizing the objective with gradient descent. REALM pre-trains end-to-end with an
MLM approach but uses an extractive BERT-style model (Devlin et al., 2019). Guu et al. (2020) also explore
a query-side ﬁnetuning at ﬁnetuning time to avoid index refreshes, which is also explored in the context of
phrase-based retrieval by Lee et al. (2021b). Izacard & Grave (2020) proposed to use cross-attention scores
as supervision with knowledge distillation. Sachan et al. (2021) perform joint training of the reader and the
retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee
et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention
scores from the language model. Theinverse cloze taskwas proposed by Lee et al. (2019) to pre-train dense
retrievers in an unsupervised way. Paranjape et al. (2021) propose a method to train retrieval-augmented
generators using a second “informed” retriever with access to the output, which the test-time retriever can be
distilled from, and Hofstätter et al. (2022) recently proposed a training set ﬁltering/weighting approach to
train stronger retrieval-augmented generators. Izacard et al. (2022) explored diﬀerent contrastive learning
methods to train retrievers, while Ram et al. (2022) used recurring spans within a document to create
pseudo-positive query-document pairs.
7



Source: data\tc16_2312.10997v5\referenced_papers\[27]_2310.01352.pdf (Page 7):

Published as a conference paper at ICLR 2024
shows that fine-tuning with “95% corpus data + 5% MTI data” achieves the best accuracy across all
models, outperforming the non-finetuned baseline by 0.6 points on average.10
Finally, we also compare jointly fine-tuning both the query and document encoders with only fine-
tuning the query encoder while freezing the document encoder. Table 5 shows this experiment
conducted using the corpus data, where freezing the document encoder produces significantly better
performance. As a result, we only fine-tune the query encoder in this work.
5.2 D UAL INSTRUCTION TUNING ABLATION
Table 6: The impact of LM and Retriever fine-tuning in our RA-DIT method, comparing the R E-
PLUG baseline, LM-ft only, R-ft only, and RA-DIT. 5-shot dev set performance is reported.
5-shot MMLU NQ TQA ELI5 HoPo FEV AIDA zsRE T-REx WoW Avg
LLAMA 65B + DRAGON+ 61.7 41.7 73.0 22.1 41.6 90.8 54.0 63.7 71.9 17.2 53.8
LLAMA 65B + FTed DRAGON+ 63.0 42.2 74.9 22.2 41.4 91.6 54.9 65.2 71.4 17.4 54.4
RIT 65B + DRAGON+ 64.8 42.8 73.1 23.6 41.2 92.1 53.5 62.3 69.0 16.6 53.9
RIT 65B + FTed DRAGON+ 64.3 43.8 75.0 23.3 42.0 92.3 52.8 65.2 70.1 17.3 54.6
We isolate the impact of the language model fine-tuning from retriever fine-tuning in our RA-DIT
method, and illustrate the benefit of each.11 According to Table 6, both LM-ft and R-ft are beneficial
when used alone, and outperform the R EPLUG using L LAMA 65B and the D RAGON + retriever.
On the other hand, the most gain can be achieved when combining LM-ft and R-ft in our RA-
DIT method, which outperforms the R EPLUG baseline by 0.8 points on average. In our prelimary
experiments, we also attempted iterative dual instruction tuning by fine-tuning the retriever using
LSR scores from the RA-IT LM or conduct the RA-IT step using passages returned by the fine-
tuned retriever, for one or two such iterations, but did not observe further gains. We leave the
exploration of multi-step RA-DIT to future work.
5.3 R ETRIEVER SETTINGS
Table 7: Retriever settings: We report 5-shot dev set performance using L LAMA 65B and various
retrievers in the REPLUG setting.
5-shot MMLU NQ TQA HoPo FEV AIDA zsRE T-REx WoW ELI5 Avg
LLAMA 65B 61.3 30.9 70.6 23.8 83.7 50.2 36.0 52.3 17.4 23.4 45.0
Retriever ablation usingLLAMA 65B and the 399M CC + Wiki corpus
Contriever 59.3 41.2 73.0 32.4 88.1 45.0 40.8 56.1 17.2 21.6 47.5
Contriever-msmarco 62.0 42.1 74.1 38.7 89.3 49.3 60.2 62.9 17.4 21.8 51.8
DRAGON+ 61.7 41.7 73.0 40.8 90.8 48.8 63.7 71.9 17.8 23.8 53.4
We study the impact of various retriever choices in our framework. We use LLAMA 65B as the lan-
guage model and combine it with different retrievers. Table 7 first compares D RAGON + (Lin et al.,
2023) with other state-of-the-art retrievers such as Contriever (Izacard et al., 2022a). All retrieval-
augmented models substantially improve over the L LAMA baseline, and D RAGON + significantly
outperforms both Contriever and Contriever-MSMARCO. We hence adopt D RAGON + as our base
retriever in all experiments.
6 R ELATED WORK
Retrieval-Augmented Language Models RALMs augment LMs with a non-parametric memory
to facilitate external knowledge access and provide provenance (Guu et al., 2020; Lewis et al., 2020;
10In early experiments, we also tested other mixtures and found that using 5% or 10% MTI data worked the
best. (They perform similarly to each other.)
11Minor performance differences may be observed for the L LAMA 65B + D RAGON + model in different
ablations due to the differences in few-shot example truncation in long prompts. We ensure all rows within
each table are comparable.
8



### Claim 58/179

#### Claim Text
A typical approach, such as RA-DIT [27], aligns the scoring functions between Retriever and Generator using KL divergence.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[27]_2310.01352.pdf (Page 1):

Published as a conference paper at ICLR 2024
Figure 1: The RA-DIT approach separately fine-tunes the LLM and the retriever. For a given exam-
ple, the LM-ft component updates the LLM to maximize the likelihood of the correct answer given
the retrieval-augmented instructions (§2.3); the R-ft component updates the retriever to minimize
the KL-Divergence between the retriever score distribution and the LLM preference (§2.4)
In this work, we show lightweight instruction tuning (Chung et al., 2022b; Iyer et al., 2022; Zhou
et al., 2023) alone can significantly boost the performance of RALMs, especially in knowledge
intensive scenarios. We propose Retrieval-Augmented Dual Instruction Tuning (RA-DIT), an ap-
proach that retrofits any LLM with retrieval capabilities via fine-tuning over a set of tasks selected
to cultivate knowledge utilization and contextual awareness in the language model predictions. We
initialize the framework using pre-trained LLAMA (Touvron et al., 2023a) and a state-of-the-art dual-
encoder based dense retriever, DRAGON + (Lin et al., 2023). Following Shi et al. (2023b), we retrieve
relevant text chunks based on the language model prompt. Each retrieved chunk is prepended to the
prompt, and the predictions from multiple chunks are computed in parallel and ensembled to pro-
duce the final output.
We perform instruction-tuning in two separate steps. For language model fine-tuning (LM-ft), we
adopt the label-loss objective (Chung et al., 2022b; Iyer et al., 2022) and augment each fine-tuning
prompt with a retrieved “background” field prepended to the instructions (Figure 1). We also lever-
age the design of existing NLP tasks and populate this field with the ground truth context for tasks
such as reading comprehension and summarization. By incorporating the background text during
fine-tuning, we guide the LLM to optimally utilize the retrieved information and ignore distract-
ing content (Shi et al., 2023a). For retriever fine-tuning (R-ft), we update the query encoder using
a generalized LM-Supervised Retrieval (LSR, Shi et al., 2023b) training objective computed over a
combination of supervised tasks and unsupervised text completion. This way we enable the retriever
to yield more contextually relevant results, aligned with the preferences of the LLM.
We demonstrate that each fine-tuning step offers significant performance gains, and that the fine-
tuned LLM and retriever can be combined to achieve further improvements. Our largest model,
RA-DIT 65B, attains state-of-the-art performance in zero- and few-shot settings on knowledge
intensive benchmarks, notably surpassing the un-tuned in-context RALM approach on datasets
including MMLU (Hendrycks et al., 2021a) (+8.2% 0-shot; +0.7% 5-shot) and Natural Ques-
tions (Kwiatkowski et al., 2019) (+22% 0-shot; +3.8% 5-shot). In addition, RA-DIT 65B also
substantially outperforms A TLAS 11B on 8 knowledge-intensive tasks (+7.2% on average in the
64-shot fine-tuning setting). This suggests that language models and retrievers, when optimized in-
dependently and then fused through instruction-tuning, can compete effectively with RALMs that
have undergone extensive continuous pre-training. We further conduct a comprehensive model anal-
ysis, showing the effectiveness of our approach across LLMs of varying sizes, as well as evaluating
the influence of different fine-tuning strategies and retriever configurations.1
1We release the scripts for indexing Common Crawl data and generating our fine-tuning and inference
prompts at: https://github.com/facebookresearch/RA-DIT.
2



Source: data\tc16_2312.10997v5\referenced_papers\[27]_2310.01352.pdf (Page 8):

Published as a conference paper at ICLR 2024
Borgeaud et al., 2022; Shi et al., 2023b). Previous work have proposed different ways of fusing
the LM and the non-parametric component. For example, RETRO (Borgeaud et al., 2022) and
FiD (Izacard & Grave, 2021; Hofst ¨atter et al., 2022) leverage separate encoder modules to encode
the retrieved content, which are integrated with the backbone LM via cross-attention. A more widely
adopted approach directly augments the LM input with the retrieved content (Guu et al., 2020; Lewis
et al., 2020; Shi et al., 2023b). This approach yields competitive results with a moderate inference
cost increase, as the LM can effectively contextualize the retrieved content and the original prompt
through multi-layer self-attention. RA-DIT is grounded in the in-context RA framework for its
simplicity and practicality. Instead of performing extensive pre-training (Guu et al., 2020; Borgeaud
et al., 2022; Izacard et al., 2022b), we propose a lightweight fine-tuning recipe that primarily utilizes
downstream data, and demonstrate improved few-shot generalization of the fine-tuned RALM on
knowledge-intensive language tasks.
Instruction Tuning Instruction tuning has been proposed to align pre-trained LLMs to follow nat-
ural language instructions and avoid extensive prompt engineering (Ouyang et al., 2022; Wei et al.,
2022; Chung et al., 2022a; Wang et al., 2022; Iyer et al., 2022). We propose retrieval-augmented
instruction tuning (RA-IT) as part of our dual instruction tuning framework to improve the LM’s
ability to leverage retrieved information. Concurrent work has also applied instruction tuning to
other RALM architectures. Notably, Wang et al. (2023) fine-tunes the backbone LM in the RETRO
architecture while freezing the cross-attention module and the memory encoder. In comparison,
RA-DIT fine-tunes both the LM and the retriever while decoupling the fine-tuning processes of
the two components.12 Asai et al. (2023) fine-tunes an LM to adaptively retrieve passages on de-
mand and reflect on the relevancy of the retrieved passages and its generation using special-token
markups. The most related work to ours is SAIL (Luo et al., 2023), an approach that fine-tunes
the LM with instructions augmented with retrieved content, and examines it on public instruction
following datasets (Taori et al., 2023; Chiang et al., 2023) using a moderately sized model (7B pa-
rameters). In comparison, RA-DIT conducts parallel retrieval-augmentation for multiple retrieved
passages while SAIL concatenates them in the LM context. Furthermore, RA-DIT adopts a holistic
view of the RALM architecture by employing a learnable neural retriever and proposing a dual opti-
mization framework. SAIL, in comparison, leans on non-differentiable retrievers such as BM25 and
focuses on improving the LM (e.g. it proposes an in-context retrieval selection technique to guide
the model focus towards informative content).
Information Retrieval Retrieval methods include sparse retrievers that does matching over a
sparse bag-of-words representation (Robertson & Zaragoza, 2009; Formal et al., 2021), dense re-
trievers that embed queries and documents into a fixed-size dense vector for nearest-neighbor
search (Karpukhin et al., 2020; Xiong et al., 2021), and multi-vector retrievers which uses multiple
vectors as the representation and more complex search algorithms for increased accuracy (Khat-
tab & Zaharia, 2020; Li et al., 2023). We adopt a state-of-the-art dense retriever, D RAGON (Lin
et al., 2023), as our base retriever, because of its simplicity, state-of-the-art accuracy, high retrieval
efficiency on GPUs, and the ease of further fine-tuning.
7 C ONCLUSION
In this paper, we propose RA-DIT, a lightweight Retrieval-Augmented Dual Instruction Tuning
framework that can effectively retrofit any pre-trained LLM with retrieval capabilities. RA-DIT
updates the LLM with retrieval-augmented instruction tuningto make better use of retrieved knowl-
edge and ignore irrelevant or distracting information. It also fine-tunes the retriever with supervi-
sion from the LLM to retrieve texts that can better help the LLM generate correct outputs. RA-
DIT achieves state-of-the-art performance in zero- and few-shot evaluations on knowledge intensive
benchmarks, surpassing un-tuned in-context RALM approaches such as R EPLUG and compete ef-
fectively against methods that require extensive pre-training such as ATLAS .
12Although the differences in the base LMs, fine-tuning datasets and inference settings make direct compar-
isons between the two models challenging, RA-DIT 65B compares favorably to InstructRetro 48B (Wang et al.,
2023) in zero-shot setting on the shared evaluation datasets.
9



Source: data\tc16_2312.10997v5\referenced_papers\[27]_2310.01352.pdf (Page 7):

Published as a conference paper at ICLR 2024
shows that fine-tuning with “95% corpus data + 5% MTI data” achieves the best accuracy across all
models, outperforming the non-finetuned baseline by 0.6 points on average.10
Finally, we also compare jointly fine-tuning both the query and document encoders with only fine-
tuning the query encoder while freezing the document encoder. Table 5 shows this experiment
conducted using the corpus data, where freezing the document encoder produces significantly better
performance. As a result, we only fine-tune the query encoder in this work.
5.2 D UAL INSTRUCTION TUNING ABLATION
Table 6: The impact of LM and Retriever fine-tuning in our RA-DIT method, comparing the R E-
PLUG baseline, LM-ft only, R-ft only, and RA-DIT. 5-shot dev set performance is reported.
5-shot MMLU NQ TQA ELI5 HoPo FEV AIDA zsRE T-REx WoW Avg
LLAMA 65B + DRAGON+ 61.7 41.7 73.0 22.1 41.6 90.8 54.0 63.7 71.9 17.2 53.8
LLAMA 65B + FTed DRAGON+ 63.0 42.2 74.9 22.2 41.4 91.6 54.9 65.2 71.4 17.4 54.4
RIT 65B + DRAGON+ 64.8 42.8 73.1 23.6 41.2 92.1 53.5 62.3 69.0 16.6 53.9
RIT 65B + FTed DRAGON+ 64.3 43.8 75.0 23.3 42.0 92.3 52.8 65.2 70.1 17.3 54.6
We isolate the impact of the language model fine-tuning from retriever fine-tuning in our RA-DIT
method, and illustrate the benefit of each.11 According to Table 6, both LM-ft and R-ft are beneficial
when used alone, and outperform the R EPLUG using L LAMA 65B and the D RAGON + retriever.
On the other hand, the most gain can be achieved when combining LM-ft and R-ft in our RA-
DIT method, which outperforms the R EPLUG baseline by 0.8 points on average. In our prelimary
experiments, we also attempted iterative dual instruction tuning by fine-tuning the retriever using
LSR scores from the RA-IT LM or conduct the RA-IT step using passages returned by the fine-
tuned retriever, for one or two such iterations, but did not observe further gains. We leave the
exploration of multi-step RA-DIT to future work.
5.3 R ETRIEVER SETTINGS
Table 7: Retriever settings: We report 5-shot dev set performance using L LAMA 65B and various
retrievers in the REPLUG setting.
5-shot MMLU NQ TQA HoPo FEV AIDA zsRE T-REx WoW ELI5 Avg
LLAMA 65B 61.3 30.9 70.6 23.8 83.7 50.2 36.0 52.3 17.4 23.4 45.0
Retriever ablation usingLLAMA 65B and the 399M CC + Wiki corpus
Contriever 59.3 41.2 73.0 32.4 88.1 45.0 40.8 56.1 17.2 21.6 47.5
Contriever-msmarco 62.0 42.1 74.1 38.7 89.3 49.3 60.2 62.9 17.4 21.8 51.8
DRAGON+ 61.7 41.7 73.0 40.8 90.8 48.8 63.7 71.9 17.8 23.8 53.4
We study the impact of various retriever choices in our framework. We use LLAMA 65B as the lan-
guage model and combine it with different retrievers. Table 7 first compares D RAGON + (Lin et al.,
2023) with other state-of-the-art retrievers such as Contriever (Izacard et al., 2022a). All retrieval-
augmented models substantially improve over the L LAMA baseline, and D RAGON + significantly
outperforms both Contriever and Contriever-MSMARCO. We hence adopt D RAGON + as our base
retriever in all experiments.
6 R ELATED WORK
Retrieval-Augmented Language Models RALMs augment LMs with a non-parametric memory
to facilitate external knowledge access and provide provenance (Guu et al., 2020; Lewis et al., 2020;
10In early experiments, we also tested other mixtures and found that using 5% or 10% MTI data worked the
best. (They perform similarly to each other.)
11Minor performance differences may be observed for the L LAMA 65B + D RAGON + model in different
ablations due to the differences in few-shot example truncation in long prompts. We ensure all rows within
each table are comparable.
8



Source: data\tc16_2312.10997v5\referenced_papers\[26]_2401.06954.pdf (Page 2):

Figure 2: Differing from previous RAGs that update
LLMs, retrievers, or both, BGM connects a frozen LLM
and a frozen retriever through a lightweight bridge
model which adapts the retrieved information to the
LLM’s preference. This makes BGM applicable to
“large” LMs and off-the-shelf retrievers.
from various knowledge sources is proven effective
across numerous NLP tasks, including language
modeling (Borgeaud et al., 2022; Khandelwal et al.,
2020; Shi et al., 2023b), question answering (Lewis
et al., 2020; Izacard et al., 2022; de Jong et al.,
2023; De Jong et al., 2023; Shi et al., 2023b; Guu
et al., 2020; Izacard and Grave, 2020; Xu et al.,
2023), fact versification (Lewis et al., 2020) and
text generation (Lewis et al., 2020). Specifically,
RAG utilizes input as a query and comprises two
main components: (1) a retriever retrieves a set
of items from a side corpus. Particular items may
vary across different tasks, including documents,
passages, or even tokens. In this study, we focus on
retrieving passages; and (2) a LLM incorporates
the retrieved items, as additional information in the
input context, and makes final predictions.
A fundamental question in this process arises
regarding the disparate preferences between LLMs
and retrievers, as LLMs performing optimally only
when their preferences are satisfied. Bridging the
preference gap is crucial. Depending on which
components are subject to updates, this challenge
can be categorized into three families.
Finetuning retrievers and LLMs jointly. This
is the most widely used setting of RAG (Izacard
et al., 2022; Khandelwal et al., 2020; Wu et al.,
2022; Guu et al., 2020; Lewis et al., 2020). How-
ever, most prior work is based on relative small
LMs (< 1B). For example, Altas (Izacard et al.,
2022) finetunes LLM (T5 (Raffel et al., 2020a))
and retriever (Contriever (Izacard et al., 2021))
jointly by leveraging the LLM to provide super-
visory signal to train the retriever. RAG (Lewis
et al., 2020) uses a tunnable query encoder and
DPR (Karpukhin et al., 2020) as retriever, BART
as LLM, and design an end-to-end schema to train
the query encoder and the LLM.
Finetuning LLMs only. Updating retrievers is
not always desirable as it is costly and requires
the document index to be periodically updated. To
bridge the preference gap, it is also possible to only
update the LLMs. FiD (Izacard and Grave, 2020)
takes the retrieved documents and query as input,
finetunes the LLM to adapt to the external infor-
mation. Similarly, Lummen (De Jong et al., 2023)
and Glimmer (de Jong et al., 2023) improve FiD
via adding reranker and pre-encoding memeory.
Finetuning retrievers only. Although above
systems have shown improves results, they are not
always applicable in practice. Many LLMs (espe-
cially larger ones) are only available as black-box
APIs. Given this constraint, a natural approach
is to only update the retrievers to ensure they can
retrieve passages that are more compatible with
LLMs. REPLUG (Shi et al., 2023b) adapts a sim-
ilar idea as Atlas but fix the LM. RECOMP (Xu
et al., 2023) trains a compressors to summarize
the retrieved document from retriever. However,
this family of models is incapable of performing
any sample-level selection and can only choose top
passages by setting a fixed threshold.
Unlike the existing approaches, BGM does not
fine-tune the LLM or the retriever and instead
employs a bridge model in between (Fig. 2).
RL for information retrieval. Before the LLM
era, RL has been used in information retrieval (IR)
(Xia et al., 2017; Wei et al., 2017; Zeng et al., 2018;
Xu et al., 2020). The core approach was to frame
the IR problem as a Markov Decision Process and
apply an RL algorithm to solve it. Typically, an
IR task would be structured to determine which
document to select for a ranking position, using
ranking metrics such as DCG as the reward. None
of these existing studies explore the application of
RL in the context of RAG.
In the LLM era, RL has been used in query
rewriting for retrieval (Wu et al., 2021; Nogueira
and Cho, 2017; Adolphs et al., 2021), where a
black-box retriever is assumed. This is a different
problem from RAG. Bacciu et al. (2023) suggest us-
ing RL to fine-tune retriever in the RAG context in
their opinion paper, not supported by experiments.
Their work does not recognize the importance of
bridging the gap between retrievers and LLMs, nor



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 18):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:19
QA 1-document QA 2-document QA 3-document hallucination
modification
0
10
20
30
40
50
60MRR
49.8
42.3
38.5
49.349.1
41.3
37.3
50.452.5
44.2
41.2
49.3
MRR Values for Different Tasks and Retrievers
Dense BM25 Hybrid + Rerank
Fig. 7. Comparison of Mean Reciprocal Rank (MRR) scores for different retrieval methods in our benchmark.
coherence of LLMs for long texts. We will show how the chunk overlap rate affects the system
performance for different tasks in Table 4.
Text Continuation: With an increase in chunk overlap, we observe a slight enhancement in
the metrics that evaluate the alignment of generated text with a reference answer (bleu, rouge-
L, and bertScore). The RAGQuestEval metric, which evaluates the accuracy and completeness
of important information, improves more obviously. These results indicate that a greater chunk
overlap is beneficial for preserving the flow of ideas in the text, which is essential for tasks that
require generating new, creative content.
Open-Domain Multi-Document Summarization: During summarization tasks, all evaluation
metrics show a slight improvement as chunk overlap grows. Interestingly, despite assumptions
that more overlap might reduce the variety of context information available, this does not result in
a lower rate of recalling important information. In fact, the best performance in terms of recall
occurs at a chunk overlap of 70%. This could mean that a larger overlap allows the model to focus
more on the main points and ignore less relevant or redundant information.
Question Answering: In question answering tasks, chunk overlap has minimal impact on overall
semantic similarity metrics such as bleu, rouge-L, and bertScore. However, it significantly affects
the accuracy and recall metrics for key information. The results indicate that as chunk overlap
increases, the accuracy and recall of key information in single-document question answering tasks
improve substantially. Similar improvements are observed in two-document question answering
tasks. However, for three-document question answering tasks, the improvement is less pronounced.
This may be because three-document question answering tasks require richer context, and larger
chunk overlaps may reduce the available context.
Hallucination Modification: Changes in chunk overlap have a minimal effect on the perfor-
mance metrics for tasks that involve correcting hallucinations. This is likely due to the errors in
these tasks typically being specific to individual entities or words, making the consistency of the
chunks less impactful.
4.4 Analyzing the Impact of Retriever on RAG Performance in Different Tasks
A retriever is a key component of the RAG pipeline, which finds relevant documents from a large
database based on the user input, and provides contextual information for the large model. There
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



### Claim 59/179

#### Claim Text
A UGMENTATION PROCESS IN RAG In the domain of RAG, the standard practice often involves a singular (once) retrieval step followed by generation, which can lead to inefficiencies and sometimes is typically insufficient for complex problems demanding multi-step reasoning, as it provides a limited scope of information [105].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[73]_2311.04177.pdf (Page 1):

Generation), a system that learns from its successes
without incurring high training costs. ARM-RAG
retains the detailed reasoning steps it took when it
successfully answered problems. We demonstrate
that the retrieval of these reasoning chains, known
as "Rationales", improves the performance of sub-
sequent problem-solving attempts.
2 Prior Literature
In this section, we review the literature on three top-
ics pertinent to the current work. First, we examine
the recent trend of LLMs in tackling increasingly
complex problems. Next, we look at the emergence
of Retrieval Augmented Generation (RAG) and its
impact on intelligent systems. Finally, we explore
the literature on the use of chain-of-thought rea-
soning patterns to enhance LLM performance in
problem-solving tasks.
2.1 LLMs are trending toward AGI
(Bubeck et al., 2023) claim that modern LLMs
exhibit "sparks of AGI (Artificial General Intel-
ligence)". In their paper, they extensively study
GPT-4 and argue that it is significantly more pow-
erful than the previous releases of GPT, even ap-
proaching human intelligence for some tasks. For
instance, they compare GPT-3.5 versus GPT-4 on
a math problem depicted in Figure 1. The answer
produced by GPT-3.5, shown in Figure 2 is non-
sensical, while the answer produced by GPT-4, pre-
sented in Figure 3, is mostly coherent.
Within the duration of one year, a rabbit population first multi-
plies itself by a factora and on the last day of the yearb rabbits
are taken by humans for adoption. Supposing that on first day
of the first year there are x rabbits, we know that exactly 3
years afterwards there will be a population of27x−26 rabbits.
What are the values of a and b?
Figure 1: Example prompt.
This and other examples provide compelling ev-
idence that LLMs are rapidly advancing in intelli-
gence and, in some instances, can solve problems
as well or better than humans. While this is a
positive development, as mentioned in Section 1,
even the most advanced LLMs are limited by the
inherent fact that they possess only the memory
with which they were initially equipped and do not
improve over time without costly retraining.
2.2 Retrieval Augmented Generation
(Lewis et al., 2021) describe Retrieval Augmented
Let n be the total number of rabbits at the end of the first year.
Then,
n = ax
At the end of the third year, there will be 27x − 26 rabbits.
Therefore,
(ax − b)(a2x − b)(a3x − b) = 27x − 26
Expanding the left side of the equation, we get
a6x3−(a3+a2+a+1)bx2+(a2+a+1)b2x−b3 = 27x−26
Figure 2: GPT-3.5 Answer
Let’s call the rabbit population after one year y. We know
that y = ax − b. After two years, the population will be
ay − b = a(ax − b) − b = a2x − ab − b. After three years,
the population will bea(a2x−ab−b)−b = a3x−a2b−ab−b.
We are given that this is equal to 27x − 26, so we have the
equation:
a3x − a2b − ab − b = 27x − 26
We can rewrite this equation as
a3x − 27x = a2b + ab + b − 26
Now we can factor out an x on the left side and a b on the
right side:
x(a3 − 27) =b(a2 + a + 1− 26/b)
We can see that a = 3 is a solution for the left side of the
equation, so we can substitute that in:
x(0) =b(9 + 3 + 1− 26/b)
This simplified to:
0 = 13b − 26
Solving for b, we find that b = 2. So a = 3and b = 2are the
values we are looking for.
Figure 3: GPT-4 Answer
Generation (RAG), which combines a retrieval
model with a language model. The retrieval model,
referred to as the retriever or non-parametric mem-
ory, is an information retrieval system. The lan-
guage model is known as the generator or paramet-
ric memory. The retrieval model is an Information
Retrieval (IR) model. In this paper, the generator
is based on FAISS (Meta, 2017) dense representa-
tions and employs Maximum Inner Product Search
(MIPS) to select the top-k documents from the non-
parametric memory. The generator is BART (Lewis
et al., 2019), a pre-trained seq2seq transformer with
400 million parameters, trained on a diverse set of
generation tasks.
Generally, tasks utilizing the RAG system pro-



Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 1):

output shows what might be needed to fulfill the
task, and thus can serve as an informative context to
retrieve more relevant knowledge, i.e., generation-
augmented retrieval. The newly retrieved knowl-
edge can benefit another iteration of retrieval-
augmented generation. We can also leverage model
generations to adapt retrieval, by distilling knowl-
edge from a re-ranker with access to model genera-
tions to a dense retriever with access to task inputs
only, which may be beneficial in scenarios where
user inputs can be easily collected, but relevant
knowledge or desirable outputs are not annotated.
We evaluate our method on three tasks, includ-
ing multi-hop question answering, fact verification,
and commonsense reasoning. Our method prompts
an LLM to produce a chain of reasoning steps fol-
lowed by the final answer under a few-shot set-
ting. For in-context demonstrations, we focus on
problem-solving and follow Wei et al. (2022) to
annotate chains of thoughts, without explicitly con-
sidering how generation-augmented retrieval might
be affected, which makes it conceptually simple
and easy to implement. Our method achieves up
to 8.6% absolute gains over previous state-of-the-
art retrieval-augmented methods on four out of six
datasets while being competitive on the remaining
two. According to our experiments, generation gen-
erally benefits from more iterations, with two itera-
tions giving the most performance gains. One may
customize the performance-cost tradeoffs by choos-
ing an appropriate number of iterations. We can
further improve performance and also reduce itera-
tions via the aforementioned generation-augmented
retrieval adaptation.
We summarize our findings as follows:
• Automatic metrics such as exact match can
significantly underestimate the performance
of LLMs in question answering tasks. More-
over, improvements in exact match do not
always reflect improvements in generations.
Evaluation using LLMs may be more reliable.
• ITER -RETGEN is superior to or competi-
tive with state-of-the-art retrieval-augmented
methods, while being simpler and causing
fewer overheads of retrieval and generation.
With generation-augmented retrieval adapta-
tion, we can further improve performance
and also reduce overheads (by reducing itera-
tions).
• It is desirable for an LLM to leverage both
parametric knowledge and non-parametric
knowledge effectively. I TER -RETGEN con-
sistently outperforms Self-Ask on question
answering tasks, regardless of whether in-
context non-parametric knowledge mentions
the answers or not.
2 Related Work
In recent months, there has been a surge in LLM-
powered applications, such as ChatGPT, Bing Chat,
and CoPilot (Chen et al., 2021). While showing an
unprecedented level of performance, LLMs are sub-
ject to the following limitations: (1) Due to a high
demand for compute and data, it remains an open
research question to continually update LLMs both
efficiently and effectively (Scialom et al., 2022);
(2) LLMs also tend to hallucinate (OpenAI, 2023),
i.e., generating plausible but non-factual texts. To
alleviate these issues, there is a growing trend of
augmenting LLMs with tools (Mialon et al., 2023;
Gou et al., 2023), e.g., a code interpreter (Gao
et al., 2022b; Shao et al., 2023) or a search engine
(Nakano et al., 2021), in an attempt to offload sub-
tasks to more qualified experts, or to enrich the
input context for LLMs by providing more relevant
information.
Retrieval augmentation is a mainstream direc-
tion to connect LLMs to the external world. Previ-
ous retrieval-augmented LMs (Izacard and Grave,
2021; Shao and Huang, 2022) typically receive re-
trieved knowledge in a passive way: knowledge
is retrieved based on the task inputs without LMs’
intervention. As it is difficult for a retriever to cap-
ture relevance, especially in the zero-shot setting,
recent work shows a shift towards having LLMs
actively involved in retrieval to improve relevance
modeling, e.g., to provide a specific context for
retrieval with model generations (e.g., generated
search queries (Nakano et al., 2021; Press et al.,
2022; Yao et al., 2022), partial generation (Trivedi
et al., 2022a), or forward-looking sentences (Jiang
et al., 2023)). Khattab et al. (2022) proposed a
DSP programming framework that supports vari-
ous retrieval-augmented methods.
Recent work interleaves retrieval with generation
when completing a single output. Such a structured
workflow may reduce the flexibility in generation
(Yao et al., 2022). ITER -RETGEN avoids interrupt-
ing generation with retrieval, but iterates retrieval
and generation, i.e., to leverage the complete gen-
eration from the previous iteration to retrieve more



Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 2):

Question:
Retrieval:
Retrieval -Augmented Generation: 
What is the height of the player who won the 2015 AFL 
Rising Star award?
Title: 2015 AFL Rising Star Context: The NAB AFL Rising 
Star award is given annually to a stand out young player in 
the Australian Football League. The award was won by Jesse 
Hogan of Melbourne
The 2015 AFL Rising Star award was won by Jesse Hogan of 
Melbourne. Jesse Hogan is a professional Australian rules 
footballer. He is 198 cm tall. So the answer is 198 cm
Title: Jesse Hogan Context: Jesse Hogan … playing for the 
Melbourne Football Club. A key forward, Hogan is 1.95 m 
tall … made his AFL debut in the 2015 season and won the 
Ron Evans Medal as the AFL Rising Star
The 2015 AFL Rising Star award was won by Jesse Hogan of 
Melbourne. Jesse Hogan is 1.95 m tall.
So the answer is 1.95 m
Iteration 1 Iteration 2
Generation-Augmented Retrieval:
Retrieval -Augmented Generation: 
Question:
What is the height of the player who won the 2015 AFL 
Rising Star award?
Figure 1: ITER -RETGEN iterates retrieval and generation. In each iteration, I TER -RETGEN leverages the model
output from the previous iteration as a specific context to help retrieve more relevant knowledge, which may help
improve model generation (e.g., correcting the height of Hesse Hogan in this figure). We only show two iterations
in this figure for brevity. Solid arrows connect queries to the retrieved knowledge, and dashed arrows denote
retrieval-augmented generation.
relevant information which helps improve genera-
tion in the next iteration. ITER -RETGEN also has
the advantage of processing all retrieved knowl-
edge as a whole during the generation process, and
is conceptually simpler and easier-to-implement,
while being empirically strong in multi-hop ques-
tion answering, fact verification, and commonsense
reasoning.
A closely related work called GAR (Mao et al.,
2021) augments queries with generated background
information. HyDE (Gao et al., 2022a) also shares
a similar spirit, but focuses on zero-shot informa-
tion retrieval, and proposes to first prompt an LLM
to produce “hypothetical” paragraphs that cover the
information needed to answer a given question, and
then use the generated paragraphs to retrieve the
real ones. RepoCoder (Zhang et al., 2023) focuses
on repository-level code completion, and proposes
a 2-iteration retrieval-generation paradigm where
the second iteration leverages the intermediate code
completion for retrieval. By contrast, we propose
to synergize retrieval and generation with I TER -
RETGEN on various natural language tasks, and
explore how we can further adapt retrieval with
model generations.
3 Iterative Retrieval-Generation Synergy
3.1 Overview
Given a question q and a retrieval corpus D =
{d} where d is a paragraph, ITER -RETGEN repeats
retrieval-generation for T iterations; in iteration
t, we (1) leverage the generation yt−1 from the
previous iteration, concatenated with q, to retrieve
top-k paragraphs, and then (2) prompt an LLM M
to produce an output yt, with both the retrieved
paragraphs (denoted as Dyt−1||q) and q integrated
into the prompt. Therefore, each iteration can be
formulated as follows:
yt = M(yt|prompt(Dyt−1||q, q)), ∀1 ≤ t ≤ T (1)
The last output yT will be produced as the final
response.
3.2 Generation-Augmented Retrieval
There are many natural language tasks with com-
plex information needs. For example, in open-
domain multi-hop question answering, specific in-
formation needs may manifest themselves only
after correctly answering some prerequisite sub-
questions. In other words, there may exist semantic
gaps between the original question q and its sup-
porting knowledge, which can not be effectively
addressed by a retriever with a representation bot-
tleneck. In the first iteration, we can retrieve knowl-
edge with only the question q. In later iterations,
the LLM output from the previous iteration, though
having no guarantee of correctness, shows what
might be needed to answer the question, and thus
can be leveraged to bridge the semantic gaps; with
improved retrieval, an LLM can potentially pro-
duce a better output.
3.3 Retrieval-Augmented Generation
In each iteration, we generate an output using
Chain-of-Thought prompting except that we also
prepend retrieved knowledge to the question q.
Though there may exist more advanced prompting



Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 0):

Enhancing Retrieval-Augmented Large Language Models with Iterative
Retrieval-Generation Synergy
Zhihong Shao1, Yeyun Gong2, yelong shen3, Minlie Huang1∗, Nan Duan2, Weizhu Chen3
1 The CoAI Group, DCST, Institute for Artificial Intelligence,
1 State Key Lab of Intelligent Technology and Systems,
1 Beijing National Research Center for Information Science and Technology,
1 Tsinghua University, Beijing 100084, China
2 Microsoft Research Asia 3 Microsoft Azure AI
szh19@mails.tsinghua.edu.cn aihuang@tsinghua.edu.cn
Abstract
Retrieval-augmented generation has raise exten-
sive attention as it is promising to address the
limitations of large language models including
outdated knowledge and hallucinations. How-
ever, retrievers struggle to capture relevance,
especially for queries with complex informa-
tion needs. Recent work has proposed to im-
prove relevance modeling by having large lan-
guage models actively involved in retrieval, i.e.,
to guide retrieval with generation. In this pa-
per, we show that strong performance can be
achieved by a method we call ITER -RETGEN,
which synergizes retrieval and generation in an
iterative manner: a model’s response to a task
input shows what might be needed to finish
the task, and thus can serve as an informative
context for retrieving more relevant knowledge
which in turn helps generate a better response
in another iteration. Compared with recent
work which interleaves retrieval with gener-
ation when completing a single output, I TER -
RETGEN processes all retrieved knowledge as
a whole and largely preserves the flexibility in
generation without structural constraints. We
evaluate ITER -RETGEN on multi-hop question
answering, fact verification, and commonsense
reasoning, and show that it can flexibly lever-
age parametric knowledge and non-parametric
knowledge, and is superior to or competitive
with state-of-the-art retrieval-augmented base-
lines while causing fewer overheads of retrieval
and generation. We can further improve per-
formance via generation-augmented retrieval
adaptation.
1 Introduction
Generative Large Language Models (LLMs)
have powered numerous applications, with well-
perceived utility. Despite being powerful, LLMs
lack knowledge that is under-represented in their
training data, and are prone to hallucinations, es-
pecially in open-domain settings (OpenAI, 2023).
∗*Corresponding author: Minlie Huang.
Retrieval-augmented LLMs, therefore, have raised
widespread attention as LLM outputs can be poten-
tially grounded on external knowledge.
Previous retrieval-augmented LMs (Izacard
et al., 2022b; Shi et al., 2023) typically adopted
one-time retrieval, i.e., to retrieve knowledge us-
ing only the task input (e.g., a user question for
open-domain question answering). One-time re-
trieval should suffice to fulfill the information needs
if they are clearly stated in the original input,
which is applicable to factoid question answering
(Kwiatkowski et al., 2019) and single-hop fact ver-
ification (Thorne et al., 2018), but not to tasks with
complex information needs, e.g., multi-hop rea-
soning (Yang et al., 2018) and long-form question
answering (Fan et al., 2019).
To fulfill complex information needs, recent
work proposes to gather required knowledge multi-
ple times throughout the generation process, using
partial generation (Trivedi et al., 2022a; Press et al.,
2022)) or forward-looking sentence(s) (Jiang et al.,
2023) as search queries. However, such structured
workflows of interleaving retrieval with generation
have the following limitations: (1) as intermediate
generation is conditioned on knowledge retrieved
before, with no awareness of knowledge retrieved
afterwards, they fail to process all retrieved knowl-
edge as a whole during the generation process; (2)
they require multi-round retrieval to gather a com-
prehensive set of knowledge, and may frequently
change the prompts by updating newly retrieved
knowledge, thus increasing the overheads of both
retrieval and generation.
In this paper, we find it simple but effective to
enhance retrieval-augmented LLMs through itera-
tive retrieval-generation synergy (ITER -RETGEN,
Fig 1). ITER -RETGEN iterates retrieval-augmented
generation and generation-augmented retrieval:
Retrieval-augmented generation outputs a response
to a task input based on all retrieved knowledge
(initially using the task input as the query). This
arXiv:2305.15294v2  [cs.CL]  23 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[59]_2310.05149.pdf (Page 1):

2. ITERATIVE RETRIEV AL-GENERATION
SYNERGY
In this section, we first introduce the overall framework, and
then introduce the retrieval-generation collaboration frame-
work in detail, including generation augmented retrieval and
retrieval augmented generation.
2.1. Overview
We show the framework of ITRG in Figure 2. Given a user
question q and a document corpus D = {di}|D|
i=1 (i.e, di is a
Wikipedia paragraph.), ITRG repeats generation augmented
retrieval (GAR) and retrieval augmented generation (RAG)
for T iterations. In the GAR process of iteration t, we con-
catenate the output yt−1 of the last iteration and question q to
form a new query, and then use a dense retriever to retrieve
top-k paragraphs. In the first iteration, we only use the ques-
tion as the query. In the RAG process of iteration t, based on
the question q and the retrieved top-k paragraphs, we exploit
large language models to generate new paragraphs to answer
questions. Specifically, we propose two methods to generate
new paragraphs, which will be introduced in detail in §2.3.
2.2. Generation Augmented Retrieval
Knowledge-intensive tasks (e.g., open-domain question an-
swering) often require access to additional documents. A
common approach is to directly employ the question as the
query, and then equip a sparse or dense retriever to retrieve
relevant documents. In practice, we find that in some cases
using the question directly as the query fails to retrieve rel-
evant documents because there may exist semantic gaps be-
tween them. To alleviate this problem, we propose a simple
query expansion method. At the first iteration (t = 1), we use
the original question q as the query. At iteration t (t >1), we
concatenate the original question q and the document gener-
ated yt−1 in the last iteration as the new query qt = [q; yt−1].
Then, we utilize a pre-trained dense retriever to retrieve top-k
documents, which are denoted as Rt = {d}.
Given an input question q, the retriever aims to retrieve
a small set of documents from a corpus D = {di}|D|
i=1 that
are relevant to q. Following prior work [14], we use a dense
retriever based on the dual encoder architecture, where an en-
coder is used to encode both the input contextq and the docu-
ment d. Specifically, the encoder maps each document d ∈ D
to an embedding E(d) by taking the mean pooling of the last
hidden representation over the tokens in d. At query time,
the same encoder is applied to the input context q to obtain a
query embedding E(q). The similarity between the query em-
bedding and the document embedding is computed by their
cosine similarity: s(d, q) = cos(E(d), E(q)). The top-k doc-
uments that have the highest similarity scores are retrieved.
Question: What is the date of birth of Emilie HeghArntzen'smother?Retrieval:
Generation:
Retrieval:
Generation:
Retrieval:
Generation:
Iteration 1
Iteration 2
Iteration 3
infoboxname: Emilie HeghArntzen; caption: HeghArntzenin 2018 ; birth_date: January 1, 1994 ; birth_place: Skien, Norway ; nationality: Norwegian ; 
Emilie HeghArntzenwas born on January 1, 1994 in Skien, Norway. Her mother is unknown.
Camilla Marie Gjersemwas born together with a twin sister, Anne Line, on 6 January 1994 in Hønefoss, Norway. Their mother, PerlinaBangug, is a Filipina from Ilagan, Isabela, and their father, PetterGjersem, a Norwegian from Raufoss. Camilla Gjersemis a law student at the University of Oslo.
Hanne Hegh(born 19 January 1960) is a Norwegian handball player. She played 220 matches for the Norwegian national handball team between 1978 and 1992. She is the mother of Emilie HeghArntzen.
infoboxname: Hanne Hegh; caption: Hanne Hegh2008 ; nationality: Norwegian ; birth_date: April 27, 1960; birth_place: Oslo, Norway ; 
Hanne Heghwas born on April 27, 1960 in Oslo, Norway. She is the mother of Emilie HeghArntzen, who was born on January 1, 1994 in Skien, Norway.
Fig. 2: Iterative retrieval-generation synergy framework con-
tains two steps in each iteration: (1) generation augmented
retrieval (GAR): utilize the output of the previous iteration to
expand the query to help retrieve more relevant documents;
(2) retrieval augmented generation (RAG): utilize retrieved
documents to generate new documents to answer questions.
We only show three iterations in this figure for brevity. Solid
arrows indicate RAG within an iteration, and dashed arrows
indicate GAR between iterations. Purple represents correct
and useful information, and red represents wrong or invalid
information.
2.3. Retrieval Augmented Generation
Following previous work [13], for a given question q, we
could directly prompt large language models to generate re-
lated documents without retrieving them from an external cor-
pus. However, we find that if only the parametric knowledge
learned by the large model in the pre-training stage is used,
the generated documents may be incomplete. Retrieval aug-
mented generation (RAG) aims to comprehensively under-
stand the retrieved non-parametric knowledge and the para-
metric knowledge inside large language models to generate
more accurate factual knowledge. Specifically, we propose
two strategies, which will be described in detail below.
2.3.1. Refine
An intuitive idea is to refine the previously generated docu-
ment yt−1 based on the original question q and the retrieved
top-k documents at the current iteration step Rt to obtain a
new document yt. We call this method refine. Considering
that the document retrieved in the last iterationRt−1 has been
used to generate the last document yt−1, we refine the previ-
ous output yt−1 with updated documents Rupdate.
Rupdate = Rt − Rt−1, (1)
yt = M(prompt (yt−1, q, Rupdate)) , (2)



### Claim 60/179

#### Claim Text
ITERRETGEN [14] employs a synergistic approach that leverages “retrieval-enhanced generation” alongside “generationenhanced retrieval” for tasks that necessitate the reproduction of specific information.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 0):

Enhancing Retrieval-Augmented Large Language Models with Iterative
Retrieval-Generation Synergy
Zhihong Shao1, Yeyun Gong2, yelong shen3, Minlie Huang1∗, Nan Duan2, Weizhu Chen3
1 The CoAI Group, DCST, Institute for Artificial Intelligence,
1 State Key Lab of Intelligent Technology and Systems,
1 Beijing National Research Center for Information Science and Technology,
1 Tsinghua University, Beijing 100084, China
2 Microsoft Research Asia 3 Microsoft Azure AI
szh19@mails.tsinghua.edu.cn aihuang@tsinghua.edu.cn
Abstract
Retrieval-augmented generation has raise exten-
sive attention as it is promising to address the
limitations of large language models including
outdated knowledge and hallucinations. How-
ever, retrievers struggle to capture relevance,
especially for queries with complex informa-
tion needs. Recent work has proposed to im-
prove relevance modeling by having large lan-
guage models actively involved in retrieval, i.e.,
to guide retrieval with generation. In this pa-
per, we show that strong performance can be
achieved by a method we call ITER -RETGEN,
which synergizes retrieval and generation in an
iterative manner: a model’s response to a task
input shows what might be needed to finish
the task, and thus can serve as an informative
context for retrieving more relevant knowledge
which in turn helps generate a better response
in another iteration. Compared with recent
work which interleaves retrieval with gener-
ation when completing a single output, I TER -
RETGEN processes all retrieved knowledge as
a whole and largely preserves the flexibility in
generation without structural constraints. We
evaluate ITER -RETGEN on multi-hop question
answering, fact verification, and commonsense
reasoning, and show that it can flexibly lever-
age parametric knowledge and non-parametric
knowledge, and is superior to or competitive
with state-of-the-art retrieval-augmented base-
lines while causing fewer overheads of retrieval
and generation. We can further improve per-
formance via generation-augmented retrieval
adaptation.
1 Introduction
Generative Large Language Models (LLMs)
have powered numerous applications, with well-
perceived utility. Despite being powerful, LLMs
lack knowledge that is under-represented in their
training data, and are prone to hallucinations, es-
pecially in open-domain settings (OpenAI, 2023).
∗*Corresponding author: Minlie Huang.
Retrieval-augmented LLMs, therefore, have raised
widespread attention as LLM outputs can be poten-
tially grounded on external knowledge.
Previous retrieval-augmented LMs (Izacard
et al., 2022b; Shi et al., 2023) typically adopted
one-time retrieval, i.e., to retrieve knowledge us-
ing only the task input (e.g., a user question for
open-domain question answering). One-time re-
trieval should suffice to fulfill the information needs
if they are clearly stated in the original input,
which is applicable to factoid question answering
(Kwiatkowski et al., 2019) and single-hop fact ver-
ification (Thorne et al., 2018), but not to tasks with
complex information needs, e.g., multi-hop rea-
soning (Yang et al., 2018) and long-form question
answering (Fan et al., 2019).
To fulfill complex information needs, recent
work proposes to gather required knowledge multi-
ple times throughout the generation process, using
partial generation (Trivedi et al., 2022a; Press et al.,
2022)) or forward-looking sentence(s) (Jiang et al.,
2023) as search queries. However, such structured
workflows of interleaving retrieval with generation
have the following limitations: (1) as intermediate
generation is conditioned on knowledge retrieved
before, with no awareness of knowledge retrieved
afterwards, they fail to process all retrieved knowl-
edge as a whole during the generation process; (2)
they require multi-round retrieval to gather a com-
prehensive set of knowledge, and may frequently
change the prompts by updating newly retrieved
knowledge, thus increasing the overheads of both
retrieval and generation.
In this paper, we find it simple but effective to
enhance retrieval-augmented LLMs through itera-
tive retrieval-generation synergy (ITER -RETGEN,
Fig 1). ITER -RETGEN iterates retrieval-augmented
generation and generation-augmented retrieval:
Retrieval-augmented generation outputs a response
to a task input based on all retrieved knowledge
(initially using the task input as the query). This
arXiv:2305.15294v2  [cs.CL]  23 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 1):

output shows what might be needed to fulfill the
task, and thus can serve as an informative context to
retrieve more relevant knowledge, i.e., generation-
augmented retrieval. The newly retrieved knowl-
edge can benefit another iteration of retrieval-
augmented generation. We can also leverage model
generations to adapt retrieval, by distilling knowl-
edge from a re-ranker with access to model genera-
tions to a dense retriever with access to task inputs
only, which may be beneficial in scenarios where
user inputs can be easily collected, but relevant
knowledge or desirable outputs are not annotated.
We evaluate our method on three tasks, includ-
ing multi-hop question answering, fact verification,
and commonsense reasoning. Our method prompts
an LLM to produce a chain of reasoning steps fol-
lowed by the final answer under a few-shot set-
ting. For in-context demonstrations, we focus on
problem-solving and follow Wei et al. (2022) to
annotate chains of thoughts, without explicitly con-
sidering how generation-augmented retrieval might
be affected, which makes it conceptually simple
and easy to implement. Our method achieves up
to 8.6% absolute gains over previous state-of-the-
art retrieval-augmented methods on four out of six
datasets while being competitive on the remaining
two. According to our experiments, generation gen-
erally benefits from more iterations, with two itera-
tions giving the most performance gains. One may
customize the performance-cost tradeoffs by choos-
ing an appropriate number of iterations. We can
further improve performance and also reduce itera-
tions via the aforementioned generation-augmented
retrieval adaptation.
We summarize our findings as follows:
• Automatic metrics such as exact match can
significantly underestimate the performance
of LLMs in question answering tasks. More-
over, improvements in exact match do not
always reflect improvements in generations.
Evaluation using LLMs may be more reliable.
• ITER -RETGEN is superior to or competi-
tive with state-of-the-art retrieval-augmented
methods, while being simpler and causing
fewer overheads of retrieval and generation.
With generation-augmented retrieval adapta-
tion, we can further improve performance
and also reduce overheads (by reducing itera-
tions).
• It is desirable for an LLM to leverage both
parametric knowledge and non-parametric
knowledge effectively. I TER -RETGEN con-
sistently outperforms Self-Ask on question
answering tasks, regardless of whether in-
context non-parametric knowledge mentions
the answers or not.
2 Related Work
In recent months, there has been a surge in LLM-
powered applications, such as ChatGPT, Bing Chat,
and CoPilot (Chen et al., 2021). While showing an
unprecedented level of performance, LLMs are sub-
ject to the following limitations: (1) Due to a high
demand for compute and data, it remains an open
research question to continually update LLMs both
efficiently and effectively (Scialom et al., 2022);
(2) LLMs also tend to hallucinate (OpenAI, 2023),
i.e., generating plausible but non-factual texts. To
alleviate these issues, there is a growing trend of
augmenting LLMs with tools (Mialon et al., 2023;
Gou et al., 2023), e.g., a code interpreter (Gao
et al., 2022b; Shao et al., 2023) or a search engine
(Nakano et al., 2021), in an attempt to offload sub-
tasks to more qualified experts, or to enrich the
input context for LLMs by providing more relevant
information.
Retrieval augmentation is a mainstream direc-
tion to connect LLMs to the external world. Previ-
ous retrieval-augmented LMs (Izacard and Grave,
2021; Shao and Huang, 2022) typically receive re-
trieved knowledge in a passive way: knowledge
is retrieved based on the task inputs without LMs’
intervention. As it is difficult for a retriever to cap-
ture relevance, especially in the zero-shot setting,
recent work shows a shift towards having LLMs
actively involved in retrieval to improve relevance
modeling, e.g., to provide a specific context for
retrieval with model generations (e.g., generated
search queries (Nakano et al., 2021; Press et al.,
2022; Yao et al., 2022), partial generation (Trivedi
et al., 2022a), or forward-looking sentences (Jiang
et al., 2023)). Khattab et al. (2022) proposed a
DSP programming framework that supports vari-
ous retrieval-augmented methods.
Recent work interleaves retrieval with generation
when completing a single output. Such a structured
workflow may reduce the flexibility in generation
(Yao et al., 2022). ITER -RETGEN avoids interrupt-
ing generation with retrieval, but iterates retrieval
and generation, i.e., to leverage the complete gen-
eration from the previous iteration to retrieve more



Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 8):

of whether the in-context non-parametric knowl-
edge mentions the answers or not. This indicates
that when the in-context non-parametric knowledge
is irrelevant or incomplete, ITER -RETGEN exploits
parametric knowledge better than Self-Ask.
4.7 Error Analysis
On HotPotQA, we manually analyzed 20 random
cases where ITER -RETGEN (T = 2) fails. 25% of
predictions are false negatives. On 10% of cases,
ITER -RETGEN retrieves all necessary information
but fails to perform correct reasoning. The remain-
ing 65% of error cases are related with retrieval, on
76.9% of which, retrieval is misled by completely
wrong reasoning from the first iteration, while on
the other cases, reasoning in the first iteration is
partially correct, but the retriever fails to retrieve
the missing pieces in the second iteration. We also
observed that, in the first iteration, reasoning can be
negatively affected by noisy and possibly distrac-
tive knowledge retrieved using only the questions
as the queries.
5 Case Study
Table 7 demonstrates retrieval-generation synergy
with two examples from HotPotQA and Strate-
gyQA, respectively. In the first iteration, as both
questions need multi-hop reasoning, the retriever
fails to retrieve all supporting knowledge using only
the questions. Despite being affected by distrac-
tive retrieved knowledge (the capacity of a different
arena in the example from HotPotQA) and show-
ing imperfect parametric knowledge (the generated
statement that Raclette is unlikely to be found in
Paris in the example from StrategyQA) in the first
iteration, the LLM generates phrases that help re-
trieve relevant knowledge in the second iteration,
and successfully corrects its outputs.
6 Conclusion
We demonstrate the effectiveness of ITER -RETGEN
in answering questions with complex information
needs. Despite simple, I TER -RETGEN outper-
forms retrieval-augmented methods that have a
more complex workflow, which we believe could
serve as a strong baseline for future research on
retrieval-augmented generation. We also show that
generation-augmented retrieval adaptation can fur-
ther improve the performance of I TER -RETGEN
while also reducing overheads.
Limitations
In this work, we propose to enhance retrieval-
augmented large language models with I TER -
RETGEN which synergizes retrieval and generation
in an iterative manner, and demonstrates strong
performance compared to more structured prompt-
ing techniques such as Self-Ask. However, it’s
worth noting that our experiments utilized a fixed
black-box large language model, which may not
have been equally optimized for various forms of
prompting. It would be intriguing to investigate the
potential of prompting-specific (gradient-based) op-
timization in pushing the limits further. This could
involve enabling a large language model to leverage
parametric and non-parametric knowledge more
flexibly and effectively. By exploring this avenue,
we may uncover new insights and advancements
in the field. Furthermore, our experiments did not
cover long-form generation which would probably
benefit from more fine-grained retrieval than ITER -
RETGEN does in this work. We acknowledge that
this area warrants further exploration, and we leave
it for future work.
Acknowledgements
Zhihong Shao and Minlie Huang were supported by
the National Science Foundation for Distinguished
Young Scholars (with No. 62125604) and the
NSFC projects (Key project with No. 61936010).
They were also supported by the Guoqiang In-
stitute of Tsinghua University, with Grant No.
2020GQG0005.



Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 2):

Question:
Retrieval:
Retrieval -Augmented Generation: 
What is the height of the player who won the 2015 AFL 
Rising Star award?
Title: 2015 AFL Rising Star Context: The NAB AFL Rising 
Star award is given annually to a stand out young player in 
the Australian Football League. The award was won by Jesse 
Hogan of Melbourne
The 2015 AFL Rising Star award was won by Jesse Hogan of 
Melbourne. Jesse Hogan is a professional Australian rules 
footballer. He is 198 cm tall. So the answer is 198 cm
Title: Jesse Hogan Context: Jesse Hogan … playing for the 
Melbourne Football Club. A key forward, Hogan is 1.95 m 
tall … made his AFL debut in the 2015 season and won the 
Ron Evans Medal as the AFL Rising Star
The 2015 AFL Rising Star award was won by Jesse Hogan of 
Melbourne. Jesse Hogan is 1.95 m tall.
So the answer is 1.95 m
Iteration 1 Iteration 2
Generation-Augmented Retrieval:
Retrieval -Augmented Generation: 
Question:
What is the height of the player who won the 2015 AFL 
Rising Star award?
Figure 1: ITER -RETGEN iterates retrieval and generation. In each iteration, I TER -RETGEN leverages the model
output from the previous iteration as a specific context to help retrieve more relevant knowledge, which may help
improve model generation (e.g., correcting the height of Hesse Hogan in this figure). We only show two iterations
in this figure for brevity. Solid arrows connect queries to the retrieved knowledge, and dashed arrows denote
retrieval-augmented generation.
relevant information which helps improve genera-
tion in the next iteration. ITER -RETGEN also has
the advantage of processing all retrieved knowl-
edge as a whole during the generation process, and
is conceptually simpler and easier-to-implement,
while being empirically strong in multi-hop ques-
tion answering, fact verification, and commonsense
reasoning.
A closely related work called GAR (Mao et al.,
2021) augments queries with generated background
information. HyDE (Gao et al., 2022a) also shares
a similar spirit, but focuses on zero-shot informa-
tion retrieval, and proposes to first prompt an LLM
to produce “hypothetical” paragraphs that cover the
information needed to answer a given question, and
then use the generated paragraphs to retrieve the
real ones. RepoCoder (Zhang et al., 2023) focuses
on repository-level code completion, and proposes
a 2-iteration retrieval-generation paradigm where
the second iteration leverages the intermediate code
completion for retrieval. By contrast, we propose
to synergize retrieval and generation with I TER -
RETGEN on various natural language tasks, and
explore how we can further adapt retrieval with
model generations.
3 Iterative Retrieval-Generation Synergy
3.1 Overview
Given a question q and a retrieval corpus D =
{d} where d is a paragraph, ITER -RETGEN repeats
retrieval-generation for T iterations; in iteration
t, we (1) leverage the generation yt−1 from the
previous iteration, concatenated with q, to retrieve
top-k paragraphs, and then (2) prompt an LLM M
to produce an output yt, with both the retrieved
paragraphs (denoted as Dyt−1||q) and q integrated
into the prompt. Therefore, each iteration can be
formulated as follows:
yt = M(yt|prompt(Dyt−1||q, q)), ∀1 ≤ t ≤ T (1)
The last output yT will be produced as the final
response.
3.2 Generation-Augmented Retrieval
There are many natural language tasks with com-
plex information needs. For example, in open-
domain multi-hop question answering, specific in-
formation needs may manifest themselves only
after correctly answering some prerequisite sub-
questions. In other words, there may exist semantic
gaps between the original question q and its sup-
porting knowledge, which can not be effectively
addressed by a retriever with a representation bot-
tleneck. In the first iteration, we can retrieve knowl-
edge with only the question q. In later iterations,
the LLM output from the previous iteration, though
having no guarantee of correctness, shows what
might be needed to answer the question, and thus
can be leveraged to bridge the semantic gaps; with
improved retrieval, an LLM can potentially pro-
duce a better output.
3.3 Retrieval-Augmented Generation
In each iteration, we generate an output using
Chain-of-Thought prompting except that we also
prepend retrieved knowledge to the question q.
Though there may exist more advanced prompting



Source: data\tc16_2312.10997v5\referenced_papers\[59]_2310.05149.pdf (Page 1):

2. ITERATIVE RETRIEV AL-GENERATION
SYNERGY
In this section, we first introduce the overall framework, and
then introduce the retrieval-generation collaboration frame-
work in detail, including generation augmented retrieval and
retrieval augmented generation.
2.1. Overview
We show the framework of ITRG in Figure 2. Given a user
question q and a document corpus D = {di}|D|
i=1 (i.e, di is a
Wikipedia paragraph.), ITRG repeats generation augmented
retrieval (GAR) and retrieval augmented generation (RAG)
for T iterations. In the GAR process of iteration t, we con-
catenate the output yt−1 of the last iteration and question q to
form a new query, and then use a dense retriever to retrieve
top-k paragraphs. In the first iteration, we only use the ques-
tion as the query. In the RAG process of iteration t, based on
the question q and the retrieved top-k paragraphs, we exploit
large language models to generate new paragraphs to answer
questions. Specifically, we propose two methods to generate
new paragraphs, which will be introduced in detail in §2.3.
2.2. Generation Augmented Retrieval
Knowledge-intensive tasks (e.g., open-domain question an-
swering) often require access to additional documents. A
common approach is to directly employ the question as the
query, and then equip a sparse or dense retriever to retrieve
relevant documents. In practice, we find that in some cases
using the question directly as the query fails to retrieve rel-
evant documents because there may exist semantic gaps be-
tween them. To alleviate this problem, we propose a simple
query expansion method. At the first iteration (t = 1), we use
the original question q as the query. At iteration t (t >1), we
concatenate the original question q and the document gener-
ated yt−1 in the last iteration as the new query qt = [q; yt−1].
Then, we utilize a pre-trained dense retriever to retrieve top-k
documents, which are denoted as Rt = {d}.
Given an input question q, the retriever aims to retrieve
a small set of documents from a corpus D = {di}|D|
i=1 that
are relevant to q. Following prior work [14], we use a dense
retriever based on the dual encoder architecture, where an en-
coder is used to encode both the input contextq and the docu-
ment d. Specifically, the encoder maps each document d ∈ D
to an embedding E(d) by taking the mean pooling of the last
hidden representation over the tokens in d. At query time,
the same encoder is applied to the input context q to obtain a
query embedding E(q). The similarity between the query em-
bedding and the document embedding is computed by their
cosine similarity: s(d, q) = cos(E(d), E(q)). The top-k doc-
uments that have the highest similarity scores are retrieved.
Question: What is the date of birth of Emilie HeghArntzen'smother?Retrieval:
Generation:
Retrieval:
Generation:
Retrieval:
Generation:
Iteration 1
Iteration 2
Iteration 3
infoboxname: Emilie HeghArntzen; caption: HeghArntzenin 2018 ; birth_date: January 1, 1994 ; birth_place: Skien, Norway ; nationality: Norwegian ; 
Emilie HeghArntzenwas born on January 1, 1994 in Skien, Norway. Her mother is unknown.
Camilla Marie Gjersemwas born together with a twin sister, Anne Line, on 6 January 1994 in Hønefoss, Norway. Their mother, PerlinaBangug, is a Filipina from Ilagan, Isabela, and their father, PetterGjersem, a Norwegian from Raufoss. Camilla Gjersemis a law student at the University of Oslo.
Hanne Hegh(born 19 January 1960) is a Norwegian handball player. She played 220 matches for the Norwegian national handball team between 1978 and 1992. She is the mother of Emilie HeghArntzen.
infoboxname: Hanne Hegh; caption: Hanne Hegh2008 ; nationality: Norwegian ; birth_date: April 27, 1960; birth_place: Oslo, Norway ; 
Hanne Heghwas born on April 27, 1960 in Oslo, Norway. She is the mother of Emilie HeghArntzen, who was born on January 1, 1994 in Skien, Norway.
Fig. 2: Iterative retrieval-generation synergy framework con-
tains two steps in each iteration: (1) generation augmented
retrieval (GAR): utilize the output of the previous iteration to
expand the query to help retrieve more relevant documents;
(2) retrieval augmented generation (RAG): utilize retrieved
documents to generate new documents to answer questions.
We only show three iterations in this figure for brevity. Solid
arrows indicate RAG within an iteration, and dashed arrows
indicate GAR between iterations. Purple represents correct
and useful information, and red represents wrong or invalid
information.
2.3. Retrieval Augmented Generation
Following previous work [13], for a given question q, we
could directly prompt large language models to generate re-
lated documents without retrieving them from an external cor-
pus. However, we find that if only the parametric knowledge
learned by the large model in the pre-training stage is used,
the generated documents may be incomplete. Retrieval aug-
mented generation (RAG) aims to comprehensively under-
stand the retrieved non-parametric knowledge and the para-
metric knowledge inside large language models to generate
more accurate factual knowledge. Specifically, we propose
two strategies, which will be described in detail below.
2.3.1. Refine
An intuitive idea is to refine the previously generated docu-
ment yt−1 based on the original question q and the retrieved
top-k documents at the current iteration step Rt to obtain a
new document yt. We call this method refine. Considering
that the document retrieved in the last iterationRt−1 has been
used to generate the last document yt−1, we refine the previ-
ous output yt−1 with updated documents Rupdate.
Rupdate = Rt − Rt−1, (1)
yt = M(prompt (yt−1, q, Rupdate)) , (2)



### Claim 61/179

#### Claim Text
IRCoT [61] uses chain-of-thought to guide the retrieval process and refines the CoT with the obtained retrieval results.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[61]_2212.10509.pdf (Page 2):

Recently three approaches have been proposed
for multi-step open-domain QA. SelfAsk (Press
et al., 2022) prompts LLMs to decompose a ques-
tion into subquestions and answers subquestions by
a call to Google Search API. DecomP (Khot et al.,
2023) is a general framework that decomposes a
task and delegates sub-tasks to appropriate sub-
models. They also decompose questions but dele-
gate retrieval to a BM25-based retriever. Both of
these approaches are not developed for CoT reason-
ing, do not focus on the retrieval problem, and re-
quire a single-hop QA model to answer the decom-
posed questions. Recently proposed ReAct (Yao
et al., 2022) system frames the problem as generat-
ing a sequence of reasoning and action steps. These
steps are much more complex, rely on much larger
models (PaLM-540B), and require fine-tuning to
outperform CoT for multi-step ODQA. Further-
more, none of these works have been shown to be
effective for smaller models without any training.
While a direct comparison with these approaches is
not straightforward (difference in knowledge cor-
pus, LLMs, examples), we find that our ODQA
performance is much higher than all their reported
numbers where available (§5).
Supervised Multi-Step Open-Domain QA.
Prior work has explored iterative retrieval for
open-domain QA in a fully supervised setting. Das
et al. (2019) proposes an iterative retrieval model
that retrieves using a neural query representation
and then updates it based on a reading compre-
hension model’s output. Feldman and El-Yaniv
(2019) apply similar neural query reformulation
idea for multihop open-domain QA. Xiong et al.
(2021) extends the widely-used Dense Passage
Retrieval (DPR) (Karpukhin et al., 2020) to
multihop setting, which has since been improved
by Khattab et al. (2021). Asai et al. (2020)
leverages the graph structure induced by the entity
links present in Wikipedia paragraphs to perform
iterative multi-step retrieval. GoldEn (Gold Entity)
retriever (Qi et al., 2019) iteratively generates
text queries based on paragraphs retrieved from
an off-the-shelf retriever but requires training
data for this next query generator. Nakano et al.
(2021) used GPT3 to answer long-form questions
by interacting with the browser but relied on
human annotations of these interactions. All of
these methods rely on supervised training on a
large-scale dataset and can not be easily extended
to a few-shot setting.
3 Chain-of-Thought-Guided Retrieval
and Open-Domain QA
Our goal is to answer a knowledge-intensive multi-
step reasoning question Q in a few-shot setting
by using a knowledge source containing a large
number of documents. To do this we follow a
retrieve-and-read paradigm (Zhu et al., 2021),
where the retriever first retrieves documents from
the knowledge source and the QA model reads the
retrieved documents and the question to generate
the final answer. Our contribution is mainly in the
retrieve step (§3.1), and we use standard prompt-
ing strategies for the read step (§3.2).
As noted earlier, for multi-step reasoning, re-
trieval can help guide the next reasoning step,
which in turn can inform what to retrieve next. This
motivates our interleaving strategy, discussed next.
3.1 Interleaving Retrieval with
Chain-of-Thought Reasoning
Our proposed retriever method, IRCoT, can be
instantiated from the following three ingredients:
(i) a base retriever that can take a query and re-
turn a given number of paragraphs from a corpus
or knowledge source; (ii) a language model with
zero/few-shot Chain-of-Thought (CoT) generation
capabilities; and (iii) a small number of annotated
questions with reasoning steps explaining how to
arrive at the answer in natural language (chain of
thoughts) and a set of paragraphs from the knowl-
edge source that collectively support the reasoning
chain and the answer.
The overview of IRCoT is given in Fig. 2. We
first gather a base set of paragraphs by retrievingK
paragraphs using the questionQ as the query. Then,
we interleave two steps ( reason and retrieve)
iteratively until the termination criterion is met.
The retrieval-guided reasoning step (“Rea-
son”) generates the next CoT sentence using the
question, the paragraphs collected thus far, and
the CoT sentences generated thus far. The prompt
template for the task looks as follows:
Wikipedia Title: <Page Title>
<Paragraph Text>
...
Wikipedia Title: <Page Title>
<Paragraph Text>
Q: <Question>
A: <CoT-Sent-1> ... <CoT-Sent-n>
For in-context demonstrations, we use the com-
plete CoT in the above format. For a test instance,



Source: data\tc16_2312.10997v5\referenced_papers\[61]_2212.10509.pdf (Page 0):

Interleaving Retrieval with Chain-of-Thought Reasoning
for Knowledge-Intensive Multi-Step Questions
Harsh Trivedi† Niranjan Balasubramanian†
†Stony Brook University
Stony Brook, U.S.A.
{hjtrivedi,niranjan}@cs.stonybrook.edu
Tushar Khot‡ Ashish Sabharwal‡
‡Allen Institute for AI
Seattle, U.S.A.
{tushark,ashishs}@allenai.org
Abstract
Prompting-based large language models
(LLMs) are surprisingly powerful at gener-
ating natural language reasoning steps or
Chains-of-Thoughts (CoT) for multi-step
question answering (QA). They struggle,
however, when the necessary knowledge is
either unavailable to the LLM or not up-to-date
within its parameters. While using the question
to retrieve relevant text from an external
knowledge source helps LLMs, we observe
that this one-step retrieve-and-read approach
is insufficient for multi-step QA. Here, what
to retrieve depends on what has already
been derived, which in turn may depend on
what was previously retrieved . To address
this, we propose IRCoT, a new approach
for multi-step QA that interleaves retrieval
with steps (sentences) in a CoT, guiding the
retrieval with CoT and in turn using retrieved
results to improve CoT. Using IRCoT with
GPT3 substantially improves retrieval (up to
21 points) as well as downstream QA (up
to 15 points) on four datasets: HotpotQA,
2WikiMultihopQA, MuSiQue, and IIRC. We
observe similar substantial gains in out-of-
distribution (OOD) settings as well as with
much smaller models such as Flan-T5-large
without additional training. IRCoT reduces
model hallucination, resulting in factually
more accurate CoT reasoning.1.
1 Introduction
Large language models are capable of answer-
ing complex questions by generating step-by-
step natural language reasoning steps—so called
chains of thoughts (CoT)—when prompted appro-
priately (Wei et al., 2022). This approach has been
successful when all information needed to answer
the question is either provided as context (e.g., al-
gebra questions) or assumed to be present in the
model’s parameters (e.g., commonsense reasoning).
1Code, data, and prompts are available at https://
github.com/stonybrooknlp/ircot
In what country was  
Lost Gravity manufactured?
The Lost Gravity was  
manufactured by Mack Rides.
Mack Rides is a company  
from Germany .
The answer is Germany .
cumulate docs
cumulate docs
cumulate docs
Figure 1: IRCoT interleaves chain-of-thought (CoT)
generation and knowledge retrieval steps in order to
guide the retrieval by CoT and vice-versa. This inter-
leaving allows retrieving more relevant information for
later reasoning steps, compared to standard retrieval us-
ing solely the question as the query.
However, for many open-domain questions, all re-
quired knowledge is not always available or up-to-
date in models’ parameters and it’s beneficial to
retrieve knowledge from external sources (Lazari-
dou et al., 2022; Kasai et al., 2022).
How can we augment chain-of-thought prompt-
ing for open-domain, knowledge-intensive tasks
that require complex, multi-step reasoning?
While a one-shot retrieval from a knowledge
source based solely on the question can success-
fully augment LMs with relevant knowledge for
many factoid-based tasks (Lewis et al., 2020; Guu
et al., 2020; Borgeaud et al., 2022; Izacard et al.,
2022), this strategy has clear limitations for more
complex multi-step reasoning questions. For such
questions, one often must retrieve partial knowl-
edge, perform partial reasoning, retrieve additional
information based on the outcome of the partial
arXiv:2212.10509v2  [cs.CL]  23 Jun 2023



Source: data\tc16_2312.10997v5\referenced_papers\[61]_2212.10509.pdf (Page 1):

reasoning done so far, and iterate. As an example,
consider the question illustrated in Fig. 1, “In what
country was Lost Gravity manufactured?” . The
Wikipedia document retrieved using the question
(in particular, the roller coaster Lost Gravity) as the
query does not mention where Lost Gravity was
manufactured. Instead, one must first infer that
it was manufactured by a company called Mack
Rides, and then perform further retrieval, guided
by the inferred company name, to obtain evidence
pointing to the manufacturing country.
Thus, the retrieval and reasoning steps must in-
form each other. Without retrieval, a model is likely
to generate an incorrect reasoning step due to hallu-
cination. Additionally, without generating the first
reasoning step, the text supporting the second step
can’t be identified easily given the lack of lexical or
even semantic overlap with the question. In other
words, we need retrieved facts in order to generate
factually correct reasoning steps and the reasoning
steps to retrieve relevant facts.
Based on this intuition, we propose an interleav-
ing approach to this problem, where the idea is to
use retrieval to guide the chain-of-thought (CoT)
reasoning steps and use CoT reasoning to guide the
retrieval. Fig. 1 shows an overview of our retrieval
method, which we call IRCoT.2 We begin by re-
trieving a base set of paragraphs using the question
as a query. Subsequently, we alternate between the
following two steps: (i) extend CoT: use the ques-
tion, the paragraphs collected thus far, and the CoT
sentences generated thus far to generate the next
CoT sentence; (ii) expand retrieved information:
use the last CoT sentence as a query to retrieve
additional paragraphs to add to the collected set.
We repeat these steps till the CoT reports an an-
swer or we reach the maximum allowed number
of reasoning steps. Upon termination, all collected
paragraphs are returned as the retrieval outcome.
Finally, we use these as the context for answering
the question via direct QA prompting (Brown et al.,
2020) or CoT prompting (Wei et al., 2022).
We evaluate the efficacy of our system
on 4 multi-step reasoning datasets under an
open-domain setting: HotpotQA (Yang et al.,
2018), 2WikiMultihopQA (Ho et al., 2020),
MuSiQue (Trivedi et al., 2022), and IIRC (Fer-
guson et al., 2020). Our experiments using OpenAI
GPT3 (code-davinci-002) (Brown et al., 2020;
Ouyang et al., 2022; Chen et al., 2021) demon-
2Interleaved Retrieval guided by Chain-of-Thought.
strate that retrieval using IRCoT is substantially
more effective than the baseline, one-step, question-
based retrieval by 11-21 recall points under a fixed-
budget optimal recall setup.3 When IRCoT is used
in conjunction with a prompting-based reader, it
also leads to substantial improvement (up to 15 F1
points) in downstream few-shot QA performance
and reduces factual errors in generated CoT by
up to 50%. Our approach also works on much
smaller Flan-T5 models (11B, 3B, and 0.7B) show-
ing similar trends. In particular, we find QA using
Flan-T5-XL (3B) with IRCoT even outperforms
the 58X larger GPT3 with a one-step question-
based retrieval. Furthermore, these improvements
also hold up in an out-of-distribution (OOD) setting
where the demonstrations from one dataset are used
when testing on another dataset. Lastly, we note
that our QA scores exceed those reported by recent
works on few-shot prompting for open-domain QA
(ODQA) (Khot et al., 2023; Press et al., 2022; Yao
et al., 2022), although a fair apples-to-apples com-
parison with them isn’t possible (cf. Appendix C).
In summary, our maincontribution is a novel re-
trieval method, IRCoT, that leverages LMs’ chain-
of-thought generation capabilities to guide retrieval
and uses retrieval in turn to improve CoT reasoning.
We demonstrate that IRCoT:
1. improves both retrieval and few-shot QA per-
formance on several multi-step open-domain
QA datasets, in both IID and OOD settings;
2. reduces factual errors in generated CoTs; and
3. improves performance with both large-scale
(175B models) as well as smaller-scale mod-
els (Flan-T5-*, ≤11B) without any training.
2 Related Work
Prompting for Open-Domain QA.LLMs can
learn various tasks by simply using a few exam-
ples as prompts (Brown et al., 2020). They’ve
also been shown to answer complex questions
by producing step-by-step reasoning (chain-of-
thoughts, or CoT) when prompted with a few or
zero demonstrations (Wei et al., 2022; Kojima et al.,
2022). Prompting has been applied to open-domain
QA (Lazaridou et al., 2022; Sun et al., 2022; Yu
et al., 2023) but its value in improving retrieval and
QA for multi-step open-domain questions remains
relatively underexplored.
3We explain later (in the Metric section and Footnote 7)
the appropriateness of this metric in our setting as opposed to
more mainstream information recall metrics.



Source: data\tc16_2312.10997v5\referenced_papers\[61]_2212.10509.pdf (Page 8):

Question Gold Facts NoR QA OneR QA IRCoT QA
HotpotQA
The tap dancing
soloist in My
Feet Keep
Dancing also
starred in what
1935 MGM
musical?
⇒ The tap dancing
soloist in My Feet
Keep Dancing is Fayard
Nicholas.
⇒ Fayard Nicholas
starred in the 1935
MGM musical An
All-Colored Vaudeville
Show.
The tap dancing soloist
in My Feet Keep Danc-
ing is Eleanor Pow-
ell. Eleanor Powell
also starred in the 1935
MGM musical Broad-
way Melody of 1936.
So the answer is: Broad-
way Melody of 1936.
The tap dancing soloist
in My Feet Keep Danc-
ing is Fayard Nicholas.
Fayard Nicholas also
starred in the 1935
MGM musical Top Hat.
So the answer is: Top
Hat.
The tap dancing soloist
in My Feet Keep Danc-
ing is Fayard Nicholas.
Fayard Nicholas starred
in the 1935 MGM mu-
sical An All-Colored
Vaudeville Show. So
the answer is: An
All-Colored Vaudeville
Show.
2WikiMQA
Who is the
father-in-law
of Queen Hyo-
jeong?
⇒ Queen Hyojeong
is the wife of King
Heonjong of Joseon.
⇒ King Heonjong
of Joseon is the son
of Crown Prince Hy-
omyeong.
Queen Hyojeong has a
husband named King
Gojong. King Gojong
has a father named
Heungseon Daewongun.
Thus, Queen Hyojeong
has a father-in-law
named Heungseon
Daewongun. So the
answer is: Heungseon
Daewongun.
Queen Hyojeong is the
wife of King Heonjong
of Joseon. King Heon-
jong of Joseon is the
son of King Sejo of
Joseon. Thus, King Sejo
of Joseon is the father-
in-law of Queen Hyo-
jeong. So the answer is:
King Sejo of Joseon.
Queen Hyojeong is the
wife of King Heonjong
of Joseon. King Heon-
jong of Joseon is the son
of Crown Prince Hy-
omyeong. Thus, Crown
Prince Hyomyeong is
the father-in-law of
Queen Hyojeong. So
the answer is: Crown
Prince Hyomyeong.
MuSiQue
What is the
name of the
castle in the
city where the
performer of
A Collection
1984–1989 was
born?
⇒ A Collection
1984–1989 was per-
formed by Jane Siberry.
⇒ Jane Siberry was
born in Toronto.
⇒ The castle in Toronto
is the Casa Loma.
The performer of A Col-
lection 1984–1989 is
The The. The The was
born in London. The
name of the castle in
London is the Tower of
London. So the answer
is: the Tower of Lon-
don.
A Collection
1984–1989 was
performed by Jane
Siberry. Jane Siberry
was born in Toronto.
The castle in Toronto
is Peqin Castle. So the
answer is: Peqin Castle.
A Collection
1984–1989 was
performed by Jane
Siberry. Jane Siberry
was born in Toronto.
The castle in Toronto is
the Casa Loma. So the
answer is: Casa Loma.
Table 2: Example CoTs generated by GPT3 with different methods. Since NoR relies on parametric knowledge, it
often makes a factual error in the first sentence derailing the full CoT. OneR can retrieve relevant information closest
to the question and is less likely to make such errors early on, but it still makes errors later in the CoT. AsIRCoT
performs retrieval after each step, it is often able to prevent such errors in each step. More examples are in App. D.
ing. We leveraged this ability to improve retrieval,
and in turn, improve QA performance for com-
plex knowledge-intensive open-domain tasks in a
few-shot setting. We argued that one-step question-
based retrieval is insufficient for such tasks, and
introduced IRCoT, which uses interleaved CoT rea-
soning and retrieval steps that guide each other
step-by-step. On four datasets, IRCoT significantly
improves both retrieval and QA performance when
compared to one-step retrieval, for both large and
relatively smaller-scale LMs. Additionally, CoTs
generated by IRCoT contain fewer factual errors.
Limitations
IRCoT relies on the base LM to have a zero or
few-shot CoT-generation ability. While this is com-
monly available in large LMs (over 100B), it’s not
as common for small LMs (under 20B), which to
some extent limits IRCoT adoptability. Given the
recent surge of interest (Tay et al., 2023; Magis-
ter et al., 2022; Ho et al., 2022), however, smaller
LMs will likely increasingly acquire such ability,
making IRCoT compatible with many more LMs.
IRCoT also relies on the base LM to support
long inputs as multiple retrieved paragraphs need
to fit in the LM’s input, in addition to at least
a few demonstrations of QA or CoT with para-
graphs. This was supported by the models we used
as code-davinci-002 (GPT3) allows 8K tokens
and Flan-T5-* uses relative position embeddings
making it as extensible as the GPU memory con-
straints allow. Future work can explore strategies to
rerank and select the retrieved paragraphs instead
of passing all of them to the LM to alleviate the
need for the LM to support long input.
The performance gain of IRCoT retriever and
QA (over OneR and ZeroR baselines) come with
an additional computational cost. This is because
IRCoT makes a separate call to an (L)LM for each
sentence of CoT. Future work can focus on, for
instance, dynamically deciding when to retrieve
more information and when to perform additional
reasoning with the current information.



Source: data\tc16_2312.10997v5\referenced_papers\[61]_2212.10509.pdf (Page 5):

Figure 3: Retrieval recall for one-step retriever (OneR) and IRCoT instantiated from Flan-T5-XXL (left) and GPT3
(right) models. IRCoT outperforms OneR for both models and all datasets.
Figure 4: Answer F1 for ODQA model made using (i) no retriever (NoR QA) (ii) one-step retriever (OneR QA) and
(iii) IRCoT QA instantiated from Flan-T5-XXL (left) and GPT3 (right) models. IRCoT QA outperforms OneR QA
and NoR QA for both models on all datasets, except for GPT3 on IIRC.
Flan-T5-XXL and GPT3 LMs. For both models,
IRCoT significantly outperforms one-step retrieval
across all datasets. For Flan-T5-XXL, IRCoT im-
proves our recall metric relative to one-step re-
trieval, on HotpotQA by 7.9, on 2WikiMultihopQA
by 14.3, on MuSiQue by 3.5, and on IIRC by 10.2
points. For GPT3, this improvement is by 11.3, 22.6,
12.5, and 21.2 points, respectively.
IRCoT QA outperforms NoR and OneR QA.
Fig. 4 compares ODQA performance using
NoR, OneR and IRCoT retriever made from
Flan-T5-XXL and GPT3 LMs. For Flan-T5-XXL,
IRCoT QA outperforms OneR QA on HotpotQA
by 9.4, on 2WikiMultihopQA by 15.3, on MuSiQue
by 5.0 and IIRC by 2.5 F1 points. For GPT3, the
corresponding numbers (except for IIRC) are 7.1,
13.2, and 7.1 F1 points. For GPT3, IRCoT doesn’t
improve the QA score on IIRC, despite signifi-
cantly improved retrieval (21 points as shown in
Fig. 3). This is likely because IIRC relevant knowl-
edge may already be present in GPT3, as also ev-
idenced by its NoR QA score being similar. For
other datasets and model combinations, NoR QA is
much worse than IRCoT QA, indicating the limits
of the models’ parametric knowledge.
IRCoT is effective in OOD setting.Since CoT
may not always be easy to write for new datasets,
we evaluate NoR, OneR, and IRCoT on generaliza-
tion to new datasets, i.e. OOD setting. To do so,
we use prompt demonstrations from one dataset to
evaluate on another dataset.9 For all pairs of the
datasets10 and for both Flan-T5-XXL and GPT3, we
find the same trend as in the IID setting: IRCoT re-
trieval outperforms OneR (Fig. 5), and IRCoT QA
outperforms both OneR QA and NoR QA (Fig. 6).
IRCoT generates CoT with fewer factual errors.
To assess whether our approach also improves the
factuality of generated CoTs, we manually anno-
tated CoTs generated by NoR QA, OneR QA, and
IRCoT QA using GPT3 for 40 randomly sampled
questions from each of the four datasets. We con-
sidered CoT to have a factual error if at least one
9We use the evaluation dataset’s corpus for retrieval.
10We skip IIRC in this exploration as the task is structured
a bit differently and requires special handling (see App. B).



### Claim 62/179

#### Claim Text
ToC [57] creates a clarification tree that systematically optimizes the ambiguous parts in the Query.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[57]_2310.14696.pdf (Page 1):

prompting leveraging external knowledge—and
uses it to generate a long-form answer. More specif-
ically, first, relevant passages for the AQ are re-
trieved. Then, leveraging the passages, DQs for the
AQ are recursively generated via few-shot prompt-
ing and pruned as necessary. Lastly, a long-form
answer addressing all DQs is generated. The tree
structure promotes exploring DQs in targeting par-
ticular dimensions of clarification, addressing the
first challenge, and the external sources offer ad-
ditional knowledge to cope with the second chal-
lenge.
Experiments demonstrate that our proposed use
of LLMs with retrieval-augmentation and guid-
ance to pursue diverse paths of clarification results
in the new state-of-the-art on ASQA (Stelmakh
et al., 2022)—a long-form QA benchmark for AQs.
TOC outperforms existing baselines on ASQA in
a few-shot setup across all metrics. In addition,
this 5-shot performance surpasses that of the fully-
supervised baselines trained on the whole training
set by 7.3 and 2.9 in terms of Disambig-F1 and
Disambig-ROUGE, respectively.
The main contribution of this work is proposing
a novel framework, TREE OF CLARIFICATIONS
(TOC), for generating long-form answers to AQs
in ODQA, advancing the state-of-the-art on the
ASQA benchmark. TOC introduces two main in-
novations:
• It guides LLMs to explore diverse paths of
clarification of the given AQ in a tree structure
with the ability to prune unhelpful DQs.
• To the best of our knowledge, it is the first
to combine retrieval systems with LLM for
generating long-form answers to AQs.
2 Related Work
A line of studies (Min et al., 2020, 2021; Gao et al.,
2021; Shao and Huang, 2022) extends retrieve-and-
read frameworks dominant in ODQA task (Chen
et al., 2017; Karpukhin et al., 2020; Lewis et al.,
2020; Izacard and Grave, 2021) to clarify AQ and
generate DQs with corresponding answers to them.
However, their approaches require fine-tuning mod-
els on the large-scale train set. On the other hand,
our framework enables LLM to generate a compre-
hensive response addressing all DQs via few-shot
prompting.
Recent studies introduce LLM-based methods to
generate a long-form answer to the AQ. Amplayo
et al. (2023) suggest optimal prompts specifically
engineered for the task. Kuhn et al. (2022) prompt
LLMs to clarify ambiguous questions selectively.
However, the studies do not utilize external infor-
mation to ensure the factual correctness of the dis-
ambiguations, thereby potentially increasing the
risk of hallucinations from LLMs. Moreover, the
results could be bounded by inherent parametric
knowledge of LLM. Concurrently, Lee et al. (2023)
automatically generate clarifying questions to re-
solve ambiguity.
Our framework involves the recursive tree archi-
tecture, inspired by several prior studies. Min et al.
(2021) propose the tree-decoding algorithm to au-
toregressively rerank passages in ambiguous QA.
Gao et al. (2021) iteratively explore additional inter-
pretations and verify them in a round-trip manner.
Concurrently, extending chain of thoughts (Wei
et al., 2022) prompting, Yao et al. (2023) apply
the tree architecture to reasoning tasks for deduc-
tive or mathematical problems. On the contrary,
TOC recursively clarifies questions and introduces
a self-verification method to prune unhelpful DQs.
3 Tree of Clarifications
We introduce a novel framework,TREE OF CLARI -
FICATIONS (TOC), as illustrated in Figure 1. We
first devise retrieval-augmented clarification (RAC;
Sec. 3.1), a basic component that clarifies AQ and
generates DQs based on relevant passages. TOC
explores various fine-grained interpretations, rep-
resented as a tree structure (TS; Sec. 3.2) by re-
cursively performing RAC and pruning unhelpful
DQs. Lastly, it aggregates the tree and generates
a long-form answer addressing all valid interpreta-
tions.
3.1 Retrieval-Augmented Clarification (RAC)
We first retrieve relevant Wikipedia documents
for the AQ by using two retrieval systems, Col-
BERT (Khattab and Zaharia, 2020) and Bing search
engine1. ColBERT is a recent dense retriever that
has effective and efficient zero-shot search qual-
ity. Following Khattab et al. (2022), we use the
off-the-shelf model pre-trained on MS-Marco (Ba-
jaj et al., 2016). We additionally include the Bing
search engine to promote the diversity of retrieved
Wikipedia passages. Finally, we obtain over 200
passages by combining passages retrieved by each
system.
1https://www.microsoft.com/bing



Source: data\tc16_2312.10997v5\referenced_papers\[57]_2310.14696.pdf (Page 0):

Tree of Clarifications: Answering Ambiguous Questions
with Retrieval-Augmented Large Language Models
Gangwoo Kim1 Sungdong Kim2,3,4 Byeongguk Jeon1 Joonsuk Park2,3,5 Jaewoo Kang1†
Korea University1 NA VER Cloud2 NA VER AI Lab3
KAIST AI4 University of Richmond5
{gangwoo_kim, bkjeon1211, kangj}@korea.ac.kr
sungdong.kim@navercorp.com park@joonsuk.org
Abstract
Questions in open-domain question answering
are often ambiguous, allowing multiple inter-
pretations. One approach to handling them is
to identify all possible interpretations of the
ambiguous question (AQ) and to generate a
long-form answer addressing them all, as sug-
gested by Stelmakh et al. (2022). While it
provides a comprehensive response without
bothering the user for clarification, considering
multiple dimensions of ambiguity and gather-
ing corresponding knowledge remains a chal-
lenge. To cope with the challenge, we propose a
novel framework, TREE OF CLARIFICATIONS
(TOC): It recursively constructs a tree of disam-
biguations for the AQ—via few-shot prompt-
ing leveraging external knowledge—and uses
it to generate a long-form answer. TOC out-
performs existing baselines on ASQA in a few-
shot setup across all metrics, while surpass-
ing fully-supervised baselines trained on the
whole training set in terms of Disambig-F1
and Disambig-ROUGE. Code is available at
github.com/gankim/tree-of-clarifications.
1 Introduction
In open-domain question answering (ODQA),
users often ask ambiguous questions (AQs), which
can be interpreted in multiple ways. To handle
AQs, several approaches have been proposed, such
as providing individual answers to disambiguated
questions (DQs) for all plausible interpretations of
the given AQ (Min et al., 2020) or asking a clarifi-
cation question (Guo et al., 2021). Among them,
we adopt that of Stelmakh et al. (2022), which pro-
vides a comprehensive response without bothering
the user for clarification: The task is to identify
all DQs of the given AQ and generate a long-form
answer addressing all the DQs (See Figure 1).
There are two main challenges to this task: (1)
the AQ may need to be clarified by considering mul-
† Corresponding author
𝑫𝑸𝟐𝑫𝑸𝟏 𝑫𝑸𝟑
Question 
Clarification
Question 
Clarification
𝑫𝑸𝟐𝟐𝑫𝑸𝟐𝟏 𝑫𝑸𝟐𝟑 𝑫𝑸𝟐𝟒𝑫𝑸𝟏𝟐 𝑫𝑸𝟏𝟑𝑫𝑸𝟏𝟏
𝑨𝒎𝒃𝒊𝒈𝒖𝒐𝒖𝒔 𝑸𝒖𝒆𝒔𝒕𝒊𝒐𝒏 (𝑨𝑸)
𝑷𝒂𝒔𝒔𝒂𝒈𝒆𝒔
Answer
Generation
“The United States has the 
most total medals . . . 
Norway has won most 
medals  in winter Olympic.”
𝑳𝒐𝒏𝒈 𝑭𝒐𝒓𝒎 𝑨𝒏𝒔𝒘𝒆𝒓
Pruned
…
“What country has the most medals 
in Olympic history?”
“What country has 
the most gold medals
in Olympic history?”
Question 
Clarification
Information
Retrieval
“What country has the 
most  medals in winter 
Olympic history?”
“What  country has 
the most total medals
in Olympic history?” 
𝑻𝒓𝒆𝒆 𝒐𝒇 𝑪𝒍𝒂𝒓𝒊𝒇𝒊𝒄𝒂𝒕𝒊𝒐𝒏𝒔
𝑫𝑸𝟐𝟒
Figure 1: Overview of TREE OF CLARIFICATIONS . (1)
relevant passages for the ambiguous question (AQ) are
retrieved. (2) leveraging the passages, disambiguated
questions (DQs) for the AQ are recursively generated
via few-shot prompting and pruned as necessary. (3) a
long-form answer addressing all DQs is generated.
tiple dimensions of ambiguity. For example, the
AQ “what country has the most medals in Olympic
history” in Figure 1 can be clarified with respect
to the type of medals—gold, silver, or bronze—or
Olympics—summer or winter; and (2) substantial
knowledge is required to identify DQs and respec-
tive answers. For example, it requires knowledge
to be aware of the existence of different types of
medals and the exact counts for each country.
To address the challenges and provide a long-
form answer to AQ, we propose a novel framework,
TREE OF CLARIFICATIONS (TOC): It recursively
constructs a tree of DQs for the AQ—via few-shot
arXiv:2310.14696v1  [cs.CL]  23 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[57]_2310.14696.pdf (Page 4):

5 Discussion
Ambiguity Detection TOC is designed to clar-
ify AQs without bothering users; hence does not
explicitly identify whether the given question is am-
biguous or not. It tries to perform clarification even
if the question cannot be disambiguated anymore,
often resulting in generating duplicate or irrele-
vant DQs7. However, we could presume a question
to be unambiguous if it can no longer be disam-
biguated8. In TOC, when it fails to disambiguate
the given question or all generated disambiguations
are pruned, the question could be regarded as un-
ambiguous.
Computational Complexity Although TOC re-
quires multiple LLM calls, its maximum number
is less than 20 times per question. Exploration of
the tree ends when it obtains the pre-defined num-
ber of valid nodes (10 in our experiments). Since
the clarification process generates from two to five
disambiguations for each question, it satisfies the
termination condition in a few steps without the
pruning method. Failing to expand three times in a
row also terminates the exploration. Pruning steps
consume a smaller amount of tokens since they
encode a single passage without few-shot exem-
plars. Compared to the existing ensemble methods
such as self-consistency (Wei et al., 2022) which
cannot be directly adopted to the generative task,
ToC achieves a state-of-the-art performance with a
comparable number of LLM calls.
Generalizability The key idea of ToC could be
potentially generalized to other tasks and model
architectures. It has a model-agnostic structure that
could effectively explore diverse paths of recursive
reasoning, which would be helpful for tasks that re-
quire multi-step reasoning, such as multi-hop QA.
Future work might investigate the generalizability
of TOC to diverse tasks, datasets, and LM architec-
tures.
6 Conclusion
In this work, we propose a novel framework, TREE
OF CLARIFICATIONS . It recursively builds a tree of
disambiguations for the AQ via few-shot prompting
with external knowledge and utilizes it to generate a
7See Appendix 7 for failure cases
8The idea is aligned with the annotation process of Am-
bigQA (Min et al., 2020), in which the target question is
classified as ambiguous if multiple distinct answers to it were
observed.
long-form answer. Our framework explores diverse
dimensions of interpretations of ambiguity. Experi-
mental results demonstrate TOC successfully guide
LLMs to traverse diverse paths of clarification for
a given AQ within tree structure and generate com-
prehensive answers. We hope this work could shed
light on building robust clarification models, which
can be generalized toward real-world scenarios.
Limitations
Although TOC is a model-agnostic framework that
could be combined with other components, our
study is limited in demonstrating the generalizabil-
ity of different kinds or sizes of LLMs. In addition,
the experiments are only conducted on a bench-
mark, ASQA (Stelmakh et al., 2022). Although
TOC enables LLM to explore diverse reasoning
paths by iteratively prompting LLM, the cost of
multiple prompting is not negligible.
We tried the recent prompting method, chain
of thoughts (Wei et al., 2022), but failed to en-
hance the performance in our pilot experiments. It
might indicate the disambiguation process requires
external knowledge, which shows the importance
of document-grounded or retrieval-augmented sys-
tems. Future work could suggest other pruning
methods that identify unhelpful DQs more effec-
tively. The performance could be further enhanced
by using the state-of-the-art reranker in the an-
swer sentence selection task, as proposed by recent
works (Garg et al., 2020; Lauriola and Moschitti,
2021).
Acknowledgements
The first author, Gangwoo Kim, has been sup-
ported by the Hyundai Motor Chung Mong-Koo
Foundation. This research was supported by the
National Research Foundation of Korea (NRF-
2023R1A2C3004176, RS-2023-00262002), the
MSIT (Ministry of Science and ICT), Korea, under
the ICT Creative Consilience program (IITP-2022-
2020-0-01819) supervised by the IITP (Institute
for Information & communications Technology
Planning & Evaluation), and the Electronics and
Telecommunications Research Institute (RS-2023-
00220195).



Source: data\tc16_2312.10997v5\referenced_papers\[41]_2312.05708.pdf (Page 7):

I have selected the following tool to
perform the task :
{ tool }
Can you come up with fully resolved plan
using the following schema ?
{ format_instructions }
A.6 Prompt to generate CoT
You are an expert in processing context -
seeking or under - specified queries
by finding missing context in the
query . As an expert , your task is to
generate concise chain of thought
which when used to augment the
context - seeking query , increases the
semantic similarity of the updated
query with relevant context items .
Please only use the following
context types : 'Mail ', 'Calendar ', '
Reminders ', 'Notes ', 'Photos ', '
PhoneCall ', 'Message ', 'Messenger ',
'Maps ', 'Google Maps ', 'Music ', '
Spotify ', 'Find My ', 'Workout '; and
do not create new context types .
Context - seeking Query : { query }
Your expert Chain of Thought :
Examples showing generated implicit queries
along with CoT, context and plan labels are shown
in Table 5.



Source: data\tc16_2312.10997v5\referenced_papers\[57]_2310.14696.pdf (Page 3):

Model D-F1 R-L DR
GPT-3 (Baseline) 24.2 36.0 29.5
GPT-3 w/ RAC 31.1 39.6 35.1
− Disambiguations 30.5 37.3 33.7
− Bing Search Engine 28.5 37.4 32.7
− Retrieval Systems 25.6 35.1 30.0
Table 2: Ablation study on all components of retrieval-
augmented clarification (RAC).
two scores, which assesses the overall performance.
For validating intermediate nodes, we additionally
use Answer-F1 that measures the accuracy of gen-
erated short answers in disambiguation. Further
details are in Appendix A.2.
Baselines Stelmakh et al. (2022) propose fine-
tuned baselines. They fine-tune T5-large (Raf-
fel et al., 2020) to generate long-form answers
on the whole train set. Models are evaluated in
the closed-book setup or combined with JPR (Min
et al., 2021), task-specific dense retriever for am-
biguous QA by enhancing DPR (Karpukhin et al.,
2020). On the other hand, Amplayo et al. (2023)
propose a prompt engineering method to adapt
LLMs to the ASQA benchmark. They employ
PaLM (Chowdhery et al., 2022) and Instruct-
GPT (Ouyang et al., 2022) that learn the soft
prompts or adopt in-context learning with few-shot
examples. They conduct experiments in the closed-
book setup. Note that they share the same back-
bone with our models, GPT-3 with 175B parame-
ters (text-davinci-002).
4.2 Experimental Results
TOC outperforms fully-supervised and few-shot
prompting baselines. Table 1 shows the long-form
QA performance of baselines and TOC on the de-
velopment set of ASQA. Among baselines, using
the whole training set (Fully-supervised) achieves
greater performances than Few-shot Prompting
in all metrics. It implies that long-form QA
task is challenging in the few-shot setup. In the
closed-book setup, GPT-3 shows competitive per-
formances with T5-large with JPR in D-F1 score,
showing LLM’s strong reasoning ability over its
inherent knowledge.
Among our models, LLM with RAC outper-
forms all other baselines in D-F1 and DR scores.
It indicates the importance of leveraging external
knowledge in clarifying AQs. Employing the tree
structure (TS) helps the model to explore diverse
interpretations, improving D-F1 and DR scores by
Filtration #(DQs) Answer-F1
w/o Pruning (None) 12,838 40.9
w Pruning
+ Deduplication 10,598 40.1
+ Self-Verification 4,239 59.3
Table 3: Ablated results with and without pruning meth-
ods. The number of retained DQs after pruning and
Answer-F1 are reported.
1.3 and 0.9. When pruning the tree with our pro-
posed self-verification (TS w/ Pruning), the model
achieves state-of-the-art performance in D-F1 and
DR score, surpassing the previous few-shot base-
line by 8.4 and 7.0. Notably, it outperforms the best
model in a fully-supervised setup (T5-large with
JPR) by 7.3 and 2.9. In the experiment, T5-Large
in a closed-book setup achieves comparable per-
formance with LLM baselines in ROUGE-L score
despite its poor D-F1 scores. It reconfirms the ob-
servation from Krishna et al. (2021) that shows the
limitations of the ROUGE-L metric.
Integrating retrieval systems largely con-
tributes to accurate and diverse disambigua-
tions. Table 2 displays the ablation study for mea-
suring the contributions of each proposed compo-
nent. When removing disambiguations from few-
shot training examples, the ROUGE-L score is sig-
nificantly degraded, which shows the importance
of the intermediate step to provide the complete
answer. Integrating retrieval systems (i.e., Bing
search engine and ColBERT) largely improves the
model performance, especially in the D-F1 score.
It indicates using external knowledge is key to en-
hancing the factual correctness of clarification. We
report intrinsic evaluation for each retrieval system
in Appendix B.
Our pruning method precisely identifies help-
ful disambiguations from the tree. Table 3 shows
intrinsic evaluation for generated disambiguations,
where all baselines are evaluated with Answer-F1
score that measures the F1 accuracy of the answer
to the target DQ. Compared to the baseline, the
valid nodes that pass self-verification contain more
accurate disambiguations, achieving much higher
Answer-F1 score ( +18.4). On the other hand,
solely using deduplication does not advance the
accuracy, indicating the efficacy of our proposed
self-verification method.



### Claim 63/179

#### Claim Text
In contrast, multi-hop retrieval is designed to delve deeper into graph-structured data sources, extracting interconnected information [106].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[84]_2402.07630.pdf (Page 17):

discrimination between capital and lowercase words, we have converted all words to lowercase. We
used the same dataset split as in the original dataset.
D Graph Retrieval-Augmented Generation (GraphRAG)
D.1 Comparison with Existing GraphRAG Methods
Most existing GraphRAG methods are designed specifically for knowledge graphs and focus on node,
edge, or triples-level retrieval [1, 36, 16, 38]. Our method is different in two main ways: 1) It focuses
on more general textual graphs, not just knowledge graphs. 2) It enables the return of a subgraph most
closely related to a query, rather than a list of top-k triples. The triples in other methods are chosen in
isolation from the graph, failing to capture neighborhood information effectively. In contrast, our
method takes the context into account during retrieval.
D.2 The Impact of K for Retrieval
We identify the most relevant nodes and edges and use a k-nearest neighbors retrieval approach (see
Equation 6). Small k values may omit crucial knowledge or information relevant to the query, while
large k values could introduce excessive information, distracting the model from the essential details.
To evaluate the impact of the number of k, we have conducted additional experiments by varying the
choice of k to 3, 5, 10, and 20.
Table 10: The impact of k on the webqsp dataset.
k 3 5 10 20
Hit@1 0.6977 0.7063 0.7248 0.7039
As shown in Table 10, the Hit@1 metric initially rises for small k values, peaks at a certain point, and
then declines for large k values. Determining the optimal k value can be achieved through techniques
like cross-validation using a validation set.
D.3 The Choice of Similarity Function
The choice of similarity function is also important. In this work, we use cosine similarity, a widely
adopted metric for measuring vector similarity in models that process vision and language. For
instance, CLIP also employs cosine similarity to assess the similarity between text and image
features. Although it might not be the optimal choice, we believe that cosine similarity is a general,
representative, and valid choice for facilitating fast retrieval tasks.
D.4 The Quality of Retrieval
We quantify the quality of our retrieval method as follows: We examine the retrieval subgraph, and if
the label is contained within it, we consider it a successful retrieval. We calculate the retrieval success
rate of our method and the retrieval method proposed in KAPING [1] on the WebQSP dataset.
Table 11: The quality of retrieval methods on the WebQSP dataset.
Method Retrieval Accuracy
KAPING [1] (top-k triple retrieval) 60.81
G-Retriever 70.49
As shown in Table 11, these results validate the effectiveness of our retrieval method. In contrast to
the triple-based retrieval in KAPING, which relies on the similarity between triples and the query,
our PCST-based subgraph retrieval is more accurate as it takes the graph structure into account. By
design, it retrieves connected subgraphs, capturing not just nodes or edges with high relevance scores,
but also those that act as "bridges" connecting other highly relevant nodes and edges.
18



Source: data\tc16_2312.10997v5\referenced_papers\[84]_2402.07630.pdf (Page 5):

Graph Encoder
Question: What is the name
of justin bieber brother?
Step 1: Indexing
Storage
Step 2: Retrieval Step 3: Subgraph Construction
LLM
(Self Attention Layers)
LLM
(Text Embedder)
Step 4: Generation
bieberjaxon
Projection
justin
bieber
jeremy
bieberparent
jaxon
bieber
children
sibling
m.0gxnnwp
sibling
all bad
album
record
producer
profession
canada
nationality
male
gender
LM
node attributes
justin bieber, this is justin bieber, jeremy bieber, 
justin bieber fan club, justin ...
sibling, sibling_s, hangout, friendship, friend ...
LM
LM
justin
bieber
jeremy
bieberparent
jaxon
bieber
children
siblingm.0gxnnwp
sibling
node_id, node_attr
15, justin bieber
294, jaxon bieber
356, jeremy bieber
551, m.0gxnnwp
src, edge_attr, dst
294, parents, 356
356, children, 15
551, sibling, 294
551, sibling, 15
Question: What is the name
of justin bieber brother?
Answer:
edge attributes
frozen
trainable
Figure 3: Overview of the proposed G-Retriever: 1) Indexing: Graphs are indexed for efficient query
processing; 2) Retrieval: The most semantically relevant nodes and edges are retrieved, conditioned
on the query; 3) Subgraph Construction: A connected subgraph is extracted, covering as many
relevant nodes and edges as possible while maintaining a manageable graph size; 4) Generation: An
answer is generated using a ‘graph prompt’, a textualized graph, and the query.
require multi-hop reasoning. Given the possibility of multiple answers for the same question, the
hit@1 metric is used to assess the precision of the top returned answer.
5 G-Retriever
In this section, we introduce G-Retriever, a new architecture tailored for GraphQA, which integrates
the strengths of GNNs, LLMs, and RAG. To allow efficient fine-tuning while preserving the LLM’s
pretrained language capabilities, we freeze the LLM and use a soft prompting approach on the output
of the GNN. Our RAG-based design mitigates hallucinations through direct retrieval of the graph,
while allowing our approach to scale to graphs exceeding the LLM’s context window size. To adapt
RAG to graphs, we formulate subgraph retrieval as a PCST optimization problem. This approach
also allows us to enhance explainability by returning the retrieved subgraph.
G-Retriever comprises four main steps: indexing, retrieval, subgraph construction and generation, as
depicted in Figure 3. The implementation details of each step are elaborated in the following sections.
5.1 Indexing
We initiate the RAG approach by generating node and graph embeddings using a pre-trained LM.
These embeddings are then stored in a nearest neighbor data structure.
To elaborate, consider xn ∈ DLn as the text attributes of node n. Utilizing a pre-trained LM, such as
SentenceBert [34], we apply the LM to xn, yielding the representation zn:
zn = LM(xn) ∈ Rd, (3)
where d denotes the dimension of the output vector. Similar preprocessing steps are applied to edges.
Refer to Figure 3, Step 1 for an illustrative representation.
5.2 Retrieval
For retrieval, we employ the same encoding strategy to the query xq, to ensure consistent treatment
of textual information:
zq = LM(xq) ∈ Rd. (4)
Next, to identify the most relevant nodes and edges for the current query, we use a k-nearest neighbors
retrieval approach. This method yields a set of ‘relevant nodes/edges’ based on the similarity between
the query and each node or edge. The retrieval operation is defined as:
Vk = argtopkn∈V cos(zq, zn)
Ek = argtopke∈E cos(zq, ze), (5)
6



Source: data\tc16_2312.10997v5\referenced_papers\[84]_2402.07630.pdf (Page 19):

with only 31% of nodes and 12% of edges being valid, and the entire set of nodes and edges being
valid only 8% of the time. In contrast, G-Retriever showed substantial improvements: 77% validity
in nodes, 76% in edges, and 62% in the overall validity of referenced node-edge sets. These results
underscore the significant reduction in hallucinations withG-Retriever, particularly in the challenging
task of accurately citing both nodes and edges in graph-based contexts.
G Demonstrations
We demonstrate the interaction capabilities of G-Retriever with creative questions on different
datasets: ExplaGraphs, SceneGraphs, and WebQSP. These examples are showcased in Tables 13,
14, and 15, respectively. Additionally, the examples are visualized in Figure 1.
20



Source: data\tc16_2312.10997v5\referenced_papers\[84]_2402.07630.pdf (Page 8):

Table 4: Retrieval on graphs significantly improves efficiency.
Dataset Before Retrieval (Avg.) After Retrieval (Avg.)
# Tokens # Nodes Min/Epoch # Tokens # Nodes Min/Epoch
SceneGraphs 1,396 19 123.1 235 ( ↓83%) 5 ( ↓74%) 86.8 ( ↓29%)
WebQSP 100,627 1,371 18.7 610 ( ↓99%) 18 ( ↓99%) 6.2( ↓67%)
Table 5: Quantitative comparison of halluci-
nation on the SceneGraphs dataset.
Baseline G-Retriever
Valid Nodes 31% 77%
Valid Edges 12% 76%
Fully Valid Graphs 8% 62%
Table 6: Ablation study on the WebQSP dataset.
Method Hit@1 ∆G-Retriever
w/o Graph Encoder 54.62 ± 0.78 ↓ 22.51%
w/o Projection Layer 69.70 ± 0.68 ↓ 1.11%
w/o Textualized Graph 56.96 ± 1.83 ↓ 19.19%
w/o Retrieval 63.84 ± 0.41 ↓ 9.43%
6.3 Efficiency Evaluation
The efficiency of our approach is highlighted by the data in Table 4. Implementing our graph-based
retrieval significantly decreases the number of tokens required to describe the graphs in text, reduces
the number of nodes in graphs, and speeds up the training process. Specifically, for theSceneGraphs
dataset, tokens decreased by 83%, nodes by 74%, and training time by 29%. For the WebQSP dataset,
tokens decreased by 99%, nodes by 99%, and training time by 67%. These substantial reductions
demonstrate the method’s efficiency and potential in managing large-scale graph data.
6.4 Mitigation of Hallucination
To evaluate hallucination, we instructed the models to answer graph-related questions, specifically
identifying supporting nodes or edges from the graph. We manually reviewed 100 responses from
both our method and the baseline ( i.e., LLM with graph prompt tuning), verifying the existence
of the nodes and edges referenced in the model’s output within the actual graph. Table 5 shows
that G-Retriever significantly reduces hallucinations by 54% compared to the baseline, as our graph
retrieval ensures that the data is sourced directly from the actual graph, leading to less hallucination.
See Appendix F for details.
6.5 Ablation Study
In this ablation study, we assess the individual impact of key components within our pipeline. As
shown in Table 6, there are performance drops when any of these components are removed, with the
graph encoder and textualized graph showing declines of 22.51% and 19.19%, respectively. This
demonstrates their complementary effects in representing the graph in both textual and embedded
formats. Additionally, the retrieval on graphs is also important to the overall performance. Further
details are available in Appendix B.3. We also present additional studies on our framework: it is
robust to the choice of graph encoders (see Appendix B.4) and benefits from the increased scale of
LLMs (see Appendix B.5).
Additionally, we include a detailed comparison with existing retrieval methods (see Appendix D), a
discussion on the complexity (see Appendix E), and demonstrations on how to use G-Retriever to
‘chat with your graph’ (see Appendix G).
7 Conclusion
In this work, we introduce a new GraphQA benchmark for real-world graph question answering and
present G-Retriever, an architecture adept at complex and creative queries. Experimental results show
that G-Retriever surpasses baselines in textual graph tasks across multiple domains, scales effectively
with larger graph sizes, and demonstrates resistance to hallucination.
9



Source: data\tc16_2312.10997v5\referenced_papers\[84]_2402.07630.pdf (Page 1):

New Benchmark
GraphQA
New Architecture
G-Retriever
Techniques
Retrieval Augmented Generation
Graph Prompt Tuning
Graph Textualization
Sure! Here's an example of a 500-word advertisement
for the place based on the scene graph:
"Experience the breathtaking beauty of our place,
where the rolling hills meet the sparkling water of the
swimming pool. Take a leisurely stroll through the
lush green  elds, and let the gentle breeze caress
your face as you gaze out at the picturesque view. The
sheep graze peacefully in the distance, their white
coats glistening in the sunlight. The sky is a brilliant
blue, with  u y white clouds drifting lazily across.
Come and bask in the serene ambiance of our place,
where the natural beauty will leave you in awe."
Unified Conversational Interface
grazing on
above
eating
grass
water
moutains
covered
Scene Graph
sky
blue, clear, cloudly
sheep
white,  u y
Knowledge Graph
benjamin franklin
lighting rod
glass harmonica
franklin
stoveinventions
inventors
bifocals
inventions
inventions
women change
appearance
addiction
stop
positive
human
searching for
perfectioncapable of
created by
is not a
not capable of
cosmetic
surgery
receive action
Explanation Graph
used for
part of
The explanation graph provides a detailed analysis of
the reasons why women undergo cosmetic surgery.
According to the graph, the main reasons include the
desire to change one's appearance, the in uence of
societal beauty standards, and the addiction to
searching for perfection. These reasons are further
supported by the various nodes and edges in the
graph, which provide additional insights into the
motivations and consequences of cosmetic surgery.
Firstly, the node labeled "human" highlights the
fundamental human desire to feel attractive and
con dent in one's appearance...
Write a 500-word argument essay based on the
information provided in the explanation graph.
Write an 500-word advertisement for this place in the
scene graph that would make people want to visit it.
Great! Based on the provided knowledge graph,
here's an educational tour that highlights interesting
facts and connections related to Benjamin Franklin's
inventions and other notable achievements:
1. Franklin's Lightning Rod: Start by exploring
Franklin's most famous invention - the lightning rod.
Discover how Franklin's invention protected buildings
from lightning strikes and how it changed the way
people understood electricity...
What else did ben franklin invent? 
Surrounding this question, develop an educational
tour through the knowledge graph, highlighting
interesting facts and connections in an engaging way.
Figure 1: We develop a flexible question-answering framework targeting real-world textual graph ap-
plications via a unified conversational interface. Presented here are examples showcasing the model’s
adeptness in handling generative and creative queries in practical graph-related tasks: common sense
reasoning, scene understanding, and knowledge graph reasoning, respectively.
classification [8], or answering simple questions on small or synthetic graphs [44, 31]. In contrast,
we develop a flexible question-answering framework targeting complex and real-world graphs. This
framework enables users to ‘chat with their graph’ via a unified conversational interface, representing
a leap towards intuitive interaction with graph data, as demonstrated in Figure 1.
The Need for a Comprehensive GraphQA Benchmark. Question answering (QA) is a fundamen-
tally important task in natural language processing, serving as a key benchmark for assessing LLMs
and providing a unified interface for various capabilities. Despite extensive research in QA, a com-
prehensive benchmark specifically tailored for the graph modality is lacking. In contrast to existing
benchmarks that focus on basic graph-based reasoning tasks such as node degree, edge existence, and
shortest path [6, 44], our benchmark addresses complex and real-world graph applications including
common sense reasoning, scene understanding, and knowledge graph reasoning (refer to Figure 2).
This is vital for measuring progress toward a model capable of answering a wide range of questions
about graphs from diverse applications.
New Architecture for GraphQA. To enable effective and efficient graph QA, even on large graphs,
we propose G-Retriever, a new framework combining the strengths of GNNs, LLMs, and RAG
(Figure 3). Next, we will discuss the motivation, strengths, and details of our model.
Tackling Hallucination in Graph LLMs.LLMs are prone to hallucination, a phenomenon where the
generated content is factually inaccurate or nonsensical [12]. We validate the presence of this issue in
graph settings. In particular, we employ a baseline method that adapts MiniGPT-4 [57] to graphs,
where a frozen LLM interacts with a trainable GNN that encodes graph data as a soft prompt, as in
GraphToken [31]. Our findings, shown in Table 1, indicate that hallucination, an important problem
in text-based LLMs, is also prevalent in Graph LLMs. This may be attributed to the baseline’s
inability to recall the entire graph structure from a single graph embedding, leading to the generation
of incorrect nodes or edges during the QA task. In contrast, by employing RAG for direct information
retrieval from the actual graph, our G-Retriever mitigates this issue, as substantiated by Table 1.
Enhancing Scalability and Efficiency in Graph LLMs. Recent research endeavors have explored
translating graphs into natural language, such as by flattening nodes and edges into a text sequence,
enabling their processing by LLMs for graph-based tasks [56, 6]. However, this method faces critical
scalability issues. Converting a graph with thousands of nodes and edges into a text sequence results
in an excessive number of tokens, surpassing the input capacity of many LLMs. An alternative of
truncating the graph text sequence to fit the LLM’s input token limit leads to loss of information and
response quality. G-Retriever overcomes these issues with its RAG component, which allows for
effective scaling to larger graphs by selectively retrieving only relevant parts of the graph.
Tailoring the RAG Approach to Graphs. Existing RAG methodologies are primarily designed
for simpler data types or knowledge graphs, where information is retrieved in a manner isolated
from the graph structure [7, 1, 36, 16]. Hence, we introduce a new retrieval approach for general
textual graphs. Notably, we formulate subgraph retrieval as a Prize-Collecting Steiner Tree (PCST)
2



### Claim 64/179

#### Claim Text
Adaptive Retrieval Adaptive retrieval methods, exemplified by Flare [24] and Self-RAG [25], refine the RAG framework by enabling LLMs to actively determine the optimal moments and content for retrieval, thus enhancing the efficiency and relevance of the information sourced.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[24]_2305.06983.pdf (Page 2):

sidering the impressive performance achieved by
GPT-3.5 (Ouyang et al., 2022) on a variety of
tasks, we examine the effectiveness of our meth-
ods on text-davinci-003. We evaluate FLARE
on 4 diverse tasks/datasets involving generating
long outputs, including multihop QA (2WikiMul-
tihopQA), commonsense reasoning (StrategyQA),
long-form QA (ASQA), and open-domain summa-
rization (WikiAsp) (Ho et al., 2020; Geva et al.,
2021; Stelmakh et al., 2022; Hayashi et al., 2021).
Over all tasks, FLARE achieves superior or com-
petitive performance compared to single-time and
multi-time retrieval baselines, demonstrating the
effectiveness and generalizability of our method.
2 Retrieval Augmented Generation
We formally define single-time retrieval augmented
generation and propose the framework of active
retrieval augmented generation.
2.1 Notations and Definitions
Given a user input x and a document corpus D =
{di}|D|
i=1 (such as all Wikipedia articles), the goal of
retrieval augmented LMs is to generate the answer
y = [s1, s2, ...,sm] = [w1, w2, ..., wn] containing
m sentences or n tokens leveraging information
retrieved from the corpus.
In retrieval augmented LM, the LM typically
pairs with a retriever that can retrieve a list of
documents Dq = ret(q) for a query q; the LM
conditions on both the user input x and retrieved
documents Dq to generate the answer. Since we
focus on examining various methods of determin-
ing when and what to retrieve, we follow exist-
ing methods (Ram et al., 2023; Trivedi et al.,
2022) to prepend the retrieved documents before
the user input to aid future generation for both
baselines and our method for fair comparisons:
y = LM([Dq, x]), where [·, ·] is concatenation fol-
lowing the specified order.
2.2 Single-time Retrieval Augmented
Generation
The most common choice is to directly use the user
input as the query for retrieval and generate the
complete answer at once y = LM([Dx, x]).
2.3 Active Retrieval Augmented Generation
To aid long-form generation with retrieval, we pro-
pose active retrieval augmented generation. It is a
generic framework that actively decides when and
what to retrieve through the generation process,
resulting in the interleaving of retrieval and genera-
tion. Formally, at step t(t ≥ 1), the retrieval query
qt is formulated based on both the user input x and
previously generated output y<t = [y0, ...,yt−1]:
qt = qry(x, y<t),
where qry(·) is the query formulation function. At
the beginning (t = 1), the previous generation is
empty (y<1 = ∅), and the user input is used as the
initial query (q1 = x). Given retrieved documents
Dqt, LMs continually generate the answer until the
next retrieval is triggered or reaches the end:
yt = LM([Dqt, x, y<t]),
where yt represents the generated tokens at the cur-
rent step t, and the input to LMs is the concatena-
tion of the retrieved documents Dqt, the user input
x, and the previous generation y<t. We discard
previously retrieved documents ∪t′<tDqt′ and only
use the retrieved documents from the current step
to condition the next generation to prevent reaching
the input length limit of LMs.
3 FLARE: Forward-Looking Active
REtrieval Augmented Generation
Our intuition is that (1) LMs should only retrieve
information when they do not have the necessary
knowledge to avoid unnecessary or inappropriate
retrieval, and (2) the retrieval queries should reflect
the intents of future generations. We propose two
forward-looking active retrieval augmented gener-
ation (FLARE) methods to implement the active
retrieval augmented generation framework. The
first method prompts the LM to generate retrieval
queries when necessary while generating the an-
swer using retrieval-encouraging instructions, de-
noted as FLAREinstruct. The second method directly
uses the LM’s generation as search queries, denoted
as FLAREdirect, which iteratively generates the next
sentence to gain insight into the future topic, and
if uncertain tokens are present, retrieves relevant
documents to regenerate the next sentence.
3.1 FLARE with Retrieval Instructions
Inspired by Toolformer (Schick et al., 2023), a
straightforward way of expressing information
needs for retrieval is to generate “[Search(query)]”
when additional information is needed (Schick
et al., 2023), e.g., “The colors on the flag of
Ghana have the following meanings. Red is for
[Search(Ghana flag red meaning)] the blood of mar-
tyrs, ...” When working with GPT-3.5 models that



Source: data\tc16_2312.10997v5\referenced_papers\[24]_2305.06983.pdf (Page 14):

gon) including the following aspects: academics,
history.”, the output we aim to generate is “# Aca-
demics. In 2008, 91% of the school’s seniors re-
ceived their high school diploma... # History. The
class of 2008 was the 100th class in the school’s
history.” where # is used to indicate aspects. We
manually annotate 4 exemplars (Prompt D.10), and
use the Bing search engine to retrieve 5 documents
from the open web. To avoid leaking, we exclude
several Wikipedia-related domains listed in Table 8
from Bing’s search results.
C Hyperparameters
Hyperparameters of FLARE on different datasets
are listed in Table 9.
D Prompts and Few-shot exemplars
The prompt used to linearize multiple documents
is shown in Prompt D.1. The prompt used in self-
ask (Press et al., 2022) is shown in Prompt D.2.
Prompts and exemplars of different tasks/datasets
are shown in Prompt D.3, D.4, D.5, D.6, D.8, and
D.10, respectively.
Prompt D.1: document formatting
Search results:
[1] Document 1
[2] Document 2
...
The user input x
Prompt D.2: multihop QA with self-ask
Question: Who lived longer, Theodor Haecker or Harry
Vaughan Watkins?
Are follow up questions needed here: Yes.
Follow up: How old was Theodor Haecker when he died?
Intermediate answer: Theodor Haecker was 65 years old
when he died.
Follow up: How old was Harry Vaughan Watkins when he
died?
Intermediate answer: Harry Vaughan Watkins was 69 years
old when he died.
So the final answer is: Harry Vaughan Watkins.



Source: data\tc16_2312.10997v5\referenced_papers\[24]_2305.06983.pdf (Page 13):

A FLARE Implementation Details
FLAREinstruct implementation details We
found that LMs can effectively combine retrieval
and downstream task-related skills and generate
meaningful search queries while performing the
task. However, there are two issues: (1) LMs tend
to generate fewer search queries than necessary.
(2) Generating excessive search queries can
disrupt answer generation and adversely affect
performance. We address these issues using two
methods respectively. First, we increase the logit
of the token “[” by 2.0 to improve the chances
of LMs generating “[Search(query)]”. Second,
whenever LMs generate a search query, we use it
to retrieve relevant information, promptly remove
it from the generation, and generate the next few
tokens while forbidding “[” by adding a large
negative value to the logit of “[”.
The initial query of FLARE. FLARE starts
with the user input x as the initial query to re-
trieve documents to generate the first sentence
ˆs1 = LM([Dx, x]) to bootstrap the iterative gener-
ation process. For the following steps, the tempo-
rary forward-looking sentence is generated without
retrieved documents.
Sentence tokenization. For each step t, we gen-
erate 64 tokens which are longer than most sen-
tences, and use NLTK sentence tokenizer 5 to ex-
tract the first sentence and discard the rest.
Efficiency As shown in subsection 6.2, on aver-
age retrieval is triggered for 30% ∼ 60% of sen-
tences depending on downstream tasks. In compar-
ision, KNN-LM (Khandelwal et al., 2020) retrieves
every token, RETRO or IC-RALM (Borgeaud et al.,
2022; Ram et al., 2023) retrievers every 4∼32 to-
kens, and IRCoT (Trivedi et al., 2022) retrieves
every sentence. Compared to single-time retrieval,
however, interleaving retrieval and generation with
a naive implementation indeed increases overheads,
which we discuss in the limitation section (sec-
tion 9).
B Datasets and Settings
Datasets, metrics, and experimental settings are
summarized in Table 7.
5https://www.nltk.org/api/nltk.tokenize.
PunktSentenceTokenizer.html
Multihop QA For “Why did the founder of Ver-
sus die?”, the output we aim to generate is “The
founder of Versus was Gianni Versace. Gianni Ver-
sace was shot and killed on the steps of his Miami
Beach mansion on July 15, 1997. So the answer
is shot.” We use 8 exemplars from Trivedi et al.
(2022) listed in Prompt D.4 for in-context learn-
ing, BM25 as the retriever, and Wikipedia articles
as the retrieval corpus. Similar to the observation
in Trivedi et al. (2022), we found incorporating
retrieval results for exemplars improves the per-
formance, we use the input x of each exemplar to
retrieve several documents and then add them using
the format in Prompt D.1. We found increasing the
number of retrieval documents often increases per-
formance. Therefore, we use the maximum number
of documents that can fit within the input length
limit of text-davinci-003, which is 2 for 2Wiki-
MultihopQA.
Commonsense Reasoning For “Would a pear
sink in water?”, the output we aim to generate is
“The density of a pear is about 0.6g/cm3, which is
less than water. Objects less dense than water float.
Thus, a pear would float. So the final answer is no.”
We use 6 exemplars from Wei et al. (2022) listed in
Prompt D.5, BM25 on the Wikipedia corpus, and 3
retrieved documents to run experiments.
Long-form QA For “Where do the Philadelphia
Eagles play their home games?”, the output we
aim to generate is “We need to consider the dif-
ferent possible locations or venues that could be
considered the home field of the Philadelphia Ea-
gles. These include the city, the sports complex,
or the stadium. Therefore, this question has 3 in-
terpretations and the answers are: (1) The city is
Philadelphia. (2) The sports complex is the South
Philadelphia Sports Complex. (3) The stadium is
the Lincoln Financial Field stadium.” For both the
original setting (ASQA) and the setting with hints
(ASQA-hint), we manually annotate 8 exemplars
(Prompt D.6 and D.8), use BM25 on the Wikipedia
corpus, and 3 retrieved documents to run experi-
ments.
Open-domain Summarization The original
WikiAsp dataset is designed for multi-document
summarization and provides a list of references to
systems. We converted it into the open-domain
setting by removing the associated references and
instead gathering information from the open web.
For “Generate a summary about Echo School (Ore-



Source: data\tc16_2312.10997v5\referenced_papers\[24]_2305.06983.pdf (Page 8):

β EM F 1 Prec. Rec.
0.0 0.488 0.576 0.571 0.605
0.2 0.498 0.588 0.582 0.616
0.4 0.510 0.597 0.591 0.627
0.6 0.506 0.593 0.586 0.622
Table 5: Performance of FLARE with respect to the
masking threshold β on 2WikiMultihopQA.
ASQA-hint WikiAsp
EM D-F 1 R-L DR UniEval E-F 1 R-L
Implicit 45.7 36.9 37.7 37.3 53.4 18.8 27.7
Explicit 46.2 36.7 37.7 37.2 53.4 18.9 27.6
Table 6: A comparison between implicit and explicit
query formulation methods in FLARE.
thresholds β. Retrieving directly with the complete
sentence (β = 0) is worse than masking tokens
with low probabilities, confirming our hypothesis
that low-confidence erroneous tokens can distract
retrievers. We compare implicit and explicit query
formulation methods in Table 6. Performances of
both methods are similar, indicating that both meth-
ods can effectively reflect information needs.
7 Related Work
We refer to subsection 2.2 and section 4 for ex-
tensively discussion on single-time and multi-time
retrieval augmented LMs, which is the most rele-
vant area to this paper.
Iterative and adaptive retrieval Iterative re-
trieval and refinement has been studied in both
text and code generation tasks (Peng et al., 2023;
Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu
et al., 2023). FLARE differs from these methods in
the granularity of generation and retrieval strategies.
Adaptive retrieval has been studied in single-time
retrieval scenarios based on either question pop-
ularity or generation probabilities (Mallen et al.,
2022; Li et al., 2023), while we focus on long-form
generation requiring active information access.
Browser-enhanced LMs WebGPT (Nakano
et al., 2021) and WebCPM (Qin et al., 2023) train
LMs to interact with browser to enhance factuality
using reinforcement learning or supervised train-
ing where multiple queries can be triggered before
generation. FLARE is built on text-based retrievers
but can be combined with a browser to potentially
improve retrieval quality.
8 Conclusion
To aid long-form generation with retrieval aug-
mentation, we propose an active retrieval aug-
mented generation framework that decides when
and what to retrieve during generation. We imple-
ment this framework with forward-looking active
retrieval that iteratively uses the upcoming sentence
to retrieve relevant information if it contains low-
confidence tokens and regenerates the next sen-
tence. Experimental results on 4 tasks/datasets
demonstrate the effectiveness of our methods. Fu-
ture directions include better strategies for active
retrieval and developing efficient LM architectures
for active information integration.
9 Limitations
We also conduct experiments on Wizard of
Wikipedia (Dinan et al., 2019) and ELI5 (Fan et al.,
2019), and found that FLARE did not provide sig-
nificant gains. Wizard of Wikipedia is a knowledge-
intensive dialogue generation dataset where the out-
put is relatively short (∼20 tokens on average) so
retrieving multiple disparate pieces of information
might not be necessary. ELI5 (Fan et al., 2019)
is a long-form QA dataset requiring in-depth an-
swers to open-ended questions. Due to issues men-
tioned in Krishna et al. (2021) such as difficulties
of grounding generation in retrieval and evalua-
tion, both single-time retrieval and FLARE did not
provide significant gains over not using retrieval.
From an engineering perspective, interleaving gen-
eration and retrieval with a naive implementation
increases both overheads and the cost of generation.
LMs need to be activated multiple times (once for
each retrieval) and a caching-free implementation
also requires recomputing the previous activation
each time after retrieval. This issue can be poten-
tially alleviated with special architectural designs
that encode the retrieved documents Dqt and the
input/generation (x/y<t) independently.
Acknowledgements
This work was supported in part by a grant from
the Singapore Defence Science and Technology
Agency and the IBM PhD Fellowship. We thank
Chunting Zhou, Amanda Bertsch, Uri Alon, Hi-
roaki Hayashi, Harsh Trivedi, Patrick Lewis, Timo
Schick, Kaixin Ma, Shuyan Zhou, and Songwei Ge
for their insightful discussions and help with the
experiments.



Source: data\tc16_2312.10997v5\referenced_papers\[24]_2305.06983.pdf (Page 6):

0.0
20.0
40.0
60.0
80.0
2WikiMultihopQA StrategyQA ASQA ASQA-hint WikiAsp
No ret. Single-time ret. Previous-window ret. Forward-Looking Active REtrieval augmented generation (FLARE)
Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for
each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp.
following aspects: academics, history.” Experimen-
tal setting details are included in Appendix B.
Metrics include ROUGE, named entity-based F1,
and UniEval (Zhong et al., 2022) which measures
factual consistency.
6 Experimental Results
We first report overall results across 4 tasks/datasets
and compare the performance of FLARE with all
the baselines introduced in section 4. We then
run ablation experiments to study the efficacy of
various design choices of our method.
6.1 Comparison with Baselines
Overall results. The overall performance of
FLARE and baseline across all tasks/datasets are
reported in Figure 4. FLARE outperforms all base-
line on all tasks/datasets, indicating that FLARE
is a generic method that can effectively retrieve
additional information throughout the generation.
Among various tasks, multihop QA shows the
most significant improvement. This is largely due
to the task’s clear definition and specific objective
of producing the final answer through a 2-hop rea-
soning process, which makes it easier for LMs to
generate on-topic output. In contrast, ASQA and
WikiAsp are more open-ended, which increases the
difficulty of both generation and evaluation. The
improvement on ASQA-hint is larger than that of
ASQA because identifying ambiguous aspects is
challenging even for humans in many cases, and
providing a generic hint helps LMs to stay on topic.
Thorough comparisons with baselines. The per-
formance of all baselines on 2WikiMultihopQA
are reported in Table 1. FLARE outperforms all
baselines by a large margin, which confirms that
forward-looking active retrieval is highly effective.
Most multi-time retrieval augmented approaches
outperform single-time retrieval but with different
Methods EM F 1 Prec. Rec.
No retrieval 28.2 36.8 36.5 38.6
Single-time retrieval 39.4 48.8 48.6 51.5
Multi-time retrieval
Previous-window 43.2 52.3 51.7 54.5
Previous-sentence 39.0 49.2 48.9 51.8
Question decomposition 47.8 56.4 56.1 58.6
FLAREinstruct (ours) 42.4 49.8 49.1 52.5
FLAREdirect (ours) 51.0 59.7 59.1 62.6
Table 1: FLARE and baselines on 2WikiMultihopQA.
Previous-window (Borgeaud et al., 2022; Ram et al.,
2023), previous-sentence (Trivedi et al., 2022), and ques-
tion decomposition (Press et al., 2022; Yao et al., 2022)
methods are reimplemented for fair comparisons.
margins. The improvement of retrieving using the
previous sentence is relatively small which we hy-
pothesize is mainly because the previous sentence
often describes entities or relations different from
those in the next sentence in 2WikiMultihopQA.
While the previous-window approach might use
the first half of a sentence to retrieve information
potentially helpful for generating the second half.
Among all baselines, the question decomposition
approach (Press et al., 2022) achieves the best per-
formance. which is not surprising since the in-
context exemplars manually annotated with decom-
posed sub-questions (Prompt D.2) guide LMs to
generate sub-questions that align with the topic/in-
tent of future generations. FLARE outperforms
this baseline, indicating that manual exemplar an-
notation is not necessary for effective future-aware
retrieval. The gap between FLAREinstruct and ques-
tion decomposition is large, indicating that teaching
LMs to generate search queries using task-generic
retrieval instructions and exemplars is challenging.
We report all metrics for the other datasets in
Table 2. FLARE outperforms baselines with re-
spect to all metrics. Retrieval using the previ-



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 15):

Preprint.
APPENDIX
A S ELF -RAG Details 17
A.1 Reflection Tokens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.2 S ELF -RAG Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.3 S ELF -RAG Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B Experimental Details 19
B.1 More Details of Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.2 More Details of Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C Results 20
C.1 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C.2 Human Evaluation Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.3 Qualitative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
D Full List of Instructions and Demonstrations for GPT-4 21
16



Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 0):

Preprint.
SELF -RAG : L EARNING TO RETRIEVE , GENERATE , AND
CRITIQUE THROUGH SELF -REFLECTION
Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§
†University of Washington §Allen Institute for AI ‡IBM Research AI
{akari,zeqiuwu,yizhongw,hannaneh}@cs.washington.edu, avi@us.ibm.com
ABSTRACT
Despite their remarkable capabilities, large language models (LLMs) often produce
responses containing factual inaccuracies due to their sole reliance on the paramet-
ric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad
hoc approach that augments LMs with retrieval of relevant knowledge, decreases
such issues. However, indiscriminately retrieving and incorporating a fixed number
of retrieved passages, regardless of whether retrieval is necessary, or passages are
relevant, diminishes LM versatility or can lead to unhelpful response generation.
We introduce a new framework called Self-Reflective Retrieval-Augmented Gen-
eration (SELF -RAG) that enhances an LM’s quality and factuality through retrieval
and self-reflection. Our framework trains a single arbitrary LM that adaptively
retrieves passages on-demand, and generates and reflects on retrieved passages
and its own generations using special tokens, called reflection tokens. Generating
reflection tokens makes the LM controllable during the inference phase, enabling it
to tailor its behavior to diverse task requirements. Experiments show that SELF -
RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs
and retrieval-augmented models on a diverse set of tasks. Specifically, SELF -RAG
outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,
reasoning and fact verification tasks, and it shows significant gains in improving
factuality and citation accuracy for long-form generations relative to these models.1
1 I NTRODUCTION
State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)
despite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation
(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs
with relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,
2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce
unnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they
retrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,
the output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since
the models are not explicitly trained to leverage and follow facts from provided passages. This
work introduces Self-Reflective Retrieval-augmented Generation ( SELF -RAG) to improve an
LLM’s generation quality, including its factual accuracy without hurting its versatility, via on-demand
retrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on
its own generation process given a task input by generating both task output and intermittent special
tokens (i.e., reflection tokens). Reflection tokens are categorized into retrieval and critique tokens to
indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,
given an input prompt and preceding generations, SELF -RAG first determines if augmenting the
continued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that
calls a retriever model on demand (Step 1). Subsequently,SELF -RAG concurrently processes multiple
retrieved passages, evaluating their relevance and thengenerating corresponding task outputs (Step
2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms
of factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which
1Our code and trained models are available at https://selfrag.github.io/.
1
arXiv:2310.11511v1  [cs.CL]  17 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 9):

Preprint.
0 50 100 150
Num of training (k)
35
40
45
50
55Perfomance
(a) PopQA
0 100
Num of training (k)
71
72
73
 (b) PubHealth
0 100
Num of training (k)
40
60
 (c) ASQA (prec)
Pop Bio.
S & P 92.5 70.0
ISREL 95.0 90.0
ISSUP 90.0 85.0
(d) Human evaluation on PopQA
and Bio generation.
Figure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect
of the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)
Human analysis on SELF -RAG outputs as well as reflection tokens.
contrary, a larger weight results in lower MAUVE scores: when generation gets longer and more
fluent, there are often more claims that are not fully supported by citations, consistent with findings
by Liu et al. (2023a). Our framework lets practitioners choose and customize models’ behaviors at
test time by adjusting such parameters without requiring additional training.
Efficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval
occurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects
overall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers
of threshold δ (larger δ results in less retrieval) on PubHealth and PopQA. Figure 3c shows that
the model’s retrieval frequencies dramatically change on both datasets. as δ varies. On one hand,
performance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.
Effects of training data size. We conduct an analysis of how the data scale affects the model’s
performance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original
150k training instances, and fine-tune four SELF -RAG 7B variants on those subsets. Then, we compare
the model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF -
RAG trained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models’
performance trained on different amount of data. Across all datasets, increasing data size often shows
upward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do
not observed such significant improvements on Llama2-FT7B when increasing the training data from
50k to 150k. These results also indicate that further expanding the training data of SELF -RAG may
lead to further improvements, although in this work we limit our training data size to 150k.
Human evaluations. We conduct small human evaluations on SELF -RAG outputs, as well as the
reliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio
results. Following Menick et al. (2022), human annotators evaluate S&P, which indicates whether
the model output is plausible (i.e., the output is a reasonable and on-topic response to the question
as if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to
verify the validity of the answer). For S&P, we do not consider the instances where SELF -RAG
predicts irrelevant or no support. We then ask our annotators whether the model-predicted
reflection tokens about ISREL and ISSUP match their inspections (e.g., whether the fully supported
output is supported by the cited evidence). Human annotators find SELF -RAG answers are often
plausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is
consistent with Menick et al. (2022). Human annotators also find ISREL and ISSUP reflection token
predictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated
examples and explanations on assessments.
6 C ONCLUSION
This work introduces SELF -RAG, a new framework to enhance the quality and factuality of LLMs
through retrieval on demand and self-reflection. SELF -RAG trains an LM to learn to retrieve, generate,
and critique text passages and its own generation by predicting the next tokens from its original
vocabulary as well as newly added special tokens, called reflection tokens.SELF -RAG further enables
the tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on
six tasks using multiple metrics demonstrate that SELF -RAG significantly outperforms LLMs with
more parameters or with conventional retrieval-augmented generation approaches.
10



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 5):

111:6 Lyu, et al.
2.2 RAG Benchmarks
When investigating the development and optimization of RAG, the effective evaluation of their
performance becomes a fundamental concern. Table 1 shows some commonly used benchmarks
for evaluating RAG. LangChain provides benchmark tasks, such as LangChain Docs Q&A and
Semi-structured Reports [27], designed to assess various RAG architectures. These datasets are
constructed from snapshots of Python documentation and PDFs containing tables and charts.
They emphasize the model’s capability to handle structured and semi-structured data. Evaluation
standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing
large models for question-answering generation has emerged as a prevalent approach in building
evaluation datasets. For instance, RGB [8] creates its evaluation dataset by gathering recent news
reports and employing LLM to generate relevant events, questions, and answers. Conversely,
ARES [44]. relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
These methods not only showcase the RAG system’s proficiency in handling real-time data but also
illustrate the utility of automation and synthetic data in the evaluation process. For evaluating the
capabilities of models across various professional domains, the Instruct-Benchmark-Tester dataset
encompasses a range of question types, with a particular focus on financial services, legal, and
intricate business scenarios [39].
Depending on whether the evaluation phase incorporates ground truth, metrics of existing
evaluation methods can be categorized into those necessitating reference and those not requiring
it. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by
contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-
Benchmark-Tester [39] employs accuracy score as an evaluation metric, a widely acknowledged
measure of model performance that assesses the extent to which model-generated answers align
with reference answers. The primary objective of RGB [8] is to evaluate whether large models can
effectively utilize external documents to acquire knowledge and generate accurate answers. Its
evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.
Reference-free evaluation methods, including TruLens-Eval [14], RAGAS [13], and ARES [44],
provide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning
context relevance, answer faithfulness, and answer relevance. TruLens-Eval [14] introduces the
RAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,
encompassing context relevance, groundedness, and answer relevance. RAGAS [13], serving as
a reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity
to identify pertinent and concentrated context passages, along with the LLMs’ proficiency in
faithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a
predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each
aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy
when compared to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-
powered inference to offer statistical assurances for its scoring, generating confidence intervals.
ARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer
relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts
and producing both faithful and relevant answers. Regarding evaluation methods, [32] places an
emphasis on assessing the credibility and accuracy of responses generated by generative search
engines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including
high costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,
exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely
approximates manual evaluation outcomes.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 1):

111:2 Lyu, et al.
Additional Key Words and Phrases: Retrieval-Augmented Generation, Large Language Models, Evaluation
ACM Reference Format:
Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong
Xu, and Enhong Chen. 2018. CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented
Generation of Large Language Models. J. ACM 37, 4, Article 111 (August 2018), 31 pages. https://doi.org/
XXXXXXX.XXXXXXX
1 INTRODUCTION
Retrieval-augmented generation (RAG) is an advanced technique that leverages external knowledge
sources to enhance the text generation capabilities of large language models(LLMs). It retrieves
relevant paragraphs from a corpus based on the input, and feeds them to the LLMs along with the
input. With the help of external knowledge, LLMs can generate more accurate and credible responses
and effectively address challenges such as outdated knowledge [19], hallucinations [3, 9, 35, 62],
and lack of domain expertise [30, 46]. Therefore, RAG technology is attracting increasing attention.
Although the effectiveness of retrieval-augmented strategies has been proven through extensive
practice, their implementation still requires a significant amount of tuning. The overall performance
of the RAG system is affected by multiple factors, such as the retrieval model, construction of the
external knowledge base, and language model. Therefore, automatic evaluation of RAG systems
is crucial. Currently, there are only a few existing benchmarks for evaluating RAG performance,
as creating high-quality datasets and experimenting with them entail significant costs. These
benchmarks can be classified into two types: reference-required and reference-free evaluation.
Reference-free evaluation frameworks, such as RAGAS [13] and ARES [44], use LLM-generated
data to evaluate RAG systems on contextual relevance, faithfulness, and informativeness. These
frameworks do not depend on ground truth references, but only assess the coherence of the
generated text with the retrieved context. This approach may be unreliable if the retrieved external
information is low-quality.
Consequently, reference-required evaluations remain the predominant method for assessing RAG
systems. Existing benchmarks for reference-required evaluations, such as RGB [8] and NQ [26].
do have their limitations. First, they all rely on question answering tasks to measure the perfor-
mance of RAG systems. Question answering is not the only RAG application scenario, and an
optimization strategy that works well for question answering may not be generalized to other
scenarios. Thus, these benchmarks may not capture the full potential of RAG systems. Second, in
the experiments, current evaluations usually focus on evaluating the LLM part of the RAG pipeline,
or focus on retriever performance in the knowledge-intensive scenario [40], while ignoring the
retrieval methods in non-knowledge-intensive scenarios and external knowledge base construction.
These components are also crucial for RAG systems. Therefore, a comprehensive evaluation of the
RAG system may not be obtained using any existing benchmarks.
To evaluate the performance of RAG in different application scenarios, we need a comprehensive
benchmark that covers more than just the question-answering task. Lewis et al. [28] argue that the
core of RAG systems is their interactive way of combining LLMs with external knowledge sources.
And following [25], we can group any interaction with external knowledge sources into four basic
actions: create, read, update, and delete, which are also known as CRUD actions [48]. Therefore,
we can use the CRUD framework to classify the RAG systems’ application scenarios. As shown in
Figure 1, each CRUD category demonstrates different capabilities of the RAG system:
•In "CREATE", the system improves the input text by adding relevant information from
external sources, making creative outputs such as poetry, stories, or code.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



### Claim 65/179

#### Claim Text
These methods are part of a broader trend wherein LLMs employ active judgment in their operations, as seen in model agents like AutoGPT, Toolformer, and GraphToolformer [107]–[109].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 1):

Jiawei Zhang
A connected graph
(Lollipop graph)
Bibliographic network
Molecular graph
Recommender system
Online social network
Knowledge graph
ChatGPT (Prompt Augmentation)
Graph  
Reasoning  
Toolkits
Causal LLMs
GPT-J 
(6B)
LLaMA 
(7B)
In the lollipop graph, its node number is 
<API>GR(GL(“lollipop-graph”), “toolx:order”)
—>10</API>  10.
The function of insulin protein is to   
<API>GR(GL(“insulin-graph”), “seg-bert:function”) —>  
“control blood glucose”</API> control blood glucose.
According to Wikipedia, from 2017 to 2021, 
Donald Trump was the president of
<API>GR(GL(“wikipedia-knowledge-graph”), 
“transe:tail_entity”, “Donald Trump”, “president 
of”) —>  “United States”</API> United States.
… …Graph property reasoning
Protein molecular graph function reasoning Knowledge graph reasoning
1
0
2
3
4
5
6
7
9
8
Figure 1: An Illustration of LLMs based Graph Reasoning Tasks. Based on the input graph data from various domains and
a handful number of prompt examples with brief instructions, we propose to use ChatGPT to annotate and augment a large
prompt dataset that contains graph reasoning API calls of external graph reasoning tools. The generated prompt dataset will be
used to fine-tune the existing pre-trained LLMs, like GPT-J or LLaMA, to teach them to automatically use the most appropriate
external API tools for accomplishing the input graph reasoning tasks.
since graph learning may involve (1) lots of precise mathematical
calculations of graph properties, (2) multi-hop logical reasoning
through the links, (3) capturing the extensively connected graph
spatial and topological structures, and (4) sometimes we also need
to handle the dynamics of graphs that are changing with time. Care-
ful readers may have noticed that these requirements mentioned
for graph learning actually hit the nail on the head, which exactly
correspond to the weaknesses of the current LLMs we mentioned
at the very beginning.
Regardless of the potential challenges ahead of us, “an AGI with-
out graph reasoning ability will never be the AGI we may desire ”.
Based on such motivations, we write this paper trying to incorpo-
rate graph data into LLMs for various graph reasoning tasks. On
the one hand, we really hope the currently AI-leading companies
like OpenAI, Microsoft, Google and Meta can take graph structured
data reasoning into consideration when they develop their missions
and plans for achieving the AGI, so that the graph learning com-
munity will be able to contribute our efforts to building the AGI
system together with the language and vision communities. On
the other hand, we also hope to empower the existing LLMs with
the ability to overcome the weaknesses in their performance when
handling graph structured data for complex graph reasoning tasks.
So, the latest developed LLMs can also benefit the graph learning
community for solving various graph reasoning tasks as well.
Considering the current language models and their extremely
high pre-training costs, we cannot fundamentally re-design a new
LLM with pre-training to equip them with the graph reasoning
capabilities. Pre-training such LLMs from scratch is an infeasible
task for most research groups in academia and majority of compa-
nies in the industry as well. To adapt to the common practices of
NLP approaches, we will introduce the Graph Reasoning oriented
Toolformer framework (Graph-ToolFormer) by fine-tuning some
existing pre-trained LLMs ( e.g., GPT-J or LLaMA) in this paper.
Technically, as illustrated in Figure 1, based on the latest ChatGPT
from OpenAI and Toolformer model from Meta [43], we propose
to provide the existing pre-trained LLMs ( e.g., GPT-J or LLaMA)
with the ability to perform various complex graph reasoning tasks
by allowing them to use external graph learning tools, such as
other pre-trained graph neural network models and existing graph
reasoning toolkits. Instead of manually hard-coding the graph data
loading and external graph learning tool usage function calls in the
reasoning statements, to make Graph-ToolFormer as a general
graph reasoning interface, we will fine-tune the LLMs to teach the
models to decide not only where to retrieve the graph data, but also
what tools to be used, as well as when and how to use these tools.



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 2):

Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT
More technical details about the Graph-ToolFormer model will
be introduced in the following methodology section.
As the first exploration attempt to use LLMs for general graph
reasoning tasks, we summarize the contributions of this paper as
follows:
•Graph Reasoning with LLMs : This paper is the first pa-
per that attempts to propose a general LLM, i.e., Graph-
ToolFormer, that can handle graph reasoning tasks. It effec-
tively remedies the weaknesses of existing LLMs on graph
reasoning. More importantly, it helps bridge the graph learn-
ing community with the latest development on LLMs and
AIGC led by the language and vision learning communities.
So people in the graph learning community will also have the
stage to demonstrate our skills and expertises in the current
era of AIGC and the future era AGI.
•Graph Reasoning Prompt Dataset : In this paper, we cre-
ate a handful number of human-written language instruc-
tions and prompt examples of how graph learning tools can
be used. Based on the self-supervised in-context learning,
we use ChatGPT to annotate and augment a large graph
reasoning dataset with API calls of different external graph
learning tools, which will also be post-processed with selec-
tive filtering. Via the github page1, we have released both
the graph raw datasets and the generated graph reasoning
prompt dataset used in this paper with the community for
future explorations.
•Extensive Experimental Studies : We have extensively
tested the effectiveness of our proposedGraph-ToolFormer
with various graph reasoning based application tasks stud-
ied in the real-world, which include the most basic graph
data loading and general graph property computation tasks,
as well as some more advanced ones. Specifically, we study
several challenging advanced graph reasoning tasks in the
experiments, which include paper topic inference in bib-
liographic networks, molecular graph function prediction,
online social network community detection, personalized
sequential recommendation in recommender systems and
knowledge graph entity and relation reasoning.
The remaining sections of this paper are organized as follows. We
will briefly introduce the related work in Section 2. The definitions
of some terminologies and the formulation of the studied problem
will be provided in Section 3. A detailed introduction about the
Graph-ToolFormer framework will be provided in Section 4. The
effectiveness of Graph-ToolFormer will be tested with extensive
experiments on real-world benchmark graph datasets in Section 5.
Finally, we will conclude this paper in Section 6 and briefly discuss
about some potential future exploration directions in Section 7.
2 RELATED WORK
In this section, we will discuss about several research topics that
are related to our Graph-ToolFormer framework proposed in
this paper, which include graph neural networks , language models ,
language model based graph learning and prompt tuning .
1https://github.com/jwzhanggy/Graph_Toolformer/tree/main/data
2.1 Graph Neural Networks
Graph neural networks (GNNs) aim to learn the embedding repre-
sentations of the graph structured data. Representative examples
of GNNs proposed already include GCN [19] and Graph-Bert [60],
based on which various extended variants [20, 46, 51] have been
introduced as well. As mentioned above, GCN and its variant mod-
els are all based on the approximated graph convolutional operator
[13], which may lead to the suspended animation problem [59] and
over-smoothing problem [23] for deep model architectures. Theo-
retic analyses of the causes are provided in [12, 23, 59]. To handle
such problems, [59] generalizes the graph raw residual terms and
proposes a method based on graph residual learning; [23] proposes
to adopt residual/dense connections and dilated convolutions into
the GCN architecture. Besides the GCN and Graph-Bert based mod-
els, several other work [17, 46] also seeks to involve the recurrent
network for deep graph representation learning instead.
2.2 Language Models
Since the proposal of Transformer [ 50], large language models
(LLMs) have become the dominant deep model for various NLP
tasks. Assisted with pre-training, the giant tech-companies have
also introduced their own versions of different LLMs, like BERT
from Google [8], BART from Meta [22], GPT from OpenAI [5, 38, 39],
ELMo from AI2 [37] and MT-DNN from Microsoft [25]. Many of
these LLMs have also been open-sourced with both model algo-
rithm and learned parameters released to the community for both
research and application purposes. One research paper closely re-
lated to this work is Toolformer [43] from Meta, which proposes
to incorporate external APIs into language models. Equipped with
such external APIs, the models will be able to automatically decide
how to use which tool. Meanwhile, even prior to the Toolformer
model, several other previous papers [29, 35] have also explored to
augment language models with external tools.
2.3 Prompt Tuning
Prompts have been shown to be effective in tuning the pre-trained
language models with zero-shot or few-shot learning [ 5], which
can help language models learn faster than traditional fine tuning
tasks. By now, we have witnessed three categories of prompt tuning
approaches, i.e.,, discrete prompts [44], continuous prompts [24] and
priming [5]. Discrete prompts [44] reformat data instances with
some template text, like,
“{ premise } Should we assume that { hypothesis }? [prediction] ”.
Discrete prompts will typically tune all parameters of the model.
On the other hand, continuous prompts [24] will prepend examples
with embedding vectors of special tokens, which will only update
a much smaller set of model parameters. Very different from the
discrete and continuous prompts, priming [5] initially adopted in
GPT-3 will prepend several priming examples to the target evalua-
tion example instead, like
“Example 1: { sentence 1 } True or False? { label 1 }.
Example 2: { sentence 2 } True or False? { label 2 }.
···
Example k: { sentence k } True or False? { label k }.
Evaluation: { eval-sentence } True or False? [prediction] . ”



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 0):

Graph-ToolFormer: To Empower LLMs with Graph Reasoning
Ability via Prompt Augmented by ChatGPT
Jiawei Zhang
jiawei@ifmlab.org
IFM Lab
Department of Computer Science,
University of California, Davis
Davis, California, USA
https://github.com/jwzhanggy/Graph_Toolformer
ABSTRACT
In this paper, we aim to develop a large language model (LLM) with
the reasoning ability on complex graph data. Currently, LLMs have
achieved very impressive performance on various natural language
learning tasks, extensions of which have also been applied to study
the vision tasks with data in multiple modalities. However, when
it comes to the graph learning tasks, existing LLMs present very
serious flaws due to their inherited weaknesses in performing pre-
cise mathematical calculation , multi-step logic reasoning , perception
about the spatial and topological factors , and handling the temporal
progression.
To address such challenges, in this paper, we will investigate
the principles, methodologies and algorithms to empower existing
LLMs with the graph reasoning ability, which will have tremendous
impacts on the current research of both LLMs and graph learning.
Inspired by the latest ChatGPT and Toolformer models, we propose
the Graph-ToolFormer (Graph Reasoning oriented Toolformer)
framework to teach LLMs themselves with prompts augmented by
ChatGPT to use external graph reasoning API tools. Specifically,
we will investigate to teach Graph-ToolFormer to handle various
graph data reasoning tasks in this paper, including both (1) very
basic graph data loading and graph property reasoning tasks , ranging
from simple graph order and size to the graph diameter and periph-
ery, and (2) more advanced reasoning tasks on real-world graph data ,
such as bibliographic paper citation networks, protein molecular
graphs, sequential recommender systems, online social networks
and knowledge graphs.
Technically, to build Graph-ToolFormer, we propose to hand-
craft both the instruction and a small amount of prompt templates
for each of the graph reasoning tasks, respectively. Via in-context
learning, based on such instructions and prompt template examples,
we adopt ChatGPT to annotate and augment a larger graph reason-
ing statement dataset with the most appropriate calls of external API
functions. Such augmented prompt datasets will be post-processed
with selective filtering and used for fine-tuning existing pre-trained
causal LLMs, such as the GPT-J and LLaMA, to teach them how to
use graph reasoning tools in the output generation. To demonstrate
the effectiveness of Graph-ToolFormer, we conduct extensive
experimental studies on various graph reasoning datasets and tasks,
and have also launched a LLM demo with various graph reasoning
abilities. All the source code of Graph-ToolFormer framework,
the demo for graph reasoning, and the graph and prompt datasets
have been released online at the project github page.
KEYWORDS
Tool Transformer; ChatGPT; In-Context Learning; Language Model;
Graph Learning
1 INTRODUCTION
In recent years, large language models (LLMs) [ 8, 34, 50] have
achieved very impressive performance on a variety of natural lan-
guage processing tasks [32, 34, 49], extensions of which have also
been extensively applied to solve many other problems with data
in different modalities as well [10, 32, 40, 41]. With the launch of
ChatGPT and new Microsoft Bing Chat based on both GPT-3.5 and
GPT-4, LLMs have also been widely used in people’s daily produc-
tion and life. At the same time, due to their inherent limitations,
these LLMs have also received lots of criticisms in their usages due
to their inherited weaknesses, like inability in performing precise
calculations [36], difficulty in addressing multi-step logic reasoning
problems [6], incapable to conduct spatial and topological reasoning
[1], and unawareness of progression of temporal factors [9].
With the parallel development of natural language processing
and computer vision, transformer based deep learning models on
graph structured data has also received lots of attention from the
community in recent years [16, 56, 60]. Graph provides a unified
representation for many inter-connected data in the real-world,
which models both the diverse attributes of the nodes and the ex-
tensive links connecting the nodes with each other. Besides the
classic graph structures we learn from the discrete math and algo-
rithm courses, as shown in Figure 1, lots of real-world data can also
be modeled as graphs [ 45], like bibliographic networks [47], pro-
tein molecular graphs [52], recommender systems [28], online social
networks [31], and knowledge graphs [18].
Meanwhile, compared with the prosperous research explorations
on incorporating vision and language data into LLMs for designing
the ambitious AGI development plan [ 33], it seems researchers
have either “unintentionally” or “intentionally” ignored the widely
existed graph data and don’t seem to have any plans to include
them into the LLMs building for achieving the AGI.
Here, we say researchers have “unintentionally” ignored graphs,
since compared with texts and images that we deal with everyday,
graph has long-time been merely used as an intermediate model-
ing data structure for real-world data and we normally have no
direct interactions with graph actually. It is natural that people
may mistakenly think graph should not be the focus at the current
stage for creating AIGC and building the AGI systems. At the same
time, we say researchers may have “intentionally” ignored graphs,
arXiv:2304.11116v3  [cs.AI]  11 May 2023



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 22):

Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT
Table 4: A summary of the experimental results of Graph-ToolFormer on various graph reasoning tasks on the correspond-
ing benchmark datasets. The results are evaluated by the Rouge scores, BLEU and BP scores. Except for the graph loading task,
we also evaluate the results on other tasks/datasets by comparing the graph reasoning API calls (other textual contents are
excluded) with the ground-truth API calls, and report the Accuracy on reasoning API calls in the table as well.
Tasks Datasets Methods Evaluation Metrics
Rouge-1 Rouge-2 Rouge-L Rouge-LSum BLEU BP API-Gen Acc
Graph
Loading GL-Prompt Graph-ToolFormer 82.28 67.74 70.93 70.85 63.53 89.98 4.38
Property
Reasoning GPR-Prompt Graph-ToolFormer 94.56 92.10 91.69 91.69 91.53 99.93 80.00
Paper
Topic
Reasoning
Cora Graph-ToolFormer 99.69 99.68 99.69 99.69 99.2 100.0 100.0
Citeseer Graph-ToolFormer 100.0 100.0 100.0 100.0 99.39 100.0 97.5
Pubmed Graph-ToolFormer 99.91 99.84 99.91 99.91 99.04 100.0 99.38
Molecule
Function
Reasoning
PROTEINS Graph-ToolFormer 99.61 99.19 99.61 99.61 98.27 100.0 100.0
PTC Graph-ToolFormer 100.0 100.0 100.0 100.0 98.52 100.0 100.0
NCI1 Graph-ToolFormer 100.0 100.0 100.0 100.0 98.28 100.0 100.0
MUTAG Graph-ToolFormer 100.0 100.0 100.0 100.0 98.72 100.0 100.0
Sequential
Recommendation
Reasoning
MovieLens Graph-ToolFormer 97.47 96.56 97.47 97.47 94.63 95.31 93.12
Last.FM Graph-ToolFormer 89.24 86.69 88.75 88.79 83.43 89.67 85.62
Amazon Graph-ToolFormer 99.9 99.8 99.9 99.9 99.74 100.0 100.0
Social
Community
Reasoning
Foursquare Graph-ToolFormer 98.6 98.01 98.51 98.46 97.41 100.0 95.0
Twitter Graph-ToolFormer 99.86 99.71 99.78 99.76 99.75 99.89 98.75
Knowledge
Graph
Reasoning
Freebase Graph-ToolFormer 91.98 91.79 91.97 92.0 78.17 78.29 53.75
WordNet Graph-ToolFormer 98.73 98.73 98.73 98.73 97.99 98.69 96.88
•System and Software : We run the experiment on Ubuntu
22.04, with CUDA toolkit version 11.8, Nvidia Driver version
520, PyTorch version 1.13.1 and Python 3.9. For the optimizer
of Graph-ToolFormer (GPT-J 6B, 8bit), we use the 8-bit
AdamW from bitsandbytes with version 0.37.1. We load the
pre-trained GPT-J 6B 8bit from Huggingface with weight pa-
rameter checkpoint “hivemind/gpt-j-6B-8bit” and config/tok-
enizer checkpoint “EleutherAI/gpt-j-6b” as the base model of
Graph-ToolFormer, and the installed transformer toolkit
version is 4.28.0.dev0. More information about other sys-
tem and software configurations can be found at the shared
anaconda environment file4.
•Hyper-parameters: For fine-tuning Graph-ToolFormer
(GPT-J 6B, 8bit), we use AdamW with a very small learning
rate 1e-5 with weight decay 1e-2, and a max-epoch of 3. Both
the training and testing instances are divided into batches
with shuffle with batch size 32 and we set the max input/out-
put token length as 128. For the generation function of the
4https://github.com/jwzhanggy/Graph_Toolformer/blob/main/environment.yml
language model, the following hyper-parameters are used,
i.e., num-beams: 5, top-k: 5, top-p: 0.95, temperature: 1.9,
num-return-sequence: 1, max-length: 128.
Especially, when the batch size and input/output max token
length are assigned with small values (e.g., batch-size: 1 or 2 and
max-length: 64), we can also fine-tune Graph-ToolFormer (GPT-J
6B, 8bit) model on GPUs with smaller RAM (like Nvidia 1080ti with
11GB memory). It will allow most research groups and individuals
to tune and deploy Graph-ToolFormer to provide LLMs based
graph reasoning functions and services.
5.4.2 Performance Evaluation. For the preliminary performance
evaluation of Graph-ToolFormer, we have used several evaluation
metrics as follows in the experiments:
•ROUGE scores : By comparing the outputs of the Graph-
ToolFormer framework with the ground-truth, we calculate
the Rouge-1, Rouge-2, Rouge-L and Rouge-LSum scores ob-
tained by the model.



Source: data\tc16_2312.10997v5\referenced_papers\[107]_2306.02224.pdf (Page 6):

Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions
adaptability of this approach, especially for tasks with eas-
ily collectible training data for action policy. The code of this
work is shared in: https://github.com/younghuman/LLMAgent
REFERENCES
[1] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson
Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, and
Cameron McKinnon. 2022. Constitutional AI: Harmlessness from AI
Feedback. In arXiv:2212.08073. arXiv. https://ar5iv.org/abs/2212.08073
[2] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Jo-
hannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat
Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi,
Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial
General Intelligence: Early experiments with GPT-4. (March 2023).
https://www.microsoft.com/en-us/research/publication/sparks-of-
artificial-general-intelligence-early-experiments-with-gpt-4/
[3] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,
Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,
Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language
modeling with pathways. arXiv preprint arXiv:2204.02311(2022).
[4] Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian
Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri,
Mahmoud Adada, et al. 2018. Textworld: A learning environment for
text-based games. (2018), 41–75.
[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
2018. Bert: Pre-training of deep bidirectional transformers for language
understanding. arXiv preprint arXiv:1810.04805(2018).
[6] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar
Khot. 2023. Complexity-Based Prompting for Multi-Step Reasoning.
arXiv:2210.00720 [cs.CL]
[7] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for
self-supervised learning of language representations. arXiv preprint
arXiv:1909.11942 (2019).
[8] Yang Li, Zhi-Ping Cai, and Hong Xu. 2018. LLMP: exploiting LLDP
for latency measurement in software-defined data center networks.
Journal of Computer Science and Technology33 (2018), 277–285.
[9] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu,
Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, et al. 2023. Taskmatrix. ai:
Completing tasks by connecting foundation models with millions of
apis. arXiv preprint arXiv:2303.16434(2023).
[10] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi
Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
2019. Roberta: A robustly optimized bert pretraining approach. arXiv
preprint arXiv:1907.11692(2019).
[11] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
[12] Joon Sung Park, Joseph C. O’Brien, Carrie Cai, Meredith Ringel Morris,
Percy Liang, and Michael Bernstein. 2023. Generative Agents: Interac-
tive Simulacra of Human Behavior. https://arxiv.org/pdf/2304.03442.
pdf
[13] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez.
2023. Gorilla: Large Language Model Connected with Massive APIs.
arXiv:2305.15334 [cs.CL]
[14] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng
Gao. 2022. Instruction Tuning with GPT-4. https://ar5iv.org/abs/2304.
03277
[15] Stephane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell. 2011. A
Reduction of Imitation Learning and Structured Prediction to No-Regret
Online Learning. arXiv preprint arXiv:1011.0686(2011). arXiv:1011.0686
[16] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and
Yueting Zhuang. 2023. HuggingGPT: Solving AI Tasks with ChatGPT
and its Friends in HuggingFace. arXiv:2303.17580 [cs.CL]
[17] Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an
autonomous agent with dynamic memory and self-reflection. arXiv
preprint arXiv:2303.11366(2023).
[18] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Win-
son Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020.
ALFRED: A Benchmark for Interpreting Grounded Instructions for
Everyday Tasks. arXiv preprint arXiv:1912.01734(2020).
[19] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk,
Adam Trischler, and Matthew Hausknecht. 2021. AlfWorld: Align-
ing Text and Embodied Environments for Interactive Learning. arXiv
preprint arXiv:2010.03768(2021).
[20] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sha-
ran Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-
Consistency Improves Chain of Thought Reasoning in Language Mod-
els. arXiv:2203.11171 [cs.CL]
[21] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits
reasoning in large language models. arXiv preprint arXiv:2201.11903
(2022).
[22] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale
Schuurmans. 2023. Foundation Models for Decision Making: Problems,
Methods, and Opportunities. arXiv e-prints(2023), arXiv–2303.
[23] Ilan Yaniv. 2004. The benefit of additional opinions. Current directions
in psychological science13, 2 (2004), 75–78.
[24] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022.
Webshop: Towards scalable real-world web interaction with grounded
language agents. arXiv preprint arXiv:2207.01206(2022).
[25] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik
Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and
acting in language models. arXiv preprint arXiv:2210.03629(2022).
A APPENDIX



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 1):

Jiawei Zhang
A connected graph
(Lollipop graph)
Bibliographic network
Molecular graph
Recommender system
Online social network
Knowledge graph
ChatGPT (Prompt Augmentation)
Graph  
Reasoning  
Toolkits
Causal LLMs
GPT-J 
(6B)
LLaMA 
(7B)
In the lollipop graph, its node number is 
<API>GR(GL(“lollipop-graph”), “toolx:order”)
—>10</API>  10.
The function of insulin protein is to   
<API>GR(GL(“insulin-graph”), “seg-bert:function”) —>  
“control blood glucose”</API> control blood glucose.
According to Wikipedia, from 2017 to 2021, 
Donald Trump was the president of
<API>GR(GL(“wikipedia-knowledge-graph”), 
“transe:tail_entity”, “Donald Trump”, “president 
of”) —>  “United States”</API> United States.
… …Graph property reasoning
Protein molecular graph function reasoning Knowledge graph reasoning
1
0
2
3
4
5
6
7
9
8
Figure 1: An Illustration of LLMs based Graph Reasoning Tasks. Based on the input graph data from various domains and
a handful number of prompt examples with brief instructions, we propose to use ChatGPT to annotate and augment a large
prompt dataset that contains graph reasoning API calls of external graph reasoning tools. The generated prompt dataset will be
used to fine-tune the existing pre-trained LLMs, like GPT-J or LLaMA, to teach them to automatically use the most appropriate
external API tools for accomplishing the input graph reasoning tasks.
since graph learning may involve (1) lots of precise mathematical
calculations of graph properties, (2) multi-hop logical reasoning
through the links, (3) capturing the extensively connected graph
spatial and topological structures, and (4) sometimes we also need
to handle the dynamics of graphs that are changing with time. Care-
ful readers may have noticed that these requirements mentioned
for graph learning actually hit the nail on the head, which exactly
correspond to the weaknesses of the current LLMs we mentioned
at the very beginning.
Regardless of the potential challenges ahead of us, “an AGI with-
out graph reasoning ability will never be the AGI we may desire ”.
Based on such motivations, we write this paper trying to incorpo-
rate graph data into LLMs for various graph reasoning tasks. On
the one hand, we really hope the currently AI-leading companies
like OpenAI, Microsoft, Google and Meta can take graph structured
data reasoning into consideration when they develop their missions
and plans for achieving the AGI, so that the graph learning com-
munity will be able to contribute our efforts to building the AGI
system together with the language and vision communities. On
the other hand, we also hope to empower the existing LLMs with
the ability to overcome the weaknesses in their performance when
handling graph structured data for complex graph reasoning tasks.
So, the latest developed LLMs can also benefit the graph learning
community for solving various graph reasoning tasks as well.
Considering the current language models and their extremely
high pre-training costs, we cannot fundamentally re-design a new
LLM with pre-training to equip them with the graph reasoning
capabilities. Pre-training such LLMs from scratch is an infeasible
task for most research groups in academia and majority of compa-
nies in the industry as well. To adapt to the common practices of
NLP approaches, we will introduce the Graph Reasoning oriented
Toolformer framework (Graph-ToolFormer) by fine-tuning some
existing pre-trained LLMs ( e.g., GPT-J or LLaMA) in this paper.
Technically, as illustrated in Figure 1, based on the latest ChatGPT
from OpenAI and Toolformer model from Meta [43], we propose
to provide the existing pre-trained LLMs ( e.g., GPT-J or LLaMA)
with the ability to perform various complex graph reasoning tasks
by allowing them to use external graph learning tools, such as
other pre-trained graph neural network models and existing graph
reasoning toolkits. Instead of manually hard-coding the graph data
loading and external graph learning tool usage function calls in the
reasoning statements, to make Graph-ToolFormer as a general
graph reasoning interface, we will fine-tune the LLMs to teach the
models to decide not only where to retrieve the graph data, but also
what tools to be used, as well as when and how to use these tools.



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 2):

Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT
More technical details about the Graph-ToolFormer model will
be introduced in the following methodology section.
As the first exploration attempt to use LLMs for general graph
reasoning tasks, we summarize the contributions of this paper as
follows:
•Graph Reasoning with LLMs : This paper is the first pa-
per that attempts to propose a general LLM, i.e., Graph-
ToolFormer, that can handle graph reasoning tasks. It effec-
tively remedies the weaknesses of existing LLMs on graph
reasoning. More importantly, it helps bridge the graph learn-
ing community with the latest development on LLMs and
AIGC led by the language and vision learning communities.
So people in the graph learning community will also have the
stage to demonstrate our skills and expertises in the current
era of AIGC and the future era AGI.
•Graph Reasoning Prompt Dataset : In this paper, we cre-
ate a handful number of human-written language instruc-
tions and prompt examples of how graph learning tools can
be used. Based on the self-supervised in-context learning,
we use ChatGPT to annotate and augment a large graph
reasoning dataset with API calls of different external graph
learning tools, which will also be post-processed with selec-
tive filtering. Via the github page1, we have released both
the graph raw datasets and the generated graph reasoning
prompt dataset used in this paper with the community for
future explorations.
•Extensive Experimental Studies : We have extensively
tested the effectiveness of our proposedGraph-ToolFormer
with various graph reasoning based application tasks stud-
ied in the real-world, which include the most basic graph
data loading and general graph property computation tasks,
as well as some more advanced ones. Specifically, we study
several challenging advanced graph reasoning tasks in the
experiments, which include paper topic inference in bib-
liographic networks, molecular graph function prediction,
online social network community detection, personalized
sequential recommendation in recommender systems and
knowledge graph entity and relation reasoning.
The remaining sections of this paper are organized as follows. We
will briefly introduce the related work in Section 2. The definitions
of some terminologies and the formulation of the studied problem
will be provided in Section 3. A detailed introduction about the
Graph-ToolFormer framework will be provided in Section 4. The
effectiveness of Graph-ToolFormer will be tested with extensive
experiments on real-world benchmark graph datasets in Section 5.
Finally, we will conclude this paper in Section 6 and briefly discuss
about some potential future exploration directions in Section 7.
2 RELATED WORK
In this section, we will discuss about several research topics that
are related to our Graph-ToolFormer framework proposed in
this paper, which include graph neural networks , language models ,
language model based graph learning and prompt tuning .
1https://github.com/jwzhanggy/Graph_Toolformer/tree/main/data
2.1 Graph Neural Networks
Graph neural networks (GNNs) aim to learn the embedding repre-
sentations of the graph structured data. Representative examples
of GNNs proposed already include GCN [19] and Graph-Bert [60],
based on which various extended variants [20, 46, 51] have been
introduced as well. As mentioned above, GCN and its variant mod-
els are all based on the approximated graph convolutional operator
[13], which may lead to the suspended animation problem [59] and
over-smoothing problem [23] for deep model architectures. Theo-
retic analyses of the causes are provided in [12, 23, 59]. To handle
such problems, [59] generalizes the graph raw residual terms and
proposes a method based on graph residual learning; [23] proposes
to adopt residual/dense connections and dilated convolutions into
the GCN architecture. Besides the GCN and Graph-Bert based mod-
els, several other work [17, 46] also seeks to involve the recurrent
network for deep graph representation learning instead.
2.2 Language Models
Since the proposal of Transformer [ 50], large language models
(LLMs) have become the dominant deep model for various NLP
tasks. Assisted with pre-training, the giant tech-companies have
also introduced their own versions of different LLMs, like BERT
from Google [8], BART from Meta [22], GPT from OpenAI [5, 38, 39],
ELMo from AI2 [37] and MT-DNN from Microsoft [25]. Many of
these LLMs have also been open-sourced with both model algo-
rithm and learned parameters released to the community for both
research and application purposes. One research paper closely re-
lated to this work is Toolformer [43] from Meta, which proposes
to incorporate external APIs into language models. Equipped with
such external APIs, the models will be able to automatically decide
how to use which tool. Meanwhile, even prior to the Toolformer
model, several other previous papers [29, 35] have also explored to
augment language models with external tools.
2.3 Prompt Tuning
Prompts have been shown to be effective in tuning the pre-trained
language models with zero-shot or few-shot learning [ 5], which
can help language models learn faster than traditional fine tuning
tasks. By now, we have witnessed three categories of prompt tuning
approaches, i.e.,, discrete prompts [44], continuous prompts [24] and
priming [5]. Discrete prompts [44] reformat data instances with
some template text, like,
“{ premise } Should we assume that { hypothesis }? [prediction] ”.
Discrete prompts will typically tune all parameters of the model.
On the other hand, continuous prompts [24] will prepend examples
with embedding vectors of special tokens, which will only update
a much smaller set of model parameters. Very different from the
discrete and continuous prompts, priming [5] initially adopted in
GPT-3 will prepend several priming examples to the target evalua-
tion example instead, like
“Example 1: { sentence 1 } True or False? { label 1 }.
Example 2: { sentence 2 } True or False? { label 2 }.
···
Example k: { sentence k } True or False? { label k }.
Evaluation: { eval-sentence } True or False? [prediction] . ”



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 0):

Graph-ToolFormer: To Empower LLMs with Graph Reasoning
Ability via Prompt Augmented by ChatGPT
Jiawei Zhang
jiawei@ifmlab.org
IFM Lab
Department of Computer Science,
University of California, Davis
Davis, California, USA
https://github.com/jwzhanggy/Graph_Toolformer
ABSTRACT
In this paper, we aim to develop a large language model (LLM) with
the reasoning ability on complex graph data. Currently, LLMs have
achieved very impressive performance on various natural language
learning tasks, extensions of which have also been applied to study
the vision tasks with data in multiple modalities. However, when
it comes to the graph learning tasks, existing LLMs present very
serious flaws due to their inherited weaknesses in performing pre-
cise mathematical calculation , multi-step logic reasoning , perception
about the spatial and topological factors , and handling the temporal
progression.
To address such challenges, in this paper, we will investigate
the principles, methodologies and algorithms to empower existing
LLMs with the graph reasoning ability, which will have tremendous
impacts on the current research of both LLMs and graph learning.
Inspired by the latest ChatGPT and Toolformer models, we propose
the Graph-ToolFormer (Graph Reasoning oriented Toolformer)
framework to teach LLMs themselves with prompts augmented by
ChatGPT to use external graph reasoning API tools. Specifically,
we will investigate to teach Graph-ToolFormer to handle various
graph data reasoning tasks in this paper, including both (1) very
basic graph data loading and graph property reasoning tasks , ranging
from simple graph order and size to the graph diameter and periph-
ery, and (2) more advanced reasoning tasks on real-world graph data ,
such as bibliographic paper citation networks, protein molecular
graphs, sequential recommender systems, online social networks
and knowledge graphs.
Technically, to build Graph-ToolFormer, we propose to hand-
craft both the instruction and a small amount of prompt templates
for each of the graph reasoning tasks, respectively. Via in-context
learning, based on such instructions and prompt template examples,
we adopt ChatGPT to annotate and augment a larger graph reason-
ing statement dataset with the most appropriate calls of external API
functions. Such augmented prompt datasets will be post-processed
with selective filtering and used for fine-tuning existing pre-trained
causal LLMs, such as the GPT-J and LLaMA, to teach them how to
use graph reasoning tools in the output generation. To demonstrate
the effectiveness of Graph-ToolFormer, we conduct extensive
experimental studies on various graph reasoning datasets and tasks,
and have also launched a LLM demo with various graph reasoning
abilities. All the source code of Graph-ToolFormer framework,
the demo for graph reasoning, and the graph and prompt datasets
have been released online at the project github page.
KEYWORDS
Tool Transformer; ChatGPT; In-Context Learning; Language Model;
Graph Learning
1 INTRODUCTION
In recent years, large language models (LLMs) [ 8, 34, 50] have
achieved very impressive performance on a variety of natural lan-
guage processing tasks [32, 34, 49], extensions of which have also
been extensively applied to solve many other problems with data
in different modalities as well [10, 32, 40, 41]. With the launch of
ChatGPT and new Microsoft Bing Chat based on both GPT-3.5 and
GPT-4, LLMs have also been widely used in people’s daily produc-
tion and life. At the same time, due to their inherent limitations,
these LLMs have also received lots of criticisms in their usages due
to their inherited weaknesses, like inability in performing precise
calculations [36], difficulty in addressing multi-step logic reasoning
problems [6], incapable to conduct spatial and topological reasoning
[1], and unawareness of progression of temporal factors [9].
With the parallel development of natural language processing
and computer vision, transformer based deep learning models on
graph structured data has also received lots of attention from the
community in recent years [16, 56, 60]. Graph provides a unified
representation for many inter-connected data in the real-world,
which models both the diverse attributes of the nodes and the ex-
tensive links connecting the nodes with each other. Besides the
classic graph structures we learn from the discrete math and algo-
rithm courses, as shown in Figure 1, lots of real-world data can also
be modeled as graphs [ 45], like bibliographic networks [47], pro-
tein molecular graphs [52], recommender systems [28], online social
networks [31], and knowledge graphs [18].
Meanwhile, compared with the prosperous research explorations
on incorporating vision and language data into LLMs for designing
the ambitious AGI development plan [ 33], it seems researchers
have either “unintentionally” or “intentionally” ignored the widely
existed graph data and don’t seem to have any plans to include
them into the LLMs building for achieving the AGI.
Here, we say researchers have “unintentionally” ignored graphs,
since compared with texts and images that we deal with everyday,
graph has long-time been merely used as an intermediate model-
ing data structure for real-world data and we normally have no
direct interactions with graph actually. It is natural that people
may mistakenly think graph should not be the focus at the current
stage for creating AIGC and building the AGI systems. At the same
time, we say researchers may have “intentionally” ignored graphs,
arXiv:2304.11116v3  [cs.AI]  11 May 2023



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 22):

Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT
Table 4: A summary of the experimental results of Graph-ToolFormer on various graph reasoning tasks on the correspond-
ing benchmark datasets. The results are evaluated by the Rouge scores, BLEU and BP scores. Except for the graph loading task,
we also evaluate the results on other tasks/datasets by comparing the graph reasoning API calls (other textual contents are
excluded) with the ground-truth API calls, and report the Accuracy on reasoning API calls in the table as well.
Tasks Datasets Methods Evaluation Metrics
Rouge-1 Rouge-2 Rouge-L Rouge-LSum BLEU BP API-Gen Acc
Graph
Loading GL-Prompt Graph-ToolFormer 82.28 67.74 70.93 70.85 63.53 89.98 4.38
Property
Reasoning GPR-Prompt Graph-ToolFormer 94.56 92.10 91.69 91.69 91.53 99.93 80.00
Paper
Topic
Reasoning
Cora Graph-ToolFormer 99.69 99.68 99.69 99.69 99.2 100.0 100.0
Citeseer Graph-ToolFormer 100.0 100.0 100.0 100.0 99.39 100.0 97.5
Pubmed Graph-ToolFormer 99.91 99.84 99.91 99.91 99.04 100.0 99.38
Molecule
Function
Reasoning
PROTEINS Graph-ToolFormer 99.61 99.19 99.61 99.61 98.27 100.0 100.0
PTC Graph-ToolFormer 100.0 100.0 100.0 100.0 98.52 100.0 100.0
NCI1 Graph-ToolFormer 100.0 100.0 100.0 100.0 98.28 100.0 100.0
MUTAG Graph-ToolFormer 100.0 100.0 100.0 100.0 98.72 100.0 100.0
Sequential
Recommendation
Reasoning
MovieLens Graph-ToolFormer 97.47 96.56 97.47 97.47 94.63 95.31 93.12
Last.FM Graph-ToolFormer 89.24 86.69 88.75 88.79 83.43 89.67 85.62
Amazon Graph-ToolFormer 99.9 99.8 99.9 99.9 99.74 100.0 100.0
Social
Community
Reasoning
Foursquare Graph-ToolFormer 98.6 98.01 98.51 98.46 97.41 100.0 95.0
Twitter Graph-ToolFormer 99.86 99.71 99.78 99.76 99.75 99.89 98.75
Knowledge
Graph
Reasoning
Freebase Graph-ToolFormer 91.98 91.79 91.97 92.0 78.17 78.29 53.75
WordNet Graph-ToolFormer 98.73 98.73 98.73 98.73 97.99 98.69 96.88
•System and Software : We run the experiment on Ubuntu
22.04, with CUDA toolkit version 11.8, Nvidia Driver version
520, PyTorch version 1.13.1 and Python 3.9. For the optimizer
of Graph-ToolFormer (GPT-J 6B, 8bit), we use the 8-bit
AdamW from bitsandbytes with version 0.37.1. We load the
pre-trained GPT-J 6B 8bit from Huggingface with weight pa-
rameter checkpoint “hivemind/gpt-j-6B-8bit” and config/tok-
enizer checkpoint “EleutherAI/gpt-j-6b” as the base model of
Graph-ToolFormer, and the installed transformer toolkit
version is 4.28.0.dev0. More information about other sys-
tem and software configurations can be found at the shared
anaconda environment file4.
•Hyper-parameters: For fine-tuning Graph-ToolFormer
(GPT-J 6B, 8bit), we use AdamW with a very small learning
rate 1e-5 with weight decay 1e-2, and a max-epoch of 3. Both
the training and testing instances are divided into batches
with shuffle with batch size 32 and we set the max input/out-
put token length as 128. For the generation function of the
4https://github.com/jwzhanggy/Graph_Toolformer/blob/main/environment.yml
language model, the following hyper-parameters are used,
i.e., num-beams: 5, top-k: 5, top-p: 0.95, temperature: 1.9,
num-return-sequence: 1, max-length: 128.
Especially, when the batch size and input/output max token
length are assigned with small values (e.g., batch-size: 1 or 2 and
max-length: 64), we can also fine-tune Graph-ToolFormer (GPT-J
6B, 8bit) model on GPUs with smaller RAM (like Nvidia 1080ti with
11GB memory). It will allow most research groups and individuals
to tune and deploy Graph-ToolFormer to provide LLMs based
graph reasoning functions and services.
5.4.2 Performance Evaluation. For the preliminary performance
evaluation of Graph-ToolFormer, we have used several evaluation
metrics as follows in the experiments:
•ROUGE scores : By comparing the outputs of the Graph-
ToolFormer framework with the ground-truth, we calculate
the Rouge-1, Rouge-2, Rouge-L and Rouge-LSum scores ob-
tained by the model.



Source: data\tc16_2312.10997v5\referenced_papers\[107]_2306.02224.pdf (Page 6):

Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions
adaptability of this approach, especially for tasks with eas-
ily collectible training data for action policy. The code of this
work is shared in: https://github.com/younghuman/LLMAgent
REFERENCES
[1] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson
Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, and
Cameron McKinnon. 2022. Constitutional AI: Harmlessness from AI
Feedback. In arXiv:2212.08073. arXiv. https://ar5iv.org/abs/2212.08073
[2] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Jo-
hannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat
Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi,
Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial
General Intelligence: Early experiments with GPT-4. (March 2023).
https://www.microsoft.com/en-us/research/publication/sparks-of-
artificial-general-intelligence-early-experiments-with-gpt-4/
[3] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,
Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,
Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language
modeling with pathways. arXiv preprint arXiv:2204.02311(2022).
[4] Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian
Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri,
Mahmoud Adada, et al. 2018. Textworld: A learning environment for
text-based games. (2018), 41–75.
[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
2018. Bert: Pre-training of deep bidirectional transformers for language
understanding. arXiv preprint arXiv:1810.04805(2018).
[6] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar
Khot. 2023. Complexity-Based Prompting for Multi-Step Reasoning.
arXiv:2210.00720 [cs.CL]
[7] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for
self-supervised learning of language representations. arXiv preprint
arXiv:1909.11942 (2019).
[8] Yang Li, Zhi-Ping Cai, and Hong Xu. 2018. LLMP: exploiting LLDP
for latency measurement in software-defined data center networks.
Journal of Computer Science and Technology33 (2018), 277–285.
[9] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu,
Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, et al. 2023. Taskmatrix. ai:
Completing tasks by connecting foundation models with millions of
apis. arXiv preprint arXiv:2303.16434(2023).
[10] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi
Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
2019. Roberta: A robustly optimized bert pretraining approach. arXiv
preprint arXiv:1907.11692(2019).
[11] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
[12] Joon Sung Park, Joseph C. O’Brien, Carrie Cai, Meredith Ringel Morris,
Percy Liang, and Michael Bernstein. 2023. Generative Agents: Interac-
tive Simulacra of Human Behavior. https://arxiv.org/pdf/2304.03442.
pdf
[13] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez.
2023. Gorilla: Large Language Model Connected with Massive APIs.
arXiv:2305.15334 [cs.CL]
[14] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng
Gao. 2022. Instruction Tuning with GPT-4. https://ar5iv.org/abs/2304.
03277
[15] Stephane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell. 2011. A
Reduction of Imitation Learning and Structured Prediction to No-Regret
Online Learning. arXiv preprint arXiv:1011.0686(2011). arXiv:1011.0686
[16] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and
Yueting Zhuang. 2023. HuggingGPT: Solving AI Tasks with ChatGPT
and its Friends in HuggingFace. arXiv:2303.17580 [cs.CL]
[17] Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an
autonomous agent with dynamic memory and self-reflection. arXiv
preprint arXiv:2303.11366(2023).
[18] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Win-
son Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020.
ALFRED: A Benchmark for Interpreting Grounded Instructions for
Everyday Tasks. arXiv preprint arXiv:1912.01734(2020).
[19] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk,
Adam Trischler, and Matthew Hausknecht. 2021. AlfWorld: Align-
ing Text and Embodied Environments for Interactive Learning. arXiv
preprint arXiv:2010.03768(2021).
[20] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sha-
ran Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-
Consistency Improves Chain of Thought Reasoning in Language Mod-
els. arXiv:2203.11171 [cs.CL]
[21] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits
reasoning in large language models. arXiv preprint arXiv:2201.11903
(2022).
[22] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale
Schuurmans. 2023. Foundation Models for Decision Making: Problems,
Methods, and Opportunities. arXiv e-prints(2023), arXiv–2303.
[23] Ilan Yaniv. 2004. The benefit of additional opinions. Current directions
in psychological science13, 2 (2004), 75–78.
[24] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022.
Webshop: Towards scalable real-world web interaction with grounded
language agents. arXiv preprint arXiv:2207.01206(2022).
[25] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik
Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and
acting in language models. arXiv preprint arXiv:2210.03629(2022).
A APPENDIX



### Claim 66/179

#### Claim Text
WebGPT [110] integrates a reinforcement learning framework to train the GPT-3 model in autonomously using a search engine during text generation.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[36]_2303.08559.pdf (Page 10):

Conference 2022, Virtual Event, Lyon, France, April
25 - 29, 2022, pages 2778–2788. ACM.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
bert Webson, Shixiang Shane Gu, Zhuyun Dai,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-
ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,
Dasha Valter, Sharan Narang, Gaurav Mishra, Adams
Yu, Vincent Zhao, Yanping Huang, Andrew Dai,
Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-
cob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,
and Jason Wei. 2022. Scaling instruction-finetuned
language models.
Bosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken
Chia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023.
Is GPT-3 a good data annotator? In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 11173–11195, Toronto, Canada. Association
for Computational Linguistics.
Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang,
Xu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan
Liu. 2021. Few-NERD: A few-shot named entity
recognition dataset. In Proceedings of the 59th An-
nual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers), pages 3198–3213, Online. Association
for Computational Linguistics.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion (ACE) program – tasks, data, and evaluation. In
Proceedings of the Fourth International Conference
on Language Resources and Evaluation (LREC’04),
Lisbon, Portugal. European Language Resources As-
sociation (ELRA).
Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins,
and Benjamin Van Durme. 2020. Multi-sentence ar-
gument linking. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics, pages 8057–8077, Online. Association for
Computational Linguistics.
Jun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu.
2023. Exploring the feasibility of chatgpt for event
extraction.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
SimCSE: Simple contrastive learning of sentence em-
beddings. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 6894–6910, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-
berger. 2017. On calibration of modern neural net-
works. In Proceedings of the 34th International Con-
ference on Machine Learning, ICML 2017, Sydney,
NSW, Australia, 6-11 August 2017 , volume 70 of
Proceedings of Machine Learning Research, pages
1321–1330. PMLR.
Bernal Jimenez Gutierrez, Nikolas McNeal, Clayton
Washington, You Chen, Lang Li, Huan Sun, and
Yu Su. 2022. Thinking about GPT-3 in-context learn-
ing for biomedical IE? think again. In Findings of the
Association for Computational Linguistics: EMNLP
2022, pages 4497–4512, Abu Dhabi, United Arab
Emirates. Association for Computational Linguistics.
Martin Josifoski, Marija Sakota, Maxime Peyrard, and
Robert West. 2023. Exploiting asymmetry for syn-
thetic training data generation: Synthie and the case
of information extraction.
Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah-
man, Chandra Bhagavatula, Ronan Le Bras, and
Yejin Choi. 2022. Maieutic prompting: Logically
consistent reasoning with recursive explanations. In
Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing , pages
1266–1279, Abu Dhabi, United Arab Emirates. Asso-
ciation for Computational Linguistics.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020.
BART: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and com-
prehension. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,



Source: data\tc16_2312.10997v5\referenced_papers\[103]_2303.08559.pdf (Page 10):

Conference 2022, Virtual Event, Lyon, France, April
25 - 29, 2022, pages 2778–2788. ACM.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
bert Webson, Shixiang Shane Gu, Zhuyun Dai,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-
ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,
Dasha Valter, Sharan Narang, Gaurav Mishra, Adams
Yu, Vincent Zhao, Yanping Huang, Andrew Dai,
Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-
cob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,
and Jason Wei. 2022. Scaling instruction-finetuned
language models.
Bosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken
Chia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023.
Is GPT-3 a good data annotator? In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 11173–11195, Toronto, Canada. Association
for Computational Linguistics.
Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang,
Xu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan
Liu. 2021. Few-NERD: A few-shot named entity
recognition dataset. In Proceedings of the 59th An-
nual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers), pages 3198–3213, Online. Association
for Computational Linguistics.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion (ACE) program – tasks, data, and evaluation. In
Proceedings of the Fourth International Conference
on Language Resources and Evaluation (LREC’04),
Lisbon, Portugal. European Language Resources As-
sociation (ELRA).
Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins,
and Benjamin Van Durme. 2020. Multi-sentence ar-
gument linking. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics, pages 8057–8077, Online. Association for
Computational Linguistics.
Jun Gao, Huan Zhao, Changlong Yu, and Ruifeng Xu.
2023. Exploring the feasibility of chatgpt for event
extraction.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
SimCSE: Simple contrastive learning of sentence em-
beddings. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 6894–6910, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-
berger. 2017. On calibration of modern neural net-
works. In Proceedings of the 34th International Con-
ference on Machine Learning, ICML 2017, Sydney,
NSW, Australia, 6-11 August 2017 , volume 70 of
Proceedings of Machine Learning Research, pages
1321–1330. PMLR.
Bernal Jimenez Gutierrez, Nikolas McNeal, Clayton
Washington, You Chen, Lang Li, Huan Sun, and
Yu Su. 2022. Thinking about GPT-3 in-context learn-
ing for biomedical IE? think again. In Findings of the
Association for Computational Linguistics: EMNLP
2022, pages 4497–4512, Abu Dhabi, United Arab
Emirates. Association for Computational Linguistics.
Martin Josifoski, Marija Sakota, Maxime Peyrard, and
Robert West. 2023. Exploiting asymmetry for syn-
thetic training data generation: Synthie and the case
of information extraction.
Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah-
man, Chandra Bhagavatula, Ronan Le Bras, and
Yejin Choi. 2022. Maieutic prompting: Logically
consistent reasoning with recursive explanations. In
Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing , pages
1266–1279, Abu Dhabi, United Arab Emirates. Asso-
ciation for Computational Linguistics.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020.
BART: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and com-
prehension. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,



Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 12):

Preprint.
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg
evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023b.
URL https://arxiv.org/abs/2303.16634.
Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Am-
manabrolu, and Yejin Choi. QUARK: Controllable text generation with reinforced unlearning.
In Advances in Neural Information Processing Systems, 2022. URL https://openreview.
net/forum?id=5HaIds3ux5O.
Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox,
Helen Meng, and James Glass. Sail: Search-augmented instruction learning. arXiv preprint
arXiv:2305.15225, 2023. URL https://arxiv.org/abs/2305.15225.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri
Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad
Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-
refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. URL
https://arxiv.org/abs/2303.17651.
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.
When not to trust language models: Investigating effectiveness of parametric and non-parametric
memories. In Proceedings of the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , 2023. URL https://aclanthology.org/2023.
acl-long.546.
Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick,
Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching
language models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022.
URL https://arxiv.org/abs/2203.11147.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering. InProceedings of the 2018 Conference
on Empirical Methods in Natural Language Processing, 2018. URL https://aclanthology.
org/D18-1260.
Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. A discrete hard EM approach
for weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), 2019. URL https://aclanthology.org/
D19-1284.
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,
Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual
precision in long form text generation. arXiv preprint arXiv:2305.14251, 2023. URL https:
//arxiv.org/abs/2305.14251.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher
Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted
question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. URL https:
//arxiv.org/abs/2112.09332.
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao,
Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable
retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing, 2022. URL https://aclanthology.org/2022.emnlp-main.669.
OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https://arxiv.
org/abs/2303.08774.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton,
Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and
13



Source: data\tc16_2312.10997v5\referenced_papers\[160]_2308.10633.pdf (Page 6):

ness and consistency across evaluations, as the an-
swers of the test set remain confidential11.
While R-LLMs exhibit high validity, it falls be-
hind the smaller yet specialized model, RAG, on
the KILT downstream task (refer to Table 2). This
disparity can be attributed to various factors, includ-
ing prompt maturity and the ability of LLMs to gen-
erate responses. Although the employed prompts
were carefully developed, it is likely that more opti-
mal prompts exist (discussed in Section 4.3). More-
over, fine-tuning LLMs with retrieval-augmented
generation tasks might enhance their performance
on downstream tasks. Therefore, the evaluation
accuracy reported herein would represent a conser-
vative estimate.
Prompt engineering is a crucial aspect of the
retrieval-augmented generation process, as the gen-
erated outputs can differ significantly between mod-
els, even when provided with the same prompt.
RALLE offers an advantage in this regard, allow-
ing users to effortlessly experiment with diverse
prompts for varying behaviors, datasets, and intri-
cate chain of actions.
In the realm of prompt development, techniques
like Automatic Prompt Engineer (APE) (Zhou
et al., 2023) automate the creation of prompts from
input-output pairs and sampling to identify the most
effective prompts. However, the input-output pairs
in retrieval-augmented generation are distinctly dif-
ferent from those of the simple instruction induc-
tion tasks. Because the input text for retrieval-
augmented generation can often be lengthy and
complex, it is difficult to automatically induce the
effective prompts from the input-output pairs.
This tool enables developers to construct an in-
ference chain with predefined actions, while recent
advances have also introduced methods allowing
LLMs to determine the actions (Yao et al., 2023).
One approach entails retrieving documents using
a query rewritten by an LLM and then summariz-
ing them until the desired information is obtained.
However, in our initial experiments (not described
in this paper), we observed instances where rela-
tively small LLMs (typically less than 100 billion
parameters) became trapped in cycles of repeated
retrieval and summarization, hindering their ability
to reach the final answer generation. Our tool ad-
dresses this issue by intentionally building explicit
inference chains to avoid unintended operations.
11https://eval.ai/web/challenges/
challenge-page/689/overview
References
Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan,
Abdulrahman Alfozan, and James Zou. 2019. Gradio:
Hassle-free sharing and testing of ml models in the
wild. arXiv preprint arXiv:1906.02569.
Akari Asai and Eunsol Choi. 2021. Challenges in
information-seeking QA: Unanswerable questions
and paragraph retrieval. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers), pages 1492–1504, Online. Association
for Computational Linguistics.
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-
liang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei
Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multi-
task, multilingual, multimodal evaluation of chatgpt
on reasoning, hallucination, and interactivity. arXiv
preprint arXiv:2302.04023.
Ali Borji. 2023. A categorical archive of ChatGPT
failures. arXiv preprint arXiv:2302.03494.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Harrison Chase. 2023. LangChain. https://
langchain.com/.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading Wikipedia to answer open-
domain questions. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1870–1879,
Vancouver, Canada. Association for Computational
Linguistics.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing GPT-4 with 90%* Chat-
GPT quality.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben



Source: data\tc16_2312.10997v5\referenced_papers\[15]_2308.11761.pdf (Page 10):

nism may better allow LLMs to autonomously ex-
plore KBs. As LLMs are not aware of the contents
within KBs, they might generate search that appear
logical but yield no results. For example, a query
like “Who is the voice actor for the heroine in ... ”
may require a two-hop searching for the relations
heroine and voice actor subsequently in certain
KBs, or just a single relation main voice actor in
others. In these scenarios, a multi-round mecha-
nism empowers LLMs to probe and revisit the KBs
autonomously, which might yield better results but
with increased costs. Second, we experiment with
KnowledGPT on representative yet small datasets,
constrained by the expenses of accessingGPT-4 via
API. While the results validate the effectiveness of
KnowledGPT, more comprehensive evaluations on
full benchmarks are expected to better compare
KnowledGPT to related methods. We plan to study
fine-tuning LLMs like Llama for KnowledGPT in
our future work to improve the efficiency and con-
duct more thorough experiments. Finally, it re-
mains a practical issue when LLMs need to access
external KGs, rather than solving problems inde-
pendently. In this work, we simply let LLMs make
this decision, while better approaches remain to be
explored.
6 Conclusion
In this paper, we introduce KnowledGPT, a compre-
hensive framework to integrate LLMs with external
knowledge bases, facilitating LLMs’ retrieval and
storage on KBs. For retrieval, KnowledGPT adopts
“program of thought” prompting, which retrieves
knowledge via code generation and execution. For
storage, KnowledGPT extracts various forms of
knowledge from user provided texts, and populate
the personalized KB with the extracted knowledge.
KnowledGPT tackles several challenges inherent
in integrating LLMs with KBs, including complex
question answering, ambiguation in entity linking,
and limited forms of knowledge representations.
We show with extensive experiments that Knowl-
edGPT effectively provides LLMs with the capa-
bility to operate on external KBs.
References
Ibrahim Abdelaziz, Srinivas Ravishankar, Pavan Kapa-
nipathi, Salim Roukos, and Alexander Gray. 2021. A
semantic parsing and reasoning-based approach to
knowledge base question answering. Proceedings
of the AAAI Conference on Artificial Intelligence ,
35(18):15985–15987.
Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023.
Knowledge-augmented language model prompting
for zero-shot knowledge graph question answering.
arXiv preprint arXiv:2306.04136.
Jonathan Berant, Andrew K. Chou, Roy Frostig, and
Percy Liang. 2013. Semantic parsing on freebase
from question-answer pairs. In Conference on Em-
pirical Methods in Natural Language Processing.
Edward Loper Bird, Steven and Ewan Klein. 2009. Nat-
ural language processing with python.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. CoRR,
abs/2005.14165.
Shulin Cao, Jiaxin Shi, Zijun Yao, Xin Lv, Jifan Yu, Lei
Hou, Juanzi Li, Zhiyuan Liu, and Jinghui Xiao. 2022.
Program transfer for answering complex questions
over knowledge bases.
Harrison Chase. 2022. LangChain.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W Cohen. 2022. Program of thoughts
prompting: Disentangling computation from reason-
ing for numerical reasoning tasks. arXiv preprint
arXiv:2211.12588.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Nan Duan. 2016. Overview of the nlpcc-iccpol 2016
shared task: Open domain chinese question answer-
ing. In Natural Language Understanding and Intelli-
gent Applications, pages 942–948. Springer Interna-
tional Publishing.
Yu Gu, Xiang Deng, and Yu Su. 2022. Don’t generate,
discriminate: A proposal for grounding language
models to real-world environments. arXiv preprint
arXiv:2212.09736.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Mingwei Chang. 2020. Retrieval augmented
language model pre-training. In International confer-
ence on machine learning, pages 3929–3938. PMLR.



### Claim 67/179

#### Claim Text
Flare automates timing retrieval by monitoring the confidence of the generation process, as indicated by the 12 probability of generated terms [24].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[105]_2310.01558.pdf (Page 14):

Published as a conference paper at ICLR 2024
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha
Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language
models. In The Eleventh International Conference on Learning Representations , 2023. URL
https://openreview.net/forum?id=1PL1NIMMrw.
Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing datasets for multi-hop read-
ing comprehension across documents.Transactions of the Association for Computational Linguis-
tics, 6:287–302, 2018. doi: 10.1162/tacl a 00021. URL https://aclanthology.org/
Q18-1021.
Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-
tence understanding through inference. InProceedings of the 2018 Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, 2018. Association for Com-
putational Linguistics. doi: 10.18653/v1/N18-1101. URL https://aclanthology.org/
N18-1101.
Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and
Jonathan Berant. Break it down: A question understanding benchmark. Transactions of the
Association for Computational Linguistics , 8:183–198, 2020. doi: 10.1162/tacl a 00309. URL
https://aclanthology.org/2020.tacl-1.13.
Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. An-
swering questions by meta-reasoning over multiple chains of thought. In Houda Bouamor,
Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Meth-
ods in Natural Language Processing , pp. 5942–5966, Singapore, December 2023. Associa-
tion for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.364. URL https:
//aclanthology.org/2023.emnlp-main.364.
Zexuan Zhong, Zhengxuan Wu, Christopher Manning, Christopher Potts, and Danqi Chen.
MQuAKE: Assessing knowledge editing in language models via multi-hop questions. In Houda
Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing , pp. 15686–15702, Singapore, December 2023.
Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.971. URL
https://aclanthology.org/2023.emnlp-main.971.
Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. Context-faithful prompting for
large language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),Findings of the As-
sociation for Computational Linguistics: EMNLP 2023, pp. 14544–14556, Singapore, December
2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.968.
URL https://aclanthology.org/2023.findings-emnlp.968.
A A PPENDIX
A.1 M ODELS
Llama-2 In all cases, we use the vanilla variant of the Llama-2 models from https://
huggingface.co/meta-llama, with half precision.
Decomposition Generation Questions in our multi-hop datasets require between 2-4 decomposi-
tion steps. Hence we limit the number of generation steps to 5. In Tab. 8 we show that the number
of cases in which the model does not arrive at an answer in 5 steps, termed as failures, is very
small when generating with top-1 results from G OOGLE SEARCH , at 0.4% for 2W IKI MQA and
1.2% for STRATEGY QA. Failures are much higher when retrieving random contexts, at 37.0% for
2WIKI MQA and 34.4% for S TRATEGY QA. These are usually cases the model enters an infinite
loop. Following recent work, (Wang et al., 2023; Yoran et al., 2023) we use greedy decoding when
generating decompositions.
15



Source: data\tc16_2312.10997v5\referenced_papers\[48]_2310.01558.pdf (Page 14):

Published as a conference paper at ICLR 2024
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha
Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language
models. In The Eleventh International Conference on Learning Representations , 2023. URL
https://openreview.net/forum?id=1PL1NIMMrw.
Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing datasets for multi-hop read-
ing comprehension across documents.Transactions of the Association for Computational Linguis-
tics, 6:287–302, 2018. doi: 10.1162/tacl a 00021. URL https://aclanthology.org/
Q18-1021.
Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-
tence understanding through inference. InProceedings of the 2018 Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, 2018. Association for Com-
putational Linguistics. doi: 10.18653/v1/N18-1101. URL https://aclanthology.org/
N18-1101.
Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and
Jonathan Berant. Break it down: A question understanding benchmark. Transactions of the
Association for Computational Linguistics , 8:183–198, 2020. doi: 10.1162/tacl a 00309. URL
https://aclanthology.org/2020.tacl-1.13.
Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. An-
swering questions by meta-reasoning over multiple chains of thought. In Houda Bouamor,
Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Meth-
ods in Natural Language Processing , pp. 5942–5966, Singapore, December 2023. Associa-
tion for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.364. URL https:
//aclanthology.org/2023.emnlp-main.364.
Zexuan Zhong, Zhengxuan Wu, Christopher Manning, Christopher Potts, and Danqi Chen.
MQuAKE: Assessing knowledge editing in language models via multi-hop questions. In Houda
Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing , pp. 15686–15702, Singapore, December 2023.
Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.971. URL
https://aclanthology.org/2023.emnlp-main.971.
Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. Context-faithful prompting for
large language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),Findings of the As-
sociation for Computational Linguistics: EMNLP 2023, pp. 14544–14556, Singapore, December
2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.968.
URL https://aclanthology.org/2023.findings-emnlp.968.
A A PPENDIX
A.1 M ODELS
Llama-2 In all cases, we use the vanilla variant of the Llama-2 models from https://
huggingface.co/meta-llama, with half precision.
Decomposition Generation Questions in our multi-hop datasets require between 2-4 decomposi-
tion steps. Hence we limit the number of generation steps to 5. In Tab. 8 we show that the number
of cases in which the model does not arrive at an answer in 5 steps, termed as failures, is very
small when generating with top-1 results from G OOGLE SEARCH , at 0.4% for 2W IKI MQA and
1.2% for STRATEGY QA. Failures are much higher when retrieving random contexts, at 37.0% for
2WIKI MQA and 34.4% for S TRATEGY QA. These are usually cases the model enters an infinite
loop. Following recent work, (Wang et al., 2023; Yoran et al., 2023) we use greedy decoding when
generating decompositions.
15



Source: data\tc16_2312.10997v5\referenced_papers\[68]_2310.16568.pdf (Page 12):

A Appendix
A.1 Constrain Computation
1P relies on two key operations for constrain com-
putation:
a) F(D, k) : Documents that contain keyword k
b) C(k, D) : Next tokens for keyword k in arbi-
trary document set D
F(D, k) is preprocessed and cached to allow for
quick computation. C(k, D) is trickier to compute.
When D represents the full corpus, FM-index can
fetch the next tokens in O(|V |log(|V |)), where V
is the token vocabulary and independent of |D|.
However, arbitrary D requires a traversal over all
documents and can be very expensive. In practise,
the LLM training guides it to generate effective
keywords such that |D| is small.
We also apply certain other optimizations to re-
duce the compute cost:
• Constrains are computed lazily over a decod-
ing pass.
• Several computations are cached, eg: keyword
to document id mapping
• To cap the cost of constraints at each decoding
step, we allow for unconstrained generation
in rare scenarios, when the estimated cost is
too high. If the generated path is absent in the
corpus (<1% examples), these can be filtered
out later.
Despite these optimizations, inference continues
to be expensive and we perhaps need a special data
structure for next token look-up.
A.2 Training data size
Dataset Queries Paths Hits@1 EM
Open-NQ 55k 55k 41.9 28.1
Open-NQ 55k 310k 42.9 28.7
Open-NQ 55k 310k 43.6 29.5+ PAQ + 9M + 9M
Table 7: Comparison of different dataset sizes for
queries and paths
In Table 7, we observe the effect of dataset size
on performance. Increasing the numbers of paths
sampled per query improves performance, perhaps
due to higher diversity in training. However, this
method of dataset expansion is limited by the num-
ber of relevant paths we could extract for a query.
We also experiment with increasing the query
set manifold by mixing in unsupervised datasets. A
total of 9M QA pairs are sampled from PAQ (Lewis
et al., 2021), a synthetic QA dataset, and search
paths extracted with heuristic scoring described in
Section 5. The original 1P training dataset is mixed
in 1:1 ratio. This further boosts performance, but
not proportionally to the amount of data added,
indicating diminishing returns from silver datasets.
A.3 SEAL keywords
SEAL generates a set of document substrings con-
strained on the corpus, that are combined to form
document identifiers. Besides using a LM to gener-
ate keywords, SEAL utilizes several other mecha-
nisms for extracting keywords. This includes par-
tial beam sequences, heuristically adding query n-
grams, sampling the top-k tokens from the logprobs
of the first decoding step, force decoding title etc.
The keywords are re-scored using the LM as well as
FM-index count and all keyword combinations are
retrieved. Table 8 illustrates keywords generated
by both the systems. Note that SEAL keywords
can be repetitive and therefore require a large num-
ber of keywords to narrow down to meaningful
documents. This also makes SEAL suitable for re-
trieving a much larger set of documents that can be
re-ranked later. The maximum number of retrieved
documents for SEAL are capped by a hyperparam-
eter with default value of 500. In contrast, 1P is
geared towards retrieving only the top-document.
A.4 Hits@5
SEAL does significantly better than 1P for Hits@5
(Table 9). We attribute this to the large set of key-
words generated by SEAL as explained in the Ap-
pendix A.3.
A.5 Normalizing sequence likelihood over
constrained space
During constrained decoding a sequence X, we
need to choose the next token from C(X, D) and
not the entire vocabulary space V . Should the se-
quence likelihood be re-normalized over this con-
strained space? We find that re-normalizing the
probabilities results in inflated likelihoods, making
it hard for the model to back-track.
Consider the query, "where did the butchers in
the slaughterhouse cases live" to which our model



Source: data\tc16_2312.10997v5\referenced_papers\[49]_2310.12150.pdf (Page 6):

Under review as a conference paper at ICLR 2024
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Avg
Answer Index Fraction
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9Document Sentence Index Fraction
11.2
9.4
11.3
9.8
10.2
9.6
9.4
8.6
9.1
11.3
(a) Location of supporting sentences on generation
settings with in-context evidence documents.
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Avg
Answer Index Fraction
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9Document Sentence Index Fraction
11.0
12.6
12.4
10.5
11.1
9.4
6.7
8.7
8.3
9.3
(b) Location of supporting sentences on generation
settings without in-context evidence documents.
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.02 0.08 0.11 0.07 0.08 0.14 0.09 0.09 0.12 0.20
(c) Location of unsupported sentences on our data.
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.00 0.00 0.09 0.07 0.02 0.21 0.11 0.07 0.04 0.39 (d) On data from Liu et al. (2023b).
Figure 3: On the top (a)(b), we show the distribution of location of supporting sentences in the
document set D for Nth answer sentence chunk. We normalize by the column to visualize the
distribution of supporting sentences across evidence documents for each answer sentence chunk. The
“Avg” column shows the average across answer sentences, indicating how frequently each document
chunk are supporting the answer. We report aggregate results on generation with documents in (a)
and without documents (the bottom two generation settings in Table 2) in (b) as a control study. On
the bottom, two graphs (c)(d) show the percentage of unsupported sentences by the relative location
(sentence index divided by the number of sentences in the answer).
documents, except for the last portion, are less cited by the generated answer (see Avg. column in
Fig. 3).
Which parts of the answer are less supported by the evidence documents? Generated long-form
answers consist of 5-10 sentences. Are sentences generated earlier more likely to be supported by
evidence documents? Fig. 3(c)(d) report the percentage of unsupported sentences by the relative
position of the answer sentence on our data and attribution annotation on long-form answers from
commercial generative search engines from Liu et al. (2023b) respectively. We find that the last
sentence is almost twice as likely to be unsupported compared to other sentence in the answer. This
phenomenon is even more pronounced on Liu et al. (2023b). Related to our study, recent study Min
et al. (2023) also showed in biography generation, that sentences generated later are more likely to
contain false information.
What causes the model to produce unsupported sentences? We manually examine 30 answer
sentences labeled as Not Supported for each setting that has access to evidence documents.4 We
identify three categories of unsupported sentences: retrieval failure, hallucinated facts, and incorrect
synthesis.5 Table 3 provides a description for each category along with an example. In Table 6 in
the appendix, we further provide a breakdown of error types for each generation setting. During our
analysis, we found that about 14% of errors corresponds to annotation error.
We found that attribution error happens more frequently when the retrieved documents do not
provide sufficient evidences for answering the question. Generating ungrounded concepts is a more
common cause of unsupported sentences than incorrectly synthesizing information from incompatible
documents. However, incorrect synthesis happens relatively more frequently in the WebGPT model,
potentially as it grounds its information more heavily from the documents. This suggests multi-
document summarization and synthesis is an important direction for future work, especially for more
faithful retrieval-augmented LMs.
4We analyze all unsupported answer sentences generated by WebGPT, as there are only 17 of them in total.
5Categories are not mutually exclusive (one can contain irrelevant documents and combine facets from each).
7



Source: data\tc16_2312.10997v5\referenced_papers\[68]_2310.16568.pdf (Page 3):

Figure 3: Illustration of the 1P decoding process. A keyword can only be generated from the documents matching
previously generated keywords. Right panel shows a magnified view of applying constraints to a decoding step.
Any keyword not present in the documents is masked out.
Consider Figure 3. The three keywords corre-
spond to the decoded token sequence [ Ten, Com-
mandments, →, twice, in, the, Hebrew, Bible,→,
books, of, Exodus,EOS ]. At the start of decoding,
any token in D is allowed. After decoding Ten,
only those tokens that follow Ten as an n-gram
in D are allowed, along with the default separa-
tors. After decoding [ Ten, Commandments, →]
we are ready to start a new keyword, but only to-
kens from documents that contain the keyword Ten
Commandments are allowed. Decoding continues
in this manner until EOS is generated.
To efficiently implement these constraints, we
need a data-structure that can quickly determine
both C(k, Dp), the continuation tokens given a doc-
ument set and P(Dp, k), the subset of documents
that contain a given path.
For this, we extend the usage of an FM-
index (Ferragina and Manzini, 2000) as described
in (Bevilacqua et al., 2022). The FM-index is a
compressed token-based index over a corpus D0
with a few important properties for our usage: (1) it
can efficiently list possible token continuations for
a sequence prefix that occur in D0 i.e., C(k, D0),
(2) it can list the set of documents in the corpus that
match an n-gram i.e., F(D0, k), and (3) it supports
search over arbitrary n-grams that occurwithin doc-
uments. Note that the FM-index operations are
optimized for D0, the original corpus it is built
over. We extend these to an arbitraryDp ⊂ D0 at
additional cost described in Appendix A.1.
5 Training data generation
For training 1P, we produce a dataset with exam-
ples of queries and search paths as described above.
At a high-level, we generate search paths by itera-
tively selecting n-grams from an answer passage,
and simulating, using the FM-Index of the retrieval
corpus, the partitioning of the corpus after selecting
each keyword, until only a few documents remain.
Finally, the answer spana is appended to the search
path. Each example produced can be serialized as
sequence-to-sequence pair of inputs and targets as:
inputs: Generate keywords for: <q>?
targets: K_SEP k0 K_SEP k1 ... K_SEP A_SEP a EOS
5.1 Keyword Selection
A good keyword should have a) high relevance to
the query and b) effectively narrow down the search
space. To identify relevant keywords, we restrict
to only the gold document g. All ngrams in g of
length up to five are extracted. Irrelevant keywords
are filtered out such as those starting or ending
with stop words. Similarly, keywords that are too
rare in the corpus, e.g., "Philippines at Luzon" or
too frequent, e.g., "part" are excluded based on a
threshold on their count in corpus. The remaining
keywords are scored with a combinations of heuris-
tics, mainly Rouge-1 similarity with the query (Lin,
2004) along with minor award for keywords con-
taining entities and penalty for keywords highly
frequent in the corpus.



### Claim 68/179

#### Claim Text
Self-RAG [25] introduces “reflection tokens” that allow the model to introspect its outputs.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 0):

Preprint.
SELF -RAG : L EARNING TO RETRIEVE , GENERATE , AND
CRITIQUE THROUGH SELF -REFLECTION
Akari Asai†, Zeqiu Wu†, Yizhong Wang†§, Avirup Sil‡, Hannaneh Hajishirzi†§
†University of Washington §Allen Institute for AI ‡IBM Research AI
{akari,zeqiuwu,yizhongw,hannaneh}@cs.washington.edu, avi@us.ibm.com
ABSTRACT
Despite their remarkable capabilities, large language models (LLMs) often produce
responses containing factual inaccuracies due to their sole reliance on the paramet-
ric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad
hoc approach that augments LMs with retrieval of relevant knowledge, decreases
such issues. However, indiscriminately retrieving and incorporating a fixed number
of retrieved passages, regardless of whether retrieval is necessary, or passages are
relevant, diminishes LM versatility or can lead to unhelpful response generation.
We introduce a new framework called Self-Reflective Retrieval-Augmented Gen-
eration (SELF -RAG) that enhances an LM’s quality and factuality through retrieval
and self-reflection. Our framework trains a single arbitrary LM that adaptively
retrieves passages on-demand, and generates and reflects on retrieved passages
and its own generations using special tokens, called reflection tokens. Generating
reflection tokens makes the LM controllable during the inference phase, enabling it
to tailor its behavior to diverse task requirements. Experiments show that SELF -
RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs
and retrieval-augmented models on a diverse set of tasks. Specifically, SELF -RAG
outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,
reasoning and fact verification tasks, and it shows significant gains in improving
factuality and citation accuracy for long-form generations relative to these models.1
1 I NTRODUCTION
State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)
despite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation
(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs
with relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,
2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce
unnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they
retrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,
the output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since
the models are not explicitly trained to leverage and follow facts from provided passages. This
work introduces Self-Reflective Retrieval-augmented Generation ( SELF -RAG) to improve an
LLM’s generation quality, including its factual accuracy without hurting its versatility, via on-demand
retrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on
its own generation process given a task input by generating both task output and intermittent special
tokens (i.e., reflection tokens). Reflection tokens are categorized into retrieval and critique tokens to
indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,
given an input prompt and preceding generations, SELF -RAG first determines if augmenting the
continued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that
calls a retriever model on demand (Step 1). Subsequently,SELF -RAG concurrently processes multiple
retrieved passages, evaluating their relevance and thengenerating corresponding task outputs (Step
2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms
of factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which
1Our code and trained models are available at https://selfrag.github.io/.
1
arXiv:2310.11511v1  [cs.CL]  17 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 9):

Preprint.
0 50 100 150
Num of training (k)
35
40
45
50
55Perfomance
(a) PopQA
0 100
Num of training (k)
71
72
73
 (b) PubHealth
0 100
Num of training (k)
40
60
 (c) ASQA (prec)
Pop Bio.
S & P 92.5 70.0
ISREL 95.0 90.0
ISSUP 90.0 85.0
(d) Human evaluation on PopQA
and Bio generation.
Figure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect
of the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)
Human analysis on SELF -RAG outputs as well as reflection tokens.
contrary, a larger weight results in lower MAUVE scores: when generation gets longer and more
fluent, there are often more claims that are not fully supported by citations, consistent with findings
by Liu et al. (2023a). Our framework lets practitioners choose and customize models’ behaviors at
test time by adjusting such parameters without requiring additional training.
Efficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval
occurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects
overall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers
of threshold δ (larger δ results in less retrieval) on PubHealth and PopQA. Figure 3c shows that
the model’s retrieval frequencies dramatically change on both datasets. as δ varies. On one hand,
performance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.
Effects of training data size. We conduct an analysis of how the data scale affects the model’s
performance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original
150k training instances, and fine-tune four SELF -RAG 7B variants on those subsets. Then, we compare
the model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF -
RAG trained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models’
performance trained on different amount of data. Across all datasets, increasing data size often shows
upward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do
not observed such significant improvements on Llama2-FT7B when increasing the training data from
50k to 150k. These results also indicate that further expanding the training data of SELF -RAG may
lead to further improvements, although in this work we limit our training data size to 150k.
Human evaluations. We conduct small human evaluations on SELF -RAG outputs, as well as the
reliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio
results. Following Menick et al. (2022), human annotators evaluate S&P, which indicates whether
the model output is plausible (i.e., the output is a reasonable and on-topic response to the question
as if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to
verify the validity of the answer). For S&P, we do not consider the instances where SELF -RAG
predicts irrelevant or no support. We then ask our annotators whether the model-predicted
reflection tokens about ISREL and ISSUP match their inspections (e.g., whether the fully supported
output is supported by the cited evidence). Human annotators find SELF -RAG answers are often
plausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is
consistent with Menick et al. (2022). Human annotators also find ISREL and ISSUP reflection token
predictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated
examples and explanations on assessments.
6 C ONCLUSION
This work introduces SELF -RAG, a new framework to enhance the quality and factuality of LLMs
through retrieval on demand and self-reflection. SELF -RAG trains an LM to learn to retrieve, generate,
and critique text passages and its own generation by predicting the next tokens from its original
vocabulary as well as newly added special tokens, called reflection tokens.SELF -RAG further enables
the tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on
six tasks using multiple metrics demonstrate that SELF -RAG significantly outperforms LLMs with
more parameters or with conventional retrieval-augmented generation approaches.
10



Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 2):

Preprint.
of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few-
shot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only
once at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation
on top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named
entities. Yet, the improved task performance of such approaches often comes at the expense of
runtime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of
attributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to
learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled
generation guided by reflections tokens to further improve generation quality and attributions.
Concurrent RAG work. A few concurrent works2 on RAG propose new training or prompting
strategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever
and LM on instruction-tuning datasets in two steps. While we also train our model on diverse
instruction-following datasets, SELF -RAG enables retrieval on demand and selection of the best
possible model output via fine-grained self-reflection, making it widely applicable and more robust
and controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use
a summarization model to filter out or compress retrieved passages before using them to prompt the
LM to generate the output. SELF -RAG processes passages in parallel and filters out irrelevant ones
through self-reflection, without relying on external models at inference. Moreover, our self-reflection
mechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou
et al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks
and to generate with tree search, guided by LM-generated value scores. While their value function
simply indicates an overall score of each generation, SELF -RAG trains to an arbitrary LM to learn to
generate fine-grained self-reflection and customizable inference.
Training and generating with critics. Training LLMs with reinforcement learning (e.g., Proximal
Policy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven
effective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce
fine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique
on retrieval and generation, we train our target LM on task examples augmented with reflection
tokens from a critic model offline, with a far lower training cost compared to RLHF. In addition,
reflection tokens in SELF -RAG enable controllable generation at inference, while RLHF focuses on
human preference alignment during training. Other works use general control tokens to guide LM
generation (Lu et al., 2022; Korbak et al., 2023), whileSELF -RAG uses reflection tokens to decide the
need for retrieval and to self-evaluate generation quality. Xie et al. (2023) propose a self-evaluation-
guided decoding framework, but they focus only on reasoning tasks with one evaluation dimension
(reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala
et al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural
language feedback and refined task output iteratively, but at the cost of inference efficiency.
3 S ELF -RAG: L EARNING TO RETRIEVE , GENERATE AND CRITIQUE
We introduce Self-Reflective Retrieval-Augmented Generation ( SELF -RAG), shown in Figure 1.
SELF -RAG is a framework that enhances the quality and factuality of an LLM through retrieval and
self-reflection, without sacrificing LLM’s original creativity and versatility. Our end-to-end training
lets an LM M generate text informed by retrieved passages, if needed, and criticize the output by
learning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval
or confirm the output’s relevance, support, or completeness. In contrast, common RAG approaches
retrieve passages indiscriminately, without ensuring complete support from cited sources.
3.1 P ROBLEM FORMALIZATION AND OVERVIEW
Formally, given input x, we train M to sequentially generate textual outputs y consisting of multiple
segments y = [y1, . . . , yT ], where yt indicates a sequence of tokens for the t-th segment.3 Generated
tokens in yt include text from the original vocabulary as well as the reflection tokens (Table 1).
2All work is arXived within a week of this preprint.
3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any
segment unit (i.e., sub-sentence).
3



Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 3):

Preprint.
Type Input Output Definitions
Retrieve x / x, y {yes, no, continue} Decides when to retrieve with R
ISREL x, d {relevant, irrelevant} d provides useful information to solve x.
ISSUP x, d, y {fully supported, partially
supported, no support}
All of the verification-worthy statement in y
is supported by d.
ISUSE x, y {5, 4, 3, 2, 1} y is a useful response to x.
Table 1: Four types of reflection tokens used inSELF -RAG. Each type uses several tokens to represent
its output values. The bottom three rows are three types of Critique tokens, and the bold text indicates
the most desirable critique tokens. x, y, dindicate input, output, and a relevant passage, respectively.
Algorithm 1 SELF -RAG Inference
Require: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , dN }
1: Input: input prompt x and preceding generation y<t, Output: next output segment yt
2: M predicts Retrieve given (x, y<t)
3: if Retrieve == Yes then
4: Retrieve relevant text passages D using R given (x, yt−1) ▷ Retrieve
5: M predicts ISREL given x, dand yt given x, d, y<t for each d ∈ D ▷ Generate
6: M predicts ISSUP and ISUSE given x, yt, dfor each d ∈ D ▷ Critique
7: Rank yt based on ISREL , ISSUP , ISUSE ▷ Detailed in Section 3.3
8: else if Retrieve == No then
9: Mgen predicts yt given x ▷ Generate
10: Mgen predicts ISUSE given x, yt ▷ Critique
Inference overview. Figure 1 and Algorithm 1 present an overview of SELF -RAG at inference. For
every x and preceding generation y<t, the model decodes a retrieval token to evaluate the utility
of retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a
standard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved
passage’s relevance, the next response segment, and a critique token to evaluate if the information in
the response segment is supported by the passage. Finally, a new critique token evaluates the overall
utility of the response.4 To generate each segment, SELF -RAG processes multiple passages in parallel
and uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control
(Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages
d1 is selected at the first time step since d2 does not provide direct evidence ( ISREL is Irrelevant)
and d3 output is only partially supported while d1 are fully supported.
Training overview. SELF -RAG enables an arbitrary LM to generate text with reflection tokens
by unifying them as next token predictions from the expanded model vocabulary (i.e., the original
vocabulary plus reflection tokens). Specifically, we train the generator model M on a curated corpus
with interleaving passages retrieved by a retriever R and reflection tokens predicted by a critic model
C (summarized in Appendix Algorithm 2). We train C to generate reflection tokens for evaluating
retrieved passages and the quality of a given task output (Section 3.2.1). Using the critic model, we
update the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we
train the final generator model (M) using the conventional LM objective (Section 3.2.2) to enable
M to generate reflection tokens by itself without relying on the critic at inference time.
3.2 S ELF -RAG TRAINING
Here, we describe the supervised data collection and training of two models, the criticC (Section 3.2.1)
and the generator M (Section 3.2.2).
3.2.1 T RAINING THE CRITIC MODEL
Data collection for critic model. Manual annotation of reflection tokens for each segment is
expensive (Wu et al., 2023). A state-of-the-art LLM like GPT-4 (OpenAI, 2023) can be effectively
4We follow Liu et al. (2023a) in using a “perceived” utility value that is independent of retrieved passages.
4



Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 5):

Preprint.
separate reward models during training, we compute critique offline and directly insert them into the
training corpus, where the generator LM is trained with a standard LM objective. This significantly
reduces training costs compared to PPO. Our work also relates to prior work that incorporates special
tokens to control generation (Keskar et al., 2019; Lu et al., 2022; Korbak et al., 2023). OurSELF -RAG
learns to generate special tokens to evaluate its own predictionafter each generated segment, enabling
the use of a soft re-ranking mechanism or hard constraints at inference (discussed next).
3.3 S ELF -RAG INFERENCE
Generating reflection tokens to self-evaluate its own output makes SELF -RAG controllable during the
inference phase, enabling it to tailor its behavior to diverse task requirements. For tasks demanding
factual accuracy (Min et al., 2023), we aim for the model to retrieve passages more frequently to
ensure that the output aligns closely with the available evidence. Conversely, in more open-ended
tasks, like composing a personal experience essay, the emphasis shifts towards retrieving less and
prioritizing the overall creativity or utility score. In this section, we describe approaches to enforce
control to meet these distinct objectives during the inference process.
Adaptive retrieval with threshold.SELF -RAG dynamically decides when to retrieve text passages by
predicting Retrieve . Alternatively, our framework allows a threshold to be set. Specifically, if the prob-
ability of generating the Retrieve =Yes token normalized over all output tokens in Retrieve surpasses a
designated threshold, we trigger retrieval (details in Appendix Section A.3).
Tree-decoding with critique tokens. At each segment step t, when retrieval is required, based either
on hard or soft conditions, R retrieves K passages, and the generator M processes each passage in
parallel and outputs K different continuation candidates. We conduct a segment-level beam search
(with the beam size=B) to obtain the top-B segment continuations at each timestamp t, and return
the best sequence at the end of generation. The score of each segment yt with respect to passage d is
updated with a critic score S that is the linear weighted sum of the normalized probability of each
Critique token type. For each critique token group G (e.g., ISREL ), we denote its score at timestamp
t as sG
t , and we compute a segment score as follows:
f(yt, d,Critique ) =p(yt|x, d, y<t)) +S( Critique ), where (3)
S( Critique ) =
X
G∈G
wGsG
t for G = { ISREL , ISSUP , ISUSE }, (4)
where sG
t = pt(ˆr)PNG
i=1 pt(ri)
stands for the generation probability of the most desirable reflection token
ˆr (e.g., ISREL =Relevant) for the critique token type G with NG distinct tokens (that represent
different possible values for G). The weights wG in Eq. 4 are hyperparameters that can be adjusted
at inference time to enable customized behaviors at test time. For instance, to ensure that result
y is mostly supported by evidence, we can set a weight term for the ISSUP score higher, while
relatively lowering weights for other aspects. Alternatively, we could further enforce hard constraints
during decoding using Critique . Instead of using a soft reward function in Eq. 4, we could explicitly
filter out a segment continuation when the model generates an undesirable Critique token (e.g.,
ISSUP =No support) . Balancing the trade-off between multiple preferences has been studied
in RLHF (Touvron et al., 2023; Wu et al., 2023), which often requires training to change models’
behaviors. SELF -RAG tailors an LM with no additional training.
4 E XPERIMENTS
4.1 T ASKS AND DATASETS
We conduct evaluations of our SELF -RAG and diverse baselines on a range of downstream tasks,
holistically evaluating outputs with metrics designed to assess overall correctness, factuality, and
fluency. Throughout these experiments, we conduct zero-shot evaluations, where we provide instruc-
tions describing tasks without few-shot demonstrations (Wei et al., 2022; Sanh et al., 2022). Details of
our experiments’ settings, including test-time instructions, are available in the Appendix Section B.1.
Closed-set tasks include two datasets, i.e., a factverification dataset about public health (PubHealth;
Zhang et al. 2023) and a multiple-choice reasoning dataset created from scientific exams ( ARC-
6



### Claim 69/179

#### Claim Text
For instance, question answering evaluations might rely on EM and F1 scores [7], [45], [59], [72], whereas fact-checking tasks often hinge on Accuracy as the primary metric [4], [14], [42].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 5):

Method HotPotQA 2WikiMultiHopQA MuSiQue Bamboogle Feverous StrategyQA
EM F1 Acc † EM F1 Acc † EM F1 Acc † EM F1 Acc † Acc Acc† Acc Acc†
Without Retrieval
Direct 21.9 36.8 44.8 21.3 29.2 33.9 7.0 18.7 15.8 11.2 24.4 28.0 60.1 60.1 66.5 66.7
CoT 30.0 44.1 50.0 30.0 39.6 44.0 19.4 30.9 28.6 43.2 51.1 60.0 59.8 59.8 71.0 71.0
With Retrieval
Direct 31.6 44.7 53.3 27.3 35.4 43.6 13.9 28.2 26.5 17.6 31.8 43.2 69.8 69.8 65.6 65.6
ReAct 24.9 44.7 61.1 28.0 38.5 45.9 23.4 37.0 37.9 21.8 31.0 40.3 66.4 66.4 66.9 66.9
Self-Ask 36.8 55.2 64.8 37.3 48.8 55.9 27.6 41.5 42.9 31.5 41.2 54.8 70.7 70.7 70.2 70.2
DSP 43.8 55.0 60.8 - - - - - - - - - - - - -
ITER-RETGEN 1 39.2 53.9 65.5 33.7 45.2 55.4 24.2 38.6 38.1 36.8 47.7 57.6 67.0 67.0 72.0 72.0
ITER-RETGEN 2 44.1 58.6 71.2 34.9 47.0 58.1 26.4 41.1 41.0 38.4 48.7 59.2 68.8 68.8 73.0 73.0
ITER-RETGEN 3 45.2 59.9 71.4 34.8 47.8 58.3 25.7 41.4 40.8 37.6 47.0 59.2 69.0 69.0 72.3 72.3
ITER-RETGEN 4 45.8 61.1 73.4 36.0 47.4 58.5 26.7 41.8 40.8 38.4 49.6 60.0 71.5 71.5 73.8 73.8
ITER-RETGEN 5 45.2 60.3 72.8 35.5 47.5 58.8 25.7 40.7 39.6 39.2 49.7 60.8 70.3 70.3 73.2 73.2
ITER-RETGEN 6 45.9 61.0 73.3 35.5 48.1 59.4 25.9 40.5 39.8 40.0 50.0 59.2 70.9 70.9 72.4 72.4
ITER-RETGEN 7 45.1 60.4 72.9 35.5 47.4 58.4 26.1 42.0 41.0 40.0 50.7 60.8 70.5 70.5 74.1 74.1
Table 2: Evaluation results on multi-hop question answering, fact verification, and commonsense reasoning datasets.
Acc† is the accuracy of model outputs evaluated with text-davinci-003. For ITER -RETGEN, we evaluated LLM
outputs in different iterations (up to 7 iterations). Underlined metric values are higher than those of Self-Ask.
Method HotPotQA 2WikiMultiHopQA MuSiQue Bamboogle Feverous StrategyQA
# API # Doc # API # Doc # API # Doc # API # Doc # API # Doc # API # Doc
ReAct 2.9 14.3 3.0 15.0 2.9 14.4 2.8 14.1 2.1 10.6 2.8 14.2
Self-Ask 3.2 16.0 3.2 15.9 3.0 14.8 3.0 14.9 2.3 11.3 3.0 15.1
Table 3: Average numbers of API calls to text-davinci-003 and retrieved paragraphs for ReAct and Self-Ask.
Note that ITER -RETGEN (T = 2) achieves significantly higher or competitive Acc† with fewer API calls (i.e., 2)
and fewer retrieved paragraphs (5 per iteration, 10 in total).
It is worth noting that, as shown by Table 3, ITER -
RETGEN (T = 2) is superior to or competitive with
ReAct and Self-Ask using fewer API calls to the
LLM (i.e., 2) and fewer retrieved paragraphs (i.e.,
5 per iteration, 10 in total). ITER -RETGEN is also
conceptually simple, which is to iterate retrieval-
augmented CoT, without complex processing.
We also compared I TER -RETGEN with DSP
which also generates the answer using CoT based
on retrieved knowledge but differs in information
collection and processing. In each iteration, ITER -
RETGEN retrieves knowledge based on (1) the ques-
tion and (2) the previous model output which shows
what may be needed to answer the question. With
the number of iterations increasing, we tend to
obtain a more comprehensive and relevant set of
knowledge. Besides, unlike DSP, we do not summa-
rize the retrieved documents for answer generation,
and thus will not introduce summarization errors.
As shown in Table 2, ITER -RETGEN outperforms
DSP significantly. We manually investigate 10 ran-
dom questions where DSP fails but ITER -RETGEN
provides correct answers. On 40% of them, DSP
fails to retrieve documents that cover the correct
answers, while on 50% of them, the summarized
knowledge is misleading, e.g., for the question
“What occupation do Chris Menges and Aram
Avakian share?”, DSP generates a wrong sum-
mary “Chris Menges and Aram Avakian are
both members of the American and British
Societies of Cinematographers.”, while the
retrieved documents mention that Aram Avakian is
a film editor and director, and only Chris Menges
is with the American and British Societies of Cine-
matographers.
Acc† is a Reliable Metric To investigate how re-
liable Acc† is, we focused on model outputs where
EM and Acc† disagree, and manually checked
which metric gives more correct labels. On each
of the four multi-hop question answering datasets,



Source: data\tc16_2312.10997v5\referenced_papers\[20]_2303.08518.pdf (Page 5):

Task Metric GPT-Neo-2.7B BLOOM-7.1B OPT-66B Davinci Davinci-001
0-SHOT UPRISE 0-SHOT UPRISE 0-SHOT UPRISE 0-SHOT UPRISE 0-SHOT UPRISE
Reading Comprehension
SQuADv1 F1 4.4 26.4 4.5 5.5 6.1 7.5 6.5 6.0 41.6 57.7
EM 0.4 14.3 0.0 0.0 0.0 0.6 0.0 0.0 16.4 36.8
BoolQ Acc 54.5 59.4 54.0 60.2 60.7 63.5 62.0 65.7 64.2 65.7
MultiRC F1 57.1 58.1 58.8 59.8 59.6 60.4 59.8 60.0 54.3 58.9
OBQA Acc 41.8 42.2 44.0 41.8 46.4 48.8 49.2 52.4 52.8 48.8
Average 31.6 40.1 32.3 33.5 34.6 36.2 35.5 36.8 45.9 53.6
Closed-book QA
ARC-e Acc 45.7 55.6 53.7 60.9 56.2 66.0 64.1 71.8 67.0 74.4
ARC-c Acc 29.3 30.0 33.2 34.2 36.7 40.2 40.8 45.2 46.2 50.4
NQ F1 1.3 5.6 0.9 1.4 2.5 2.1 0.0 2.2 18.3 18.2
EM 0.5 2.2 0.0 0.1 0.3 0.4 0.0 0.0 4.8 8.7
Average 19.2 23.3 22.0 24.2 23.9 27.2 26.2 29.8 34.1 37.9
Paraphrase Detection
MRPC Acc 46.6 67.9 51.0 70.6 51.0 68.9 54.4 62.3 40.0 61.3
F1 46.0 80.4 58.0 82.1 57.8 81.5 68.9 81.4 39.2 72.9
QQP Acc 48.4 54.3 49.5 53.1 50.5 49.7 55.2 52.4 60.9 62.6
F1 42.2 59.8 46.7 59.6 43.7 58.5 33.7 57.9 43.0 45.9
PAWS Acc 51.7 45.7 50.8 45.9 50.5 44.4 52.4 44.5 53.2 52.3
Average 47.0 61.6 51.2 62.3 50.7 60.6 52.9 59.7 47.3 59.0
Natural Language Inference
MNLI-m Acc 35.3 41.3 35.4 36.0 37.0 40.4 34.2 38.2 44.7 41.1
MNLI-mm Acc 36.4 43.1 34.9 35.8 37.1 41.2 34.2 38.6 46.5 42.1
QNLI Acc 50.9 53.8 49.9 51.3 54.2 53.7 51.7 51.1 60.0 58.4
SNLI Acc 35.2 42.3 35.2 34.4 34.5 40.2 33.5 37.9 47.5 42.0
RTE Acc 33.6 34.7 50.5 49.8 52.3 46.9 51.3 45.5 52.3 50.9
Average 38.3 43.0 41.2 41.5 43.0 44.5 41.0 42.3 50.2 46.9
Sentiment Analysis
SST-2 Acc 52.4 56.2 63.2 69.1 57.9 65.3 52.3 64.3 90.5 90.5
Yelp Acc 71.7 67.8 56.1 58.0 67.6 63.5 59.8 65.3 80.3 80.2
Sent140 Acc 64.1 61.3 74.5 72.1 59.1 61.6 64.3 72.1 87.2 89.1
Average 62.7 61.8 64.6 66.4 61.5 63.5 58.8 67.3 86.0 86.6
Commonsense Reasoning
PiQA Acc 70.2 70.4 71.5 72.1 76.5 80.4 79.1 81.3 79.1 79.1
COPA Acc 67.0 64.0 67.0 67.0 74.0 76.0 80.0 83.0 83.0 80.0
HellaSwag Acc 54.4 52.1 59.6 58.8 72.9 71.4 76.9 76.7 77.6 78.2
Average 63.9 62.2 66.0 66.0 74.5 75.9 78.7 80.3 79.9 79.1
Coreference Resolution
WSC273 Acc 73.6 76.6 78.0 81.0 83.9 86.1 60.6 50.0 78.8 75.5
DPR Acc 59.6 51.0 64.4 55.8 66.3 50.0 82.1 83.9 64.4 58.7
Winogrande Acc 58.9 58.6 65.9 64.3 69.2 67.8 68.6 70.2 66.3 64.7
Average 64.0 62.1 69.4 67.0 73.1 68.0 70.4 68.0 69.8 66.3
Table 1: Zero-shot performance across tasks and LLMs. The model Davinci-001 is the fine-tuned version
text-davinci-001 of Davinci. The method 0-SHOT is the vanilla zero-shot method with only the in-
put instruction fed into the LLM.



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 6):

ARES Ranking of Pseudo RAG Systems
NQ HotpotQA WoW FEVER MultiRC ReCoRD
C.R A.R. C.R A.R. C.R A.R. C.R A.R. C.R A.R. C.R A.R.
Kendall’s Tau for
Sampled Annotations 0.83 0.89 0.78 0.78 0.78 0.83 0.89 0.89 0.83 0.83 0.72 0.94
Kendall’s Tau
for RAGAS 0.89 0.89 0.94 0.89 0.94 0.94 0.72 0.61 0.83 0.94 0.89 0.44
Kendall’s Tau
for GPT-3.5 Judge 0.89 0.94 0.67 0.94 0.94 0.89 0.78 0.78 0.83 0.89 0.83 0.94
Kendall’s Tau for
ARES LLM Judge 0.89 1.0 0.89 0.94 0.94 1.0 0.83 0.72 0.94 0.83 0.78 0.83
Kendall’s Tau
for ARES 0.94 1.0 0.94 0.94 1.0 1.0 0.89 0.78 0.94 0.89 0.83 0.89
RAGAS Accuracy 31.4% 71.2% 17.2% 76.0% 36.4% 77.8% 23.7% 69.2% 16.1% 75.0% 15.0% 72.8%
GPT-3.5 Judge Accuracy 73.8% 95.5% 75.3% 71.6% 84.3% 85.2% 60.4% 59.6% 72.4% 60.3% 81.0% 65.8%
ARES Accuracy 79.3% 97.2% 92.3% 81.3% 85.7% 96.1% 88.4% 78.5% 85.8% 82.7% 67.8% 92.3%
Table 1: ARES Ranking with Fine-tuned LLM Judges vs. Sampled Annotations, RAGAS and GPT-3.5 Judge:
For scoring context relevance and answer relevance (C.R. and A.R. in the table, respectively), we compare ARES
with our fine-tuned LLM judges against sampled annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge.
For our sampled annotations, we gather 150 annotated datapoints from each mock RAG system and use those labels
to score the system. RAGAS also uses GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for
each evaluation domain. Overall, we found that ARES ranked RAG systems more accurately than RAGAS and
GPT-3.5 across all the explored datasets. The Kendall’s tau for ARES was 0.065 higher on average for scoring
context relevance and 0.132 higher on average for scoring answer relevance than RAGAS. Additionally, we include
the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted the ranking accuracy of
the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial costs required to run.
For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human preference validation
set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.
used ARES with human annotation sets ranging
in size from 25 to 400 and found that 150 is the
minimum number required (Table 3). Second, we
explored whether GPT-4 generations could replace
human annotations entirely, finding that GPT-4 is
less good than humans in this role, though the idea
arguably has promise (Table 4).
5.2 ARES Performance on AIS
WoW CNN / DM
ARES Split Prediction 0.478 0.835
Correct Positive/Negative Split 0.458 0.859
ARES Judge Accuracy 62.5% 84.0%
Evaluation Set Size 707 510
Human Preference Data Size 200 200
Table 2: ARES Results on the AIS benchmark
To evaluate whether ARES can effectively gauge
answer faithfulness in real RAG systems, we tested
ARES on the AIS attribution benchmark (Rashkin
et al., 2022). In AIS, we selected the Wizards
of Wikipedia (WoW) and CNN/DM datasets; the
other benchmark datasets involve either table rea-
soning (ToTTo) or focus on passage summariza-
tion (QRECC) so we excluded them. In WoW
and CNN/DM, each evaluation example includes
a query, a retrieved passage, and a generated an-
swer (which is either faithful or non-attributed to
the retrieved passage).
Table 2 summarizes our AIS results. We found
that ARES can effectively score the AIS datasets,
getting within 2.5 accuracy points of the correct
scores. Furthermore, for scoring each system,
we only use 200 annotated datapoints for our hu-
man preference validation set. Our results on AIS
demonstrate the ability of ARES to reliably dis-
tinguish faithful and hallucinated answers in real-
world RAG systems.
5.3 ARES Ranking of Existing RAG Systems
We also wanted to evaluate whether ARES can
score and rank existing RAG systems across both
context relevance and answer relevance. For eval-
uation, we selected the NQ, WoW, and FEVER
datasets from KILT. We consider the answer gen-



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 15):

ARES Ranking of Real RAG Systems
NQ WoW FEVER
C.R. A.R. C.R. A.R. C.R. A.R.
Kendall’s Tau for
Sampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82
Kendall’s Tau
for RAGAS 0.82 0.82 0.73 0.82 0.73 0.87
Kendall’s Tau
for GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87
Kendall’s Tau
for ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87
Kendall’s Tau
for ARES 1.0 0.96 0.91 1.0 0.82 1.0
RAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%
GPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%
ARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%
Table 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance
(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled
annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150
annotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses
GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we
found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.
Additionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted
the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial
costs required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human
preference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.
ARES Cross-Domain Ranking of Pseudo RAG Systems
NQ to
FEVER
FEVER to
NQ
NQ to
MultiRC
MultiRC to
NQ
NQ to
ReCoRD
ReCoRD to
NQ
C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.
Kendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94
Kendall’s Tau of
In-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0
Average PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%
Accuracy on
RAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%
Table 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the
fine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance
(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains
when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and
ReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that
additional examples further improved the performance of ARES. Furthermore, we found that even in scenarios
where the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ
to FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of
percentage points from the lower bound to the upper bound of the PPI confidence interval.



Source: data\tc16_2312.10997v5\referenced_papers\[164]_2309.15217.pdf (Page 2):

we usually do not have access to human-annotated
datasets or reference answers. We therefore fo-
cus on metrics that are fully self-contained and
reference-free. We focus in particular three quality
aspects, which we argue are of central importance.
First, Faithfulness refers to the idea that the an-
swer should be grounded in the given context. This
is important to avoid hallucinations, and to ensure
that the retrieved context can act as a justification
for the generated answer. Indeed, RAG systems are
often used in applications where the factual con-
sistency of the generated text w.r.t. the grounded
sources is highly important, e.g. in domains such as
law, where information is constantly evolving. Sec-
ond, Answer Relevancerefers to the idea that the
generated answer should address the actual ques-
tion that was provided. Finally,Context Relevance
refers to the idea that the retrieved context should
be focused, containing as little irrelevant informa-
tion as possible. This is important given the cost
associated with feeding long context passages to
LLMs. Moreover, when context passages are too
long, LLMs are often less effective in exploiting
that context, especially for information that is pro-
vided in the middle of the context passage (Liu
et al., 2023).
We now explain how these three quality aspects
can be measured in a fully automated way, by
prompting an LLM. In our implementation and
experiments, all prompts are evaluated using the
gpt-3.5-turbo-16k model, which is available
through the OpenAI API2.
Faithfulness We say that the answer as(q) is
faithful to the context c(q) if the claims that are
made in the answer can be inferred from the con-
text. To estimate faithfulness, we first use an LLM
to extract a set of statements, S(as(q)). The aim
of this step is to decompose longer sentences into
shorter and more focused assertions. We use the
following prompt for this step3:
Given a question and answer, create one
or more statements from each sentence
in the given answer.
question: [question]
answer: [answer]
where [question] and [answer] refer to the
given question and answer. For each statement si
2https://platform.openai.com
3To help clarify the task, we include a demonstration as
part of the prompt. This demonstration is not explicitly shown
in the listing of the prompts throughout this paper.
in S, the LLM determines ifsi can be inferred from
c(q) using a verification function v(si, c(q)). This
verification step is carried out using the following
prompt:
Consider the given context and following
statements, then determine whether they
are supported by the information present
in the context. Provide a brief explana-
tion for each statement before arriving
at the verdict (Yes/No). Provide a final
verdict for each statement in order at the
end in the given format. Do not deviate
from the specified format.
statement: [statement 1]
...
statement: [statement n]
The final faithfulness score, F, is then computed
as F = |V |
|S| , where |V | is the number of statements
that were supported according to the LLM and |S|
is the total number of statements.
Answer relevance We say that the answer as(q)
is relevant if it directly addresses the question in
an appropriate way. In particular, our assessment
of answer relevance does not take into account fac-
tuality, but penalises cases where the answer is
incomplete or where it contains redundant informa-
tion. To estimate answer relevance, for the given
answer as(q), we prompt the LLM to generate n
potential questions qi based on as(q), as follows:
Generate a question for the given answer.
answer: [answer]
We then obtain embeddings for all questions us-
ing the text-embedding-ada-002 model, avail-
able from the OpenAI API. For each qi, we cal-
culate the similarity sim(q, qi) with the original
question q, as the cosine between the correspond-
ing embeddings. The answer relevance score, AR,
for question q is then computed as:
AR = 1
n
nX
i=1
sim(q, qi) (1)
This metric evaluates how closely the generated
answer aligns with the initial question or instruc-
tion.
Context relevance The context c(q) is consid-
ered relevant to the extent that it exclusively con-
tains information that is needed to answer the ques-
tion. In particular, this metric aims to penalise the



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[62]_2307.11019.pdf (Page 2):

Retrieval-augmented 
Setting    Evaluation
Normal Setting
  
  
  QA Prompting
  Judgemental Prompting
Compare with
ground truth
Give up
answering?
Give-up
True or False?
Eval-Right
EM score
F1 score
Right/G
Right/¬G
Eval-Acc
Retriever
Relative 
documents
Retrieval-augmented
(optional)
Answer the given question with 
one or few words.
The answer to the 
question is ...
Do you know the answer with 
existing knowledge?
Discriminate 
right or wrong
I’m sorry that I can 
not answer.
Please check if the given answer 
is correct.
I think the answer 
is correct.
QA
Priori
Posteriori
Question
Given the following 
information...Given the following 
information... Given the following 
information...Given the following 
information...
Figure 1: The illustration of different settings to instruct LLMs, the evaluation metrics are also displayed.
to match the short-answer format for assessment
with normally used metrics. To this end, we pro-
pose two distinct instructional approaches. (1) Nor-
mal setting:In this approach, LLMs are instructed
to respond to questions based solely on their in-
ternal knowledge, as formulated in Equation (1).
(2) Retrieval-augmented setting: This approach
requires LLMs to answer the given questions us-
ing a combination of their internal knowledge and
information from supporting documents retrieved,
as outlined in Equation (2). In this setting, sup-
porting documents are optional, and ideally, LLMs
should determine whether to refer to the supporting
documents based on their reliability.
2.2.2 Judgemental Prompting
To investigate whether LLMs are capable of per-
ceiving their own factual knowledge boundary, we
propose judgemental prompting to evaluate the
judging abilities of LLMs.
Similar to QA prompting, the concepts of the
normal setting and the retrieval-augmented set-
ting are also applicable for judgemental prompt-
ing, where LLMs utilizing their own knowledge or
consulting supporting documents from retrievers to
carry out the judgement process. Furthermore, we
construct instructions from different judgement per-
spectives. (1) Priori judgement:LLMs are tasked
with determining their capability to provide an an-
swer to the question, employing either the normal
setting or the retrieval-augmented setting. Priori
judgment is carried out before the answering of
LLMs, through this predictive approach, we are
able to evaluate the confidence levels and assess-
ment accuracies of LLMs in their mastery of knowl-
edge. (2) Posteriori judgement:LLMs are tasked
with evaluating the correctness of the answer to the
question provided by itself, employing either a nor-
mal setting or a retrieval-augmented setting. Poste-
riori judgement aims to enable LLMs to judge their
responses. Through this reflective approach, we
can evaluate the confidence levels and assessment
accuracies of LLMs in the content they generate.
2.3 Experimental Settings
In this part, we set up our experiments on open-
domain QA, including evaluation metrics and re-
trieval sources. Due to the space limitaion, more
settings can be found in Appendix A, including
datasets, evaluation models, implemental details
and instruction designs.
2.3.1 Evaluation Metrics
Following previous works (Chen et al., 2017;
Izacard and Grave, 2021a; Sun et al., 2023), we use
the exact match (EM)score and F1 score to evalu-
ate the QA performance of LLMs. EM calculates
the percentage of questions in which the answer
predicted by LLMs precisely matches the correct
answer to the question. F1 is used to measure the
overlap between the predicted answer and the cor-
rect answer, combines precision and recall into a
single metric by taking their harmonic mean.
We also propose several metrics for evaluat-
ing LLMs’ judgement abilities. Give-up denotes
the percentage of questions that LLMs give up



Source: data\tc16_2312.10997v5\referenced_papers\[62]_2307.11019.pdf (Page 3):

answering, which reflects the LLMs’ judgement
on whether they possess the relevant knowledge.
Right/G represents the probability that LLMs give
up answering but can actually answer correctly.
Right/¬G represents the probability that LLMs
do not give up answering and can answer cor-
rectly. Eval-Right refers to the proportion of ques-
tions where LLMs assess their answers as correct.
Eval-Acc represents the percentage of questions for
which the assessment of the answer by LLMs aligns
with the fact. Among them, Give-up, Right/G
and Right/ ¬G are metrics for priori judgement,
Eval-Right and Eval-Acc are metrics for posteriori
judgement. All metrics are illustrated in Figure 1.
2.3.2 Retrieval Sources
We consider multiple retrieval sources to ac-
quire supporting documents, including dense re-
trieval (Gao and Callan, 2021; Ren et al., 2021a;
Zhuang et al., 2022; Zhou et al., 2022), sparse re-
trieval (Robertson et al., 2009) and ChatGPT.
For the dense retriever, we utilize RocketQAv2
(Ren et al., 2021b) to find semantically relevant
documents for questions. To achieve this, we train
the model on each dataset with the constructed
in-domain training data under the settings of Rock-
etQAv2 and leverage Faiss (Johnson et al., 2019) to
obtain relevant documents for each question from
the candidate corpus. For the sparse retriever, we
use BM25 (Yang et al., 2017) to find lexical rele-
vant documents for questions. Similar to previous
works (Yu et al., 2022; Ren et al., 2023), we re-
gard the generative language model as a “retriever”
that “retrieves” knowledge from its memory, where
ChatGPT is instructed to produce relevant docu-
ments in response to a given question. The retrieval
results can be found in Appendix A.4.
3 Experimental Analysis and Findings
In this section, we focus on addressing the three
research questions and tackle them by investigating
the judgement abilityand the QA abilityof LLMs
with the proposed strategies in Section 2.2.
3.1 To What Extent Can LLMs Perceive
Their Factual Knowledge Boundaries?
We deconstruct the question and investigate the
following points: (a) Before answering: how do
LLMs decide to abstain from a question? (b) When
answering: can LLMs accurately address a given
question? (c) After answering: how do LLMs as-
sess the correctness of their answers?
LLM
QA Priori Judgement Posteriori Judgement
EM F1 Give-up Right/ ¬G Eval-Right Eval-Acc
NQ
Davinci003 27.20 36.20 29.20% 32.77% 72.80% 45.01%
ChatGPT 33.40 45.32 57.40% 42.72% 84.40% 43.40%
GPT-4 34.60 48.72 15.20% 39.15% 90.20% 38.87%
LLaMA2 16.60 24.26 6.60% 17.56% 58.40% 46.74%
Mistral 11.20 19.30 49.80% 15.94% 68.00% 37.90%
TriviaQA
Davinci003 65.20 69.57 7.40% 67.17% 87.00% 69.82%
ChatGPT 69.00 75.29 25.00% 75.73% 88.80% 71.95%
GPT-4 75.80 84.52 8.80% 77.85% 93.00% 76.57%
LLaMA2 48.80 53.40 4.80% 50.21% 75.60% 57.60%
Mistral 36.20 42.09 34.80% 46.63% 86.00% 48.10%
HotpotQA
Davinci003 18.40 26.78 35.40% 24.15% 70.60% 43.99%
ChatGPT 20.80 29.27 78.40% 31.48% 66.80% 43.12%
GPT-4 28.60 40.33 54.80% 42.92% 72.40% 45.74%
LLaMA2 11.40 16.88 25.60% 12.63% 49.80% 54.88%
Mistral 10.80 17.86 64.00% 19.44% 81.80% 27.40%
Table 1: Evaluation results on three datasets without
retrieval augmentation. The abbreviations are explained
in Section 2.3.1.
3.1.1 Settings
In this part of the experiments, we employ the
priori judgement with the normal setting to instruct
LLMs on the decision of whether to give up an-
swering questions based on their inherent knowl-
edge, and we use the QA prompting with the nor-
mal setting to instruct LLMs to provide answers.
Moreover, we employ posteriori judgement with
the normal setting to instruct LLMs in evaluating
the correctness of their answers.
3.1.2 Main Findings
LLMs struggle to perceive their factual knowl-
edge boundary, and tend to be overconfident.
Overall, the overestimation of LLMs is evident
both before and after answering questions in Ta-
ble 1. Before answering, LLMs frequently over-
estimate their knowledge, leading to a high rate
of incorrect responses ( Right/¬G). Additionally,
their Give-up rate is substantially low, misaligned
with their actual QA abilities (reflected by EM).
When we instruct LLMs to evaluate their answers
for posteriori judgement, they also exhibit a signifi-
cant tendency to believe that their answers are cor-
rect. For this reason, we can observe much higher
Eval-Right values compared to EM. However, there
exists a substantial disparity between Eval-Right
value and the actual evaluation accuracy, as indi-
cated by relatively low Eval-Acc metrics. Similar
to previous studies (Kamalloo et al., 2023), LLMs
still demonstrate commendable performance in QA
tasks (EM and F1), even without external docu-
ments. This indicates that LLMs possess a sub-
stantial knowledge base and can leverage it to a



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 7):

Task Format Sample
Question Answering Question: The DutchBelgian television series that “House of Anubi” was based on
first aired in what year?
Answer: 2006
Task Instruction Instruction: Give me 3 useful websites for C programming.
Response:
1. GeeksforGeeks: This website provides tutorials and practice problems on C pro-
gramming.
2. Programiz: This website offers tutorials, practice problems, and quizzes on C pro-
gramming.
3. Codeacademy: This website provides free interactive tutorials on C programming.
Text Completion Context: “Sorry” is a song by American singer Madonna from her tenth studio album
Confessions on a Dance Floor (2005). It was written and produced by Madonna and
Stuart Price, and released as the second single from the album on February 7, 2006. It
later appeared on Celebration, her 2009 greatest hits album. An uptempo dance song,
“Sorry” was one of the first tracks developed for the album and had numerous remix
treatments before the ultimate version of the track was finalized.
Completion: One of the remixes was done by the known band the Pet Shop Boys,
featuring added lyrics by the band
Table 5: Illustrative examples for the task format where existing benchmarks evaluate hallucinations.
tion platform to facilitate fine-grained event anno-
tations.
3.2 Evaluation Metrics
The free-form and open-ended nature of language
generation makes it difficult to evaluate the hal-
lucinations produced by LLMs. The most com-
monly used and reliable methods for evaluating
hallucinations rely on human experts following
specific principles (Lin et al., 2021; Lee et al.,
2022; Min et al., 2023; Li et al., 2023a). It is worth
noting that although existing benchmarks use hu-
man evaluation to ensure reliability, they also seek
to support automatic methods to facilitate effi-
cient and consistent evaluation.
Human evaluation. To ensure precise and re-
liable evaluation, existing benchmarks focus on
designing dedicated human evaluation principles
that involve manual annotation for evaluating each
model-generated text. TruthfulQA (Lin et al.,
2021) proposes a human-annotation guideline,
which instructs annotators to assign one of thir-
teen qualitative labels to the model output and ver-
ify answers by consulting a reliable source. Lee
et al. (2022) conduct human annotation to ver-
ify the validity of the proposed automatic evalua-
tion metrics. FactScore (Min et al., 2023) requires
annotators to assign three labels to each atomic
fact: "Supported" or "Not-supported" for facts that
are supported or unsupported by the knowledge
source, and "Irrelevant" for statements that are not
related to the prompt. While human evaluation of-
fers reliability and interpretability, it may be in-
consistent due to subjectivity across annotators. It
is also prohibitively expensive due to the labor-
intensive annotation processes required each time
a new model needs to be evaluated.
Model-based automatic evaluation. Several
studies (Lin et al., 2021; Min et al., 2023; Zha
et al., 2023; Mündler et al., 2023) have devised
model-based methods as a proxy for human eval-
uation. Specifically, TruthfulQA (Lin et al., 2021)
trains a GPT-3-6.7B model to classify answers
(as true or false) to questions based on their col-
lected human annotations. They observe that the
fine-tuned GPT-judge model achieves a validation
accuracy of 90-96% and effectively generalizes
to new answer formats. AlignScore (Zha et al.,
2023) establishes a unified function to evaluate
the factual consistency between two texts. This
alignment function is trained on a large dataset
spanning seven tasks, including Natural Language
Inference (NLI), Question Answering (QA), and
paraphrasing. Differently, Min et al. (2023) and
Mündler et al. (2023) harness the capabilities of
off-the-shelf models to serve as automatic evalu-
8



Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 5):

Method HotPotQA 2WikiMultiHopQA MuSiQue Bamboogle Feverous StrategyQA
EM F1 Acc † EM F1 Acc † EM F1 Acc † EM F1 Acc † Acc Acc† Acc Acc†
Without Retrieval
Direct 21.9 36.8 44.8 21.3 29.2 33.9 7.0 18.7 15.8 11.2 24.4 28.0 60.1 60.1 66.5 66.7
CoT 30.0 44.1 50.0 30.0 39.6 44.0 19.4 30.9 28.6 43.2 51.1 60.0 59.8 59.8 71.0 71.0
With Retrieval
Direct 31.6 44.7 53.3 27.3 35.4 43.6 13.9 28.2 26.5 17.6 31.8 43.2 69.8 69.8 65.6 65.6
ReAct 24.9 44.7 61.1 28.0 38.5 45.9 23.4 37.0 37.9 21.8 31.0 40.3 66.4 66.4 66.9 66.9
Self-Ask 36.8 55.2 64.8 37.3 48.8 55.9 27.6 41.5 42.9 31.5 41.2 54.8 70.7 70.7 70.2 70.2
DSP 43.8 55.0 60.8 - - - - - - - - - - - - -
ITER-RETGEN 1 39.2 53.9 65.5 33.7 45.2 55.4 24.2 38.6 38.1 36.8 47.7 57.6 67.0 67.0 72.0 72.0
ITER-RETGEN 2 44.1 58.6 71.2 34.9 47.0 58.1 26.4 41.1 41.0 38.4 48.7 59.2 68.8 68.8 73.0 73.0
ITER-RETGEN 3 45.2 59.9 71.4 34.8 47.8 58.3 25.7 41.4 40.8 37.6 47.0 59.2 69.0 69.0 72.3 72.3
ITER-RETGEN 4 45.8 61.1 73.4 36.0 47.4 58.5 26.7 41.8 40.8 38.4 49.6 60.0 71.5 71.5 73.8 73.8
ITER-RETGEN 5 45.2 60.3 72.8 35.5 47.5 58.8 25.7 40.7 39.6 39.2 49.7 60.8 70.3 70.3 73.2 73.2
ITER-RETGEN 6 45.9 61.0 73.3 35.5 48.1 59.4 25.9 40.5 39.8 40.0 50.0 59.2 70.9 70.9 72.4 72.4
ITER-RETGEN 7 45.1 60.4 72.9 35.5 47.4 58.4 26.1 42.0 41.0 40.0 50.7 60.8 70.5 70.5 74.1 74.1
Table 2: Evaluation results on multi-hop question answering, fact verification, and commonsense reasoning datasets.
Acc† is the accuracy of model outputs evaluated with text-davinci-003. For ITER -RETGEN, we evaluated LLM
outputs in different iterations (up to 7 iterations). Underlined metric values are higher than those of Self-Ask.
Method HotPotQA 2WikiMultiHopQA MuSiQue Bamboogle Feverous StrategyQA
# API # Doc # API # Doc # API # Doc # API # Doc # API # Doc # API # Doc
ReAct 2.9 14.3 3.0 15.0 2.9 14.4 2.8 14.1 2.1 10.6 2.8 14.2
Self-Ask 3.2 16.0 3.2 15.9 3.0 14.8 3.0 14.9 2.3 11.3 3.0 15.1
Table 3: Average numbers of API calls to text-davinci-003 and retrieved paragraphs for ReAct and Self-Ask.
Note that ITER -RETGEN (T = 2) achieves significantly higher or competitive Acc† with fewer API calls (i.e., 2)
and fewer retrieved paragraphs (5 per iteration, 10 in total).
It is worth noting that, as shown by Table 3, ITER -
RETGEN (T = 2) is superior to or competitive with
ReAct and Self-Ask using fewer API calls to the
LLM (i.e., 2) and fewer retrieved paragraphs (i.e.,
5 per iteration, 10 in total). ITER -RETGEN is also
conceptually simple, which is to iterate retrieval-
augmented CoT, without complex processing.
We also compared I TER -RETGEN with DSP
which also generates the answer using CoT based
on retrieved knowledge but differs in information
collection and processing. In each iteration, ITER -
RETGEN retrieves knowledge based on (1) the ques-
tion and (2) the previous model output which shows
what may be needed to answer the question. With
the number of iterations increasing, we tend to
obtain a more comprehensive and relevant set of
knowledge. Besides, unlike DSP, we do not summa-
rize the retrieved documents for answer generation,
and thus will not introduce summarization errors.
As shown in Table 2, ITER -RETGEN outperforms
DSP significantly. We manually investigate 10 ran-
dom questions where DSP fails but ITER -RETGEN
provides correct answers. On 40% of them, DSP
fails to retrieve documents that cover the correct
answers, while on 50% of them, the summarized
knowledge is misleading, e.g., for the question
“What occupation do Chris Menges and Aram
Avakian share?”, DSP generates a wrong sum-
mary “Chris Menges and Aram Avakian are
both members of the American and British
Societies of Cinematographers.”, while the
retrieved documents mention that Aram Avakian is
a film editor and director, and only Chris Menges
is with the American and British Societies of Cine-
matographers.
Acc† is a Reliable Metric To investigate how re-
liable Acc† is, we focused on model outputs where
EM and Acc† disagree, and manually checked
which metric gives more correct labels. On each
of the four multi-hop question answering datasets,



Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 6):

distillation of query rewriting is sub-optimal.
The scores on multiple-choice QA are presented
in Table 3. With ChatGPT as a reader, it can be ob-
served that query rewriting improves the scores in
most of the settings, except for the social sciences
category. With Vicuna as a reader, our method
achieves more gains on the four categories com-
pared to ChatGPT. This agrees with the intuition
that a more powerful reader has more parametric
memories, thus more difficult to compensate with
external knowledge.
Model EM F 1
HotpotQA
Direct 32.36 43.05
Retrieve-then-read 30.47 41.34
LLM rewriter 32.80 43.85
Trainable rewriter 34.38 45.97
AmbigNQ
Direct 42.10 53.05
Retrieve-then-read 45.80 58.50
LLM rewriter 46.40 58.74
Trainable rewriter 47.80 60.71
PopQA
Direct 41.94 44.61
Retrieve-then-read 43.20 47.53
LLM rewriter 46.00 49.74
Trainable rewriter 45.72 49.51
Table 2: Metrics of open-domain QA.
MMLU EM
Human. STEM Other Social
ChatGPT
Direct 75.6 58.8 69.0 71.6
Retrieve-then-read 76.7 63.3 70.0 78.2
LLM rewriter 77.0 63.5 72.6 76.4
Vicuna-13B
Direct 39.8 34.9 50.2 46.6
Retrieve-then-read 40.2 39.8 55.2 50.6
LLM rewriter 42.0 41.5 57.1 52.2
Trainable rewriter 43.2 40.9 59.3 51.2
Table 3: Metrics of multiple choice QA.
6 Analysis
6.1 Training Process
The training process includes two stages, warm-up
and reinforcement learning. This section shows
the validation scores of the three open-domain QA
datasets for further analysis. Figure 2 presents
the metric scores through training iterations in the
process of reinforcement learning. As the rewriting
models have been warmed up on the pseudo data
before RL, scores at “0 iteration” denote the ability
acquired from the warm-up training.
0 5 10 15 20 25
Interation
30
31
32
33
34EM
(a)HotpotQA
Retrieve-then-read
LLM rewriter
41
42
43
44
45
F1
0 2 4 6 8 10
Interation
44
45
46
47
48EM
(b)AmbigNQ
Retrieve-then-read
LLM rewriter
57
58
59
60
F1
0 2 4 6 8 10 12
Interation
40
41
42
43
44
45
46EM
(c)PopQA
Retrieve-then-read
LLM rewriter 43
44
45
46
47
48
49
F1
Figure 2: Reinforcement learning validation scores of
(a)HotpotQA, (b)AmbigNQ, and (c)PopQA. The solid
lines show EM (red) and F1 (blue) numbers through
training iterations. The dashed lines are EM scores
of the standard retrieve-then-read method (orange) and
retrieval with an LLM as the rewriter (green).
It can be observed that the curves show upward
trends with some fluctuations on all the datasets. (i)
For multi-hop questions in HotpotQA, the standard
retrieval is relatively weaker. Complex questions
can be not specific search queries and show a larger
gap from rewritten queries, i.e., the green and red
lines. (ii) On AmbigNQ and PopQA, our method
surpasses the baselines after several iterations (3
or 4). This indicates that the RL training stage can
compensate for the insufficiency of the distillation
on the pseudo data during warm-up training. (iii)
In particular, on PopQA, the trainable rewriter re-
mains inferior to the LLM rewriter. This can be
explained as the dataset is constructed for adaptive
retrieval (Mallen et al., 2022), which only uses re-
trieval where it helps to avoid harmful redundant
retrieval. Thus, “None” is a possible query that
means no retrieval. This causes more complex-
ity and uncertainty. LLM rewriter knows better
when the retrieval is needed for itself as a reader,
although the rewriting step is not concatenated as



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[93]_2309.11495.pdf (Page 18):

Figure 8: Examples where questions asking for a fact are answered correctly, but verifying via a
yes/no question is incorrect (the model tends to agree with the way the question is stated, even if it
was stated incorrectly).
19



Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 5):

Method HotPotQA 2WikiMultiHopQA MuSiQue Bamboogle Feverous StrategyQA
EM F1 Acc † EM F1 Acc † EM F1 Acc † EM F1 Acc † Acc Acc† Acc Acc†
Without Retrieval
Direct 21.9 36.8 44.8 21.3 29.2 33.9 7.0 18.7 15.8 11.2 24.4 28.0 60.1 60.1 66.5 66.7
CoT 30.0 44.1 50.0 30.0 39.6 44.0 19.4 30.9 28.6 43.2 51.1 60.0 59.8 59.8 71.0 71.0
With Retrieval
Direct 31.6 44.7 53.3 27.3 35.4 43.6 13.9 28.2 26.5 17.6 31.8 43.2 69.8 69.8 65.6 65.6
ReAct 24.9 44.7 61.1 28.0 38.5 45.9 23.4 37.0 37.9 21.8 31.0 40.3 66.4 66.4 66.9 66.9
Self-Ask 36.8 55.2 64.8 37.3 48.8 55.9 27.6 41.5 42.9 31.5 41.2 54.8 70.7 70.7 70.2 70.2
DSP 43.8 55.0 60.8 - - - - - - - - - - - - -
ITER-RETGEN 1 39.2 53.9 65.5 33.7 45.2 55.4 24.2 38.6 38.1 36.8 47.7 57.6 67.0 67.0 72.0 72.0
ITER-RETGEN 2 44.1 58.6 71.2 34.9 47.0 58.1 26.4 41.1 41.0 38.4 48.7 59.2 68.8 68.8 73.0 73.0
ITER-RETGEN 3 45.2 59.9 71.4 34.8 47.8 58.3 25.7 41.4 40.8 37.6 47.0 59.2 69.0 69.0 72.3 72.3
ITER-RETGEN 4 45.8 61.1 73.4 36.0 47.4 58.5 26.7 41.8 40.8 38.4 49.6 60.0 71.5 71.5 73.8 73.8
ITER-RETGEN 5 45.2 60.3 72.8 35.5 47.5 58.8 25.7 40.7 39.6 39.2 49.7 60.8 70.3 70.3 73.2 73.2
ITER-RETGEN 6 45.9 61.0 73.3 35.5 48.1 59.4 25.9 40.5 39.8 40.0 50.0 59.2 70.9 70.9 72.4 72.4
ITER-RETGEN 7 45.1 60.4 72.9 35.5 47.4 58.4 26.1 42.0 41.0 40.0 50.7 60.8 70.5 70.5 74.1 74.1
Table 2: Evaluation results on multi-hop question answering, fact verification, and commonsense reasoning datasets.
Acc† is the accuracy of model outputs evaluated with text-davinci-003. For ITER -RETGEN, we evaluated LLM
outputs in different iterations (up to 7 iterations). Underlined metric values are higher than those of Self-Ask.
Method HotPotQA 2WikiMultiHopQA MuSiQue Bamboogle Feverous StrategyQA
# API # Doc # API # Doc # API # Doc # API # Doc # API # Doc # API # Doc
ReAct 2.9 14.3 3.0 15.0 2.9 14.4 2.8 14.1 2.1 10.6 2.8 14.2
Self-Ask 3.2 16.0 3.2 15.9 3.0 14.8 3.0 14.9 2.3 11.3 3.0 15.1
Table 3: Average numbers of API calls to text-davinci-003 and retrieved paragraphs for ReAct and Self-Ask.
Note that ITER -RETGEN (T = 2) achieves significantly higher or competitive Acc† with fewer API calls (i.e., 2)
and fewer retrieved paragraphs (5 per iteration, 10 in total).
It is worth noting that, as shown by Table 3, ITER -
RETGEN (T = 2) is superior to or competitive with
ReAct and Self-Ask using fewer API calls to the
LLM (i.e., 2) and fewer retrieved paragraphs (i.e.,
5 per iteration, 10 in total). ITER -RETGEN is also
conceptually simple, which is to iterate retrieval-
augmented CoT, without complex processing.
We also compared I TER -RETGEN with DSP
which also generates the answer using CoT based
on retrieved knowledge but differs in information
collection and processing. In each iteration, ITER -
RETGEN retrieves knowledge based on (1) the ques-
tion and (2) the previous model output which shows
what may be needed to answer the question. With
the number of iterations increasing, we tend to
obtain a more comprehensive and relevant set of
knowledge. Besides, unlike DSP, we do not summa-
rize the retrieved documents for answer generation,
and thus will not introduce summarization errors.
As shown in Table 2, ITER -RETGEN outperforms
DSP significantly. We manually investigate 10 ran-
dom questions where DSP fails but ITER -RETGEN
provides correct answers. On 40% of them, DSP
fails to retrieve documents that cover the correct
answers, while on 50% of them, the summarized
knowledge is misleading, e.g., for the question
“What occupation do Chris Menges and Aram
Avakian share?”, DSP generates a wrong sum-
mary “Chris Menges and Aram Avakian are
both members of the American and British
Societies of Cinematographers.”, while the
retrieved documents mention that Aram Avakian is
a film editor and director, and only Chris Menges
is with the American and British Societies of Cine-
matographers.
Acc† is a Reliable Metric To investigate how re-
liable Acc† is, we focused on model outputs where
EM and Acc† disagree, and manually checked
which metric gives more correct labels. On each
of the four multi-hop question answering datasets,



Source: data\tc16_2312.10997v5\referenced_papers\[106]_2305.13269.pdf (Page 7):

Published as a conference paper at ICLR 2024
Table 5: Parallel vs. dynamic
knowledge adapting.
Method HotpotQA (3-shot)
CoT 29.9%Verify-and-Edit 31.8%CoK (parallel) 31.2%CoK (dynamic) 34.1%
Table 6: Comparison of the
factual accuracy of rationales
on HotpotQA.
Method Rationale 1 Rationale 2
CoT-SC 54.3% 52.1%CoK 66.3% 69.5%
Table 7: Human study results
on the factuality of reasoning
chains.
Predictions CoK CoT-SC Tie
Correct predictions68% 4% 28%Incorrect predictions44% 24% 32%All predictions56% 14% 30%
5 A NALYSIS
5.1 S INGLE VS . M ULTIPLE KNOWLEDGE DOMAINS AND SOURCES
As outlined in Section 2.1, CoK integrates a step to select the appropriate knowledge domains for
each question. This step is crucial to ensure that CoK can retrieve the most pertinent knowledge to
correct the rationales and answer the questions accurately. It is possible that multiple knowledge
domains can be chosen for one question, and within each domain, there are several knowledge
sources. In this subsection, we investigate the necessity of utilizing multiple knowledge domains
and sources. We also include an evaluation of the domain selection performance in Appendix F.1.
Single vs. Multiple Knowledge Domains We show the domain distributions identified for each
dataset in Figure 3. Notably, we find that CoK predominantly selects one knowledge domain for
each dataset, while a small number of cases call for multiple domains. For instance, the primary
knowledge domain for MedMCQA is Medical, and 17.8% of the questions identify Biology as a rel-
evant domain as well. Furthermore, we conduct ablation experiments to demonstrate the necessity
of utilizing multiple domains. As shown in Table 4, compared to only using Medical domain knowl-
edge, CoK using additional knowledge from the Biology domain further improves the performance
by 1.3%. This indicates that knowledge spanning multiple domains is needed for answering some
questions, underscoring the necessity of incorporating various knowledge domains.
Single vs. Multiple Knowledge Sources Within one domain, numerous credible knowledge
sources exist, and it is unfeasible for a single source to encompass all knowledge from the domain.
Therefore, it is important to utilize multiple knowledge sources within one domain. For instance,
as shown in Table 4, the performance of CoK improves by 2.1% when utilizing both Flashcard and
UpToDate as medical knowledge sources, compared to using only Flashcard. 7
5.2 P ARALLEL VS . DYNAMIC KNOWLEDGE ADAPTING
As aforementioned, dynamic knowledge adapting helps CoK prevent error propagation, here we take
a closer look at how much improvement it brings in. As shown in Table 5, the performance of CoK
improves by 4.2% compared with CoT when dynamic knowledge adapting is applied. However,
parallel editing leads to poorer performance due to error propagation for rationales.
5.3 E VALUATING FACTUALITY IMPROVEMENT OF THE RATIONALES
While the main results have shown that CoK effectively improves the performance of LLMs in
knowledge-intensive tasks, we are also interested in reducing hallucination for the generated ratio-
nales. Hence, we conduct quantitative and qualitative evaluations to assess the factual accuracy.
Quantitative Evaluation To automatically evaluate how CoK can reduce hallucination in the
model outputs, we employ an existing fact-checking method to compare the original and corrected
rationales. Specifically, we use ProgramFC (Pan et al., 2023) which is a state-of-the-art method for
judging the factuality of claims with respect to Wikipedia. As shown in Table 6, we observe that
CoK has improved factual accuracy compared to the CoT-SC baseline on the HotpotQA dataset. No-
tably, the factual accuracy of CoT-SC decreases for rationale 2 compared to rationale 1, which could
7Note that using external sources may have limitations such as noise or conflicts between different sources.
We mainly address this by selecting authoritative knowledge sources, and discuss this further in Appendix G
8



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 7):

Task Format Sample
Question Answering Question: The DutchBelgian television series that “House of Anubi” was based on
first aired in what year?
Answer: 2006
Task Instruction Instruction: Give me 3 useful websites for C programming.
Response:
1. GeeksforGeeks: This website provides tutorials and practice problems on C pro-
gramming.
2. Programiz: This website offers tutorials, practice problems, and quizzes on C pro-
gramming.
3. Codeacademy: This website provides free interactive tutorials on C programming.
Text Completion Context: “Sorry” is a song by American singer Madonna from her tenth studio album
Confessions on a Dance Floor (2005). It was written and produced by Madonna and
Stuart Price, and released as the second single from the album on February 7, 2006. It
later appeared on Celebration, her 2009 greatest hits album. An uptempo dance song,
“Sorry” was one of the first tracks developed for the album and had numerous remix
treatments before the ultimate version of the track was finalized.
Completion: One of the remixes was done by the known band the Pet Shop Boys,
featuring added lyrics by the band
Table 5: Illustrative examples for the task format where existing benchmarks evaluate hallucinations.
tion platform to facilitate fine-grained event anno-
tations.
3.2 Evaluation Metrics
The free-form and open-ended nature of language
generation makes it difficult to evaluate the hal-
lucinations produced by LLMs. The most com-
monly used and reliable methods for evaluating
hallucinations rely on human experts following
specific principles (Lin et al., 2021; Lee et al.,
2022; Min et al., 2023; Li et al., 2023a). It is worth
noting that although existing benchmarks use hu-
man evaluation to ensure reliability, they also seek
to support automatic methods to facilitate effi-
cient and consistent evaluation.
Human evaluation. To ensure precise and re-
liable evaluation, existing benchmarks focus on
designing dedicated human evaluation principles
that involve manual annotation for evaluating each
model-generated text. TruthfulQA (Lin et al.,
2021) proposes a human-annotation guideline,
which instructs annotators to assign one of thir-
teen qualitative labels to the model output and ver-
ify answers by consulting a reliable source. Lee
et al. (2022) conduct human annotation to ver-
ify the validity of the proposed automatic evalua-
tion metrics. FactScore (Min et al., 2023) requires
annotators to assign three labels to each atomic
fact: "Supported" or "Not-supported" for facts that
are supported or unsupported by the knowledge
source, and "Irrelevant" for statements that are not
related to the prompt. While human evaluation of-
fers reliability and interpretability, it may be in-
consistent due to subjectivity across annotators. It
is also prohibitively expensive due to the labor-
intensive annotation processes required each time
a new model needs to be evaluated.
Model-based automatic evaluation. Several
studies (Lin et al., 2021; Min et al., 2023; Zha
et al., 2023; Mündler et al., 2023) have devised
model-based methods as a proxy for human eval-
uation. Specifically, TruthfulQA (Lin et al., 2021)
trains a GPT-3-6.7B model to classify answers
(as true or false) to questions based on their col-
lected human annotations. They observe that the
fine-tuned GPT-judge model achieves a validation
accuracy of 90-96% and effectively generalizes
to new answer formats. AlignScore (Zha et al.,
2023) establishes a unified function to evaluate
the factual consistency between two texts. This
alignment function is trained on a large dataset
spanning seven tasks, including Natural Language
Inference (NLI), Question Answering (QA), and
paraphrasing. Differently, Min et al. (2023) and
Mündler et al. (2023) harness the capabilities of
off-the-shelf models to serve as automatic evalu-
8



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 6):

Evaluation Sample
Generation
Question: What happens to you if you eat watermelon seeds?
Reference: Nothing happens; You eat watermelon seeds; The watermelon seeds pass
through your digestive system; You will not digest the watermelon seeds; The water-
melon seeds will be excreted; There is an old wives’ tale that watermelons will grow
in your stomach, but this is impossible.
Discrimination Question: What happens to you if you eat watermelon seeds?
Choices:
(A) The watermelon seeds pass through your digestive system
(B) You grow watermelons in your stomach
(C) You get sick
(D) You have bad dreams
Reference: (A) The watermelon seeds pass through your digestive system.
Table 4: Illustrative examples of two methods for evaluating hallucinations (Generation v.s. Discrimination).
instance, TruthfulQA (Lin et al., 2021) evaluates
the truthfulness of LLMs’ responses to questions,
while FActScore (Min et al., 2023) scrutinizes
the factual accuracy of biographies generated
by LLMs for specific individuals. In contrast,
discrimination benchmarks (Li et al., 2023a;
Muhlgay et al., 2023) consider LLMs’ ability to
discriminate truthful statements from hallucinated
ones. Specifically, HaluEval (Li et al., 2023a)
requires the model to determine whether a state-
ment contains hallucinated information, while
FACTOR (Muhlgay et al., 2023) investigates
whether the LLM assigns a higher likelihood to
the factual statement compared to non-factual
ones. Note that TruthfulQA (Lin et al., 2021)
also supports discrimination format by offering
a multiple-choice alternative to test a model’s
ability to identify truthful statements.
Task format. Existing benchmarks evaluate
LLM hallucinations across various application
tasks. Firstly, certain benchmarks (Lin et al.,
2021; Li et al., 2023a) explore the issue of hal-
lucination in the context of question-answering,
evaluating the ability of LLMs to provide truthful
answers to knowledge-intensive questions. Sec-
ondly, FActScore (Min et al., 2023) and HaluE-
val (Li et al., 2023a) employ task instructions,
such as biography introduction instructions and
52K instructions from the Alpaca project (Taori
et al., 2023), to prompt LLMs to generate re-
sponses. The factuality of these responses is then
evaluated. Thirdly, a line of work (Lee et al., 2022;
Muhlgay et al., 2023) directly prompts LLMs to
complete text given a prefix, and diagnoses po-
tential hallucination during the generation of in-
formative and factual statements. For instance,
FACTOR (Muhlgay et al., 2023) considers con-
text prefixes in Wikipedia documents, while Fac-
tualityPrompt (Lee et al., 2022) designs prefixes
specifically for factual or non-factual statements
to elicit hallucinations. Table 5 provides samples
under different task formats.
Construction methods. Most aforementioned
benchmarks involve human annotators for dataset
creation or quality assurance. TruthfulQA (Lin
et al., 2021) carefully designs the questions to
elicit imitative falsehoods, i.e., false statements
with a high likelihood on the training distribu-
tion. They then hire human annotators to fur-
ther validate the agreement of golden answers.
FActScore (Min et al., 2023) conducts a man-
ual annotation pipeline to transform a long-form
model generation into pieces of atomic statements.
HaluEval (Li et al., 2023a) employs two construc-
tion methods. For the automatic generation track,
they design prompts to query ChatGPT to sam-
ple diverse hallucinations and automatically fil-
ter high-quality ones. For the human-annotation
track, they hire human annotators to annotate the
existence of hallucination in the model responses
and list the corresponding spans. FACTOR (Muhl-
gay et al., 2023) first uses external LLMs to gen-
erate non-factual completion. Then, they man-
ually validate whether the automatically created
datasets meet the predefined requirements, i.e.,
they should be non-factual, fluent, and similar to
the factual completion. To construct knowledge
creation task, Yu et al. (2023a) build an annota-
7



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[93]_2309.11495.pdf (Page 18):

Figure 8: Examples where questions asking for a fact are answered correctly, but verifying via a
yes/no question is incorrect (the model tends to agree with the way the question is stated, even if it
was stated incorrectly).
19



Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 5):

Method HotPotQA 2WikiMultiHopQA MuSiQue Bamboogle Feverous StrategyQA
EM F1 Acc † EM F1 Acc † EM F1 Acc † EM F1 Acc † Acc Acc† Acc Acc†
Without Retrieval
Direct 21.9 36.8 44.8 21.3 29.2 33.9 7.0 18.7 15.8 11.2 24.4 28.0 60.1 60.1 66.5 66.7
CoT 30.0 44.1 50.0 30.0 39.6 44.0 19.4 30.9 28.6 43.2 51.1 60.0 59.8 59.8 71.0 71.0
With Retrieval
Direct 31.6 44.7 53.3 27.3 35.4 43.6 13.9 28.2 26.5 17.6 31.8 43.2 69.8 69.8 65.6 65.6
ReAct 24.9 44.7 61.1 28.0 38.5 45.9 23.4 37.0 37.9 21.8 31.0 40.3 66.4 66.4 66.9 66.9
Self-Ask 36.8 55.2 64.8 37.3 48.8 55.9 27.6 41.5 42.9 31.5 41.2 54.8 70.7 70.7 70.2 70.2
DSP 43.8 55.0 60.8 - - - - - - - - - - - - -
ITER-RETGEN 1 39.2 53.9 65.5 33.7 45.2 55.4 24.2 38.6 38.1 36.8 47.7 57.6 67.0 67.0 72.0 72.0
ITER-RETGEN 2 44.1 58.6 71.2 34.9 47.0 58.1 26.4 41.1 41.0 38.4 48.7 59.2 68.8 68.8 73.0 73.0
ITER-RETGEN 3 45.2 59.9 71.4 34.8 47.8 58.3 25.7 41.4 40.8 37.6 47.0 59.2 69.0 69.0 72.3 72.3
ITER-RETGEN 4 45.8 61.1 73.4 36.0 47.4 58.5 26.7 41.8 40.8 38.4 49.6 60.0 71.5 71.5 73.8 73.8
ITER-RETGEN 5 45.2 60.3 72.8 35.5 47.5 58.8 25.7 40.7 39.6 39.2 49.7 60.8 70.3 70.3 73.2 73.2
ITER-RETGEN 6 45.9 61.0 73.3 35.5 48.1 59.4 25.9 40.5 39.8 40.0 50.0 59.2 70.9 70.9 72.4 72.4
ITER-RETGEN 7 45.1 60.4 72.9 35.5 47.4 58.4 26.1 42.0 41.0 40.0 50.7 60.8 70.5 70.5 74.1 74.1
Table 2: Evaluation results on multi-hop question answering, fact verification, and commonsense reasoning datasets.
Acc† is the accuracy of model outputs evaluated with text-davinci-003. For ITER -RETGEN, we evaluated LLM
outputs in different iterations (up to 7 iterations). Underlined metric values are higher than those of Self-Ask.
Method HotPotQA 2WikiMultiHopQA MuSiQue Bamboogle Feverous StrategyQA
# API # Doc # API # Doc # API # Doc # API # Doc # API # Doc # API # Doc
ReAct 2.9 14.3 3.0 15.0 2.9 14.4 2.8 14.1 2.1 10.6 2.8 14.2
Self-Ask 3.2 16.0 3.2 15.9 3.0 14.8 3.0 14.9 2.3 11.3 3.0 15.1
Table 3: Average numbers of API calls to text-davinci-003 and retrieved paragraphs for ReAct and Self-Ask.
Note that ITER -RETGEN (T = 2) achieves significantly higher or competitive Acc† with fewer API calls (i.e., 2)
and fewer retrieved paragraphs (5 per iteration, 10 in total).
It is worth noting that, as shown by Table 3, ITER -
RETGEN (T = 2) is superior to or competitive with
ReAct and Self-Ask using fewer API calls to the
LLM (i.e., 2) and fewer retrieved paragraphs (i.e.,
5 per iteration, 10 in total). ITER -RETGEN is also
conceptually simple, which is to iterate retrieval-
augmented CoT, without complex processing.
We also compared I TER -RETGEN with DSP
which also generates the answer using CoT based
on retrieved knowledge but differs in information
collection and processing. In each iteration, ITER -
RETGEN retrieves knowledge based on (1) the ques-
tion and (2) the previous model output which shows
what may be needed to answer the question. With
the number of iterations increasing, we tend to
obtain a more comprehensive and relevant set of
knowledge. Besides, unlike DSP, we do not summa-
rize the retrieved documents for answer generation,
and thus will not introduce summarization errors.
As shown in Table 2, ITER -RETGEN outperforms
DSP significantly. We manually investigate 10 ran-
dom questions where DSP fails but ITER -RETGEN
provides correct answers. On 40% of them, DSP
fails to retrieve documents that cover the correct
answers, while on 50% of them, the summarized
knowledge is misleading, e.g., for the question
“What occupation do Chris Menges and Aram
Avakian share?”, DSP generates a wrong sum-
mary “Chris Menges and Aram Avakian are
both members of the American and British
Societies of Cinematographers.”, while the
retrieved documents mention that Aram Avakian is
a film editor and director, and only Chris Menges
is with the American and British Societies of Cine-
matographers.
Acc† is a Reliable Metric To investigate how re-
liable Acc† is, we focused on model outputs where
EM and Acc† disagree, and manually checked
which metric gives more correct labels. On each
of the four multi-hop question answering datasets,



Source: data\tc16_2312.10997v5\referenced_papers\[106]_2305.13269.pdf (Page 7):

Published as a conference paper at ICLR 2024
Table 5: Parallel vs. dynamic
knowledge adapting.
Method HotpotQA (3-shot)
CoT 29.9%Verify-and-Edit 31.8%CoK (parallel) 31.2%CoK (dynamic) 34.1%
Table 6: Comparison of the
factual accuracy of rationales
on HotpotQA.
Method Rationale 1 Rationale 2
CoT-SC 54.3% 52.1%CoK 66.3% 69.5%
Table 7: Human study results
on the factuality of reasoning
chains.
Predictions CoK CoT-SC Tie
Correct predictions68% 4% 28%Incorrect predictions44% 24% 32%All predictions56% 14% 30%
5 A NALYSIS
5.1 S INGLE VS . M ULTIPLE KNOWLEDGE DOMAINS AND SOURCES
As outlined in Section 2.1, CoK integrates a step to select the appropriate knowledge domains for
each question. This step is crucial to ensure that CoK can retrieve the most pertinent knowledge to
correct the rationales and answer the questions accurately. It is possible that multiple knowledge
domains can be chosen for one question, and within each domain, there are several knowledge
sources. In this subsection, we investigate the necessity of utilizing multiple knowledge domains
and sources. We also include an evaluation of the domain selection performance in Appendix F.1.
Single vs. Multiple Knowledge Domains We show the domain distributions identified for each
dataset in Figure 3. Notably, we find that CoK predominantly selects one knowledge domain for
each dataset, while a small number of cases call for multiple domains. For instance, the primary
knowledge domain for MedMCQA is Medical, and 17.8% of the questions identify Biology as a rel-
evant domain as well. Furthermore, we conduct ablation experiments to demonstrate the necessity
of utilizing multiple domains. As shown in Table 4, compared to only using Medical domain knowl-
edge, CoK using additional knowledge from the Biology domain further improves the performance
by 1.3%. This indicates that knowledge spanning multiple domains is needed for answering some
questions, underscoring the necessity of incorporating various knowledge domains.
Single vs. Multiple Knowledge Sources Within one domain, numerous credible knowledge
sources exist, and it is unfeasible for a single source to encompass all knowledge from the domain.
Therefore, it is important to utilize multiple knowledge sources within one domain. For instance,
as shown in Table 4, the performance of CoK improves by 2.1% when utilizing both Flashcard and
UpToDate as medical knowledge sources, compared to using only Flashcard. 7
5.2 P ARALLEL VS . DYNAMIC KNOWLEDGE ADAPTING
As aforementioned, dynamic knowledge adapting helps CoK prevent error propagation, here we take
a closer look at how much improvement it brings in. As shown in Table 5, the performance of CoK
improves by 4.2% compared with CoT when dynamic knowledge adapting is applied. However,
parallel editing leads to poorer performance due to error propagation for rationales.
5.3 E VALUATING FACTUALITY IMPROVEMENT OF THE RATIONALES
While the main results have shown that CoK effectively improves the performance of LLMs in
knowledge-intensive tasks, we are also interested in reducing hallucination for the generated ratio-
nales. Hence, we conduct quantitative and qualitative evaluations to assess the factual accuracy.
Quantitative Evaluation To automatically evaluate how CoK can reduce hallucination in the
model outputs, we employ an existing fact-checking method to compare the original and corrected
rationales. Specifically, we use ProgramFC (Pan et al., 2023) which is a state-of-the-art method for
judging the factuality of claims with respect to Wikipedia. As shown in Table 6, we observe that
CoK has improved factual accuracy compared to the CoT-SC baseline on the HotpotQA dataset. No-
tably, the factual accuracy of CoT-SC decreases for rationale 2 compared to rationale 1, which could
7Note that using external sources may have limitations such as noise or conflicts between different sources.
We mainly address this by selecting authoritative knowledge sources, and discuss this further in Appendix G
8



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 7):

Task Format Sample
Question Answering Question: The DutchBelgian television series that “House of Anubi” was based on
first aired in what year?
Answer: 2006
Task Instruction Instruction: Give me 3 useful websites for C programming.
Response:
1. GeeksforGeeks: This website provides tutorials and practice problems on C pro-
gramming.
2. Programiz: This website offers tutorials, practice problems, and quizzes on C pro-
gramming.
3. Codeacademy: This website provides free interactive tutorials on C programming.
Text Completion Context: “Sorry” is a song by American singer Madonna from her tenth studio album
Confessions on a Dance Floor (2005). It was written and produced by Madonna and
Stuart Price, and released as the second single from the album on February 7, 2006. It
later appeared on Celebration, her 2009 greatest hits album. An uptempo dance song,
“Sorry” was one of the first tracks developed for the album and had numerous remix
treatments before the ultimate version of the track was finalized.
Completion: One of the remixes was done by the known band the Pet Shop Boys,
featuring added lyrics by the band
Table 5: Illustrative examples for the task format where existing benchmarks evaluate hallucinations.
tion platform to facilitate fine-grained event anno-
tations.
3.2 Evaluation Metrics
The free-form and open-ended nature of language
generation makes it difficult to evaluate the hal-
lucinations produced by LLMs. The most com-
monly used and reliable methods for evaluating
hallucinations rely on human experts following
specific principles (Lin et al., 2021; Lee et al.,
2022; Min et al., 2023; Li et al., 2023a). It is worth
noting that although existing benchmarks use hu-
man evaluation to ensure reliability, they also seek
to support automatic methods to facilitate effi-
cient and consistent evaluation.
Human evaluation. To ensure precise and re-
liable evaluation, existing benchmarks focus on
designing dedicated human evaluation principles
that involve manual annotation for evaluating each
model-generated text. TruthfulQA (Lin et al.,
2021) proposes a human-annotation guideline,
which instructs annotators to assign one of thir-
teen qualitative labels to the model output and ver-
ify answers by consulting a reliable source. Lee
et al. (2022) conduct human annotation to ver-
ify the validity of the proposed automatic evalua-
tion metrics. FactScore (Min et al., 2023) requires
annotators to assign three labels to each atomic
fact: "Supported" or "Not-supported" for facts that
are supported or unsupported by the knowledge
source, and "Irrelevant" for statements that are not
related to the prompt. While human evaluation of-
fers reliability and interpretability, it may be in-
consistent due to subjectivity across annotators. It
is also prohibitively expensive due to the labor-
intensive annotation processes required each time
a new model needs to be evaluated.
Model-based automatic evaluation. Several
studies (Lin et al., 2021; Min et al., 2023; Zha
et al., 2023; Mündler et al., 2023) have devised
model-based methods as a proxy for human eval-
uation. Specifically, TruthfulQA (Lin et al., 2021)
trains a GPT-3-6.7B model to classify answers
(as true or false) to questions based on their col-
lected human annotations. They observe that the
fine-tuned GPT-judge model achieves a validation
accuracy of 90-96% and effectively generalizes
to new answer formats. AlignScore (Zha et al.,
2023) establishes a unified function to evaluate
the factual consistency between two texts. This
alignment function is trained on a large dataset
spanning seven tasks, including Natural Language
Inference (NLI), Question Answering (QA), and
paraphrasing. Differently, Min et al. (2023) and
Mündler et al. (2023) harness the capabilities of
off-the-shelf models to serve as automatic evalu-
8



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 6):

Evaluation Sample
Generation
Question: What happens to you if you eat watermelon seeds?
Reference: Nothing happens; You eat watermelon seeds; The watermelon seeds pass
through your digestive system; You will not digest the watermelon seeds; The water-
melon seeds will be excreted; There is an old wives’ tale that watermelons will grow
in your stomach, but this is impossible.
Discrimination Question: What happens to you if you eat watermelon seeds?
Choices:
(A) The watermelon seeds pass through your digestive system
(B) You grow watermelons in your stomach
(C) You get sick
(D) You have bad dreams
Reference: (A) The watermelon seeds pass through your digestive system.
Table 4: Illustrative examples of two methods for evaluating hallucinations (Generation v.s. Discrimination).
instance, TruthfulQA (Lin et al., 2021) evaluates
the truthfulness of LLMs’ responses to questions,
while FActScore (Min et al., 2023) scrutinizes
the factual accuracy of biographies generated
by LLMs for specific individuals. In contrast,
discrimination benchmarks (Li et al., 2023a;
Muhlgay et al., 2023) consider LLMs’ ability to
discriminate truthful statements from hallucinated
ones. Specifically, HaluEval (Li et al., 2023a)
requires the model to determine whether a state-
ment contains hallucinated information, while
FACTOR (Muhlgay et al., 2023) investigates
whether the LLM assigns a higher likelihood to
the factual statement compared to non-factual
ones. Note that TruthfulQA (Lin et al., 2021)
also supports discrimination format by offering
a multiple-choice alternative to test a model’s
ability to identify truthful statements.
Task format. Existing benchmarks evaluate
LLM hallucinations across various application
tasks. Firstly, certain benchmarks (Lin et al.,
2021; Li et al., 2023a) explore the issue of hal-
lucination in the context of question-answering,
evaluating the ability of LLMs to provide truthful
answers to knowledge-intensive questions. Sec-
ondly, FActScore (Min et al., 2023) and HaluE-
val (Li et al., 2023a) employ task instructions,
such as biography introduction instructions and
52K instructions from the Alpaca project (Taori
et al., 2023), to prompt LLMs to generate re-
sponses. The factuality of these responses is then
evaluated. Thirdly, a line of work (Lee et al., 2022;
Muhlgay et al., 2023) directly prompts LLMs to
complete text given a prefix, and diagnoses po-
tential hallucination during the generation of in-
formative and factual statements. For instance,
FACTOR (Muhlgay et al., 2023) considers con-
text prefixes in Wikipedia documents, while Fac-
tualityPrompt (Lee et al., 2022) designs prefixes
specifically for factual or non-factual statements
to elicit hallucinations. Table 5 provides samples
under different task formats.
Construction methods. Most aforementioned
benchmarks involve human annotators for dataset
creation or quality assurance. TruthfulQA (Lin
et al., 2021) carefully designs the questions to
elicit imitative falsehoods, i.e., false statements
with a high likelihood on the training distribu-
tion. They then hire human annotators to fur-
ther validate the agreement of golden answers.
FActScore (Min et al., 2023) conducts a man-
ual annotation pipeline to transform a long-form
model generation into pieces of atomic statements.
HaluEval (Li et al., 2023a) employs two construc-
tion methods. For the automatic generation track,
they design prompts to query ChatGPT to sam-
ple diverse hallucinations and automatically fil-
ter high-quality ones. For the human-annotation
track, they hire human annotators to annotate the
existence of hallucination in the model responses
and list the corresponding spans. FACTOR (Muhl-
gay et al., 2023) first uses external LLMs to gen-
erate non-factual completion. Then, they man-
ually validate whether the automatically created
datasets meet the predefined requirements, i.e.,
they should be non-factual, fluent, and similar to
the factual completion. To construct knowledge
creation task, Yu et al. (2023a) build an annota-
7



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 5):

Method HotPotQA 2WikiMultiHopQA MuSiQue Bamboogle Feverous StrategyQA
EM F1 Acc † EM F1 Acc † EM F1 Acc † EM F1 Acc † Acc Acc† Acc Acc†
Without Retrieval
Direct 21.9 36.8 44.8 21.3 29.2 33.9 7.0 18.7 15.8 11.2 24.4 28.0 60.1 60.1 66.5 66.7
CoT 30.0 44.1 50.0 30.0 39.6 44.0 19.4 30.9 28.6 43.2 51.1 60.0 59.8 59.8 71.0 71.0
With Retrieval
Direct 31.6 44.7 53.3 27.3 35.4 43.6 13.9 28.2 26.5 17.6 31.8 43.2 69.8 69.8 65.6 65.6
ReAct 24.9 44.7 61.1 28.0 38.5 45.9 23.4 37.0 37.9 21.8 31.0 40.3 66.4 66.4 66.9 66.9
Self-Ask 36.8 55.2 64.8 37.3 48.8 55.9 27.6 41.5 42.9 31.5 41.2 54.8 70.7 70.7 70.2 70.2
DSP 43.8 55.0 60.8 - - - - - - - - - - - - -
ITER-RETGEN 1 39.2 53.9 65.5 33.7 45.2 55.4 24.2 38.6 38.1 36.8 47.7 57.6 67.0 67.0 72.0 72.0
ITER-RETGEN 2 44.1 58.6 71.2 34.9 47.0 58.1 26.4 41.1 41.0 38.4 48.7 59.2 68.8 68.8 73.0 73.0
ITER-RETGEN 3 45.2 59.9 71.4 34.8 47.8 58.3 25.7 41.4 40.8 37.6 47.0 59.2 69.0 69.0 72.3 72.3
ITER-RETGEN 4 45.8 61.1 73.4 36.0 47.4 58.5 26.7 41.8 40.8 38.4 49.6 60.0 71.5 71.5 73.8 73.8
ITER-RETGEN 5 45.2 60.3 72.8 35.5 47.5 58.8 25.7 40.7 39.6 39.2 49.7 60.8 70.3 70.3 73.2 73.2
ITER-RETGEN 6 45.9 61.0 73.3 35.5 48.1 59.4 25.9 40.5 39.8 40.0 50.0 59.2 70.9 70.9 72.4 72.4
ITER-RETGEN 7 45.1 60.4 72.9 35.5 47.4 58.4 26.1 42.0 41.0 40.0 50.7 60.8 70.5 70.5 74.1 74.1
Table 2: Evaluation results on multi-hop question answering, fact verification, and commonsense reasoning datasets.
Acc† is the accuracy of model outputs evaluated with text-davinci-003. For ITER -RETGEN, we evaluated LLM
outputs in different iterations (up to 7 iterations). Underlined metric values are higher than those of Self-Ask.
Method HotPotQA 2WikiMultiHopQA MuSiQue Bamboogle Feverous StrategyQA
# API # Doc # API # Doc # API # Doc # API # Doc # API # Doc # API # Doc
ReAct 2.9 14.3 3.0 15.0 2.9 14.4 2.8 14.1 2.1 10.6 2.8 14.2
Self-Ask 3.2 16.0 3.2 15.9 3.0 14.8 3.0 14.9 2.3 11.3 3.0 15.1
Table 3: Average numbers of API calls to text-davinci-003 and retrieved paragraphs for ReAct and Self-Ask.
Note that ITER -RETGEN (T = 2) achieves significantly higher or competitive Acc† with fewer API calls (i.e., 2)
and fewer retrieved paragraphs (5 per iteration, 10 in total).
It is worth noting that, as shown by Table 3, ITER -
RETGEN (T = 2) is superior to or competitive with
ReAct and Self-Ask using fewer API calls to the
LLM (i.e., 2) and fewer retrieved paragraphs (i.e.,
5 per iteration, 10 in total). ITER -RETGEN is also
conceptually simple, which is to iterate retrieval-
augmented CoT, without complex processing.
We also compared I TER -RETGEN with DSP
which also generates the answer using CoT based
on retrieved knowledge but differs in information
collection and processing. In each iteration, ITER -
RETGEN retrieves knowledge based on (1) the ques-
tion and (2) the previous model output which shows
what may be needed to answer the question. With
the number of iterations increasing, we tend to
obtain a more comprehensive and relevant set of
knowledge. Besides, unlike DSP, we do not summa-
rize the retrieved documents for answer generation,
and thus will not introduce summarization errors.
As shown in Table 2, ITER -RETGEN outperforms
DSP significantly. We manually investigate 10 ran-
dom questions where DSP fails but ITER -RETGEN
provides correct answers. On 40% of them, DSP
fails to retrieve documents that cover the correct
answers, while on 50% of them, the summarized
knowledge is misleading, e.g., for the question
“What occupation do Chris Menges and Aram
Avakian share?”, DSP generates a wrong sum-
mary “Chris Menges and Aram Avakian are
both members of the American and British
Societies of Cinematographers.”, while the
retrieved documents mention that Aram Avakian is
a film editor and director, and only Chris Menges
is with the American and British Societies of Cine-
matographers.
Acc† is a Reliable Metric To investigate how re-
liable Acc† is, we focused on model outputs where
EM and Acc† disagree, and manually checked
which metric gives more correct labels. On each
of the four multi-hop question answering datasets,



Source: data\tc16_2312.10997v5\referenced_papers\[20]_2303.08518.pdf (Page 5):

Task Metric GPT-Neo-2.7B BLOOM-7.1B OPT-66B Davinci Davinci-001
0-SHOT UPRISE 0-SHOT UPRISE 0-SHOT UPRISE 0-SHOT UPRISE 0-SHOT UPRISE
Reading Comprehension
SQuADv1 F1 4.4 26.4 4.5 5.5 6.1 7.5 6.5 6.0 41.6 57.7
EM 0.4 14.3 0.0 0.0 0.0 0.6 0.0 0.0 16.4 36.8
BoolQ Acc 54.5 59.4 54.0 60.2 60.7 63.5 62.0 65.7 64.2 65.7
MultiRC F1 57.1 58.1 58.8 59.8 59.6 60.4 59.8 60.0 54.3 58.9
OBQA Acc 41.8 42.2 44.0 41.8 46.4 48.8 49.2 52.4 52.8 48.8
Average 31.6 40.1 32.3 33.5 34.6 36.2 35.5 36.8 45.9 53.6
Closed-book QA
ARC-e Acc 45.7 55.6 53.7 60.9 56.2 66.0 64.1 71.8 67.0 74.4
ARC-c Acc 29.3 30.0 33.2 34.2 36.7 40.2 40.8 45.2 46.2 50.4
NQ F1 1.3 5.6 0.9 1.4 2.5 2.1 0.0 2.2 18.3 18.2
EM 0.5 2.2 0.0 0.1 0.3 0.4 0.0 0.0 4.8 8.7
Average 19.2 23.3 22.0 24.2 23.9 27.2 26.2 29.8 34.1 37.9
Paraphrase Detection
MRPC Acc 46.6 67.9 51.0 70.6 51.0 68.9 54.4 62.3 40.0 61.3
F1 46.0 80.4 58.0 82.1 57.8 81.5 68.9 81.4 39.2 72.9
QQP Acc 48.4 54.3 49.5 53.1 50.5 49.7 55.2 52.4 60.9 62.6
F1 42.2 59.8 46.7 59.6 43.7 58.5 33.7 57.9 43.0 45.9
PAWS Acc 51.7 45.7 50.8 45.9 50.5 44.4 52.4 44.5 53.2 52.3
Average 47.0 61.6 51.2 62.3 50.7 60.6 52.9 59.7 47.3 59.0
Natural Language Inference
MNLI-m Acc 35.3 41.3 35.4 36.0 37.0 40.4 34.2 38.2 44.7 41.1
MNLI-mm Acc 36.4 43.1 34.9 35.8 37.1 41.2 34.2 38.6 46.5 42.1
QNLI Acc 50.9 53.8 49.9 51.3 54.2 53.7 51.7 51.1 60.0 58.4
SNLI Acc 35.2 42.3 35.2 34.4 34.5 40.2 33.5 37.9 47.5 42.0
RTE Acc 33.6 34.7 50.5 49.8 52.3 46.9 51.3 45.5 52.3 50.9
Average 38.3 43.0 41.2 41.5 43.0 44.5 41.0 42.3 50.2 46.9
Sentiment Analysis
SST-2 Acc 52.4 56.2 63.2 69.1 57.9 65.3 52.3 64.3 90.5 90.5
Yelp Acc 71.7 67.8 56.1 58.0 67.6 63.5 59.8 65.3 80.3 80.2
Sent140 Acc 64.1 61.3 74.5 72.1 59.1 61.6 64.3 72.1 87.2 89.1
Average 62.7 61.8 64.6 66.4 61.5 63.5 58.8 67.3 86.0 86.6
Commonsense Reasoning
PiQA Acc 70.2 70.4 71.5 72.1 76.5 80.4 79.1 81.3 79.1 79.1
COPA Acc 67.0 64.0 67.0 67.0 74.0 76.0 80.0 83.0 83.0 80.0
HellaSwag Acc 54.4 52.1 59.6 58.8 72.9 71.4 76.9 76.7 77.6 78.2
Average 63.9 62.2 66.0 66.0 74.5 75.9 78.7 80.3 79.9 79.1
Coreference Resolution
WSC273 Acc 73.6 76.6 78.0 81.0 83.9 86.1 60.6 50.0 78.8 75.5
DPR Acc 59.6 51.0 64.4 55.8 66.3 50.0 82.1 83.9 64.4 58.7
Winogrande Acc 58.9 58.6 65.9 64.3 69.2 67.8 68.6 70.2 66.3 64.7
Average 64.0 62.1 69.4 67.0 73.1 68.0 70.4 68.0 69.8 66.3
Table 1: Zero-shot performance across tasks and LLMs. The model Davinci-001 is the fine-tuned version
text-davinci-001 of Davinci. The method 0-SHOT is the vanilla zero-shot method with only the in-
put instruction fed into the LLM.



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 6):

ARES Ranking of Pseudo RAG Systems
NQ HotpotQA WoW FEVER MultiRC ReCoRD
C.R A.R. C.R A.R. C.R A.R. C.R A.R. C.R A.R. C.R A.R.
Kendall’s Tau for
Sampled Annotations 0.83 0.89 0.78 0.78 0.78 0.83 0.89 0.89 0.83 0.83 0.72 0.94
Kendall’s Tau
for RAGAS 0.89 0.89 0.94 0.89 0.94 0.94 0.72 0.61 0.83 0.94 0.89 0.44
Kendall’s Tau
for GPT-3.5 Judge 0.89 0.94 0.67 0.94 0.94 0.89 0.78 0.78 0.83 0.89 0.83 0.94
Kendall’s Tau for
ARES LLM Judge 0.89 1.0 0.89 0.94 0.94 1.0 0.83 0.72 0.94 0.83 0.78 0.83
Kendall’s Tau
for ARES 0.94 1.0 0.94 0.94 1.0 1.0 0.89 0.78 0.94 0.89 0.83 0.89
RAGAS Accuracy 31.4% 71.2% 17.2% 76.0% 36.4% 77.8% 23.7% 69.2% 16.1% 75.0% 15.0% 72.8%
GPT-3.5 Judge Accuracy 73.8% 95.5% 75.3% 71.6% 84.3% 85.2% 60.4% 59.6% 72.4% 60.3% 81.0% 65.8%
ARES Accuracy 79.3% 97.2% 92.3% 81.3% 85.7% 96.1% 88.4% 78.5% 85.8% 82.7% 67.8% 92.3%
Table 1: ARES Ranking with Fine-tuned LLM Judges vs. Sampled Annotations, RAGAS and GPT-3.5 Judge:
For scoring context relevance and answer relevance (C.R. and A.R. in the table, respectively), we compare ARES
with our fine-tuned LLM judges against sampled annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge.
For our sampled annotations, we gather 150 annotated datapoints from each mock RAG system and use those labels
to score the system. RAGAS also uses GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for
each evaluation domain. Overall, we found that ARES ranked RAG systems more accurately than RAGAS and
GPT-3.5 across all the explored datasets. The Kendall’s tau for ARES was 0.065 higher on average for scoring
context relevance and 0.132 higher on average for scoring answer relevance than RAGAS. Additionally, we include
the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted the ranking accuracy of
the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial costs required to run.
For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human preference validation
set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.
used ARES with human annotation sets ranging
in size from 25 to 400 and found that 150 is the
minimum number required (Table 3). Second, we
explored whether GPT-4 generations could replace
human annotations entirely, finding that GPT-4 is
less good than humans in this role, though the idea
arguably has promise (Table 4).
5.2 ARES Performance on AIS
WoW CNN / DM
ARES Split Prediction 0.478 0.835
Correct Positive/Negative Split 0.458 0.859
ARES Judge Accuracy 62.5% 84.0%
Evaluation Set Size 707 510
Human Preference Data Size 200 200
Table 2: ARES Results on the AIS benchmark
To evaluate whether ARES can effectively gauge
answer faithfulness in real RAG systems, we tested
ARES on the AIS attribution benchmark (Rashkin
et al., 2022). In AIS, we selected the Wizards
of Wikipedia (WoW) and CNN/DM datasets; the
other benchmark datasets involve either table rea-
soning (ToTTo) or focus on passage summariza-
tion (QRECC) so we excluded them. In WoW
and CNN/DM, each evaluation example includes
a query, a retrieved passage, and a generated an-
swer (which is either faithful or non-attributed to
the retrieved passage).
Table 2 summarizes our AIS results. We found
that ARES can effectively score the AIS datasets,
getting within 2.5 accuracy points of the correct
scores. Furthermore, for scoring each system,
we only use 200 annotated datapoints for our hu-
man preference validation set. Our results on AIS
demonstrate the ability of ARES to reliably dis-
tinguish faithful and hallucinated answers in real-
world RAG systems.
5.3 ARES Ranking of Existing RAG Systems
We also wanted to evaluate whether ARES can
score and rank existing RAG systems across both
context relevance and answer relevance. For eval-
uation, we selected the NQ, WoW, and FEVER
datasets from KILT. We consider the answer gen-



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 15):

ARES Ranking of Real RAG Systems
NQ WoW FEVER
C.R. A.R. C.R. A.R. C.R. A.R.
Kendall’s Tau for
Sampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82
Kendall’s Tau
for RAGAS 0.82 0.82 0.73 0.82 0.73 0.87
Kendall’s Tau
for GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87
Kendall’s Tau
for ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87
Kendall’s Tau
for ARES 1.0 0.96 0.91 1.0 0.82 1.0
RAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%
GPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%
ARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%
Table 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance
(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled
annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150
annotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses
GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we
found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.
Additionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted
the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial
costs required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human
preference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.
ARES Cross-Domain Ranking of Pseudo RAG Systems
NQ to
FEVER
FEVER to
NQ
NQ to
MultiRC
MultiRC to
NQ
NQ to
ReCoRD
ReCoRD to
NQ
C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.
Kendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94
Kendall’s Tau of
In-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0
Average PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%
Accuracy on
RAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%
Table 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the
fine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance
(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains
when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and
ReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that
additional examples further improved the performance of ARES. Furthermore, we found that even in scenarios
where the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ
to FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of
percentage points from the lower bound to the upper bound of the PPI confidence interval.



Source: data\tc16_2312.10997v5\referenced_papers\[164]_2309.15217.pdf (Page 2):

we usually do not have access to human-annotated
datasets or reference answers. We therefore fo-
cus on metrics that are fully self-contained and
reference-free. We focus in particular three quality
aspects, which we argue are of central importance.
First, Faithfulness refers to the idea that the an-
swer should be grounded in the given context. This
is important to avoid hallucinations, and to ensure
that the retrieved context can act as a justification
for the generated answer. Indeed, RAG systems are
often used in applications where the factual con-
sistency of the generated text w.r.t. the grounded
sources is highly important, e.g. in domains such as
law, where information is constantly evolving. Sec-
ond, Answer Relevancerefers to the idea that the
generated answer should address the actual ques-
tion that was provided. Finally,Context Relevance
refers to the idea that the retrieved context should
be focused, containing as little irrelevant informa-
tion as possible. This is important given the cost
associated with feeding long context passages to
LLMs. Moreover, when context passages are too
long, LLMs are often less effective in exploiting
that context, especially for information that is pro-
vided in the middle of the context passage (Liu
et al., 2023).
We now explain how these three quality aspects
can be measured in a fully automated way, by
prompting an LLM. In our implementation and
experiments, all prompts are evaluated using the
gpt-3.5-turbo-16k model, which is available
through the OpenAI API2.
Faithfulness We say that the answer as(q) is
faithful to the context c(q) if the claims that are
made in the answer can be inferred from the con-
text. To estimate faithfulness, we first use an LLM
to extract a set of statements, S(as(q)). The aim
of this step is to decompose longer sentences into
shorter and more focused assertions. We use the
following prompt for this step3:
Given a question and answer, create one
or more statements from each sentence
in the given answer.
question: [question]
answer: [answer]
where [question] and [answer] refer to the
given question and answer. For each statement si
2https://platform.openai.com
3To help clarify the task, we include a demonstration as
part of the prompt. This demonstration is not explicitly shown
in the listing of the prompts throughout this paper.
in S, the LLM determines ifsi can be inferred from
c(q) using a verification function v(si, c(q)). This
verification step is carried out using the following
prompt:
Consider the given context and following
statements, then determine whether they
are supported by the information present
in the context. Provide a brief explana-
tion for each statement before arriving
at the verdict (Yes/No). Provide a final
verdict for each statement in order at the
end in the given format. Do not deviate
from the specified format.
statement: [statement 1]
...
statement: [statement n]
The final faithfulness score, F, is then computed
as F = |V |
|S| , where |V | is the number of statements
that were supported according to the LLM and |S|
is the total number of statements.
Answer relevance We say that the answer as(q)
is relevant if it directly addresses the question in
an appropriate way. In particular, our assessment
of answer relevance does not take into account fac-
tuality, but penalises cases where the answer is
incomplete or where it contains redundant informa-
tion. To estimate answer relevance, for the given
answer as(q), we prompt the LLM to generate n
potential questions qi based on as(q), as follows:
Generate a question for the given answer.
answer: [answer]
We then obtain embeddings for all questions us-
ing the text-embedding-ada-002 model, avail-
able from the OpenAI API. For each qi, we cal-
culate the similarity sim(q, qi) with the original
question q, as the cosine between the correspond-
ing embeddings. The answer relevance score, AR,
for question q is then computed as:
AR = 1
n
nX
i=1
sim(q, qi) (1)
This metric evaluates how closely the generated
answer aligns with the initial question or instruc-
tion.
Context relevance The context c(q) is consid-
ered relevant to the extent that it exclusively con-
tains information that is needed to answer the ques-
tion. In particular, this metric aims to penalise the



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 13):

0 1000 2000 3000 4000
24.5
25
25.5
26
26.5
27
FLOPS (MACs)
Rouge-L
2%
(a) ELI5
0 1000 2000 3000 4000
20
21
22
23
24
25
FLOPS (MACs)
2% (b) MS MARCO
0 1000 2000 3000 4000
24
25
26
27
28
29
30
31
32
FLOPS (MACs)
2% (c) NQ
0 20 40 60 80 100
24.5
25
25.5
26
26.5
27
Input P sgs
Rouge-L
2%
(d) ELI5
0 20 40 60 80 100
20
21
22
23
24
25
Input P sgs
2% (e) MS MARCO
0 20 40 60 80 100
24
25
26
27
28
29
30
31
32
FiD
CALM
T ok en Filtering
Combined
Input P sgs
2% (f) NQ
Figure 6: The ROUGE-L performance on FiD-Base vs. the FLOPS (MACs) (First row), and vs. the input passage
amount (Input Psg., second row). The results are on the test set for each dataset, for the various methods utilized.
We observe that the trends of the FLOPS (MACs) and the Input Psg. are very similar, since the passages effect the
encoder the most, and the encoder has the most impact on FLOPS (MACs) (as stated by de Jong et al. (2022)).



Source: data\tc16_2312.10997v5\referenced_papers\[42]_2208.03299.pdf (Page 30):

5-shot 5-shot (multi-task) Full / Transfer
770M 3B 11B 770M 3B 11B 770M 3B 11B
All 29.2 35.7 36.1 26.5 40.0 43.5 42.4 50.4 54.0
Humanities 30.5 35.4 35.5 27.3 38.5 41.6 41.0 48.6 51.3
Social Sciences 29.7 38.0 39.4 24.8 43.8 48.9 48.6 57.8 64.7
STEM 29.0 31.4 30.8 26.5 32.8 35.8 33.4 40.6 41.7
Other 26.7 37.7 38.6 27.0 45.0 48.5 46.8 55.2 59.1
abstract algebra 26.0 23.0 21.0 29.0 30.0 26.0 23.0 29.0 26.0
anatomy 21.5 40.0 40.7 27.4 39.3 45.9 35.6 43.7 42.2
astronomy 37.5 38.8 37.5 27.6 39.5 41.4 36.2 50.7 55.3
business ethics 29.0 54.0 42.0 26.0 47.0 55.0 53.0 64.0 60.0
clinical knowledge 32.5 33.6 40.0 28.7 44.2 47.9 45.3 52.8 57.7
college biology 29.9 34.7 34.0 29.9 34.7 40.3 38.2 46.5 52.1
college chemistry 37.0 22.0 32.0 20.0 35.0 33.0 36.0 34.0 36.0
college computer science 28.0 35.0 34.0 28.0 27.0 36.0 31.0 44.0 35.0
college mathematics 31.0 29.0 27.0 22.0 34.0 27.0 30.0 33.0 32.0
college medicine 24.3 34.7 34.1 27.2 40.5 40.5 35.8 41.6 48.6
college physics 33.3 23.5 23.5 22.5 19.6 26.5 22.5 32.4 24.5
computer security 36.0 42.0 46.0 31.0 49.0 52.0 50.0 65.0 61.0
conceptual physics 26.4 35.7 30.2 23.4 30.6 32.8 34.5 37.4 43.8
econometrics 26.3 21.9 28.9 17.5 19.3 24.6 29.8 25.4 29.8
electrical engineering 31.0 33.1 31.7 31.0 31.0 36.6 41.4 47.6 51.7
elementary mathematics 26.2 27.5 28.0 27.0 31.2 33.3 25.9 31.2 35.5
formal logic 34.1 34.1 31.7 15.1 34.9 31.0 31.7 38.1 42.1
global facts 32.0 30.0 25.0 34.0 34.0 27.0 28.0 34.0 30.0
high school biology 22.6 31.9 29.7 27.1 41.6 50.0 43.5 57.7 60.6
high school chemistry 27.1 26.6 27.6 28.6 31.5 29.1 30.5 36.5 38.9
high school computer science 26.0 32.0 25.0 33.0 37.0 45.0 45.0 55.0 48.0
high school european history 34.5 43.0 42.4 24.2 60.0 59.4 58.2 69.1 76.4
high school geography 31.3 40.4 36.9 24.7 45.5 50.5 56.1 66.7 74.2
high school gov. and pol. 28.0 49.2 51.3 19.2 56.0 59.6 55.4 70.5 75.6
high school macroeconomics 25.6 37.7 32.1 26.7 42.3 43.6 41.0 51.5 56.4
high school mathematics 35.9 35.2 35.9 28.1 26.7 31.1 27.8 36.7 31.9
high school microeconomics 27.3 29.8 36.1 20.6 35.7 42.9 42.9 50.8 60.5
high school physics 21.9 25.2 22.5 24.5 28.5 29.1 27.8 31.1 27.8
high school psychology 26.1 46.4 51.0 24.8 54.3 60.2 56.3 67.3 76.1
high school statistics 27.8 33.3 33.3 17.6 30.6 33.8 32.9 33.3 37.0
high school us history 30.4 39.7 45.6 27.5 46.1 58.3 51.0 63.2 72.5
high school world history 42.6 50.6 41.8 29.1 54.0 64.6 66.7 72.2 73.8
human aging 28.3 37.2 29.6 26.0 45.3 46.2 46.6 57.0 62.8
human sexuality 29.8 34.4 41.2 25.2 42.0 44.3 51.1 58.0 59.5
international law 57.9 57.9 41.3 44.6 57.9 58.7 62.8 71.9 71.1
jurisprudence 30.6 33.3 34.3 32.4 49.1 52.8 55.6 67.6 74.1
logical fallacies 40.5 55.8 46.6 25.8 51.5 62.0 43.6 69.3 71.2
machine learning 33.0 34.8 36.6 29.5 35.7 37.5 32.1 37.5 42.9
management 21.4 29.1 40.8 24.3 47.6 50.5 60.2 69.9 70.9
marketing 38.9 58.5 60.7 31.2 67.9 75.6 69.2 79.9 85.9
medical genetics 26.0 36.0 36.0 29.0 43.0 44.0 40.0 54.0 50.0
miscellaneous 24.5 45.2 46.4 27.1 52.2 58.2 51.3 64.6 72.7
moral disputes 32.4 37.3 38.7 28.6 43.4 43.4 49.7 64.7 64.7
moral scenarios 24.7 24.7 24.7 23.0 23.9 24.7 23.8 24.0 23.8
nutrition 30.1 33.0 34.6 25.8 42.5 44.1 50.3 55.6 61.1
philosophy 28.6 32.5 37.3 31.2 38.9 45.0 44.1 56.6 59.2
prehistory 33.6 37.0 41.4 27.5 39.8 50.6 41.0 51.5 57.7
professional accounting 21.3 28.0 30.5 25.9 35.5 34.0 37.2 41.5 42.2
professional law 28.2 33.4 34.0 27.6 35.4 35.5 38.3 43.0 45.6
professional medicine 19.5 26.5 24.3 20.2 32.0 37.9 38.6 40.8 46.0
professional psychology 27.8 32.8 32.8 26.6 39.5 43.6 38.4 48.0 58.3
public relations 22.7 43.6 40.0 21.8 47.3 56.4 50.0 55.5 60.0
security studies 37.6 26.1 31.0 20.4 34.7 44.1 56.3 61.6 66.9
sociology 43.3 41.8 38.8 30.8 45.8 52.7 60.2 66.7 72.1
us foreign policy 49.0 57.0 66.0 38.0 56.0 61.0 59.0 75.0 76.0
virology 29.5 26.5 34.3 30.1 36.1 39.8 44.0 46.4 41.6
world religions 24.0 40.9 47.4 32.7 49.1 57.3 48.0 63.7 70.2
Table 17: MMLU Test scores for the T5 closed book baseline for each model size and each of the 57 domains.
31



Source: data\tc16_2312.10997v5\referenced_papers\[104]_2306.16092.pdf (Page 3):

Understanding
Application
Single Choice
Memorization
Multi Choice
Uncertain Choice
0%
50%
100%
Models
ChatLaw
GPT4
GPT−3.5
Baichuan2
ChatGLM2
InternLM2
Qwen
Fuzi
a c
b d
Legal pr
ovisions
Legal inte
rpretations
RegulationsPrecedentsTypical cases
Cases of pu
blic interest
LAIC
CAIL
Other Competitions
Examination questions
Examination answers
Examination analysis
Lawyer consultationPublic consultationLegal Q&A
Keyword 
extraction
Text classificationText gene
ration
Issue 
Topic Identification
Reading ComprehensionEntity recognition
Relationship extractionIntent recognition
Named Entity RecognitionDispute 
Focus Recognition
Argument Mining
Document proofreading
Document merging
Document splitting
Sub−case segmentation
Clarify detailsLegal document d
rafting
Legal argumentation
Event Detection
Trigger Word Ext
raction
Statute Prediction
Charge Prediction
Sentence PredictionCase Analysis
Statute matching
Sentence calculation
In
ference of charges
Crime Amount Calculation
Public Opinion Summary
25
50
75
100
ChatL
aw
GPT4
GPT−3.5 Baichuan2 ChatGLM2 Inte
rnLM2 Qwen Fuzi
Score
Model
ChatLaw
GPT4
GPT−3.5
Baichuan2
ChatGLM2
InternLM2
Qwen
Fuzi
Model Performance on Lawbench
50
100
ChatL
aw
GPT4
GPT−3.5 Baichuan2 ChatGLM2 Inte
rnLM2 Qwen Fuzi
Score
Model
ChatLaw
GPT4
GPT−3.5
Baichuan2
ChatGLM2
InternLM2
Qwen
Fuzi
Model Performance on Unified Qualification Exam for Legal Professionals
Figure 2.Data Visualization and Performance Comparison of ChatLaw and Other Models.(a) Visualization of the
diverse task range covered by our legal dataset. The dataset encompasses various legal domains, including case classification,
statute prediction, document drafting, and more specialized tasks like public opinion analysis and named entity recognition. (b)
Radar chart illustrating model performance across different categories in the legal dataset. Categories include Understanding,
Memorization and Application are included in Lawbench, while Single Choice, Multi Choice and Uncertain Choice are
different types of Unified Legal Exam. ChatLaw demonstrates superior performance across multiple categories, highlighting its
robustness and versatility. (c) Box plot comparing model performance on Lawbench. The models compared include ChatLaw,
GPT-4, GPT-3.5, Baichuan2, ChatGLM2, InternLM2, Qwen, and Fuzi. ChatLaw consistently outperforms other models,
showcasing its effectiveness in legal cognitive tasks. (d) Box plot comparing model performance on the Unified Qualification
Exam for Legal Professionals. ChatLaw maintains high performance across 5 years from 2018 to 2022, demonstrating its
comprehensive understanding and application of legal knowledge.
1.2.2 Performance on Unified Qualification Exam for Legal Professionals
The other benchmark is the China’s Unified Qualification Exam for Legal Professionals, including single-choice questions,
multiple-choice questions, and uncertain-choice questions. These questions cover various legal fields and can effectively assess
the understanding and application ability of legal concepts, principles, and provisions for LLMs.
As shown in Fig. 2(d), in the unified legal professional exam from 2018 to 2022, our Chatlaw-MoE model consistently
outperformed all other models. With an average score of 115, Chatlaw-MoE significantly surpasses GPT-4’s average score
of 104. Specifically, Chatlaw-MoE achieved scores of 113, 124, 143, 115, and 78 across the five years, consistently demon-
strating superior performance. In comparison, GPT-4’s scores were 102, 108, 82, 82, and 118 respectively. This consistent
outperformance highlights Chatlaw-MoE’s enhanced capability in handling legal examination questions, likely due to the
multi-expert system design which dynamically selects the most suitable experts for processing based on input features. Among
the Legal LLMs, Fuzi-Mingcha had an average score of 34, with its highest performance in 2019 at 40. For General LLMs,
Baichuan2-7B was the strongest, with an average score of 61 and its highest score of 70 in 2019. These results clearly indicate
4/11



Source: data\tc16_2312.10997v5\referenced_papers\[42]_2208.03299.pdf (Page 29):

5-shot 5-shot (multi-task) Full / Transfer
770M 3B 11B 770M 3B 11B 770M 3B 11B
All 38.9 42.3 43.4 42.1 48.7 56.4 56.3 59.9 65.8
Humanities 37.3 40.0 41.9 37.7 46.4 50.0 50.9 53.0 60.3
Social Sciences 41.7 46.8 49.3 47.5 53.7 65.6 66.0 70.8 77.2
STEM 32.3 35.0 33.9 34.4 39.4 46.2 44.8 50.7 53.4
Other 44.9 48.1 48.8 50.4 55.9 66.6 65.5 68.1 74.4
abstract algebra 30.0 27.0 28.0 27.0 31.0 30.0 22.0 27.0 33.0
anatomy 28.9 50.4 45.2 44.4 57.8 64.4 57.8 68.9 69.6
astronomy 55.3 59.9 59.2 52.6 66.4 67.8 69.1 78.3 79.6
business ethics 49.0 51.0 48.0 50.0 62.0 60.0 51.0 70.0 68.0
clinical knowledge 41.9 44.9 40.0 46.8 54.3 64.9 64.2 72.5 74.0
college biology 38.2 45.8 50.0 36.8 52.1 63.2 63.2 72.2 78.5
college chemistry 32.0 29.0 29.0 31.0 33.0 38.0 45.0 39.0 45.0
college computer science 33.0 35.0 30.0 23.0 29.0 30.0 43.0 48.0 47.0
college mathematics 31.0 31.0 28.0 29.0 27.0 34.0 32.0 29.0 36.0
college medicine 31.2 35.8 38.2 50.3 40.5 52.0 60.1 59.5 63.6
college physics 20.6 26.5 31.4 21.6 28.4 39.2 27.5 44.1 42.2
computer security 53.0 50.0 55.0 49.0 61.0 64.0 69.0 71.0 76.0
conceptual physics 34.9 41.7 37.4 40.9 43.4 57.0 53.2 58.3 59.6
econometrics 28.9 21.1 27.2 26.3 25.4 34.2 28.9 37.7 36.8
electrical engineering 26.9 31.7 31.7 38.6 44.1 51.7 61.4 60.7 67.6
elementary mathematics 25.9 28.8 29.4 29.6 30.2 32.8 29.6 35.5 33.9
formal logic 34.9 33.3 33.3 23.0 30.2 29.4 34.1 38.9 34.1
global facts 28.0 34.0 34.0 36.0 40.0 49.0 50.0 49.0 52.0
high school biology 24.8 37.7 27.7 48.7 57.1 66.5 66.5 76.8 81.9
high school chemistry 34.5 31.0 31.0 31.5 36.5 48.3 44.8 52.2 52.2
high school computer science 31.0 39.0 28.0 37.0 42.0 42.0 50.0 59.0 57.0
high school european history 42.4 49.7 53.3 50.9 58.2 69.7 70.9 73.9 80.0
high school geography 38.9 42.4 50.0 46.5 56.6 69.2 74.2 80.8 82.8
high school gov. and pol. 57.5 60.6 60.1 52.9 64.8 76.7 80.8 85.5 91.7
high school macroeconomics 32.8 39.7 44.9 39.0 45.6 57.2 55.1 63.1 66.7
high school mathematics 30.7 33.0 35.6 28.1 27.8 37.0 30.7 34.8 37.0
high school microeconomics 34.5 42.9 45.4 44.1 51.7 68.9 63.4 70.2 81.1
high school physics 18.5 24.5 22.5 25.8 25.8 33.1 27.2 30.5 39.7
high school psychology 52.8 61.1 59.8 56.7 67.2 79.4 76.3 84.0 87.0
high school statistics 39.8 29.6 34.7 27.3 34.7 38.0 37.0 43.1 45.8
high school us history 43.6 49.0 55.9 46.1 57.8 59.8 62.7 72.5 76.5
high school world history 48.1 52.7 59.9 48.1 66.2 65.4 70.0 78.5 79.7
human aging 46.2 44.8 39.5 48.0 55.2 60.1 56.1 68.2 73.1
human sexuality 41.2 43.5 27.5 46.6 51.1 59.5 77.1 72.5 81.7
international law 54.5 57.9 60.3 55.4 72.7 73.6 81.8 82.6 85.1
jurisprudence 38.9 55.6 32.4 53.7 60.2 73.1 76.9 73.1 81.5
logical fallacies 43.6 54.0 57.1 44.2 58.3 70.6 64.4 73.0 76.7
machine learning 36.6 34.8 28.6 31.3 37.5 46.4 36.6 47.3 50.9
management 45.6 51.5 52.4 48.5 52.4 81.6 78.6 75.7 87.4
marketing 59.4 67.1 70.5 66.7 74.4 83.8 83.8 83.3 91.9
medical genetics 50.0 53.0 58.0 56.0 61.0 75.0 68.0 78.0 81.0
miscellaneous 63.0 64.2 68.8 64.0 72.4 84.3 85.4 83.9 90.9
moral disputes 37.0 41.3 41.3 40.8 50.3 60.1 61.9 66.2 73.7
moral scenarios 24.7 24.7 26.5 21.9 26.9 26.6 23.8 23.8 35.8
nutrition 40.9 45.1 45.1 49.0 52.3 67.0 64.7 68.6 76.8
philosophy 48.6 50.5 56.3 49.8 59.2 69.5 70.4 73.0 77.8
prehistory 45.7 50.0 52.8 54.9 64.8 74.4 69.8 75.0 80.6
professional accounting 28.4 33.0 34.0 35.1 34.0 45.7 43.6 46.1 51.8
professional law 32.4 33.5 34.8 30.4 37.6 39.1 41.5 41.5 50.5
professional medicine 29.4 26.1 27.6 34.6 40.8 52.2 47.8 43.4 59.6
professional psychology 37.7 43.0 50.2 45.1 51.0 60.6 59.5 62.4 74.0
public relations 40.0 46.4 44.5 51.8 54.5 66.4 63.6 66.4 68.2
security studies 35.1 33.5 38.8 44.1 39.6 57.6 60.8 61.6 72.2
sociology 45.3 51.2 51.2 52.7 60.2 69.2 74.1 78.6 85.1
us foreign policy 58.0 70.0 73.0 63.0 63.0 74.0 80.0 80.0 83.0
virology 34.3 34.3 32.5 38.0 42.8 45.2 47.6 49.4 53.0
world religions 65.5 69.0 71.9 70.2 82.5 80.1 83.6 83.6 87.1
Table 16: MMLU Test set scores forAtlas for each model size and each of the 57 domains.
30



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 16):

0 50 100 150
1
1.5
2
2.5
3 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens
(a) ELI5 - p = 10%
0 50 100 150
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (b) ELI5 - p = 30%
0 50 100 150
0.85
0.9
0.95
1
1.05
1.1
1.15
1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (c) ELI5 - p = 50%
0 10 20 30 40 50
0.5
1
1.5
2
2.5
3
3.5 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens
(d) MS MARCO - p = 10%
0 10 20 30 40 50
0.6
0.8
1
1.2
1.4
1.6
1.8 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.8
0.9
1
1.1
1.2
1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (f) MS MARCO - p = 50%
0 20 40 60 80 100
0.5
1
1.5
2
2.5
3
3.5 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens
(g) NQ - p = 10%
0 20 40 60 80 100
0.8
1
1.2
1.4
1.6
1.8
2 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens (h) NQ - p = 30%
0 20 40 60 80 100
0.9
1
1.1
1.2
1.3
1.4 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens (i) NQ - p = 50%
Figure 8: The percentage of tokens that were chosen from each passage, for FiD-Base models. The gold passage
(labeled as 1) is colored red. Each row represents a different dataset, and every column represents a different filtering
percentage (10,30,50).



### Claim 70/179

#### Claim Text
BLEU and ROUGE metrics are also commonly used to evaluate answer quality [26], [32], [52], [78].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 23):

111:24 Lyu, et al.
Table 7. The experimental results for evaluating different top-k values in our benchmark.
task name topk bleu rouge-L bertScore RAGQuestEval length
precision recall
text continuation
2 2.89 17 .20 83 .60 25 .35 23 .14 367.0
4 3.34 17 .49 83 .80 26 .66 23 .54 369.3
6 3.53 17 .64 83 .81 27.66 24.32 375.4
8 3.66 17 .78 83 .99 26 .96 24 .68 367.6
10 3.91 17.84 84.01 27.61 25.00 355.7
summarization
2 26.86 33.87 87.34 70.08 42.21 161.0
4 24.78 33 .62 87 .95 68 .91 44 .19 185.6
6 23.71 33 .36 88 .16 68 .28 45 .08 198.6
8 23.69 33 .53 88 .49 68 .06 46 .18 205.9
10 23.62 33 .56 88.51 68.17 46.70 208.3
question answering
2 39.13 56 .26 82 .57 50 .81 65 .80 67.7
1-document
4 39.47 56 .58 83 .39 52 .14 69 .53 70.6
6 39.40 56 .86 83 .81 52 .60 70 .80 72.5
8 39.76 57.24 83.81 52 .67 70.82 73.3
10 38.84 56 .52 83.93 53.67 70.31 74.1
question answering
2 21.65 35 .16 84 .72 36 .91 47 .41 126.5
2-document
4 22.33 36 .68 86 .39 41 .15 52 .78 139.5
6 23.04 37.43 87 .01 43 .29 55 .47 143.7
8 22.75 37 .25 87 .16 42 .93 56 .73 149.8
10 22.90 37.63 87.43 43.88 57.34 153.4
question answering
2 19.27 32 .57 85 .65 33 .70 43 .90 136.3
3-document
4 20.23 34 .21 86 .93 37 .26 48 .35 145.5
6 20.73 34 .95 87 .66 39 .59 51 .03 151.3
8 21.05 35.04 87.81 40 .32 51 .37 156.6
10 20.61 35 .01 88.02 40.90 52.11 162.5
hallucination
2 32.12 53 .00 80.54 64.95 79 .24 59.6
modification
4 32.50 52.94 80 .53 65.18 79.34 60.2
6 32.32 52 .70 80 .36 64 .48 79 .27 61.8
8 32.35 53.04 80.49 65 .07 80 .85 64.8
10 31.30 51 .71 80 .09 64 .84 80.90 68.3
can significantly increase the recall and precision scores. This is because when the retrieved content
is small, it may not be helpful for answering.
For multi-document QA (2-document and 3-document), increasing top-k significantly improves
the recall and precision scores, as there are more chances to retrieve two relevant and complementary
documents. More documents can also provide additional information, which helps to bridge the
knowledge gap between documents and give more comprehensive answers. The results of 2-
document and 3-document question answering are similar.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 5):

111:6 Lyu, et al.
2.2 RAG Benchmarks
When investigating the development and optimization of RAG, the effective evaluation of their
performance becomes a fundamental concern. Table 1 shows some commonly used benchmarks
for evaluating RAG. LangChain provides benchmark tasks, such as LangChain Docs Q&A and
Semi-structured Reports [27], designed to assess various RAG architectures. These datasets are
constructed from snapshots of Python documentation and PDFs containing tables and charts.
They emphasize the model’s capability to handle structured and semi-structured data. Evaluation
standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing
large models for question-answering generation has emerged as a prevalent approach in building
evaluation datasets. For instance, RGB [8] creates its evaluation dataset by gathering recent news
reports and employing LLM to generate relevant events, questions, and answers. Conversely,
ARES [44]. relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
These methods not only showcase the RAG system’s proficiency in handling real-time data but also
illustrate the utility of automation and synthetic data in the evaluation process. For evaluating the
capabilities of models across various professional domains, the Instruct-Benchmark-Tester dataset
encompasses a range of question types, with a particular focus on financial services, legal, and
intricate business scenarios [39].
Depending on whether the evaluation phase incorporates ground truth, metrics of existing
evaluation methods can be categorized into those necessitating reference and those not requiring
it. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by
contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-
Benchmark-Tester [39] employs accuracy score as an evaluation metric, a widely acknowledged
measure of model performance that assesses the extent to which model-generated answers align
with reference answers. The primary objective of RGB [8] is to evaluate whether large models can
effectively utilize external documents to acquire knowledge and generate accurate answers. Its
evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.
Reference-free evaluation methods, including TruLens-Eval [14], RAGAS [13], and ARES [44],
provide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning
context relevance, answer faithfulness, and answer relevance. TruLens-Eval [14] introduces the
RAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,
encompassing context relevance, groundedness, and answer relevance. RAGAS [13], serving as
a reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity
to identify pertinent and concentrated context passages, along with the LLMs’ proficiency in
faithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a
predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each
aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy
when compared to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-
powered inference to offer statistical assurances for its scoring, generating confidence intervals.
ARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer
relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts
and producing both faithful and relevant answers. Regarding evaluation methods, [32] places an
emphasis on assessing the credibility and accuracy of responses generated by generative search
engines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including
high costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,
exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely
approximates manual evaluation outcomes.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 13):

111:14 Lyu, et al.
Fig. 6. Overview of RAGQuestEval. A set of questions is generated based on the ground truth references.
The questions are then answered using both the ground truth and the response. For the recall score of
RAGQuestEval, we calculate the ratio of answerable questions to all questions(in this case, recall = 2/3). For
the precision score of RAGQuestEval, corresponding answers are compared using a similarity function and
averaged across questions(in this case, precision = (0.5 + 1) / 2 = 0.75). The recall metric of RAGQuestEval
indicates how much of the key information in the ground truth reference is included in the generated text,
while the precision metric of RAGQuestEval indicates how correct the recalled key information is.
The RAG system’s experimental results on this dataset can confirm if the system can retrieve the
real news information from the document database based on the input text, which consists of the
beginning text and the hallucination continuation text, and then correct the hallucination text to
generate the text without hallucination.
3.6 Evaluation Method
The aim of this benchmark is to evaluate how well RAG systems can retrieve relevant documents,
and use them to generate sensible responses. Therefore, we adopt an end-to-end evaluation method,
which directly compares the similarity between the model output and the reference answers.
Evaluating the performance of RAG systems requires choosing appropriate evaluation metrics.
We considered the previous evaluation metrics for text generation, ROUGE and BLEU, which are
both based on word overlap. ROUGE mainly counts the recall rate on the n-gram, while BLEU
mainly counts the precision rate on the n-gram. However, BLEU and ROUGE are word overlap-
based accuracy metrics that depend on the overall expression of the text, and do not capture the
accuracy of the particular key information in the text. Therefore, they may not reflect the factual
consistency of a text well, especially for long texts. To alleviate this issue, recent work [12, 45, 49]
has proposed new evaluation metrics for abstractive summarization evaluation. These metrics are
based on the intuition that if you ask questions about the summary and the original document,
you will get a similar answer if the summary realistically matches the original document. They
evaluate the accuracy of each local piece of key information in the summary.
We also consider question-answering-based metrics to evaluate the factual accuracy of generation.
In this paper, we examine QuestEval [45], a metric that improves the correlation with human
judgments over previous metrics in their extensive experiments. QuestEval evaluates the factual
consistency between the generated text and the source document, which is mainly used for text
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 13):

Figure 2: RAG Systems Evaluation on NQ - Context Relevance
Figure 3: RAG Systems Evaluation on NQ - Answer Relevance



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 25):

111:26 Lyu, et al.
Table 8. The experimental results for evaluating different large language models in our benchmark.
task name model name bleu rouge-L bertScore RAGQuestEvallength
precision recall
text continuation
ChatGLM2-6B2.06 13 .35 68 .51 20 .68 15 .44 363.3
Qwen-7B 7.10 15.31 77 .94 28 .06 18 .44 159.6
Baichuan2-13B3.97 14 .21 71 .75 28 .62 22 .95 358.4
Qwen-14B 5.70 18 .48 82 .97 27 .89 21 .68 240.1
GPT-3.5-turbo3.66 17 .78 83 .99 26 .96 24 .68 367.6
GPT-4-0613 5.58 19.47 84.91 30.34 28.02 369.8
Qwen2-7B 2.94 16.76 83.82 26.90 23.68 350.0
GPT-4o 4.48 18.85 84.45 30.89 26.11 356.7
summarization
ChatGLM2-6B17.09 28 .16 83 .00 58 .94 40 .35 228.1
Qwen-7B 28.30 30 .21 84 .26 67 .62 40 .03 240.5
Baichuan2-13B24.49 32 .49 85 .64 65 .96 42 .53 179.5
Qwen-14B 32.51 33.33 85 .62 68 .94 40 .57 139.1
GPT-3.5-turbo23.69 33 .53 88 .49 68 .06 46 .18 205.9
GPT-4-0613 24.54 35.91 89.39 71.24 50.53 194.6
Qwen2-7B 14.82 30.00 88.60 62.04 45.93 283.2
GPT-4o 23.24 35.40 89.65 68.28 50.93 217.7
question answering
ChatGLM2-6B29.11 47 .57 79 .59 50 .06 69 .35 90.8
1-document
Qwen-7B 39.63 56 .71 82 .64 51 .77 72 .02 68.8
Baichuan2-13B35.40 53 .85 83 .59 54 .35 76.92 91.3
Qwen-14B 37.95 55 .13 83 .25 53 .03 73 .92 73.8
GPT-3.5-turbo39.76 57.24 83.81 52.67 70 .82 73.3
GPT-4-0613 33.87 51 .42 80 .92 53 .14 62 .39 95.9
Qwen2-7B 23.06 41.25 82.10 60.07 72.17 123.3
GPT-4o 33.32 51.78 83.35 65.33 66.59 74.7
question answering
ChatGLM2-6B15.15 29 .12 82 .30 37 .61 51 .51 193.4
2-document
Qwen-7B 22.61 36 .07 85 .84 42 .32 56 .26 157.6
Baichuan2-13B20.32 35 .56 87 .49 45 .01 61 .47 208.8
Qwen-14B 21.11 34 .97 85 .87 42 .23 56 .59 151.1
GPT-3.5-turbo22.75 37.25 87.16 42 .93 56 .73 149.8
GPT-4-0613 20.38 36 .08 88 .10 49.56 62.56 223.0
Qwen2-7B 15.26 41.25 82.10 48.89 61.41 209.1
GPT-4o 22.84 36.61 88.38 44.04 67.44 124.3
question answering
ChatGLM2-6B14.01 27 .71 83 .42 35 .60 45 .28 204.1
3-document
Qwen-7B 21.63 33 .42 86 .31 39 .14 50 .55 160.6
Baichuan2-13B18.30 33 .34 88 .08 41 .35 55 .75 227.5
Qwen-14B 19.83 33 .33 86 .93 42 .01 51 .70 161.2
GPT-3.5-turbo21.05 35 .04 87 .81 40 .32 51 .37 156.6
GPT-4-0613 19.11 34 .58 88 .88 48.24 56.48 235.1
Qwen2-7B 16.23 32.18 87.69 45.72 55.29 207.2
GPT-4o 22.84 35.98 89.21 43.56 63.90 139.9
hallucination
ChatGLM2-6B13.51 28 .70 71 .26 59 .63 73 .02 176.0
modification
Qwen-7B 22.87 38 .10 73 .52 60 .00 73 .72 172.5
Baichuan2-13B10.56 27 .28 68 .90 54 .42 67 .47 124.8
Qwen-14B 33.78 51 .90 79 .49 67 .05 84.08 89.7
GPT-3.5-turbo32.35 53 .04 80 .49 65 .07 80 .85 64.8
GPT-4-0613 36.69 55.70 81.27 69.18 82.06 63.5
Qwen2-7B 31.07 52.91 80.25 65.48 79.16 49.3
GPT-4o 36.73 54.79 80.90 63.61 73.75 51.9
The top-k value is a crucial parameter for the RAG system, as it determines how many documents
are retrieved for each query. Depending on the scenario, the optimal top-k value may vary. For
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 23):

111:24 Lyu, et al.
Table 7. The experimental results for evaluating different top-k values in our benchmark.
task name topk bleu rouge-L bertScore RAGQuestEval length
precision recall
text continuation
2 2.89 17 .20 83 .60 25 .35 23 .14 367.0
4 3.34 17 .49 83 .80 26 .66 23 .54 369.3
6 3.53 17 .64 83 .81 27.66 24.32 375.4
8 3.66 17 .78 83 .99 26 .96 24 .68 367.6
10 3.91 17.84 84.01 27.61 25.00 355.7
summarization
2 26.86 33.87 87.34 70.08 42.21 161.0
4 24.78 33 .62 87 .95 68 .91 44 .19 185.6
6 23.71 33 .36 88 .16 68 .28 45 .08 198.6
8 23.69 33 .53 88 .49 68 .06 46 .18 205.9
10 23.62 33 .56 88.51 68.17 46.70 208.3
question answering
2 39.13 56 .26 82 .57 50 .81 65 .80 67.7
1-document
4 39.47 56 .58 83 .39 52 .14 69 .53 70.6
6 39.40 56 .86 83 .81 52 .60 70 .80 72.5
8 39.76 57.24 83.81 52 .67 70.82 73.3
10 38.84 56 .52 83.93 53.67 70.31 74.1
question answering
2 21.65 35 .16 84 .72 36 .91 47 .41 126.5
2-document
4 22.33 36 .68 86 .39 41 .15 52 .78 139.5
6 23.04 37.43 87 .01 43 .29 55 .47 143.7
8 22.75 37 .25 87 .16 42 .93 56 .73 149.8
10 22.90 37.63 87.43 43.88 57.34 153.4
question answering
2 19.27 32 .57 85 .65 33 .70 43 .90 136.3
3-document
4 20.23 34 .21 86 .93 37 .26 48 .35 145.5
6 20.73 34 .95 87 .66 39 .59 51 .03 151.3
8 21.05 35.04 87.81 40 .32 51 .37 156.6
10 20.61 35 .01 88.02 40.90 52.11 162.5
hallucination
2 32.12 53 .00 80.54 64.95 79 .24 59.6
modification
4 32.50 52.94 80 .53 65.18 79.34 60.2
6 32.32 52 .70 80 .36 64 .48 79 .27 61.8
8 32.35 53.04 80.49 65 .07 80 .85 64.8
10 31.30 51 .71 80 .09 64 .84 80.90 68.3
can significantly increase the recall and precision scores. This is because when the retrieved content
is small, it may not be helpful for answering.
For multi-document QA (2-document and 3-document), increasing top-k significantly improves
the recall and precision scores, as there are more chances to retrieve two relevant and complementary
documents. More documents can also provide additional information, which helps to bridge the
knowledge gap between documents and give more comprehensive answers. The results of 2-
document and 3-document question answering are similar.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 5):

111:6 Lyu, et al.
2.2 RAG Benchmarks
When investigating the development and optimization of RAG, the effective evaluation of their
performance becomes a fundamental concern. Table 1 shows some commonly used benchmarks
for evaluating RAG. LangChain provides benchmark tasks, such as LangChain Docs Q&A and
Semi-structured Reports [27], designed to assess various RAG architectures. These datasets are
constructed from snapshots of Python documentation and PDFs containing tables and charts.
They emphasize the model’s capability to handle structured and semi-structured data. Evaluation
standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing
large models for question-answering generation has emerged as a prevalent approach in building
evaluation datasets. For instance, RGB [8] creates its evaluation dataset by gathering recent news
reports and employing LLM to generate relevant events, questions, and answers. Conversely,
ARES [44]. relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
These methods not only showcase the RAG system’s proficiency in handling real-time data but also
illustrate the utility of automation and synthetic data in the evaluation process. For evaluating the
capabilities of models across various professional domains, the Instruct-Benchmark-Tester dataset
encompasses a range of question types, with a particular focus on financial services, legal, and
intricate business scenarios [39].
Depending on whether the evaluation phase incorporates ground truth, metrics of existing
evaluation methods can be categorized into those necessitating reference and those not requiring
it. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by
contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-
Benchmark-Tester [39] employs accuracy score as an evaluation metric, a widely acknowledged
measure of model performance that assesses the extent to which model-generated answers align
with reference answers. The primary objective of RGB [8] is to evaluate whether large models can
effectively utilize external documents to acquire knowledge and generate accurate answers. Its
evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.
Reference-free evaluation methods, including TruLens-Eval [14], RAGAS [13], and ARES [44],
provide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning
context relevance, answer faithfulness, and answer relevance. TruLens-Eval [14] introduces the
RAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,
encompassing context relevance, groundedness, and answer relevance. RAGAS [13], serving as
a reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity
to identify pertinent and concentrated context passages, along with the LLMs’ proficiency in
faithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a
predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each
aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy
when compared to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-
powered inference to offer statistical assurances for its scoring, generating confidence intervals.
ARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer
relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts
and producing both faithful and relevant answers. Regarding evaluation methods, [32] places an
emphasis on assessing the credibility and accuracy of responses generated by generative search
engines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including
high costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,
exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely
approximates manual evaluation outcomes.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 13):

111:14 Lyu, et al.
Fig. 6. Overview of RAGQuestEval. A set of questions is generated based on the ground truth references.
The questions are then answered using both the ground truth and the response. For the recall score of
RAGQuestEval, we calculate the ratio of answerable questions to all questions(in this case, recall = 2/3). For
the precision score of RAGQuestEval, corresponding answers are compared using a similarity function and
averaged across questions(in this case, precision = (0.5 + 1) / 2 = 0.75). The recall metric of RAGQuestEval
indicates how much of the key information in the ground truth reference is included in the generated text,
while the precision metric of RAGQuestEval indicates how correct the recalled key information is.
The RAG system’s experimental results on this dataset can confirm if the system can retrieve the
real news information from the document database based on the input text, which consists of the
beginning text and the hallucination continuation text, and then correct the hallucination text to
generate the text without hallucination.
3.6 Evaluation Method
The aim of this benchmark is to evaluate how well RAG systems can retrieve relevant documents,
and use them to generate sensible responses. Therefore, we adopt an end-to-end evaluation method,
which directly compares the similarity between the model output and the reference answers.
Evaluating the performance of RAG systems requires choosing appropriate evaluation metrics.
We considered the previous evaluation metrics for text generation, ROUGE and BLEU, which are
both based on word overlap. ROUGE mainly counts the recall rate on the n-gram, while BLEU
mainly counts the precision rate on the n-gram. However, BLEU and ROUGE are word overlap-
based accuracy metrics that depend on the overall expression of the text, and do not capture the
accuracy of the particular key information in the text. Therefore, they may not reflect the factual
consistency of a text well, especially for long texts. To alleviate this issue, recent work [12, 45, 49]
has proposed new evaluation metrics for abstractive summarization evaluation. These metrics are
based on the intuition that if you ask questions about the summary and the original document,
you will get a similar answer if the summary realistically matches the original document. They
evaluate the accuracy of each local piece of key information in the summary.
We also consider question-answering-based metrics to evaluate the factual accuracy of generation.
In this paper, we examine QuestEval [45], a metric that improves the correlation with human
judgments over previous metrics in their extensive experiments. QuestEval evaluates the factual
consistency between the generated text and the source document, which is mainly used for text
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 13):

Figure 2: RAG Systems Evaluation on NQ - Context Relevance
Figure 3: RAG Systems Evaluation on NQ - Answer Relevance



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 25):

111:26 Lyu, et al.
Table 8. The experimental results for evaluating different large language models in our benchmark.
task name model name bleu rouge-L bertScore RAGQuestEvallength
precision recall
text continuation
ChatGLM2-6B2.06 13 .35 68 .51 20 .68 15 .44 363.3
Qwen-7B 7.10 15.31 77 .94 28 .06 18 .44 159.6
Baichuan2-13B3.97 14 .21 71 .75 28 .62 22 .95 358.4
Qwen-14B 5.70 18 .48 82 .97 27 .89 21 .68 240.1
GPT-3.5-turbo3.66 17 .78 83 .99 26 .96 24 .68 367.6
GPT-4-0613 5.58 19.47 84.91 30.34 28.02 369.8
Qwen2-7B 2.94 16.76 83.82 26.90 23.68 350.0
GPT-4o 4.48 18.85 84.45 30.89 26.11 356.7
summarization
ChatGLM2-6B17.09 28 .16 83 .00 58 .94 40 .35 228.1
Qwen-7B 28.30 30 .21 84 .26 67 .62 40 .03 240.5
Baichuan2-13B24.49 32 .49 85 .64 65 .96 42 .53 179.5
Qwen-14B 32.51 33.33 85 .62 68 .94 40 .57 139.1
GPT-3.5-turbo23.69 33 .53 88 .49 68 .06 46 .18 205.9
GPT-4-0613 24.54 35.91 89.39 71.24 50.53 194.6
Qwen2-7B 14.82 30.00 88.60 62.04 45.93 283.2
GPT-4o 23.24 35.40 89.65 68.28 50.93 217.7
question answering
ChatGLM2-6B29.11 47 .57 79 .59 50 .06 69 .35 90.8
1-document
Qwen-7B 39.63 56 .71 82 .64 51 .77 72 .02 68.8
Baichuan2-13B35.40 53 .85 83 .59 54 .35 76.92 91.3
Qwen-14B 37.95 55 .13 83 .25 53 .03 73 .92 73.8
GPT-3.5-turbo39.76 57.24 83.81 52.67 70 .82 73.3
GPT-4-0613 33.87 51 .42 80 .92 53 .14 62 .39 95.9
Qwen2-7B 23.06 41.25 82.10 60.07 72.17 123.3
GPT-4o 33.32 51.78 83.35 65.33 66.59 74.7
question answering
ChatGLM2-6B15.15 29 .12 82 .30 37 .61 51 .51 193.4
2-document
Qwen-7B 22.61 36 .07 85 .84 42 .32 56 .26 157.6
Baichuan2-13B20.32 35 .56 87 .49 45 .01 61 .47 208.8
Qwen-14B 21.11 34 .97 85 .87 42 .23 56 .59 151.1
GPT-3.5-turbo22.75 37.25 87.16 42 .93 56 .73 149.8
GPT-4-0613 20.38 36 .08 88 .10 49.56 62.56 223.0
Qwen2-7B 15.26 41.25 82.10 48.89 61.41 209.1
GPT-4o 22.84 36.61 88.38 44.04 67.44 124.3
question answering
ChatGLM2-6B14.01 27 .71 83 .42 35 .60 45 .28 204.1
3-document
Qwen-7B 21.63 33 .42 86 .31 39 .14 50 .55 160.6
Baichuan2-13B18.30 33 .34 88 .08 41 .35 55 .75 227.5
Qwen-14B 19.83 33 .33 86 .93 42 .01 51 .70 161.2
GPT-3.5-turbo21.05 35 .04 87 .81 40 .32 51 .37 156.6
GPT-4-0613 19.11 34 .58 88 .88 48.24 56.48 235.1
Qwen2-7B 16.23 32.18 87.69 45.72 55.29 207.2
GPT-4o 22.84 35.98 89.21 43.56 63.90 139.9
hallucination
ChatGLM2-6B13.51 28 .70 71 .26 59 .63 73 .02 176.0
modification
Qwen-7B 22.87 38 .10 73 .52 60 .00 73 .72 172.5
Baichuan2-13B10.56 27 .28 68 .90 54 .42 67 .47 124.8
Qwen-14B 33.78 51 .90 79 .49 67 .05 84.08 89.7
GPT-3.5-turbo32.35 53 .04 80 .49 65 .07 80 .85 64.8
GPT-4-0613 36.69 55.70 81.27 69.18 82.06 63.5
Qwen2-7B 31.07 52.91 80.25 65.48 79.16 49.3
GPT-4o 36.73 54.79 80.90 63.61 73.75 51.9
The top-k value is a crucial parameter for the RAG system, as it determines how many documents
are retrieved for each query. Depending on the scenario, the optimal top-k value may vary. For
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 23):

111:24 Lyu, et al.
Table 7. The experimental results for evaluating different top-k values in our benchmark.
task name topk bleu rouge-L bertScore RAGQuestEval length
precision recall
text continuation
2 2.89 17 .20 83 .60 25 .35 23 .14 367.0
4 3.34 17 .49 83 .80 26 .66 23 .54 369.3
6 3.53 17 .64 83 .81 27.66 24.32 375.4
8 3.66 17 .78 83 .99 26 .96 24 .68 367.6
10 3.91 17.84 84.01 27.61 25.00 355.7
summarization
2 26.86 33.87 87.34 70.08 42.21 161.0
4 24.78 33 .62 87 .95 68 .91 44 .19 185.6
6 23.71 33 .36 88 .16 68 .28 45 .08 198.6
8 23.69 33 .53 88 .49 68 .06 46 .18 205.9
10 23.62 33 .56 88.51 68.17 46.70 208.3
question answering
2 39.13 56 .26 82 .57 50 .81 65 .80 67.7
1-document
4 39.47 56 .58 83 .39 52 .14 69 .53 70.6
6 39.40 56 .86 83 .81 52 .60 70 .80 72.5
8 39.76 57.24 83.81 52 .67 70.82 73.3
10 38.84 56 .52 83.93 53.67 70.31 74.1
question answering
2 21.65 35 .16 84 .72 36 .91 47 .41 126.5
2-document
4 22.33 36 .68 86 .39 41 .15 52 .78 139.5
6 23.04 37.43 87 .01 43 .29 55 .47 143.7
8 22.75 37 .25 87 .16 42 .93 56 .73 149.8
10 22.90 37.63 87.43 43.88 57.34 153.4
question answering
2 19.27 32 .57 85 .65 33 .70 43 .90 136.3
3-document
4 20.23 34 .21 86 .93 37 .26 48 .35 145.5
6 20.73 34 .95 87 .66 39 .59 51 .03 151.3
8 21.05 35.04 87.81 40 .32 51 .37 156.6
10 20.61 35 .01 88.02 40.90 52.11 162.5
hallucination
2 32.12 53 .00 80.54 64.95 79 .24 59.6
modification
4 32.50 52.94 80 .53 65.18 79.34 60.2
6 32.32 52 .70 80 .36 64 .48 79 .27 61.8
8 32.35 53.04 80.49 65 .07 80 .85 64.8
10 31.30 51 .71 80 .09 64 .84 80.90 68.3
can significantly increase the recall and precision scores. This is because when the retrieved content
is small, it may not be helpful for answering.
For multi-document QA (2-document and 3-document), increasing top-k significantly improves
the recall and precision scores, as there are more chances to retrieve two relevant and complementary
documents. More documents can also provide additional information, which helps to bridge the
knowledge gap between documents and give more comprehensive answers. The results of 2-
document and 3-document question answering are similar.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 5):

111:6 Lyu, et al.
2.2 RAG Benchmarks
When investigating the development and optimization of RAG, the effective evaluation of their
performance becomes a fundamental concern. Table 1 shows some commonly used benchmarks
for evaluating RAG. LangChain provides benchmark tasks, such as LangChain Docs Q&A and
Semi-structured Reports [27], designed to assess various RAG architectures. These datasets are
constructed from snapshots of Python documentation and PDFs containing tables and charts.
They emphasize the model’s capability to handle structured and semi-structured data. Evaluation
standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing
large models for question-answering generation has emerged as a prevalent approach in building
evaluation datasets. For instance, RGB [8] creates its evaluation dataset by gathering recent news
reports and employing LLM to generate relevant events, questions, and answers. Conversely,
ARES [44]. relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
These methods not only showcase the RAG system’s proficiency in handling real-time data but also
illustrate the utility of automation and synthetic data in the evaluation process. For evaluating the
capabilities of models across various professional domains, the Instruct-Benchmark-Tester dataset
encompasses a range of question types, with a particular focus on financial services, legal, and
intricate business scenarios [39].
Depending on whether the evaluation phase incorporates ground truth, metrics of existing
evaluation methods can be categorized into those necessitating reference and those not requiring
it. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by
contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-
Benchmark-Tester [39] employs accuracy score as an evaluation metric, a widely acknowledged
measure of model performance that assesses the extent to which model-generated answers align
with reference answers. The primary objective of RGB [8] is to evaluate whether large models can
effectively utilize external documents to acquire knowledge and generate accurate answers. Its
evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.
Reference-free evaluation methods, including TruLens-Eval [14], RAGAS [13], and ARES [44],
provide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning
context relevance, answer faithfulness, and answer relevance. TruLens-Eval [14] introduces the
RAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,
encompassing context relevance, groundedness, and answer relevance. RAGAS [13], serving as
a reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity
to identify pertinent and concentrated context passages, along with the LLMs’ proficiency in
faithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a
predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each
aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy
when compared to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-
powered inference to offer statistical assurances for its scoring, generating confidence intervals.
ARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer
relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts
and producing both faithful and relevant answers. Regarding evaluation methods, [32] places an
emphasis on assessing the credibility and accuracy of responses generated by generative search
engines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including
high costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,
exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely
approximates manual evaluation outcomes.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 13):

111:14 Lyu, et al.
Fig. 6. Overview of RAGQuestEval. A set of questions is generated based on the ground truth references.
The questions are then answered using both the ground truth and the response. For the recall score of
RAGQuestEval, we calculate the ratio of answerable questions to all questions(in this case, recall = 2/3). For
the precision score of RAGQuestEval, corresponding answers are compared using a similarity function and
averaged across questions(in this case, precision = (0.5 + 1) / 2 = 0.75). The recall metric of RAGQuestEval
indicates how much of the key information in the ground truth reference is included in the generated text,
while the precision metric of RAGQuestEval indicates how correct the recalled key information is.
The RAG system’s experimental results on this dataset can confirm if the system can retrieve the
real news information from the document database based on the input text, which consists of the
beginning text and the hallucination continuation text, and then correct the hallucination text to
generate the text without hallucination.
3.6 Evaluation Method
The aim of this benchmark is to evaluate how well RAG systems can retrieve relevant documents,
and use them to generate sensible responses. Therefore, we adopt an end-to-end evaluation method,
which directly compares the similarity between the model output and the reference answers.
Evaluating the performance of RAG systems requires choosing appropriate evaluation metrics.
We considered the previous evaluation metrics for text generation, ROUGE and BLEU, which are
both based on word overlap. ROUGE mainly counts the recall rate on the n-gram, while BLEU
mainly counts the precision rate on the n-gram. However, BLEU and ROUGE are word overlap-
based accuracy metrics that depend on the overall expression of the text, and do not capture the
accuracy of the particular key information in the text. Therefore, they may not reflect the factual
consistency of a text well, especially for long texts. To alleviate this issue, recent work [12, 45, 49]
has proposed new evaluation metrics for abstractive summarization evaluation. These metrics are
based on the intuition that if you ask questions about the summary and the original document,
you will get a similar answer if the summary realistically matches the original document. They
evaluate the accuracy of each local piece of key information in the summary.
We also consider question-answering-based metrics to evaluate the factual accuracy of generation.
In this paper, we examine QuestEval [45], a metric that improves the correlation with human
judgments over previous metrics in their extensive experiments. QuestEval evaluates the factual
consistency between the generated text and the source document, which is mainly used for text
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 13):

Figure 2: RAG Systems Evaluation on NQ - Context Relevance
Figure 3: RAG Systems Evaluation on NQ - Answer Relevance



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 25):

111:26 Lyu, et al.
Table 8. The experimental results for evaluating different large language models in our benchmark.
task name model name bleu rouge-L bertScore RAGQuestEvallength
precision recall
text continuation
ChatGLM2-6B2.06 13 .35 68 .51 20 .68 15 .44 363.3
Qwen-7B 7.10 15.31 77 .94 28 .06 18 .44 159.6
Baichuan2-13B3.97 14 .21 71 .75 28 .62 22 .95 358.4
Qwen-14B 5.70 18 .48 82 .97 27 .89 21 .68 240.1
GPT-3.5-turbo3.66 17 .78 83 .99 26 .96 24 .68 367.6
GPT-4-0613 5.58 19.47 84.91 30.34 28.02 369.8
Qwen2-7B 2.94 16.76 83.82 26.90 23.68 350.0
GPT-4o 4.48 18.85 84.45 30.89 26.11 356.7
summarization
ChatGLM2-6B17.09 28 .16 83 .00 58 .94 40 .35 228.1
Qwen-7B 28.30 30 .21 84 .26 67 .62 40 .03 240.5
Baichuan2-13B24.49 32 .49 85 .64 65 .96 42 .53 179.5
Qwen-14B 32.51 33.33 85 .62 68 .94 40 .57 139.1
GPT-3.5-turbo23.69 33 .53 88 .49 68 .06 46 .18 205.9
GPT-4-0613 24.54 35.91 89.39 71.24 50.53 194.6
Qwen2-7B 14.82 30.00 88.60 62.04 45.93 283.2
GPT-4o 23.24 35.40 89.65 68.28 50.93 217.7
question answering
ChatGLM2-6B29.11 47 .57 79 .59 50 .06 69 .35 90.8
1-document
Qwen-7B 39.63 56 .71 82 .64 51 .77 72 .02 68.8
Baichuan2-13B35.40 53 .85 83 .59 54 .35 76.92 91.3
Qwen-14B 37.95 55 .13 83 .25 53 .03 73 .92 73.8
GPT-3.5-turbo39.76 57.24 83.81 52.67 70 .82 73.3
GPT-4-0613 33.87 51 .42 80 .92 53 .14 62 .39 95.9
Qwen2-7B 23.06 41.25 82.10 60.07 72.17 123.3
GPT-4o 33.32 51.78 83.35 65.33 66.59 74.7
question answering
ChatGLM2-6B15.15 29 .12 82 .30 37 .61 51 .51 193.4
2-document
Qwen-7B 22.61 36 .07 85 .84 42 .32 56 .26 157.6
Baichuan2-13B20.32 35 .56 87 .49 45 .01 61 .47 208.8
Qwen-14B 21.11 34 .97 85 .87 42 .23 56 .59 151.1
GPT-3.5-turbo22.75 37.25 87.16 42 .93 56 .73 149.8
GPT-4-0613 20.38 36 .08 88 .10 49.56 62.56 223.0
Qwen2-7B 15.26 41.25 82.10 48.89 61.41 209.1
GPT-4o 22.84 36.61 88.38 44.04 67.44 124.3
question answering
ChatGLM2-6B14.01 27 .71 83 .42 35 .60 45 .28 204.1
3-document
Qwen-7B 21.63 33 .42 86 .31 39 .14 50 .55 160.6
Baichuan2-13B18.30 33 .34 88 .08 41 .35 55 .75 227.5
Qwen-14B 19.83 33 .33 86 .93 42 .01 51 .70 161.2
GPT-3.5-turbo21.05 35 .04 87 .81 40 .32 51 .37 156.6
GPT-4-0613 19.11 34 .58 88 .88 48.24 56.48 235.1
Qwen2-7B 16.23 32.18 87.69 45.72 55.29 207.2
GPT-4o 22.84 35.98 89.21 43.56 63.90 139.9
hallucination
ChatGLM2-6B13.51 28 .70 71 .26 59 .63 73 .02 176.0
modification
Qwen-7B 22.87 38 .10 73 .52 60 .00 73 .72 172.5
Baichuan2-13B10.56 27 .28 68 .90 54 .42 67 .47 124.8
Qwen-14B 33.78 51 .90 79 .49 67 .05 84.08 89.7
GPT-3.5-turbo32.35 53 .04 80 .49 65 .07 80 .85 64.8
GPT-4-0613 36.69 55.70 81.27 69.18 82.06 63.5
Qwen2-7B 31.07 52.91 80.25 65.48 79.16 49.3
GPT-4o 36.73 54.79 80.90 63.61 73.75 51.9
The top-k value is a crucial parameter for the RAG system, as it determines how many documents
are retrieved for each query. Depending on the scenario, the optimal top-k value may vary. For
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 23):

111:24 Lyu, et al.
Table 7. The experimental results for evaluating different top-k values in our benchmark.
task name topk bleu rouge-L bertScore RAGQuestEval length
precision recall
text continuation
2 2.89 17 .20 83 .60 25 .35 23 .14 367.0
4 3.34 17 .49 83 .80 26 .66 23 .54 369.3
6 3.53 17 .64 83 .81 27.66 24.32 375.4
8 3.66 17 .78 83 .99 26 .96 24 .68 367.6
10 3.91 17.84 84.01 27.61 25.00 355.7
summarization
2 26.86 33.87 87.34 70.08 42.21 161.0
4 24.78 33 .62 87 .95 68 .91 44 .19 185.6
6 23.71 33 .36 88 .16 68 .28 45 .08 198.6
8 23.69 33 .53 88 .49 68 .06 46 .18 205.9
10 23.62 33 .56 88.51 68.17 46.70 208.3
question answering
2 39.13 56 .26 82 .57 50 .81 65 .80 67.7
1-document
4 39.47 56 .58 83 .39 52 .14 69 .53 70.6
6 39.40 56 .86 83 .81 52 .60 70 .80 72.5
8 39.76 57.24 83.81 52 .67 70.82 73.3
10 38.84 56 .52 83.93 53.67 70.31 74.1
question answering
2 21.65 35 .16 84 .72 36 .91 47 .41 126.5
2-document
4 22.33 36 .68 86 .39 41 .15 52 .78 139.5
6 23.04 37.43 87 .01 43 .29 55 .47 143.7
8 22.75 37 .25 87 .16 42 .93 56 .73 149.8
10 22.90 37.63 87.43 43.88 57.34 153.4
question answering
2 19.27 32 .57 85 .65 33 .70 43 .90 136.3
3-document
4 20.23 34 .21 86 .93 37 .26 48 .35 145.5
6 20.73 34 .95 87 .66 39 .59 51 .03 151.3
8 21.05 35.04 87.81 40 .32 51 .37 156.6
10 20.61 35 .01 88.02 40.90 52.11 162.5
hallucination
2 32.12 53 .00 80.54 64.95 79 .24 59.6
modification
4 32.50 52.94 80 .53 65.18 79.34 60.2
6 32.32 52 .70 80 .36 64 .48 79 .27 61.8
8 32.35 53.04 80.49 65 .07 80 .85 64.8
10 31.30 51 .71 80 .09 64 .84 80.90 68.3
can significantly increase the recall and precision scores. This is because when the retrieved content
is small, it may not be helpful for answering.
For multi-document QA (2-document and 3-document), increasing top-k significantly improves
the recall and precision scores, as there are more chances to retrieve two relevant and complementary
documents. More documents can also provide additional information, which helps to bridge the
knowledge gap between documents and give more comprehensive answers. The results of 2-
document and 3-document question answering are similar.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 5):

111:6 Lyu, et al.
2.2 RAG Benchmarks
When investigating the development and optimization of RAG, the effective evaluation of their
performance becomes a fundamental concern. Table 1 shows some commonly used benchmarks
for evaluating RAG. LangChain provides benchmark tasks, such as LangChain Docs Q&A and
Semi-structured Reports [27], designed to assess various RAG architectures. These datasets are
constructed from snapshots of Python documentation and PDFs containing tables and charts.
They emphasize the model’s capability to handle structured and semi-structured data. Evaluation
standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing
large models for question-answering generation has emerged as a prevalent approach in building
evaluation datasets. For instance, RGB [8] creates its evaluation dataset by gathering recent news
reports and employing LLM to generate relevant events, questions, and answers. Conversely,
ARES [44]. relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
These methods not only showcase the RAG system’s proficiency in handling real-time data but also
illustrate the utility of automation and synthetic data in the evaluation process. For evaluating the
capabilities of models across various professional domains, the Instruct-Benchmark-Tester dataset
encompasses a range of question types, with a particular focus on financial services, legal, and
intricate business scenarios [39].
Depending on whether the evaluation phase incorporates ground truth, metrics of existing
evaluation methods can be categorized into those necessitating reference and those not requiring
it. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by
contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-
Benchmark-Tester [39] employs accuracy score as an evaluation metric, a widely acknowledged
measure of model performance that assesses the extent to which model-generated answers align
with reference answers. The primary objective of RGB [8] is to evaluate whether large models can
effectively utilize external documents to acquire knowledge and generate accurate answers. Its
evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.
Reference-free evaluation methods, including TruLens-Eval [14], RAGAS [13], and ARES [44],
provide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning
context relevance, answer faithfulness, and answer relevance. TruLens-Eval [14] introduces the
RAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,
encompassing context relevance, groundedness, and answer relevance. RAGAS [13], serving as
a reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity
to identify pertinent and concentrated context passages, along with the LLMs’ proficiency in
faithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a
predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each
aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy
when compared to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-
powered inference to offer statistical assurances for its scoring, generating confidence intervals.
ARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer
relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts
and producing both faithful and relevant answers. Regarding evaluation methods, [32] places an
emphasis on assessing the credibility and accuracy of responses generated by generative search
engines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including
high costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,
exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely
approximates manual evaluation outcomes.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 13):

111:14 Lyu, et al.
Fig. 6. Overview of RAGQuestEval. A set of questions is generated based on the ground truth references.
The questions are then answered using both the ground truth and the response. For the recall score of
RAGQuestEval, we calculate the ratio of answerable questions to all questions(in this case, recall = 2/3). For
the precision score of RAGQuestEval, corresponding answers are compared using a similarity function and
averaged across questions(in this case, precision = (0.5 + 1) / 2 = 0.75). The recall metric of RAGQuestEval
indicates how much of the key information in the ground truth reference is included in the generated text,
while the precision metric of RAGQuestEval indicates how correct the recalled key information is.
The RAG system’s experimental results on this dataset can confirm if the system can retrieve the
real news information from the document database based on the input text, which consists of the
beginning text and the hallucination continuation text, and then correct the hallucination text to
generate the text without hallucination.
3.6 Evaluation Method
The aim of this benchmark is to evaluate how well RAG systems can retrieve relevant documents,
and use them to generate sensible responses. Therefore, we adopt an end-to-end evaluation method,
which directly compares the similarity between the model output and the reference answers.
Evaluating the performance of RAG systems requires choosing appropriate evaluation metrics.
We considered the previous evaluation metrics for text generation, ROUGE and BLEU, which are
both based on word overlap. ROUGE mainly counts the recall rate on the n-gram, while BLEU
mainly counts the precision rate on the n-gram. However, BLEU and ROUGE are word overlap-
based accuracy metrics that depend on the overall expression of the text, and do not capture the
accuracy of the particular key information in the text. Therefore, they may not reflect the factual
consistency of a text well, especially for long texts. To alleviate this issue, recent work [12, 45, 49]
has proposed new evaluation metrics for abstractive summarization evaluation. These metrics are
based on the intuition that if you ask questions about the summary and the original document,
you will get a similar answer if the summary realistically matches the original document. They
evaluate the accuracy of each local piece of key information in the summary.
We also consider question-answering-based metrics to evaluate the factual accuracy of generation.
In this paper, we examine QuestEval [45], a metric that improves the correlation with human
judgments over previous metrics in their extensive experiments. QuestEval evaluates the factual
consistency between the generated text and the source document, which is mainly used for text
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 13):

Figure 2: RAG Systems Evaluation on NQ - Context Relevance
Figure 3: RAG Systems Evaluation on NQ - Answer Relevance



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 25):

111:26 Lyu, et al.
Table 8. The experimental results for evaluating different large language models in our benchmark.
task name model name bleu rouge-L bertScore RAGQuestEvallength
precision recall
text continuation
ChatGLM2-6B2.06 13 .35 68 .51 20 .68 15 .44 363.3
Qwen-7B 7.10 15.31 77 .94 28 .06 18 .44 159.6
Baichuan2-13B3.97 14 .21 71 .75 28 .62 22 .95 358.4
Qwen-14B 5.70 18 .48 82 .97 27 .89 21 .68 240.1
GPT-3.5-turbo3.66 17 .78 83 .99 26 .96 24 .68 367.6
GPT-4-0613 5.58 19.47 84.91 30.34 28.02 369.8
Qwen2-7B 2.94 16.76 83.82 26.90 23.68 350.0
GPT-4o 4.48 18.85 84.45 30.89 26.11 356.7
summarization
ChatGLM2-6B17.09 28 .16 83 .00 58 .94 40 .35 228.1
Qwen-7B 28.30 30 .21 84 .26 67 .62 40 .03 240.5
Baichuan2-13B24.49 32 .49 85 .64 65 .96 42 .53 179.5
Qwen-14B 32.51 33.33 85 .62 68 .94 40 .57 139.1
GPT-3.5-turbo23.69 33 .53 88 .49 68 .06 46 .18 205.9
GPT-4-0613 24.54 35.91 89.39 71.24 50.53 194.6
Qwen2-7B 14.82 30.00 88.60 62.04 45.93 283.2
GPT-4o 23.24 35.40 89.65 68.28 50.93 217.7
question answering
ChatGLM2-6B29.11 47 .57 79 .59 50 .06 69 .35 90.8
1-document
Qwen-7B 39.63 56 .71 82 .64 51 .77 72 .02 68.8
Baichuan2-13B35.40 53 .85 83 .59 54 .35 76.92 91.3
Qwen-14B 37.95 55 .13 83 .25 53 .03 73 .92 73.8
GPT-3.5-turbo39.76 57.24 83.81 52.67 70 .82 73.3
GPT-4-0613 33.87 51 .42 80 .92 53 .14 62 .39 95.9
Qwen2-7B 23.06 41.25 82.10 60.07 72.17 123.3
GPT-4o 33.32 51.78 83.35 65.33 66.59 74.7
question answering
ChatGLM2-6B15.15 29 .12 82 .30 37 .61 51 .51 193.4
2-document
Qwen-7B 22.61 36 .07 85 .84 42 .32 56 .26 157.6
Baichuan2-13B20.32 35 .56 87 .49 45 .01 61 .47 208.8
Qwen-14B 21.11 34 .97 85 .87 42 .23 56 .59 151.1
GPT-3.5-turbo22.75 37.25 87.16 42 .93 56 .73 149.8
GPT-4-0613 20.38 36 .08 88 .10 49.56 62.56 223.0
Qwen2-7B 15.26 41.25 82.10 48.89 61.41 209.1
GPT-4o 22.84 36.61 88.38 44.04 67.44 124.3
question answering
ChatGLM2-6B14.01 27 .71 83 .42 35 .60 45 .28 204.1
3-document
Qwen-7B 21.63 33 .42 86 .31 39 .14 50 .55 160.6
Baichuan2-13B18.30 33 .34 88 .08 41 .35 55 .75 227.5
Qwen-14B 19.83 33 .33 86 .93 42 .01 51 .70 161.2
GPT-3.5-turbo21.05 35 .04 87 .81 40 .32 51 .37 156.6
GPT-4-0613 19.11 34 .58 88 .88 48.24 56.48 235.1
Qwen2-7B 16.23 32.18 87.69 45.72 55.29 207.2
GPT-4o 22.84 35.98 89.21 43.56 63.90 139.9
hallucination
ChatGLM2-6B13.51 28 .70 71 .26 59 .63 73 .02 176.0
modification
Qwen-7B 22.87 38 .10 73 .52 60 .00 73 .72 172.5
Baichuan2-13B10.56 27 .28 68 .90 54 .42 67 .47 124.8
Qwen-14B 33.78 51 .90 79 .49 67 .05 84.08 89.7
GPT-3.5-turbo32.35 53 .04 80 .49 65 .07 80 .85 64.8
GPT-4-0613 36.69 55.70 81.27 69.18 82.06 63.5
Qwen2-7B 31.07 52.91 80.25 65.48 79.16 49.3
GPT-4o 36.73 54.79 80.90 63.61 73.75 51.9
The top-k value is a crucial parameter for the RAG system, as it determines how many documents
are retrieved for each query. Depending on the scenario, the optimal top-k value may vary. For
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



### Claim 71/179

#### Claim Text
Tools like RALLE, designed for the automatic evaluation of RAG applications, similarly base their assessments on these taskspecific metrics [160].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 5):

111:6 Lyu, et al.
2.2 RAG Benchmarks
When investigating the development and optimization of RAG, the effective evaluation of their
performance becomes a fundamental concern. Table 1 shows some commonly used benchmarks
for evaluating RAG. LangChain provides benchmark tasks, such as LangChain Docs Q&A and
Semi-structured Reports [27], designed to assess various RAG architectures. These datasets are
constructed from snapshots of Python documentation and PDFs containing tables and charts.
They emphasize the model’s capability to handle structured and semi-structured data. Evaluation
standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing
large models for question-answering generation has emerged as a prevalent approach in building
evaluation datasets. For instance, RGB [8] creates its evaluation dataset by gathering recent news
reports and employing LLM to generate relevant events, questions, and answers. Conversely,
ARES [44]. relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
These methods not only showcase the RAG system’s proficiency in handling real-time data but also
illustrate the utility of automation and synthetic data in the evaluation process. For evaluating the
capabilities of models across various professional domains, the Instruct-Benchmark-Tester dataset
encompasses a range of question types, with a particular focus on financial services, legal, and
intricate business scenarios [39].
Depending on whether the evaluation phase incorporates ground truth, metrics of existing
evaluation methods can be categorized into those necessitating reference and those not requiring
it. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by
contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-
Benchmark-Tester [39] employs accuracy score as an evaluation metric, a widely acknowledged
measure of model performance that assesses the extent to which model-generated answers align
with reference answers. The primary objective of RGB [8] is to evaluate whether large models can
effectively utilize external documents to acquire knowledge and generate accurate answers. Its
evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.
Reference-free evaluation methods, including TruLens-Eval [14], RAGAS [13], and ARES [44],
provide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning
context relevance, answer faithfulness, and answer relevance. TruLens-Eval [14] introduces the
RAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,
encompassing context relevance, groundedness, and answer relevance. RAGAS [13], serving as
a reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity
to identify pertinent and concentrated context passages, along with the LLMs’ proficiency in
faithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a
predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each
aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy
when compared to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-
powered inference to offer statistical assurances for its scoring, generating confidence intervals.
ARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer
relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts
and producing both faithful and relevant answers. Regarding evaluation methods, [32] places an
emphasis on assessing the credibility and accuracy of responses generated by generative search
engines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including
high costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,
exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely
approximates manual evaluation outcomes.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 1):

RAG system finds relevant contexts and generates
answers that are both faithful and relevant.
Many existing RAG evaluation frameworks re-
quire substantial human annotations for scoring.
ARES significantly improves data efficiency dur-
ing evaluation by only requiring three inputs: an in-
domain passage set, a human preference validation
set of approximately 150 annotated datapoints or
more, and few-shot examples of in-domain queries
and answers (e.g. five examples or more), which
are used for prompting LLMs in synthetic data gen-
eration.
Given the corpus of in-domain passages, ARES
proceeds in three stages. First, it leverages an LM
to construct a synthetic dataset of question–answer
pairs, derived from the passages in the corpus. Sec-
ond, it defines three separate judge models to per-
form three classification tasks (context relevance,
answer faithfulness, and answer relevance). These
judges are lightweight models fine-tuned against a
contrastive learning objective. Third, ARES scores
the different RAG systems being assessed using
prediction-powered inference (PPI; Angelopoulos
et al. 2023) to improve model-based evaluation ac-
curacy and provide statistical confidence intervals
for RAG scoring. PPI utilizes a small set of human
annotated datapoints for computing its confidence
intervals; we designate this annotated set as our hu-
man preference validation set, which is composed
of approximately 150 annotated datapoints or more
that designate both positive and negative examples
for context relevance, answer faithfulness, and an-
swer relevance.
We conduct extensive empirical evaluations,
demonstrating that ARES accurately scores
RAG systems across the six knowledge-intensive
datasets in KILT and SuperGLUE, beating exist-
ing automated evaluation approaches like RAGAS
by 59.3 and 14.4 percentage points on average
across context relevance and answer relevance eval-
uation accuracy, respectively. Additionally, ARES
accurately calculates answer hallucination occur-
rences in the AIS attribution dataset (Rashkin et al.,
2022), predicting within 2.5 percentage points of
the ground truth average for answer hallucinations.
Compared to annotation-based evaluation methods,
ARES is substantially more accurate and efficient,
requiring 78% less annotations than the baseline
approach. We also find that ARES consistently
distinguishes competitive RAG systems that are
only a few points apart in ground-truth metrics.
This precision enables ARES to guide the develop-
ment and comparison of competitive approaches
and configurations.
We make the ARES code and datasets publicly
available on Github.
2 Related Work
RAG (Guu et al., 2020; Lewis et al., 2020; Khat-
tab et al., 2021; Izacard et al., 2022)) is now a
common strategy for bolstering LLMs by combin-
ing them with retrieval systems. Through retrieval,
RAG helps LM systems gather domain-specific
knowledge, ground generations in factual informa-
tion (Shuster et al., 2021; Huo et al., 2023), and
offer a degree of transparency or interpretability
via citing sources (Mialon et al., 2023).
Multiple LLM-based evaluation techniques have
emerged for gauging LLM systems. This is essen-
tial for rapid deployment in new settings, where it
is difficult to build a traditional benchmark dataset
from scratch. Early attempts at this use LLMs
out of the box, as in MT-Bench and Chatbot
Arena (Zheng et al., 2023). AutoCalibrate (Liu
et al., 2023b) seeks to align an LLM-judge with
human preferences, leveraging a self-refinement
prompt to iteratively improve the LLM judge. How-
ever, AutoCalibrate does not offer any statistical
guarantees for the accuracy of its predictions. Other
work has used LLM prompting to evaluate system
quality across natural language generation tasks,
such as translation, summarization, and dialogue
(Kocmi and Federmann, 2023; Fu et al., 2023; Liu
et al., 2023a; Wang et al., 2023).
In the context of knowledge-intensive NLP tasks,
LLMs have been explored for assessing attribution
and factuality in LLMs (Min et al., 2023; Gekhman
et al., 2023; Yue et al., 2023). New guidelines
like LongEval (Krishna et al., 2023) and datasets
like Hagrid and ALCE (Kamalloo et al., 2023;
Gao et al., 2023) provide resources for analyzing
knowledge-intensive LLM pipelines.
The two most-closely related projects to ARES
are EXAM (Sander and Dietz, 2021) and RA-
GAS (James and Es, 2023). To evaluate RAG sys-
tems, the EXAM metric estimates how many exam
questions a reader (simulated as a QA system) can
answer correctly based on the generated response.
This requires a set of queries with several asso-
ciated sub-questions each, which adds a burden
that ARES does not bring. RAGAS is based on a
handful of heuristic hand-written prompts. These
offer little adaptability to new RAG evaluation set-



Source: data\tc16_2312.10997v5\referenced_papers\[164]_2309.15217.pdf (Page 0):

RAGAS: Automated Evaluation of Retrieval Augmented Generation
Shahul Es†, Jithin James†, Luis Espinosa-Anke∗♢, Steven Schockaert∗
†Exploding Gradients
∗CardiffNLP, Cardiff University, United Kingdom
♢AMPLYFI, United Kingdom
shahules786@gmail.com,jamesjithin97@gmail.com
{espinosa-ankel,schockaerts1}@cardiff.ac.uk
Abstract
We introduce RAGA S (Retrieval Augmented
Generation Assessment), a framework for
reference-free evaluation of Retrieval Aug-
mented Generation (RAG) pipelines. RAG
systems are composed of a retrieval and an
LLM based generation module, and provide
LLMs with knowledge from a reference textual
database, which enables them to act as a natu-
ral language layer between a user and textual
databases, reducing the risk of hallucinations.
Evaluating RAG architectures is, however, chal-
lenging because there are several dimensions to
consider: the ability of the retrieval system to
identify relevant and focused context passages,
the ability of the LLM to exploit such passages
in a faithful way, or the quality of the gener-
ation itself. With RAGA S, we put forward a
suite of metrics which can be used to evaluate
these different dimensions without having to
rely on ground truth human annotations. We
posit that such a framework can crucially con-
tribute to faster evaluation cycles of RAG archi-
tectures, which is especially important given
the fast adoption of LLMs.
1 Introduction
Language Models (LMs) capture a vast amount
of knowledge about the world, which allows them
to answer questions without accessing any exter-
nal sources. This idea of LMs as repositories of
knowledge emerged shortly after the introduction
of BERT (Devlin et al., 2019) and became more
firmly established with the introduction of ever
larger LMs (Roberts et al., 2020). While the most
recent Large Language Models (LLMs) capture
enough knowledge to rival human performance
across a wide variety of question answering bench-
marks (Bubeck et al., 2023), the idea of using
LLMs as knowledge bases still has two fundamen-
tal limitations. First, LLMs are not able to answer
questions about events that have happened after
they were trained. Second, even the largest models
struggle to memorise knowledge that is only rarely
mentioned in the training corpus (Kandpal et al.,
2022; Mallen et al., 2023). The standard solution
to these issues is to rely on Retrieval Augmented
Generation (RAG) (Lee et al., 2019; Lewis et al.,
2020; Guu et al., 2020). Answering a question
then essentially involves retrieving relevant pas-
sages from a corpus and feeding these passages,
along with the original question, to the LM. While
initial approaches relied on specialised LMs for
retrieval-augmented language modelling (Khandel-
wal et al., 2020; Borgeaud et al., 2022), recent work
has suggested that simply adding retrieved docu-
ments to the input of a standard LM can also work
well (Khattab et al., 2022; Ram et al., 2023; Shi
et al., 2023), thus making it possible to use retrieval-
augmented strategies in combination with LLMs
that are only available through APIs.
While the usefulness of retrieval-augmented
strategies is clear, their implementation requires
a significant amount of tuning, as the overall per-
formance will be affected by the retrieval model,
the considered corpus, the LM, or the prompt for-
mulation, among others. Automated evaluation of
retrieval-augmented systems is thus paramount. In
practice, RAG systems are often evaluated in terms
of the language modelling task itself, i.e. by mea-
suring perplexity on some reference corpus. How-
ever, such evaluations are not always predictive
of downstream performance (Wang et al., 2023c).
Moreover, this evaluation strategy relies on the LM
probabilities, which are not accessible for some
closed models (e.g. ChatGPT and GPT-4). Ques-
tion answering is another common evaluation task,
but usually only datasets with short extractive an-
swers are considered, which may not be represen-
tative of how the system will be used.
To address these issues, in this paper we present
RAGA S1, a framework for the automated assess-
1RAGA S is available at https://github.com/
explodinggradients/ragas.
arXiv:2309.15217v1  [cs.CL]  26 Sep 2023



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 6):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:7
2.3 Citation-Enhanced RAG
In traditional RAG methods, despite the rich information sources provided by retrieved contexts for
text generation, these models often do not explicitly require responses to provide corresponding
citations, making traceability difficult. Therefore, enhancing text verifiability by introducing citation
links, i.e., explicit references, has become an important research direction in the RAG field [17, 54].
Providing citation indicators in the response text offers several clear benefits. First, users can
easily verify the claims made by LLMs based on the provided citations, thus improving the trans-
parency and credibility of the text. Second, if the text generated by LLMs adheres faithfully to the
cited contexts, it can significantly improve its accuracy and reduce the phenomenon of "halluci-
nations" [17]. Given this, generating high-quality citations and evaluating the quality of citation
generation have become crucial elements of assessing RAG performance. Constructing appropriate
prompts directly through the retrieval context to guide the model in generating corresponding
citations constitutes a direct and effective method of citation generation [22].
In terms of evaluation, early research primarily focused on the fluency, accuracy, and basic
citation quality of the text generated by LLMs [17, 29]. For example, Rashkin et al. proposed the
"Attributable to Identified Sources" (AIS) score [42], which serves as a valuable tool for measuring the
degree to which generated text is faithful to its sources. As research progressed, scholars recognized
the need for more detailed evaluation methods to differentiate between various levels of citation
support. By creating specialized datasets such as SCIFI [7], researchers can more precisely evaluate
fine-grained citations at the clause level in texts generated by LLMs. The ALiiCE framework [54], by
analyzing the atomic structure of sentence claims, introduced fine-grained evaluation metrics, such
as location citation recall and precision, and the coefficient of variation of citation locations, to more
granularly evaluate the quality of citation generation in RAG [54]. In practical applications, [61]
found that RAG requires more complex evaluation frameworks to distinguish between various
levels of citation support by comparing different fidelity metrics. These RAG evaluation methods
not only consider the presence of citations but also their accuracy and relevance.
While Citation-Enhanced RAG delves deeply into the specific domain of citation generation,
aiming to improve the credibility and accuracy of text generated by RAG systems, our benchmark
provides a comprehensive evaluation framework encompassing various aspects of RAG systems
and multiple application scenarios.
3 CRUD-RAG: A COMPREHENSIVE CHINESE BENCHMARK FOR RAG
As we discussed earlier, implementing RAG effectively requires careful tuning of multiple com-
ponents, such as the retrieval model, the knowledge corpus, the language model, and the query
formulation. Therefore, we need a framework that can evaluate the RAG system automatically. This
framework would enable us to examine how these components affect the system’s performance,
and provide us with useful insights for improving and innovating the system.
However, The current RAG benchmarks have several drawbacks: they only evaluate question
answering tasks [1, 41, 57], ignoring other diverse application of RAG. The optimization strategy
for question-answering tasks may not suit other tasks; And in the evaluation experiment, current
RAG benchmarks only account for the LLM component in the RAG pipeline, or the retriever in the
knowledge-intensive scenario, disregarding the vital roles of retrieval database construction and
retrieval in non-knowledge-intensive scenarios.
To address the shortcomings of previous benchmarks, we introduce CRUD-RAG, a comprehensive
Chinese benchmark for RAG. Figure 2 illustrates the features of our CRUD-RAG benchmark. It
classifies the RAG application scenarios into four categories: Create, Read, Update, and Delete,
then we construct appropriate evaluation tasks and datasets for each category. Besides, in the
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 0):

ARES: An Automated Evaluation Framework for Retrieval-Augmented
Generation Systems
Jon Saad-Falcon
Stanford University ∗
jonsaadfalcon@stanford.edu
Omar Khattab
Stanford University
okhattab@stanford.edu
Christopher Potts
Stanford University
cgpotts@stanford.edu
Matei Zaharia
Databricks and UC Berkeley
matei@databricks.com
Abstract
Evaluating retrieval-augmented generation
(RAG) systems traditionally relies on hand
annotations for input queries, passages to re-
trieve, and responses to generate. We intro-
duce ARES, an Automated RAG Evaluation
System, for evaluating RAG systems along
the dimensions of context relevance, answer
faithfulness, and answer relevance. By cre-
ating its own synthetic training data, ARES
finetunes lightweight LM judges to assess the
quality of individual RAG components. To
mitigate potential prediction errors, ARES uti-
lizes a small set of human-annotated datapoints
for prediction-powered inference (PPI). Across
eight different knowledge-intensive tasks in
KILT, SuperGLUE, and AIS, ARES accurately
evaluates RAG systems while using only a few
hundred human annotations during evaluation.
Furthermore, ARES judges remain effective
across domain shifts, proving accurate even
after changing the type of queries and/or docu-
ments used in the evaluated RAG systems. We
make our code and datasets publicly available
on Github.
1 Introduction
Retrieval-augmented generation (RAG) has be-
come a prominent approach for building user-
facing NLP applications, such as systems for ques-
tion answering (QA), fact-checking, and customer
support (Petroni et al., 2021; Wang et al., 2019).
Typically, a RAG system consists of a retriever and
a downstream language model (LM). Given a user
question, the retriever finds relevant passages from
a corpus and the LM uses these passages to gener-
ate a response. This formulation admits a multitude
of choices: what retrieval model to use, how to di-
vide the documents into retrieval chunks, and how
to prompt or finetune the LM to use the retrieved
information, to name only a few of the simplest
design decisions.
∗Project started during research internship at Databricks
The best design for a RAG system is not neces-
sarily universal across data domains, corpus sizes,
and cost/latency budgets. To tune their own RAG
systems, practitioners traditionally need hand an-
notations for test questions, passages to retrieve
(to assess the retriever), and responses to generate,
labeled specifically for their target domain. Alter-
natively, they may evaluate different approaches in
production by collecting human preferences that
compare the candidate systems. Unfortunately,
both of these strategies demand high expertise and
impose considerable annotation costs.
Model-based evaluation is an inexpensive strat-
egy to test generative output quality (Zheng et al.,
2023). For instance, the open-source RAGAS
framework (James and Es, 2023) prompts an LM
for evaluating the relevance of retrieved informa-
tion and the faithfulness and accuracy of generated
responses. Unfortunately, such strategies currently
rely for evaluation on a fixed set of heuristically
hand-written prompts, offering little adaptability
to various evaluation contexts and no guarantees
about quality.
To evaluate RAG systems rapidly and accu-
rately, we propose ARES, the Automated RAG
Evaluation System. ARES is the first automated
RAG evaluation system to generate tailored LLM
judges for each component of a RAG pipeline, lead-
ing to substantial boosts in evaluation precision and
accuracy compared to existing approaches like RA-
GAS. Furthermore, unlike existing RAG evaluation
systems, ARES provides confidence intervals for
its scoring by leveraging prediction-powered in-
ference (PPI; Angelopoulos et al. 2023). Given a
corpus of documents and a RAG system, ARES
reports three evaluation scores: context relevance
(is the retrieved information pertinent to the test
question), answer faithfulness (is the response gen-
erated by the language model properly grounded
in the retrieved context), and answer relevance (is
the response also relevant to the question). A good
arXiv:2311.09476v2  [cs.CL]  31 Mar 2024



### Claim 72/179

#### Claim Text
These quality scores evaluate the efficiency of the RAG model from different perspectives in the process of information retrieval and generation [164]–[166].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 4):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:5
Table 1. Relate Work.
Method Dataset Scale Evaluation Metrics Evaluation Method Application Field Ref. Lang.
[27] Based on LangChain Pythondocumentation QA dataset86 Accuracy of answer, Faithfulness ofresponse to the retrieved documentEvaluating retrieval andgeneration consistencyGeneral QA scenarios(Read) Yes EN
[27] PDF documents containingtables and charts5 Accuracy of answer, Faithfulness ofresponse to the retrieved documentEvaluating retrieval andgeneration consistency
Semi-structureddata scenarios(Read) Yes EN
[32] Query and responses(with citations)1450 Fluency, Perceived utility,Citation recall and precisionHuman evaluation Citation(Read) Yes EN
[17, 22] Questions, answersand contexts(with citations)- Fluency, Correctness,Citation quality Self-devised metrics,Human evaluation Citation(Read) Yes EN
[54] Questions, answersand contexts(with citations)1948 Location citation recall, Location precision,The coefficient of variation of citation locationsSelf-devised metricsCitation(Read) Yes EN
[29] Questions, answersand contexts(with citations)3422 Accuracy of factual and non-factual,AUC-PR, and so on Self-devised metrics,Common metrics Citation(Read) Yes EN
[61] Statement-citation Pairs12681 Correlation, Classification performance,Retrieval effectiveness, Faithfulness
Self-devised metrics,Common metrics,Human evaluation
Citation(Read) Yes EN
[7] Paragraphs(with citations)10000 Citation density,The coverage of reference factsSelf-devised metricsCitation(Read) Yes EN
[39] Questions, answersand contexts 200
Categorization ability,Logical/Mathematical reasoning,Complex question solving,Summarization ability
Accuracy Financial services,Legal, Business(Read,Delete) Yes EN
[8] LLM-generated dataset 1000
Noise robustness,Negative rejection,Information integration,Counterfactual robustness
Self-devised metricsGeneral, especiallynews domain(Read,Update) Yes CNEN
[14] — — Context relevance, Groundedness,Answer relevance Analyzing the RAG triadGeneral(Create,Read) No —
[13] — — Faithfulness, Answer relevance,Context relevance Automated evaluationusing LLM prompts General(Create,Read) No —
[44] LLM-generated dataset 150Context relevance, Answer faithfulness,Answer relevance Generating custom LLM judges foreach component of a RAG systemGeneral(Create,Read) No EN
Ours LLM-generated dataset 36166ROUGE, BLEU,bertScore, RAGQuestEvalEvaluating retrieval andgeneration consistency
General(Create,ReadUpdate,Delete) Yes CN
optimization, and post-retrieval processing [20]. Pre-retrieval processing encompasses data trans-
former to enhance text standardization, ensuring factual accuracy, optimizing index structures,
adjusting block sizes, and rewriting query [ 4, 16, 50, 52]. Retrieval model optimization entails
the fine-tuning of domain-specific embedding models and the application of dynamic embedding
techniques [11, 60]. Post-retrieval processing minimizes context length through reranking and
compression operations, aiming to emphasize critical information, diminish noise interference, and
enhance integration and utilization by the generator [37, 53, 55].
Furthermore, to enhance the precision and efficiency of the generator when handling retrieval
content, scholars have undertaken a series of optimization measures. As an illustration, researchers
have devised methods such as Chain-of-Note (CON) for the generator [58]. CON generates contin-
uous reading notes to comprehensively evaluate the relevance of retrieved documents to the posed
question, integrating this information to produce precise final responses. This approach further
enhances the capability of RAG in managing retrieval information, guaranteeing the production of
responses that are simultaneously accurate and pertinent. In specific domains, such as medical and
legal, models undergo fine-tuning to enhance the generator’s performance within those particular
fields [10, 24, 56]. Through the implementation of these methods, the generator can more effectively
process retrieved information and furnish responses that are more accurate and relevant.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 5):

111:6 Lyu, et al.
2.2 RAG Benchmarks
When investigating the development and optimization of RAG, the effective evaluation of their
performance becomes a fundamental concern. Table 1 shows some commonly used benchmarks
for evaluating RAG. LangChain provides benchmark tasks, such as LangChain Docs Q&A and
Semi-structured Reports [27], designed to assess various RAG architectures. These datasets are
constructed from snapshots of Python documentation and PDFs containing tables and charts.
They emphasize the model’s capability to handle structured and semi-structured data. Evaluation
standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing
large models for question-answering generation has emerged as a prevalent approach in building
evaluation datasets. For instance, RGB [8] creates its evaluation dataset by gathering recent news
reports and employing LLM to generate relevant events, questions, and answers. Conversely,
ARES [44]. relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
These methods not only showcase the RAG system’s proficiency in handling real-time data but also
illustrate the utility of automation and synthetic data in the evaluation process. For evaluating the
capabilities of models across various professional domains, the Instruct-Benchmark-Tester dataset
encompasses a range of question types, with a particular focus on financial services, legal, and
intricate business scenarios [39].
Depending on whether the evaluation phase incorporates ground truth, metrics of existing
evaluation methods can be categorized into those necessitating reference and those not requiring
it. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by
contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-
Benchmark-Tester [39] employs accuracy score as an evaluation metric, a widely acknowledged
measure of model performance that assesses the extent to which model-generated answers align
with reference answers. The primary objective of RGB [8] is to evaluate whether large models can
effectively utilize external documents to acquire knowledge and generate accurate answers. Its
evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.
Reference-free evaluation methods, including TruLens-Eval [14], RAGAS [13], and ARES [44],
provide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning
context relevance, answer faithfulness, and answer relevance. TruLens-Eval [14] introduces the
RAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,
encompassing context relevance, groundedness, and answer relevance. RAGAS [13], serving as
a reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity
to identify pertinent and concentrated context passages, along with the LLMs’ proficiency in
faithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a
predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each
aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy
when compared to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-
powered inference to offer statistical assurances for its scoring, generating confidence intervals.
ARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer
relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts
and producing both faithful and relevant answers. Regarding evaluation methods, [32] places an
emphasis on assessing the credibility and accuracy of responses generated by generative search
engines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including
high costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,
exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely
approximates manual evaluation outcomes.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 14):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:15
summarization tasks. Therefore, it does not require any ground truth reference. However, for
RAG systems, the retrieved texts may be irrelevant or incorrect, so consistency with them is not a
valid criterion. Instead, we use this metric to measure how well the generated text matches the
ground-truth reference. We call this metric RAGQuestEval. We will explain this metric in detail.
Let 𝐺𝑇 and 𝐺𝑀 be two sequences of tokens, where 𝐺𝑇 denotes the ground truth references and
𝐺𝑀 the corresponding evaluated generations. First, we generate a series of questions from the
ground truth references 𝐺𝑇 using the QuestEval method, which extracts entities and noun phrases
from the text. The goal of RAGQuestEval is to check if the generated text includes and conveys
correctly all the key information from the ground truth reference.
Next, we answer these questions using both real references and model-generated text. If the
question is unanswerable, the model returns "<Unanswerable>".
Finally, we calculate two scores to evaluate the quality of the generated text: recall and precision.
Recall. Recall is the ratio of answerable questions to all questions. This score shows how much
information in the ground truth reference is captured by the text generated by the RAG system. A
higher recall means that the generated text covers more information from the reference.
Recall(𝐺𝑇,𝐺𝑀 )= 1
|𝑄𝐺(𝐺𝑇)|
∑︁
(𝑞,𝑟)∈𝑄𝐺 (𝐺𝑇)
I[𝑄𝐴(𝐺𝑀,𝑞)≠ < Unanswerable >] (1)
In the above equation, 𝑄𝐺 is the question generator and 𝑄𝐴 is the question answerer.
Precision. Precision is the average answer similarity of all questions, excluding the unanswerable
ones. We use the token level F1 score to measure the answer similarity, which is a standard metric
for evaluating factoid question answering models. Higher precision means that the generated text
is more accurate and consistent with the reference.
Prec(𝐺𝑇,𝐺𝑀 )= 1
|𝑄𝐺(𝐺𝑇)|
∑︁
(𝑞,𝑟)∈𝑄𝐺 (𝐺𝑇)
𝐹1 (𝑄𝐴(𝐺𝑀,𝑞),𝑟) (2)
4 EXPERIMENT
The current evaluation of RAG Benchmark only focuses on the large language model component
in the RAG pipeline, and overlooks the importance of retrieval database construction and retriever.
To address this gap, we examine how different aspects of RAG systems affect their performance in
our benchmark. We also discuss some possible ways to improve existing RAG systems.
4.1 Experimental Settings
In this section, we will introduce the components of the RAG system, and describe how we conduct
experiments to evaluate their impact on system performance. The RAG system consists of the
following components:
•Chunk size: The RAG system splits the external knowledge into chunks of a certain length
and stores them in a vector database. The chunk size affects the retrieval accuracy and the
completeness of the context.
•Chunk overlap: Chunk overlap refers to the shared tokens between two consecutive text
chunks and is used to ensure semantic coherence when chunking.
•Embedding model : The RAG system converts the text chunks and the user’s query into
vectors using an embedding model or other methods. The embedding model affects the
quality and relevance of the context.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 22):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:23
4.5 Analyzing the Impact of Embedding Model on RAG Performance in Different Tasks
Most RAG systems use vector similarity-based algorithms as retrievers. Therefore, the embedding
model that converts document blocks into vectors is crucial for the retrieval effect. We tested
various embedding models optimized for retrieval tasks and compared their performance in the
RAG system. We evaluated several embedding models with similar parameter sizes. According
to [38], the embedding models’ performance on the retrieval task should follow the order of GTE >
STELLA > BGE > M3E. Our results show some variations with this order.
For creative tasks like continuation, the relevance of the retrieved content was often ambiguous.
Thus, we noticed that the performance difference between the embedding models was small.
For single-document question answering tasks that required precise localization of relevant
documents, we found that m3e-base performed much worse than others. This matched the finding
of [38]. However, for the hallucination modification task, m3e-base, which ranked the lowest on
the retrieval benchmark, outperformed the other models on all metrics. These results further show
that the retrieval benchmark may not be fully appropriate for RAG.
Retrieval Accuracy Evaluation: Similar to the experiments in the retriever evaluation, we use
the MRR metric to evaluate four mainstream embedding methods: BGE, GTE, M3E, and STELLA.
The results in Figure 8 indicate that the performance of these methods is relatively close across
different tasks, with no single method outperforming the others in all tasks. This underscores the
importance of considering specific task requirements when selecting an embedding method.
As the number of documents increases from 1 to 3, the MRR values for all methods show a
downward trend. This trend aligns with our previous end-to-end experimental results, highlighting
the challenges of multi-document understanding tasks.
4.6 Analyzing the Impact of Top-k on RAG Performance in Different Tasks
The RAG system converts the user’s query into a vector using the same embedding model as
the vector database. Then, it searches the index for the top-k most similar vectors to the query
vector, and retrieves the corresponding text blocks from the database. These text blocks serve as
the context for the LLM prompt. The amount of information that the model receives depends on
the size of k. We will show how the amount of context information affects the system performance
for different tasks in Table 7.
Text Continuation: Text continuation is a highly creative task. Table 7 shows that increasing
top-k improves both the overall semantic similarity metrics (bertScore, bleu, and rouge-L) and the
RAGQuestEval metrics. The recall metric of RAGQuestEval shows how much key information from
the reference is included in the generated text, while the precision metric shows how correct and
relevant that information is. We found that higher top-k values lead to higher recall and precision
scores, indicating that the generated text contains more and better key information. We attribute
this to the increased diversity and accuracy of the generated text from more documents.
Open-Domain Multi-Document Summarization: Increasing the top-k value leads to longer
and lower-quality summaries. The rouge-L and bertScore metrics stay almost the same, but the
bleu metric drops significantly, indicating less similarity between the summaries and the references.
The top-k value also affects the key information metrics. Higher top-k values increase the recall
scores, meaning more key information is included, but decrease the precision scores, meaning more
errors or redundancies are present.
Question Answering: For single-document QA, increasing top-k has little impact on the
semantic similarity metric, but improves the RAGQuestEval metrics, which measure the accuracy
and recall of key information. When the top-k value is too small, increasing the value of the top-k
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 13):

111:14 Lyu, et al.
Fig. 6. Overview of RAGQuestEval. A set of questions is generated based on the ground truth references.
The questions are then answered using both the ground truth and the response. For the recall score of
RAGQuestEval, we calculate the ratio of answerable questions to all questions(in this case, recall = 2/3). For
the precision score of RAGQuestEval, corresponding answers are compared using a similarity function and
averaged across questions(in this case, precision = (0.5 + 1) / 2 = 0.75). The recall metric of RAGQuestEval
indicates how much of the key information in the ground truth reference is included in the generated text,
while the precision metric of RAGQuestEval indicates how correct the recalled key information is.
The RAG system’s experimental results on this dataset can confirm if the system can retrieve the
real news information from the document database based on the input text, which consists of the
beginning text and the hallucination continuation text, and then correct the hallucination text to
generate the text without hallucination.
3.6 Evaluation Method
The aim of this benchmark is to evaluate how well RAG systems can retrieve relevant documents,
and use them to generate sensible responses. Therefore, we adopt an end-to-end evaluation method,
which directly compares the similarity between the model output and the reference answers.
Evaluating the performance of RAG systems requires choosing appropriate evaluation metrics.
We considered the previous evaluation metrics for text generation, ROUGE and BLEU, which are
both based on word overlap. ROUGE mainly counts the recall rate on the n-gram, while BLEU
mainly counts the precision rate on the n-gram. However, BLEU and ROUGE are word overlap-
based accuracy metrics that depend on the overall expression of the text, and do not capture the
accuracy of the particular key information in the text. Therefore, they may not reflect the factual
consistency of a text well, especially for long texts. To alleviate this issue, recent work [12, 45, 49]
has proposed new evaluation metrics for abstractive summarization evaluation. These metrics are
based on the intuition that if you ask questions about the summary and the original document,
you will get a similar answer if the summary realistically matches the original document. They
evaluate the accuracy of each local piece of key information in the summary.
We also consider question-answering-based metrics to evaluate the factual accuracy of generation.
In this paper, we examine QuestEval [45], a metric that improves the correlation with human
judgments over previous metrics in their extensive experiments. QuestEval evaluates the factual
consistency between the generated text and the source document, which is mainly used for text
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



### Claim 73/179

#### Claim Text
Answer Relevance requires that the generated answers are directly pertinent to the posed questions, effectively addressing the core inquiry. 2) Required Abilities: RAG evaluation also encompasses four abilities indicative of its adaptability and efficiency: noise robustness, negative rejection, information integration, and counterfactual robustness [167], [168].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[167]_2309.01431.pdf (Page 6):

English ChineseNoise Ratio 0 0.2 0.4 0 0.2 0.4ChatGPT 55 51 34 63 58 47ChatGLM-6B45 36 35 60 53 52ChatGLM2-6B34 32 21 44 43 32Vicuna-7B-v1.360 53 43 43 36 25Qwen-7B-Chat55 50 37 67 56 55BELLE-7B-2M40 34 24 49 41 38
Table 5: The experimental result of information integration
measured by accuracy (%) under different noise ratios. We
can see that information integration poses a challenge for
RAG in LLMs.
Question Answer Response Errors
What groupings are France andGermany in World Cup 2022?Group DGroup E
France and Germany are inGroup D of the 2022 FIFAWorld Cup.
MergingError
Who were the MVP of SuperBowl 2022 and 2023?
Cooper KuppPatrickMahomes
According to the provideddocument, the MVP of SuperBowl LVI was Cooper Kuppof the Los Angeles Rams.
IgnoringError
What films won the 2022 and2023 Academy Awards forBest Picture?
CODAEverythingEverywhereAll at Once
The film CODA won theaward for Best Picture at the95th Academy Awardsceremony held on 2023.
MisalignmentError
Table 6: Error cases of information integration, the re-
sponses are generated by ChatGLM2-6B. The blue and red
texts represent the answers to two sub-questions.
(1) Merging Error (28% of the total).The model some-
times merges the answers of the two sub-questions, resulting
in an error. It mistakenly uses the answer from one question
to address both two questions. At this point, the model will
disregard any documents related to one sub-question. For
example, in Table 6, it incorrectly states that Group D is the
World Cup group for both France and Germany, while in fact
Germany is actually assigned to Group E.
(2) Ignoring Error (28% of the total).Sometimes, the
model may ignore one of the sub-questions and only answer
the other. This error occurs when the model lacks a complete
understanding of the problem and fails to recognize that it
consists of multiple sub-problems. As a result, the model
only considers relevant documents for one sub-problem in
order to generate an answer, disregarding the question posed
by another sub-problem. For example, in Table 6, the model
only provides the answer for the MVP of Super Bowl 2022
and does not consider 2023.
(3) Misalignment Error (6% of the total).Sometimes,
the model incorrectly identifies the documents for one sub-
question as the documents for another sub-question, leading
to misaligned answers. For example, in Table 6, the third an-
swer has two errors: an ignoring error and a misalignment er-
ror. Firstly, the model only mentioned the Best Picture of the
2023 (95th) Academy Awards, completely disregarding the
2022 awards. Additionally, it incorrectly stated that “CODA”
is the Best Picture of 2023 when it was actually awarded as
the Best Picture in 2022.
The errors mentioned above are primarily caused by the
limited understanding of complex questions, which hinders
the ability to effectively utilize information from different
sub-problems. The key lies in improving the model’s rea-
soning capability. One possible solution is to use a chain-of-
Acc Accdoc ED ED∗ CR
ChatGPT-zh 91 17 1 3 33.33Qwen-7B-Chat-zh77 12 5 4 25.00ChatGPT-en 89 9 8 7 57.14
Table 7: The result of counterfactual robustness. ACC is the
accuracy (%) of LLMs without external documents. ACCdoc
is the accuracy (%) of LLMs with counterfactual documents.
ED and ED ∗ are error detection rates evaluated by exact
matching and ChatGPT, respectively. CR is the error cor-
rection rate.
thought approach to break down complex problems (Zhou
et al. 2023a; Xu et al. 2023b; Drozdov et al. 2023). How-
ever, these methods slow down the inference speed and can-
not provide timely responses.
Results on Counterfactual Robustness testbed
In order to ensure that LLMs possess relevant knowledge,
we assess their performance by directly asking them ques-
tions. However, we found that most LLMs struggle to an-
swer them correctly. To ensure a more reasonable evalua-
tion, we only consider LLMs that have an accuracy rate of
over 70% as this threshold is relatively high and encom-
passes more LLMs. The results are shown in Table 7. We
present the following metrics: accuracy without any docu-
ments, accuracy with counterfactual documents, error de-
tection rates, and error correction rates. We can see that It
is hard for LLMs to identify and correct factual errors in the
documents. This suggests that the model can be easily mis-
led by documents containing incorrect facts.
It is important to note that retrieval-augmented generation
is not designed to automatically address factual errors within
a given context, as this contradicts the underlying assump-
tion that the model lacks knowledge and relies on retrieved
documents for additional information. However, this issue is
crucial in practical applications due to the abundance of fake
news on the internet. Existing LLMs do not have a safeguard
to handle inaccurate responses caused by misinformation. In
fact, they heavily depend on the information they retrieve.
Even when LLMs contain the internal knowledge about the
questions, they often trust false information that is retrieved.
This presents significant a challenge for the future develop-
ment of RAG in LLMs.
Conclusion
In this paper, we evaluated four abilities of retrieval-
augmented generation in LLMs: noise robustness, nega-
tive rejection, information integration, and counterfactual
robustness. To conduct the evaluation, we built Retrieval-
Augmented Generation Benchmark (RGB). The instances of
RGB are generated from latest news articles and the external
documents obtained from search engines. The experimental
results suggest that current LLMs have limitations in the 4
abilities. This indicates that there is still a significant amount
of work needed to effectively apply RAG to LLMs. To en-
sure accurate and reliable responses from LLMs, it is crucial
to exercise caution and carefully design for RAG.



Source: data\tc16_2312.10997v5\referenced_papers\[50]_2311.09210.pdf (Page 6):

Models Noise NQ TriviaQA WebQ Average
Ratio EM F1 EM F1 EM F1 EM F1
Retrieve-Read 100% 34.28 41.74 55.30 61.67 29.58 46.34 39.72 49.92
+ CHAIN-OF-NOTE 41.83 49.58 64.30 70.00 36.85 53.07 47.66 57.55
(+7.55) (+7.84) (+9.00) (+8.33) (+7.27) (+6.73) (+7.94) (+7.63)
Retrieve-Read 80% 54.28 61.03 73.83 80.02 35.46 52.70 54.52 64.58
+ CHAIN-OF-NOTE 56.63 63.23 75.89 81.24 40.60 56.54 57.70 67.00
(+2.35) (+2.20) (+2.06) (+1.22) (+5.14) (+3.84) (+3.18) (+2.42)
Retrieve-Read 60% 61.44 67.94 78.44 83.65 37.01 54.16 58.96 68.58
+ CHAIN-OF-NOTE 63.43 69.33 78.79 84.07 41.26 56.91 61.16 70.10
(+1.99) (+1.39) (+0.35) (+0.42) (+4.25) (+2.75) (+2.20) (+1.52)
Retrieve-Read 40% 64.62 71.12 80.56 86.76 38.40 55.60 61.19 71.16
+ CHAIN-OF-NOTE 65.91 72.22 81.72 87.11 42.16 58.15 63.26 72.49
(+1.29) (+1.10) (+1.16) (+0.35) (+3.76) (+2.55) (+2.07) (+1.33)
Retrieve-Read 20% 67.21 73.69 81.73 87.89 39.95 56.66 62.96 72.75
+ CHAIN-OF-NOTE 70.00 76.08 82.86 88.24 44.36 60.13 65.74 74.82
(+2.79) (+2.39) (+1.13) (+0.35) (+4.41) (+3.47) (+2.78) (+2.07)
Retrieve-Read 0% 69.23 75.57 83.34 89.44 42.24 58.59 64.93 74.53
+ CHAIN-OF-NOTE 73.28 79.86 83.52 88.94 46.16 62.38 67.65 77.06
(+4.05) (+4.29) (+0.18) (-0.50) (+3.92) (+3.79) (+2.72) (+2.53)
Table 3: Evaluation on Noise Robustness. The backbone language model is LLaMa-2 7B. The CHAIN -OF-NOTE
framework shows superior performance compared to the standard RALM system, particularly notable at higher
noise ratios.We explain how we synthesize data with different noise ratios under real-world scenarios in§ 3.1.1.
Models↓ RealTimeQA
EM F1 RR
Retrieve-Read (Shi et al., 2023c)15.6 19.9 6.1
+ CHAIN-OF-NOTE (ours) 15.7 20.3 13.0
Table 4: Evaluation on Unknown Robustness. TheCON
shows better performance than standard RALM system.
3.3 Evaluation on Noise Robustness
As illustrated in Table 2, when faced with entirely
noisy documents, both the standard RALM and
our CHAIN -OF-NOTE enhanced RALM underper-
formed compared to the no-retrieval setting. This
suggests that RALMs can be misled by noisy infor-
mation, leading to more hallucinations.
Notably, equipping the model withCON enables
it to perform nearly as well as the baseline model
directly fine-tuned with QA pairs without retrieval,
showcasing its robustness to noise and its ability
to disregard irrelevant information. The CON ap-
proach is effective not only in fine-tuned, smaller-
sized models but also in large language models,
such as GPT-4, with adjustments made only to the
prompt. Besides, in comparison to the CHAIN -OF-
THOUGHT technique, commonly utilized in rea-
soning scenarios, CON presents a more efficient
strategy for retrieval-augmented settings, particu-
larly in addressing knowledge-intensive tasks.
Table 3 shows that RALM enhanced with CON
consistently outperforms the standard RALM, es-
pecially in scenarios with exclusively noisy doc-
uments. An average improvement of +7.9 in EM
score on fully noisy documents is observed on three
open-domain QA datasets, in average. Experiments
with lower noise ratios also consistently demon-
strate the improvements brought by CON, aligning
the overall QA performance.
3.4 Evaluation on Unknown Robustness
Table 4 illustrates that our RALM equipped with
CON exhibits superior robustness in handling un-
known scenario, particularly evident in the Real-
TimeQA benchmark. This benchmark falls com-
pletely outside the model’s domain and contains
real-time information that was not part of the
LLaMa-2 pre-training data. Despite this, models
are still capable of providing correct answers in
some cases, as the answers remain consistent over
time. In comparison to the standard RALM sys-
tem, our method shows a significant improvement,
exceeding +10.5 in its ability to reject to answer
questions in unknown scenario. The evaluation is
based on reject rate (RR), i.e., number of rejected



Source: data\tc16_2312.10997v5\referenced_papers\[167]_2309.01431.pdf (Page 4):

English Chinese
Noise Ratio 0 0.2 0.4 0.6 0.8 0 0.2 0.4 0.6 0.8
ChatGPT (OpenAI 2022) 96.33 94.67 94.00 90.00 76.00 95.67 94.67 91.00 87.67 70.67
ChatGLM-6B (THUDM 2023a) 93.67 90.67 89.33 84.67 70.67 94.33 90.67 89.00 82.33 69.00
ChatGLM2-6B (THUDM 2023b)91.33 89.67 83.00 77.33 57.33 86.67 82.33 76.67 72.33 54.00
Vicuna-7B-v1.3 (Chiang et al. 2023)87.67 83.33 86.00 82.33 60.33 85.67 82.67 77.00 69.33 49.67
Qwen-7B-Chat (QwenLM 2023)94.33 91.67 91.00 87.67 73.67 94.00 92.33 88.00 84.33 68.67
BELLE-7B-2M (Yunjie Ji 2023)83.33 81.00 79.00 71.33 64.67 92.00 88.67 85.33 78.33 67.68
Table 1: The experimental result of noise robustness measured by accuracy (%) under different noise ratios. We can see that the
increasing noise rate poses a challenge for RAG in LLMs.
Long-distance information. Evidence uncertainty. Concept confusion.
QuestionWho did Iga Swiatek defeat to win the Qatar Open 2022?What is the name of Apple’s headset?What was Tesla’s revenue in Q1 2022?
AnswerAnett Kontaveit Vision Pro 18.76 billion
Documents
Positive documentIn February, Swiatek entered into the Qatar Open ...In the final, she won ...Anett Kontaveit...
Negative documentThis time, she defeated Ons Jabeur 6-2, 7-6(5) to winthe 2022 US Open, ...
Positive documentApple (AAPL.O) on Monday unveiled a costlyaugmented-reality headset called theVision Pro...
Negative document... is what Gurman believes will be calledApple Reality Pro. ...
Positive documentTesla, Inc. (TSLA) reported Q1 FY 2022 earnings results... detailed revenues of $18.76 billion...
Negative document...first-quarter earnings for 2022 ......Automotive revenue reached $16.86 billion...
ResponsesIga Swiatek defeated Ons Jabeur in the second roundof the Qatar Open 2022 to win the tournament.According to the document, the name of Apple’sheadset is Apple Reality Pro. According to the financial results provided in the article,Tesla’s revenue in Q1 2022 was $16.86 billion.
Table 2: Error cases of noise robustness, and only one positive document and one negative document are shown. The responses
are generated by ChatGLM2-6B. The blue text indicates the matching parts between the document and the question or answer,
while the red text highlights the non-matching parts.
structions to inform the model.). If the model generates this
content, it indicates that the model has detected erroneous
information in the document.
Error correction ratemeasures whether the model can
provide the correct answer after identifying errors for coun-
terfactual robustness. The model is asked to generate the cor-
rect answer after identifying the factual errors. If the model
generates the correct answer, it indicates that the model is
capable of correcting errors in the document.
Considering that the model may not fully adhere to in-
structions, for rejection rate and error detection rate, we
also use ChatGPT to conduct additional evaluation of the
answers. Specifically, we assess the model’s responses by
using instructions and demonstrations to determine if they
can reflect information that is not present in the document or
identify any factual errors.
Experiments
In this section, we evaluate the performance of various
LLMs, analyze and discuss the results, summarizing the
main challenges that existing LLMs encounter when using
external knowledge.
Settings
Task formats.Due to contextual limitations, we provide 5
external documents for each question. In our experiments
on noise robustness, we evaluate scenarios with noise ra-
tios ranging from 0 to 0.8. To comprehensively evaluate the
overall capabilities, we have adopted a unified instruction
for each language, as shown in Figure 3. The experiments
were conducted using an NVIDIA GeForce RTX 3090.
Models We conduct evaluation on 6 state-of-the-art
large language models which can generate both En-
glish and Chinese including ChatGPT (OpenAI 2022) 3,
ChatGLM-6B (THUDM 2023a), ChatGLM2-6B (THUDM
2023b), Vicuna-7b-v1.3 (Chiang et al. 2023), Qwen-7B-
Chat (QwenLM 2023), BELLE-7B-2M (Yunjie Ji 2023).
Results on Noise Robustness
We evaluated the accuracy based on the different noise ratios
in external documents, and the results are shown in Table 1.
We can see that:
(1) RAG can effect improve the responses of LLMs.
LLMs have shown strong performance even in the presence
of noise, indicating that RAG is a promising way for LLMs
to generate accurate and reliable responses.
(2) The increasing noise rate poses a challenge for
RAG in LLMs.Specifically, when the noise ratio exceeds
80%, the accuracy decreases significantly at a significance
level of 0.05. For example, the performance of ChatGPT has
decreased from 96.33% to 76.00%, while the performance
of ChatGLM2-6B has decreased from 91.33% to 57.33%.
Error Analysis. To better comprehend the negative im-
pact of noise on model generation, we examined the incor-
rect answers and found that these errors typically originate
from three reasons, as shown in Table 2.
(1) Long-distance information.LLMs often face diffi-
culty in identifying the correct answer from external docu-
ments when the information related to the question is distant
from the information related to the answer. This scenario
is quite common as longer texts are frequently encountered
3We use gpt-3.5-turbo api in the experiments.



Source: data\tc16_2312.10997v5\referenced_papers\[167]_2309.01431.pdf (Page 5):

on the internet. In such cases, it is typical for the question’s
information to be initially presented at the start of the doc-
ument and subsequently referred to using pronouns. In Ta-
ble 2, the question information (“Qatar Open 2022”) is only
mentioned once at the beginning and is far from where the
answer text “Anett Kontaveit” appears. This situation may
cause LLMs to depend on information from other docu-
ments and create false impressions, i.e., hallucination.
(2) Evidence uncertainty. Before highly anticipated
events, like the release of new Apple products or the an-
nouncement of the Oscars, there is often a significant
amount of speculative information circulating on the inter-
net. Although the relevant documents explicitly state that
it is uncertain or speculative content, they can still impact
on the retrieval-augmented generation of LLMs. In Table 2,
when the noise ratio increases, the content of erroneous
documents is all about some people’s predictions about the
name of the headset (“Apple Reality Pro”). Even if there is
a correct answer (“Vision Pro”) in the relevant documents,
LLMs can still be misled by uncertain evidences.
(3) Concept confusion.The concepts in external docu-
ments may be similar to, but different from, the concepts in
the question. This can cause confusion for LLMs and make
LLMs generate incorrect answers. In Table 2, the model an-
swer focuses on the concept “automotive revenue” in the
document rather than “revenue” in the question.
Based on the analysis above, we have identified certain
limitations in LLMs regarding retrieval-augmented genera-
tion. To effectively handle the vast amount of noise present
on the internet, further detailed enhancements are required
for the model such as long documents modeling and precise
concept comprehension.
Results on Negative Rejection testbed
We evaluated the rejection rate when only noise documents
were provided. The results are shown in Table 3. In addi-
tion to evaluating the rejection rate through exact matching
(Rej in Table 3), we also utilize ChatGPT to determine if
the responses from the LLMs contain any rejection informa-
tion (Rej∗ in Table 3). We can see that: Negative Rejection
poses a challenge for RAG in LLMs.The highest rejection
rates for LLMs in English and Chinese were only 45% and
43.33%, respectively. This suggests that LLMs can be easily
misled by noisy documents, leading to incorrect answers.
In addition, through comparing Rej and Rej ∗, we found
that LLMs fail to strictly follow instructions, and they often
generate unpredictable responses, which make it hard to use
them as state triggers (such as for recognizing rejection).
We conduct case studies in Table 4. The first error is
because of Evidence uncertainty. Although the document
only mentions contact with “Adam McKay” and does not
explicitly state that he is the director of the movie, the
model still concludes that he holds this role. The first er-
ror is because of Concept confusion. The information pro-
vided in the answer pertains to “the 2018 Winter Olympics”
instead of “the 2022 Olympics” mentioned in the question.
Retrieval-augmented generation poses a greater challenge of
negative rejection compared to answer directly as it presents
relevant documents that could potentially mislead the LLMs
Languages English Chinese
Rej Rej ∗ Rej Rej ∗
ChatGPT 24.67 45.00 5.33 43.33
ChatGLM-6B 9.00 25.00 6.33 17.00
ChatGLM2-6B10.33 41.33 6.33 36.33
Vicuna-7B-v1.317.00 33.33 3.37 24.67
Qwen-7B-Chat31.00 35.67 8.67 25.33
BELLE-7B-2M5.67 32.33 5.33 13.67
Table 3: The result of negative rejection. Rej means the re-
jection rate (%) and Rej∗ means the rejection rate evaluated
by ChatGPT. We can see that negative rejection poses a chal-
lenge for RAG in LLMs.
Question Answer Response
who will directIrredeemable film?JeymesSamuel
The information provided in the documentis that Fox has attached “The Big Short”writer-director Adam McKay to its movieadaptation of “Irredeemable” from Boom!Comics. Therefore, the answer is Adam McKay.
Which country won themost medals at the2022 Winter Olympics?Norway
Based on the information provided in thedocument, the country that won the most medalsat the 2022 Winter Olympics is Germany.Germany has won a total of 31 medals, including 14gold medals, 14 bronze medals, and 31 silver medals.
Table 4: Error cases of negative rejection generated by
ChatGLM2-6B. The red text highlights the error answers.
and result in incorrect responses. In future developments, it
will be crucial for LLMs to enhance their ability to accu-
rately match questions with the appropriate documents.
Results on Information Integration testbed
We evaluated the accuracy based on the different noise ratios
in external documents, and the results are shown in Table 5.
When comparing the model to Table 1, we observed that
it has a weak information integration ability, which in turn
affects its noise robustness. We can see that:
(1) Information integration poses a challenge for RAG
in LLMs.Even without noise, the highest accuracy of LLMs
can only reach 60% and 67% for English and Chinese,
respectively. After adding noise, the highest accuracy de-
creases to 43% and 55%. These results suggest that LLMs
struggle with integrating information effectively and are not
well-suited for directly answering complex questions.
(2) Complex questions are more challenging for RAG
with noisy documents.Performance decline becomes sig-
nificant when the noise ratio is 0.4, but for simple problems,
a significant decline occurs only at a noise ratio of 0.8 at a
significance level of 0.05. This indicates that complex prob-
lems are more vulnerable to interference from noise. We
speculate that this is because solving complex problems re-
quires integrating information from multiple documents, and
this information can be considered as noise to each other,
making it harder for the model to extract relevant informa-
tion from the documents.
Error Analysis. We conducted an error analysis on
ChatGLM2-6B (noise ratio is 0). Apart from the similar er-
rors founded in the noise robustness experiment (38% of the
total), there are also three types of unique errors. We have
presented these cases in Table 6.



Source: data\tc16_2312.10997v5\referenced_papers\[48]_2310.01558.pdf (Page 0):

Published as a conference paper at ICLR 2024
MAKING RETRIEVAL -AUGMENTED LANGUAGE
MODELS ROBUST TO IRRELEVANT CONTEXT
Ori Yoran1 Tomer Wolfson1,2 Ori Ram1 Jonathan Berant1
1Tel Aviv University, 2Allen Institute for AI
{ori.yoran, ori.ram, joberant}@cs.tau.ac.il tomerw@allenai.org
ABSTRACT
Retrieval-augmented language models (RALMs) hold promise to produce lan-
guage understanding systems that are are factual, efficient, and up-to-date. An
important desideratum of RALMs, is that retrieved information helps model per-
formance when it is relevant, and does not harm performance when it is not. This
is particularly important in multi-hop reasoning scenarios, where misuse of irrele-
vant evidence can lead to cascading errors. However, recent work has shown that
retrieval augmentation can sometimes have a negative effect on performance. In
this work, we present a thorough analysis on five open-domain question answer-
ing benchmarks, characterizing cases when retrieval reduces accuracy. We then
propose two methods to mitigate this issue. First, a simple baseline that filters out
retrieved passages that do not entail question-answer pairs according to a natural
language inference (NLI) model. This is effective in preventing performance re-
duction, but at a cost of also discarding relevant passages. Thus, we propose a
method for automatically generating data to fine-tune the language model to prop-
erly leverage retrieved passages, including for challenging multi-hop tasks, using
a mix of relevant and irrelevant contexts at training time. We empirically show that
even 1,000 examples suffice to train the model to be robust to irrelevant contexts
while maintaining high performance on examples with relevant ones.
1 I NTRODUCTION
Large Language Models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023)
are the foundation on top of which modern language systems are built. However, open-domain
question answering (ODQA; Chen et al. 2017) and other knowledge-intensive tasks (Thorne et al.,
2018; Petroni et al., 2021) require vast amounts of up-to-date factual knowledge about rare entities
that even very large models cannot memorize (Roberts et al., 2020; Dhingra et al., 2022). A domi-
nant approach for combating this issue has been Retrieval Augmented Language Models (RALMs),
which incorporate a retrieval mechanism to reduce the need for storing information in the LLM
parameters (Guu et al., 2020; Lewis et al., 2020b; Izacard et al., 2023; Rubin & Berant, 2023). Fur-
thermore, RALMs have also been shown to improve ODQA performance in an in-context setting
(without any training), simply by prepending retrieved sentences to the input question (Ram et al.,
2023). Nevertheless, retrievers are not perfect and past work has shown that noisy retrieval can
negatively affect LLM performance (Petroni et al., 2020; Li et al., 2023). For example, in Fig. 1,
when posed with the questions “Who is playing Jason on General Hospital?” a vanilla LLM (left)
correctly answers the question while the RALM (right) is “distracted” by irrelevant context about
the actor portraying Cooper, not Jason.
In this work, we analyze and improve the robustness of RALMs to noisy retrieved contexts. Our def-
inition for retrieval-robust LLMsstates that: (a) when relevant, the retrieved context should improve
model performance; (b) when irrelevant, the retrieved context should not hurt model performance.
To this end, we present two methods for retrieval-robustness in RALMs (§2).
First, we consider a setting where we have black-box access to the LLM and cannot train it. Rather
than solely relying on in-context prompting (Brown et al., 2020), we frame retrieval robustness as
a natural language inference (NLI) problem (Dagan et al., 2006; Bowman et al., 2015). Namely,
given a question and retrieved context, an NLI model can predict whether a question-answer pair
1
arXiv:2310.01558v2  [cs.CL]  5 May 2024



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[167]_2309.01431.pdf (Page 6):

English ChineseNoise Ratio 0 0.2 0.4 0 0.2 0.4ChatGPT 55 51 34 63 58 47ChatGLM-6B45 36 35 60 53 52ChatGLM2-6B34 32 21 44 43 32Vicuna-7B-v1.360 53 43 43 36 25Qwen-7B-Chat55 50 37 67 56 55BELLE-7B-2M40 34 24 49 41 38
Table 5: The experimental result of information integration
measured by accuracy (%) under different noise ratios. We
can see that information integration poses a challenge for
RAG in LLMs.
Question Answer Response Errors
What groupings are France andGermany in World Cup 2022?Group DGroup E
France and Germany are inGroup D of the 2022 FIFAWorld Cup.
MergingError
Who were the MVP of SuperBowl 2022 and 2023?
Cooper KuppPatrickMahomes
According to the provideddocument, the MVP of SuperBowl LVI was Cooper Kuppof the Los Angeles Rams.
IgnoringError
What films won the 2022 and2023 Academy Awards forBest Picture?
CODAEverythingEverywhereAll at Once
The film CODA won theaward for Best Picture at the95th Academy Awardsceremony held on 2023.
MisalignmentError
Table 6: Error cases of information integration, the re-
sponses are generated by ChatGLM2-6B. The blue and red
texts represent the answers to two sub-questions.
(1) Merging Error (28% of the total).The model some-
times merges the answers of the two sub-questions, resulting
in an error. It mistakenly uses the answer from one question
to address both two questions. At this point, the model will
disregard any documents related to one sub-question. For
example, in Table 6, it incorrectly states that Group D is the
World Cup group for both France and Germany, while in fact
Germany is actually assigned to Group E.
(2) Ignoring Error (28% of the total).Sometimes, the
model may ignore one of the sub-questions and only answer
the other. This error occurs when the model lacks a complete
understanding of the problem and fails to recognize that it
consists of multiple sub-problems. As a result, the model
only considers relevant documents for one sub-problem in
order to generate an answer, disregarding the question posed
by another sub-problem. For example, in Table 6, the model
only provides the answer for the MVP of Super Bowl 2022
and does not consider 2023.
(3) Misalignment Error (6% of the total).Sometimes,
the model incorrectly identifies the documents for one sub-
question as the documents for another sub-question, leading
to misaligned answers. For example, in Table 6, the third an-
swer has two errors: an ignoring error and a misalignment er-
ror. Firstly, the model only mentioned the Best Picture of the
2023 (95th) Academy Awards, completely disregarding the
2022 awards. Additionally, it incorrectly stated that “CODA”
is the Best Picture of 2023 when it was actually awarded as
the Best Picture in 2022.
The errors mentioned above are primarily caused by the
limited understanding of complex questions, which hinders
the ability to effectively utilize information from different
sub-problems. The key lies in improving the model’s rea-
soning capability. One possible solution is to use a chain-of-
Acc Accdoc ED ED∗ CR
ChatGPT-zh 91 17 1 3 33.33Qwen-7B-Chat-zh77 12 5 4 25.00ChatGPT-en 89 9 8 7 57.14
Table 7: The result of counterfactual robustness. ACC is the
accuracy (%) of LLMs without external documents. ACCdoc
is the accuracy (%) of LLMs with counterfactual documents.
ED and ED ∗ are error detection rates evaluated by exact
matching and ChatGPT, respectively. CR is the error cor-
rection rate.
thought approach to break down complex problems (Zhou
et al. 2023a; Xu et al. 2023b; Drozdov et al. 2023). How-
ever, these methods slow down the inference speed and can-
not provide timely responses.
Results on Counterfactual Robustness testbed
In order to ensure that LLMs possess relevant knowledge,
we assess their performance by directly asking them ques-
tions. However, we found that most LLMs struggle to an-
swer them correctly. To ensure a more reasonable evalua-
tion, we only consider LLMs that have an accuracy rate of
over 70% as this threshold is relatively high and encom-
passes more LLMs. The results are shown in Table 7. We
present the following metrics: accuracy without any docu-
ments, accuracy with counterfactual documents, error de-
tection rates, and error correction rates. We can see that It
is hard for LLMs to identify and correct factual errors in the
documents. This suggests that the model can be easily mis-
led by documents containing incorrect facts.
It is important to note that retrieval-augmented generation
is not designed to automatically address factual errors within
a given context, as this contradicts the underlying assump-
tion that the model lacks knowledge and relies on retrieved
documents for additional information. However, this issue is
crucial in practical applications due to the abundance of fake
news on the internet. Existing LLMs do not have a safeguard
to handle inaccurate responses caused by misinformation. In
fact, they heavily depend on the information they retrieve.
Even when LLMs contain the internal knowledge about the
questions, they often trust false information that is retrieved.
This presents significant a challenge for the future develop-
ment of RAG in LLMs.
Conclusion
In this paper, we evaluated four abilities of retrieval-
augmented generation in LLMs: noise robustness, nega-
tive rejection, information integration, and counterfactual
robustness. To conduct the evaluation, we built Retrieval-
Augmented Generation Benchmark (RGB). The instances of
RGB are generated from latest news articles and the external
documents obtained from search engines. The experimental
results suggest that current LLMs have limitations in the 4
abilities. This indicates that there is still a significant amount
of work needed to effectively apply RAG to LLMs. To en-
sure accurate and reliable responses from LLMs, it is crucial
to exercise caution and carefully design for RAG.



Source: data\tc16_2312.10997v5\referenced_papers\[50]_2311.09210.pdf (Page 6):

Models Noise NQ TriviaQA WebQ Average
Ratio EM F1 EM F1 EM F1 EM F1
Retrieve-Read 100% 34.28 41.74 55.30 61.67 29.58 46.34 39.72 49.92
+ CHAIN-OF-NOTE 41.83 49.58 64.30 70.00 36.85 53.07 47.66 57.55
(+7.55) (+7.84) (+9.00) (+8.33) (+7.27) (+6.73) (+7.94) (+7.63)
Retrieve-Read 80% 54.28 61.03 73.83 80.02 35.46 52.70 54.52 64.58
+ CHAIN-OF-NOTE 56.63 63.23 75.89 81.24 40.60 56.54 57.70 67.00
(+2.35) (+2.20) (+2.06) (+1.22) (+5.14) (+3.84) (+3.18) (+2.42)
Retrieve-Read 60% 61.44 67.94 78.44 83.65 37.01 54.16 58.96 68.58
+ CHAIN-OF-NOTE 63.43 69.33 78.79 84.07 41.26 56.91 61.16 70.10
(+1.99) (+1.39) (+0.35) (+0.42) (+4.25) (+2.75) (+2.20) (+1.52)
Retrieve-Read 40% 64.62 71.12 80.56 86.76 38.40 55.60 61.19 71.16
+ CHAIN-OF-NOTE 65.91 72.22 81.72 87.11 42.16 58.15 63.26 72.49
(+1.29) (+1.10) (+1.16) (+0.35) (+3.76) (+2.55) (+2.07) (+1.33)
Retrieve-Read 20% 67.21 73.69 81.73 87.89 39.95 56.66 62.96 72.75
+ CHAIN-OF-NOTE 70.00 76.08 82.86 88.24 44.36 60.13 65.74 74.82
(+2.79) (+2.39) (+1.13) (+0.35) (+4.41) (+3.47) (+2.78) (+2.07)
Retrieve-Read 0% 69.23 75.57 83.34 89.44 42.24 58.59 64.93 74.53
+ CHAIN-OF-NOTE 73.28 79.86 83.52 88.94 46.16 62.38 67.65 77.06
(+4.05) (+4.29) (+0.18) (-0.50) (+3.92) (+3.79) (+2.72) (+2.53)
Table 3: Evaluation on Noise Robustness. The backbone language model is LLaMa-2 7B. The CHAIN -OF-NOTE
framework shows superior performance compared to the standard RALM system, particularly notable at higher
noise ratios.We explain how we synthesize data with different noise ratios under real-world scenarios in§ 3.1.1.
Models↓ RealTimeQA
EM F1 RR
Retrieve-Read (Shi et al., 2023c)15.6 19.9 6.1
+ CHAIN-OF-NOTE (ours) 15.7 20.3 13.0
Table 4: Evaluation on Unknown Robustness. TheCON
shows better performance than standard RALM system.
3.3 Evaluation on Noise Robustness
As illustrated in Table 2, when faced with entirely
noisy documents, both the standard RALM and
our CHAIN -OF-NOTE enhanced RALM underper-
formed compared to the no-retrieval setting. This
suggests that RALMs can be misled by noisy infor-
mation, leading to more hallucinations.
Notably, equipping the model withCON enables
it to perform nearly as well as the baseline model
directly fine-tuned with QA pairs without retrieval,
showcasing its robustness to noise and its ability
to disregard irrelevant information. The CON ap-
proach is effective not only in fine-tuned, smaller-
sized models but also in large language models,
such as GPT-4, with adjustments made only to the
prompt. Besides, in comparison to the CHAIN -OF-
THOUGHT technique, commonly utilized in rea-
soning scenarios, CON presents a more efficient
strategy for retrieval-augmented settings, particu-
larly in addressing knowledge-intensive tasks.
Table 3 shows that RALM enhanced with CON
consistently outperforms the standard RALM, es-
pecially in scenarios with exclusively noisy doc-
uments. An average improvement of +7.9 in EM
score on fully noisy documents is observed on three
open-domain QA datasets, in average. Experiments
with lower noise ratios also consistently demon-
strate the improvements brought by CON, aligning
the overall QA performance.
3.4 Evaluation on Unknown Robustness
Table 4 illustrates that our RALM equipped with
CON exhibits superior robustness in handling un-
known scenario, particularly evident in the Real-
TimeQA benchmark. This benchmark falls com-
pletely outside the model’s domain and contains
real-time information that was not part of the
LLaMa-2 pre-training data. Despite this, models
are still capable of providing correct answers in
some cases, as the answers remain consistent over
time. In comparison to the standard RALM sys-
tem, our method shows a significant improvement,
exceeding +10.5 in its ability to reject to answer
questions in unknown scenario. The evaluation is
based on reject rate (RR), i.e., number of rejected



Source: data\tc16_2312.10997v5\referenced_papers\[167]_2309.01431.pdf (Page 4):

English Chinese
Noise Ratio 0 0.2 0.4 0.6 0.8 0 0.2 0.4 0.6 0.8
ChatGPT (OpenAI 2022) 96.33 94.67 94.00 90.00 76.00 95.67 94.67 91.00 87.67 70.67
ChatGLM-6B (THUDM 2023a) 93.67 90.67 89.33 84.67 70.67 94.33 90.67 89.00 82.33 69.00
ChatGLM2-6B (THUDM 2023b)91.33 89.67 83.00 77.33 57.33 86.67 82.33 76.67 72.33 54.00
Vicuna-7B-v1.3 (Chiang et al. 2023)87.67 83.33 86.00 82.33 60.33 85.67 82.67 77.00 69.33 49.67
Qwen-7B-Chat (QwenLM 2023)94.33 91.67 91.00 87.67 73.67 94.00 92.33 88.00 84.33 68.67
BELLE-7B-2M (Yunjie Ji 2023)83.33 81.00 79.00 71.33 64.67 92.00 88.67 85.33 78.33 67.68
Table 1: The experimental result of noise robustness measured by accuracy (%) under different noise ratios. We can see that the
increasing noise rate poses a challenge for RAG in LLMs.
Long-distance information. Evidence uncertainty. Concept confusion.
QuestionWho did Iga Swiatek defeat to win the Qatar Open 2022?What is the name of Apple’s headset?What was Tesla’s revenue in Q1 2022?
AnswerAnett Kontaveit Vision Pro 18.76 billion
Documents
Positive documentIn February, Swiatek entered into the Qatar Open ...In the final, she won ...Anett Kontaveit...
Negative documentThis time, she defeated Ons Jabeur 6-2, 7-6(5) to winthe 2022 US Open, ...
Positive documentApple (AAPL.O) on Monday unveiled a costlyaugmented-reality headset called theVision Pro...
Negative document... is what Gurman believes will be calledApple Reality Pro. ...
Positive documentTesla, Inc. (TSLA) reported Q1 FY 2022 earnings results... detailed revenues of $18.76 billion...
Negative document...first-quarter earnings for 2022 ......Automotive revenue reached $16.86 billion...
ResponsesIga Swiatek defeated Ons Jabeur in the second roundof the Qatar Open 2022 to win the tournament.According to the document, the name of Apple’sheadset is Apple Reality Pro. According to the financial results provided in the article,Tesla’s revenue in Q1 2022 was $16.86 billion.
Table 2: Error cases of noise robustness, and only one positive document and one negative document are shown. The responses
are generated by ChatGLM2-6B. The blue text indicates the matching parts between the document and the question or answer,
while the red text highlights the non-matching parts.
structions to inform the model.). If the model generates this
content, it indicates that the model has detected erroneous
information in the document.
Error correction ratemeasures whether the model can
provide the correct answer after identifying errors for coun-
terfactual robustness. The model is asked to generate the cor-
rect answer after identifying the factual errors. If the model
generates the correct answer, it indicates that the model is
capable of correcting errors in the document.
Considering that the model may not fully adhere to in-
structions, for rejection rate and error detection rate, we
also use ChatGPT to conduct additional evaluation of the
answers. Specifically, we assess the model’s responses by
using instructions and demonstrations to determine if they
can reflect information that is not present in the document or
identify any factual errors.
Experiments
In this section, we evaluate the performance of various
LLMs, analyze and discuss the results, summarizing the
main challenges that existing LLMs encounter when using
external knowledge.
Settings
Task formats.Due to contextual limitations, we provide 5
external documents for each question. In our experiments
on noise robustness, we evaluate scenarios with noise ra-
tios ranging from 0 to 0.8. To comprehensively evaluate the
overall capabilities, we have adopted a unified instruction
for each language, as shown in Figure 3. The experiments
were conducted using an NVIDIA GeForce RTX 3090.
Models We conduct evaluation on 6 state-of-the-art
large language models which can generate both En-
glish and Chinese including ChatGPT (OpenAI 2022) 3,
ChatGLM-6B (THUDM 2023a), ChatGLM2-6B (THUDM
2023b), Vicuna-7b-v1.3 (Chiang et al. 2023), Qwen-7B-
Chat (QwenLM 2023), BELLE-7B-2M (Yunjie Ji 2023).
Results on Noise Robustness
We evaluated the accuracy based on the different noise ratios
in external documents, and the results are shown in Table 1.
We can see that:
(1) RAG can effect improve the responses of LLMs.
LLMs have shown strong performance even in the presence
of noise, indicating that RAG is a promising way for LLMs
to generate accurate and reliable responses.
(2) The increasing noise rate poses a challenge for
RAG in LLMs.Specifically, when the noise ratio exceeds
80%, the accuracy decreases significantly at a significance
level of 0.05. For example, the performance of ChatGPT has
decreased from 96.33% to 76.00%, while the performance
of ChatGLM2-6B has decreased from 91.33% to 57.33%.
Error Analysis. To better comprehend the negative im-
pact of noise on model generation, we examined the incor-
rect answers and found that these errors typically originate
from three reasons, as shown in Table 2.
(1) Long-distance information.LLMs often face diffi-
culty in identifying the correct answer from external docu-
ments when the information related to the question is distant
from the information related to the answer. This scenario
is quite common as longer texts are frequently encountered
3We use gpt-3.5-turbo api in the experiments.



Source: data\tc16_2312.10997v5\referenced_papers\[167]_2309.01431.pdf (Page 5):

on the internet. In such cases, it is typical for the question’s
information to be initially presented at the start of the doc-
ument and subsequently referred to using pronouns. In Ta-
ble 2, the question information (“Qatar Open 2022”) is only
mentioned once at the beginning and is far from where the
answer text “Anett Kontaveit” appears. This situation may
cause LLMs to depend on information from other docu-
ments and create false impressions, i.e., hallucination.
(2) Evidence uncertainty. Before highly anticipated
events, like the release of new Apple products or the an-
nouncement of the Oscars, there is often a significant
amount of speculative information circulating on the inter-
net. Although the relevant documents explicitly state that
it is uncertain or speculative content, they can still impact
on the retrieval-augmented generation of LLMs. In Table 2,
when the noise ratio increases, the content of erroneous
documents is all about some people’s predictions about the
name of the headset (“Apple Reality Pro”). Even if there is
a correct answer (“Vision Pro”) in the relevant documents,
LLMs can still be misled by uncertain evidences.
(3) Concept confusion.The concepts in external docu-
ments may be similar to, but different from, the concepts in
the question. This can cause confusion for LLMs and make
LLMs generate incorrect answers. In Table 2, the model an-
swer focuses on the concept “automotive revenue” in the
document rather than “revenue” in the question.
Based on the analysis above, we have identified certain
limitations in LLMs regarding retrieval-augmented genera-
tion. To effectively handle the vast amount of noise present
on the internet, further detailed enhancements are required
for the model such as long documents modeling and precise
concept comprehension.
Results on Negative Rejection testbed
We evaluated the rejection rate when only noise documents
were provided. The results are shown in Table 3. In addi-
tion to evaluating the rejection rate through exact matching
(Rej in Table 3), we also utilize ChatGPT to determine if
the responses from the LLMs contain any rejection informa-
tion (Rej∗ in Table 3). We can see that: Negative Rejection
poses a challenge for RAG in LLMs.The highest rejection
rates for LLMs in English and Chinese were only 45% and
43.33%, respectively. This suggests that LLMs can be easily
misled by noisy documents, leading to incorrect answers.
In addition, through comparing Rej and Rej ∗, we found
that LLMs fail to strictly follow instructions, and they often
generate unpredictable responses, which make it hard to use
them as state triggers (such as for recognizing rejection).
We conduct case studies in Table 4. The first error is
because of Evidence uncertainty. Although the document
only mentions contact with “Adam McKay” and does not
explicitly state that he is the director of the movie, the
model still concludes that he holds this role. The first er-
ror is because of Concept confusion. The information pro-
vided in the answer pertains to “the 2018 Winter Olympics”
instead of “the 2022 Olympics” mentioned in the question.
Retrieval-augmented generation poses a greater challenge of
negative rejection compared to answer directly as it presents
relevant documents that could potentially mislead the LLMs
Languages English Chinese
Rej Rej ∗ Rej Rej ∗
ChatGPT 24.67 45.00 5.33 43.33
ChatGLM-6B 9.00 25.00 6.33 17.00
ChatGLM2-6B10.33 41.33 6.33 36.33
Vicuna-7B-v1.317.00 33.33 3.37 24.67
Qwen-7B-Chat31.00 35.67 8.67 25.33
BELLE-7B-2M5.67 32.33 5.33 13.67
Table 3: The result of negative rejection. Rej means the re-
jection rate (%) and Rej∗ means the rejection rate evaluated
by ChatGPT. We can see that negative rejection poses a chal-
lenge for RAG in LLMs.
Question Answer Response
who will directIrredeemable film?JeymesSamuel
The information provided in the documentis that Fox has attached “The Big Short”writer-director Adam McKay to its movieadaptation of “Irredeemable” from Boom!Comics. Therefore, the answer is Adam McKay.
Which country won themost medals at the2022 Winter Olympics?Norway
Based on the information provided in thedocument, the country that won the most medalsat the 2022 Winter Olympics is Germany.Germany has won a total of 31 medals, including 14gold medals, 14 bronze medals, and 31 silver medals.
Table 4: Error cases of negative rejection generated by
ChatGLM2-6B. The red text highlights the error answers.
and result in incorrect responses. In future developments, it
will be crucial for LLMs to enhance their ability to accu-
rately match questions with the appropriate documents.
Results on Information Integration testbed
We evaluated the accuracy based on the different noise ratios
in external documents, and the results are shown in Table 5.
When comparing the model to Table 1, we observed that
it has a weak information integration ability, which in turn
affects its noise robustness. We can see that:
(1) Information integration poses a challenge for RAG
in LLMs.Even without noise, the highest accuracy of LLMs
can only reach 60% and 67% for English and Chinese,
respectively. After adding noise, the highest accuracy de-
creases to 43% and 55%. These results suggest that LLMs
struggle with integrating information effectively and are not
well-suited for directly answering complex questions.
(2) Complex questions are more challenging for RAG
with noisy documents.Performance decline becomes sig-
nificant when the noise ratio is 0.4, but for simple problems,
a significant decline occurs only at a noise ratio of 0.8 at a
significance level of 0.05. This indicates that complex prob-
lems are more vulnerable to interference from noise. We
speculate that this is because solving complex problems re-
quires integrating information from multiple documents, and
this information can be considered as noise to each other,
making it harder for the model to extract relevant informa-
tion from the documents.
Error Analysis. We conducted an error analysis on
ChatGLM2-6B (noise ratio is 0). Apart from the similar er-
rors founded in the noise robustness experiment (38% of the
total), there are also three types of unique errors. We have
presented these cases in Table 6.



Source: data\tc16_2312.10997v5\referenced_papers\[48]_2310.01558.pdf (Page 0):

Published as a conference paper at ICLR 2024
MAKING RETRIEVAL -AUGMENTED LANGUAGE
MODELS ROBUST TO IRRELEVANT CONTEXT
Ori Yoran1 Tomer Wolfson1,2 Ori Ram1 Jonathan Berant1
1Tel Aviv University, 2Allen Institute for AI
{ori.yoran, ori.ram, joberant}@cs.tau.ac.il tomerw@allenai.org
ABSTRACT
Retrieval-augmented language models (RALMs) hold promise to produce lan-
guage understanding systems that are are factual, efficient, and up-to-date. An
important desideratum of RALMs, is that retrieved information helps model per-
formance when it is relevant, and does not harm performance when it is not. This
is particularly important in multi-hop reasoning scenarios, where misuse of irrele-
vant evidence can lead to cascading errors. However, recent work has shown that
retrieval augmentation can sometimes have a negative effect on performance. In
this work, we present a thorough analysis on five open-domain question answer-
ing benchmarks, characterizing cases when retrieval reduces accuracy. We then
propose two methods to mitigate this issue. First, a simple baseline that filters out
retrieved passages that do not entail question-answer pairs according to a natural
language inference (NLI) model. This is effective in preventing performance re-
duction, but at a cost of also discarding relevant passages. Thus, we propose a
method for automatically generating data to fine-tune the language model to prop-
erly leverage retrieved passages, including for challenging multi-hop tasks, using
a mix of relevant and irrelevant contexts at training time. We empirically show that
even 1,000 examples suffice to train the model to be robust to irrelevant contexts
while maintaining high performance on examples with relevant ones.
1 I NTRODUCTION
Large Language Models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023)
are the foundation on top of which modern language systems are built. However, open-domain
question answering (ODQA; Chen et al. 2017) and other knowledge-intensive tasks (Thorne et al.,
2018; Petroni et al., 2021) require vast amounts of up-to-date factual knowledge about rare entities
that even very large models cannot memorize (Roberts et al., 2020; Dhingra et al., 2022). A domi-
nant approach for combating this issue has been Retrieval Augmented Language Models (RALMs),
which incorporate a retrieval mechanism to reduce the need for storing information in the LLM
parameters (Guu et al., 2020; Lewis et al., 2020b; Izacard et al., 2023; Rubin & Berant, 2023). Fur-
thermore, RALMs have also been shown to improve ODQA performance in an in-context setting
(without any training), simply by prepending retrieved sentences to the input question (Ram et al.,
2023). Nevertheless, retrievers are not perfect and past work has shown that noisy retrieval can
negatively affect LLM performance (Petroni et al., 2020; Li et al., 2023). For example, in Fig. 1,
when posed with the questions “Who is playing Jason on General Hospital?” a vanilla LLM (left)
correctly answers the question while the RALM (right) is “distracted” by irrelevant context about
the actor portraying Cooper, not Jason.
In this work, we analyze and improve the robustness of RALMs to noisy retrieved contexts. Our def-
inition for retrieval-robust LLMsstates that: (a) when relevant, the retrieved context should improve
model performance; (b) when irrelevant, the retrieved context should not hurt model performance.
To this end, we present two methods for retrieval-robustness in RALMs (§2).
First, we consider a setting where we have black-box access to the LLM and cannot train it. Rather
than solely relying on in-context prompting (Brown et al., 2020), we frame retrieval robustness as
a natural language inference (NLI) problem (Dagan et al., 2006; Bowman et al., 2015). Namely,
given a question and retrieved context, an NLI model can predict whether a question-answer pair
1
arXiv:2310.01558v2  [cs.CL]  5 May 2024



### Claim 74/179

#### Claim Text
Prominent benchmarks such as RGB, RECALL and CRUD [167]–[169] focus on appraising the essential abilities of RAG models.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 0):

CRUD-RAG: A Comprehensive Chinese Benchmark for
Retrieval-Augmented Generation of Large Language Models
YUANJIE LYU∗, University of Science and Technology of China, China
ZHIYU LI∗, Institute for Advanced Algorithms Research (Shanghai), China
SIMIN NIU, Renmin University of China, China
FEIYU XIONG and BO TANG, Institute for Advanced Algorithms Research (Shanghai), China
WENJIN WANG and HAO WU,Institute for Advanced Algorithms Research (Shanghai), China
HUANYONG LIU, 360 AI Research Institute, China
TONG XU†, University of Science and Technology of China, China
ENHONG CHEN, University of Science and Technology of China, China
Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models
(LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations,
including outdated information and the tendency to produce inaccurate "hallucinated" content. However,
evaluating RAG systems is a challenge. Most benchmarks focus primarily on question answering applications,
neglecting other potential scenarios where RAG could be beneficial. Accordingly, in the experiments, these
benchmarks often assess only the LLM components of the RAG pipeline or the retriever in knowledge-intensive
scenarios, overlooking the impact of external knowledge base construction and the retrieval component on
the entire RAG pipeline in non-knowledge-intensive scenarios. To address these issues, this paper constructs
a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems in
various RAG application scenarios. Specifically, we refer to the CRUD actions that describe interactions
between users and knowledge bases, and also categorize the range of RAG applications into four distinct
types—Create, Read, Update, and Delete (CRUD). "Create" refers to scenarios requiring the generation of
original, varied content. "Read" involves responding to intricate questions in knowledge-intensive situations.
"Update" focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. "Delete"
pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories,
we have developed different datasets to evaluate the performance of RAG systems. We also analyze the effects
of various components of the RAG system, such as the retriever, context length, knowledge base construction,
and LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios1.
CCS Concepts: • Computing methodologies →Natural language generation ; • Information systems
→Information retrieval.
∗Both authors contributed equally to this research.
†Corresponding author.
1The source code is available at GitHub: https://github.com/IAAR-Shanghai/CRUD_RAG
Authors’ addresses: Yuanjie Lyu, University of Science and Technology of China, Hefei, China, s1583050085@gmail.com;
Zhiyu Li, Institute for Advanced Algorithms Research (Shanghai), China, lizy@iaar.ac.cn; Simin Niu, niusimin@ruc.edu.cn,
Renmin University of China, Beijing, China; Feiyu Xiong, xiongfy@iaar.ac.cn; Bo Tang, tangb@iaar.ac.cn, Institute for
Advanced Algorithms Research (Shanghai), China; Wenjin Wang, wangwj@iaar.ac.cn; Hao Wu, wuh@iaar.ac.cn, Institute
for Advanced Algorithms Research (Shanghai), China; Huanyong Liu, liuhuanyong@360.cn, 360 AI Research Institute,
Beijing, China; Tong Xu, University of Science and Technology of China, Hefei, China, tongxu@ustc.edu.cn; Enhong Chen,
University of Science and Technology of China, Hefei, China, cheneh@ustc.edu.cn.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the
full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM 0004-5411/2018/8-ART111
https://doi.org/XXXXXXX.XXXXXXX
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
arXiv:2401.17043v3  [cs.CL]  15 Jul 2024



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 6):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:7
2.3 Citation-Enhanced RAG
In traditional RAG methods, despite the rich information sources provided by retrieved contexts for
text generation, these models often do not explicitly require responses to provide corresponding
citations, making traceability difficult. Therefore, enhancing text verifiability by introducing citation
links, i.e., explicit references, has become an important research direction in the RAG field [17, 54].
Providing citation indicators in the response text offers several clear benefits. First, users can
easily verify the claims made by LLMs based on the provided citations, thus improving the trans-
parency and credibility of the text. Second, if the text generated by LLMs adheres faithfully to the
cited contexts, it can significantly improve its accuracy and reduce the phenomenon of "halluci-
nations" [17]. Given this, generating high-quality citations and evaluating the quality of citation
generation have become crucial elements of assessing RAG performance. Constructing appropriate
prompts directly through the retrieval context to guide the model in generating corresponding
citations constitutes a direct and effective method of citation generation [22].
In terms of evaluation, early research primarily focused on the fluency, accuracy, and basic
citation quality of the text generated by LLMs [17, 29]. For example, Rashkin et al. proposed the
"Attributable to Identified Sources" (AIS) score [42], which serves as a valuable tool for measuring the
degree to which generated text is faithful to its sources. As research progressed, scholars recognized
the need for more detailed evaluation methods to differentiate between various levels of citation
support. By creating specialized datasets such as SCIFI [7], researchers can more precisely evaluate
fine-grained citations at the clause level in texts generated by LLMs. The ALiiCE framework [54], by
analyzing the atomic structure of sentence claims, introduced fine-grained evaluation metrics, such
as location citation recall and precision, and the coefficient of variation of citation locations, to more
granularly evaluate the quality of citation generation in RAG [54]. In practical applications, [61]
found that RAG requires more complex evaluation frameworks to distinguish between various
levels of citation support by comparing different fidelity metrics. These RAG evaluation methods
not only consider the presence of citations but also their accuracy and relevance.
While Citation-Enhanced RAG delves deeply into the specific domain of citation generation,
aiming to improve the credibility and accuracy of text generated by RAG systems, our benchmark
provides a comprehensive evaluation framework encompassing various aspects of RAG systems
and multiple application scenarios.
3 CRUD-RAG: A COMPREHENSIVE CHINESE BENCHMARK FOR RAG
As we discussed earlier, implementing RAG effectively requires careful tuning of multiple com-
ponents, such as the retrieval model, the knowledge corpus, the language model, and the query
formulation. Therefore, we need a framework that can evaluate the RAG system automatically. This
framework would enable us to examine how these components affect the system’s performance,
and provide us with useful insights for improving and innovating the system.
However, The current RAG benchmarks have several drawbacks: they only evaluate question
answering tasks [1, 41, 57], ignoring other diverse application of RAG. The optimization strategy
for question-answering tasks may not suit other tasks; And in the evaluation experiment, current
RAG benchmarks only account for the LLM component in the RAG pipeline, or the retriever in the
knowledge-intensive scenario, disregarding the vital roles of retrieval database construction and
retrieval in non-knowledge-intensive scenarios.
To address the shortcomings of previous benchmarks, we introduce CRUD-RAG, a comprehensive
Chinese benchmark for RAG. Figure 2 illustrates the features of our CRUD-RAG benchmark. It
classifies the RAG application scenarios into four categories: Create, Read, Update, and Delete,
then we construct appropriate evaluation tasks and datasets for each category. Besides, in the
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 2):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:3
Fig. 1. We have classified the application scenarios of RAG into four primary aspects: Create, Read, Update,
and Delete. The figure provides an illustrative example for each category, showcasing the wide-ranging
potential of RAG technology.
•In "READ", the system uses external knowledge retrieval to answer questions, solve problems
in question-answering, dialogue, and reasoning, and increase understanding of the input text.
•In "UPDATE", the system fixes errors in the input text using retrieved content, correcting
spelling, grammar, or factual errors to make the text better.
•In "DELETE", the system simplifies the input by improving retrieval results, removing
unnecessary details, and doing tasks like text summarization or simplification.
To evaluate the RAG system in these four scenarios, we introduce CRUD-RAG, a comprehensive,
large-scale Chinese RAG benchmark. CRUD-RAG consists of four evaluation tasks: text continu-
ation, question answering (with single-document and multi-document questions), hallucination
modification, and open-domain multi-document summarization, which respectively correspond to
the CRUD-RAG classification of RAG application scenarios. We construct CRUD-RAG by crawling
the latest high-quality news data from major news websites in China, which aims to minimize the
likelihood of LLMs encountering these data during training. Then, we automatically create datasets
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 1):

111:2 Lyu, et al.
Additional Key Words and Phrases: Retrieval-Augmented Generation, Large Language Models, Evaluation
ACM Reference Format:
Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong
Xu, and Enhong Chen. 2018. CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented
Generation of Large Language Models. J. ACM 37, 4, Article 111 (August 2018), 31 pages. https://doi.org/
XXXXXXX.XXXXXXX
1 INTRODUCTION
Retrieval-augmented generation (RAG) is an advanced technique that leverages external knowledge
sources to enhance the text generation capabilities of large language models(LLMs). It retrieves
relevant paragraphs from a corpus based on the input, and feeds them to the LLMs along with the
input. With the help of external knowledge, LLMs can generate more accurate and credible responses
and effectively address challenges such as outdated knowledge [19], hallucinations [3, 9, 35, 62],
and lack of domain expertise [30, 46]. Therefore, RAG technology is attracting increasing attention.
Although the effectiveness of retrieval-augmented strategies has been proven through extensive
practice, their implementation still requires a significant amount of tuning. The overall performance
of the RAG system is affected by multiple factors, such as the retrieval model, construction of the
external knowledge base, and language model. Therefore, automatic evaluation of RAG systems
is crucial. Currently, there are only a few existing benchmarks for evaluating RAG performance,
as creating high-quality datasets and experimenting with them entail significant costs. These
benchmarks can be classified into two types: reference-required and reference-free evaluation.
Reference-free evaluation frameworks, such as RAGAS [13] and ARES [44], use LLM-generated
data to evaluate RAG systems on contextual relevance, faithfulness, and informativeness. These
frameworks do not depend on ground truth references, but only assess the coherence of the
generated text with the retrieved context. This approach may be unreliable if the retrieved external
information is low-quality.
Consequently, reference-required evaluations remain the predominant method for assessing RAG
systems. Existing benchmarks for reference-required evaluations, such as RGB [8] and NQ [26].
do have their limitations. First, they all rely on question answering tasks to measure the perfor-
mance of RAG systems. Question answering is not the only RAG application scenario, and an
optimization strategy that works well for question answering may not be generalized to other
scenarios. Thus, these benchmarks may not capture the full potential of RAG systems. Second, in
the experiments, current evaluations usually focus on evaluating the LLM part of the RAG pipeline,
or focus on retriever performance in the knowledge-intensive scenario [40], while ignoring the
retrieval methods in non-knowledge-intensive scenarios and external knowledge base construction.
These components are also crucial for RAG systems. Therefore, a comprehensive evaluation of the
RAG system may not be obtained using any existing benchmarks.
To evaluate the performance of RAG in different application scenarios, we need a comprehensive
benchmark that covers more than just the question-answering task. Lewis et al. [28] argue that the
core of RAG systems is their interactive way of combining LLMs with external knowledge sources.
And following [25], we can group any interaction with external knowledge sources into four basic
actions: create, read, update, and delete, which are also known as CRUD actions [48]. Therefore,
we can use the CRUD framework to classify the RAG systems’ application scenarios. As shown in
Figure 1, each CRUD category demonstrates different capabilities of the RAG system:
•In "CREATE", the system improves the input text by adding relevant information from
external sources, making creative outputs such as poetry, stories, or code.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 7):

111:8 Lyu, et al.
RAG-based System
Search
Database
Context
User Query
Recall and Ranking
Pre-Rank Filter Rerank
Vector Database Graph Database Text Database
Base LLM
No-RAG
Retrieval-Augmented Generation （RAG）
Embedding Model Retrieval StrategyChunk Size Top-k Overlapping
1 2 3 4 5
6
7
Task8
Evaluation PointsText
Continuation
Question
Answering
Hallucination
Modiﬁcation
Multi-Doc
Summarization
Create
Read
Update
Delete
C R U D
Fig. 2. Illustration of CRUD-RAG, our comprehensive Chinese benchmark for RAG. It classifies the RAG
application scenarios into four categories: create, read, update, and delete. For each category, we create
appropriate evaluation tasks and datasets. In the experiments, we evaluate various components of the RAG
system using our benchmarks.
experiments, we will assess the impact of various components of RAG, such as chunk size, retrieval
strategy, top-k, LLM, etc., on all tasks.
In the following section, we will describe the evaluation tasks and the datasets that we design for
each RAG application scenario type. We select text continuation, question answering (single and
multi-document), hallucination modification, and multi-document summarization as representative
tasks in the CRUD (Create, Read, Update, Delete) scenario and construct corresponding datasets.
The summarization (D) and continuation (C) datasets were constructed simultaneously, since the
construction of both datasets requires the use of a search engine. They will be discussed together
in the following section. The construction of the question-answering(R) and hallucination modifi-
cation(D) datasets is relatively independent. To maintain narrative coherence, we will introduce
the dataset construction process in the order of DCRU. Table 2 presents the size and composition
of our datasets, and Figure 3 illustrates an example of our datasets.
3.1 News Collection
As mentioned above, the existing benchmarks for evaluating RAG systems are mainly constructed
for question answering tasks. Therefore, the datasets, such as NQ [26] and RGB [8], are also tailored
for this type of task. Hence, we need to construct new datasets.
We argue that the latest news data is the most suitable choice for creating an RAG evaluation
dataset. Unlike other types of data, such as encyclopedias, questions, or conversations, the latest
news minimizes the possibility that the model has been exposed to similar content during training.
This dependency on external retrieval mechanisms allows for a comprehensive evaluation of the
entire RAG process, not just the model’s generation ability. Additionally, news data is easy to
collect, enabling us to maintain dataset timeliness. When the existing dataset loses its timeliness,
we can quickly gather the latest news to rebuild a more challenging dataset. Moreover, the latest
news data offer rich and diverse topics and content, which can test the model’s performance and
adaptability in various domains and situations.
Therefore, we select news as the base of our datasets. To ensure the authenticity and currency
of the datasets, we collected nearly 300,000 of historical news articles from major Chinese news
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 5):

111:6 Lyu, et al.
2.2 RAG Benchmarks
When investigating the development and optimization of RAG, the effective evaluation of their
performance becomes a fundamental concern. Table 1 shows some commonly used benchmarks
for evaluating RAG. LangChain provides benchmark tasks, such as LangChain Docs Q&A and
Semi-structured Reports [27], designed to assess various RAG architectures. These datasets are
constructed from snapshots of Python documentation and PDFs containing tables and charts.
They emphasize the model’s capability to handle structured and semi-structured data. Evaluation
standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing
large models for question-answering generation has emerged as a prevalent approach in building
evaluation datasets. For instance, RGB [8] creates its evaluation dataset by gathering recent news
reports and employing LLM to generate relevant events, questions, and answers. Conversely,
ARES [44]. relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
These methods not only showcase the RAG system’s proficiency in handling real-time data but also
illustrate the utility of automation and synthetic data in the evaluation process. For evaluating the
capabilities of models across various professional domains, the Instruct-Benchmark-Tester dataset
encompasses a range of question types, with a particular focus on financial services, legal, and
intricate business scenarios [39].
Depending on whether the evaluation phase incorporates ground truth, metrics of existing
evaluation methods can be categorized into those necessitating reference and those not requiring
it. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by
contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-
Benchmark-Tester [39] employs accuracy score as an evaluation metric, a widely acknowledged
measure of model performance that assesses the extent to which model-generated answers align
with reference answers. The primary objective of RGB [8] is to evaluate whether large models can
effectively utilize external documents to acquire knowledge and generate accurate answers. Its
evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.
Reference-free evaluation methods, including TruLens-Eval [14], RAGAS [13], and ARES [44],
provide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning
context relevance, answer faithfulness, and answer relevance. TruLens-Eval [14] introduces the
RAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,
encompassing context relevance, groundedness, and answer relevance. RAGAS [13], serving as
a reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity
to identify pertinent and concentrated context passages, along with the LLMs’ proficiency in
faithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a
predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each
aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy
when compared to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-
powered inference to offer statistical assurances for its scoring, generating confidence intervals.
ARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer
relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts
and producing both faithful and relevant answers. Regarding evaluation methods, [32] places an
emphasis on assessing the credibility and accuracy of responses generated by generative search
engines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including
high costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,
exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely
approximates manual evaluation outcomes.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 6):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:7
2.3 Citation-Enhanced RAG
In traditional RAG methods, despite the rich information sources provided by retrieved contexts for
text generation, these models often do not explicitly require responses to provide corresponding
citations, making traceability difficult. Therefore, enhancing text verifiability by introducing citation
links, i.e., explicit references, has become an important research direction in the RAG field [17, 54].
Providing citation indicators in the response text offers several clear benefits. First, users can
easily verify the claims made by LLMs based on the provided citations, thus improving the trans-
parency and credibility of the text. Second, if the text generated by LLMs adheres faithfully to the
cited contexts, it can significantly improve its accuracy and reduce the phenomenon of "halluci-
nations" [17]. Given this, generating high-quality citations and evaluating the quality of citation
generation have become crucial elements of assessing RAG performance. Constructing appropriate
prompts directly through the retrieval context to guide the model in generating corresponding
citations constitutes a direct and effective method of citation generation [22].
In terms of evaluation, early research primarily focused on the fluency, accuracy, and basic
citation quality of the text generated by LLMs [17, 29]. For example, Rashkin et al. proposed the
"Attributable to Identified Sources" (AIS) score [42], which serves as a valuable tool for measuring the
degree to which generated text is faithful to its sources. As research progressed, scholars recognized
the need for more detailed evaluation methods to differentiate between various levels of citation
support. By creating specialized datasets such as SCIFI [7], researchers can more precisely evaluate
fine-grained citations at the clause level in texts generated by LLMs. The ALiiCE framework [54], by
analyzing the atomic structure of sentence claims, introduced fine-grained evaluation metrics, such
as location citation recall and precision, and the coefficient of variation of citation locations, to more
granularly evaluate the quality of citation generation in RAG [54]. In practical applications, [61]
found that RAG requires more complex evaluation frameworks to distinguish between various
levels of citation support by comparing different fidelity metrics. These RAG evaluation methods
not only consider the presence of citations but also their accuracy and relevance.
While Citation-Enhanced RAG delves deeply into the specific domain of citation generation,
aiming to improve the credibility and accuracy of text generated by RAG systems, our benchmark
provides a comprehensive evaluation framework encompassing various aspects of RAG systems
and multiple application scenarios.
3 CRUD-RAG: A COMPREHENSIVE CHINESE BENCHMARK FOR RAG
As we discussed earlier, implementing RAG effectively requires careful tuning of multiple com-
ponents, such as the retrieval model, the knowledge corpus, the language model, and the query
formulation. Therefore, we need a framework that can evaluate the RAG system automatically. This
framework would enable us to examine how these components affect the system’s performance,
and provide us with useful insights for improving and innovating the system.
However, The current RAG benchmarks have several drawbacks: they only evaluate question
answering tasks [1, 41, 57], ignoring other diverse application of RAG. The optimization strategy
for question-answering tasks may not suit other tasks; And in the evaluation experiment, current
RAG benchmarks only account for the LLM component in the RAG pipeline, or the retriever in the
knowledge-intensive scenario, disregarding the vital roles of retrieval database construction and
retrieval in non-knowledge-intensive scenarios.
To address the shortcomings of previous benchmarks, we introduce CRUD-RAG, a comprehensive
Chinese benchmark for RAG. Figure 2 illustrates the features of our CRUD-RAG benchmark. It
classifies the RAG application scenarios into four categories: Create, Read, Update, and Delete,
then we construct appropriate evaluation tasks and datasets for each category. Besides, in the
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 1):

111:2 Lyu, et al.
Additional Key Words and Phrases: Retrieval-Augmented Generation, Large Language Models, Evaluation
ACM Reference Format:
Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong
Xu, and Enhong Chen. 2018. CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented
Generation of Large Language Models. J. ACM 37, 4, Article 111 (August 2018), 31 pages. https://doi.org/
XXXXXXX.XXXXXXX
1 INTRODUCTION
Retrieval-augmented generation (RAG) is an advanced technique that leverages external knowledge
sources to enhance the text generation capabilities of large language models(LLMs). It retrieves
relevant paragraphs from a corpus based on the input, and feeds them to the LLMs along with the
input. With the help of external knowledge, LLMs can generate more accurate and credible responses
and effectively address challenges such as outdated knowledge [19], hallucinations [3, 9, 35, 62],
and lack of domain expertise [30, 46]. Therefore, RAG technology is attracting increasing attention.
Although the effectiveness of retrieval-augmented strategies has been proven through extensive
practice, their implementation still requires a significant amount of tuning. The overall performance
of the RAG system is affected by multiple factors, such as the retrieval model, construction of the
external knowledge base, and language model. Therefore, automatic evaluation of RAG systems
is crucial. Currently, there are only a few existing benchmarks for evaluating RAG performance,
as creating high-quality datasets and experimenting with them entail significant costs. These
benchmarks can be classified into two types: reference-required and reference-free evaluation.
Reference-free evaluation frameworks, such as RAGAS [13] and ARES [44], use LLM-generated
data to evaluate RAG systems on contextual relevance, faithfulness, and informativeness. These
frameworks do not depend on ground truth references, but only assess the coherence of the
generated text with the retrieved context. This approach may be unreliable if the retrieved external
information is low-quality.
Consequently, reference-required evaluations remain the predominant method for assessing RAG
systems. Existing benchmarks for reference-required evaluations, such as RGB [8] and NQ [26].
do have their limitations. First, they all rely on question answering tasks to measure the perfor-
mance of RAG systems. Question answering is not the only RAG application scenario, and an
optimization strategy that works well for question answering may not be generalized to other
scenarios. Thus, these benchmarks may not capture the full potential of RAG systems. Second, in
the experiments, current evaluations usually focus on evaluating the LLM part of the RAG pipeline,
or focus on retriever performance in the knowledge-intensive scenario [40], while ignoring the
retrieval methods in non-knowledge-intensive scenarios and external knowledge base construction.
These components are also crucial for RAG systems. Therefore, a comprehensive evaluation of the
RAG system may not be obtained using any existing benchmarks.
To evaluate the performance of RAG in different application scenarios, we need a comprehensive
benchmark that covers more than just the question-answering task. Lewis et al. [28] argue that the
core of RAG systems is their interactive way of combining LLMs with external knowledge sources.
And following [25], we can group any interaction with external knowledge sources into four basic
actions: create, read, update, and delete, which are also known as CRUD actions [48]. Therefore,
we can use the CRUD framework to classify the RAG systems’ application scenarios. As shown in
Figure 1, each CRUD category demonstrates different capabilities of the RAG system:
•In "CREATE", the system improves the input text by adding relevant information from
external sources, making creative outputs such as poetry, stories, or code.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[167]_2309.01431.pdf (Page 1):

formation. These challenges result in LLMs being unable to
consistently generate reliable and accurate responses. Un-
fortunately, currently there lacks of comprehensive under-
standing on how these factors can influence RAG, and how
could each model survives from these drawbacks and im-
provement their performance via information retrieval. As a
result, there is a pressing need for a comprehensive evalua-
tion of LLMs on their ability to effectively utilize retrieved
information, as well as their ability to withstand the various
drawbacks present in information retrieval.
To this end, this paper conducts a comprehensive evalua-
tion of RAG for current LLMs. Specifically, we create a new
Retrieval-Augmented Generation Benchmark, namely RGB,
in both English and Chinese. In order to ensure that the in-
ternal knowledge of LLMs does not introduce bias into the
evaluation results, RGB chooses to aggregate the latest news
information and constructs queries based on the news infor-
mation. Then, based on these queries, we use Search API to
fetch relevant documents and select most relevant snippets
from the content as external retrieved documents. Finally,
based on different compositions of query and document-set
pairs, we expand the corpus and divided it into 4 testbeds to
evaluate the following basic abilities of LLMs according to
the common challenges in RAG, as shown in Figure 1:
• Noise Robustness, which means a LLM can extract use-
ful information from noisy documents. In this paper, we
define noisy documents as those that are relevant to the
question but do not contain any information of the an-
swer. For the instance in Figure 1, the noisy documents
related to the question “Who was awarded the 2022 No-
bel Prize in Literature” include reports about the 2021
Nobel Prize in Literature. To this end, the testbed for
noise robustness contains instances whose external doc-
uments contain a certain number of noisy documents
based on the desired noise ratio.
• Negative Rejection, which means that a LLM should re-
ject to answer the question when the required knowledge
is not present in any retrieved document. The testbed for
negative rejection contains instances whose external doc-
uments are only with noisy documents. LLMs are ex-
pected to indicate “insufficient information” or other re-
jection signals.
• Information Integration , which evaluates whether
LLMs can answer complex questions that require inte-
grating information from multiple documents. For the in-
stance in Figure 1, for the question “When were the Chat-
GPT app for iOS and ChatGPT api launched?”, LLMs
are expected to provide information of the launch dates
for both the ChatGPT iOS app and ChatGPT API. The
testbed for information integration contains instances
that can only be answered using multiple documents.
• Counterfactual Robustness, which evaluates whether
LLMs can identify risks of known factual errors in the
retrieved documents when the LLMs are given warnings
about potential risks in the retrieved information through
instruction. The testbed for counterfactual robustness in-
cludes instances that can be answered directly by the
LLMs, but the external documents contain factual errors.
Based on RGB, we conduct evaluation on 6 state-of-
the-art large language models including ChatGPT (Ope-
nAI 2022), ChatGLM-6B (THUDM 2023a), ChatGLM2-
6B (THUDM 2023b), Vicuna-7b (Chiang et al. 2023),
Qwen-7B-Chat (QwenLM 2023), BELLE-7B (Yunjie Ji
2023). We found that even though RAG can improve the re-
sponse accuracy of LLMs, they still suffer from the above-
mentioned challenges significantly. Specifically, we found
that even though LLMs demonstrate some level of noise ro-
bustness, they tend to confuse similar information and fre-
quently generate inaccurate answers when relevant informa-
tion exists. For example, when faced with a question about
the 2022 Nobel Prize in Literature, if there are noisy docu-
ments about the 2021 Nobel Prize in Literature in external
documents, LLMs may become confused and provide inac-
curate answers. Besides, LLMs frequently fail to reject an-
swering and generate incorrect answers when none of the
external documents contain relevant information. Further-
more, LLMs lack the ability to summarize from multiple
documents, and therefore if multiple documents are needed
to answer a question, LLMs often fail to provide accurate
answer. Finally, we found that even when the LLMs contain
the required knowledge and are given warnings about po-
tential risks in the retrieved information through instruction,
they still tend to trust and prioritize the retrieved information
over their own existing knowledge. The experimental results
mentioned above highlight the need for further resolution of
important issues in the existing RAG method. Therefore, it
is crucial to exercise caution and carefully design its usage.
Generally speaking, the contributions of this paper are1:
• We proposed to evaluate four capabilities for retrieval-
augmented generation of LLMs and created the
Retrieval-Augmented Generation Benchmark in both En-
glish and Chinese. To best of our knowledge, it is the first
benchmark designed to assess these four capabilities for
retrieval-augmented generation of LLMs.
• We evaluated the existing LLMs using RGB and found
the limitations of them in the four different abilities.
• We analyzed the responses of LLMs in RGB and identi-
fied their current shortcomings as well as suggested di-
rections for improvement.
Related work
Retrieval-augmented models The knowledge stored in
large language models is commonly out-of-date (He, Zhang,
and Roth 2022) and they also sometimes generate hallu-
cination (Cao et al. 2020; Raunak, Menezes, and Junczys-
Dowmunt 2021; Ji et al. 2023) i.e., they may generate ir-
relevant or factually incorrect contents. By using external
knowledge as guidance, retrieval-augmented models can
generate more accurate and reliable responses (Guu et al.
2020; Lewis et al. 2020; Borgeaud et al. 2022; Izacard
et al. 2022; Shi et al. 2023; Ren et al. 2023). Retrieval-
augmented models have achieved remarkable results in var-
ious tasks such as open-domain QA (Izacard and Grave
2021; Trivedi et al. 2023; Li et al. 2023a), dialogue (Cai
1Our code&data: https://github.com/chen700564/RGB.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 14):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:15
summarization tasks. Therefore, it does not require any ground truth reference. However, for
RAG systems, the retrieved texts may be irrelevant or incorrect, so consistency with them is not a
valid criterion. Instead, we use this metric to measure how well the generated text matches the
ground-truth reference. We call this metric RAGQuestEval. We will explain this metric in detail.
Let 𝐺𝑇 and 𝐺𝑀 be two sequences of tokens, where 𝐺𝑇 denotes the ground truth references and
𝐺𝑀 the corresponding evaluated generations. First, we generate a series of questions from the
ground truth references 𝐺𝑇 using the QuestEval method, which extracts entities and noun phrases
from the text. The goal of RAGQuestEval is to check if the generated text includes and conveys
correctly all the key information from the ground truth reference.
Next, we answer these questions using both real references and model-generated text. If the
question is unanswerable, the model returns "<Unanswerable>".
Finally, we calculate two scores to evaluate the quality of the generated text: recall and precision.
Recall. Recall is the ratio of answerable questions to all questions. This score shows how much
information in the ground truth reference is captured by the text generated by the RAG system. A
higher recall means that the generated text covers more information from the reference.
Recall(𝐺𝑇,𝐺𝑀 )= 1
|𝑄𝐺(𝐺𝑇)|
∑︁
(𝑞,𝑟)∈𝑄𝐺 (𝐺𝑇)
I[𝑄𝐴(𝐺𝑀,𝑞)≠ < Unanswerable >] (1)
In the above equation, 𝑄𝐺 is the question generator and 𝑄𝐴 is the question answerer.
Precision. Precision is the average answer similarity of all questions, excluding the unanswerable
ones. We use the token level F1 score to measure the answer similarity, which is a standard metric
for evaluating factoid question answering models. Higher precision means that the generated text
is more accurate and consistent with the reference.
Prec(𝐺𝑇,𝐺𝑀 )= 1
|𝑄𝐺(𝐺𝑇)|
∑︁
(𝑞,𝑟)∈𝑄𝐺 (𝐺𝑇)
𝐹1 (𝑄𝐴(𝐺𝑀,𝑞),𝑟) (2)
4 EXPERIMENT
The current evaluation of RAG Benchmark only focuses on the large language model component
in the RAG pipeline, and overlooks the importance of retrieval database construction and retriever.
To address this gap, we examine how different aspects of RAG systems affect their performance in
our benchmark. We also discuss some possible ways to improve existing RAG systems.
4.1 Experimental Settings
In this section, we will introduce the components of the RAG system, and describe how we conduct
experiments to evaluate their impact on system performance. The RAG system consists of the
following components:
•Chunk size: The RAG system splits the external knowledge into chunks of a certain length
and stores them in a vector database. The chunk size affects the retrieval accuracy and the
completeness of the context.
•Chunk overlap: Chunk overlap refers to the shared tokens between two consecutive text
chunks and is used to ensure semantic coherence when chunking.
•Embedding model : The RAG system converts the text chunks and the user’s query into
vectors using an embedding model or other methods. The embedding model affects the
quality and relevance of the context.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



### Claim 75/179

#### Claim Text
Concurrently, state-of-the-art automated tools like RAGAS [164], ARES [165], and TruLens 8 employ LLMs to adjudicate the quality scores.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 6):

ARES Ranking of Pseudo RAG Systems
NQ HotpotQA WoW FEVER MultiRC ReCoRD
C.R A.R. C.R A.R. C.R A.R. C.R A.R. C.R A.R. C.R A.R.
Kendall’s Tau for
Sampled Annotations 0.83 0.89 0.78 0.78 0.78 0.83 0.89 0.89 0.83 0.83 0.72 0.94
Kendall’s Tau
for RAGAS 0.89 0.89 0.94 0.89 0.94 0.94 0.72 0.61 0.83 0.94 0.89 0.44
Kendall’s Tau
for GPT-3.5 Judge 0.89 0.94 0.67 0.94 0.94 0.89 0.78 0.78 0.83 0.89 0.83 0.94
Kendall’s Tau for
ARES LLM Judge 0.89 1.0 0.89 0.94 0.94 1.0 0.83 0.72 0.94 0.83 0.78 0.83
Kendall’s Tau
for ARES 0.94 1.0 0.94 0.94 1.0 1.0 0.89 0.78 0.94 0.89 0.83 0.89
RAGAS Accuracy 31.4% 71.2% 17.2% 76.0% 36.4% 77.8% 23.7% 69.2% 16.1% 75.0% 15.0% 72.8%
GPT-3.5 Judge Accuracy 73.8% 95.5% 75.3% 71.6% 84.3% 85.2% 60.4% 59.6% 72.4% 60.3% 81.0% 65.8%
ARES Accuracy 79.3% 97.2% 92.3% 81.3% 85.7% 96.1% 88.4% 78.5% 85.8% 82.7% 67.8% 92.3%
Table 1: ARES Ranking with Fine-tuned LLM Judges vs. Sampled Annotations, RAGAS and GPT-3.5 Judge:
For scoring context relevance and answer relevance (C.R. and A.R. in the table, respectively), we compare ARES
with our fine-tuned LLM judges against sampled annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge.
For our sampled annotations, we gather 150 annotated datapoints from each mock RAG system and use those labels
to score the system. RAGAS also uses GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for
each evaluation domain. Overall, we found that ARES ranked RAG systems more accurately than RAGAS and
GPT-3.5 across all the explored datasets. The Kendall’s tau for ARES was 0.065 higher on average for scoring
context relevance and 0.132 higher on average for scoring answer relevance than RAGAS. Additionally, we include
the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted the ranking accuracy of
the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial costs required to run.
For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human preference validation
set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.
used ARES with human annotation sets ranging
in size from 25 to 400 and found that 150 is the
minimum number required (Table 3). Second, we
explored whether GPT-4 generations could replace
human annotations entirely, finding that GPT-4 is
less good than humans in this role, though the idea
arguably has promise (Table 4).
5.2 ARES Performance on AIS
WoW CNN / DM
ARES Split Prediction 0.478 0.835
Correct Positive/Negative Split 0.458 0.859
ARES Judge Accuracy 62.5% 84.0%
Evaluation Set Size 707 510
Human Preference Data Size 200 200
Table 2: ARES Results on the AIS benchmark
To evaluate whether ARES can effectively gauge
answer faithfulness in real RAG systems, we tested
ARES on the AIS attribution benchmark (Rashkin
et al., 2022). In AIS, we selected the Wizards
of Wikipedia (WoW) and CNN/DM datasets; the
other benchmark datasets involve either table rea-
soning (ToTTo) or focus on passage summariza-
tion (QRECC) so we excluded them. In WoW
and CNN/DM, each evaluation example includes
a query, a retrieved passage, and a generated an-
swer (which is either faithful or non-attributed to
the retrieved passage).
Table 2 summarizes our AIS results. We found
that ARES can effectively score the AIS datasets,
getting within 2.5 accuracy points of the correct
scores. Furthermore, for scoring each system,
we only use 200 annotated datapoints for our hu-
man preference validation set. Our results on AIS
demonstrate the ability of ARES to reliably dis-
tinguish faithful and hallucinated answers in real-
world RAG systems.
5.3 ARES Ranking of Existing RAG Systems
We also wanted to evaluate whether ARES can
score and rank existing RAG systems across both
context relevance and answer relevance. For eval-
uation, we selected the NQ, WoW, and FEVER
datasets from KILT. We consider the answer gen-



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 0):

ARES: An Automated Evaluation Framework for Retrieval-Augmented
Generation Systems
Jon Saad-Falcon
Stanford University ∗
jonsaadfalcon@stanford.edu
Omar Khattab
Stanford University
okhattab@stanford.edu
Christopher Potts
Stanford University
cgpotts@stanford.edu
Matei Zaharia
Databricks and UC Berkeley
matei@databricks.com
Abstract
Evaluating retrieval-augmented generation
(RAG) systems traditionally relies on hand
annotations for input queries, passages to re-
trieve, and responses to generate. We intro-
duce ARES, an Automated RAG Evaluation
System, for evaluating RAG systems along
the dimensions of context relevance, answer
faithfulness, and answer relevance. By cre-
ating its own synthetic training data, ARES
finetunes lightweight LM judges to assess the
quality of individual RAG components. To
mitigate potential prediction errors, ARES uti-
lizes a small set of human-annotated datapoints
for prediction-powered inference (PPI). Across
eight different knowledge-intensive tasks in
KILT, SuperGLUE, and AIS, ARES accurately
evaluates RAG systems while using only a few
hundred human annotations during evaluation.
Furthermore, ARES judges remain effective
across domain shifts, proving accurate even
after changing the type of queries and/or docu-
ments used in the evaluated RAG systems. We
make our code and datasets publicly available
on Github.
1 Introduction
Retrieval-augmented generation (RAG) has be-
come a prominent approach for building user-
facing NLP applications, such as systems for ques-
tion answering (QA), fact-checking, and customer
support (Petroni et al., 2021; Wang et al., 2019).
Typically, a RAG system consists of a retriever and
a downstream language model (LM). Given a user
question, the retriever finds relevant passages from
a corpus and the LM uses these passages to gener-
ate a response. This formulation admits a multitude
of choices: what retrieval model to use, how to di-
vide the documents into retrieval chunks, and how
to prompt or finetune the LM to use the retrieved
information, to name only a few of the simplest
design decisions.
∗Project started during research internship at Databricks
The best design for a RAG system is not neces-
sarily universal across data domains, corpus sizes,
and cost/latency budgets. To tune their own RAG
systems, practitioners traditionally need hand an-
notations for test questions, passages to retrieve
(to assess the retriever), and responses to generate,
labeled specifically for their target domain. Alter-
natively, they may evaluate different approaches in
production by collecting human preferences that
compare the candidate systems. Unfortunately,
both of these strategies demand high expertise and
impose considerable annotation costs.
Model-based evaluation is an inexpensive strat-
egy to test generative output quality (Zheng et al.,
2023). For instance, the open-source RAGAS
framework (James and Es, 2023) prompts an LM
for evaluating the relevance of retrieved informa-
tion and the faithfulness and accuracy of generated
responses. Unfortunately, such strategies currently
rely for evaluation on a fixed set of heuristically
hand-written prompts, offering little adaptability
to various evaluation contexts and no guarantees
about quality.
To evaluate RAG systems rapidly and accu-
rately, we propose ARES, the Automated RAG
Evaluation System. ARES is the first automated
RAG evaluation system to generate tailored LLM
judges for each component of a RAG pipeline, lead-
ing to substantial boosts in evaluation precision and
accuracy compared to existing approaches like RA-
GAS. Furthermore, unlike existing RAG evaluation
systems, ARES provides confidence intervals for
its scoring by leveraging prediction-powered in-
ference (PPI; Angelopoulos et al. 2023). Given a
corpus of documents and a RAG system, ARES
reports three evaluation scores: context relevance
(is the retrieved information pertinent to the test
question), answer faithfulness (is the response gen-
erated by the language model properly grounded
in the retrieved context), and answer relevance (is
the response also relevant to the question). A good
arXiv:2311.09476v2  [cs.CL]  31 Mar 2024



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 1):

RAG system finds relevant contexts and generates
answers that are both faithful and relevant.
Many existing RAG evaluation frameworks re-
quire substantial human annotations for scoring.
ARES significantly improves data efficiency dur-
ing evaluation by only requiring three inputs: an in-
domain passage set, a human preference validation
set of approximately 150 annotated datapoints or
more, and few-shot examples of in-domain queries
and answers (e.g. five examples or more), which
are used for prompting LLMs in synthetic data gen-
eration.
Given the corpus of in-domain passages, ARES
proceeds in three stages. First, it leverages an LM
to construct a synthetic dataset of question–answer
pairs, derived from the passages in the corpus. Sec-
ond, it defines three separate judge models to per-
form three classification tasks (context relevance,
answer faithfulness, and answer relevance). These
judges are lightweight models fine-tuned against a
contrastive learning objective. Third, ARES scores
the different RAG systems being assessed using
prediction-powered inference (PPI; Angelopoulos
et al. 2023) to improve model-based evaluation ac-
curacy and provide statistical confidence intervals
for RAG scoring. PPI utilizes a small set of human
annotated datapoints for computing its confidence
intervals; we designate this annotated set as our hu-
man preference validation set, which is composed
of approximately 150 annotated datapoints or more
that designate both positive and negative examples
for context relevance, answer faithfulness, and an-
swer relevance.
We conduct extensive empirical evaluations,
demonstrating that ARES accurately scores
RAG systems across the six knowledge-intensive
datasets in KILT and SuperGLUE, beating exist-
ing automated evaluation approaches like RAGAS
by 59.3 and 14.4 percentage points on average
across context relevance and answer relevance eval-
uation accuracy, respectively. Additionally, ARES
accurately calculates answer hallucination occur-
rences in the AIS attribution dataset (Rashkin et al.,
2022), predicting within 2.5 percentage points of
the ground truth average for answer hallucinations.
Compared to annotation-based evaluation methods,
ARES is substantially more accurate and efficient,
requiring 78% less annotations than the baseline
approach. We also find that ARES consistently
distinguishes competitive RAG systems that are
only a few points apart in ground-truth metrics.
This precision enables ARES to guide the develop-
ment and comparison of competitive approaches
and configurations.
We make the ARES code and datasets publicly
available on Github.
2 Related Work
RAG (Guu et al., 2020; Lewis et al., 2020; Khat-
tab et al., 2021; Izacard et al., 2022)) is now a
common strategy for bolstering LLMs by combin-
ing them with retrieval systems. Through retrieval,
RAG helps LM systems gather domain-specific
knowledge, ground generations in factual informa-
tion (Shuster et al., 2021; Huo et al., 2023), and
offer a degree of transparency or interpretability
via citing sources (Mialon et al., 2023).
Multiple LLM-based evaluation techniques have
emerged for gauging LLM systems. This is essen-
tial for rapid deployment in new settings, where it
is difficult to build a traditional benchmark dataset
from scratch. Early attempts at this use LLMs
out of the box, as in MT-Bench and Chatbot
Arena (Zheng et al., 2023). AutoCalibrate (Liu
et al., 2023b) seeks to align an LLM-judge with
human preferences, leveraging a self-refinement
prompt to iteratively improve the LLM judge. How-
ever, AutoCalibrate does not offer any statistical
guarantees for the accuracy of its predictions. Other
work has used LLM prompting to evaluate system
quality across natural language generation tasks,
such as translation, summarization, and dialogue
(Kocmi and Federmann, 2023; Fu et al., 2023; Liu
et al., 2023a; Wang et al., 2023).
In the context of knowledge-intensive NLP tasks,
LLMs have been explored for assessing attribution
and factuality in LLMs (Min et al., 2023; Gekhman
et al., 2023; Yue et al., 2023). New guidelines
like LongEval (Krishna et al., 2023) and datasets
like Hagrid and ALCE (Kamalloo et al., 2023;
Gao et al., 2023) provide resources for analyzing
knowledge-intensive LLM pipelines.
The two most-closely related projects to ARES
are EXAM (Sander and Dietz, 2021) and RA-
GAS (James and Es, 2023). To evaluate RAG sys-
tems, the EXAM metric estimates how many exam
questions a reader (simulated as a QA system) can
answer correctly based on the generated response.
This requires a set of queries with several asso-
ciated sub-questions each, which adds a burden
that ARES does not bring. RAGAS is based on a
handful of heuristic hand-written prompts. These
offer little adaptability to new RAG evaluation set-



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 5):

test ARES success at both scoring and ranking the
mock RAG systems appropriately across the three
evaluation criteria.
4.3 Metrics
To calculate the correlation between the correct
ranking and the ARES ranking, we use the Kendall
rank correlation coefficient or Kendall’s τ :
τ = (# of concordant pairs) − (# of discordant pairs)
# of pairs total
Concordant pairs are defined as two ordinal val-
ues in the ranking where the earlier value in the
sequence is lower than the later value in the se-
quence. Discordant pairs are defined as two ordinal
values in the ranking where the earlier value in the
sequence is greater than or equal to the later value
in the sequence. A Kendall’s τ greater than 0.9 is
considered successful but it ranges from 0.0 to 1.0.
In development, researchers and engineers
will be comparing different RAG configurations
through individual pairwise comparisons of model
choices, retriever selection, and document prepro-
cessing. We want to make sure that ARES has satis-
factory accuracy in pairwise comparisons across a
variety of performance gaps between RAG systems.
Kendall’s τ is explicitly designed for measuring the
accuracy of such pairwise comparisons, calculating
the correlation between a perfectly accurate pair-
wise ranking and an experimental pairwise ranking.
Thus, it is a popular and widespread metric used in
information retrieval, allowing developers to eval-
uate ranking systems empirically. Therefore, we
believe Kendall’s tau and prediction accuracy pro-
vide meaningful metrics for testing the efficacy of
ARES as a RAG evaluation system.
5 Results & Analysis
5.1 ARES Ranking
Table 1 summarizes our main evaluation of ARES
(with DeBERTa-v3-Large as the pretrained basis
for the judges). We compare against RAGAS (ver-
sion 0.0.18) and a baseline few-shot prompted GPT-
3.5 judge ( gpt-3.5-turbo-16k). For the few-shot
GPT-3.5 judge, we provide few-shot examples for
guiding predictions; the prompts are included in
Appendices A.2, A.3, and A.4. For both ARES
and the GPT-3.5 judge baseline, we augment the
LLM with PPI, using a 300-datapoint human pref-
erence validation set to rectify the ML predictions
and produce confidence intervals.
Across almost all settings across the datasets
from KILT and SuperGLUE, ARES provides a
more accurate ranking of RAG systems than RA-
GAS. ARES averages a Kendall’sτ 0.065 higher
for context relevance and 0.132 higher for answer
relevance than RAGAS . Additionally, the LLM-
judge is substantially more accurate than RAGAS
at predicting context relevance and answer rele-
vance of a query-passage-answer triple. For con-
text relevance, ARES with a fine-tuned LLM-judge
is 59.9 percentage points higher than RAGASwhile
for answer relevance, our system is 14.4 percent-
age points higher than RAGAS . Overall, ARES
provides a more accurate system for automatically
evaluating RAG configurations than RAGAS by
leveraging domain-adaptive techniques for prompt-
ing and training as well as utilizing PPI to bolster
model predictions.
As an additional comparison, we also include
the Kendall’s τ for RAG ranking with the ARES
LLM judge without PPI; for all datasets tested, PPI
improved the ranking prediction accuracy of the
fine-tuned LLM judge. Furthermore, we included
a sampled annotations configuration, in which we
sampled 150-datapoints from each mock RAG sys-
tem, totalling 1,350 annotations. Even with all
these annotations, the Kendall’s τ for ARES is
0.08 higher on average, across both context and an-
swer relevance, compared to sampled annotations,
despite using 78% less annotations. In sum, ARES
proves significantly more data-efficient with human
annotations while being more accurate at scoring
than standard sampled annotation methods.
Compared to the GPT-3.5 judge, ARES provides
a more accurate ranking of the RAG systems than
the GPT-3.5 judge, averaging a Kendall’s tau 0.06
higher over both context relevance and answer rel-
evance. Between the judge configurations, the fine-
tuned LLM judge of ARES can more precisely dis-
tinguish between RAG systems and guide configu-
ration decisions surrounding document splitting, re-
triever selection, and generative LLM choice. How-
ever, while the fine-tuned LLM judge had a higher
Kendall’s tau on average, the GPT-3.5 judge is
more readily deployable and does not require any
additional fine-tuning. The GPT-3.5 judge does
come with its own querying costs, which can vary
based on the date of querying as well as the total
tokens used in evaluation.
We also wanted to better understand the impor-
tance of human annotations for ARES. To this end,
we conducted two sets of experiments. First, we



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 15):

ARES Ranking of Real RAG Systems
NQ WoW FEVER
C.R. A.R. C.R. A.R. C.R. A.R.
Kendall’s Tau for
Sampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82
Kendall’s Tau
for RAGAS 0.82 0.82 0.73 0.82 0.73 0.87
Kendall’s Tau
for GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87
Kendall’s Tau
for ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87
Kendall’s Tau
for ARES 1.0 0.96 0.91 1.0 0.82 1.0
RAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%
GPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%
ARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%
Table 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance
(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled
annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150
annotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses
GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we
found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.
Additionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted
the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial
costs required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human
preference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.
ARES Cross-Domain Ranking of Pseudo RAG Systems
NQ to
FEVER
FEVER to
NQ
NQ to
MultiRC
MultiRC to
NQ
NQ to
ReCoRD
ReCoRD to
NQ
C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.
Kendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94
Kendall’s Tau of
In-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0
Average PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%
Accuracy on
RAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%
Table 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the
fine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance
(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains
when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and
ReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that
additional examples further improved the performance of ARES. Furthermore, we found that even in scenarios
where the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ
to FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of
percentage points from the lower bound to the upper bound of the PPI confidence interval.



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 5):

111:6 Lyu, et al.
2.2 RAG Benchmarks
When investigating the development and optimization of RAG, the effective evaluation of their
performance becomes a fundamental concern. Table 1 shows some commonly used benchmarks
for evaluating RAG. LangChain provides benchmark tasks, such as LangChain Docs Q&A and
Semi-structured Reports [27], designed to assess various RAG architectures. These datasets are
constructed from snapshots of Python documentation and PDFs containing tables and charts.
They emphasize the model’s capability to handle structured and semi-structured data. Evaluation
standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing
large models for question-answering generation has emerged as a prevalent approach in building
evaluation datasets. For instance, RGB [8] creates its evaluation dataset by gathering recent news
reports and employing LLM to generate relevant events, questions, and answers. Conversely,
ARES [44]. relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
These methods not only showcase the RAG system’s proficiency in handling real-time data but also
illustrate the utility of automation and synthetic data in the evaluation process. For evaluating the
capabilities of models across various professional domains, the Instruct-Benchmark-Tester dataset
encompasses a range of question types, with a particular focus on financial services, legal, and
intricate business scenarios [39].
Depending on whether the evaluation phase incorporates ground truth, metrics of existing
evaluation methods can be categorized into those necessitating reference and those not requiring
it. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by
contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-
Benchmark-Tester [39] employs accuracy score as an evaluation metric, a widely acknowledged
measure of model performance that assesses the extent to which model-generated answers align
with reference answers. The primary objective of RGB [8] is to evaluate whether large models can
effectively utilize external documents to acquire knowledge and generate accurate answers. Its
evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.
Reference-free evaluation methods, including TruLens-Eval [14], RAGAS [13], and ARES [44],
provide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning
context relevance, answer faithfulness, and answer relevance. TruLens-Eval [14] introduces the
RAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,
encompassing context relevance, groundedness, and answer relevance. RAGAS [13], serving as
a reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity
to identify pertinent and concentrated context passages, along with the LLMs’ proficiency in
faithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a
predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each
aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy
when compared to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-
powered inference to offer statistical assurances for its scoring, generating confidence intervals.
ARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer
relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts
and producing both faithful and relevant answers. Regarding evaluation methods, [32] places an
emphasis on assessing the credibility and accuracy of responses generated by generative search
engines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including
high costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,
exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely
approximates manual evaluation outcomes.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 13):

Figure 2: RAG Systems Evaluation on NQ - Context Relevance
Figure 3: RAG Systems Evaluation on NQ - Answer Relevance



Source: data\tc16_2312.10997v5\referenced_papers\[164]_2309.15217.pdf (Page 0):

RAGAS: Automated Evaluation of Retrieval Augmented Generation
Shahul Es†, Jithin James†, Luis Espinosa-Anke∗♢, Steven Schockaert∗
†Exploding Gradients
∗CardiffNLP, Cardiff University, United Kingdom
♢AMPLYFI, United Kingdom
shahules786@gmail.com,jamesjithin97@gmail.com
{espinosa-ankel,schockaerts1}@cardiff.ac.uk
Abstract
We introduce RAGA S (Retrieval Augmented
Generation Assessment), a framework for
reference-free evaluation of Retrieval Aug-
mented Generation (RAG) pipelines. RAG
systems are composed of a retrieval and an
LLM based generation module, and provide
LLMs with knowledge from a reference textual
database, which enables them to act as a natu-
ral language layer between a user and textual
databases, reducing the risk of hallucinations.
Evaluating RAG architectures is, however, chal-
lenging because there are several dimensions to
consider: the ability of the retrieval system to
identify relevant and focused context passages,
the ability of the LLM to exploit such passages
in a faithful way, or the quality of the gener-
ation itself. With RAGA S, we put forward a
suite of metrics which can be used to evaluate
these different dimensions without having to
rely on ground truth human annotations. We
posit that such a framework can crucially con-
tribute to faster evaluation cycles of RAG archi-
tectures, which is especially important given
the fast adoption of LLMs.
1 Introduction
Language Models (LMs) capture a vast amount
of knowledge about the world, which allows them
to answer questions without accessing any exter-
nal sources. This idea of LMs as repositories of
knowledge emerged shortly after the introduction
of BERT (Devlin et al., 2019) and became more
firmly established with the introduction of ever
larger LMs (Roberts et al., 2020). While the most
recent Large Language Models (LLMs) capture
enough knowledge to rival human performance
across a wide variety of question answering bench-
marks (Bubeck et al., 2023), the idea of using
LLMs as knowledge bases still has two fundamen-
tal limitations. First, LLMs are not able to answer
questions about events that have happened after
they were trained. Second, even the largest models
struggle to memorise knowledge that is only rarely
mentioned in the training corpus (Kandpal et al.,
2022; Mallen et al., 2023). The standard solution
to these issues is to rely on Retrieval Augmented
Generation (RAG) (Lee et al., 2019; Lewis et al.,
2020; Guu et al., 2020). Answering a question
then essentially involves retrieving relevant pas-
sages from a corpus and feeding these passages,
along with the original question, to the LM. While
initial approaches relied on specialised LMs for
retrieval-augmented language modelling (Khandel-
wal et al., 2020; Borgeaud et al., 2022), recent work
has suggested that simply adding retrieved docu-
ments to the input of a standard LM can also work
well (Khattab et al., 2022; Ram et al., 2023; Shi
et al., 2023), thus making it possible to use retrieval-
augmented strategies in combination with LLMs
that are only available through APIs.
While the usefulness of retrieval-augmented
strategies is clear, their implementation requires
a significant amount of tuning, as the overall per-
formance will be affected by the retrieval model,
the considered corpus, the LM, or the prompt for-
mulation, among others. Automated evaluation of
retrieval-augmented systems is thus paramount. In
practice, RAG systems are often evaluated in terms
of the language modelling task itself, i.e. by mea-
suring perplexity on some reference corpus. How-
ever, such evaluations are not always predictive
of downstream performance (Wang et al., 2023c).
Moreover, this evaluation strategy relies on the LM
probabilities, which are not accessible for some
closed models (e.g. ChatGPT and GPT-4). Ques-
tion answering is another common evaluation task,
but usually only datasets with short extractive an-
swers are considered, which may not be represen-
tative of how the system will be used.
To address these issues, in this paper we present
RAGA S1, a framework for the automated assess-
1RAGA S is available at https://github.com/
explodinggradients/ragas.
arXiv:2309.15217v1  [cs.CL]  26 Sep 2023



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 1):

RAG system finds relevant contexts and generates
answers that are both faithful and relevant.
Many existing RAG evaluation frameworks re-
quire substantial human annotations for scoring.
ARES significantly improves data efficiency dur-
ing evaluation by only requiring three inputs: an in-
domain passage set, a human preference validation
set of approximately 150 annotated datapoints or
more, and few-shot examples of in-domain queries
and answers (e.g. five examples or more), which
are used for prompting LLMs in synthetic data gen-
eration.
Given the corpus of in-domain passages, ARES
proceeds in three stages. First, it leverages an LM
to construct a synthetic dataset of question–answer
pairs, derived from the passages in the corpus. Sec-
ond, it defines three separate judge models to per-
form three classification tasks (context relevance,
answer faithfulness, and answer relevance). These
judges are lightweight models fine-tuned against a
contrastive learning objective. Third, ARES scores
the different RAG systems being assessed using
prediction-powered inference (PPI; Angelopoulos
et al. 2023) to improve model-based evaluation ac-
curacy and provide statistical confidence intervals
for RAG scoring. PPI utilizes a small set of human
annotated datapoints for computing its confidence
intervals; we designate this annotated set as our hu-
man preference validation set, which is composed
of approximately 150 annotated datapoints or more
that designate both positive and negative examples
for context relevance, answer faithfulness, and an-
swer relevance.
We conduct extensive empirical evaluations,
demonstrating that ARES accurately scores
RAG systems across the six knowledge-intensive
datasets in KILT and SuperGLUE, beating exist-
ing automated evaluation approaches like RAGAS
by 59.3 and 14.4 percentage points on average
across context relevance and answer relevance eval-
uation accuracy, respectively. Additionally, ARES
accurately calculates answer hallucination occur-
rences in the AIS attribution dataset (Rashkin et al.,
2022), predicting within 2.5 percentage points of
the ground truth average for answer hallucinations.
Compared to annotation-based evaluation methods,
ARES is substantially more accurate and efficient,
requiring 78% less annotations than the baseline
approach. We also find that ARES consistently
distinguishes competitive RAG systems that are
only a few points apart in ground-truth metrics.
This precision enables ARES to guide the develop-
ment and comparison of competitive approaches
and configurations.
We make the ARES code and datasets publicly
available on Github.
2 Related Work
RAG (Guu et al., 2020; Lewis et al., 2020; Khat-
tab et al., 2021; Izacard et al., 2022)) is now a
common strategy for bolstering LLMs by combin-
ing them with retrieval systems. Through retrieval,
RAG helps LM systems gather domain-specific
knowledge, ground generations in factual informa-
tion (Shuster et al., 2021; Huo et al., 2023), and
offer a degree of transparency or interpretability
via citing sources (Mialon et al., 2023).
Multiple LLM-based evaluation techniques have
emerged for gauging LLM systems. This is essen-
tial for rapid deployment in new settings, where it
is difficult to build a traditional benchmark dataset
from scratch. Early attempts at this use LLMs
out of the box, as in MT-Bench and Chatbot
Arena (Zheng et al., 2023). AutoCalibrate (Liu
et al., 2023b) seeks to align an LLM-judge with
human preferences, leveraging a self-refinement
prompt to iteratively improve the LLM judge. How-
ever, AutoCalibrate does not offer any statistical
guarantees for the accuracy of its predictions. Other
work has used LLM prompting to evaluate system
quality across natural language generation tasks,
such as translation, summarization, and dialogue
(Kocmi and Federmann, 2023; Fu et al., 2023; Liu
et al., 2023a; Wang et al., 2023).
In the context of knowledge-intensive NLP tasks,
LLMs have been explored for assessing attribution
and factuality in LLMs (Min et al., 2023; Gekhman
et al., 2023; Yue et al., 2023). New guidelines
like LongEval (Krishna et al., 2023) and datasets
like Hagrid and ALCE (Kamalloo et al., 2023;
Gao et al., 2023) provide resources for analyzing
knowledge-intensive LLM pipelines.
The two most-closely related projects to ARES
are EXAM (Sander and Dietz, 2021) and RA-
GAS (James and Es, 2023). To evaluate RAG sys-
tems, the EXAM metric estimates how many exam
questions a reader (simulated as a QA system) can
answer correctly based on the generated response.
This requires a set of queries with several asso-
ciated sub-questions each, which adds a burden
that ARES does not bring. RAGAS is based on a
handful of heuristic hand-written prompts. These
offer little adaptability to new RAG evaluation set-



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 6):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:7
2.3 Citation-Enhanced RAG
In traditional RAG methods, despite the rich information sources provided by retrieved contexts for
text generation, these models often do not explicitly require responses to provide corresponding
citations, making traceability difficult. Therefore, enhancing text verifiability by introducing citation
links, i.e., explicit references, has become an important research direction in the RAG field [17, 54].
Providing citation indicators in the response text offers several clear benefits. First, users can
easily verify the claims made by LLMs based on the provided citations, thus improving the trans-
parency and credibility of the text. Second, if the text generated by LLMs adheres faithfully to the
cited contexts, it can significantly improve its accuracy and reduce the phenomenon of "halluci-
nations" [17]. Given this, generating high-quality citations and evaluating the quality of citation
generation have become crucial elements of assessing RAG performance. Constructing appropriate
prompts directly through the retrieval context to guide the model in generating corresponding
citations constitutes a direct and effective method of citation generation [22].
In terms of evaluation, early research primarily focused on the fluency, accuracy, and basic
citation quality of the text generated by LLMs [17, 29]. For example, Rashkin et al. proposed the
"Attributable to Identified Sources" (AIS) score [42], which serves as a valuable tool for measuring the
degree to which generated text is faithful to its sources. As research progressed, scholars recognized
the need for more detailed evaluation methods to differentiate between various levels of citation
support. By creating specialized datasets such as SCIFI [7], researchers can more precisely evaluate
fine-grained citations at the clause level in texts generated by LLMs. The ALiiCE framework [54], by
analyzing the atomic structure of sentence claims, introduced fine-grained evaluation metrics, such
as location citation recall and precision, and the coefficient of variation of citation locations, to more
granularly evaluate the quality of citation generation in RAG [54]. In practical applications, [61]
found that RAG requires more complex evaluation frameworks to distinguish between various
levels of citation support by comparing different fidelity metrics. These RAG evaluation methods
not only consider the presence of citations but also their accuracy and relevance.
While Citation-Enhanced RAG delves deeply into the specific domain of citation generation,
aiming to improve the credibility and accuracy of text generated by RAG systems, our benchmark
provides a comprehensive evaluation framework encompassing various aspects of RAG systems
and multiple application scenarios.
3 CRUD-RAG: A COMPREHENSIVE CHINESE BENCHMARK FOR RAG
As we discussed earlier, implementing RAG effectively requires careful tuning of multiple com-
ponents, such as the retrieval model, the knowledge corpus, the language model, and the query
formulation. Therefore, we need a framework that can evaluate the RAG system automatically. This
framework would enable us to examine how these components affect the system’s performance,
and provide us with useful insights for improving and innovating the system.
However, The current RAG benchmarks have several drawbacks: they only evaluate question
answering tasks [1, 41, 57], ignoring other diverse application of RAG. The optimization strategy
for question-answering tasks may not suit other tasks; And in the evaluation experiment, current
RAG benchmarks only account for the LLM component in the RAG pipeline, or the retriever in the
knowledge-intensive scenario, disregarding the vital roles of retrieval database construction and
retrieval in non-knowledge-intensive scenarios.
To address the shortcomings of previous benchmarks, we introduce CRUD-RAG, a comprehensive
Chinese benchmark for RAG. Figure 2 illustrates the features of our CRUD-RAG benchmark. It
classifies the RAG application scenarios into four categories: Create, Read, Update, and Delete,
then we construct appropriate evaluation tasks and datasets for each category. Besides, in the
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



### Claim 76/179

#### Claim Text
RAG vs Long Context With the deepening of related research, the context of LLMs is continuously expanding [170]–[172].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 12):

knowledge base. The temporal misalignment prob-
lem on a fixed candidate database can be alleviated.
On the other hand, web search APIs are commer-
cial products requiring subscriptions. Also, the vast
amount of knowledge on the web can be difficult
to control. The retrieved context from the Internet
can be occasionally inconsistent, redundant, and
toxic, which hinders the LLM reader.
Beyond retrieval augmentation, in a general
scope, other tools called by LLMs, like code in-
terpreters, online models, and expert applications,
are all similar to search engines, without trainable
parameters to optimize. There could be a gap be-
tween the LM and these tools. This paper proposes
an idea to align them through a trainable small
model.



Source: data\tc16_2312.10997v5\referenced_papers\[170]_2310.03025.pdf (Page 0):

Published as a conference paper at ICLR 2024
RETRIEVAL MEETS LONG CONTEXT LARGE LANGUAGE
MODELS
Peng Xu†, Wei Ping†, Xianchao Wu, Lawrence McAfee
Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina
Mohammad Shoeybi, Bryan Catanzaro
NVIDIA
†{pengx, wping}@nvidia.com
ABSTRACT
Extending the context window of large language models (LLMs) is getting popular
recently, while the solution of augmenting LLMs with retrieval has existed for years.
The natural questions are: i) Retrieval-augmentation versus long context window,
which one is better for downstream tasks? ii) Can both methods be combined to
get the best of both worlds?In this work, we answer these questions by studying
both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B
GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context
window using simple retrieval-augmentation at generation can achieve compa-
rable performance to finetuned LLM with 16K context window via positional
interpolation on long context tasks, while taking much less computation. More
importantly, we demonstrate that retrieval can significantly improve the perfor-
mance of LLMs regardless of their extended context window sizes. Our best model,
retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-
turbo-16k and Davinci003 in terms of average score on nine long context tasks
including question answering, query-based summarization, and in-context few-shot
learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a
margin, while being much faster at generation. Our study provides general insights
on the choice of retrieval-augmentation versus long context extension of LLM for
practitioners.
1 I NTRODUCTION
The long context large language models (LLM) have recently received a lot of attention in produc-
tion (e.g., Anthropic, 2023; OpenAI, 2023b), research community (e.g., Chen et al., 2023; Liu et al.,
2023; Tworkowski et al., 2023), and open source community (e.g., Kaiokendev, 2023). Although
the approximate attention methods have been studied for years (e.g., Tay et al., 2022a) (due to the
quadratic time and memory complexities of self-attention mechanism in sequence length), the recent
advance for long context LLMs with exact attention is mainly driven by the development of faster
GPU with more memory and memory-efficient exact attention (Dao et al., 2022; Dao, 2023).
An alternative and long-standing solution for handling long context is retrieval. Specifically, the
LLMs only read relevant context retrieved from a standalone retriever (e.g., Karpukhin et al., 2020;
Wang et al., 2022; Lin et al., 2023), which is much easier to scale1 and runs orders of magnitudes
faster than LLMs for selecting relevant context. Conceptually, the retrieval-augmented decoder-only
LLM can be viewed as applying the sparse attention over its long context window, where the sparsity
pattern is not predefined as Child et al. (2019) but determined by the standalone retriever. In other
words, unretrieved context is treated as irrelevant and has zero-valued attention weights.
Given the surge of interest in long context LLM research and much more required computation
at inference 2, it is still unclear for practitioners whether extending the context window of LLM
1The dense embedding retriever can easily retrieve context from billions of tokens using the fast similarity
search library (Johnson et al., 2019).
2For example, the price of GPT-4 with 32k context length is twice the 8k context model.
1
arXiv:2310.03025v2  [cs.CL]  23 Jan 2024



Source: data\tc16_2312.10997v5\referenced_papers\[60]_2310.03025.pdf (Page 0):

Published as a conference paper at ICLR 2024
RETRIEVAL MEETS LONG CONTEXT LARGE LANGUAGE
MODELS
Peng Xu†, Wei Ping†, Xianchao Wu, Lawrence McAfee
Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina
Mohammad Shoeybi, Bryan Catanzaro
NVIDIA
†{pengx, wping}@nvidia.com
ABSTRACT
Extending the context window of large language models (LLMs) is getting popular
recently, while the solution of augmenting LLMs with retrieval has existed for years.
The natural questions are: i) Retrieval-augmentation versus long context window,
which one is better for downstream tasks? ii) Can both methods be combined to
get the best of both worlds?In this work, we answer these questions by studying
both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B
GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context
window using simple retrieval-augmentation at generation can achieve compa-
rable performance to finetuned LLM with 16K context window via positional
interpolation on long context tasks, while taking much less computation. More
importantly, we demonstrate that retrieval can significantly improve the perfor-
mance of LLMs regardless of their extended context window sizes. Our best model,
retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-
turbo-16k and Davinci003 in terms of average score on nine long context tasks
including question answering, query-based summarization, and in-context few-shot
learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a
margin, while being much faster at generation. Our study provides general insights
on the choice of retrieval-augmentation versus long context extension of LLM for
practitioners.
1 I NTRODUCTION
The long context large language models (LLM) have recently received a lot of attention in produc-
tion (e.g., Anthropic, 2023; OpenAI, 2023b), research community (e.g., Chen et al., 2023; Liu et al.,
2023; Tworkowski et al., 2023), and open source community (e.g., Kaiokendev, 2023). Although
the approximate attention methods have been studied for years (e.g., Tay et al., 2022a) (due to the
quadratic time and memory complexities of self-attention mechanism in sequence length), the recent
advance for long context LLMs with exact attention is mainly driven by the development of faster
GPU with more memory and memory-efficient exact attention (Dao et al., 2022; Dao, 2023).
An alternative and long-standing solution for handling long context is retrieval. Specifically, the
LLMs only read relevant context retrieved from a standalone retriever (e.g., Karpukhin et al., 2020;
Wang et al., 2022; Lin et al., 2023), which is much easier to scale1 and runs orders of magnitudes
faster than LLMs for selecting relevant context. Conceptually, the retrieval-augmented decoder-only
LLM can be viewed as applying the sparse attention over its long context window, where the sparsity
pattern is not predefined as Child et al. (2019) but determined by the standalone retriever. In other
words, unretrieved context is treated as irrelevant and has zero-valued attention weights.
Given the surge of interest in long context LLM research and much more required computation
at inference 2, it is still unclear for practitioners whether extending the context window of LLM
1The dense embedding retriever can easily retrieve context from billions of tokens using the fast similarity
search library (Johnson et al., 2019).
2For example, the price of GPT-4 with 32k context length is twice the 8k context model.
1
arXiv:2310.03025v2  [cs.CL]  23 Jan 2024



Source: data\tc16_2312.10997v5\referenced_papers\[60]_2310.03025.pdf (Page 1):

Published as a conference paper at ICLR 2024
provides higher accuracy than the retrieval-augmentation for downstream tasks with informative
queries. Moreover, it would be compelling if we could combine the strength of both methods and
achieve even higher accuracies. In this work, we attempt to answer the above questions through a
comprehensive study.
Specifically, we make the following contributions:
1. We perform comprehensive study using two state-of-the-art LLMs, a proprietary 43B pre-
trained GPT and Llama2-70B (Touvron et al., 2023b) on 9 downstream long context tasks,
including single and multi document question answering (QA), query-based summarization,
and in context few-shot learning tasks.
2. We demonstrate that retrieval-augmentation significantly improves the performance of 4K
context LLMs. Perhaps surprisingly, we find this simple retrieval-augmented baseline can
perform comparable to 16K long context LLMs, i.e., average score 29.32 vs. 29.45 by using
GPT-43B, and 36.02 vs. 36.78 by using Llama2-70B, while using much less computation.
3. Furthermore, we demonstrate that the performance of long context LLM (i.e., 16K or 32K)
can still be improved by retrieval, especially for the larger Llama2-70B. As a result, our best
model, retrieval augmented Llama2-70B-32k-ret with 32K context window (avg. score 43.6),
outperforms GPT-3.5-turbo-16k (avg. score 42.8) and Davinci-003 in terms of average score.
It also largely outperforms its non-retrieval Llama2-70B-32k baseline (avg. score 40.9), while
can be much faster at generation (e.g., 4× faster on NarrativeQA).
We organize the rest of the paper as follows. We discuss related work in Section 2, and present the
experimental setup in Section 3. We report results in Section 4 and conclude the paper in Section 5.
2 R ELATED WORK
In this section, we discuss the related work in long context LLM, efficient attention methods, and
retrieval-augmented language models.
2.1 P ARALLEL WORK
When we are preparing this manuscript, we notice that a concurrent work (Bai et al., 2023) (arXived
on 28 Aug 2023) also studies the impact of retrieval on long context LLM, including black-box model
GPT-3.5-Turbo-16k (OpenAI, 2022), white-box model Llama2-7B-chat-4k (Touvron et al., 2023b),
and ChatGLM2-6B-32k (Zeng et al., 2022). Different from our findings, they find that retrieval is
only helpful for Llama2-7B-chat-4k with 4K context window, but not helpful for long context model,
i.e., GPT-3.5-Turbo-16k and ChatGLM2-6B-32k. We hypothesize the major reasons are: i) it is
challenging to do controlled experiments using black-box APIs, ii) the white-box LLMs used in their
study are relatively small, thus they have limited zero-shot capability of incorporating context through
retrieval. Our conclusions are drawn from much larger LLMs. In particular, our best long context
model Llama2-70B-32k performs as well as Davinci003 and GPT-3.5-turbo-16k, while it can still be
further enhanced by retrieval (see Table 3).
2.2 L ONG CONTEXT LARGE LANGUAGE MODELS
Over the past few years, pretraining large language models (LLMs) with long context window
becomes a viable solution thanks to faster GPU with more memory and memory-efficient exact
attention (e.g., Dao et al., 2022). For example, the context window for pretrained LLM have been
increased from 1024 of GPT-2 (Radford et al., 2019), 2048 of GPT-3 (Brown et al., 2020), 4096 of
Llama 2 (Touvron et al., 2023b), to 8192 of GPT-4 (OpenAI, 2023a). However, further extending the
context window in pretraining can be challenging, because,i) pretraining LLM from scratch with long
context (e.g., >16K tokens) is very expensive due to the quadratic time and memory complexities of
exact attention, and ii) most of documents in pretraining corpus (e.g., Common Crawl) are relatively
short.
Most recently, researchers start to extend the context window of LLMs with continued training or
fine-tuning (e.g., Kaiokendev, 2023; Nijkamp et al., 2023; Chen et al., 2023; Tworkowski et al., 2023;
Mohtashami & Jaggi, 2023). Tworkowski et al. (2023) introduced LongLLaMA by fine-tuning the
2



Source: data\tc16_2312.10997v5\referenced_papers\[170]_2310.03025.pdf (Page 1):

Published as a conference paper at ICLR 2024
provides higher accuracy than the retrieval-augmentation for downstream tasks with informative
queries. Moreover, it would be compelling if we could combine the strength of both methods and
achieve even higher accuracies. In this work, we attempt to answer the above questions through a
comprehensive study.
Specifically, we make the following contributions:
1. We perform comprehensive study using two state-of-the-art LLMs, a proprietary 43B pre-
trained GPT and Llama2-70B (Touvron et al., 2023b) on 9 downstream long context tasks,
including single and multi document question answering (QA), query-based summarization,
and in context few-shot learning tasks.
2. We demonstrate that retrieval-augmentation significantly improves the performance of 4K
context LLMs. Perhaps surprisingly, we find this simple retrieval-augmented baseline can
perform comparable to 16K long context LLMs, i.e., average score 29.32 vs. 29.45 by using
GPT-43B, and 36.02 vs. 36.78 by using Llama2-70B, while using much less computation.
3. Furthermore, we demonstrate that the performance of long context LLM (i.e., 16K or 32K)
can still be improved by retrieval, especially for the larger Llama2-70B. As a result, our best
model, retrieval augmented Llama2-70B-32k-ret with 32K context window (avg. score 43.6),
outperforms GPT-3.5-turbo-16k (avg. score 42.8) and Davinci-003 in terms of average score.
It also largely outperforms its non-retrieval Llama2-70B-32k baseline (avg. score 40.9), while
can be much faster at generation (e.g., 4× faster on NarrativeQA).
We organize the rest of the paper as follows. We discuss related work in Section 2, and present the
experimental setup in Section 3. We report results in Section 4 and conclude the paper in Section 5.
2 R ELATED WORK
In this section, we discuss the related work in long context LLM, efficient attention methods, and
retrieval-augmented language models.
2.1 P ARALLEL WORK
When we are preparing this manuscript, we notice that a concurrent work (Bai et al., 2023) (arXived
on 28 Aug 2023) also studies the impact of retrieval on long context LLM, including black-box model
GPT-3.5-Turbo-16k (OpenAI, 2022), white-box model Llama2-7B-chat-4k (Touvron et al., 2023b),
and ChatGLM2-6B-32k (Zeng et al., 2022). Different from our findings, they find that retrieval is
only helpful for Llama2-7B-chat-4k with 4K context window, but not helpful for long context model,
i.e., GPT-3.5-Turbo-16k and ChatGLM2-6B-32k. We hypothesize the major reasons are: i) it is
challenging to do controlled experiments using black-box APIs, ii) the white-box LLMs used in their
study are relatively small, thus they have limited zero-shot capability of incorporating context through
retrieval. Our conclusions are drawn from much larger LLMs. In particular, our best long context
model Llama2-70B-32k performs as well as Davinci003 and GPT-3.5-turbo-16k, while it can still be
further enhanced by retrieval (see Table 3).
2.2 L ONG CONTEXT LARGE LANGUAGE MODELS
Over the past few years, pretraining large language models (LLMs) with long context window
becomes a viable solution thanks to faster GPU with more memory and memory-efficient exact
attention (e.g., Dao et al., 2022). For example, the context window for pretrained LLM have been
increased from 1024 of GPT-2 (Radford et al., 2019), 2048 of GPT-3 (Brown et al., 2020), 4096 of
Llama 2 (Touvron et al., 2023b), to 8192 of GPT-4 (OpenAI, 2023a). However, further extending the
context window in pretraining can be challenging, because,i) pretraining LLM from scratch with long
context (e.g., >16K tokens) is very expensive due to the quadratic time and memory complexities of
exact attention, and ii) most of documents in pretraining corpus (e.g., Common Crawl) are relatively
short.
Most recently, researchers start to extend the context window of LLMs with continued training or
fine-tuning (e.g., Kaiokendev, 2023; Nijkamp et al., 2023; Chen et al., 2023; Tworkowski et al., 2023;
Mohtashami & Jaggi, 2023). Tworkowski et al. (2023) introduced LongLLaMA by fine-tuning the
2



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[7]_2305.14283.pdf (Page 12):

knowledge base. The temporal misalignment prob-
lem on a fixed candidate database can be alleviated.
On the other hand, web search APIs are commer-
cial products requiring subscriptions. Also, the vast
amount of knowledge on the web can be difficult
to control. The retrieved context from the Internet
can be occasionally inconsistent, redundant, and
toxic, which hinders the LLM reader.
Beyond retrieval augmentation, in a general
scope, other tools called by LLMs, like code in-
terpreters, online models, and expert applications,
are all similar to search engines, without trainable
parameters to optimize. There could be a gap be-
tween the LM and these tools. This paper proposes
an idea to align them through a trainable small
model.



Source: data\tc16_2312.10997v5\referenced_papers\[170]_2310.03025.pdf (Page 0):

Published as a conference paper at ICLR 2024
RETRIEVAL MEETS LONG CONTEXT LARGE LANGUAGE
MODELS
Peng Xu†, Wei Ping†, Xianchao Wu, Lawrence McAfee
Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina
Mohammad Shoeybi, Bryan Catanzaro
NVIDIA
†{pengx, wping}@nvidia.com
ABSTRACT
Extending the context window of large language models (LLMs) is getting popular
recently, while the solution of augmenting LLMs with retrieval has existed for years.
The natural questions are: i) Retrieval-augmentation versus long context window,
which one is better for downstream tasks? ii) Can both methods be combined to
get the best of both worlds?In this work, we answer these questions by studying
both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B
GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context
window using simple retrieval-augmentation at generation can achieve compa-
rable performance to finetuned LLM with 16K context window via positional
interpolation on long context tasks, while taking much less computation. More
importantly, we demonstrate that retrieval can significantly improve the perfor-
mance of LLMs regardless of their extended context window sizes. Our best model,
retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-
turbo-16k and Davinci003 in terms of average score on nine long context tasks
including question answering, query-based summarization, and in-context few-shot
learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a
margin, while being much faster at generation. Our study provides general insights
on the choice of retrieval-augmentation versus long context extension of LLM for
practitioners.
1 I NTRODUCTION
The long context large language models (LLM) have recently received a lot of attention in produc-
tion (e.g., Anthropic, 2023; OpenAI, 2023b), research community (e.g., Chen et al., 2023; Liu et al.,
2023; Tworkowski et al., 2023), and open source community (e.g., Kaiokendev, 2023). Although
the approximate attention methods have been studied for years (e.g., Tay et al., 2022a) (due to the
quadratic time and memory complexities of self-attention mechanism in sequence length), the recent
advance for long context LLMs with exact attention is mainly driven by the development of faster
GPU with more memory and memory-efficient exact attention (Dao et al., 2022; Dao, 2023).
An alternative and long-standing solution for handling long context is retrieval. Specifically, the
LLMs only read relevant context retrieved from a standalone retriever (e.g., Karpukhin et al., 2020;
Wang et al., 2022; Lin et al., 2023), which is much easier to scale1 and runs orders of magnitudes
faster than LLMs for selecting relevant context. Conceptually, the retrieval-augmented decoder-only
LLM can be viewed as applying the sparse attention over its long context window, where the sparsity
pattern is not predefined as Child et al. (2019) but determined by the standalone retriever. In other
words, unretrieved context is treated as irrelevant and has zero-valued attention weights.
Given the surge of interest in long context LLM research and much more required computation
at inference 2, it is still unclear for practitioners whether extending the context window of LLM
1The dense embedding retriever can easily retrieve context from billions of tokens using the fast similarity
search library (Johnson et al., 2019).
2For example, the price of GPT-4 with 32k context length is twice the 8k context model.
1
arXiv:2310.03025v2  [cs.CL]  23 Jan 2024



Source: data\tc16_2312.10997v5\referenced_papers\[60]_2310.03025.pdf (Page 0):

Published as a conference paper at ICLR 2024
RETRIEVAL MEETS LONG CONTEXT LARGE LANGUAGE
MODELS
Peng Xu†, Wei Ping†, Xianchao Wu, Lawrence McAfee
Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina
Mohammad Shoeybi, Bryan Catanzaro
NVIDIA
†{pengx, wping}@nvidia.com
ABSTRACT
Extending the context window of large language models (LLMs) is getting popular
recently, while the solution of augmenting LLMs with retrieval has existed for years.
The natural questions are: i) Retrieval-augmentation versus long context window,
which one is better for downstream tasks? ii) Can both methods be combined to
get the best of both worlds?In this work, we answer these questions by studying
both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B
GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context
window using simple retrieval-augmentation at generation can achieve compa-
rable performance to finetuned LLM with 16K context window via positional
interpolation on long context tasks, while taking much less computation. More
importantly, we demonstrate that retrieval can significantly improve the perfor-
mance of LLMs regardless of their extended context window sizes. Our best model,
retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-
turbo-16k and Davinci003 in terms of average score on nine long context tasks
including question answering, query-based summarization, and in-context few-shot
learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a
margin, while being much faster at generation. Our study provides general insights
on the choice of retrieval-augmentation versus long context extension of LLM for
practitioners.
1 I NTRODUCTION
The long context large language models (LLM) have recently received a lot of attention in produc-
tion (e.g., Anthropic, 2023; OpenAI, 2023b), research community (e.g., Chen et al., 2023; Liu et al.,
2023; Tworkowski et al., 2023), and open source community (e.g., Kaiokendev, 2023). Although
the approximate attention methods have been studied for years (e.g., Tay et al., 2022a) (due to the
quadratic time and memory complexities of self-attention mechanism in sequence length), the recent
advance for long context LLMs with exact attention is mainly driven by the development of faster
GPU with more memory and memory-efficient exact attention (Dao et al., 2022; Dao, 2023).
An alternative and long-standing solution for handling long context is retrieval. Specifically, the
LLMs only read relevant context retrieved from a standalone retriever (e.g., Karpukhin et al., 2020;
Wang et al., 2022; Lin et al., 2023), which is much easier to scale1 and runs orders of magnitudes
faster than LLMs for selecting relevant context. Conceptually, the retrieval-augmented decoder-only
LLM can be viewed as applying the sparse attention over its long context window, where the sparsity
pattern is not predefined as Child et al. (2019) but determined by the standalone retriever. In other
words, unretrieved context is treated as irrelevant and has zero-valued attention weights.
Given the surge of interest in long context LLM research and much more required computation
at inference 2, it is still unclear for practitioners whether extending the context window of LLM
1The dense embedding retriever can easily retrieve context from billions of tokens using the fast similarity
search library (Johnson et al., 2019).
2For example, the price of GPT-4 with 32k context length is twice the 8k context model.
1
arXiv:2310.03025v2  [cs.CL]  23 Jan 2024



Source: data\tc16_2312.10997v5\referenced_papers\[60]_2310.03025.pdf (Page 1):

Published as a conference paper at ICLR 2024
provides higher accuracy than the retrieval-augmentation for downstream tasks with informative
queries. Moreover, it would be compelling if we could combine the strength of both methods and
achieve even higher accuracies. In this work, we attempt to answer the above questions through a
comprehensive study.
Specifically, we make the following contributions:
1. We perform comprehensive study using two state-of-the-art LLMs, a proprietary 43B pre-
trained GPT and Llama2-70B (Touvron et al., 2023b) on 9 downstream long context tasks,
including single and multi document question answering (QA), query-based summarization,
and in context few-shot learning tasks.
2. We demonstrate that retrieval-augmentation significantly improves the performance of 4K
context LLMs. Perhaps surprisingly, we find this simple retrieval-augmented baseline can
perform comparable to 16K long context LLMs, i.e., average score 29.32 vs. 29.45 by using
GPT-43B, and 36.02 vs. 36.78 by using Llama2-70B, while using much less computation.
3. Furthermore, we demonstrate that the performance of long context LLM (i.e., 16K or 32K)
can still be improved by retrieval, especially for the larger Llama2-70B. As a result, our best
model, retrieval augmented Llama2-70B-32k-ret with 32K context window (avg. score 43.6),
outperforms GPT-3.5-turbo-16k (avg. score 42.8) and Davinci-003 in terms of average score.
It also largely outperforms its non-retrieval Llama2-70B-32k baseline (avg. score 40.9), while
can be much faster at generation (e.g., 4× faster on NarrativeQA).
We organize the rest of the paper as follows. We discuss related work in Section 2, and present the
experimental setup in Section 3. We report results in Section 4 and conclude the paper in Section 5.
2 R ELATED WORK
In this section, we discuss the related work in long context LLM, efficient attention methods, and
retrieval-augmented language models.
2.1 P ARALLEL WORK
When we are preparing this manuscript, we notice that a concurrent work (Bai et al., 2023) (arXived
on 28 Aug 2023) also studies the impact of retrieval on long context LLM, including black-box model
GPT-3.5-Turbo-16k (OpenAI, 2022), white-box model Llama2-7B-chat-4k (Touvron et al., 2023b),
and ChatGLM2-6B-32k (Zeng et al., 2022). Different from our findings, they find that retrieval is
only helpful for Llama2-7B-chat-4k with 4K context window, but not helpful for long context model,
i.e., GPT-3.5-Turbo-16k and ChatGLM2-6B-32k. We hypothesize the major reasons are: i) it is
challenging to do controlled experiments using black-box APIs, ii) the white-box LLMs used in their
study are relatively small, thus they have limited zero-shot capability of incorporating context through
retrieval. Our conclusions are drawn from much larger LLMs. In particular, our best long context
model Llama2-70B-32k performs as well as Davinci003 and GPT-3.5-turbo-16k, while it can still be
further enhanced by retrieval (see Table 3).
2.2 L ONG CONTEXT LARGE LANGUAGE MODELS
Over the past few years, pretraining large language models (LLMs) with long context window
becomes a viable solution thanks to faster GPU with more memory and memory-efficient exact
attention (e.g., Dao et al., 2022). For example, the context window for pretrained LLM have been
increased from 1024 of GPT-2 (Radford et al., 2019), 2048 of GPT-3 (Brown et al., 2020), 4096 of
Llama 2 (Touvron et al., 2023b), to 8192 of GPT-4 (OpenAI, 2023a). However, further extending the
context window in pretraining can be challenging, because,i) pretraining LLM from scratch with long
context (e.g., >16K tokens) is very expensive due to the quadratic time and memory complexities of
exact attention, and ii) most of documents in pretraining corpus (e.g., Common Crawl) are relatively
short.
Most recently, researchers start to extend the context window of LLMs with continued training or
fine-tuning (e.g., Kaiokendev, 2023; Nijkamp et al., 2023; Chen et al., 2023; Tworkowski et al., 2023;
Mohtashami & Jaggi, 2023). Tworkowski et al. (2023) introduced LongLLaMA by fine-tuning the
2



Source: data\tc16_2312.10997v5\referenced_papers\[170]_2310.03025.pdf (Page 1):

Published as a conference paper at ICLR 2024
provides higher accuracy than the retrieval-augmentation for downstream tasks with informative
queries. Moreover, it would be compelling if we could combine the strength of both methods and
achieve even higher accuracies. In this work, we attempt to answer the above questions through a
comprehensive study.
Specifically, we make the following contributions:
1. We perform comprehensive study using two state-of-the-art LLMs, a proprietary 43B pre-
trained GPT and Llama2-70B (Touvron et al., 2023b) on 9 downstream long context tasks,
including single and multi document question answering (QA), query-based summarization,
and in context few-shot learning tasks.
2. We demonstrate that retrieval-augmentation significantly improves the performance of 4K
context LLMs. Perhaps surprisingly, we find this simple retrieval-augmented baseline can
perform comparable to 16K long context LLMs, i.e., average score 29.32 vs. 29.45 by using
GPT-43B, and 36.02 vs. 36.78 by using Llama2-70B, while using much less computation.
3. Furthermore, we demonstrate that the performance of long context LLM (i.e., 16K or 32K)
can still be improved by retrieval, especially for the larger Llama2-70B. As a result, our best
model, retrieval augmented Llama2-70B-32k-ret with 32K context window (avg. score 43.6),
outperforms GPT-3.5-turbo-16k (avg. score 42.8) and Davinci-003 in terms of average score.
It also largely outperforms its non-retrieval Llama2-70B-32k baseline (avg. score 40.9), while
can be much faster at generation (e.g., 4× faster on NarrativeQA).
We organize the rest of the paper as follows. We discuss related work in Section 2, and present the
experimental setup in Section 3. We report results in Section 4 and conclude the paper in Section 5.
2 R ELATED WORK
In this section, we discuss the related work in long context LLM, efficient attention methods, and
retrieval-augmented language models.
2.1 P ARALLEL WORK
When we are preparing this manuscript, we notice that a concurrent work (Bai et al., 2023) (arXived
on 28 Aug 2023) also studies the impact of retrieval on long context LLM, including black-box model
GPT-3.5-Turbo-16k (OpenAI, 2022), white-box model Llama2-7B-chat-4k (Touvron et al., 2023b),
and ChatGLM2-6B-32k (Zeng et al., 2022). Different from our findings, they find that retrieval is
only helpful for Llama2-7B-chat-4k with 4K context window, but not helpful for long context model,
i.e., GPT-3.5-Turbo-16k and ChatGLM2-6B-32k. We hypothesize the major reasons are: i) it is
challenging to do controlled experiments using black-box APIs, ii) the white-box LLMs used in their
study are relatively small, thus they have limited zero-shot capability of incorporating context through
retrieval. Our conclusions are drawn from much larger LLMs. In particular, our best long context
model Llama2-70B-32k performs as well as Davinci003 and GPT-3.5-turbo-16k, while it can still be
further enhanced by retrieval (see Table 3).
2.2 L ONG CONTEXT LARGE LANGUAGE MODELS
Over the past few years, pretraining large language models (LLMs) with long context window
becomes a viable solution thanks to faster GPU with more memory and memory-efficient exact
attention (e.g., Dao et al., 2022). For example, the context window for pretrained LLM have been
increased from 1024 of GPT-2 (Radford et al., 2019), 2048 of GPT-3 (Brown et al., 2020), 4096 of
Llama 2 (Touvron et al., 2023b), to 8192 of GPT-4 (OpenAI, 2023a). However, further extending the
context window in pretraining can be challenging, because,i) pretraining LLM from scratch with long
context (e.g., >16K tokens) is very expensive due to the quadratic time and memory complexities of
exact attention, and ii) most of documents in pretraining corpus (e.g., Common Crawl) are relatively
short.
Most recently, researchers start to extend the context window of LLMs with continued training or
fine-tuning (e.g., Kaiokendev, 2023; Nijkamp et al., 2023; Chen et al., 2023; Tworkowski et al., 2023;
Mohtashami & Jaggi, 2023). Tworkowski et al. (2023) introduced LongLLaMA by fine-tuning the
2



### Claim 77/179

#### Claim Text
Conversely, the expansion of context provides new opportunities for the development of RAG, enabling it to address more complex problems and integrative or summary questions that require reading a large amount of material to answer [49].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 10):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:11
Researchers at West Virginia University in the United States 
have observed synthetic DNA at the atomic level and learned 
how to alter its structure to enhance its scissor-like functions.
Researchers at West Virginia University have successfully observed
 the structure of synthetic DNA at the atomic level and discovered
 that altering its structure can enhance its scissor-like functions. 
Their research findings were published in the Nature sub-journal, 
Communications Chemistry. The study found that these DNA 
molecules can fold into complex shapes capable of performing ...
Science and Technology Daily, Beijing, July 27 (Reporter Zhang 
Mengran) - Researchers at West Virginia University have achieved 
the observation of synthetic DNA at the atomic level, gaining 
insights into how to alter its structure to enhance its scissor-like 
functions. Understanding these synthetic DNA reactions in greater 
detail could be key to unlocking new medical technologies in the 
future. The research findings were published in the recent issue of 
the Nature sub-journal, Communications Chemistry. Atomic-level ...
The synthetic DNA used in this study, also known as DNAzymes, 
differs from human DNA in that it is created in a laboratory, has low 
production costs, and can catalyze chemical reactions. Researchers 
noted that DNA is typically regarded as inert when it comes to 
storing human genetic information; however, some types of DNA 
evolved in the laboratory defy traditional rules. These DNA ...
Fig. 4. The dataset construction pipeline for text continuation and multi-document summarization task.
our news corpus, removing the 10,000 articles 𝑑 simultaneously. The new news corpus 𝐷−𝑑+𝐸
serves as our retrieval corpus, and we expect the model to use the events and relevant information
from the retrieval corpus to generate a summary of the articles 𝑑.
3.3 Text continuation: RAG Application in "Create"
RAG is useful not only for "Delete", where it retrieves and summarizes key information from massive
texts, but also for "Create". In this scenario, RAG systems show strong creativity by expanding
existing texts, and we take the text continuation task as an evaluation. The text continuation task
aims to automatically produce coherent and relevant subsequent content based on the beginning
of the text, making the text more complete and vivid.
To construct the continuation task dataset, we follow the same method as the summary task
dataset. Figure 4 shows the construction process of text continuation. Specifically, we select a news
article from a high-quality corpus and use a specialized Chinese word segmentation tool, to split it
into sentences. Then, we divide the article into two equal parts: the first half serves as the input,
and the second half as the output of the continuation dataset. We expect the model to use RAG
technology to retrieve relevant information from the document library and generate a continuation
that is coherent, informative, and consistent with the input and output.
To ensure that the retrieval database covers the real continuation text, we use the Baidu search
engine to find external documents and add them to the database. The continuation text differs from
the event text in that it consists of multiple sentences. Therefore, we split the continuation text
into paragraphs by sentences and retrieve relevant documents for each paragraph using the search
engine. This way, we guarantee that the retrieval database contains most of the information to
reconstruct the continuation text.
3.4 Question Answering: RAG Application in "Read"
Another application scenario of RAG is to use external knowledge bases to enhance the question-
answering capabilities of LLMs, which can be applied to various knowledge-intensive tasks. Cur-
rently, there are many evaluation benchmarks to measure the performance of RAG in this scenario,
and multiple question answering datasets have been created.
However, the existing question answering datasets also have some limitations. On the one hand,
some datasets (such as NQ and WEBQA) are outdated, and may have been covered by LLMs in
the pre-training stage, which reduces the advantage of RAG systems. On the other hand, some
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 13):

Figure 2: RAG Systems Evaluation on NQ - Context Relevance
Figure 3: RAG Systems Evaluation on NQ - Answer Relevance



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 1):

111:2 Lyu, et al.
Additional Key Words and Phrases: Retrieval-Augmented Generation, Large Language Models, Evaluation
ACM Reference Format:
Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong
Xu, and Enhong Chen. 2018. CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented
Generation of Large Language Models. J. ACM 37, 4, Article 111 (August 2018), 31 pages. https://doi.org/
XXXXXXX.XXXXXXX
1 INTRODUCTION
Retrieval-augmented generation (RAG) is an advanced technique that leverages external knowledge
sources to enhance the text generation capabilities of large language models(LLMs). It retrieves
relevant paragraphs from a corpus based on the input, and feeds them to the LLMs along with the
input. With the help of external knowledge, LLMs can generate more accurate and credible responses
and effectively address challenges such as outdated knowledge [19], hallucinations [3, 9, 35, 62],
and lack of domain expertise [30, 46]. Therefore, RAG technology is attracting increasing attention.
Although the effectiveness of retrieval-augmented strategies has been proven through extensive
practice, their implementation still requires a significant amount of tuning. The overall performance
of the RAG system is affected by multiple factors, such as the retrieval model, construction of the
external knowledge base, and language model. Therefore, automatic evaluation of RAG systems
is crucial. Currently, there are only a few existing benchmarks for evaluating RAG performance,
as creating high-quality datasets and experimenting with them entail significant costs. These
benchmarks can be classified into two types: reference-required and reference-free evaluation.
Reference-free evaluation frameworks, such as RAGAS [13] and ARES [44], use LLM-generated
data to evaluate RAG systems on contextual relevance, faithfulness, and informativeness. These
frameworks do not depend on ground truth references, but only assess the coherence of the
generated text with the retrieved context. This approach may be unreliable if the retrieved external
information is low-quality.
Consequently, reference-required evaluations remain the predominant method for assessing RAG
systems. Existing benchmarks for reference-required evaluations, such as RGB [8] and NQ [26].
do have their limitations. First, they all rely on question answering tasks to measure the perfor-
mance of RAG systems. Question answering is not the only RAG application scenario, and an
optimization strategy that works well for question answering may not be generalized to other
scenarios. Thus, these benchmarks may not capture the full potential of RAG systems. Second, in
the experiments, current evaluations usually focus on evaluating the LLM part of the RAG pipeline,
or focus on retriever performance in the knowledge-intensive scenario [40], while ignoring the
retrieval methods in non-knowledge-intensive scenarios and external knowledge base construction.
These components are also crucial for RAG systems. Therefore, a comprehensive evaluation of the
RAG system may not be obtained using any existing benchmarks.
To evaluate the performance of RAG in different application scenarios, we need a comprehensive
benchmark that covers more than just the question-answering task. Lewis et al. [28] argue that the
core of RAG systems is their interactive way of combining LLMs with external knowledge sources.
And following [25], we can group any interaction with external knowledge sources into four basic
actions: create, read, update, and delete, which are also known as CRUD actions [48]. Therefore,
we can use the CRUD framework to classify the RAG systems’ application scenarios. As shown in
Figure 1, each CRUD category demonstrates different capabilities of the RAG system:
•In "CREATE", the system improves the input text by adding relevant information from
external sources, making creative outputs such as poetry, stories, or code.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 14):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:15
summarization tasks. Therefore, it does not require any ground truth reference. However, for
RAG systems, the retrieved texts may be irrelevant or incorrect, so consistency with them is not a
valid criterion. Instead, we use this metric to measure how well the generated text matches the
ground-truth reference. We call this metric RAGQuestEval. We will explain this metric in detail.
Let 𝐺𝑇 and 𝐺𝑀 be two sequences of tokens, where 𝐺𝑇 denotes the ground truth references and
𝐺𝑀 the corresponding evaluated generations. First, we generate a series of questions from the
ground truth references 𝐺𝑇 using the QuestEval method, which extracts entities and noun phrases
from the text. The goal of RAGQuestEval is to check if the generated text includes and conveys
correctly all the key information from the ground truth reference.
Next, we answer these questions using both real references and model-generated text. If the
question is unanswerable, the model returns "<Unanswerable>".
Finally, we calculate two scores to evaluate the quality of the generated text: recall and precision.
Recall. Recall is the ratio of answerable questions to all questions. This score shows how much
information in the ground truth reference is captured by the text generated by the RAG system. A
higher recall means that the generated text covers more information from the reference.
Recall(𝐺𝑇,𝐺𝑀 )= 1
|𝑄𝐺(𝐺𝑇)|
∑︁
(𝑞,𝑟)∈𝑄𝐺 (𝐺𝑇)
I[𝑄𝐴(𝐺𝑀,𝑞)≠ < Unanswerable >] (1)
In the above equation, 𝑄𝐺 is the question generator and 𝑄𝐴 is the question answerer.
Precision. Precision is the average answer similarity of all questions, excluding the unanswerable
ones. We use the token level F1 score to measure the answer similarity, which is a standard metric
for evaluating factoid question answering models. Higher precision means that the generated text
is more accurate and consistent with the reference.
Prec(𝐺𝑇,𝐺𝑀 )= 1
|𝑄𝐺(𝐺𝑇)|
∑︁
(𝑞,𝑟)∈𝑄𝐺 (𝐺𝑇)
𝐹1 (𝑄𝐴(𝐺𝑀,𝑞),𝑟) (2)
4 EXPERIMENT
The current evaluation of RAG Benchmark only focuses on the large language model component
in the RAG pipeline, and overlooks the importance of retrieval database construction and retriever.
To address this gap, we examine how different aspects of RAG systems affect their performance in
our benchmark. We also discuss some possible ways to improve existing RAG systems.
4.1 Experimental Settings
In this section, we will introduce the components of the RAG system, and describe how we conduct
experiments to evaluate their impact on system performance. The RAG system consists of the
following components:
•Chunk size: The RAG system splits the external knowledge into chunks of a certain length
and stores them in a vector database. The chunk size affects the retrieval accuracy and the
completeness of the context.
•Chunk overlap: Chunk overlap refers to the shared tokens between two consecutive text
chunks and is used to ensure semantic coherence when chunking.
•Embedding model : The RAG system converts the text chunks and the user’s query into
vectors using an embedding model or other methods. The embedding model affects the
quality and relevance of the context.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 2):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:3
Fig. 1. We have classified the application scenarios of RAG into four primary aspects: Create, Read, Update,
and Delete. The figure provides an illustrative example for each category, showcasing the wide-ranging
potential of RAG technology.
•In "READ", the system uses external knowledge retrieval to answer questions, solve problems
in question-answering, dialogue, and reasoning, and increase understanding of the input text.
•In "UPDATE", the system fixes errors in the input text using retrieved content, correcting
spelling, grammar, or factual errors to make the text better.
•In "DELETE", the system simplifies the input by improving retrieval results, removing
unnecessary details, and doing tasks like text summarization or simplification.
To evaluate the RAG system in these four scenarios, we introduce CRUD-RAG, a comprehensive,
large-scale Chinese RAG benchmark. CRUD-RAG consists of four evaluation tasks: text continu-
ation, question answering (with single-document and multi-document questions), hallucination
modification, and open-domain multi-document summarization, which respectively correspond to
the CRUD-RAG classification of RAG application scenarios. We construct CRUD-RAG by crawling
the latest high-quality news data from major news websites in China, which aims to minimize the
likelihood of LLMs encountering these data during training. Then, we automatically create datasets
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



### Claim 78/179

#### Claim Text
Improving RAG’s resistance to such adversarial or counterfactual inputs is gaining research momentum and has become a key performance metric [48], [50], [82].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 5):

111:6 Lyu, et al.
2.2 RAG Benchmarks
When investigating the development and optimization of RAG, the effective evaluation of their
performance becomes a fundamental concern. Table 1 shows some commonly used benchmarks
for evaluating RAG. LangChain provides benchmark tasks, such as LangChain Docs Q&A and
Semi-structured Reports [27], designed to assess various RAG architectures. These datasets are
constructed from snapshots of Python documentation and PDFs containing tables and charts.
They emphasize the model’s capability to handle structured and semi-structured data. Evaluation
standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing
large models for question-answering generation has emerged as a prevalent approach in building
evaluation datasets. For instance, RGB [8] creates its evaluation dataset by gathering recent news
reports and employing LLM to generate relevant events, questions, and answers. Conversely,
ARES [44]. relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
These methods not only showcase the RAG system’s proficiency in handling real-time data but also
illustrate the utility of automation and synthetic data in the evaluation process. For evaluating the
capabilities of models across various professional domains, the Instruct-Benchmark-Tester dataset
encompasses a range of question types, with a particular focus on financial services, legal, and
intricate business scenarios [39].
Depending on whether the evaluation phase incorporates ground truth, metrics of existing
evaluation methods can be categorized into those necessitating reference and those not requiring
it. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by
contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-
Benchmark-Tester [39] employs accuracy score as an evaluation metric, a widely acknowledged
measure of model performance that assesses the extent to which model-generated answers align
with reference answers. The primary objective of RGB [8] is to evaluate whether large models can
effectively utilize external documents to acquire knowledge and generate accurate answers. Its
evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.
Reference-free evaluation methods, including TruLens-Eval [14], RAGAS [13], and ARES [44],
provide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning
context relevance, answer faithfulness, and answer relevance. TruLens-Eval [14] introduces the
RAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,
encompassing context relevance, groundedness, and answer relevance. RAGAS [13], serving as
a reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity
to identify pertinent and concentrated context passages, along with the LLMs’ proficiency in
faithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a
predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each
aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy
when compared to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-
powered inference to offer statistical assurances for its scoring, generating confidence intervals.
ARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer
relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts
and producing both faithful and relevant answers. Regarding evaluation methods, [32] places an
emphasis on assessing the credibility and accuracy of responses generated by generative search
engines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including
high costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,
exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely
approximates manual evaluation outcomes.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 1):

RAG system finds relevant contexts and generates
answers that are both faithful and relevant.
Many existing RAG evaluation frameworks re-
quire substantial human annotations for scoring.
ARES significantly improves data efficiency dur-
ing evaluation by only requiring three inputs: an in-
domain passage set, a human preference validation
set of approximately 150 annotated datapoints or
more, and few-shot examples of in-domain queries
and answers (e.g. five examples or more), which
are used for prompting LLMs in synthetic data gen-
eration.
Given the corpus of in-domain passages, ARES
proceeds in three stages. First, it leverages an LM
to construct a synthetic dataset of question–answer
pairs, derived from the passages in the corpus. Sec-
ond, it defines three separate judge models to per-
form three classification tasks (context relevance,
answer faithfulness, and answer relevance). These
judges are lightweight models fine-tuned against a
contrastive learning objective. Third, ARES scores
the different RAG systems being assessed using
prediction-powered inference (PPI; Angelopoulos
et al. 2023) to improve model-based evaluation ac-
curacy and provide statistical confidence intervals
for RAG scoring. PPI utilizes a small set of human
annotated datapoints for computing its confidence
intervals; we designate this annotated set as our hu-
man preference validation set, which is composed
of approximately 150 annotated datapoints or more
that designate both positive and negative examples
for context relevance, answer faithfulness, and an-
swer relevance.
We conduct extensive empirical evaluations,
demonstrating that ARES accurately scores
RAG systems across the six knowledge-intensive
datasets in KILT and SuperGLUE, beating exist-
ing automated evaluation approaches like RAGAS
by 59.3 and 14.4 percentage points on average
across context relevance and answer relevance eval-
uation accuracy, respectively. Additionally, ARES
accurately calculates answer hallucination occur-
rences in the AIS attribution dataset (Rashkin et al.,
2022), predicting within 2.5 percentage points of
the ground truth average for answer hallucinations.
Compared to annotation-based evaluation methods,
ARES is substantially more accurate and efficient,
requiring 78% less annotations than the baseline
approach. We also find that ARES consistently
distinguishes competitive RAG systems that are
only a few points apart in ground-truth metrics.
This precision enables ARES to guide the develop-
ment and comparison of competitive approaches
and configurations.
We make the ARES code and datasets publicly
available on Github.
2 Related Work
RAG (Guu et al., 2020; Lewis et al., 2020; Khat-
tab et al., 2021; Izacard et al., 2022)) is now a
common strategy for bolstering LLMs by combin-
ing them with retrieval systems. Through retrieval,
RAG helps LM systems gather domain-specific
knowledge, ground generations in factual informa-
tion (Shuster et al., 2021; Huo et al., 2023), and
offer a degree of transparency or interpretability
via citing sources (Mialon et al., 2023).
Multiple LLM-based evaluation techniques have
emerged for gauging LLM systems. This is essen-
tial for rapid deployment in new settings, where it
is difficult to build a traditional benchmark dataset
from scratch. Early attempts at this use LLMs
out of the box, as in MT-Bench and Chatbot
Arena (Zheng et al., 2023). AutoCalibrate (Liu
et al., 2023b) seeks to align an LLM-judge with
human preferences, leveraging a self-refinement
prompt to iteratively improve the LLM judge. How-
ever, AutoCalibrate does not offer any statistical
guarantees for the accuracy of its predictions. Other
work has used LLM prompting to evaluate system
quality across natural language generation tasks,
such as translation, summarization, and dialogue
(Kocmi and Federmann, 2023; Fu et al., 2023; Liu
et al., 2023a; Wang et al., 2023).
In the context of knowledge-intensive NLP tasks,
LLMs have been explored for assessing attribution
and factuality in LLMs (Min et al., 2023; Gekhman
et al., 2023; Yue et al., 2023). New guidelines
like LongEval (Krishna et al., 2023) and datasets
like Hagrid and ALCE (Kamalloo et al., 2023;
Gao et al., 2023) provide resources for analyzing
knowledge-intensive LLM pipelines.
The two most-closely related projects to ARES
are EXAM (Sander and Dietz, 2021) and RA-
GAS (James and Es, 2023). To evaluate RAG sys-
tems, the EXAM metric estimates how many exam
questions a reader (simulated as a QA system) can
answer correctly based on the generated response.
This requires a set of queries with several asso-
ciated sub-questions each, which adds a burden
that ARES does not bring. RAGAS is based on a
handful of heuristic hand-written prompts. These
offer little adaptability to new RAG evaluation set-



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 0):

ARES: An Automated Evaluation Framework for Retrieval-Augmented
Generation Systems
Jon Saad-Falcon
Stanford University ∗
jonsaadfalcon@stanford.edu
Omar Khattab
Stanford University
okhattab@stanford.edu
Christopher Potts
Stanford University
cgpotts@stanford.edu
Matei Zaharia
Databricks and UC Berkeley
matei@databricks.com
Abstract
Evaluating retrieval-augmented generation
(RAG) systems traditionally relies on hand
annotations for input queries, passages to re-
trieve, and responses to generate. We intro-
duce ARES, an Automated RAG Evaluation
System, for evaluating RAG systems along
the dimensions of context relevance, answer
faithfulness, and answer relevance. By cre-
ating its own synthetic training data, ARES
finetunes lightweight LM judges to assess the
quality of individual RAG components. To
mitigate potential prediction errors, ARES uti-
lizes a small set of human-annotated datapoints
for prediction-powered inference (PPI). Across
eight different knowledge-intensive tasks in
KILT, SuperGLUE, and AIS, ARES accurately
evaluates RAG systems while using only a few
hundred human annotations during evaluation.
Furthermore, ARES judges remain effective
across domain shifts, proving accurate even
after changing the type of queries and/or docu-
ments used in the evaluated RAG systems. We
make our code and datasets publicly available
on Github.
1 Introduction
Retrieval-augmented generation (RAG) has be-
come a prominent approach for building user-
facing NLP applications, such as systems for ques-
tion answering (QA), fact-checking, and customer
support (Petroni et al., 2021; Wang et al., 2019).
Typically, a RAG system consists of a retriever and
a downstream language model (LM). Given a user
question, the retriever finds relevant passages from
a corpus and the LM uses these passages to gener-
ate a response. This formulation admits a multitude
of choices: what retrieval model to use, how to di-
vide the documents into retrieval chunks, and how
to prompt or finetune the LM to use the retrieved
information, to name only a few of the simplest
design decisions.
∗Project started during research internship at Databricks
The best design for a RAG system is not neces-
sarily universal across data domains, corpus sizes,
and cost/latency budgets. To tune their own RAG
systems, practitioners traditionally need hand an-
notations for test questions, passages to retrieve
(to assess the retriever), and responses to generate,
labeled specifically for their target domain. Alter-
natively, they may evaluate different approaches in
production by collecting human preferences that
compare the candidate systems. Unfortunately,
both of these strategies demand high expertise and
impose considerable annotation costs.
Model-based evaluation is an inexpensive strat-
egy to test generative output quality (Zheng et al.,
2023). For instance, the open-source RAGAS
framework (James and Es, 2023) prompts an LM
for evaluating the relevance of retrieved informa-
tion and the faithfulness and accuracy of generated
responses. Unfortunately, such strategies currently
rely for evaluation on a fixed set of heuristically
hand-written prompts, offering little adaptability
to various evaluation contexts and no guarantees
about quality.
To evaluate RAG systems rapidly and accu-
rately, we propose ARES, the Automated RAG
Evaluation System. ARES is the first automated
RAG evaluation system to generate tailored LLM
judges for each component of a RAG pipeline, lead-
ing to substantial boosts in evaluation precision and
accuracy compared to existing approaches like RA-
GAS. Furthermore, unlike existing RAG evaluation
systems, ARES provides confidence intervals for
its scoring by leveraging prediction-powered in-
ference (PPI; Angelopoulos et al. 2023). Given a
corpus of documents and a RAG system, ARES
reports three evaluation scores: context relevance
(is the retrieved information pertinent to the test
question), answer faithfulness (is the response gen-
erated by the language model properly grounded
in the retrieved context), and answer relevance (is
the response also relevant to the question). A good
arXiv:2311.09476v2  [cs.CL]  31 Mar 2024



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 4):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:5
Table 1. Relate Work.
Method Dataset Scale Evaluation Metrics Evaluation Method Application Field Ref. Lang.
[27] Based on LangChain Pythondocumentation QA dataset86 Accuracy of answer, Faithfulness ofresponse to the retrieved documentEvaluating retrieval andgeneration consistencyGeneral QA scenarios(Read) Yes EN
[27] PDF documents containingtables and charts5 Accuracy of answer, Faithfulness ofresponse to the retrieved documentEvaluating retrieval andgeneration consistency
Semi-structureddata scenarios(Read) Yes EN
[32] Query and responses(with citations)1450 Fluency, Perceived utility,Citation recall and precisionHuman evaluation Citation(Read) Yes EN
[17, 22] Questions, answersand contexts(with citations)- Fluency, Correctness,Citation quality Self-devised metrics,Human evaluation Citation(Read) Yes EN
[54] Questions, answersand contexts(with citations)1948 Location citation recall, Location precision,The coefficient of variation of citation locationsSelf-devised metricsCitation(Read) Yes EN
[29] Questions, answersand contexts(with citations)3422 Accuracy of factual and non-factual,AUC-PR, and so on Self-devised metrics,Common metrics Citation(Read) Yes EN
[61] Statement-citation Pairs12681 Correlation, Classification performance,Retrieval effectiveness, Faithfulness
Self-devised metrics,Common metrics,Human evaluation
Citation(Read) Yes EN
[7] Paragraphs(with citations)10000 Citation density,The coverage of reference factsSelf-devised metricsCitation(Read) Yes EN
[39] Questions, answersand contexts 200
Categorization ability,Logical/Mathematical reasoning,Complex question solving,Summarization ability
Accuracy Financial services,Legal, Business(Read,Delete) Yes EN
[8] LLM-generated dataset 1000
Noise robustness,Negative rejection,Information integration,Counterfactual robustness
Self-devised metricsGeneral, especiallynews domain(Read,Update) Yes CNEN
[14] — — Context relevance, Groundedness,Answer relevance Analyzing the RAG triadGeneral(Create,Read) No —
[13] — — Faithfulness, Answer relevance,Context relevance Automated evaluationusing LLM prompts General(Create,Read) No —
[44] LLM-generated dataset 150Context relevance, Answer faithfulness,Answer relevance Generating custom LLM judges foreach component of a RAG systemGeneral(Create,Read) No EN
Ours LLM-generated dataset 36166ROUGE, BLEU,bertScore, RAGQuestEvalEvaluating retrieval andgeneration consistency
General(Create,ReadUpdate,Delete) Yes CN
optimization, and post-retrieval processing [20]. Pre-retrieval processing encompasses data trans-
former to enhance text standardization, ensuring factual accuracy, optimizing index structures,
adjusting block sizes, and rewriting query [ 4, 16, 50, 52]. Retrieval model optimization entails
the fine-tuning of domain-specific embedding models and the application of dynamic embedding
techniques [11, 60]. Post-retrieval processing minimizes context length through reranking and
compression operations, aiming to emphasize critical information, diminish noise interference, and
enhance integration and utilization by the generator [37, 53, 55].
Furthermore, to enhance the precision and efficiency of the generator when handling retrieval
content, scholars have undertaken a series of optimization measures. As an illustration, researchers
have devised methods such as Chain-of-Note (CON) for the generator [58]. CON generates contin-
uous reading notes to comprehensively evaluate the relevance of retrieved documents to the posed
question, integrating this information to produce precise final responses. This approach further
enhances the capability of RAG in managing retrieval information, guaranteeing the production of
responses that are simultaneously accurate and pertinent. In specific domains, such as medical and
legal, models undergo fine-tuning to enhance the generator’s performance within those particular
fields [10, 24, 56]. Through the implementation of these methods, the generator can more effectively
process retrieved information and furnish responses that are more accurate and relevant.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 9):

Preprint.
0 50 100 150
Num of training (k)
35
40
45
50
55Perfomance
(a) PopQA
0 100
Num of training (k)
71
72
73
 (b) PubHealth
0 100
Num of training (k)
40
60
 (c) ASQA (prec)
Pop Bio.
S & P 92.5 70.0
ISREL 95.0 90.0
ISSUP 90.0 85.0
(d) Human evaluation on PopQA
and Bio generation.
Figure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect
of the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)
Human analysis on SELF -RAG outputs as well as reflection tokens.
contrary, a larger weight results in lower MAUVE scores: when generation gets longer and more
fluent, there are often more claims that are not fully supported by citations, consistent with findings
by Liu et al. (2023a). Our framework lets practitioners choose and customize models’ behaviors at
test time by adjusting such parameters without requiring additional training.
Efficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval
occurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects
overall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers
of threshold δ (larger δ results in less retrieval) on PubHealth and PopQA. Figure 3c shows that
the model’s retrieval frequencies dramatically change on both datasets. as δ varies. On one hand,
performance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.
Effects of training data size. We conduct an analysis of how the data scale affects the model’s
performance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original
150k training instances, and fine-tune four SELF -RAG 7B variants on those subsets. Then, we compare
the model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF -
RAG trained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models’
performance trained on different amount of data. Across all datasets, increasing data size often shows
upward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do
not observed such significant improvements on Llama2-FT7B when increasing the training data from
50k to 150k. These results also indicate that further expanding the training data of SELF -RAG may
lead to further improvements, although in this work we limit our training data size to 150k.
Human evaluations. We conduct small human evaluations on SELF -RAG outputs, as well as the
reliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio
results. Following Menick et al. (2022), human annotators evaluate S&P, which indicates whether
the model output is plausible (i.e., the output is a reasonable and on-topic response to the question
as if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to
verify the validity of the answer). For S&P, we do not consider the instances where SELF -RAG
predicts irrelevant or no support. We then ask our annotators whether the model-predicted
reflection tokens about ISREL and ISSUP match their inspections (e.g., whether the fully supported
output is supported by the cited evidence). Human annotators find SELF -RAG answers are often
plausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is
consistent with Menick et al. (2022). Human annotators also find ISREL and ISSUP reflection token
predictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated
examples and explanations on assessments.
6 C ONCLUSION
This work introduces SELF -RAG, a new framework to enhance the quality and factuality of LLMs
through retrieval on demand and self-reflection. SELF -RAG trains an LM to learn to retrieve, generate,
and critique text passages and its own generation by predicting the next tokens from its original
vocabulary as well as newly added special tokens, called reflection tokens.SELF -RAG further enables
the tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on
six tasks using multiple metrics demonstrate that SELF -RAG significantly outperforms LLMs with
more parameters or with conventional retrieval-augmented generation approaches.
10



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 5):

111:6 Lyu, et al.
2.2 RAG Benchmarks
When investigating the development and optimization of RAG, the effective evaluation of their
performance becomes a fundamental concern. Table 1 shows some commonly used benchmarks
for evaluating RAG. LangChain provides benchmark tasks, such as LangChain Docs Q&A and
Semi-structured Reports [27], designed to assess various RAG architectures. These datasets are
constructed from snapshots of Python documentation and PDFs containing tables and charts.
They emphasize the model’s capability to handle structured and semi-structured data. Evaluation
standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing
large models for question-answering generation has emerged as a prevalent approach in building
evaluation datasets. For instance, RGB [8] creates its evaluation dataset by gathering recent news
reports and employing LLM to generate relevant events, questions, and answers. Conversely,
ARES [44]. relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
These methods not only showcase the RAG system’s proficiency in handling real-time data but also
illustrate the utility of automation and synthetic data in the evaluation process. For evaluating the
capabilities of models across various professional domains, the Instruct-Benchmark-Tester dataset
encompasses a range of question types, with a particular focus on financial services, legal, and
intricate business scenarios [39].
Depending on whether the evaluation phase incorporates ground truth, metrics of existing
evaluation methods can be categorized into those necessitating reference and those not requiring
it. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by
contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-
Benchmark-Tester [39] employs accuracy score as an evaluation metric, a widely acknowledged
measure of model performance that assesses the extent to which model-generated answers align
with reference answers. The primary objective of RGB [8] is to evaluate whether large models can
effectively utilize external documents to acquire knowledge and generate accurate answers. Its
evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.
Reference-free evaluation methods, including TruLens-Eval [14], RAGAS [13], and ARES [44],
provide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning
context relevance, answer faithfulness, and answer relevance. TruLens-Eval [14] introduces the
RAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,
encompassing context relevance, groundedness, and answer relevance. RAGAS [13], serving as
a reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity
to identify pertinent and concentrated context passages, along with the LLMs’ proficiency in
faithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a
predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each
aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy
when compared to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-
powered inference to offer statistical assurances for its scoring, generating confidence intervals.
ARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer
relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts
and producing both faithful and relevant answers. Regarding evaluation methods, [32] places an
emphasis on assessing the credibility and accuracy of responses generated by generative search
engines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including
high costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,
exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely
approximates manual evaluation outcomes.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 1):

RAG system finds relevant contexts and generates
answers that are both faithful and relevant.
Many existing RAG evaluation frameworks re-
quire substantial human annotations for scoring.
ARES significantly improves data efficiency dur-
ing evaluation by only requiring three inputs: an in-
domain passage set, a human preference validation
set of approximately 150 annotated datapoints or
more, and few-shot examples of in-domain queries
and answers (e.g. five examples or more), which
are used for prompting LLMs in synthetic data gen-
eration.
Given the corpus of in-domain passages, ARES
proceeds in three stages. First, it leverages an LM
to construct a synthetic dataset of question–answer
pairs, derived from the passages in the corpus. Sec-
ond, it defines three separate judge models to per-
form three classification tasks (context relevance,
answer faithfulness, and answer relevance). These
judges are lightweight models fine-tuned against a
contrastive learning objective. Third, ARES scores
the different RAG systems being assessed using
prediction-powered inference (PPI; Angelopoulos
et al. 2023) to improve model-based evaluation ac-
curacy and provide statistical confidence intervals
for RAG scoring. PPI utilizes a small set of human
annotated datapoints for computing its confidence
intervals; we designate this annotated set as our hu-
man preference validation set, which is composed
of approximately 150 annotated datapoints or more
that designate both positive and negative examples
for context relevance, answer faithfulness, and an-
swer relevance.
We conduct extensive empirical evaluations,
demonstrating that ARES accurately scores
RAG systems across the six knowledge-intensive
datasets in KILT and SuperGLUE, beating exist-
ing automated evaluation approaches like RAGAS
by 59.3 and 14.4 percentage points on average
across context relevance and answer relevance eval-
uation accuracy, respectively. Additionally, ARES
accurately calculates answer hallucination occur-
rences in the AIS attribution dataset (Rashkin et al.,
2022), predicting within 2.5 percentage points of
the ground truth average for answer hallucinations.
Compared to annotation-based evaluation methods,
ARES is substantially more accurate and efficient,
requiring 78% less annotations than the baseline
approach. We also find that ARES consistently
distinguishes competitive RAG systems that are
only a few points apart in ground-truth metrics.
This precision enables ARES to guide the develop-
ment and comparison of competitive approaches
and configurations.
We make the ARES code and datasets publicly
available on Github.
2 Related Work
RAG (Guu et al., 2020; Lewis et al., 2020; Khat-
tab et al., 2021; Izacard et al., 2022)) is now a
common strategy for bolstering LLMs by combin-
ing them with retrieval systems. Through retrieval,
RAG helps LM systems gather domain-specific
knowledge, ground generations in factual informa-
tion (Shuster et al., 2021; Huo et al., 2023), and
offer a degree of transparency or interpretability
via citing sources (Mialon et al., 2023).
Multiple LLM-based evaluation techniques have
emerged for gauging LLM systems. This is essen-
tial for rapid deployment in new settings, where it
is difficult to build a traditional benchmark dataset
from scratch. Early attempts at this use LLMs
out of the box, as in MT-Bench and Chatbot
Arena (Zheng et al., 2023). AutoCalibrate (Liu
et al., 2023b) seeks to align an LLM-judge with
human preferences, leveraging a self-refinement
prompt to iteratively improve the LLM judge. How-
ever, AutoCalibrate does not offer any statistical
guarantees for the accuracy of its predictions. Other
work has used LLM prompting to evaluate system
quality across natural language generation tasks,
such as translation, summarization, and dialogue
(Kocmi and Federmann, 2023; Fu et al., 2023; Liu
et al., 2023a; Wang et al., 2023).
In the context of knowledge-intensive NLP tasks,
LLMs have been explored for assessing attribution
and factuality in LLMs (Min et al., 2023; Gekhman
et al., 2023; Yue et al., 2023). New guidelines
like LongEval (Krishna et al., 2023) and datasets
like Hagrid and ALCE (Kamalloo et al., 2023;
Gao et al., 2023) provide resources for analyzing
knowledge-intensive LLM pipelines.
The two most-closely related projects to ARES
are EXAM (Sander and Dietz, 2021) and RA-
GAS (James and Es, 2023). To evaluate RAG sys-
tems, the EXAM metric estimates how many exam
questions a reader (simulated as a QA system) can
answer correctly based on the generated response.
This requires a set of queries with several asso-
ciated sub-questions each, which adds a burden
that ARES does not bring. RAGAS is based on a
handful of heuristic hand-written prompts. These
offer little adaptability to new RAG evaluation set-



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 0):

ARES: An Automated Evaluation Framework for Retrieval-Augmented
Generation Systems
Jon Saad-Falcon
Stanford University ∗
jonsaadfalcon@stanford.edu
Omar Khattab
Stanford University
okhattab@stanford.edu
Christopher Potts
Stanford University
cgpotts@stanford.edu
Matei Zaharia
Databricks and UC Berkeley
matei@databricks.com
Abstract
Evaluating retrieval-augmented generation
(RAG) systems traditionally relies on hand
annotations for input queries, passages to re-
trieve, and responses to generate. We intro-
duce ARES, an Automated RAG Evaluation
System, for evaluating RAG systems along
the dimensions of context relevance, answer
faithfulness, and answer relevance. By cre-
ating its own synthetic training data, ARES
finetunes lightweight LM judges to assess the
quality of individual RAG components. To
mitigate potential prediction errors, ARES uti-
lizes a small set of human-annotated datapoints
for prediction-powered inference (PPI). Across
eight different knowledge-intensive tasks in
KILT, SuperGLUE, and AIS, ARES accurately
evaluates RAG systems while using only a few
hundred human annotations during evaluation.
Furthermore, ARES judges remain effective
across domain shifts, proving accurate even
after changing the type of queries and/or docu-
ments used in the evaluated RAG systems. We
make our code and datasets publicly available
on Github.
1 Introduction
Retrieval-augmented generation (RAG) has be-
come a prominent approach for building user-
facing NLP applications, such as systems for ques-
tion answering (QA), fact-checking, and customer
support (Petroni et al., 2021; Wang et al., 2019).
Typically, a RAG system consists of a retriever and
a downstream language model (LM). Given a user
question, the retriever finds relevant passages from
a corpus and the LM uses these passages to gener-
ate a response. This formulation admits a multitude
of choices: what retrieval model to use, how to di-
vide the documents into retrieval chunks, and how
to prompt or finetune the LM to use the retrieved
information, to name only a few of the simplest
design decisions.
∗Project started during research internship at Databricks
The best design for a RAG system is not neces-
sarily universal across data domains, corpus sizes,
and cost/latency budgets. To tune their own RAG
systems, practitioners traditionally need hand an-
notations for test questions, passages to retrieve
(to assess the retriever), and responses to generate,
labeled specifically for their target domain. Alter-
natively, they may evaluate different approaches in
production by collecting human preferences that
compare the candidate systems. Unfortunately,
both of these strategies demand high expertise and
impose considerable annotation costs.
Model-based evaluation is an inexpensive strat-
egy to test generative output quality (Zheng et al.,
2023). For instance, the open-source RAGAS
framework (James and Es, 2023) prompts an LM
for evaluating the relevance of retrieved informa-
tion and the faithfulness and accuracy of generated
responses. Unfortunately, such strategies currently
rely for evaluation on a fixed set of heuristically
hand-written prompts, offering little adaptability
to various evaluation contexts and no guarantees
about quality.
To evaluate RAG systems rapidly and accu-
rately, we propose ARES, the Automated RAG
Evaluation System. ARES is the first automated
RAG evaluation system to generate tailored LLM
judges for each component of a RAG pipeline, lead-
ing to substantial boosts in evaluation precision and
accuracy compared to existing approaches like RA-
GAS. Furthermore, unlike existing RAG evaluation
systems, ARES provides confidence intervals for
its scoring by leveraging prediction-powered in-
ference (PPI; Angelopoulos et al. 2023). Given a
corpus of documents and a RAG system, ARES
reports three evaluation scores: context relevance
(is the retrieved information pertinent to the test
question), answer faithfulness (is the response gen-
erated by the language model properly grounded
in the retrieved context), and answer relevance (is
the response also relevant to the question). A good
arXiv:2311.09476v2  [cs.CL]  31 Mar 2024



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 4):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:5
Table 1. Relate Work.
Method Dataset Scale Evaluation Metrics Evaluation Method Application Field Ref. Lang.
[27] Based on LangChain Pythondocumentation QA dataset86 Accuracy of answer, Faithfulness ofresponse to the retrieved documentEvaluating retrieval andgeneration consistencyGeneral QA scenarios(Read) Yes EN
[27] PDF documents containingtables and charts5 Accuracy of answer, Faithfulness ofresponse to the retrieved documentEvaluating retrieval andgeneration consistency
Semi-structureddata scenarios(Read) Yes EN
[32] Query and responses(with citations)1450 Fluency, Perceived utility,Citation recall and precisionHuman evaluation Citation(Read) Yes EN
[17, 22] Questions, answersand contexts(with citations)- Fluency, Correctness,Citation quality Self-devised metrics,Human evaluation Citation(Read) Yes EN
[54] Questions, answersand contexts(with citations)1948 Location citation recall, Location precision,The coefficient of variation of citation locationsSelf-devised metricsCitation(Read) Yes EN
[29] Questions, answersand contexts(with citations)3422 Accuracy of factual and non-factual,AUC-PR, and so on Self-devised metrics,Common metrics Citation(Read) Yes EN
[61] Statement-citation Pairs12681 Correlation, Classification performance,Retrieval effectiveness, Faithfulness
Self-devised metrics,Common metrics,Human evaluation
Citation(Read) Yes EN
[7] Paragraphs(with citations)10000 Citation density,The coverage of reference factsSelf-devised metricsCitation(Read) Yes EN
[39] Questions, answersand contexts 200
Categorization ability,Logical/Mathematical reasoning,Complex question solving,Summarization ability
Accuracy Financial services,Legal, Business(Read,Delete) Yes EN
[8] LLM-generated dataset 1000
Noise robustness,Negative rejection,Information integration,Counterfactual robustness
Self-devised metricsGeneral, especiallynews domain(Read,Update) Yes CNEN
[14] — — Context relevance, Groundedness,Answer relevance Analyzing the RAG triadGeneral(Create,Read) No —
[13] — — Faithfulness, Answer relevance,Context relevance Automated evaluationusing LLM prompts General(Create,Read) No —
[44] LLM-generated dataset 150Context relevance, Answer faithfulness,Answer relevance Generating custom LLM judges foreach component of a RAG systemGeneral(Create,Read) No EN
Ours LLM-generated dataset 36166ROUGE, BLEU,bertScore, RAGQuestEvalEvaluating retrieval andgeneration consistency
General(Create,ReadUpdate,Delete) Yes CN
optimization, and post-retrieval processing [20]. Pre-retrieval processing encompasses data trans-
former to enhance text standardization, ensuring factual accuracy, optimizing index structures,
adjusting block sizes, and rewriting query [ 4, 16, 50, 52]. Retrieval model optimization entails
the fine-tuning of domain-specific embedding models and the application of dynamic embedding
techniques [11, 60]. Post-retrieval processing minimizes context length through reranking and
compression operations, aiming to emphasize critical information, diminish noise interference, and
enhance integration and utilization by the generator [37, 53, 55].
Furthermore, to enhance the precision and efficiency of the generator when handling retrieval
content, scholars have undertaken a series of optimization measures. As an illustration, researchers
have devised methods such as Chain-of-Note (CON) for the generator [58]. CON generates contin-
uous reading notes to comprehensively evaluate the relevance of retrieved documents to the posed
question, integrating this information to produce precise final responses. This approach further
enhances the capability of RAG in managing retrieval information, guaranteeing the production of
responses that are simultaneously accurate and pertinent. In specific domains, such as medical and
legal, models undergo fine-tuning to enhance the generator’s performance within those particular
fields [10, 24, 56]. Through the implementation of these methods, the generator can more effectively
process retrieved information and furnish responses that are more accurate and relevant.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 9):

Preprint.
0 50 100 150
Num of training (k)
35
40
45
50
55Perfomance
(a) PopQA
0 100
Num of training (k)
71
72
73
 (b) PubHealth
0 100
Num of training (k)
40
60
 (c) ASQA (prec)
Pop Bio.
S & P 92.5 70.0
ISREL 95.0 90.0
ISSUP 90.0 85.0
(d) Human evaluation on PopQA
and Bio generation.
Figure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect
of the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)
Human analysis on SELF -RAG outputs as well as reflection tokens.
contrary, a larger weight results in lower MAUVE scores: when generation gets longer and more
fluent, there are often more claims that are not fully supported by citations, consistent with findings
by Liu et al. (2023a). Our framework lets practitioners choose and customize models’ behaviors at
test time by adjusting such parameters without requiring additional training.
Efficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval
occurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects
overall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers
of threshold δ (larger δ results in less retrieval) on PubHealth and PopQA. Figure 3c shows that
the model’s retrieval frequencies dramatically change on both datasets. as δ varies. On one hand,
performance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.
Effects of training data size. We conduct an analysis of how the data scale affects the model’s
performance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original
150k training instances, and fine-tune four SELF -RAG 7B variants on those subsets. Then, we compare
the model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF -
RAG trained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models’
performance trained on different amount of data. Across all datasets, increasing data size often shows
upward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do
not observed such significant improvements on Llama2-FT7B when increasing the training data from
50k to 150k. These results also indicate that further expanding the training data of SELF -RAG may
lead to further improvements, although in this work we limit our training data size to 150k.
Human evaluations. We conduct small human evaluations on SELF -RAG outputs, as well as the
reliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio
results. Following Menick et al. (2022), human annotators evaluate S&P, which indicates whether
the model output is plausible (i.e., the output is a reasonable and on-topic response to the question
as if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to
verify the validity of the answer). For S&P, we do not consider the instances where SELF -RAG
predicts irrelevant or no support. We then ask our annotators whether the model-predicted
reflection tokens about ISREL and ISSUP match their inspections (e.g., whether the fully supported
output is supported by the cited evidence). Human annotators find SELF -RAG answers are often
plausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is
consistent with Menick et al. (2022). Human annotators also find ISREL and ISSUP reflection token
predictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated
examples and explanations on assessments.
6 C ONCLUSION
This work introduces SELF -RAG, a new framework to enhance the quality and factuality of LLMs
through retrieval on demand and self-reflection. SELF -RAG trains an LM to learn to retrieve, generate,
and critique text passages and its own generation by predicting the next tokens from its original
vocabulary as well as newly added special tokens, called reflection tokens.SELF -RAG further enables
the tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on
six tasks using multiple metrics demonstrate that SELF -RAG significantly outperforms LLMs with
more parameters or with conventional retrieval-augmented generation approaches.
10



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 5):

111:6 Lyu, et al.
2.2 RAG Benchmarks
When investigating the development and optimization of RAG, the effective evaluation of their
performance becomes a fundamental concern. Table 1 shows some commonly used benchmarks
for evaluating RAG. LangChain provides benchmark tasks, such as LangChain Docs Q&A and
Semi-structured Reports [27], designed to assess various RAG architectures. These datasets are
constructed from snapshots of Python documentation and PDFs containing tables and charts.
They emphasize the model’s capability to handle structured and semi-structured data. Evaluation
standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing
large models for question-answering generation has emerged as a prevalent approach in building
evaluation datasets. For instance, RGB [8] creates its evaluation dataset by gathering recent news
reports and employing LLM to generate relevant events, questions, and answers. Conversely,
ARES [44]. relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
These methods not only showcase the RAG system’s proficiency in handling real-time data but also
illustrate the utility of automation and synthetic data in the evaluation process. For evaluating the
capabilities of models across various professional domains, the Instruct-Benchmark-Tester dataset
encompasses a range of question types, with a particular focus on financial services, legal, and
intricate business scenarios [39].
Depending on whether the evaluation phase incorporates ground truth, metrics of existing
evaluation methods can be categorized into those necessitating reference and those not requiring
it. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by
contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-
Benchmark-Tester [39] employs accuracy score as an evaluation metric, a widely acknowledged
measure of model performance that assesses the extent to which model-generated answers align
with reference answers. The primary objective of RGB [8] is to evaluate whether large models can
effectively utilize external documents to acquire knowledge and generate accurate answers. Its
evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.
Reference-free evaluation methods, including TruLens-Eval [14], RAGAS [13], and ARES [44],
provide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning
context relevance, answer faithfulness, and answer relevance. TruLens-Eval [14] introduces the
RAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,
encompassing context relevance, groundedness, and answer relevance. RAGAS [13], serving as
a reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity
to identify pertinent and concentrated context passages, along with the LLMs’ proficiency in
faithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a
predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each
aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy
when compared to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-
powered inference to offer statistical assurances for its scoring, generating confidence intervals.
ARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer
relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts
and producing both faithful and relevant answers. Regarding evaluation methods, [32] places an
emphasis on assessing the credibility and accuracy of responses generated by generative search
engines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including
high costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,
exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely
approximates manual evaluation outcomes.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 1):

RAG system finds relevant contexts and generates
answers that are both faithful and relevant.
Many existing RAG evaluation frameworks re-
quire substantial human annotations for scoring.
ARES significantly improves data efficiency dur-
ing evaluation by only requiring three inputs: an in-
domain passage set, a human preference validation
set of approximately 150 annotated datapoints or
more, and few-shot examples of in-domain queries
and answers (e.g. five examples or more), which
are used for prompting LLMs in synthetic data gen-
eration.
Given the corpus of in-domain passages, ARES
proceeds in three stages. First, it leverages an LM
to construct a synthetic dataset of question–answer
pairs, derived from the passages in the corpus. Sec-
ond, it defines three separate judge models to per-
form three classification tasks (context relevance,
answer faithfulness, and answer relevance). These
judges are lightweight models fine-tuned against a
contrastive learning objective. Third, ARES scores
the different RAG systems being assessed using
prediction-powered inference (PPI; Angelopoulos
et al. 2023) to improve model-based evaluation ac-
curacy and provide statistical confidence intervals
for RAG scoring. PPI utilizes a small set of human
annotated datapoints for computing its confidence
intervals; we designate this annotated set as our hu-
man preference validation set, which is composed
of approximately 150 annotated datapoints or more
that designate both positive and negative examples
for context relevance, answer faithfulness, and an-
swer relevance.
We conduct extensive empirical evaluations,
demonstrating that ARES accurately scores
RAG systems across the six knowledge-intensive
datasets in KILT and SuperGLUE, beating exist-
ing automated evaluation approaches like RAGAS
by 59.3 and 14.4 percentage points on average
across context relevance and answer relevance eval-
uation accuracy, respectively. Additionally, ARES
accurately calculates answer hallucination occur-
rences in the AIS attribution dataset (Rashkin et al.,
2022), predicting within 2.5 percentage points of
the ground truth average for answer hallucinations.
Compared to annotation-based evaluation methods,
ARES is substantially more accurate and efficient,
requiring 78% less annotations than the baseline
approach. We also find that ARES consistently
distinguishes competitive RAG systems that are
only a few points apart in ground-truth metrics.
This precision enables ARES to guide the develop-
ment and comparison of competitive approaches
and configurations.
We make the ARES code and datasets publicly
available on Github.
2 Related Work
RAG (Guu et al., 2020; Lewis et al., 2020; Khat-
tab et al., 2021; Izacard et al., 2022)) is now a
common strategy for bolstering LLMs by combin-
ing them with retrieval systems. Through retrieval,
RAG helps LM systems gather domain-specific
knowledge, ground generations in factual informa-
tion (Shuster et al., 2021; Huo et al., 2023), and
offer a degree of transparency or interpretability
via citing sources (Mialon et al., 2023).
Multiple LLM-based evaluation techniques have
emerged for gauging LLM systems. This is essen-
tial for rapid deployment in new settings, where it
is difficult to build a traditional benchmark dataset
from scratch. Early attempts at this use LLMs
out of the box, as in MT-Bench and Chatbot
Arena (Zheng et al., 2023). AutoCalibrate (Liu
et al., 2023b) seeks to align an LLM-judge with
human preferences, leveraging a self-refinement
prompt to iteratively improve the LLM judge. How-
ever, AutoCalibrate does not offer any statistical
guarantees for the accuracy of its predictions. Other
work has used LLM prompting to evaluate system
quality across natural language generation tasks,
such as translation, summarization, and dialogue
(Kocmi and Federmann, 2023; Fu et al., 2023; Liu
et al., 2023a; Wang et al., 2023).
In the context of knowledge-intensive NLP tasks,
LLMs have been explored for assessing attribution
and factuality in LLMs (Min et al., 2023; Gekhman
et al., 2023; Yue et al., 2023). New guidelines
like LongEval (Krishna et al., 2023) and datasets
like Hagrid and ALCE (Kamalloo et al., 2023;
Gao et al., 2023) provide resources for analyzing
knowledge-intensive LLM pipelines.
The two most-closely related projects to ARES
are EXAM (Sander and Dietz, 2021) and RA-
GAS (James and Es, 2023). To evaluate RAG sys-
tems, the EXAM metric estimates how many exam
questions a reader (simulated as a QA system) can
answer correctly based on the generated response.
This requires a set of queries with several asso-
ciated sub-questions each, which adds a burden
that ARES does not bring. RAGAS is based on a
handful of heuristic hand-written prompts. These
offer little adaptability to new RAG evaluation set-



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 0):

ARES: An Automated Evaluation Framework for Retrieval-Augmented
Generation Systems
Jon Saad-Falcon
Stanford University ∗
jonsaadfalcon@stanford.edu
Omar Khattab
Stanford University
okhattab@stanford.edu
Christopher Potts
Stanford University
cgpotts@stanford.edu
Matei Zaharia
Databricks and UC Berkeley
matei@databricks.com
Abstract
Evaluating retrieval-augmented generation
(RAG) systems traditionally relies on hand
annotations for input queries, passages to re-
trieve, and responses to generate. We intro-
duce ARES, an Automated RAG Evaluation
System, for evaluating RAG systems along
the dimensions of context relevance, answer
faithfulness, and answer relevance. By cre-
ating its own synthetic training data, ARES
finetunes lightweight LM judges to assess the
quality of individual RAG components. To
mitigate potential prediction errors, ARES uti-
lizes a small set of human-annotated datapoints
for prediction-powered inference (PPI). Across
eight different knowledge-intensive tasks in
KILT, SuperGLUE, and AIS, ARES accurately
evaluates RAG systems while using only a few
hundred human annotations during evaluation.
Furthermore, ARES judges remain effective
across domain shifts, proving accurate even
after changing the type of queries and/or docu-
ments used in the evaluated RAG systems. We
make our code and datasets publicly available
on Github.
1 Introduction
Retrieval-augmented generation (RAG) has be-
come a prominent approach for building user-
facing NLP applications, such as systems for ques-
tion answering (QA), fact-checking, and customer
support (Petroni et al., 2021; Wang et al., 2019).
Typically, a RAG system consists of a retriever and
a downstream language model (LM). Given a user
question, the retriever finds relevant passages from
a corpus and the LM uses these passages to gener-
ate a response. This formulation admits a multitude
of choices: what retrieval model to use, how to di-
vide the documents into retrieval chunks, and how
to prompt or finetune the LM to use the retrieved
information, to name only a few of the simplest
design decisions.
∗Project started during research internship at Databricks
The best design for a RAG system is not neces-
sarily universal across data domains, corpus sizes,
and cost/latency budgets. To tune their own RAG
systems, practitioners traditionally need hand an-
notations for test questions, passages to retrieve
(to assess the retriever), and responses to generate,
labeled specifically for their target domain. Alter-
natively, they may evaluate different approaches in
production by collecting human preferences that
compare the candidate systems. Unfortunately,
both of these strategies demand high expertise and
impose considerable annotation costs.
Model-based evaluation is an inexpensive strat-
egy to test generative output quality (Zheng et al.,
2023). For instance, the open-source RAGAS
framework (James and Es, 2023) prompts an LM
for evaluating the relevance of retrieved informa-
tion and the faithfulness and accuracy of generated
responses. Unfortunately, such strategies currently
rely for evaluation on a fixed set of heuristically
hand-written prompts, offering little adaptability
to various evaluation contexts and no guarantees
about quality.
To evaluate RAG systems rapidly and accu-
rately, we propose ARES, the Automated RAG
Evaluation System. ARES is the first automated
RAG evaluation system to generate tailored LLM
judges for each component of a RAG pipeline, lead-
ing to substantial boosts in evaluation precision and
accuracy compared to existing approaches like RA-
GAS. Furthermore, unlike existing RAG evaluation
systems, ARES provides confidence intervals for
its scoring by leveraging prediction-powered in-
ference (PPI; Angelopoulos et al. 2023). Given a
corpus of documents and a RAG system, ARES
reports three evaluation scores: context relevance
(is the retrieved information pertinent to the test
question), answer faithfulness (is the response gen-
erated by the language model properly grounded
in the retrieved context), and answer relevance (is
the response also relevant to the question). A good
arXiv:2311.09476v2  [cs.CL]  31 Mar 2024



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 4):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:5
Table 1. Relate Work.
Method Dataset Scale Evaluation Metrics Evaluation Method Application Field Ref. Lang.
[27] Based on LangChain Pythondocumentation QA dataset86 Accuracy of answer, Faithfulness ofresponse to the retrieved documentEvaluating retrieval andgeneration consistencyGeneral QA scenarios(Read) Yes EN
[27] PDF documents containingtables and charts5 Accuracy of answer, Faithfulness ofresponse to the retrieved documentEvaluating retrieval andgeneration consistency
Semi-structureddata scenarios(Read) Yes EN
[32] Query and responses(with citations)1450 Fluency, Perceived utility,Citation recall and precisionHuman evaluation Citation(Read) Yes EN
[17, 22] Questions, answersand contexts(with citations)- Fluency, Correctness,Citation quality Self-devised metrics,Human evaluation Citation(Read) Yes EN
[54] Questions, answersand contexts(with citations)1948 Location citation recall, Location precision,The coefficient of variation of citation locationsSelf-devised metricsCitation(Read) Yes EN
[29] Questions, answersand contexts(with citations)3422 Accuracy of factual and non-factual,AUC-PR, and so on Self-devised metrics,Common metrics Citation(Read) Yes EN
[61] Statement-citation Pairs12681 Correlation, Classification performance,Retrieval effectiveness, Faithfulness
Self-devised metrics,Common metrics,Human evaluation
Citation(Read) Yes EN
[7] Paragraphs(with citations)10000 Citation density,The coverage of reference factsSelf-devised metricsCitation(Read) Yes EN
[39] Questions, answersand contexts 200
Categorization ability,Logical/Mathematical reasoning,Complex question solving,Summarization ability
Accuracy Financial services,Legal, Business(Read,Delete) Yes EN
[8] LLM-generated dataset 1000
Noise robustness,Negative rejection,Information integration,Counterfactual robustness
Self-devised metricsGeneral, especiallynews domain(Read,Update) Yes CNEN
[14] — — Context relevance, Groundedness,Answer relevance Analyzing the RAG triadGeneral(Create,Read) No —
[13] — — Faithfulness, Answer relevance,Context relevance Automated evaluationusing LLM prompts General(Create,Read) No —
[44] LLM-generated dataset 150Context relevance, Answer faithfulness,Answer relevance Generating custom LLM judges foreach component of a RAG systemGeneral(Create,Read) No EN
Ours LLM-generated dataset 36166ROUGE, BLEU,bertScore, RAGQuestEvalEvaluating retrieval andgeneration consistency
General(Create,ReadUpdate,Delete) Yes CN
optimization, and post-retrieval processing [20]. Pre-retrieval processing encompasses data trans-
former to enhance text standardization, ensuring factual accuracy, optimizing index structures,
adjusting block sizes, and rewriting query [ 4, 16, 50, 52]. Retrieval model optimization entails
the fine-tuning of domain-specific embedding models and the application of dynamic embedding
techniques [11, 60]. Post-retrieval processing minimizes context length through reranking and
compression operations, aiming to emphasize critical information, diminish noise interference, and
enhance integration and utilization by the generator [37, 53, 55].
Furthermore, to enhance the precision and efficiency of the generator when handling retrieval
content, scholars have undertaken a series of optimization measures. As an illustration, researchers
have devised methods such as Chain-of-Note (CON) for the generator [58]. CON generates contin-
uous reading notes to comprehensively evaluate the relevance of retrieved documents to the posed
question, integrating this information to produce precise final responses. This approach further
enhances the capability of RAG in managing retrieval information, guaranteeing the production of
responses that are simultaneously accurate and pertinent. In specific domains, such as medical and
legal, models undergo fine-tuning to enhance the generator’s performance within those particular
fields [10, 24, 56]. Through the implementation of these methods, the generator can more effectively
process retrieved information and furnish responses that are more accurate and relevant.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 9):

Preprint.
0 50 100 150
Num of training (k)
35
40
45
50
55Perfomance
(a) PopQA
0 100
Num of training (k)
71
72
73
 (b) PubHealth
0 100
Num of training (k)
40
60
 (c) ASQA (prec)
Pop Bio.
S & P 92.5 70.0
ISREL 95.0 90.0
ISSUP 90.0 85.0
(d) Human evaluation on PopQA
and Bio generation.
Figure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect
of the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)
Human analysis on SELF -RAG outputs as well as reflection tokens.
contrary, a larger weight results in lower MAUVE scores: when generation gets longer and more
fluent, there are often more claims that are not fully supported by citations, consistent with findings
by Liu et al. (2023a). Our framework lets practitioners choose and customize models’ behaviors at
test time by adjusting such parameters without requiring additional training.
Efficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval
occurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects
overall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers
of threshold δ (larger δ results in less retrieval) on PubHealth and PopQA. Figure 3c shows that
the model’s retrieval frequencies dramatically change on both datasets. as δ varies. On one hand,
performance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.
Effects of training data size. We conduct an analysis of how the data scale affects the model’s
performance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original
150k training instances, and fine-tune four SELF -RAG 7B variants on those subsets. Then, we compare
the model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF -
RAG trained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models’
performance trained on different amount of data. Across all datasets, increasing data size often shows
upward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do
not observed such significant improvements on Llama2-FT7B when increasing the training data from
50k to 150k. These results also indicate that further expanding the training data of SELF -RAG may
lead to further improvements, although in this work we limit our training data size to 150k.
Human evaluations. We conduct small human evaluations on SELF -RAG outputs, as well as the
reliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio
results. Following Menick et al. (2022), human annotators evaluate S&P, which indicates whether
the model output is plausible (i.e., the output is a reasonable and on-topic response to the question
as if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to
verify the validity of the answer). For S&P, we do not consider the instances where SELF -RAG
predicts irrelevant or no support. We then ask our annotators whether the model-predicted
reflection tokens about ISREL and ISSUP match their inspections (e.g., whether the fully supported
output is supported by the cited evidence). Human annotators find SELF -RAG answers are often
plausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is
consistent with Menick et al. (2022). Human annotators also find ISREL and ISSUP reflection token
predictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated
examples and explanations on assessments.
6 C ONCLUSION
This work introduces SELF -RAG, a new framework to enhance the quality and factuality of LLMs
through retrieval on demand and self-reflection. SELF -RAG trains an LM to learn to retrieve, generate,
and critique text passages and its own generation by predicting the next tokens from its original
vocabulary as well as newly added special tokens, called reflection tokens.SELF -RAG further enables
the tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on
six tasks using multiple metrics demonstrate that SELF -RAG significantly outperforms LLMs with
more parameters or with conventional retrieval-augmented generation approaches.
10



### Claim 79/179

#### Claim Text
Cuconasu et al. [54] analyze which type of documents should be retrieved, evaluate the relevance of the documents to the prompt, their position, and the number included in the context.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[62]_2307.11019.pdf (Page 6):

Judgemental PromptingQA Prompting
Question
Retrieval-augmented
judgement
Retrieval- 
augmented  
setting
Normal  
setting
Not give up
w/o judgement
Retrieval- 
augmented  
setting
Give up
Ans
Ans
EM: 37.81 
 F1: 50.18
Ans
Question
QA Evaluation
Normal judgement
Retrieval- 
augmented  
setting
Normal  
setting
Not give up
Give up
Ans
Ans
EM: 34.04 
 F1: 45.83Question
EM: 35.79 
 F1: 47.68
Figure 3: A simple method that dynamically introduces
retrieval for LLMs based on priori judgement strategy.
We use ChatGPT with QA prompting under the retrieval-
augmented setting as the baseline (w/o judgement).
aries, the Eval-Acc of LLaMA2 and Mistral both
decline as the document number increases. We also
find an increase in the confidence level (Give-up)
of most models with an increase in the number of
supporting documents, except for LLaMA2.
In addition to the above findings, we also ex-
plore the impact of retrieval augmentation on dif-
ferent query types and LLMs with various param-
eter quantities, and obtain the following findings:
(1) Increasing the number of supporting docu-
ments improves the performance of LLMs below
a model-specific threshold. (2) Retrieval aug-
mentation is more pronounced for improving
LLMs with fewer parameters, including both
QA performances and accuracies of knowledge
boundary perception. Due to the limited space,
the comprehensive analysis of the two findings can
be found in Appendix B.2 and B.3.
3.2.2 Dynamic Retrieval Augmentation
In order to further investigate the observed im-
provement, we examine with a simple method
that employs priori judgementwith either the nor-
mal or the retrieval-augmented settings to deter-
mine whether to introduce retrieval augmentation.
Specifically, if the LLM considers a question is
challenging to answer under the current setting,
supporting documents are introduced to help pro-
vide an answer, otherwise the question will be an-
swered without supporting documents. We conduct
experiment on ChatGPT, using supporting docu-
ments from the dense retriever.
Figure 3 compares different judgemental settings
for decision-making to dynamically incorporate re-
trieval augmentation with the corresponding results
on NQ. It is evident that employing the priori judg-
ment with ChatGPT within the standard decision-
making framework results in decreased answering
accuracy when compared to the baseline that al-
ways utilizes retrieval augmentation. Nevertheless,
upon integrating retrieval augmentation for judg-
ing, the accuracy surpasses the baseline. The result
indicates that it is a promising way to dynamically
introduce supporting documents for LLMs accord-
ing to its retrieval-augmented priori judgement. It
further shows that the incorporation of retrieval
augmentation can improve LLMs’ awareness of
their factual knowledge boundaries.
3.3 How do Different Relevance Supporting
Document Affect LLMs?
We have explored the effect of retrieval augmen-
tation on LLMs. Actually, the retrieval results con-
sist of documents with varying relevance, which
might lead to different effects. For this purpose, we
continue to study how different relevance of sup-
porting documents influence LLMs. In our experi-
ments, we consider the following perspectives, the
relevance between the document and the question,
and the presence of an answer within the document.
3.3.1 Sampling Strategies
In order to thoroughly study the impact of sup-
porting documents on LLMs, we propose to pro-
vide LLMs with supporting documents of different
characteristics for obtaining answers. Golden doc-
uments refer to documents containing at least one
correct answer to the question, which are sampled
from top to bottom in the top 100 retrieval results of
the question; High-related incorrect documentsre-
fer to documents that are highly relevant to the ques-
tion but do not contain the correct answer. They
are also sampled from top to bottom in the top
100 retrieval results of the question; Weak-related
incorrect documentsare the documents weakly rel-
evant to the query and do not contain the correct
answer. We randomly sample documents from the
top 100 retrieval results of the question exclud-
ing high-related incorrect documents; Random in-
correct documentsrefer to documents randomly
sampled from the entire corpus D, which do not
contain the correct answers to the given question.



Source: data\tc16_2312.10997v5\referenced_papers\[54]_2401.14887.pdf (Page 4):

The Power of Noise: Redefining Retrieval for RAG Systems SIGIR ’24, July 14–18, 2024, Washington, DC, USA
accurate or inaccurate based on the presence of the answer in a
binary fashion. Nevertheless, this evaluation strategy is not without
challenges. A principal issue arises in determining response cor-
rectness, particularly in instances involving date representations
or varying phrasings conveying identical meanings. For example,
if the LLM generates “Roosevelt” in response to a query where the
established correct answer is “President Roosevelt”, the response
would be deemed incorrect under our current evaluation schema.
Recognizing this limitation, we acknowledge the necessity for a
more advanced analysis of answer variations, which we leave to
future research.
5 RESULTS
Studying the characteristics of optimal prompts for RAG systems
corresponds to answering our research question (RQ): "What char-
acteristics are desirable in a retriever to optimize prompt construction
for RAG systems in order to increase the LLM effectiveness?" . More
specifically, we focus on three essential elements of the configura-
tion: type, number, and positioning of the documents, and for each,
we test various prompt combinations. To facilitate the understand-
ing of our experimental setup, we employ a streamlined schema for
representing the composition of prompts via the following symbols:
[I, ⋆, ὑ7, /u♀k, , Q]. The task instruction (I) and the query (Q) are
consistently positioned at the beginning and end, respectively. The
middle section varies and represents different contextual elements
- in this instance, these are gold, relevant, distracting, and random,
appearing in that specific sequence. Additionally, the number of
contextual documents is a variable in its own right and will be
reported in the results tables below.
5.1 Impact of Distracting Documents
LLM Input - Distracting /u♀kand Gold ⋆
Task Instruction...
Documents:
Document [1](Title: Han Solo) Before the events of the
film, he and Chewbacca had lost the “Millennium Falcon”
to thieves, but they reclaim the ship after it...
Document [2](Title: Millennium Falcon) The “Falcon” has
been depicted many times in the franchise, and ownership
has changed several times...
Document [3](Title: Millennium Falcon) Han Solo won
the Millennium Falcon from Lando Calrissian in the card
game sabacc...
Question: who owned the millennium falcon be-
fore han solo
Answer: Han Solo
Figure 2: Example LLM input with an erroneous output, high-
lighted in red. The context of the prompt is composed of
distracting documents and the gold near the query. The task
instruction is as in Figure 1.
In our first set of experiments, we use a selection of 10K queries
from the training set of the NQ-open dataset and assume an oracle
setup in which the gold document for the query is known. To this
effect, we add to the gold document a set of distracting documents,
i.e., documents with high retrieval scores but not containing the
answer, in order to measure their impact on the system; schemat-
ically [I, /u♀k, ⋆, Q] . Figure 2 shows an example of this setup’s
visualization. Results of this experiment are shown in Table 1 (far,
mid, and near relate to the distance between the gold document
and the query; more details in the following sub-section). A crit-
ical observation emerging from this analysis is a clear pattern of
progressive accuracy degradation as the number of distracting doc-
uments included in the context increases. This was observed across
all LLMs, with accuracy deteriorating by more than 0.38 ( −67%)
in some cases. Even more importantly, adding just one distracting
document causes a sharp reduction in accuracy, with peaks of 0.24
(−25%), as can be seen by comparing the row with 0 distracting
documents (only gold scenario, as seen in Figure 1) with that of 1
distracting document. This experiment highlights a critical issue
for RAG systems, particularly in real-world IR settings where re-
lated but non-answer-containing documents are commonplace. Our
empirical analysis suggests that introducing semantically aligned
yet non-relevant documents adds a layer of complexity, potentially
misguiding LLMs away from the correct response. A visual ex-
planation can be seen in Figure 3, which illustrates the attention
scores within the prompt’s context for a specific example in which
the LLM incorrectly answers. This figure highlights the model’s
disproportionate focus on a distracting document (leftmost) at the
expense of the gold document (rightmost), likely contributing to
the erroneous response. Note that for consistency of results across
LLMs, we need to account for their various input token capabilities:
Llama2 can process up to 4096 tokens, but other models are lim-
ited to 2048 tokens. This led to the exclusion of evaluations with a
higher number of distracting documents (namely greater than 10)
as reflected by the empty values in the tables.
In addition, we wanted to verify that our results were not overly
dependent on the type of dense retrieval system we used. We
wanted, in particular, to check whether another dense retriever
specifically trained on “hard negatives" would better distinguish
between directly relevant and distracting documents, potentially
leading to different results. To explore this hypothesis, we used
ADORE [59], a state-of-the-art retriever trained with “dynamic
hard negatives”, to select the distracting documents. In scenarios
with 1, 2, and 4 distracting documents in the [I, /u♀k, ⋆, Q] setting
with Llama2, we obtain an accuracy of 0.4068, 0.3815, and 0.3626,
respectively. This is significantly lower than the baseline accuracy
of 0.5642, where no distracting documents were included, and than
the results obtained with Contriever in the same settings. We con-
clude from this that distinguishing between relevant and distracting
information is a hard problem that cannot be mitigated simply by
changing the dense retrieval method at this stage.
5.2 Impact of Gold Positioning
We conduct here another experiment where we systematically shift
the position of the gold document within the context to study its



Source: data\tc16_2312.10997v5\referenced_papers\[62]_2307.11019.pdf (Page 14):

Setting Accessibility Prompt Text
Normal
Closed source
Without additional specific information, use your existing knowledge to answer
the following question. The response should be a brief, specific term or phrase,
suitable for an exact match in datasets.\n\n Question: {question}\n\n Note: Since
no detailed context is provided, use your general knowledge to infer a concise and
accurate response, such as a specific year, date, or single-word term, for example,
“1998”, “May 16th, 1931”, or “James Bond”, in line with exact match dataset
criteria.
Publicly available
Answer the following question with a very short phrase, such as "1998", "May
16th, 1931", or "James Bond", to meet the criteria of exact match datasets.\n\n
Question: {question}
Retrieval-
augmented
Closed source
Given the following information:\n\n {context}\n\n If a direct answer is not present,
use your knowledge to infer a brief and specific response for the question below.
The answer should ideally be a single word or a short phrase.\n\n Question:
{question}\n\n Note: In cases where the exact information is not provided, a
speculative yet plausible and concise response, such as a specific year, date, or
single-word term, for example, “1998”, “May 16th, 1931”, or “James Bond”, is
required to meet the criteria of exact match datasets.
Publicly available
Given the following information: \n\n {context} \n\n Answer the following ques-
tion with a very short phrase, such as “1998”, “May 16th, 1931”, or “James Bond”,
to meet the criteria of exact match datasets.\n\n Question: {question}
Table 5: Prompt design for QA prompting for different categories of LLMs with various settings.
Perspectives Setting Prompt Text
Priori
Normal
Can you answer the following question based on your internal knowledge, if yes,
you should give a short answer with one or few words, if no, you should answer
“Unknown”\n\n Question: {question}
Retrieval-
augmented
Given the following information: \n\n {context}\n\n Can you answer the following
question based on the given information or your internal knowledge, if yes, you
should give a short answer with one or few words, if no, you should answer “Un-
known”.\n\n Question: {question}
Posteriori
Normal
Can you judge if the following answer about the question is correct based on your
internal knowledge, if yes, you should answer True or False, if no, you should answer
“Unknown”.\n\n Question: {question}\n\n Answer: {predicted answer}
Retrieval-
augmented
Given the following information: \n\n {context}\n\n Can you judge if the following
answer about the question is correct based on the given information or your internal
knowledge, if yes, you should answer True or False, if no, you should answer
“Unknown”.\n\n Question: {question} \n\n Answer: {predicted answer}
Table 6: Prompt design for different perspectives of judgemental prompting with various settings.
B Supplement Analyses
B.1 Analysis on Retrieval Sources
Using supporting documents with high-
efficiency contents can effectively improve the
QA performance of LLMs. We conduct the
analysis on three retrieval sources and utilize
various metrics related to retrieval and QA in
Figure 4 to thoroughly analyze the impact of
retrieval augmentation from multiple perspectives.
Here, the efficiency of documents can be evaluated
through three factors: the presence of positive
content (recall rate), proportion of positive
documents in supporting documents (positive
percentage), and token-level text density of positive
content (answers per 1K token). Firstly, the recall
rate of documents generated by ChatGPT was
significantly lower than that of the dense retriever.
However, the QA performance using supporting
documents generated by ChatGPT as supporting
documents do not lag far behind dense retrieval.
Additionally, LLaMA2’s retrieval-augmented QA
performance with ChatGPT as the retriever exceeds
that where the dense retriever is implemented. We
observe that among the documents from the three
retrieval sources, dense retrieval results have the
highest average proportion of positive documents
(containing the correct answers), but the number of
answers per token was not as high as in documents
generated by ChatGPT. This phenomenon stems



Source: data\tc16_2312.10997v5\referenced_papers\[3]_2310.20158.pdf (Page 15):

APPENDIX
A P ROMPTS
A.1 R EWRITE PROMPT
system:
You are an AI assistant that helps people find information.
user:
I am using a search engine to find relevant documents related to the given TOPIC. The search
engine doesn’t work very well.
I will give you the top search results for various QUERIES that I tried.
You should suggest me other topics that I should search in order to find more interesting
documents relevant to the TOPIC.
Since the search engine mostly does lexical matching, it could be weak in retrieving docu-
ments containing some words. Use those words to improve the overall search quality.
Also, use your own knowledge and understanding of the TOPIC to generate rewrites related
to topics which might not be present in the retrieved documents.
Enclose the answer in <Rewrite></Rewrite>. Do not give any explanation.
TOPIC: {{ query q }}
QUERY #1: {{ rewrite q1 }}
TOP RESULTS:
1. {{ Filtered Document 1 F1,1 }}
2. {{ Filtered Document 2 F1,2 }}
3. {{ Filtered Document 1 F1,3 }}
QUERY #2: {{ rewrite q2 }}
TOP RESULTS:
1. {{ Filtered Document 1 F2,1 }}
2. {{ Filtered Document 1 F2,2 }}
3. {{ Filtered Document 1 F2,3 }}
A.2 R ELEVANCE PROMPT
system:
You are an AI assistant that helps people find information.
user:
Given a QUERY and a DOCUMENT, score the DOCUMENT on a scale of 1(least relevant
to QUERY) to 5(most relevant to QUERY).
Enclose the answer in <Score></Score>. For instance if you think the score should be 4,
then answer <Score>4</Score>. Do not give any explanation.
QUERY:{{ rewrite qi }}
DOCUMENT: {{ Retrieved document Rj }}
16



Source: data\tc16_2312.10997v5\referenced_papers\[54]_2401.14887.pdf (Page 5):

SIGIR ’24, July 14–18, 2024, Washington, DC, USA Cuconasu and Trappolini, et al.
Table 1: Accuracy results of the LLMs when evaluated with prompts composed of the gold document ⋆and a varying number
of distracting /u♀kdocuments. The table illustrates how the inclusion of an increasing number of distracting documents affects
LLM’s performance. Scenarios where the prompt exceeded the model’s input limit, leading to potential data truncation, are not
included ( - ). All values not marked with an asterisk * denote statistically significant changes from the gold-only document
scenario [I, ⋆, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01). Additionally, the closed-book accuracy scores
for the models are as follows: Llama2 (0.1123), MPT (0.1205), Phi-2 (0.0488), Falcon (0.1083).
Far - [I, ⋆, /u♀k, Q] Mid - [I, /u♀k, ⋆, /u♀k, Q] Near - [I, /u♀k, ⋆, Q]
# /u♀kLlama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon
0 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330
1 0.4586 0.1976 0.3585 0.3469 no-mid no-mid no-mid no-mid 0.4283 0.1791 0.4227 0.3602
2 0.3455 0.1913 0.3430 0.3246 0.3322 0.1802 0.3375 0.2823 0.3974 0.2002 0.3975 0.3111
4 0.2745 0.2209* 0.3019 0.2670 0.2857 0.1775 0.2885 0.2378 0.3795 0.2059* 0.3701 0.2736
6 0.2898 0.2171* 0.2943 0.2392 0.2698 0.1424 0.2625 0.2103 0.3880 0.1892 0.3623 0.2656
8 0.2643 0.2077* 0.2513 0.1878 0.2268 0.1002 0.2360 0.1745 0.3748 0.1944 0.3423 0.2424
10 0.2537 - - - 0.2180 - - - 0.3716 - - -
12 0.2688 - - - 0.2382 - - - 0.3991 - - -
14 0.2583 - - - 0.2280 - - - 0.4118 - - -
16 0.2413 - - - 0.2024 - - - 0.3889 - - -
18 0.2348 - - - 0.1795 - - - 0.3781 - - -
Table 2: Accuracy results of the LLMs when evaluated with prompts composed of the gold document ⋆and a varying number
of random documents. Surprisingly, increasing the number of random documents in the Near setting improves LLM’s
performance. Scenarios where the prompt exceeded the model’s input limit, leading to potential data truncation, are not
included ( - ). All values not marked with an asterisk * denote statistically significant changes from the gold-only document
scenario [I, ⋆, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01). Additionally, the closed-book accuracy scores
for the models are as follows: Llama2 (0.1123), MPT (0.1205), Phi-2 (0.0488), Falcon (0.1083).
Far - [I, ⋆, , Q] Mid - [I, , ⋆, , Q] Near - [I, , ⋆, Q]
# Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon
0 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330
1 0.4733 0.2447 0.4329 0.4035 no-mid no-mid no-mid no-mid 0.4862 0.2125* 0.4587 0.4091
2 0.3776 0.2639 0.4249 0.3805 0.3928 0.2584 0.4293 0.3612 0.5032 0.2660 0.4614 0.3912
4 0.3109 0.2933 0.4091 0.3468 0.3998 0.2577 0.3985 0.3462 0.5221 0.2930 0.4311 0.3949
6 0.3547 0.3036 0.4130 0.3250 0.4138 0.2265 0.3891 0.3196 0.5681* 0.2890 0.4388 0.3908
8 0.3106 0.3039 0.3812 0.2543 0.3734 0.1566 0.3596 0.2767 0.5609* 0.2911 0.4258 0.3704
10 0.3390 - - - 0.3675 - - - 0.5579* - - -
12 0.3736 - - - 0.3641 - - - 0.5836 - - -
14 0.3527 - - - 0.3372 - - - 0.5859 - - -
16 0.3401 - - - 0.3159 - - - 0.5722 - - -
18 0.3466 - - - 0.2982 - - - 0.5588* - - -
impact on the model’s effectiveness. We define the positions of the
gold document as follows:
•Near: placed adjacent to the query in the prompt [I, /u♀k, ⋆,
Q] (as in Figure 2)
•Mid: inserted in the middle of the context [I, /u♀k, ⋆, /u♀k, Q]
•Far: positioned as far as possible from the query in the con-
text [I, ⋆, /u♀k, Q]
Results in these settings partially corroborate evidence from [30].
The accuracy is higher when the gold document is near the query,
lower when the gold document is furthest from it, and lowest when
the gold document is placed in the middle of the context. For in-
stance, Llama2, with 18 distracting documents, reaches an accuracy
of 0.37, 0.23, and 0.17, respectively. These results are consistent
across all models tested in the setting with distracting documents.
5.3 Impact of Noise
We devise an additional experimental setting aimed at evaluating
the robustness of the RAG system against noise. To this effect, we
take the gold document and add to it a certain number of docu-
ments picked at random from the corpus; see an example in Figure
4. Against our expectations, the performance does not deteriorate



### Claim 80/179

#### Claim Text
Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these metrics, as required. and non-parameterized advantages are areas ripe for exploration [27].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[66]_2210.00185.pdf (Page 7):

Ablated Zemi Changed Tasks Avgsetting value value OBQA Piqa RT CB COPA WiC HSwag
No Augmentation (No AugBASE) 36.6 60.2 64.1 41.5 68.5 49.9 28.0 49.8
ZemiBASE 35.6 59.2 68.6 50.1 63.6 49.6 29.7 50.9
(i) Tanh Gate ✓ ✗ 35.0 57.8 55.8 49.9 71.5 51.6 27.9 49.9
(ii) 5
1 35.6 58.9 67.1 47.6 65.2 49.5 30.1 50.6
Num of 10 35.3 59.4 62.1 46.3 64.6 51.4 29.4 49.8
Augs 20⋆ 34.7 58.7 60.3 46.5 61.6 50.1 28.4 48.6
30⋆ 35.1 60.5 58.7 48.2 67.2 50.7 28.5 49.8
(iii) Latent 64 32 34.9 58.8 64.3 44.5 67.9 51.2 28.6 50.0
size 128 34.7 57.6 63.1 47.8 69.8 50.4 28.1 50.2
(iv) Aug length 256 512 35.3 58.8 58.6 52.5 68.9 50.3 28.8 50.5
(v) Training Zemi No Aug+ 37.6 58.4 - 43.3 - 50.7 28.0 -
mixture Zemi+ 34.5 58.7 - 42.8 - 50.1 29.3 -
Table 3: Ablation study. Each ablated setting should be compared with the first two rows, i.e., the original No
Augmentation (No AugBASE) setting and ZemiBASE. The superscripted “⋆” in ablated setting (ii) indicates using the
model variant with a frozen augmentation encoder. See descriptions of each setting in Section 3.4.
Tomatoes (RT) (Pang and Lee, 2005) 7. We find
that adding new types of tasks does not necessarily
increase the performance. Although trained with
only 8 tasks (v.s. 36 tasks) we are able to achieve
state-of-the-art performance (Section 3.2), which
shows that the multiple-choice QA mixture is
highly effective for generalizing to a wide range
of held-out unseen tasks.
3.5 Analysis: computation overheads
There are two main computation overheads com-
pared with the fully-parametric counterpart, i.e., the
No Aug baseline. First, retrieving from a large-scale
corpus can be time-consuming. As mentioned in
Section 2.2, we apply document-level retrieval with
BM25 and truncation on the query to reduce the
retrieval time. We also perform the retrieval offline
to avoid repeated time commitment. As a result,
indexing 5% of the C4 corpus takes 1 hour. Offline
retrieval for the entire training and evaluation mix-
ture takes 11 hours, which is approximately 0.28
seconds per instance. Furthermore, we measure
the computation overhead on inference which is
caused by the additional retrieved inputs as well
as a small amount of newly introduced parameters
(+4.6%). The average computation overhead across
all evaluation datasets during inference is around
7We follow T0 to move tasks that are originally in the
evaluation split, i.e. COPA and RT, into the training split in
this ablated setting.
4x compared with the No Aug baseline. Notably,
Table 2 shows that ZemiBASE achieves competitive
performance with T0-3B while being 15x smaller
in scale, indicating that the benefit of the retrieval
augmentation overwhelms the computation over-
head.
4 Related Work
4.1 Semi-parametric models
Semi-parametric models (Sun et al., 2021; Verga
et al., 2021; Chen et al., 2017; Lee et al., 2019; Guu
et al., 2020; Wang et al., 2019; Karpukhin et al.,
2020; Yang et al., 2019; Lewis et al., 2020; Izac-
ard and Grave, 2020), which augmenting a para-
metric neural network with external knowledge
bases or text corpora, have been widely applied
to knowledge-intensive NLP tasks such as open-
domain question answering. Recent advancements
in semi-parametric language models (Khandelwal
et al., 2019; Yogatama et al., 2021; Borgeaud et al.,
2021; Zhong et al., 2022) have demonstrated im-
proved language modeling performance with a rela-
tively small language model and a retrieval system
based on a large-scale corpus. Although the afore-
mentioned semi-parametric language models have
shown competitive performance on language mod-
eling, compared with fully-parametric counterparts
such as GPT-3 (Brown et al., 2020), it is unclear
whether the superiority generally holds on down-



Source: data\tc16_2312.10997v5\referenced_papers\[18]_2203.08773.pdf (Page 1):

or non-parametric methods.
In the experiments, we evaluate our method
on four popular types of NLP tasks: summariza-
tion, language modeling, machine translation, and
question answering. We ﬁnd that i) after inte-
grating REINA, we can achieve signiﬁcantly bet-
ter performance on these tasks, 11 datasets in to-
tal, than models with different pre-trained mod-
els; ii) REINA leads to SOTA performance on
the datasets of XSum, CommonsenseQA (Leader-
board No.1), and BigPatent; iii) REINA can scale
up more easily by leveraging more labeled data
from other datasets via retrieval, outperforming
baselines which is trained on the same set of data.
iv) the results on 3 summarization tasks show that
BART-base with REINA rivals BART-large, which
contains twice more parameters now.
The effectiveness of our approach on summa-
rization tasks provides insights into the core of
supervised learning. Even with hundreds of mil-
lions of parameters, a model cannot memorize all
the patterns in the training data. Thus, recapturing
related training data as a side-by-side reminder can
explicitly provide needed information to enhance
the model’s performance at inference. It also points
out that instead of building models of ever increas-
ing sizes, we can make a decent-size model output
high-quality results by leveraging those training
data that resemble the instance at hand. This can
signiﬁcantly reduce the computational cost while
achieving a similar or better performance of a mega-
sized model.
2 Related Work
Retrieval-based Methods Even a pre-trained
model as large as GPT-3 (Brown et al., 2020) can-
not remember everything, and it is important to
leverage information retrieval to collect external
knowledge to solve different NLP tasks. There are
two types of representations for retriever: bag-of-
word (BOW) based sparse representation (Chen
et al., 2017) and dense representation from neural
networks (Karpukhin et al., 2020).
For the sparse representation, as the method
is based on BOW and usually rule-based score,
such as BM25, is used for ranking, it can be eas-
ily adapted to a general large-scale search. This
method has also been widely explored to solve
open domain question answering (Chen et al., 2017;
Wang et al., 2018; Lin et al., 2018) and Machine
Translation (Gu et al., 2018).
Dense representation based retrieval
(DPR) (Karpukhin et al., 2020) is the most
widely explored area in recent years. Dense
representations come from encoders, such as
Transformer, trained with task-speciﬁc data.
And these methods can achieve better recall
performance than sparse representation on
different tasks, such as open domain question
answering (Karpukhin et al., 2020; Guu et al.,
2020; Yu et al., 2021), knowledge-grounded
generation (Zhang et al., 2021), and machine
translation (Cai et al., 2021). One drawback of
DPR is that it cannot process longer documents,
usually less than 128 tokens (Karpukhin et al.,
2020). Another drawback is that it needs parallel
data for model training on speciﬁc tasks.
Considering the generalization and efﬁciency of
sparse representation, in this paper, we use BM25
score (Robertson and Zaragoza, 2009; Schütze
et al., 2008) to retrieve from the training data, and
our method is more ﬂexible with no requirement of
parallel data for model training. Compared to non-
parametric systems guided by search engine (Gu
et al., 2018; Khandelwal et al., 2020), our proposed
method is based on supervised learning and is more
general. Lewis et al. (2021) is related to our work
by retrieving related questions from pre-built large-
scale question-answer pairs. However, our method
doesn’t need addition data augmentation method,
and we have successfully applied REINA to a wide
range of downstream tasks, including summariza-
tion, question answering, machine translation and
language modeling.
Prompt Engineering With the success of large-
scale language models (Brown et al., 2020) on few-
shot learning, prompt engineering comes to be a
popular research direction. The idea is to prepend
several labeled instances to the input sequence and
then conduct the classiﬁcation or generation. Liu
et al. (2021) proposes to prepend the most related
labeled data as prompt to help fewshot inference.
Li and Liang (2021) optimizes the prompt in con-
tinuous space. Motivated by these works where a
good labeled prompt can help fewshot learning, we
also prepend/append the most similar labeled train-
ing data for all the data in training, validation, and
test set. However, different from prompt learning,
we focus on supervised learning settings.



Source: data\tc16_2312.10997v5\referenced_papers\[180]_2301.02736.pdf (Page 1):

been explored in prior work.
Our key contributions are three-fold:
1. We outline the ﬁrst method (to our knowledge) to lever-
age large-scale text data for contextual biasing of the
speech encoder.
2. We show that our approach combined with an approxi-
mate K-NN lookup yields improved WER on ASR mod-
els, particularly in scenarios where encoded catalogs
matches the target domain.
3. We show that our approach can provide an accurate
solution under the constraint of quick reactions to dis-
tribution changes (e.g., fast catalog updates for sporting
events, changes in personal catalogs), without any model
retraining.
2 Related Work
Leveraging additional context to improve the performance
of the decoder and joint model, particularly through deep
fusion techniques in ASR transducers has been relatively well
studied [9, 10, 11, 12, 13, 14, 15, 16, 17]. The majority of
these approaches are composed of two components: (1) a
method for retrieving local contextual clues for an utterance
and (2) a method for fusing these contextual clues with the
joint model or decoder in the transducer stack, and differ based
on how they implement these two components. Our work has
several key differentiating factors. We primarily focus on deep-
biasing of the speech encoder, rather than a shallow fusion of
a context network with the speech decoder, or re-ranking of
candidates produced by the language model. While in theory,
approaches like Sathyendra et al.[18] could be applied as deep
fusion approaches, those directions are left unexplored in the
referenced work, and indeed, as we demonstrate in Table 2,
deep fusion is an important differentiating factor in the quality
of the contextual learning technique. Additionally, we focus on
large contexts (> 10K context entries), in an effort to explore
applications in domain specialization, whereas existing biasing
methods are often focused on personalization, and operate on
contexts of at most 1K context entries (and in many cases,
operate on < 100 context entries). Again, while in theory the
above methods could be applied to larger contexts, we ﬁnd
that there is a strong implementational gap required, including
work in scaling, and questions about efﬁciency (which we
address in section 4).
While late-stage fusion and biasing of the decoder and joint
model have been well explored, biasing the speech encoder it-
self, particularly using early/deep-fusion approaches, remains
an under-explored area of research. The closest work to our
proposed model was presented by Chen et al. [19], who use
attention over a local set of LSTM-based grapheme/phoneme
embeddings to augment the audio encoder. They found that
biasing the encoder with only 40 contextual text entities per ut-
terance leads to improvements of up to 75% on specialized test
datasets. Similarly, Sathyendra et al. [18] and Chang et al. [20]
demonstrate WER reductions when small (<100) contexts are
fused in an attention-based process with both the speech and
language model. Our method differs in that it is designed
primarily for domain specialization, whereas existing biasing
methods are focused on personalization. This is shown fore-
most in the scale of the catalogs – while in prior work, each
utterance may have at most 100 utterances in their context, we
leverage catalogs with up to 10M samples. Thus, our models
are designed to compensate for general domain shift, rather
than local personalized improvements to ASR performance.
Additionally, our work allows fully off-policy specialization.
In existing works, context is re-encoded during each inference
pass, leaving few opportunities for caching intermediate re-
sults. Our contexts are computed entirely ofﬂine; updates to
the catalog do not impact model training, and thus, generating
specialized catalogs can thus be done in a parallel work-stream
from model development.
It is not unreasonable to believe that transformers in ASR
will beneﬁt from extended contexts. In the NLP ﬁeld Vaswani
et al. [21] showed that with longer memory attention contexts,
transformers perform better, and a family of approaches, in-
cluding Dai et al. [22], and Child et al. [23] have focused on
increasing the length of the available context in each natural
language sequence. The prevailing issue with long contexts
is efﬁciency – since transformers have quadratic scaling in
the length of the context. To reduce the processing time, ap-
proaches such as Dai et al.[22] and Child et al.[23] focused on
computing gradients through only a subset of the full context,
rather than the whole context to save memory and compute.
Such an approach is codiﬁed by Wu et al. [8], who recently
demonstrated that expanding the context of standard text trans-
formers through a large memory bank of cached external key-
value pairs can lead to signiﬁcant perplexity improvements on
the standard language modeling task. Wu et al. [8] retrieve
the most relevant context elements using a K-NN approach,
and only back-propagate through these components. While the
lookup may encourage some off-policy drift, the approach is
effective, and allows for signiﬁcantly increased performance,
particularly in copy-tasks, which require the model to point at
speciﬁc prior elements, which may not be accessible in models
with smaller contexts.
Does extended context help when the context is external,
or even, orthogonal to the current utterance? There seems
to be some evidence to that effect, as outside of the standard
ASR pipeline, it has been shown that models augmented with
external memory generated from large-scale text data have
the potential to outperform similarly sized models without
external knowledge. Borgeaud et al. [7] recently demonstrated
that leveraging external-knowledge lookup from a database
of natural language sentences, can lead to efﬁciency improve-
ments of up to 25x across a wide range of pure language tasks
from language modeling to question-answering. Similarly,
knowledge-augmented learning has been shown to be widely



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 5):

Conference’17, July 2017, Washington, DC, USA Zhang and Xiao, et al.
Table 1: Impact on knowledge enhancement. MMLU and PopQA are measured by precision and exact match (EM), respectively.
“∗” and “ †” indicates the SOTA general embedding model and the task-specific method for the corresponding scenario.
MMLU PopQA
Method STEM Social Human Other All Avg. PopQA
None 0.3468 0.5328 0.5094 0.4967 0.4599 0.2061
BM25 0.3760 0.5378 0.5051 0.5088 0.4721 0.3491
Instructor [76] 0.3702 0.5406 0.5111 0.5082 0.4721 0.3533
Contriever [30] 0.3677 0.5383 0.5080 0.5013 0.4684 0.3276
RetroMAE-BEIR [46] 0.3857 0.5456 0.5221 0.5276 0.4853 0.4364
BGE∗[89] 0.3852 0.5564 0.5194 0.5389 0.4896 0.4491
AAR†[96] 0.3802 0.5501 0.5125 0.5288 0.4826 0.4792
API-Retriever [62] 0.3535 0.5335 0.4999 0.5068 0.4625 0.2488
LLM-R [83] 0.3629 0.5277 0.5018 0.4984 0.4625 0.2506
LLM-Embedder 0.3848 0.5568 0.5255 0.5360 0.4903 0.5052
notably outperforms a series of general retrievers, including the
state-of-the-art method BGE. On the other hand, it also goes be-
yond the task-specific method, i.e. AAR for knowledge enhance-
ment, LLM-R for in-context learning, API-Retriever for tool learn-
ing, Conv-ANCE for conversational search. Such an observation
indicates that LLM-Embedder is able to provide a strong and unified
foundation to support different retrieval augmentation needs of LLMs .
Finally, we can also observe that the task-specific retrievers opti-
mized for one scenario could result in limited performances in other
scenarios, indicating that the training impacts between different
retrieval tasks are not always transferable. To better illustrate this
point, we visualize the retrieval augmentation’s impact (improve-
ments over None) from five representative methods in Figure 3: BGE,
AAR, LLM-R, API-Retriever (API-R), and LLM-Embedder (ours). The
first method is the general embedding model, while the second to
fourth are task-specific methods. We can observe that although
task-specific training can deliver a competitive performance for its
corresponding scenario, e.g., AAR for knowledge enhancement and
LLM-R for in-context learning, their impacts are severely weak-
ened when applied for other usages. In contrast, LLM-Embedder
demonstrates a steady and competitive performance across dif-
ferent scenarios. Although challenging, the seemingly irrelevant or
even adverse retrieval patterns can still be reconciled by one unified
embedding model on top of the properly optimized training recipe .
3.2.2 Individualized Analysis. Further analysis is made for the re-
trieval augmentation’s impact to each individual scenario.
•Knowledge Enhancement. The experiment results on knowl-
edge enhancement are shown in Table 1, where we can make the
following observations. 1) Benefit of external knowledge. LLMs
benefit from external knowledge when answering questions in
MMLU and PopQA, as clear empirical advantages are achieved by
the retrieval augmentation methods compared with the plain LLM,
i.e. None. 2) Importance of retrieval accuracy. The impact of knowl-
edge enhancement becomes more pronounced when knowledge
retrieval is more accurate. We observe consistent improvements
as we transition from using the BM25 retriever to more advanced
embedding models. 3) Distinction between datasets. The impact
of retrieval augmentation is more noticeable in the PopQA dataset
compared to MMLU. This difference is likely due to the nature of
0.50520.626813.48320.86450.50530.24880.594514.78340.80170.11370.25060.626214.47460.13210.02340.47920.593814.69990.42000.28770.44910.597414.29430.57610.3856QAICLLongToolC-Search
OursAPI-RLLM-RAARBGE
Figure 3: Retrieval augmentation’s impact from different
retrievers. The warmer color indicates a better performance.
the datasets. PopQA tends to be more knowledge-intensive, with a
focus on questions about long-tail entities. In contrast, many ques-
tions in MMLU rely more on common sense and logical reasoning
rather than extensive world knowledge.
•In-Context Learning. The experiment results on in-context
learning are shown in Table 2, where we can draw the following
observations. 1) Benefits of retrieved examples. When comparing
plain LLM (None) with other retrieval-augmented methods, we can
consistently observe the improved performances in most cases. This
finding underscores the enhancement of LLM’s ability to follow
instructions when retrieved examples are presented. 2) Limitation
of BM25. It’s noteworthy that BM25’s performance is comparatively
weaker than its performance in other scenarios. This discrepancy
can be attributed to the specific nature of in-context learning, where
examples need to emphasize semantic similarity rather than lexical
similarity. 3) Limited transferability. While the task-specific method,
LLM-R, exhibits a competitive performance for in-context learning,
its utility becomes severely limited when applied to other scenarios,
such as knowledge retrieval and tool using. This suggests that
example retrieval calls for a unique pattern tailored to this very
task, making it challenging to transfer to other scenarios.
•Long-Context Modeling . The experiment results on long-
context modeling are shown in Table 3. While retrieval augmenta-
tion consistently demonstrates improvements compared to having
no augmentation (None), it may not be entirely convincing due to
the utilization of more context. To address this issue, we introduce



Source: data\tc16_2312.10997v5\referenced_papers\[107]_2306.02224.pdf (Page 5):

and ALFWorld experiments. The beneficial guidance pro-
vided by the Webshop’s IL model effectively condensed the
choice spaces for LLMs, contrasted with the repetitive and
misleading advice offered by the IL in the ALFWorld con-
text. Interesting discrepancies were also observed in how
the LLMs disagreed with the IL’s recommendations: Claude
registered a disagreement rate of 0.814, GPT3.5 of 0.769,
and GPT4, leading the pack, registered a rate of 0.854. This
suggests an inherent capability within LLMs to filter out
misleading suggestions. However, the extent to which this
disagreement improved or impeded performance appeared
to be context-dependent, highlighting the importance of dis-
cernment in processing the IL’s advice. Claude and specially
GPT4 showed remarkable adeptness at avoiding the pitfalls
of misleading and repetitive advice. By contrast, GPT3.5
exhibited a clear shortfall in this respect, a performance
echoed by its pairing with a random action in our Webshop
ablation study. This underscores the importance of context
when integrating IL models with LLMs and signals the need
for careful evaluation when dealing with potentially mis-
leading input, especially on an LLM like GPT3.5 which can
get easily confused.
Figure 3: For ALFWorld, the ratios of LLMs considering
or disagreeing with additional opinion provided by
expert models. For the top 1 case, we only redeem it
as an agreement if the exact opinion matches.
3.5 Discussions
Initially, Auto-GPT was conceptualized as an experimental
idea rather than a robust workflow suitable for real-world
applications. However, our research demonstrates otherwise.
Auto-GPT not only proves its potential for practical use
but also outperforms supervised state-of-the-art IL models
with GPT4, signifying a shift in perspective towards this
innovative approach.
In the current discourse, we posit that this additional opin-
ion approach can readily find widespread adoption across
diverse industries, given the existing prevalence of expert
models such as recommendation systems and traditional
natural language processing (NLP) services, inclusive of text
classification models and the like. An immediate applica-
tion for this methodology can be envisaged in leveraging
LLMs for making definitive determinations and give explana-
tions regarding the prioritization of items, such as movies or
songs, to be displayed to the user. This is achievable by pro-
viding the selected top-k outputs derived from a supervised
recommendation model to LLMs as Additional Opinions.
It is crucial to note, however, that the two tasks we have
chosen to benchmark in this research do not fully encapsu-
late the vast array of potential real-world scenarios. They
serve merely as a starting point for the exploration of this
idea. This is the inaugural instance where the concept of
adapting Auto-GPT to handle complex tasks by introduc-
ing Additional Opinions has been proposed. This innova-
tive approach opens new avenues for further research and
development, potentially expanding the realm of practical
applications for AI models and significantly impacting our
understanding of complex decision-making mechanisms.
4 RELATED WORK
Foundation models trained on self-supervision tasks have
shown significant success in downstream tasks, particu-
larly in few-shot and zero-shot settings [ 5, 7, 10]. More
recently, generative pre-trained foundation models have
demonstrated impressive in-context learning abilities, al-
lowing them to tackle decision-making tasks that require
logic reasoning [8] and/or interactions with external APIs
[9, 13, 22]. However, adapting LLMs for decision-making
tasks often involves non-trivial prompt design, memory re-
trieval mechanisms to dynamically construct the agent’s
context [12, 17, 25], and sometimes model fine-tuning [9]
to enhance its decision-making abilities.
Several techniques have been proposed to adapt Large
Language Models (LLMs) for improved planning and rea-
soning. These include methods for enabling explicit Chain
of Thought (CoT) thinking processes [21], as well as prompt-
ing and decoding techniques aimed at enhancing the self-
consistency of LLMs [6, 20]. However, most of these tech-
niques primarily focus on offline reasoning tasks that can be
planned ahead, while their implications in online decision-
making scenarios are rarely discussed.
5 CONCLUSION
Our experimental results highlight the successful adaptation
of the Auto-GPT styled agent to complex online decision-
making tasks through straightforward prompt design, sur-
passing IL-based baseline models specifically designed for
these tasks. Among the foundational LLMs powering Auto-
GPT, GPT-4 demonstrates superior performance. Addition-
ally, we introduce an innovative strategy of incorporat-
ing additional opinions from external expert models, fur-
ther enhancing the decision-making capabilities of Auto-
GPT styled agents, particularly benefiting GPT-4. Our Addi-
tional Opinions algorithm provides a lightweight supervised
training approach for Auto-GPT styled agents, enabling
improved performance without requiring extensive fine-
tuning of the LLMs. We demonstrate the effectiveness and



### Claim 81/179

#### Claim Text
For example, CRAG [67] trains a lightweight retrieval evaluator to assess the overall quality of the retrieved documents for a query and triggers different knowledge retrieval actions based on confidence levels.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[67]_2401.15884.pdf (Page 4):

Algorithm 1: CRAG Inference
Require :E (Retrieval Evaluator), W (Query Rewriter), G (Generator)
Input : x (Input question), D = {d1, d2, ..., dk} (Retrieved documents)
Output : y (Generated response)
1 scorei = E evaluates the relevance of each pair (x, di), di ∈ D
2 Confidence = Calculate and give a final judgment based on {score1, score2, ...scorek}
// Confidence has 3 optional values: [CORRECT], [INCORRECT] or [AMBIGUOUS]
3 if Confidence == [CORRECT] then
4 Internal_Knowledge = Knowledge_Refine(x, D)
5 k = Internal_Knowledge
6 else if Confidence == [INCORRECT] then
7 External_Knowledge = Web_Search(W Rewrites x for searching)
8 k = External_Knowledge
9 else if Confidence == [AMBIGUOUS] then
10 Internal_Knowledge = Knowledge_Refine(x, D)
11 External_Knowledge = Web_Search(W Rewrites x for searching)
12 k = Internal_Knowledge + External_Knowledge
13 end
14 G predicts y given x and k
4.3 Action Trigger
To correct the irrelevant documents and refine the
target documents as needed, actions should be exe-
cuted discriminately. Based on the aforementioned
confidence score for each retrieved document, three
types of actions are designed and triggered accord-
ingly where the upper and lower thresholds are set.
If the confidence score is higher than the upper
threshold, the retrieved document is identified as
Correct, while identified as Incorrect if below
the lower threshold. Otherwise, a more soft and
intermediate action, i.e., Ambiguous is executed.
Each retrieved document is conducted individually
and integrated eventually.
Correct Here, a retrieval is assumed Correct
when the confidence score of at least one retrieved
document is higher than the upper threshold. If
so, it means that there are relevant documents in
the retrieved results, and the knowledge from the
retrieval results is supposed to be more reliable and
accurate. However, even if a relevant document can
be found, there is inevitably some noisy knowledge
strips in this document. To extract the most
critical knowledge strips within this document, a
knowledge refinement method is further designed
which will be elaborated in Section 4.4.
Incorrect Besides, a retrieval is assumed
Incorrect when the confidence scores of all
retrieved documents are below the lower threshold.
This indicates that all retrieved documents are
considered irrelevant, which are unhelpful for
generation. Once the knowledge from the retrieval
results is judged to be inaccurate, it is unwise to
still get stuck in it, which is likely to result in
fabricated facts. Therefore, we need to seek new
sources of knowledge for correction. Here, web
search is introduced to search from the Internet as
elaborated in Section 4.5. This corrective action
helps overcome the embarrassing challenge where
no reliable knowledge can be referred to.
Ambiguous Except for the above two situations,
the remaining will be assigned to an intermediate
action of Ambiguous. This generally occurs when
the accuracy of the retrieval is hard to distinguish
and the evaluator gives an intermediate score.
Since the retrieval evaluator is not confident in its
judgment, both types of processed knowledge in
Correct and Incorrect are combined to comple-
ment each other. Implementing such a moderating
and soft strategy can significantly contribute to
strengthening the robustness and resilience of the
system, fostering a more adaptable framework for
optimal performance.
Discussion Preliminary experiments of employ-
ing only the Correct and Incorrect actions show
that the efficacy of CRAG was easily affected by
the accuracy of the retrieval evaluator. The reason
might be the distinct knowledge switch for all input
cases, regardless of the level of confidence in their
judgment. The design of the Ambiguous action



Source: data\tc16_2312.10997v5\referenced_papers\[67]_2401.15884.pdf (Page 3):

x: Who was the screenwriter for Death of a Batman? d1 d2Retrieval
Input
Retrieved Documents
Ask: If retrieved documents are correct to x?
Correct
RetrievalEvaluatorAsk: If retrieved documents are correct to x?
Ambiguous
Incorrect
Knowledge Refinementd1
d2
strip1strip2
stripkDecompose… Filterstrip1stripkRecomposekin
Knowledge SearchingxRewriteq:Death of a Batman; screenwriter; WikipediaWebSearchkex
k1
kn
k2…Select
KnowledgeCorrection
Generation
Correct AmbiguousIncorrectx kin+ x kin+
Generator
kex+ x kex+
Figure 2: An overview of the proposed CRAG at inference. A retrieval evaluator is constructed to evaluate the
relevance of the retrieved documents to the input, and estimate a confidence degree based on which different
knowledge retrieval actions of {Correct, Incorrect, Ambiguous} can be triggered.
Our objective is to correct the retrieved documents
if they are irrelevant. Specifically, T5-large (Raffel
et al., 2020) is adopted for initializing the retrieval
evaluator and fine-tuned. Its parameter size is much
smaller than the most current LLMs (Touvron et al.,
2023a,b; Chowdhery et al., 2023; Anil et al., 2023;
Brown et al., 2020; Ouyang et al., 2022; OpenAI,
2023). To ensure all experimental results were
comparable with Self-RAG (Asai et al., 2024), the
same retrieval results through Contriever (Izacard
et al., 2022) provided by Self-RAG were also
adopted in our experiments. The relevance signals
for fine-tuning the evaluator can be collected from
the existing datasets. For example, PopQA (Mallen
et al., 2023) provides the golden subject wiki title
from wikipedia for each question. We can use that
to track a not 100% relevant but rather high-quality
passage. We utilized that as the relevance signals
for fine-tuning the retrieval evaluator.2 On the other
hand, the negative samples for fine-tuning were
all randomly sampled from the retrieval results,
which are rather similar to the input query but
2https://huggingface.co/datasets/akariasai/PopQA
not relevant. More details about this fine-tuning
step can be referred to in Appendix B.3. For
every question, there are generally 10 documents
retrieved. The question is concatenated with each
single document as the input, and the evaluator
predicts the relevance score for each question-
document pair individually. We also tried to prompt
ChatGPT to identify the retrieval relevance for
comparison, but it underperforms as elaborated in
Section 5.5. Based on these calculated relevance
scores, a final judgment is made as to whether
the retrieval is correct or not associated with the
action trigger. In our proposed framework, the
retrieval quality is evaluated at a relatively low
cost without the need to have access to large and
expensive LLMs. Compared with the critic model
of Self-RAG (Asai et al., 2024) that instruction-
tuned LLaMA-2 (7B), the evaluator designed in
CRAG demonstrates the advantages of being quite
lightweight (0.77B).



Source: data\tc16_2312.10997v5\referenced_papers\[67]_2401.15884.pdf (Page 2):

methods generally leverage information retrieval to
supply documents containing relevant knowledge
for generative LLMs. Earlier studies adopt either
sparse or dense retrievers at the front end of a pre-
trained language model that specializes in response
generation. Despite this, the methods above usually
ignore a question, what if the retrieval goes wrong?
Since the purpose of introducing a retrieval is to
secure that generative LMs can obtain relevant and
accurate knowledge. If retrieved documents are
irrelevant, the retrieval system can even exacerbate
the factual error that LMs make.
Advanced RAG Many advanced approaches
have been developed from the original RAG in
recent years (Zhang et al., 2024; Kim et al., 2024;
Wang et al., 2024; Liu et al., 2024). Considering
that retrieval is sometimes unnecessary for some
queries, conversely, responses without retrieval
are even more accurate in many situations. Self-
RAG (Asai et al., 2024) is proposed to selectively
retrieve knowledge and introduce a critic model
to decide whether to retrieve. Yoran et al. (2024)
designed an NLI model to identify the irrelevant
context and improve robustness. SAIL (Luo
et al., 2023) is tuned on instructions to insert
retrieved documents before instructions. While
Toolformer (Schick et al., 2023) is pre-trained for
calling APIs such as Wikipedia. In addition, in
some long-text generation tasks, external knowl-
edge is needed more than once, and when to
retrieve should be concerned. Jiang et al. (2023)
actively anticipate future content and decide when
and what to retrieve in long-form generation.
Compared with recent studies (Schick et al.,
2023; Luo et al., 2023; Asai et al., 2024) that are
the most relevant to our work, a main difference
should be highlighted. These approaches target
on exploiting retrieval as a useful tool to augment
generation or whether retrieval is necessary, while
this study particularly studies the scenarios where
the retriever returns inaccurate results. To the best
of our knowledge, this paper makes the first attempt
to explore and design corrective strategies for RAG
to improve its robustness of generation.
3 Task Formulation
Following previous work (Lewis et al., 2020; Asai
et al., 2024), given input X and an accessible
corpus containing a large amount of knowledge
documents C = {d1, ..., dN }, the system is ex-
pected to generate the output Y. The entire
framework is usually divided into a retriever R
and a generator G. The retriever R aims to retrieve
the top-K documents D = {dr1 , ..., drk } that are
relevant to the input X from the corpus C. Based
on the input X and the retrieved results D, the
generator G is responsible for generating the output
Y. This framework can be formulated as:
P(Y|X ) =P(D|X)P(Y, D|X). (1)
It shows that the retriever and generator are seam-
lessly coupled, exhibiting low risk tolerance. Any
unsuccessful retrieval can result in an unsatisfac-
tory response, regardless of the impressive abilities
of the generator. This is exactly the focus of this
paper to improve the robustness of generation.
4 CRAG
4.1 Overview of Model Inference
Figure 2 and Algorithm 1 present an overview
of CRAG at inference, which designs corrective
strategies to improve the robustness of generation.
Given an input query and the retrieved documents
from any retriever, a lightweight retrieval evaluator
is constructed to estimate the relevance score
of retrieved documents to the input query (Sec-
tion 4.2). The relevance score is quantified into a
total of three confidence degrees and then triggered
the corresponding actions: {Correct, Incorrect,
Ambiguous} (Section 4.3). If the action Correct
is triggered, the retrieved documents will be re-
fined into more precise knowledge strips. This
refinement operation involves knowledge decom-
position, filter, and recomposition (Section 4.4).
If the action Incorrect is triggered, the retrieved
documents will be discarded. Instead, web searches
are resorted to and regarded as complementary
knowledge sources for corrections (Section 4.5).
Eventually, when it cannot confidently make a
correct or incorrect judgment, a soft and balanced
action Ambiguous which combines both of them is
triggered. After optimizing the retrieval results, an
arbitrary generative model can be adopted.
4.2 Retrieval Evaluator
It is natural to wonder whether the retrieved docu-
ments are accurate or not before using them, which
is significant since irrelevant or misleading mes-
sages can be identified in this way. The accuracy
of the retrieval evaluator undeniably plays a pivotal
role in shaping the overall system performance, as
it influences the outcomes of subsequent processes.



Source: data\tc16_2312.10997v5\referenced_papers\[67]_2401.15884.pdf (Page 5):

significantly helps to mitigate the dependence on
the accuracy of the retrieval evaluator.
4.4 Knowledge Refinement
Given a retrieved relevant document, a decompose-
then-recompose knowledge refinement method
is designed to further extract the most critical
knowledge strips in it. To obtain fine-grained
retrieval results, we segmented the retrieved results
into internal strips. If a retrieved result is as short as
one or two sentences, it is regarded as an individual
strip, otherwise, retrieval documents are required to
be split into smaller units which generally consist
of a few sentences according to the total length.
The scale is assumed to include an independent
piece of information, and the filtering is based on
the segments. Then, the retrieval evaluator fine-
tuned in Section 4.2 is employed to calculate the
relevance score of each knowledge strip. Based
on these scores, irrelevant knowledge strips are
filtered out, while relevant ones are recomposed via
concatenation in order, namely internal knowledge.
4.5 Web Search
It would be more intelligent if a system itself
could determine that its existing knowledge corpus
could not solve the problem well and turn to
additional external knowledge for help. On the
contrary, even if a system knows that the existing
knowledge cannot solve the problem, but still
sticks to the limited knowledge corpus, it will only
give a fabricated fact in the end, which is called
hallucination.. Therefore, it is extremely important
to seek complementary external knowledge if
the retrieved results are all assumed irrelevant,
and we consider a system that knows what it
doesn’t know and what it cannot answer to be
more intelligent than one that clings to limited
knowledge and is incapable of seeking external
knowledge. Since retrieval from static and limited
corpora can only return sub-optimal documents
in terms of scope and diversity, large-scale web
searches (Piktus et al., 2021; Komeili et al., 2022)
are integrated as a strategic extension of RAG.
Specifically, the inputs are rewritten into queries
composed of keywords by ChatGPT to mimic the
daily usage of search engine. The prompt for
rewriting is shown in Appendix A. In CRAG ,
a public and accessible commercial web search
API is adopted to generate a series of URL links
for every query. 3 Considering that knowledge
from large-scale web searches could introduce
biases or unreliable information, authoritative and
regulated web pages like Wikipedia are preferred,
which can significantly help mitigate these issues.
Moreover, we utilize the URL links to navigate
web pages, transcribe their content, and employ the
same knowledge refinement method as Section 4.4
to derive the relevant web knowledge, namely
external knowledge.
5 Experiments
We conducted experiments to extensively demon-
strate CRAG ’s adaptability to RAG-based ap-
proaches and its generalizability across both short-
and long-form generation tasks.
5.1 Tasks, Datasets and Metrics
CRAG was evaluated on four datasets, including
PopQA (Mallen et al., 2023) ( short-form gener-
ation), Biography (Min et al., 2023) ( long-form
generation), PubHealth (Zhang et al., 2023a) (true-
or-false question), and Arc-Challenge (Bhaktha-
vatsalam et al., 2021) ( multiple-choice question).
Following previous work, accuracy was adopted
as the evaluation metric for PopQA, PubHealth,
and Arc-Challenge. FactScore (Min et al., 2023)
was adopted as the evaluation metric for Biography.
Readers can refer to Appendix B.1 for more details.
The same metrics are used because our proposed
method is comparable to previous studies, since
we used the same retrieval results as previous
work. The difference lies in that our motivation
is to improve the retrieval quality by correcting
the retrieval results that the system judges to
be of low quality. This can be analogous to
RAG’s augmentation to standalone parameterized
language models and we further augment RAG
with corrective strategies.
5.2 Baselines
We primarily compared CRAG with both ap-
proaches with and without retrieval, where the
latter can be further split into standard RAG and
latest advanced RAG, including:
Baselines without retrieval. We evaluated some
public LLMs, LLaMA2-7B,13B (Touvron et al.,
2023b), instruction-tuned models, Alpaca-7B,13B
(Dubois et al., 2023), and CoVE65B (Dhuliawala
et al., 2024) which introduces iterative engineering
3In this study, Google Search API is utilized for searching.



Source: data\tc16_2312.10997v5\referenced_papers\[67]_2401.15884.pdf (Page 8):

Accuracy
Our Retrieval Evaluator (T5-based) 84.3
ChatGPT 58.0
ChatGPT-CoT 62.4
ChatGPT-few-shot 64.7
Table 4: Evaluation of our retrieval evaluator and
ChatGPT for the retrieval results on the PopQA dataset.
The impact of each knowledge utilization oper-
ation. Table 3 illustrated how the performance
changed if a key knowledge utilization operation
was ablated. Evaluations on the PopQA dataset in
terms of accuracy were conducted by individually
removing the knowledge utilization operations of
document refinement, search query rewriting, and
external knowledge selection. Removing document
refinement denoted that the original retrieved docu-
ments were directly fed to the following generator,
as in most existing works. Additionally, removing
search query rewriting denoted that questions were
not rewritten into queries consisting of keywords
during knowledge searching. Eventually, removing
knowledge selection denoted that all searched con-
tent of web pages was all regarded as the external
knowledge without selection. These results help
derive the findings that the performance of the
final system degraded no matter which knowledge
utilization operation was removed, revealing that
each knowledge utilization operation contributed
to improving the utilization of knowledge.
5.5 Accuracy of the Retrieval Evaluator
The quality of the retrieval evaluator significantly
determined the performance of the entire system.
Given the document retrieval results, we assessed
whether the retrieval evaluator can accurately
determine the overall quality of these results. The
assessment accuracy on the PopQA dataset of
our retrieval evaluator and the commercial LLM
ChatGPT on the document retrieval results was
shown in Table 4. The prompts of ChatGPT,
ChatGPT-CoT, and ChatGPT-few-shot used in our
experiments can be referred to in Appendix A.
Results reveal that the lightweight T5-based re-
trieval evaluator significantly outperformed the
competitive ChatGPT in all settings.
5.6 Robustness to Retrieval Performance
To further verify the robustness of the proposed
method to retrieval performance, we studied how
the generation performance changed given different
69.8
(Actual)
60 50 40 30 20 10
Accuracy of retrieval
20
30
40
50
60
70Accuracy of generation
no retrieval
Self-RAG Self-CRAG
Figure 3: The generation performance of Self-RAG
and Self-CRAG given different retrieval performance
on the PopQA dataset with SelfRAG-LLaMA-7b. The
lower horizontal line demonstrates the performance of
the generator without retrieval.
LLaMA2-hf-7b SelfRAG-LLaMA2-7b
PopQA
CRAG 54.9 59.8
RAG 50.5 52.8
RAG w. web 52.2 53.8
Self-CRAG 49.0 61.8
Self-RAG 29.0 54.9
Self-RAG w. web 24.9 57.9
Table 5: Comparison results between CRAG , Self-
CRAG and RAG, Self-RAG with the same input in
terms of accuracy.
retrieval performance. A part of accurate retrieval
results were deliberately removed at random to
imitate a low-quality retriever and evaluate how
the performance changed. Figure 3 demonstrated
the performance change of Self-RAG and Self-
CRAG on the PopQA dataset. It can be seen
that the generation performance of Self-RAG and
Self-CRAG dropped as the retrieval performance
dropped, indicating that the generator relied heavily
on the quality of the retriever. Furthermore, as
the retrieval performance dropped, the generation
performance of Self-CRAG dropped more slightly
than that of Self-RAG. These results imply the
superiority of Self-CRAG over Self-RAG on en-
hancing the robustness to retrieval performance.
5.7 Consistent Supplementation of Web
Search Knowledge
This paper highlights the necessity of enhancing
the retrieved context by incorporating additional
information when the initial retrieval results are
irrelevant and unreliable. Meanwhile, it is also
crucial to confirm that the primary improvements
in our method stem from the self-correction mech-



### Claim 82/179

#### Claim Text
Scaling laws of RAG End-to-end RAG models and pre-trained models based on RAG are still one of the focuses of current researchers [173].The parameters of these models are one of the key factors.While scaling laws [174] are established for LLMs, their applicability to RAG remains uncertain.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[26]_2401.06954.pdf (Page 0):

Bridging the Preference Gap between Retrievers and LLMs
Zixuan Ke2∗, Weize Kong 1, Cheng Li 1, Mingyang Zhang 1, Qiaozhu Mei 3† and Michael Bendersky1
1Google Research
2University of Illinois at Chicago
3University of Michigan
1{weize,chgli,mingyang,bemike}@google.com
2zke4@uic.edu
3qmei@umich.edu
Abstract
Large Language Models (LLMs) have demon-
strated superior results across a wide range
of tasks, and Retrieval-augmented Generation
(RAG) is an effective way to enhance the per-
formance by locating relevant information and
placing it into the context window of the LLM.
However, the relationship between retrievers
and LLMs in a RAG is still under-investigated.
Most existing work treats the retriever and the
LLM as independent components and leaves a
gap between retrieving human-“friendly” infor-
mation and assembling a LLM-“friendly” con-
text. In this work, we examine a novel bridge
mechanism. We validate the ranking and se-
lection assumptions of retrievers in the context
of RAG and propose a framework that chains
together supervised and reinforcement learn-
ing to train a bridge model that optimizes the
connection between the retriever and the LLM.
Empirical results demonstrate the effectiveness
of our method in both question-answering and
personalized generation tasks.
1 Introduction
Large language models (LLMs) such as GPT-4
(?) and PaLM 2 (Anil et al., 2023), have demon-
strated impressive performance on a wide variety
of tasks. Retrieval-augmented generation (RAG),
which retrieves knowledge items from an external
data source and puts it into the context window
of LLMs, has produced significantly enhanced re-
sults in many NLP tasks (Khandelwal et al., 2020;
Borgeaud et al., 2022; Izacard et al., 2022; Ya-
sunaga et al., 2023).
However, most works on RAG study retrievers
and LLMs separately. On one hand, most retriev-
ers are designed to be human-friendly, usually
based on the general belief in classic information
∗ The work was done during internship at Google Re-
search.
† The work was done as a visiting researcher at Google
Research.
Figure 1: We observe a preference gap when alternating
the ranking and selection of information in RAG. Ex-
periments are conducted with retrieving passages using
GTR (Ni et al., 2021) and using top K of them as ad-
ditional context for a frozen Palm2-S LLM. Different
colors indicate different datasets (detailed in Sec. 5.1)
and the Y-axis shows the relative percentage. Alternat-
ing the selection (Top-1) of information significantly
affects (either positively or negatively) the LLM’s per-
formance, while randomizing the ranking of multiple
selected items (Top-5) does not have a comparable im-
pact (the metrics are detailed in Sec. 5.2). Note the
impact on NQ is even too small to be visible.
retrieval literature that ranking is paramount, as
humans typically read from top to bottom (?). On
the other hand, LLMs exhibit preferences different
from humans and yield accurate results only when
the information in the prompt aligns with these
preferences. This discrepancy leads to sub-optimal
design in current RAG systems, a phenomenon we
term preference gap. This gap manifests in various
aspects. For example, the general belief in ranking
may not align with LLM’s preferences due to the
self-attention mechanism of Transformers, which
can focus on any token regardless of its position.
Another aspect is selection; while humans can eas-
ily disregard irrelevant information in a context, it
has been shown that LLMs are highly sensitive to
irrelevant content (Shi et al., 2023a). There likely
exist more aspects that further diverge the LLM’s
preference from that of humans, e.g., repetition.
arXiv:2401.06954v2  [cs.CL]  20 Feb 2024



Source: data\tc16_2312.10997v5\referenced_papers\[164]_2309.15217.pdf (Page 0):

RAGAS: Automated Evaluation of Retrieval Augmented Generation
Shahul Es†, Jithin James†, Luis Espinosa-Anke∗♢, Steven Schockaert∗
†Exploding Gradients
∗CardiffNLP, Cardiff University, United Kingdom
♢AMPLYFI, United Kingdom
shahules786@gmail.com,jamesjithin97@gmail.com
{espinosa-ankel,schockaerts1}@cardiff.ac.uk
Abstract
We introduce RAGA S (Retrieval Augmented
Generation Assessment), a framework for
reference-free evaluation of Retrieval Aug-
mented Generation (RAG) pipelines. RAG
systems are composed of a retrieval and an
LLM based generation module, and provide
LLMs with knowledge from a reference textual
database, which enables them to act as a natu-
ral language layer between a user and textual
databases, reducing the risk of hallucinations.
Evaluating RAG architectures is, however, chal-
lenging because there are several dimensions to
consider: the ability of the retrieval system to
identify relevant and focused context passages,
the ability of the LLM to exploit such passages
in a faithful way, or the quality of the gener-
ation itself. With RAGA S, we put forward a
suite of metrics which can be used to evaluate
these different dimensions without having to
rely on ground truth human annotations. We
posit that such a framework can crucially con-
tribute to faster evaluation cycles of RAG archi-
tectures, which is especially important given
the fast adoption of LLMs.
1 Introduction
Language Models (LMs) capture a vast amount
of knowledge about the world, which allows them
to answer questions without accessing any exter-
nal sources. This idea of LMs as repositories of
knowledge emerged shortly after the introduction
of BERT (Devlin et al., 2019) and became more
firmly established with the introduction of ever
larger LMs (Roberts et al., 2020). While the most
recent Large Language Models (LLMs) capture
enough knowledge to rival human performance
across a wide variety of question answering bench-
marks (Bubeck et al., 2023), the idea of using
LLMs as knowledge bases still has two fundamen-
tal limitations. First, LLMs are not able to answer
questions about events that have happened after
they were trained. Second, even the largest models
struggle to memorise knowledge that is only rarely
mentioned in the training corpus (Kandpal et al.,
2022; Mallen et al., 2023). The standard solution
to these issues is to rely on Retrieval Augmented
Generation (RAG) (Lee et al., 2019; Lewis et al.,
2020; Guu et al., 2020). Answering a question
then essentially involves retrieving relevant pas-
sages from a corpus and feeding these passages,
along with the original question, to the LM. While
initial approaches relied on specialised LMs for
retrieval-augmented language modelling (Khandel-
wal et al., 2020; Borgeaud et al., 2022), recent work
has suggested that simply adding retrieved docu-
ments to the input of a standard LM can also work
well (Khattab et al., 2022; Ram et al., 2023; Shi
et al., 2023), thus making it possible to use retrieval-
augmented strategies in combination with LLMs
that are only available through APIs.
While the usefulness of retrieval-augmented
strategies is clear, their implementation requires
a significant amount of tuning, as the overall per-
formance will be affected by the retrieval model,
the considered corpus, the LM, or the prompt for-
mulation, among others. Automated evaluation of
retrieval-augmented systems is thus paramount. In
practice, RAG systems are often evaluated in terms
of the language modelling task itself, i.e. by mea-
suring perplexity on some reference corpus. How-
ever, such evaluations are not always predictive
of downstream performance (Wang et al., 2023c).
Moreover, this evaluation strategy relies on the LM
probabilities, which are not accessible for some
closed models (e.g. ChatGPT and GPT-4). Ques-
tion answering is another common evaluation task,
but usually only datasets with short extractive an-
swers are considered, which may not be represen-
tative of how the system will be used.
To address these issues, in this paper we present
RAGA S1, a framework for the automated assess-
1RAGA S is available at https://github.com/
explodinggradients/ragas.
arXiv:2309.15217v1  [cs.CL]  26 Sep 2023



Source: data\tc16_2312.10997v5\referenced_papers\[56]_2312.11361.pdf (Page 0):

“Knowing When You Don’t Know”: A Multilingual Relevance Assessment
Dataset for Robust Retrieval-Augmented Generation
Nandan Thakur1, Luiz Bonifacio1,3, Xinyu Zhang1, Odunayo Ogundepo1,
Ehsan Kamalloo1, David Alfonso-Hermelo2, Xiaoguang Li2, Qun Liu2,
Boxing Chen2, Mehdi Rezagholizadeh2, Jimmy Lin1
1 David R. Cheriton School of Computer Science, University of Waterloo, Canada
2 Huawei Noah’s Ark Lab 3 FEEC-Unicamp, Brazil
Abstract
Retrieval-Augmented Generation (RAG)
grounds Large Language Model (LLM) output
by leveraging external knowledge sources
to reduce factual hallucinations. However,
prior work lacks a comprehensive evaluation
of different language families, making it
challenging to evaluate LLM robustness
against errors in external retrieved knowledge.
To overcome this, we establish NoMIRACL, a
human-annotated dataset for evaluating LLM
robustness in RAG across 18 typologically
diverse languages. NoMIRACL includes
both a non-relevant and a relevant subset.
Queries in the non-relevant subset contain
passages judged as non-relevant, whereas
queries in the relevant subset include at least a
single judged relevant passage. We measure
relevance assessment using: (i) hallucination
rate, measuring model tendency to hallucinate,
when the answer is not present in passages
in the non-relevant subset, and (ii) error rate,
measuring model inaccuracy to recognize
relevant passages in the relevant subset. In
our work, we observe that most models
struggle to balance the two capacities. Models
such as LLAMA-2 and Orca-2 achieve over
88% hallucination rate on the non-relevant
subset. Mistral and LLAMA-3 hallucinate
less but can achieve up to a 74.9% error rate
on the relevant subset. Overall, GPT-4 is
observed to provide the best tradeoff on both
subsets, highlighting future work necessary
to improve LLM robustness. NoMIRACL
dataset and evaluation code are available at:
https://github.com/project-miracl/nomiracl.
1 Introduction
Retrieval-Augmented Generation (RAG) (Guu
et al., 2020; Lewis et al., 2020; Izacard and Grave,
2021; Borgeaud et al., 2022) is a promising way
to incorporate external knowledge via a first-stage
retrieval system. RAG instills information from re-
liable knowledge corpora (provided as external pas-
[1] Power Electronics: AC
Voltage Controller: The purpose
of an AC Voltage Controller, or
AC Regulator, is to vary the RMS
voltage across the load while at
a constant frequency ... 
[2] Calculator: Electronic
calculators contain a keyboard
with buttons for digits and
arithmetical operations; some
even contain \"00\" and \"000\"
buttons to make larger or
smaller numbers easier ... 
[1] Food pyramid (nutrition): 
A food pyramid represents the
optimal number of servings to
be eaten each day from each
of the food groups. The first
pyramid was published in
Sweden in 1974.
When was the food pyramid
first introduced?
[2] History of USDA nutrition
guides: The introduction of the
USDA's food guide pyramid in
1992 attempted to express the
recommended servings of each
food group, which previous
guides ...
Non-relevant Subset Relevant Subset
I don't know I don't knowYes, Answer is
present
Yes, Answer is
present
True
Negative (TN)
False
Negative (FN)
True
Positive (TP)
What does the AC button on the
calculator stand for?
False
Positive (FP)
relevance = 0
relevance = 0 relevance = 0
relevance = 1
LLM LLM
Figure 1: LLM robustness evaluation as a binary tree
in NoMIRACL. When dealing with queries in the non-
relevant subset, the LLM is expected to disregard all
noisy passages and refrain from answering (TN). Con-
versely, for queries in the relevant subset, the LLM
should recognize the relevant passage and provide a
valid answer (TP).
sages) to generate accurate and faithful responses
(Shuster et al., 2021; Gao et al., 2023b; Liu et al.,
2024). Ever since the advent of Large Language
Models (LLMs), such as GPT-3 (Brown et al.,
2020) or LLAMA-2 (Touvron et al., 2023), they
are the de-facto choice for answer generation in
RAG, due to their unprecedented progress in text
generation and understanding (Brown et al., 2020;
Li et al., 2024; Chang et al., 2024; Guo et al., 2023).
RAG grounds the LLM-generated answer, thereby
avoiding previously observed factual hallucination
(Maynez et al., 2020; Raunak et al., 2021) and out-
dated knowledge (Cao et al., 2021; He et al., 2023)
in LLMs.
A challenging issue in RAG is to provide ro-
bust and reliable LLM-generated answers. The
answer generation stage is dependent on the first-
arXiv:2312.11361v3  [cs.CL]  10 Nov 2024



Source: data\tc16_2312.10997v5\referenced_papers\[103]_2303.08559.pdf (Page 0):

Large Language Model Is Not a Good Few-shot InformationExtractor,
but a GoodReranker for Hard Samples!
Yubo Ma1, Yixin Cao2, YongChing Hong1, Aixin Sun1
1 S-Lab, Nanyang Technological University
2 Singapore Management University
yubo001@e.ntu.edu.sg
Abstract
Large Language Models (LLMs) have made
remarkable strides in various tasks. Whether
LLMs are competitive few-shot solvers for in-
formation extraction (IE) tasks, however, re-
mains an open problem. In this work, we
aim to provide a thorough answer to this ques-
tion. Through extensive experiments on nine
datasets across four IE tasks, we demonstrate
that current advanced LLMs consistently ex-
hibit inferior performance, higher latency, and
increased budget requirements compared to
fine-tuned SLMs under most settings. There-
fore, we conclude that LLMs are not effec-
tive few-shot information extractors in gen-
eral 1. Nonetheless, we illustrate that with
appropriate prompting strategies, LLMs can
effectively complement SLMs and tackle chal-
lenging samples that SLMs struggle with. And
moreover, we propose an adaptive filter-then-
rerank paradigm to combine the strengths of
LLMs and SLMs. In this paradigm, SLMs
serve as filters and LLMs serve as rerankers.
By prompting LLMs to rerank a small portion
of difficult samples identified by SLMs, our pre-
liminary system consistently achieves promis-
ing improvements (2.4% F1-gain on average)
on various IE tasks, with an acceptable time
and cost investment. Our code is available at
https://github.com/mayubo2333/LLM-IE.
1 Introduction
Large Language Models (LLMs, Brown et al. 2020;
Chowdhery et al. 2022; Touvron et al. 2023) have
shown remarkable abilities on various NLP applica-
tions such as factual question answering (Yu et al.,
2023; Sun et al., 2023), arithmetic reasoning (Chen
et al., 2022a; Qian et al., 2023) and logical rea-
soning (Jung et al., 2022; Pan et al., 2023). Given
the reasoning, memorization, instruction-following
and few-shot adaption capabilities emerging from
1A more precise assertion is that current LLMs, with
vanilla prompting setting and without IE-specific fine-tuning,
are not good few-shot information extractors in general.
LLMs, it prompts a compelling question: Can
LLMs be used to boost performance in few-shot
information extraction (IE) tasks?
To answer this question, we conduct an exten-
sive empirical study to compare the performance
between LLMs using in-context learning 2 (ICL)
and fine-tuned Small Language Models (SLMs).
We fairly evaluate SLMs-based and LLMs-based
methods across nine datasets spanning four com-
mon IE tasks: (1) Named Entity Recognition, (2)
Relation Extraction, (3) Event Detection and (4)
Event Argument Extraction. For each dataset, we
explored four to six settings to encompass typi-
cal low-resource extents, from 1-shot to 20-shot or
even more. Given the potential sensitivity of LLMs’
performance to the prompt context, we meticu-
lously considered variations in instruction, demon-
stration number and selection strategy, prompt for-
mat, etc. Our study reveals that LLMs excel over
SLMs only when annotations are extremely lim-
ited, i.e., both label types 3 and the samples 4 per
label are extremely scarce. With more ( e.g., hun-
dreds of) samples, SLMs significantly outperform
LLMs. Furthermore, LLMs incur greater inference
latency and costs than fine-tuned SLMs. Hence, we
claim that current LLMs are not good few-shot
information extractors in general.
We further investigate whether LLMs and SLMs
exhibit different abilities to handle various types of
samples. We categorize samples according to their
difficulty measured by SLMs’ confidence scores,
and compare LLMs’ and SLMs’ results within each
group. We find that LLMs are good at hard sam-
ples, though bad at easy samples. We posit that
the knowledge and reasoning abilities in LLMs en-
able them to handle hard samples (which are sim-
2All LLMs discussed in this paper are not fine-tuned, and
results for LLMs are based on in-context learning.
3Label types denote entity/relation/event/role types in dif-
ferent tasks. We use them interchangeably there-in-after.
4Samples refer to (i) demonstrations in ICL of LLMs, or
(ii) training samples for SLMs’ fine-tuning.
arXiv:2303.08559v2  [cs.CL]  21 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[36]_2303.08559.pdf (Page 0):

Large Language Model Is Not a Good Few-shot InformationExtractor,
but a GoodReranker for Hard Samples!
Yubo Ma1, Yixin Cao2, YongChing Hong1, Aixin Sun1
1 S-Lab, Nanyang Technological University
2 Singapore Management University
yubo001@e.ntu.edu.sg
Abstract
Large Language Models (LLMs) have made
remarkable strides in various tasks. Whether
LLMs are competitive few-shot solvers for in-
formation extraction (IE) tasks, however, re-
mains an open problem. In this work, we
aim to provide a thorough answer to this ques-
tion. Through extensive experiments on nine
datasets across four IE tasks, we demonstrate
that current advanced LLMs consistently ex-
hibit inferior performance, higher latency, and
increased budget requirements compared to
fine-tuned SLMs under most settings. There-
fore, we conclude that LLMs are not effec-
tive few-shot information extractors in gen-
eral 1. Nonetheless, we illustrate that with
appropriate prompting strategies, LLMs can
effectively complement SLMs and tackle chal-
lenging samples that SLMs struggle with. And
moreover, we propose an adaptive filter-then-
rerank paradigm to combine the strengths of
LLMs and SLMs. In this paradigm, SLMs
serve as filters and LLMs serve as rerankers.
By prompting LLMs to rerank a small portion
of difficult samples identified by SLMs, our pre-
liminary system consistently achieves promis-
ing improvements (2.4% F1-gain on average)
on various IE tasks, with an acceptable time
and cost investment. Our code is available at
https://github.com/mayubo2333/LLM-IE.
1 Introduction
Large Language Models (LLMs, Brown et al. 2020;
Chowdhery et al. 2022; Touvron et al. 2023) have
shown remarkable abilities on various NLP applica-
tions such as factual question answering (Yu et al.,
2023; Sun et al., 2023), arithmetic reasoning (Chen
et al., 2022a; Qian et al., 2023) and logical rea-
soning (Jung et al., 2022; Pan et al., 2023). Given
the reasoning, memorization, instruction-following
and few-shot adaption capabilities emerging from
1A more precise assertion is that current LLMs, with
vanilla prompting setting and without IE-specific fine-tuning,
are not good few-shot information extractors in general.
LLMs, it prompts a compelling question: Can
LLMs be used to boost performance in few-shot
information extraction (IE) tasks?
To answer this question, we conduct an exten-
sive empirical study to compare the performance
between LLMs using in-context learning 2 (ICL)
and fine-tuned Small Language Models (SLMs).
We fairly evaluate SLMs-based and LLMs-based
methods across nine datasets spanning four com-
mon IE tasks: (1) Named Entity Recognition, (2)
Relation Extraction, (3) Event Detection and (4)
Event Argument Extraction. For each dataset, we
explored four to six settings to encompass typi-
cal low-resource extents, from 1-shot to 20-shot or
even more. Given the potential sensitivity of LLMs’
performance to the prompt context, we meticu-
lously considered variations in instruction, demon-
stration number and selection strategy, prompt for-
mat, etc. Our study reveals that LLMs excel over
SLMs only when annotations are extremely lim-
ited, i.e., both label types 3 and the samples 4 per
label are extremely scarce. With more ( e.g., hun-
dreds of) samples, SLMs significantly outperform
LLMs. Furthermore, LLMs incur greater inference
latency and costs than fine-tuned SLMs. Hence, we
claim that current LLMs are not good few-shot
information extractors in general.
We further investigate whether LLMs and SLMs
exhibit different abilities to handle various types of
samples. We categorize samples according to their
difficulty measured by SLMs’ confidence scores,
and compare LLMs’ and SLMs’ results within each
group. We find that LLMs are good at hard sam-
ples, though bad at easy samples. We posit that
the knowledge and reasoning abilities in LLMs en-
able them to handle hard samples (which are sim-
2All LLMs discussed in this paper are not fine-tuned, and
results for LLMs are based on in-context learning.
3Label types denote entity/relation/event/role types in dif-
ferent tasks. We use them interchangeably there-in-after.
4Samples refer to (i) demonstrations in ICL of LLMs, or
(ii) training samples for SLMs’ fine-tuning.
arXiv:2303.08559v2  [cs.CL]  21 Oct 2023



#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 14):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:15
summarization tasks. Therefore, it does not require any ground truth reference. However, for
RAG systems, the retrieved texts may be irrelevant or incorrect, so consistency with them is not a
valid criterion. Instead, we use this metric to measure how well the generated text matches the
ground-truth reference. We call this metric RAGQuestEval. We will explain this metric in detail.
Let 𝐺𝑇 and 𝐺𝑀 be two sequences of tokens, where 𝐺𝑇 denotes the ground truth references and
𝐺𝑀 the corresponding evaluated generations. First, we generate a series of questions from the
ground truth references 𝐺𝑇 using the QuestEval method, which extracts entities and noun phrases
from the text. The goal of RAGQuestEval is to check if the generated text includes and conveys
correctly all the key information from the ground truth reference.
Next, we answer these questions using both real references and model-generated text. If the
question is unanswerable, the model returns "<Unanswerable>".
Finally, we calculate two scores to evaluate the quality of the generated text: recall and precision.
Recall. Recall is the ratio of answerable questions to all questions. This score shows how much
information in the ground truth reference is captured by the text generated by the RAG system. A
higher recall means that the generated text covers more information from the reference.
Recall(𝐺𝑇,𝐺𝑀 )= 1
|𝑄𝐺(𝐺𝑇)|
∑︁
(𝑞,𝑟)∈𝑄𝐺 (𝐺𝑇)
I[𝑄𝐴(𝐺𝑀,𝑞)≠ < Unanswerable >] (1)
In the above equation, 𝑄𝐺 is the question generator and 𝑄𝐴 is the question answerer.
Precision. Precision is the average answer similarity of all questions, excluding the unanswerable
ones. We use the token level F1 score to measure the answer similarity, which is a standard metric
for evaluating factoid question answering models. Higher precision means that the generated text
is more accurate and consistent with the reference.
Prec(𝐺𝑇,𝐺𝑀 )= 1
|𝑄𝐺(𝐺𝑇)|
∑︁
(𝑞,𝑟)∈𝑄𝐺 (𝐺𝑇)
𝐹1 (𝑄𝐴(𝐺𝑀,𝑞),𝑟) (2)
4 EXPERIMENT
The current evaluation of RAG Benchmark only focuses on the large language model component
in the RAG pipeline, and overlooks the importance of retrieval database construction and retriever.
To address this gap, we examine how different aspects of RAG systems affect their performance in
our benchmark. We also discuss some possible ways to improve existing RAG systems.
4.1 Experimental Settings
In this section, we will introduce the components of the RAG system, and describe how we conduct
experiments to evaluate their impact on system performance. The RAG system consists of the
following components:
•Chunk size: The RAG system splits the external knowledge into chunks of a certain length
and stores them in a vector database. The chunk size affects the retrieval accuracy and the
completeness of the context.
•Chunk overlap: Chunk overlap refers to the shared tokens between two consecutive text
chunks and is used to ensure semantic coherence when chunking.
•Embedding model : The RAG system converts the text chunks and the user’s query into
vectors using an embedding model or other methods. The embedding model affects the
quality and relevance of the context.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[26]_2401.06954.pdf (Page 2):

Figure 2: Differing from previous RAGs that update
LLMs, retrievers, or both, BGM connects a frozen LLM
and a frozen retriever through a lightweight bridge
model which adapts the retrieved information to the
LLM’s preference. This makes BGM applicable to
“large” LMs and off-the-shelf retrievers.
from various knowledge sources is proven effective
across numerous NLP tasks, including language
modeling (Borgeaud et al., 2022; Khandelwal et al.,
2020; Shi et al., 2023b), question answering (Lewis
et al., 2020; Izacard et al., 2022; de Jong et al.,
2023; De Jong et al., 2023; Shi et al., 2023b; Guu
et al., 2020; Izacard and Grave, 2020; Xu et al.,
2023), fact versification (Lewis et al., 2020) and
text generation (Lewis et al., 2020). Specifically,
RAG utilizes input as a query and comprises two
main components: (1) a retriever retrieves a set
of items from a side corpus. Particular items may
vary across different tasks, including documents,
passages, or even tokens. In this study, we focus on
retrieving passages; and (2) a LLM incorporates
the retrieved items, as additional information in the
input context, and makes final predictions.
A fundamental question in this process arises
regarding the disparate preferences between LLMs
and retrievers, as LLMs performing optimally only
when their preferences are satisfied. Bridging the
preference gap is crucial. Depending on which
components are subject to updates, this challenge
can be categorized into three families.
Finetuning retrievers and LLMs jointly. This
is the most widely used setting of RAG (Izacard
et al., 2022; Khandelwal et al., 2020; Wu et al.,
2022; Guu et al., 2020; Lewis et al., 2020). How-
ever, most prior work is based on relative small
LMs (< 1B). For example, Altas (Izacard et al.,
2022) finetunes LLM (T5 (Raffel et al., 2020a))
and retriever (Contriever (Izacard et al., 2021))
jointly by leveraging the LLM to provide super-
visory signal to train the retriever. RAG (Lewis
et al., 2020) uses a tunnable query encoder and
DPR (Karpukhin et al., 2020) as retriever, BART
as LLM, and design an end-to-end schema to train
the query encoder and the LLM.
Finetuning LLMs only. Updating retrievers is
not always desirable as it is costly and requires
the document index to be periodically updated. To
bridge the preference gap, it is also possible to only
update the LLMs. FiD (Izacard and Grave, 2020)
takes the retrieved documents and query as input,
finetunes the LLM to adapt to the external infor-
mation. Similarly, Lummen (De Jong et al., 2023)
and Glimmer (de Jong et al., 2023) improve FiD
via adding reranker and pre-encoding memeory.
Finetuning retrievers only. Although above
systems have shown improves results, they are not
always applicable in practice. Many LLMs (espe-
cially larger ones) are only available as black-box
APIs. Given this constraint, a natural approach
is to only update the retrievers to ensure they can
retrieve passages that are more compatible with
LLMs. REPLUG (Shi et al., 2023b) adapts a sim-
ilar idea as Atlas but fix the LM. RECOMP (Xu
et al., 2023) trains a compressors to summarize
the retrieved document from retriever. However,
this family of models is incapable of performing
any sample-level selection and can only choose top
passages by setting a fixed threshold.
Unlike the existing approaches, BGM does not
fine-tune the LLM or the retriever and instead
employs a bridge model in between (Fig. 2).
RL for information retrieval. Before the LLM
era, RL has been used in information retrieval (IR)
(Xia et al., 2017; Wei et al., 2017; Zeng et al., 2018;
Xu et al., 2020). The core approach was to frame
the IR problem as a Markov Decision Process and
apply an RL algorithm to solve it. Typically, an
IR task would be structured to determine which
document to select for a ranking position, using
ranking metrics such as DCG as the reward. None
of these existing studies explore the application of
RL in the context of RAG.
In the LLM era, RL has been used in query
rewriting for retrieval (Wu et al., 2021; Nogueira
and Cho, 2017; Adolphs et al., 2021), where a
black-box retriever is assumed. This is a different
problem from RAG. Bacciu et al. (2023) suggest us-
ing RL to fine-tune retriever in the RAG context in
their opinion paper, not supported by experiments.
Their work does not recognize the importance of
bridging the gap between retrievers and LLMs, nor



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 5):

111:6 Lyu, et al.
2.2 RAG Benchmarks
When investigating the development and optimization of RAG, the effective evaluation of their
performance becomes a fundamental concern. Table 1 shows some commonly used benchmarks
for evaluating RAG. LangChain provides benchmark tasks, such as LangChain Docs Q&A and
Semi-structured Reports [27], designed to assess various RAG architectures. These datasets are
constructed from snapshots of Python documentation and PDFs containing tables and charts.
They emphasize the model’s capability to handle structured and semi-structured data. Evaluation
standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing
large models for question-answering generation has emerged as a prevalent approach in building
evaluation datasets. For instance, RGB [8] creates its evaluation dataset by gathering recent news
reports and employing LLM to generate relevant events, questions, and answers. Conversely,
ARES [44]. relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
These methods not only showcase the RAG system’s proficiency in handling real-time data but also
illustrate the utility of automation and synthetic data in the evaluation process. For evaluating the
capabilities of models across various professional domains, the Instruct-Benchmark-Tester dataset
encompasses a range of question types, with a particular focus on financial services, legal, and
intricate business scenarios [39].
Depending on whether the evaluation phase incorporates ground truth, metrics of existing
evaluation methods can be categorized into those necessitating reference and those not requiring
it. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by
contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-
Benchmark-Tester [39] employs accuracy score as an evaluation metric, a widely acknowledged
measure of model performance that assesses the extent to which model-generated answers align
with reference answers. The primary objective of RGB [8] is to evaluate whether large models can
effectively utilize external documents to acquire knowledge and generate accurate answers. Its
evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.
Reference-free evaluation methods, including TruLens-Eval [14], RAGAS [13], and ARES [44],
provide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning
context relevance, answer faithfulness, and answer relevance. TruLens-Eval [14] introduces the
RAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,
encompassing context relevance, groundedness, and answer relevance. RAGAS [13], serving as
a reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity
to identify pertinent and concentrated context passages, along with the LLMs’ proficiency in
faithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a
predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each
aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy
when compared to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-
powered inference to offer statistical assurances for its scoring, generating confidence intervals.
ARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer
relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts
and producing both faithful and relevant answers. Regarding evaluation methods, [32] places an
emphasis on assessing the credibility and accuracy of responses generated by generative search
engines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including
high costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,
exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely
approximates manual evaluation outcomes.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 10):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:11
Researchers at West Virginia University in the United States 
have observed synthetic DNA at the atomic level and learned 
how to alter its structure to enhance its scissor-like functions.
Researchers at West Virginia University have successfully observed
 the structure of synthetic DNA at the atomic level and discovered
 that altering its structure can enhance its scissor-like functions. 
Their research findings were published in the Nature sub-journal, 
Communications Chemistry. The study found that these DNA 
molecules can fold into complex shapes capable of performing ...
Science and Technology Daily, Beijing, July 27 (Reporter Zhang 
Mengran) - Researchers at West Virginia University have achieved 
the observation of synthetic DNA at the atomic level, gaining 
insights into how to alter its structure to enhance its scissor-like 
functions. Understanding these synthetic DNA reactions in greater 
detail could be key to unlocking new medical technologies in the 
future. The research findings were published in the recent issue of 
the Nature sub-journal, Communications Chemistry. Atomic-level ...
The synthetic DNA used in this study, also known as DNAzymes, 
differs from human DNA in that it is created in a laboratory, has low 
production costs, and can catalyze chemical reactions. Researchers 
noted that DNA is typically regarded as inert when it comes to 
storing human genetic information; however, some types of DNA 
evolved in the laboratory defy traditional rules. These DNA ...
Fig. 4. The dataset construction pipeline for text continuation and multi-document summarization task.
our news corpus, removing the 10,000 articles 𝑑 simultaneously. The new news corpus 𝐷−𝑑+𝐸
serves as our retrieval corpus, and we expect the model to use the events and relevant information
from the retrieval corpus to generate a summary of the articles 𝑑.
3.3 Text continuation: RAG Application in "Create"
RAG is useful not only for "Delete", where it retrieves and summarizes key information from massive
texts, but also for "Create". In this scenario, RAG systems show strong creativity by expanding
existing texts, and we take the text continuation task as an evaluation. The text continuation task
aims to automatically produce coherent and relevant subsequent content based on the beginning
of the text, making the text more complete and vivid.
To construct the continuation task dataset, we follow the same method as the summary task
dataset. Figure 4 shows the construction process of text continuation. Specifically, we select a news
article from a high-quality corpus and use a specialized Chinese word segmentation tool, to split it
into sentences. Then, we divide the article into two equal parts: the first half serves as the input,
and the second half as the output of the continuation dataset. We expect the model to use RAG
technology to retrieve relevant information from the document library and generate a continuation
that is coherent, informative, and consistent with the input and output.
To ensure that the retrieval database covers the real continuation text, we use the Baidu search
engine to find external documents and add them to the database. The continuation text differs from
the event text in that it consists of multiple sentences. Therefore, we split the continuation text
into paragraphs by sentences and retrieve relevant documents for each paragraph using the search
engine. This way, we guarantee that the retrieval database contains most of the information to
reconstruct the continuation text.
3.4 Question Answering: RAG Application in "Read"
Another application scenario of RAG is to use external knowledge bases to enhance the question-
answering capabilities of LLMs, which can be applied to various knowledge-intensive tasks. Cur-
rently, there are many evaluation benchmarks to measure the performance of RAG in this scenario,
and multiple question answering datasets have been created.
However, the existing question answering datasets also have some limitations. On the one hand,
some datasets (such as NQ and WEBQA) are outdated, and may have been covered by LLMs in
the pre-training stage, which reduces the advantage of RAG systems. On the other hand, some
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 2):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:3
Fig. 1. We have classified the application scenarios of RAG into four primary aspects: Create, Read, Update,
and Delete. The figure provides an illustrative example for each category, showcasing the wide-ranging
potential of RAG technology.
•In "READ", the system uses external knowledge retrieval to answer questions, solve problems
in question-answering, dialogue, and reasoning, and increase understanding of the input text.
•In "UPDATE", the system fixes errors in the input text using retrieved content, correcting
spelling, grammar, or factual errors to make the text better.
•In "DELETE", the system simplifies the input by improving retrieval results, removing
unnecessary details, and doing tasks like text summarization or simplification.
To evaluate the RAG system in these four scenarios, we introduce CRUD-RAG, a comprehensive,
large-scale Chinese RAG benchmark. CRUD-RAG consists of four evaluation tasks: text continu-
ation, question answering (with single-document and multi-document questions), hallucination
modification, and open-domain multi-document summarization, which respectively correspond to
the CRUD-RAG classification of RAG application scenarios. We construct CRUD-RAG by crawling
the latest high-quality news data from major news websites in China, which aims to minimize the
likelihood of LLMs encountering these data during training. Then, we automatically create datasets
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



### Claim 83/179

#### Claim Text
Initial studies like RETRO++ [44] have begun to address this, yet the parameter count in RAG models still lags behind that of LLMs.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[26]_2401.06954.pdf (Page 0):

Bridging the Preference Gap between Retrievers and LLMs
Zixuan Ke2∗, Weize Kong 1, Cheng Li 1, Mingyang Zhang 1, Qiaozhu Mei 3† and Michael Bendersky1
1Google Research
2University of Illinois at Chicago
3University of Michigan
1{weize,chgli,mingyang,bemike}@google.com
2zke4@uic.edu
3qmei@umich.edu
Abstract
Large Language Models (LLMs) have demon-
strated superior results across a wide range
of tasks, and Retrieval-augmented Generation
(RAG) is an effective way to enhance the per-
formance by locating relevant information and
placing it into the context window of the LLM.
However, the relationship between retrievers
and LLMs in a RAG is still under-investigated.
Most existing work treats the retriever and the
LLM as independent components and leaves a
gap between retrieving human-“friendly” infor-
mation and assembling a LLM-“friendly” con-
text. In this work, we examine a novel bridge
mechanism. We validate the ranking and se-
lection assumptions of retrievers in the context
of RAG and propose a framework that chains
together supervised and reinforcement learn-
ing to train a bridge model that optimizes the
connection between the retriever and the LLM.
Empirical results demonstrate the effectiveness
of our method in both question-answering and
personalized generation tasks.
1 Introduction
Large language models (LLMs) such as GPT-4
(?) and PaLM 2 (Anil et al., 2023), have demon-
strated impressive performance on a wide variety
of tasks. Retrieval-augmented generation (RAG),
which retrieves knowledge items from an external
data source and puts it into the context window
of LLMs, has produced significantly enhanced re-
sults in many NLP tasks (Khandelwal et al., 2020;
Borgeaud et al., 2022; Izacard et al., 2022; Ya-
sunaga et al., 2023).
However, most works on RAG study retrievers
and LLMs separately. On one hand, most retriev-
ers are designed to be human-friendly, usually
based on the general belief in classic information
∗ The work was done during internship at Google Re-
search.
† The work was done as a visiting researcher at Google
Research.
Figure 1: We observe a preference gap when alternating
the ranking and selection of information in RAG. Ex-
periments are conducted with retrieving passages using
GTR (Ni et al., 2021) and using top K of them as ad-
ditional context for a frozen Palm2-S LLM. Different
colors indicate different datasets (detailed in Sec. 5.1)
and the Y-axis shows the relative percentage. Alternat-
ing the selection (Top-1) of information significantly
affects (either positively or negatively) the LLM’s per-
formance, while randomizing the ranking of multiple
selected items (Top-5) does not have a comparable im-
pact (the metrics are detailed in Sec. 5.2). Note the
impact on NQ is even too small to be visible.
retrieval literature that ranking is paramount, as
humans typically read from top to bottom (?). On
the other hand, LLMs exhibit preferences different
from humans and yield accurate results only when
the information in the prompt aligns with these
preferences. This discrepancy leads to sub-optimal
design in current RAG systems, a phenomenon we
term preference gap. This gap manifests in various
aspects. For example, the general belief in ranking
may not align with LLM’s preferences due to the
self-attention mechanism of Transformers, which
can focus on any token regardless of its position.
Another aspect is selection; while humans can eas-
ily disregard irrelevant information in a context, it
has been shown that LLMs are highly sensitive to
irrelevant content (Shi et al., 2023a). There likely
exist more aspects that further diverge the LLM’s
preference from that of humans, e.g., repetition.
arXiv:2401.06954v2  [cs.CL]  20 Feb 2024



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 5):

111:6 Lyu, et al.
2.2 RAG Benchmarks
When investigating the development and optimization of RAG, the effective evaluation of their
performance becomes a fundamental concern. Table 1 shows some commonly used benchmarks
for evaluating RAG. LangChain provides benchmark tasks, such as LangChain Docs Q&A and
Semi-structured Reports [27], designed to assess various RAG architectures. These datasets are
constructed from snapshots of Python documentation and PDFs containing tables and charts.
They emphasize the model’s capability to handle structured and semi-structured data. Evaluation
standards encompass the accuracy of answers and the faithfulness of model responses. Utilizing
large models for question-answering generation has emerged as a prevalent approach in building
evaluation datasets. For instance, RGB [8] creates its evaluation dataset by gathering recent news
reports and employing LLM to generate relevant events, questions, and answers. Conversely,
ARES [44]. relies on generating synthetic queries and answers, leveraging the FLAN-T5 XXL model.
These methods not only showcase the RAG system’s proficiency in handling real-time data but also
illustrate the utility of automation and synthetic data in the evaluation process. For evaluating the
capabilities of models across various professional domains, the Instruct-Benchmark-Tester dataset
encompasses a range of question types, with a particular focus on financial services, legal, and
intricate business scenarios [39].
Depending on whether the evaluation phase incorporates ground truth, metrics of existing
evaluation methods can be categorized into those necessitating reference and those not requiring
it. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by
contrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-
Benchmark-Tester [39] employs accuracy score as an evaluation metric, a widely acknowledged
measure of model performance that assesses the extent to which model-generated answers align
with reference answers. The primary objective of RGB [8] is to evaluate whether large models can
effectively utilize external documents to acquire knowledge and generate accurate answers. Its
evaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.
Reference-free evaluation methods, including TruLens-Eval [14], RAGAS [13], and ARES [44],
provide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning
context relevance, answer faithfulness, and answer relevance. TruLens-Eval [14] introduces the
RAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,
encompassing context relevance, groundedness, and answer relevance. RAGAS [13], serving as
a reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity
to identify pertinent and concentrated context passages, along with the LLMs’ proficiency in
faithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a
predefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each
aspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy
when compared to existing methods such as RAGAS. Furthermore, ARES [44] employs prediction-
powered inference to offer statistical assurances for its scoring, generating confidence intervals.
ARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer
relevance, highlighting the importance of a proficient RAG system in identifying relevant contexts
and producing both faithful and relevant answers. Regarding evaluation methods, [32] places an
emphasis on assessing the credibility and accuracy of responses generated by generative search
engines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including
high costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,
exact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction
rate continue to be widely adopted in the field. Furthermore, employing LLMs for evaluation closely
approximates manual evaluation outcomes.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 24):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:25
Hallucination Modification: The top-k value has little effect on the semantic similarity
metrics (bleu, rouge and bertScore) and the key information metric (RAGQuestEval). They only
drop sharply when the top-k is too large. This is because, in our hallucination modification dataset,
correcting the wrong information only requires a small amount of context, and the model has a
certain anti-interference ability in the hallucination modification task, so the top-k value is not a
decisive factor.
4.7 Analyzing the Impact of LLM on RAG Performance in Different Tasks
The core of the RAG system is an LLM, which can generate accurate and fluent answers based
on the user’s question and the retrieved information. In this paper, we conducted experiments on
several commonly used LLMs, as Table 8 displayed.
Text Continuation: The experimental results show that the larger the model parameters, the
better the performance. GPT-4 surpassed other large models in all tasks, demonstrating its powerful
generation ability.
Open-Domain Multi-Document Summarization : GPT-4 also excelled in the summary gener-
ation task. It achieved higher scores than other models on the overall semantic accuracy metric,
as well as the key information recall and precision metric. Moreover, the summary generated by
GPT-4 was relatively concise, avoiding redundant information. GPT-4 is the most suitable model
for this task.
Question Answering: For single-document QA, which only requires extracting relevant in-
formation from a sentence in the text, this task is relatively simple. Qwen and Baichuan2 even
outperformed GPT series models. However, for multi-document QA that requires a comprehensive
understanding of multiple documents, GPT-4 was far ahead of other models, showing its excellent
knowledge fusion ability. The Baichuan2-13B model also performed better than GPT-3.5, indicating
its potential.
Hallucination Modification: We found that some models generated text that was too long,
introducing redundant information. The hallucination modification task only requires modifying the
hallucination information, retaining other information, and not introducing irrelevant information.
Therefore, ChatGLM2, Qwen-7B, and Baichuan2 did not complete this task well.
In summary, the GPT-4 model performed excellently on most tasks and evaluation metrics,
proving that it is a powerful LLM. Qwen-7B and Qwen-14B models also performed well, especially
in the text continuation and summary generation tasks. Baichuan2-13B model was very competitive
with GPT-4 in the QA task, deserving more investigation.
Latest LLM Evaluation: Our dataset was constructed in December 2023. To evaluate its challenge
to the latest LLMs in 2024, we experimented with two newly released models: GPT-4o(Released in
May 2024) and Qwen2-7b(Released in June 2024).
The results show that GPT-4o performs similarly to its predecessor GPT-4, or with some slight
improvements. In contrast, Qwen2-7b demonstrates significant improvements over its predecessor
Qwen-7b in multiple tasks. These findings confirm that our benchmarks remain challenging for
the latest LLMs. Additionally, it is encouraging to observe that the performance of many LLMs
continues to improve with each new version.
4.8 Suggestions for Optimizing Your RAG System
Using the benchmark we constructed, we systematically evaluated the impact of each component
in the RAG system in various application scenarios. Subsequently, we offer some suggestions for
future researchers aiming to optimize the performance of the RAG system. Table 9 summarizes our
recommendations
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 4):

CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:5
Table 1. Relate Work.
Method Dataset Scale Evaluation Metrics Evaluation Method Application Field Ref. Lang.
[27] Based on LangChain Pythondocumentation QA dataset86 Accuracy of answer, Faithfulness ofresponse to the retrieved documentEvaluating retrieval andgeneration consistencyGeneral QA scenarios(Read) Yes EN
[27] PDF documents containingtables and charts5 Accuracy of answer, Faithfulness ofresponse to the retrieved documentEvaluating retrieval andgeneration consistency
Semi-structureddata scenarios(Read) Yes EN
[32] Query and responses(with citations)1450 Fluency, Perceived utility,Citation recall and precisionHuman evaluation Citation(Read) Yes EN
[17, 22] Questions, answersand contexts(with citations)- Fluency, Correctness,Citation quality Self-devised metrics,Human evaluation Citation(Read) Yes EN
[54] Questions, answersand contexts(with citations)1948 Location citation recall, Location precision,The coefficient of variation of citation locationsSelf-devised metricsCitation(Read) Yes EN
[29] Questions, answersand contexts(with citations)3422 Accuracy of factual and non-factual,AUC-PR, and so on Self-devised metrics,Common metrics Citation(Read) Yes EN
[61] Statement-citation Pairs12681 Correlation, Classification performance,Retrieval effectiveness, Faithfulness
Self-devised metrics,Common metrics,Human evaluation
Citation(Read) Yes EN
[7] Paragraphs(with citations)10000 Citation density,The coverage of reference factsSelf-devised metricsCitation(Read) Yes EN
[39] Questions, answersand contexts 200
Categorization ability,Logical/Mathematical reasoning,Complex question solving,Summarization ability
Accuracy Financial services,Legal, Business(Read,Delete) Yes EN
[8] LLM-generated dataset 1000
Noise robustness,Negative rejection,Information integration,Counterfactual robustness
Self-devised metricsGeneral, especiallynews domain(Read,Update) Yes CNEN
[14] — — Context relevance, Groundedness,Answer relevance Analyzing the RAG triadGeneral(Create,Read) No —
[13] — — Faithfulness, Answer relevance,Context relevance Automated evaluationusing LLM prompts General(Create,Read) No —
[44] LLM-generated dataset 150Context relevance, Answer faithfulness,Answer relevance Generating custom LLM judges foreach component of a RAG systemGeneral(Create,Read) No EN
Ours LLM-generated dataset 36166ROUGE, BLEU,bertScore, RAGQuestEvalEvaluating retrieval andgeneration consistency
General(Create,ReadUpdate,Delete) Yes CN
optimization, and post-retrieval processing [20]. Pre-retrieval processing encompasses data trans-
former to enhance text standardization, ensuring factual accuracy, optimizing index structures,
adjusting block sizes, and rewriting query [ 4, 16, 50, 52]. Retrieval model optimization entails
the fine-tuning of domain-specific embedding models and the application of dynamic embedding
techniques [11, 60]. Post-retrieval processing minimizes context length through reranking and
compression operations, aiming to emphasize critical information, diminish noise interference, and
enhance integration and utilization by the generator [37, 53, 55].
Furthermore, to enhance the precision and efficiency of the generator when handling retrieval
content, scholars have undertaken a series of optimization measures. As an illustration, researchers
have devised methods such as Chain-of-Note (CON) for the generator [58]. CON generates contin-
uous reading notes to comprehensively evaluate the relevance of retrieved documents to the posed
question, integrating this information to produce precise final responses. This approach further
enhances the capability of RAG in managing retrieval information, guaranteeing the production of
responses that are simultaneously accurate and pertinent. In specific domains, such as medical and
legal, models undergo fine-tuning to enhance the generator’s performance within those particular
fields [10, 24, 56]. Through the implementation of these methods, the generator can more effectively
process retrieved information and furnish responses that are more accurate and relevant.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[173]_2403.10131.pdf (Page 8):

Preprint, Under Review
2 4 6 8 10
# T est Documents (T op-k)
0.22
0.24
0.26
0.28
0.30
0.32Final Accuracy
Natural Questions
Train D*
Train D* + 1D
Train D* + 2D
Train D* + 3D
2 4 6 8 10
# T est Documents (T op-k)
0.125
0.150
0.175
0.200
0.225
0.250Final Accuracy
Hotpot QA
Train D*
Train D* + 1D
Train D* + 2D
Train D* + 3D
Figure 6: Test-Time Documents Varying: To analyze how robust RAFT is to varying number
of test-time documents, we study three domains – NQ, Trivia QA and HotPot QA. In NQ,
we find that training with 4 documents leads to optimal performance, and this changes to 3
and 2 for for Trivia QA and HotPot QA respectively. However, we see that training with
only golden documents leads to poor performance.
training with D∗ + 3D and it is D∗ + 1D documents with Hotpot QA. This insight has been
particularly beneficial for our algorithm, RAFT . In our experiments, we consistently employ
a training setup consisting of one golden document alongside four distractor documents.
Generalization to a variable number of test-time documents. We extended our research
to examine the impact of different quantities of test-time documents on the model’s per-
formance. Specifically, our experiments focused on assessing how models, trained with
varying numbers of distractor documents, respond to changes in the number of documents
presented at test time. The results, illustrated in Fig. 6, confirm that the inclusion of distrac-
tor documents during training indeed makes the model more resilient to fluctuations in the
number of documents encountered during testing. This ability to maintain consistent perfor-
mance despite variations in test-time document numbers further validates the robustness of
our approach, RAFT . This finding underscores the importance of a well-calibrated training
environment to prepare the model for a range of scenarios it may encounter in real-world.
6 Related Works
Retrieval-Augmented Language Models Retrieval-Augmented Language Models (RALMs)
enhance LLMs by integrating a retrieval module that sources relevant information from
external knowledge bases, significantly improving performance across various NLP tasks,
including language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,
2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;
Wang et al., 2023) and open-domain question answering (Izacard et al., 2023; Lewis et al.,
2020). For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever,
treating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the
decoder-only architecture to include retrieved texts and conducts pre-training from scratch.
kNN-LM (Khandelwal et al., 2019) interpolates between the LM’s next token distribution
and distributions computed from retrieved tokens at inference. (Shi et al., 2023d; Ram
et al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or
fine-tuned retriever.
Memorization A key question around large neural language models is whether they truly
“understand” text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern
memorization (Carlini et al., 2019; Tänzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;
2022) develop methodologies to quantify the extent of memorization in neural models.
(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization
impacts the models’ generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)
demonstrated the ability of language models to memorize and regurgitate training data,
raising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).
Finetuning for RAG More recently, several papers have been exploring the idea of fine-
tuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu
9



### Claim 84/179

#### Claim Text
RA-CM3 [176] stands as a pioneering multimodal model of both retrieving and generating text and images.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[176]_2211.12561.pdf (Page 5):

Retrieval-Augmented Multimodal Language Modeling
Figure 3.Text-to-image generation involving world knowledge.Our retrieval-augmented model (RA-CM3) can generate correct images
from entity-rich captions thanks to the access to retrieved images in the context. For example, RA-CM3’s outputs faithfully capture the
visual characteristics of various entities (e.g., the shape and painting of Ming Dynasty vase, the amount of Callanish standing stones). On
the other hand, baseline models without retrieval capabilities (vanilla CM3, Stable Diffusion) tend to struggle, especially when the caption
involves rare entities (e.g., “Ming Dynasty vase”, “Oriental Pearl tower”, “Dragon and Tiger Pagodas”).
(FID 28), which is 3x bigger than our model. This sug-
gests that retrieval augmentation provides significant help
in generating higher-quality images.
To also factor in training efficiency, Figure 2 visualizes the
image generation performance (y-axis: FID score) vs the
amount of compute used in model training (x-axis: normal-
ized A100 GPU hours) for our RA-CM3 model and baseline
models. We find that existing models in the autoregressive
Transformer paradigm follow a negatively sloped line in
this chart (the blue dots and line in Figure 2). RA-CM3 is
located significantly below this line, i.e., obtaining a bet-
ter FID with less training compute. This suggests that the
proposed retrieval-augmented method achieves significantly
better training efficiency than existing works.
Our intuition is that retrieval augmentation allows the model
to focus on learning how to use the retrieved documents
in the context rather than fitting all the documents into the
parameters of the model, speeding up the training process.
Image-to-caption generation. Table 3 shows the image-
to-caption generation performance on MS-COCO, with no
finetuning. The metric is the CIDEr score, where the higher
is the better. Our retrieval-augmented CM3 achieves a
CIDEr score of 89, significantly outperforming the baseline
CM3 with no retrieval (CIDEr 72). Moreover, RA-CM3
outperforms other strong models such as Parti (20B param-
eters) and Flamingo (3B; 4-shot), despite using just ∼3B
parameters and 2-shot in-context examples.
These results confirm that our model can perform both
image and text generation well, offering the first unified
retrieval-augmented multimodal model (Table 1).
4.4. Analysis
We analyze the scaling laws of RA-CM3 in §C.2 and the
key design choices of RA-CM3 in §C.3.
5. Qualitative results
We show novel qualitative capabilities of our RA-CM3, such
as knowledge-intensive multimodal generation (§5.1) and
multimodal in-context learning (§5.2, 5.3, 5.4). While GPT-
3 (Brown et al., 2020) and Flamingo (Alayrac et al., 2022)
showed in-context learning for text-to-text or image-to-text
generation, we show that RA-CM3 can do in-context learn-
ing for both text (§5.4) and image (§5.2, 5.3) generation.
5.1. Knowledge-intensive multimodal generation
Because of the retrieval capability, RA-CM3 is especially
good at tasks that require world knowledge or composition
of knowledge (knowledge-intensive generation). Figure 3,
4 show example outputs from RA-CM3. For each caption,
the output images were obtained by sampling 256 images
6



Source: data\tc16_2312.10997v5\referenced_papers\[176]_2211.12561.pdf (Page 1):

Retrieval-Augmented Multimodal Language Modeling
Approach Model type Image generation Text generation Retrieval In-context learning
DALL-E, Parti(Ramesh et al.; Yu et al.)Autoregressive ✔
DALL-E 2, Imagen(Ramesh et al.; Saharia et al.)Diffusion ✔
Re-Imagen(Chen et al.) Diffusion ✔ ✔
Flamingo, MetaLM(Alayrac et al.; Hao et al.)Autoregressive ✔ ✔
MuRAG(Chen et al.) Autoregressive ✔† ✔
CM3(Aghajanyan et al.) Autoregressive ✔ ✔
RA-CM3 (Ours) Autoregressive ✔ ✔ ✔ ✔
Table 1.Comparison with other multimodal models. Our RA-CM3 is the first retrieval-augmented model that can perform both image
and text generation. RA-CM3 also exhibits strong in-context learning abilities thanks to the proposed retrieval-augmented training (§3.3).
†Focus on question answering.
for entity-rich captions like “George Washington standing
in front of the Eiffel Tower”. Reference to external mem-
ory may also offer benefits such as explainable and faithful
predictions (Metzler et al., 2021).
Recently, retrieval-augmented language models have shown
promise in natural language processing (NLP) (Karpukhin
et al., 2020; Guu et al., 2020; Lewis et al., 2020b; Borgeaud
et al., 2022). Given input text, such a model uses a retriever
that retrieves relevant documents from an external mem-
ory, and uses a generator to generate predictions given the
retrieved documents. However, these retrieval-augmented
methods are studied originally for text, and extending them
to the multimodal setting remains an open problem with
challenges. Specifically, we need to design a retriever and
a generator that handle multimodal documents, consisting
of both images and text. Several concurrent works study
retrieval for multimodal data (Chen et al., 2022a;b), but their
generators are each limited to a single modality, either text
generation or image generation (Table 1).
In this work, we address the above challenge and present
the first retrieval-augmented multimodal model that can re-
trieve and generate both text and images. As in Figure 1,
our input data and external memory comprise a set of mul-
timodal documents, each of which is an arbitrary sequence
of text/images (e.g., text, image, or their combinations like
caption-image pair). First, to obtain a multimodal retriever,
we use the Dense Retrieval method (Karpukhin et al., 2020)
with a mixed-modal encoder that can encode combinations
of text and images (e.g., pretrained CLIP; Radford et al.
2021). Given this retriever, we design a technique to retrieve
diverse and informative documents for the input document.
Second, we design the retrieval-augmented generator based
on the CM3 architecture (Aghajanyan et al., 2022), which
is a Transformer sequence model capable of both text and
image generation. Concretely, we prepend the retrieved doc-
uments as in-context examples to the main input document,
and train the generator by optimizing token prediction loss
jointly for the main document and retrieved documents.
We train our retrieval-augmented CM3 ( RA-CM3), using
150M text-image pairs from the LAION dataset (Schuh-
mann et al., 2021). RA-CM3 achieves strong performance
on MS-COCO image and caption generation, significantly
outperforming the baseline CM3 with no retrieval (12 FID
and 17 CIDEr improvements). It also outperforms existing
models such as DALL-E and Flamingo, despite using fewer
parameters (<30%) and compute for training (<30%).
We further demonstrate novel capabilities of RA-CM3 (§5).
First, it can perform faithful generation for tasks that re-
quire entity knowledge, for which existing models struggle
(Figure 3, 4). Second, RA-CM3 exhibits a multimodal in-
context learning ability: it can perform controlled image
generation by prompting with demonstration examples in
context (Figure 7), and it can also perform few-shot image
classification. RA-CM3 is the first model that can perform
in-context learning for both text and image generation (Ta-
ble 1).
More broadly, our work offers a general and modular re-
trieval augmentation framework for multimodal models, and
opens up various research avenues, such as further advance-
ment of multimodal retrievers and generators.
2. Related work
We discuss related works in detail in §B.
3. Approach
We present a retrieval-augmented multimodal model that can
retrieve and generate both text and images. As illustrated in
Figure 1, given an input multimodal document (i.e., arbitrary
sequence of text/images), we use a retriever that retrieves
relevant multimodal documents from an external memory,
and uses a generator to refer to the retrieved documents
and make predictions for the input document (i.e., generate
the continuation). We design the multimodal retriever as a
dense retriever with a mixed-modal encoder that can encode
combinations of text and images (e.g., pretrained CLIP;
§3.2). We build the retrieval-augmented generator using the
CM3 Transformer architecture, and we prepend the retrieved
documents to the main input document that we feed into the
generator (§3.3). We describe how we train this model and
use it for text-to-image or image-to-text generation in §3.4.
2



Source: data\tc16_2312.10997v5\referenced_papers\[176]_2211.12561.pdf (Page 11):

Retrieval-Augmented Multimodal Language Modeling
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V .,
et al. Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068, 2022.
Zhang, Z., Han, X., Liu, Z., Jiang, X., Sun, M., and Liu, Q.
Ernie: Enhanced language representation with informa-
tive entities. In Association for Computational Linguistics
(ACL), 2019.
A. Ethics and societal impact
Multimodal generative models, including our RA-CM3 and
other models like DALL-E, Parti and Stable Diffusion, are
typically trained on large, noisy image-text datasets col-
lected from the web. These datasets may contain biases
about race, gender and other demographic attributes, and
unsafe content such as pornography and violence (Birhane
et al., 2021a). We performed extensive data filtering to
remove problematic content in training data following exist-
ing works (§4.1), but it may still be possible that RA-CM3
outputs problematic text or images. RA-CM3 is a research
prototype, and we do not encourage using it in high-risk or
sensitive domains or for generating images of people. For a
more general, detailed discussion on the ethical considera-
tions of multimodal generative models, we refer readers to
e.g., Birhane et al. (2021b).
We also highlight the potential societal benefits of our
retrieval-augmented multimodal model. First, as our model
requires much less compute for training than existing mod-
els (§4.3), it can provide energy savings. Second, retrieval
helps capture long-tail knowledge (e.g., rare entities or mi-
nority groups), which can contribute to more fair multi-
modal models. Third, retrieval naturally provides the prove-
nance of knowledge, offering better interpretability and
explainability about model predictions. Retrieval augmen-
tation also helps make image/text generation faithful to the
retrieved evidence documents ( §5.1), potentially helping
reduce unintentionally fake or hallucinated outputs.
B. Related work
Vision-language multimodal models. Various models
have been developed for text-to-image generation. Typi-
cally, these models are autoregressive Transformer-based,
e.g., DALL-E (Ramesh et al., 2021) and Parti (Yu et al.,
2022), or diffusion-based, e.g., Imagen (Saharia et al., 2022),
DALL-E 2 (Ramesh et al., 2022) and Stable Diffusion (Rom-
bach et al., 2022). Meanwhile, several works also study
image-to-text generation (Cho et al., 2020; Wang et al.,
2022b). In particular, Flamingo (Alayrac et al., 2022) is a
Transformer-based image-to-text generation model, with in-
context learning ability. Recently, CM3 (Aghajanyan et al.,
2022) provides a unified model that uses a Transformer to
perform both text and image generation. To make use of
this generality, we will build on CM3 to design our model.
While the above models have achieved strong performance
on image and text generation, they store all their knowledge
inside the model, which tends to require a lot of param-
eters (e.g., 10B) and training data (e.g., 1B images). To
address this limitation, we augment them with an ability to
refer to relevant examples from an external memory when
generating images/text. With this augmentation, our model
12



Source: data\tc16_2312.10997v5\referenced_papers\[176]_2211.12561.pdf (Page 8):

Retrieval-Augmented Multimodal Language Modeling
procedure k times, each using a different pair of demon-
stration examples, and take the average ensemble of the
predicted probability (“X” and “Y”) across the k passes.3
The table in Figure 8 bottom shows the results of k-shot
(binary) classification accuracy, with k = 1, 2, 4, 8. Across
all k’s, our RA-CM3 obtains significantly improved accu-
racy over the baseline CM3, which were not trained with
retrieved documents in context. In particular, RA-CM3 al-
ready performs reasonably well at one-shot (0.78 accuracy
at k = 1). This result suggests that RA-CM3 has acquired
a strong in-context learning ability, especially given that
we use non-semantic labels for image classes in this eval-
uation. Moreover, we find that increasing k consistently
improves accuracy for the k values above (0.90 accuracy at
k = 8). This observation suggests that ensemble is an effec-
tive method to increase the number of in-context examples
to provide for the model.
6. Conclusion
We presented a retrieval-augmented multimodal model that
can retrieve and refer to an external memory for generating
images and text. Specifically, we implemented a multi-
modal retriever using the pretrained CLIP and designed
a retrieval-augmented generator using the CM3 architec-
ture. Our resulting model, named RA-CM3, outperforms
existing multimodal models on both image and caption
generation tasks, while requiring much less training com-
pute. Moreover, RA-CM3 exhibits novel capabilities such
as knowledge-intensive image generation and multimodal
in-context learning.
This work aims to offer a general and modular retrieval aug-
mentation framework for multimodal models. We believe
this opens up various exciting research avenues, such as im-
proving the multimodal retriever and generator, extending
modalities beyond image and text, and further investigating
multimodal prompting and in-context learning.
Acknowledgements
We thank members of the Meta AI team, Stanford P-Lambda
and SNAP groups, as well as our anonymous reviewers for
providing valuable feedback.
3An alternative way to usek-shot examples could be to prepend
all the k pairs of demonstrations directly in RA-CM3’s context,
but this would take a significant sequence length in Transformer
and might not be easy to scale. We find that the ensemble-based
method performs well empirically, and comes with benefits such
as being more scalable (parallel runs of shorter-length passes) and
principled (agnostic to the order of the k examples).
References
Agarwal, O., Ge, H., Shakeri, S., and Al-Rfou, R.
Knowledge graph based synthetic corpus generation for
knowledge-enhanced language model pre-training. In
North American Chapter of the Association for Computa-
tional Linguistics (NAACL), 2021.
Aghajanyan, A., Huang, B., Ross, C., Karpukhin, V ., Xu, H.,
Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis, M.,
and Zettlemoyer, L. CM3: A causal masked multimodal
model of the internet. arXiv preprint arXiv:2201.07520,
2022.
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,
Hasson, Y ., Lenc, K., Mensch, A., Millican, K., Reynolds,
M., et al. Flamingo: a visual language model for few-shot
learning. arXiv preprint arXiv:2204.14198, 2022.
Ashual, O., Sheynin, S., Polyak, A., Singer, U., Gafni,
O., Nachmani, E., and Taigman, Y . Knn-diffusion: Im-
age generation via large-scale retrieval. arXiv preprint
arXiv:2204.02849, 2022.
Birhane, A., Prabhu, V . U., and Kahembwe, E. Multimodal
datasets: misogyny, pornography, and malignant stereo-
types. arXiv preprint arXiv:2110.01963, 2021a.
Birhane, A., Prabhu, V . U., and Kahembwe, E. Ethical
considerations of generative AI. AI for Content Creation
Workshop, CVPR, 2021b.
Blattmann, A., Rombach, R., Oktay, K., and Ommer, B.
Retrieval-augmented diffusion models. arXiv preprint
arXiv:2204.11824, 2022.
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford,
E., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B.,
Damoc, B., Clark, A., et al. Improving language models
by retrieving from trillions of tokens. In International
Conference on Machine Learning (ICML), 2022.
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
In Advances in Neural Information Processing Systems
(NeurIPS), 2020.
Chen, W., Hu, H., Chen, X., Verga, P., and Cohen, W. W.
Murag: Multimodal retrieval-augmented generator for
open question answering over images and text. In Empir-
ical Methods in Natural Language Processing (EMNLP),
2022a.
Chen, W., Hu, H., Saharia, C., and Cohen, W. W. Re-
imagen: Retrieval-augmented text-to-image generator.
arXiv preprint arXiv:2209.14491, 2022b.
9



Source: data\tc16_2312.10997v5\referenced_papers\[176]_2211.12561.pdf (Page 6):

Retrieval-Augmented Multimodal Language Modeling
Figure 4.Text-to-image generation involving rarecomposition
of knowledge. Our retrieval-augmented model (RA-CM3) can
generate faithful images from captions that contain a rare or un-
seen composition of entities (e.g., “French flag” + “moon”, “Mount
Rushmore” + “Japanese cherry”). On the other hand, baseline mod-
els without retrieval capabilities (vanilla CM3, Stable Diffusion)
tend to struggle on these examples, e.g., generate a US flag instead
of a French flag on the moon.
Figure 5.Our model can perform better image infilling.Infilling
an image requires world knowledge, e.g., to recover the masked
patches of the above image, the model needs to know about skiing.
While the vanilla CM3 (no retrieval) tends to simply infill legs, our
RA-CM3 (with retrieval) successfully recovers both legs and skis.
from the model and then re-ranking them using the CLIP
score with respect to the input caption. We then apply an
off-the-shelf super-resolution tool (Rombach et al., 2022).
World knowledge. Figure 3 shows model outputs for
caption-to-image generation that involves world knowledge
(e.g., specific entities). We find that our RA-CM3 model can
generate correct images from entity-rich captions thanks to
the access to retrieved images in the context. For example,
RA-CM3’s outputs faithfully capture the visual characteris-
tics of various entities (e.g., the shape and painting of Ming
Figure 6.Our model can perform image editing.Instead of using
retrieved examples in our RA-CM3’s context (Figure 5), we can
also intervene and manually specify the in-context examples to
control image infilling. For instance, we can place an image of a
person wearing a red jacket in the context to edit the black jacket
in the original image to be red (Figure top).
Figure 7.Controllable image generation. Our RA-CM3 model
can control the style of caption-to-image generation by prepending
demonstration examples in the generator’s context. For instance,
when generating an image of “a house taken on an autumn day”
(Figure top), we can specify a concrete style by providing demon-
stration images (e.g., image of a triangular wooden house and
image of orange autumn leaves background). Consequently, RA-
CM3 generates an image that follows the visual characteristics of
these in-context images.
Dynasty vase, the amount of Callanish standing stones). On
the other hand, baseline models without retrieval capabilities
(vanilla CM3, Stable Diffusion) tend to struggle, especially
when the caption involves rare entities (e.g., “Ming Dynasty
vase”, “Oriental Pearl tower”, “Dragon and Tiger Pagodas”).
Composition of knowledge. Figure 4 shows model outputs
for caption-to-image generation that involves rare compo-
sition of knowledge. We find that our retrieval-augmented
model can generate faithful images from captions that con-
tain a rare or unseen composition of entities (e.g., “French
7



### Claim 85/179

#### Claim Text
BLIP-2 [177] leverages frozen image encoders alongside LLMs for efficient visual language pre-training, enabling zeroshot image-to-text conversions.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[177]_2301.12597.pdf (Page 7):

BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models
Model #Trainable
Params
Flickr30K Zero-shot (1K test set) COCO Fine-tuned (5K test set)
Image → Text Text → Image Image → Text Text → Image
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
Dual-encoder models
CLIP (Radford et al., 2021) 428M 88.0 98.7 99.4 68.7 90.6 95.2 - - - - - -
ALIGN (Jia et al., 2021) 820M 88.6 98.7 99.7 75.7 93.8 96.8 77.0 93.5 96.9 59.9 83.3 89.8
FILIP (Yao et al., 2022) 417M 89.8 99.2 99.8 75.0 93.4 96.3 78.9 94.4 97.4 61.2 84.3 90.6
Florence (Yuan et al., 2021) 893M 90.9 99.1 - 76.7 93.6 - 81.8 95.2 - 63.2 85.7 -
BEIT-3(Wang et al., 2022b) 1.9B 94.9 99.9 100.0 81.5 95.6 97.8 84.8 96.5 98.3 67.2 87.7 92.8
Fusion-encoder models
UNITER (Chen et al., 2020) 303M 83.6 95.7 97.7 68.7 89.2 93.9 65.7 88.6 93.8 52.9 79.9 88.0
OSCAR (Li et al., 2020) 345M - - - - - - 70.0 91.1 95.5 54.0 80.8 88.5
VinVL (Zhang et al., 2021) 345M - - - - - - 75.4 92.9 96.2 58.8 83.5 90.3
Dual encoder + Fusion encoder reranking
ALBEF (Li et al., 2021) 233M 94.1 99.5 99.7 82.8 96.3 98.1 77.6 94.3 97.2 60.7 84.3 90.5
BLIP (Li et al., 2022) 446M 96.7 100.0 100.0 86.7 97.3 98.7 82.4 95.4 97.9 65.1 86.3 91.8
BLIP-2ViT-L 474M 96.9 100.0 100.0 88.6 97.6 98.9 83.5 96.0 98.0 66.3 86.5 91.8
BLIP-2ViT-g 1.2B 97.6 100.0 100.0 89.7 98.1 98.9 85.4 97.0 98.5 68.3 87.7 92.6
Table 5.Comparison with state-of-the-art image-text retrieval methods, finetuned on COCO and zero-shot transferred to Flickr30K.
COCO finetuning
objectives
Image→Text Text →Image
R@1 R@5 R@1 R@5
ITC + ITM 84.5 96.2 67.2 87.1
ITC + ITM + ITG 85.4 97.0 68.3 87.7
Table 6.The image-grounded text generation (ITG) loss improves
image-text retrieval performance by enforcing the queries to extract
language-relevant visual features.
4.4. Image-Text Retrieval
Since image-text retrieval does not involve language gener-
ation, we directly finetune the first-stage-pretrained model
w/o LLM. Specifically, we finetune the image encoder to-
gether with Q-Former on COCO using the same objectives
(i.e. ITC, ITM, and ITG) as pre-training. We then evaluate
the model for both image-to-text retrieval and text-to-image
retrieval on COCO and Flickr30K (Plummer et al., 2015)
datasets. During inference, we follow Li et al. (2021; 2022)
which first select k = 128candidates based on the image-
text feature similarity, followed by a re-ranking based on
pairwise ITM scores. We experiment with both ViT-L and
ViT-g as the image encoder. Detailed hyperparameters can
be found in the appendix.
The results are shown in Table 5. BLIP-2 achieves state-
of-the-art performance with significant improvement over
existing methods on zero-shot image-text retrieval.
The ITC and ITM losses are essential for image-text retrieval
as they directly learn image-text similarity. In Table 6, we
show that the ITG (image-grounded text generation) loss is
also beneficial for image-text retrieval. This result supports
our intuition in designing the representation learning objec-
tives: the ITG loss enforces the queries to extract visual
features most relevant to the text, thus improving vision-
language alignment.
5. Limitation
Recent LLMs can perform in-context learning given few-
shot examples. However, our experiments with BLIP-2
do not observe an improved VQA performance when pro-
viding the LLM with in-context VQA examples. We at-
tribute the lack of in-context learning capability to our pre-
training dataset, which only contains a single image-text
pair per sample. The LLMs cannot learn from it the correla-
tion among multiple image-text pairs in a single sequence.
The same observation is also reported in the Flamingo pa-
per, which uses a close-sourced interleaved image and text
dataset (M3W) with multiple image-text pairs per sequence.
We aim to create a similar dataset in future work.
BLIP-2’s image-to-text generation could have unsatisfactory
results due to various reasons including inaccurate knowl-
edge from the LLM, activating the incorrect reasoning path,
or not having up-to-date information about new image con-
tent (see Figure 7). Furthermore, due to the use of frozen
models, BLIP-2 inherits the risks of LLMs, such as out-
putting offensive language, propagating social bias, or leak-
ing private information. Remediation approaches include
using instructions to guide model’s generation or training
on a filtered dataset with harmful content removed.
6. Conclusion
We propose BLIP-2, a generic and compute-efficient method
for vision-language pre-training that leverages frozen pre-
trained image encoders and LLMs. BLIP-2 achieves state-
of-the-art performance on various vision-language tasks
while having a small amount of trainable parameters during
pre-training. BLIP-2 also demonstrates emerging capabil-
ities in zero-shot instructed image-to-text generation. We
consider BLIP-2 as an important step towards building a
multimodal conversational AI agent.



Source: data\tc16_2312.10997v5\referenced_papers\[177]_2301.12597.pdf (Page 1):

BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models
visual feature for the LLM to output the desired text. In
the first pre-training stage, we perform vision-language rep-
resentation learning which enforces the Q-Former to learn
visual representation most relevant to the text. In the second
pre-training stage, we perform vision-to-language genera-
tive learning by connecting the output of the Q-Former to a
frozen LLM, and trains the Q-Former such that its output
visual representation can be interpreted by the LLM.
We name our VLP framework as BLIP-2: Bootstrapping
Language-Image Pre-training with frozen unimodal models.
The key advantages of BLIP-2 include:
• BLIP-2 effectively leverages both frozen pre-trained im-
age models and language models. We bridge the modality
gap using a Q-Former pre-trained in two-stages: repre-
sentation learning stage and generative learning stage.
BLIP-2 achieves state-of-the-art performance on various
vision-language tasks including visual question answer-
ing, image captioning, and image-text retrieval.
• Powered by LLMs ( e.g. OPT (Zhang et al., 2022),
FlanT5 (Chung et al., 2022)), BLIP-2 can be prompted to
perform zero-shot image-to-text generation that follows
natural language instructions, which enables emerging
capabilities such as visual knowledge reasoning, visual
conversation, etc. (see Figure 4 for examples).
• Due to the use of frozen unimodal models and a
lightweight Q-Former, BLIP-2 is more compute-efficient
than exisiting state-of-the-arts. For example, BLIP-2 out-
performs Flamingo (Alayrac et al., 2022) by 8.7% on
zero-shot VQAv2, while using 54 × fewer trainable pa-
rameters. Furthermore, our results show that BLIP-2 is a
generic method that can harvest more advanced unimodal
models for better VLP performance.
2. Related Work
2.1. End-to-end Vision-Language Pre-training
Vision-language pre-training aims to learn multimodal foun-
dation models with improved performance on various vision-
and-language tasks. Depending on the downstream task,
different model architectures have been proposed, including
the dual-encoder architecture (Radford et al., 2021; Jia et al.,
2021), the fusion-encoder architecture (Tan & Bansal, 2019;
Li et al., 2021), the encoder-decoder architecture (Cho et al.,
2021; Wang et al., 2021b; Chen et al., 2022b), and more
recently, the unified transformer architecture (Li et al., 2022;
Wang et al., 2022b). Various pre-training objectives have
also been proposed over the years, and have progressively
converged to a few time-tested ones: image-text contrastive
learning (Radford et al., 2021; Yao et al., 2022; Li et al.,
2021; 2022), image-text matching (Li et al., 2021; 2022;
Wang et al., 2021a), and (masked) language modeling (Li
et al., 2021; 2022; Yu et al., 2022; Wang et al., 2022b).
Most VLP methods perform end-to-end pre-training using
large-scale image-text pair datasets. As the model size keeps
increasing, the pre-training can incur an extremely high
computation cost. Moreover, it is inflexible for end-to-end
pre-trained models to leverage readily-available unimodal
pre-trained models, such as LLMs (Brown et al., 2020;
Zhang et al., 2022; Chung et al., 2022).
2.2. Modular Vision-Language Pre-training
More similar to us are methods that leverage off-the-shelf
pre-trained models and keep them frozen during VLP. Some
methods freeze the image encoder, including the early work
which adopts a frozen object detector to extract visual fea-
tures (Chen et al., 2020; Li et al., 2020; Zhang et al., 2021),
and the recent LiT (Zhai et al., 2022) which uses a frozen
pre-trained image encoder for CLIP (Radford et al., 2021)
pre-training. Some methods freeze the language model
to use the knowledge from LLMs for vision-to-language
generation tasks (Tsimpoukelli et al., 2021; Alayrac et al.,
2022; Chen et al., 2022a; Ma ˜nas et al., 2023; Tiong et al.,
2022; Guo et al., 2022). The key challenge in using a frozen
LLM is to align visual features to the text space. To achieve
this, Frozen (Tsimpoukelli et al., 2021) finetunes an image
encoder whose outputs are directly used as soft prompts
for the LLM. Flamingo (Alayrac et al., 2022) inserts new
cross-attention layers into the LLM to inject visual features,
and pre-trains the new layers on billions of image-text pairs.
Both methods adopt the language modeling loss, where the
language model generates texts conditioned on the image.
Different from existing methods, BLIP-2 can effectively and
efficiently leverage both frozen image encoders and frozen
LLMs for various vision-language tasks, achieving stronger
performance at a lower computation cost.
3. Method
We propose BLIP-2, a new vision-language pre-training
method that bootstraps from frozen pre-trained unimodal
models. In order to bridge the modality gap, we propose a
Querying Transformer (Q-Former) pre-trained in two stages:
(1) vision-language representation learning stage with a
frozen image encoder and (2) vision-to-language genera-
tive learning stage with a frozen LLM. This section first
introduces the model architecture of Q-Former, and then
delineates the two-stage pre-training procedures.
3.1. Model Architecture
We propose Q-Former as the trainable module to bridge the
gap between a frozen image encoder and a frozen LLM. It
extracts a fixed number of output features from the image
encoder, independent of input image resolution. As shown
in Figure 2, Q-Former consists of two transformer submod-
ules that share the same self-attention layers: (1) an image
transformer that interacts with the frozen image encoder



Source: data\tc16_2312.10997v5\referenced_papers\[177]_2301.12597.pdf (Page 5):

BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models
Models #Trainable
Params
Open-
sourced?
Visual Question Answering Image Captioning Image-Text Retrieval
VQAv2 (test-dev) NoCaps (val) Flickr (test)
VQA acc. CIDEr SPICE TR@1 IR@1
BLIP (Li et al., 2022) 583M ✓ - 113.2 14.8 96.7 86.7
SimVLM (Wang et al., 2021b) 1.4B ✗ - 112.2 - - -
BEIT-3 (Wang et al., 2022b) 1.9B ✗ - - - 94.9 81.5
Flamingo (Alayrac et al., 2022) 10.2B ✗ 56.3 - - - -
BLIP-2 188M ✓ 65.0 121.6 15.8 97.6 89.7
Table 1.Overview of BLIP-2 results on various zero-shot vision-language tasks. Compared with previous state-of-the-art models. BLIP-2
achieves the highest zero-shot performance while requiring the least number of trainable parameters during vision-language pre-training.
Models #Trainable
Params
#Total
Params
VQAv2 OK-VQA GQA
val test-dev test test-dev
VL-T5no-vqa 224M 269M 13.5 - 5.8 6.3
FewVLM (Jin et al., 2022) 740M 785M 47.7 - 16.5 29.3
Frozen (Tsimpoukelli et al., 2021) 40M 7.1B 29.6 - 5.9 -
VLKD (Dai et al., 2022) 406M 832M 42.6 44.5 13.3 -
Flamingo3B (Alayrac et al., 2022) 1.4B 3.2B - 49.2 41.2 -
Flamingo9B (Alayrac et al., 2022) 1.8B 9.3B - 51.8 44.7 -
Flamingo80B (Alayrac et al., 2022) 10.2B 80B - 56.3 50.6 -
BLIP-2 ViT-L OPT2.7B 104M 3.1B 50.1 49.7 30.2 33.9
BLIP-2 ViT-g OPT2.7B 107M 3.8B 53.5 52.3 31.7 34.6
BLIP-2 ViT-g OPT6.7B 108M 7.8B 54.3 52.6 36.4 36.4
BLIP-2 ViT-L FlanT5XL 103M 3.4B 62.6 62.3 39.4 44.4
BLIP-2 ViT-g FlanT5XL 107M 4.1B 63.1 63.0 40.7 44.2
BLIP-2 ViT-g FlanT5XXL 108M 12.1B 65.2 65.0 45.9 44.7
Table 2.Comparison with state-of-the-art methods on zero-shot visual question answering.
4. Experiment
Table 1 provides an overview of the performance of BLIP-2
on various zero-shot vision-language tasks. Compared to
previous state-of-the-art models, BLIP-2 achieves improved
performance while requiring substantially fewer number of
trainable parameters during vision-language pre-training.
4.1. Instructed Zero-shot Image-to-Text Generation
BLIP-2 effectively enables a LLM to understand images
while preserving its capability in following text prompts,
which allows us to control image-to-text generation with
instructions. We simply append the text prompt after the
visual prompt as input to the LLM. Figure 4 shows exam-
ples to demonstrate a wide range of zero-shot image-to-text
capabilities including visual knowledge reasoning, visual
commensense reasoning, visual conversation, personalized
image-to-text generation, etc.
Zero-shot VQA. We perform quantitative evaluation on the
zero-shot visual question answering task. For OPT models,
we use the prompt “Question: {} Answer:”. For FlanT5
models, we use the prompt “Question: {} Short answer:”.
During generation, we use beam search with a beam width
of 5. We also set the length-penalty to -1 which encourages
shorter answers that align better with human annotation.
As shown in Table 2. BLIP-2 achieves state-of-the-art result
on the VQAv2 (Goyal et al., 2017) and GQA (Hudson &
Manning, 2019) datasets. It outperforms Flamingo80B by
8.7% on VQAv2, despite having 54x fewer trainable parame-
ters. On the OK-VQA (Marino et al., 2019) dataset, BLIP-2
comes secondary to Flamingo80B. We hypothesis that this is
because OK-VQA focuses more on open-world knowledge
than visual understanding, and the 70B Chinchilla (Hoff-
mann et al., 2022) language model from Flamingo80B pos-
sesses more knowledge than the 11B FlanT5XXL.
We make a promising observation from Table 2: a stronger
image encoder or a stronger LLM both lead to better per-
formance. This observation is supported by several facts:
(1) ViT-g outperforms ViT-L for both OPT and FlanT5. (2)
Within the same LLM family, larger models outperform
smaller ones. (3) FlanT5, an instruction-tuned LLM, out-
performs the unsupervised-trained OPT on VQA. This ob-
servation validates BLIP-2 as a generic vision-language
pre-training methodthat can efficiently harvest the rapid
advances in vision and natural language communities.
Effect of Vision-Language Representation Learning.
The first-stage representation learning pre-trains the Q-
Former to learn visual features relevant to the text, which
reduces the burden of the LLM to learn vision-language
alignment. Without the representation learning stage, Q-



Source: data\tc16_2312.10997v5\referenced_papers\[177]_2301.12597.pdf (Page 0):

BLIP-2: Bootstrapping Language-Image Pre-training
with Frozen Image Encoders and Large Language Models
Junnan Li Dongxu Li Silvio Savarese Steven Hoi
Salesforce Research
https://github.com/salesforce/LAVIS/tree/main/projects/blip2
Abstract
The cost of vision-and-language pre-training has
become increasingly prohibitive due to end-to-
end training of large-scale models. This paper
proposes BLIP-2, a generic and efficient pre-
training strategy that bootstraps vision-language
pre-training from off-the-shelf frozen pre-trained
image encoders and frozen large language mod-
els. BLIP-2 bridges the modality gap with a
lightweight Querying Transformer, which is pre-
trained in two stages. The first stage boot-
straps vision-language representation learning
from a frozen image encoder. The second stage
bootstraps vision-to-language generative learning
from a frozen language model. BLIP-2 achieves
state-of-the-art performance on various vision-
language tasks, despite having significantly fewer
trainable parameters than existing methods. For
example, our model outperforms Flamingo80B by
8.7% on zero-shot VQAv2 with 54x fewer train-
able parameters. We also demonstrate the model’s
emerging capabilities of zero-shot image-to-text
generation that can follow natural language in-
structions.
1. Introduction
Vision-language pre-training (VLP) research has witnessed
a rapid advancement in the past few years, where pre-trained
models with increasingly larger scale have been developed
to continuously push the state-of-the-art on various down-
stream tasks (Radford et al., 2021; Li et al., 2021; 2022;
Wang et al., 2022a; Alayrac et al., 2022; Wang et al., 2022b).
However, most state-of-the-art vision-language models in-
cur a high computation cost during pre-training, due to
end-to-end training using large-scale models and datasets.
Vision-language research sits at the intersection between
vision and language, therefore it is naturally expected
that vision-language models can harvest from the readily-
available unimodal models from the vision and natural lan-
guage communities. In this paper, we propose a generic and
Querying Transformer
Q-Former
Large
Language
Model
(LLM)
Queries
Text
Image
Encoder
Bootstrapping Pre-trained
Image Models
Bootstrapping Pre-trained
Large Language Models (LLMs)
…
Vision-and-Language
Representation Learning
Vision-to-Language 
Generative Learning
Write a romantic message 
that goes along this photo.
Love is like a sunset, it’s 
hard to see it coming but 
when it does it’s so beautiful.
Figure 1.Overview of BLIP-2’s framework. We pre-train a
lightweight Querying Transformer following a two-stage strat-
egy to bridge the modality gap. The first stage bootstraps vision-
language representation learning from a frozen image encoder. The
second stage bootstraps vision-to-language generative learning
from a frozen LLM, which enables zero-shot instructed image-to-
text generation (see Figure 4 for more examples).
compute-efficient VLP method by bootstrapping from off-
the-shelf pre-trained vision models and language models.
Pre-trained vision models offer high-quality visual represen-
tation. Pre-trained language models, in particular large lan-
guage models (LLMs), offer strong language generation and
zero-shot transfer abilities. To reduce computation cost and
counteract the issue of catastrophic forgetting, the unimodal
pre-trained models remain frozen during the pre-training.
In order to leverage pre-trained unimodal models for VLP,
it is key to facilitate cross-modal alignment. However, since
LLMs have not seen images during their unimodal pre-
training, freezing them makes vision-language alignment
in particular challenging. In this regard, existing methods
(e.g. Frozen (Tsimpoukelli et al., 2021), Flamingo (Alayrac
et al., 2022)) resort to an image-to-text generation loss,
which we show is insufficient to bridge the modality gap.
To achieve effective vision-language alignment with frozen
unimodal models, we propose a Querying Transformer (Q-
Former) pre-trained with a new two-stage pre-training strat-
egy. As shown in Figure 1, Q-Former is a lightweight trans-
former which employs a set of learnable query vectors to
extract visual features from the frozen image encoder. It
acts as an information bottleneck between the frozen image
encoder and the frozen LLM, where it feeds the most useful
arXiv:2301.12597v3  [cs.CV]  15 Jun 2023



Source: data\tc16_2312.10997v5\referenced_papers\[177]_2301.12597.pdf (Page 10):

BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models
Yu, J., Wang, Z., Vasudevan, V ., Yeung, L., Seyedhosseini,
M., and Wu, Y . Coca: Contrastive captioners are image-
text foundation models. arXiv preprint arXiv:2205.01917,
2022.
Yuan, L., Chen, D., Chen, Y ., Codella, N., Dai, X., Gao,
J., Hu, H., Huang, X., Li, B., Li, C., Liu, C., Liu, M.,
Liu, Z., Lu, Y ., Shi, Y ., Wang, L., Wang, J., Xiao, B.,
Xiao, Z., Yang, J., Zeng, M., Zhou, L., and Zhang, P.
Florence: A new foundation model for computer vision.
arXiv preprint arXiv:2111.11432, 2021.
Zhai, X., Wang, X., Mustafa, B., Steiner, A., Keysers, D.,
Kolesnikov, A., and Beyer, L. Lit: Zero-shot transfer with
locked-image text tuning. In CVPR, pp. 18102–18112,
2022.
Zhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L.,
Choi, Y ., and Gao, J. Vinvl: Making visual representa-
tions matter in vision-language models. arXiv preprint
arXiv:2101.00529, 2021.
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M. T., Li, X., Lin, X. V .,
Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig,
D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer,
L. OPT: open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068, 2022.



### Claim 86/179

#### Claim Text
The “Visualize Before You Write” method [178] employs image generation to steer the LM’s text generation, showing promise in open-ended text generation tasks.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[178]_2210.03765.pdf (Page 2):

Input Context x  
A man is seen skiing behind a boat. He holds on tight as he is pulled through the water. The man …
Target :  is water skiing until the end of the clip.y
Prediction : then moves to the side and begins to swim./uni0302 y
AE Decoder
AE Encoder
CLIP 
Visual 
Encoder
...Mapping 
Network
... Language 
Model
Projection 
Layer
Machine Imagination I 
v 
 /uni0302 t
c1 c2 cl t1 t2 tm 
Text Emb.Visual Prefix
Input Visual Feature
L
"
acher
L
con
#
as
!
ve
Predicted Text Feature
Text-to-Image 
Generation
Visually-Guided Text Generation
Diffusion 
Model
Text Encoder
randomly 
initialized 
image
Figure 2: An overview of our iNLG. Given an input contextx, we ﬁrst visualize the context with the text-to-image
generation model. Then we use the machine-generated image I as the additional visual supervision to guide the
language model in open-ended text generation. The visual feature is provided as a source of input to the LM in the
form of the visual preﬁx. Aside from the teacher forcing objective Lteacher, we also enforce the LM to generate text
that is semantically similar to the machine imagination with a contrastive training objective Lcontrastive.
3 Method
3.1 Overview
Open-ended text generation is a task that provides
an input context, and asks the model to generate a
piece of text that is consistent with the context.
This work mainly focused on introducing
machine-rendered images to assist LM in perform-
ing open-ended text generation. More speciﬁcally,
given the context xi, we ﬁrst use a text-to-image
generator to illustrate an image Ii that depicts the
input context. The LM is prompted with image
Ii as the visual preﬁx along with the text context
xi, and will incorporate the multimodal input to
generate the output text ˆyi.
Figure 2 provides an overview of our iNLG
framework, which mainly involves two modules.
The ﬁrst module is a text-to-image generator that
takes in the input context and illustrates a descrip-
tive image, which we also refer to as the machine
imagination. The second module is a visually-
guided language model that utilizes the machine
imagination as a source of input and also a supervi-
sion that encourages the LM to generate text that is
semantically similar to the visual information.
3.2 Text-to-Image Rendering
In this work, we propose to use images gener-
ated conditioning on the context by the machines
as additional visual information to the LM. The
text-to-image generation backbone is StableDiffu-
sion (Rombach et al., 2022), which mainly consists
of a text encoder, a diffusion model, and an au-
toencoder. The text encoder is from the frozen
CLIP ViT-L/14 (Radford et al., 2021) and encodes
the input text to textual embeddings. The diffu-
sion model uses UNet (Ronneberger et al., 2015)
to provide noise estimation. The UNet is modi-
ﬁed so as to attend to the input textual embeddings.
The encoder of the pretrained autoencoder encodes
images into the lower-resolution latent maps zT.
At each step t, the diffusion model provides the
noise estimation ϵand modiﬁes zt correspondingly.
The decoder of the pretrained autoencoder takes
the ﬁnal noise-free latent map z and generates the
image prediction. StableDiffusion is trained with
LAION-5B (Schuhmann et al., 2022).
3.3 Visually Guided Text Generation
Visual Preﬁx Construction One can encode the
visual information with the pre-trained visual mod-
els. However, such visual embedding may lie in a
representation space different from the LM due to
the discrepancy between models. One way of intro-
ducing features extracted by another network to the
current model is through feature mapping (Mokady
et al., 2021). With a dataset of image-text pairs
pI1,x1q, we can pre-train a mapping network F for
a given LM in an image captioning formulation.
More speciﬁcally, we encode I1 with the visual
encoder Encvisual and receive its visual features v1.
Then we apply the mapping network F over v1,
and receive a sequence of lvisual preﬁxes:
c1
1,c1
2,...,c 1
l “Fpv1q“ FpEncvisualpI1qq (1)



Source: data\tc16_2312.10997v5\referenced_papers\[178]_2210.03765.pdf (Page 1):

generated images to guide the language model
(LM) in open-ended text generation. More specif-
ically, we visualize machine imagination for the
input context by rendering images with StableD-
iffusion (Rombach et al., 2022), a state-of-the-art
text-to-image generator. The machine imagination
acts as additional visual supervision to guide LMs
in generating informative and coherent text in two
ways. Firstly, the machine-generated images are
introduced as the input to the LM in the form of the
visual preﬁx. Secondly, we designed a contrastive
training objective that enforces the generated text
to be semantically similar to the visual supervision.
We conduct experiments on three open-ended
text generation tasks, namely text completion, story
generation, and concept-to-text generation. Exten-
sive experiments in the few-shot settings show bet-
ter or competitive performance to state-of-the-art
baselines on both automatic metrics and human
evaluation. Experiments with full-data settings
show that introducing machine-generated visual
supervision with our iNLG yields consistent im-
provements on various LM models including GPT-
2 (Radford et al., 2019), BART (Lewis et al., 2020),
and T5 (Raffel et al., 2020).
Our main contributions are as follows:
• We introduce a novel paradigm that lever-
ages machine-generated images to guide open-
ended text generation. This endows the ma-
chines with the ability of creative visualization
that human writers often demonstrate.
• We distill the vision information from the pre-
trained multimodal models and further con-
struct visual preﬁxes to guide language mod-
els performing text generation with teacher
forcing and contrastive objectives.
• Extensive experiments show the effective-
ness of iNLG as a model-agnostic framework
in open-ended text generation tasks, includ-
ing text completion, story generation, and
concept-to-text in both few-shot and full-data
settings.
2 Related Work
Open-ended Conditional Text Generation is
the task of generating a coherent portion of the
text based on the given context. Recent advances
in pre-trained models have pushed frontier in the
open-ended conditional text generation, such as
text completion(See et al., 2019; Ippolito et al.,
2020), story generation (Guan et al., 2020; Fan
et al., 2018; Yao et al., 2019) and concept-to-text
generation (Zhou et al., 2021; Liu et al., 2021). De-
spite the success of large language models, text
degeneration and semantic coverage still remain
as two core technical challenges in few-shot open-
ended text generation. To improve the text cover-
age, StoryEndGen (Guan et al., 2019) leverages the
knowledge graph to encode context sequentially.
Fan et al. (2018) and Yao et al. (2019) plan the
content (premise or keywords) ﬁrst and then en-
courage the generation based on planned content.
To mitigate the text degeneration, SimCTG (Su
et al., 2022b) uses a contrastive training strategy
to encourage the model to learn isotropic token
embeddings. Similar to our approach, Wang et al.
(2022a) generates a scene graph for each concept
and combines them with text for the model input.
Previous work has proposed to add visual informa-
tion to LM by retrieving images from the Internet
or large-scale image sets (Yang et al., 2020; Cho
et al., 2021; Su et al., 2022a). However, the re-
trieved images may fail to fully incorporate the
context, which will misguide the LM from yield-
ing contextually consistent predictions. 2 Unlike
prior work, our approach leverages images gener-
ated conditioning on the context to assist the text
generation process.
Visually-aided NLP Recent work show the
power of visual guidance in natural language pro-
cessing, spanning from the language representation
learning (Lu et al., 2019; Li et al., 2019; Sun et al.,
2019; Luo et al., 2020; Chen et al., 2020; Li et al.,
2020; Tan and Bansal, 2020; Lu et al., 2022), the
downstream tasks (Grubinger et al., 2006; Elliott
et al., 2016; Xie et al., 2019; Christie et al., 2016;
Shi et al., 2019; Lu et al., 2022) and evaluation (Zhu
et al., 2021). They either leverage visual informa-
tion from an external vision-and-language corpus
or obtain such visual knowledge from the large pre-
trained model. In this line of work, imagination
achieves promising performance in various NLP
domains (Long et al., 2021; Zhu et al., 2021; Wang
et al., 2022a; Lu et al., 2022). Previous imagination-
based work in NLP either study non-generation
problems (Zhu et al., 2021; Lu et al., 2022) or
utilize non-visual information (Long et al., 2021;
Wang et al., 2022a). Our work explores the poten-
tial of generating visual imagination to improve
open-ended text generation tasks.
2Figure 8 shows examples where the image retrieved from
the search engine is irrelevant with the input context.



Source: data\tc16_2312.10997v5\referenced_papers\[178]_2210.03765.pdf (Page 8):

Tune LM Pretrain Map. Tune Map. diversityÒ MAUVEÒ
   15.52 0.47
   78.20 33.79
   27.06 1.83
   76.36 25.15
   87.45 48.06
   88.68 51.81
   89.05 55.61
   92.68 60.62
Table 6: Performance of our iNLG on few-shot sen-
tence completion with ablated settings on whether to
tune the LM, pretrain the mapping network (Pretrain
Map.) and tune the mapping network (Tune Map.).
Context: They mix the eggs around a bowl and place butter and milk into 
another bowl and mix them all together. They…
GPT2
(no ﬁnetune) will be very soft and ﬂuﬀy.
GPT2 
(text-only ﬁnetune) are ready to use.
GPT2 + iNLG then put a bowl on the end of the sink and 
put the mixture in the sink.
Generated 
Image:
Context:    Barry at the Mall. Barry and his friends spent a day at the mall.
SimCTG
Barry was at the mall with his friends. They were 
all very excited to see him at the mall. They all 
had a great time at the mall. They all had a great 
time at work. They all had a great time at school
GPT2 
(text-only)
They went to the local mall and bought a new car. 
They drove to the mall and bought a new car. 
They drove to the mall and bought a new car. 
Barry's friends were very happy they bought a 
new car.
GPT2 
+iNLG
They were swamped by the crowds. Barry was 
scared to go home. They tried to avoid the mall, 
but they were stopped by the people at the mall.
Generated 
Image:
Context:      Two girls are standing in a yard wearing cheerleading outﬁts. 
                     A girl…
GPT2 
(text-only)
is standing in a yard with a group of girls.
GPT2 
+iNLG is wearing a white bikini and blue shorts.
Generated Image:
(a) Sentence Completion
(b) Story Generation
Figure 6: Sentence ending and stories generated by
GPT2-based methods tuned with 1% of the training
data. Repetitive contents are underlined. The sentence
ending and story written by our iNLG is coherent with
the context, related to the machine-generated image,
and has minor degeneration. More demonstrative ex-
amples can be found in the Appendix.
degeneration and writes coherent sentence endings
or stories with more creative details in both tasks.
6 Conclusion
In this work, we propose iNLG, a framework
that introduces machine-generated images to guide
open-ended text generation. This endows the ma-
chines with the ability of creative visualization
that human writers often demonstrate. We distill
the vision information from the pre-trained multi-
modal models and further construct visual preﬁxes
to guide language models to perform text gener-
ation with the teacher forcing and the contrastive
objective. Extensive experiments show the effec-
tiveness of iNLG in open-ended text generation
tasks, including text completion, story generation,
and concept-to-text generation in few-shot settings.
Limitations
This work mainly focuses on open-ended text gen-
eration, where the search space for the target output
is inﬁnite, and the language model would beneﬁt
from additional visual imagination distilled from
large text-to-image generation models to produce
coherent and meaningful content. However, we
should note here that despite the commendable per-
formance of text-to-image generation models, there
are certain terms and concepts that are inherently
challenging to visualize, such as numerical values
and abstract philosophical terms. This problem it-
self is an interesting open research question for all
tasks involving text-and-vision. In our current ap-
proach, the images are generated ofﬂine. In future
work, one may explore the integration of text-to-
image and image-to-text modules in an end-to-end
manner, which may be more suitable for longer text
generation that is not covered in this work. Text-to-
image generation models currently have a length
limit on the input text prompt, which may impede
their ability to visualize long text inputs in a single
image. Furthermore, as previously discussed, text-
to-image models may also encounter difﬁculties
in generating images of complex scenes or situa-
tions that are challenging to depict through a single
image. Future research could explore the use of
multiple images or supplementary videos as visual
input in order to provide a more comprehensive
representation of the scene or situation in question.
The iNLG framework can be easily extended to
take video representation by taking longer visual
preﬁxes or iteratively applying visual preﬁxes at
each step.
Ethics Statement
In this work, we use pre-trained multimodal models
to visualize machine imagination. The machine-
generated images may contain uncontrolled bias
if any inductive bias exists from the pre-training
data. Even though we do not witness such an issue
in our study, this may be a potential factor that
affects the quality of the generated text. We do not
anticipate any major ethical concerns given that
all the datasets and models used in this study have
already been released to the public. We reproduce
baselines with the released code repository. For
human evaluation, our study is approved for IRB
exempt. The estimated hourly wage paid to MTurk
annotators is $10.



Source: data\tc16_2312.10997v5\referenced_papers\[178]_2210.03765.pdf (Page 7):

Context 1: One of the guys hits the ball over to the other side and they 
hit it back. Then on the other side of the beach there is a group of 
women also playing volleyball. They…
(a1) Retrieved Image (b1) Generated Image Context: The individual adds chicken to the pan and cooks it. The 
individual adds chopped onions and mushrooms to the pan and cooks 
them. The individual adds some other ingredients…
Repetitive to the input context.  
Not informative.
: and the individual adds them to the pan.
Text-only Input
: and stirs them into the soup.
Text Input + Visual Imagination
Machine
Imagina!on
Context 2: A boy is talking to a camera. He goes into a bathroom and 
drinks a cup of mouthwash. He…
(a2) Retrieved Image (b2) Generated Image
Text2Img Model trender
StableDiffusion   5s
OFA 57s
VQGAN+CLIP 63s
(a) (b)
Figure 3: (a) iNLG’s performance on CommonGen and
ActivityNet with visual supervisions retrieved from the
web or generated by machines. Scores are reported
with error bars. (b) Average time to render an image
on TITAN RTX with each text-to-image generator.
the ﬁrst returned result by Yahoo Image Search;9
(2) images rendered by VQGAN+CLIP (Crowson
et al., 2022);10 (3) images rendered by OFA (Wang
et al., 2022b),11 and (4) images rendered by Sta-
bleDiffusion (Rombach et al., 2022), with which
we report the main results.
As shown in Figure 3(a), the images generated
by machines act as a more effective supervision
than the retrieved images. This validates our mo-
tivation of introducing machine-generated images
over retrieved ones to guide LM in performing text
generation. Among the three text-to-image genera-
tors, VQGAN+CLIP is slightly inferior to the other
two, while StableDiffusion and OFA have mixed
performance. Images generated by StableDiffusion
rank ﬁrst on CommonGen, while images rendered
with OFA score slightly higher on ActivityNet. Fig-
ure 3(b) reports the average image rendering time,
where StableDiffusion is 10ˆfaster when render-
ing images than the other two.
Contrastive Training We examine the effect of
the contrastive training objective on CommonGen,
and the results are presented in Figure 4. We notice
that introducing Lcontrastive improves iNLG’s perfor-
mance on 4 out of 5 listed few-shot setups, which
suggests that our contrastive training objective gen-
erally can assist the LM in composing open-ended
text snippets. One exception is in the extreme few-
shot setting with only 0.1% of training data, where
the amount of data is insufﬁcient to let the LM
form a decent representation. In this case, enforc-
ing the sentence representation to be similar to the
visual supervision with Lcontrastive might misguide
the LM.
Mapping Network & Visual Preﬁx We discuss
the effects of different types of mapping networks
9https://images.search.yahoo.com/
10https://github.com/nerdyrodent/VQGAN-CLIP
11https://github.com/OFA-Sys/OFA
0.1 0.5 1.0 5.0 10.0
Amount of Data for Few-shot Training (%)
20
25
30SPICE
w/o Lcontrastive w/ Lcontrastive
Figure 4: Performance of applying our iNLG on BART-
base for few-shot concept-to-text with ablated train-
ing objective Lcontrastive on various few-shot settings.
Scores are reported with error bars.
5 10 15 20 25
Visual Prefix Length l
88
90
92
94diversity
MLP Transformer
Figure 5: Performance of our iNLG on few-shot sen-
tence completion with various visual preﬁx lengths
and with MLP and Transformer as mapping network.
Scores are reported with error bands.
and various visual preﬁx lengths. Aside from the
8-layer Transformer we used in the main experi-
ments, we also tried a simple Multi-Layer Percep-
tron (MLP) with two fully-connected layers. As
shown in Figure 5, the Transformer-based mapping
network outperforms MLP on all listed l. MLP
has the best performance when visual preﬁx length
l“15, while the Transformer-based mapping net-
work scores highest when l“20.
Model Weight Tuning Table 6 compares the in-
ﬂuence of pre-training/tuning the weights of differ-
ent modules of our iNLG. Generally speaking, tun-
ing the weights during training outperforms freez-
ing the weights, which applies to both the base LM
and the mapping network. In addition, considering
our few-show setup, pre-training the mapping net-
work also helps our iNLG gain better performances.
The best combination is applying the pre-trained
mapping network, and tuning it together with the
base LM on the few-shot downstream task.
Showcase Figure 6 provides two showcases on
few-shot sentence completion and story generation
to compare our iNLG with the GPT2-based base-
lines. SimCTG and GPT2 tuned with text-only
corpus rendering repeated segments, either copy-
ing from the input context, or simply repeating
themselves. In comparison, our iNLG has minor



Source: data\tc16_2312.10997v5\referenced_papers\[178]_2210.03765.pdf (Page 3):

We provide the list of visual preﬁx as input to the
LM with the corresponding text x1 as the target
output. Such a pre-training process enables F to
project visual features into the visual preﬁx that
lies within the same embedding distributions as
the LM. The mapping network is agnostic of the
downstream task, and only depends on the visual
source and the LM.
After generating a descriptive image Ii for the
input context xi, we use CLIP to encode Ii and
receive its visual features vi. We apply the pre-
trained mapping network F over vi, and receive
the visual preﬁx ci of length l:
ci “tci
1,ci
2,...,c i
lu“ FpCLIPpIiqq (2)
Visually-guided Language Modeling We use
the visual information to guide text generation
in two ways, reﬂected in the following two train-
ing objectives. Firstly, we directly introduce the
machine-generated visual information as input to
the LM. We concatenate the visual preﬁx ci and
the text embeddings ti for the input context xi
with m tokens. LM input can be denoted as
rci; tis “ tci
1,...,c i
l,ti
1,...,t i
mu. With yi “
tyi
1,yi
2,...,y i
nudenoting the target output of nto-
kens, and θdenoting the trainable parameters, we
can list out the teacher forcing training objective as
follows:
Lteacher “´
nÿ
j“1
log pθpyi
j|ci; ti; yi
ăjq (3)
In addition, we design a contrastive objective to
enforce the generated text to be semantically simi-
lar to the input visual supervision with the InfoNCE
loss (van den Oord et al., 2018; Yan et al., 2021):
Lcontrastive “´ log exppsimpvi,ˆtiq{τq
ř
j‰iexppsimpvi,ˆtjq{τq
(4)
in which ˆt is the projected representation of the
decoder’s last layer’s output, and can be viewed as
the sentence-level representation of the generated
text. Here simp¨,¨qﬁrst normalizes the two vectors,
then compute their cosine similarity, and τ is the
temperature.
3.4 Training & Inference
We ﬁrst pre-train the mapping network on the pre-
training dataset with the teacher-forcing objective.
Such pre-training is agnostic of the downstream
task, and only depends on the type of base LM.
When applying our iNLG on downstream tasks,
we train the base LM with the teacher forcing ob-
jective for the ﬁrst Nno_contra epochs. Then, we
introduce the contrastive objective and tune the
base LM together with the mapping network and
projection layer by minimizing the following loss
L. Here epdenotes the epoch and λis the factor:
L “
#
Lteacher, ep ăNno_contra,
Lteacher `λLcontrastive, ep ąNno_contra,
(5)
During inference, we provide the context and
machine-generated image to the LM. We use beam
search during decoding with a beam width of 10.
4 Experimental Setup
4.1 Tasks, Datasets, and Baselines
We apply our iNLG on three open-ended text gen-
eration setups: sentence completion, story genera-
tion, and concept-to-text generation. Table 1 shows
examples for each task.
Sentence Completion is a task of ﬁnishing the
sentence in a commonsense inference scenario. We
conduct experiments on the ActivityNet (Heilbron
et al., 2015) subset3 of HellaSwag (Zellers et al.,
2019), which is a benchmark for commonsense
natural language inference that ask the model to
predict the most likely follow-up among several
choices given a speciﬁc context. We compare with
StoryEndGen (Guan et al., 2019) which encodes
the given context incrementally and attends to the
one-hop knowledge graph retrieved from Concept-
Net for the context tokens. We implement our
iNLG on top of the GPT-2 (Radford et al., 2019),
which by nature, can generate the follow-up for an
arbitrary input in a zero-shot manner.
Story Generation requires the model to com-
pose a story based on the given title or context.
We conduct experiments on the widely used story
generation benchmark ROCStories (Mostafazadeh
et al., 2016). Each data item consists of a story title
and a human-written ﬁve-sentence everyday life
story that incorporates commonsense related to the
title.4 We provide the story title and the story’s ﬁrst
sentence as the input context, and ask the LM to pre-
dict the following four sentences. We consider the
314740/982/2261 samples for train/validation/test.
4We use the split provided by Su et al. (2022a), which is
based on the ROCStories Winter 2017 release and contains
49666/1500/1500 items for the train/validation/test sets.



### Claim 87/179

#### Claim Text
The GSS method retrieves and stitches together audio clips to convert machine-translated data into speech-translated data [179].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[179]_2210.08174.pdf (Page 1):

synthetic speech by retrieving and stitching spoken
words based on a text sequence, as shown in Figure
1.3 Our experiments show that this method is as
effective as TTS-generated speech, at a much lower
computational and ﬁnancial cost. For instance, aug-
menting ST data on-the-ﬂy with 100k of stitch-
converted MT data, boosts translation quality by an
average of 1.83 BLEU over 3 language pairs from
Must-C (Cattoni et al., 2021) with no additional
cost, memory, or speed footprints. Comparing the
real ST data vs. our converted version from the
same transcripts, to our positive surprise, revealed
that our synthetic data outperforms its real coun-
terpart by 0.41 BLEU score. We conduct thorough
experiments to examine SpokenVocabin boosting
translation and further showcase its use and beneﬁt
in the context of code-switching (CS) ST.
We hope this simple technique to ease the use of
MT data for ST in practice as well as other tasks
where synthetic speech is useful.
2 SpokenVocab
We describe our methodology in creating effective
synthetic ST data based on MT data in this section.
The core step is the preparation of a SpokenVocab
bank ofﬂine and stitching sounds on-the-ﬂy.
Concretely, we ﬁrst use a TTS engine to convert
items in a word vocabulary to speech to obtain a set
of SpokenVocab ofﬂine.4 Next, we can conﬁgure
the TTS engine to generate different speaker voices
and thus curate a SpokenVocab bank in which each
set corresponds to a "speaker". The purpose is to
simulate, to the greatest extent, a realistic speech
dataset consisting of various speakers. At training,
assume we have access to an MT dataset and each
pair denoted as <s,t> where sand tare source
and targets sentences, respectively. Given such a
pair, we choose one voice 5 from the bank, and pro-
duce synthetic speech by fetching corresponding
audio snippets by words in s from the bank and
stitching them together. During stitching, we de-
ploy cross-fade, a well-known technique to smooth
3During the writing of this manuscript we found out that
V oder, the ﬁrst electronic speech synthesiser developed by Bell
Labs in 1939, synthesized human speeches by decomposing
it into its acoustic components and combining them using
human operators in real time.
4SpokenV ocab could also be based on n-grams in a dataset.
5One could also generate utterances by mixing speakers at
the token level, with no additional cost with our technique. We
leave further investigation of this to future work as it requires
a test condition (i.e., including various speaker voices per
utterance) which is not available to the best of our knowledge.
transitions between two independent audio clips.6
Pairing it with tyields a synthetic ST instance.7
3 Experiments
We ﬁrst present the ST system (§3.1) and TTS sys-
tems (§3.1.2) used in this study. We then describe
the ST and MT datasets (§3.1.3), followed by pro-
viding implementation details (§3.1.4). Next we
explain howSpokenVocabis designed (§3.2) and re-
port translation results (§3.3). Lastly, we illustrate
how our method can be applied to CS ST (§3.5).
3.1 Experimental Setup
3.1.1 Speech Translation System
Pre-trained speech encoders and text decoders
have shown great performance on ST (Li et al.,
2021; Zhao et al., 2022), compared to models
trained from scratch. For this reason, we follow
the architecture in Gállego et al. (2021) that uses
Wav2vec 2 (W2V2) (Baevski et al., 2020) as the
speech encoder and mBart decoder (Liu et al.,
2020) as the text decoder, joint with a lightweight
linear adapter and a CNN-based length adapter.
3.1.2 TTS Systems
To prepare SpokenVocab, we use the Google TTS
service,8 which supports a wide range of voice con-
ﬁgurations; this allows simulating different speak-
ers with various accents, gender and geographi-
cal background. We also use a off-the-shelf TTS
toolkit, i.e., Tacotron2-DCA + Mulitband-Melgan
(short for T2+Mel).9 We use Google TTS to gener-
ate synthetic speech in raw wavforms.
3.1.3 Dataset
We conduct our major experiments on Must-C, a
multilingual ST dataset curated from Ted talks. We
focus on English (En)→German (De), Romanian
(Ro) and Italian (It). For MT data, we use a subset
of WMT14, WMT16 and OPUS10010 for De, Ro
and It, with 100k, 100k and 24k instances, respec-
tively. For the code-switching (CS) setting, we use
Prabhupadavani (Sandhan et al., 2022), multilin-
gual CS ST dataset, and we focus on En →De, It.
Its source utterances are code-mixed with English
(major language), Bengali and Sanskrit; each utter-
ance is translated manually to 25 languages. We
6https://github.com/jiaaro/pydub
7We provide a demo for stitched speeches.
8https://cloud.google.com/
text-to-speech
9https://github.com/mozilla/TTS
10http://opus.nlpl.eu/opus-100.php



Source: data\tc16_2312.10997v5\referenced_papers\[179]_2210.08174.pdf (Page 3):

Cost BLEU
Data /time /usd /uniF1C0De Ro It
ST - 26.91 24.66 22.13
ST + MTTTS 900 90 25 28.20 24.71 26.46
ST + MTstitched 9 0 0 28.02 25.05 26.13
Table 2: Translation quality on Must-C and the aver-
age costs associated for generating synthetic speech
for every 100k sentencesin terms of inference time in
minutes ( /time), USD value ( /usd) and storage required in
GB (/uniF1C0). Preparing SpokenVocab took 2 hours, free of
charge, with Google TTS, and stitched speeches are dis-
carded.
tems display similar translation performance with
28.02, 27.73 and 27.80 BLEU scores respectively,
suggesting that having a single speaker is sufﬁcient.
Our conjecture to this phenomenon is that speech
representations produced by W2V2 have removed
speaker information, as demonstrated in Nguyen
et al. (2020) where analysis was conducted on
wav2vec (Schneider et al., 2019), the predecessor
to W2V2. This could be further examined with
using dialect- or pronunciation-focused translation
settings, which we leave to future work.
3.3 Translation Performance on Must-C
Producing synthetic speech from SpokenVocab on-
the-ﬂy makes the conversion from text to speech
highly scalable in terms of time and monetary costs,
and it also avoids the need of storing speech. Ta-
ble 2 reports the time, dollar value and space re-
quired to produce every 100k speech with Google
TTS, while these numbers are negligible for Spo-
kenVocab due to its re-usability.14 Apart from scal-
ability, it is more important to see the translation
performance difference between unnatural speech
produced by SpokenVocab and ﬂuent speech gen-
erated by state-of-the-art TTS systems. Table 2
summarises results for 3 Must-C language pairs,
with stitched speech and TTS-generated speech.
As expected data augmentation of ST with MT
data method boosts translation quality, using our
method by 1.83 BLEU score on average. Our
stitched speech performs equally well as TTS-
generated counterpart, showing no loss of quality
during conversion.
14For fair comparison between TTS which operates on the
full vocabulary, we report the cost under the full vocabulary
version of our method.
Data Nature of Speech BLEU
Must-C real 26.91
Must-C + Europarl real + real 27.5
Must-C + EuroparlTTS real + synthetic 27.76
Must-C + Europarlstitched real + synthetic 27.91
Table 3: BLEU scores under different augmentations.
STCS STCS+MTCS-stitched
BLEU En-Be→De 26.11 28.09
En-Be→It 26.41 26.90
Table 4: Translation quality for CS ST dataset.
3.4 Stitched Speech vs. Real Speech
An alternative approach to augmentation is to lever-
age real ST data from any other existing domains.
To assess whether our approach as another augmen-
tation technique is still competitive, we conduct
an experiment on En →De by augmenting Must-
C with 35k training instances from the Europarl-
ST (Iranzo-Sánchez et al., 2020). Table 3 reports
the results. To our positive surprise, our stitched
speech (generated from the transcripts of eurorparl-
ST counterpart) works even better than the real
Europarl-ST speech.
3.5 Code-switching Speech Translation
Development in CS ST is constrained by the avail-
ability of relevant datasets (Sandhan et al., 2022)
and using TTS systems to augment data is practi-
cally difﬁcult. To this end, our method provides a
high degree of ﬂexibility in that it can stitch audio
clips of different languages freely. To produce a
code-switched utterance, we further prepare Spo-
kenVocab for Bengali (Google TTS does not sup-
port Sanskrit) based on an English-Bengali dictio-
nary.15 We maintained the ratio of code-switching
in the real data (i.e., 0.35 probability of CS occur-
ring, and 2 as the average number of code-switched
words in a sentence). Please see Algorithm 1 in
Appendix A.2 for the detailed utterance generation
process. Results in Table 4 suggest that the models
trained with additional 100k and 24k instances (for
De and It respectively.) from SpokenVocab outper-
form those only trained with the original data.
15https://github.com/MinhasKamal/
BengaliDictionary



Source: data\tc16_2312.10997v5\referenced_papers\[179]_2210.08174.pdf (Page 0):

Generating Synthetic Speech fromSpokenVocabfor Speech Translation
Jinming Zhao Gholamreza Haffari Ehsan Shareghi
Department of Data Science & AI, Monash University
firstname.lastname@monash.edu
Abstract
Training end-to-end speech translation (ST)
systems requires sufﬁciently large-scale data,
which is unavailable for most language pairs
and domains. One practical solution to the
data scarcity issue is to convert text-based ma-
chine translation (MT) data to ST data via text-
to-speech (TTS) systems.Yet, using TTS sys-
tems can be tedious and slow. In this work, we
propose SpokenVocab, a simple, scalable and
effective data augmentation technique to con-
vert MT data to ST data on-the-ﬂy. The idea
is to retrieve and stitch audio snippets, corre-
sponding to words in an MT sentence, from
a spoken vocabulary bank. Our experiments
on multiple language pairs show that stitched
speech helps to improve translation quality by
an average of 1.83 BLEU score, while per-
forming equally well as TTS-generated speech
in improving translation quality. We also
showcase how SpokenVocab can be applied in
code-switching ST for which often no TTS
systems exit.1
1 Introduction
End-to-end (E2E) speech-to-text translation (ST)
models require large amounts of data to train (Sper-
ber and Paulik, 2020). Despite the emerging
ST datasets (Cattoni et al., 2021; Wang et al.,
2021), their size is considerably smaller compared
to text-based machine translation (MT) data. A
common remedy to tackle the data scarcity is-
sue is to leverage text-based MT data in train-
ing ST systems. Common approaches include
multi-task learning (Anastasopoulos and Chiang,
2018; Ye et al., 2021), transfer learning & pretrain-
ing (Bansal et al., 2019; Wang et al., 2020) and
knowledge distillation (Inaguma et al., 2021; Tang
et al., 2021).
A more straightforward alternative is to convert
text-based MT data to ST via text-to-speech (TTS)
1Our code is available at https://github.com/
mingzi151/SpokenVocab
Figure 1: Overview of generating synthetic speech
from SpokenV ocab on-the-ﬂy. The ﬁrst step is to pre-
pare the SpokenV ocab bank ofﬂine and the second step
is to retrieve and stitch audio snippets from the bank by
words in a sentence.
synthesis engines (Pino et al., 2019; Jia et al., 2019).
This method is less commonly used despite its sim-
plicity and effectiveness,2 mainly due to practical
reasons: (i) TTS models have slow inference time
and may incur monetary costs; (ii) the conversion is
required for each MT datasets. Recently, Lam et al.
(2022) proposed to generate synthetic speech with-
out using TTS models. However, their approach is
based on real ST data, and thus cannot be extended
to MT data.
In this work, we propose a simple, effective and
efﬁcient data augmentation approach to convert
MT data to ST data on-the-ﬂy. The idea is to pre-
pare a set of spoken words, forming a spoken vo-
cabulary (SpokenVocab) bank, and then generate
2Only one work out of 8 uses TTS to augment data in the
IWSLT2022 ofﬂine speech translation track.
arXiv:2210.08174v2  [cs.CL]  8 Feb 2023



Source: data\tc16_2312.10997v5\referenced_papers\[179]_2210.08174.pdf (Page 4):

4 Conclusion
In this work, we proposed a simple, fast and effec-
tive data augmentation technique, SpokenVocabfor
ST. This provides an alternative for converting MT
data to ST data with TTS systems which comes
with monetary and computation costs in practice.
Our approach generates synthetic speech on-the-
ﬂy during training, with no cost or footprint. We
have shown that speech stitched from SpokenVo-
cab works as effective as TTS-generated speech,
and unlike TTS system, it could directly be applied
as a data augmentation tool in code-switching ST.
Our approach can be used in other content-driven
speech processing tasks as an uncompromising and
easy-to-use augmentation technique.
Limitations
CS ST exbihit difﬁculties (Huber et al., 2022;
Weller et al., 2022), exposing several limitations
in this study: 1) Bengali and Sanskrit (another mi-
nority language) are treated without difference, as
they originate from the same script and Sanskrit
is not supported by the Google TTS service. 2)
We use a open-source language detection tool to
calculate the oracle hyper-parameters in the dev
set; yet, imperfection of the detector on token-level
prediction and the fact that source sentences are
written in Latin regardless of the language deviate
the scores from true values.
References
Antonios Anastasopoulos and David Chiang. 2018.
Tied multitask learning for neural speech translation.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 82–91.
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed,
and Michael Auli. 2020. wav2vec 2.0: A frame-
work for self-supervised learning of speech represen-
tations. arXiv preprint arXiv:2006.11477.
Sameer Bansal, Herman Kamper, Karen Livescu,
Adam Lopez, and Sharon Goldwater. 2019. Pre-
training on high-resource speech recognition im-
proves low-resource speech-to-text translation. In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 58–68.
Roldano Cattoni, Mattia Antonino Di Gangi, Luisa
Bentivogli, Matteo Negri, and Marco Turchi. 2021.
Must-c: A multilingual corpus for end-to-end
speech translation. Computer Speech & Language,
66:101155.
Gerard I Gállego, Ioannis Tsiamas, Carlos Escolano,
José AR Fonollosa, and Marta R Costa-jussà. 2021.
End-to-end speech translation with pre-trained mod-
els and adapters: Upc at iwslt 2021. In Proceedings
of the 18th International Conference on Spoken Lan-
guage Translation (IWSLT 2021), pages 110–119.
Christian Huber, Enes Yavuz Ugan, and Alexander
Waibel. 2022. Code-switching without switching:
Language agnostic end-to-end speech translation.
arXiv preprint arXiv:2210.01512.
Hirofumi Inaguma, Tatsuya Kawahara, and Shinji
Watanabe. 2021. Source and target bidirectional
knowledge distillation for end-to-end speech trans-
lation. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1872–1881.
Javier Iranzo-Sánchez, Joan Albert Silvestre-Cerda,
Javier Jorge, Nahuel Roselló, Adria Giménez, Al-
bert Sanchis, Jorge Civera, and Alfons Juan. 2020.
Europarl-st: A multilingual corpus for speech trans-
lation of parliamentary debates. In ICASSP 2020-
2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pages
8229–8233. IEEE.
Ye Jia, Melvin Johnson, Wolfgang Macherey, Ron J
Weiss, Yuan Cao, Chung-Cheng Chiu, Naveen Ari,
Stella Laurenzo, and Yonghui Wu. 2019. Lever-
aging weakly supervised data to improve end-to-
end speech-to-text translation. In ICASSP 2019-
2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pages
7180–7184. IEEE.
Tsz Kin Lam, Shigehiko Schamoni, and Stefan Riezler.
2022. Sample, translate, recombine: Leveraging au-
dio alignments for data augmentation in end-to-end
speech translation. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 245–
254.
Xian Li, Changhan Wang, Yun Tang, Chau Tran,
Yuqing Tang, Juan Pino, Alexei Baevski, Alexis
Conneau, and Michael Auli. 2021. Multilingual
speech translation from efﬁcient ﬁnetuning of pre-
trained models. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 827–838.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising
pre-training for neural machine translation. Transac-
tions of the Association for Computational Linguis-
tics, 8:726–742.



Source: data\tc16_2312.10997v5\referenced_papers\[179]_2210.08174.pdf (Page 2):

prepare ST data following the instructions in Gál-
lego et al. (2021). We preprocess MT data with
the fairseq instructions and remove pairs with the
length of target sentences greater than 64 words to
avoid out-of-memory issues. Minimal preprocess-
ing is performed on the CS ST dataset.
3.1.4 Implementation Details
Similar to Li et al. (2021) and Gállego et al. (2021),
training different components of W2V2 and mBart
decoder yields divergent results. In our initial
experiments, we note that ﬁne-tuning the entire
W2V2 except for its feature extractor and freez-
ing mBart lead to decent translation results, and
thus we use this conﬁguration for all our exper-
iments. To ensure Must-C to be dominant, we
make the ratio of Must-C and MT data to be ap-
proximately 8:1, unless mentioned otherwise. We
use sacreBLEU (Post, 2018) to evaluate transla-
tion. Please refer to Appendix A.1 for full training
details, hyper-parameters and hardware.
3.2 SpokenVocab Preparation and Variations
Constructing the SpokenVocab bank is crucial, as
synthetic speech produced in this manner have a di-
rect impact on translation quality. In this section we
examine SpokenVocab from various dimensions.
TTS Conversion. The ﬁrst questions to ask are
which TTS system should be used to convert a
word to a spoken form and what sampling rate
(SR) is appropriate.11 To answer these questions,
we conduct intrinsic evaluation on stitched speech
by varying TTS engines and SR. Furthermore, as
it is common to diversify raw wave forms with
audio effects (Potapczyk et al., 2019), we apply
the same technique to distort our stitched speech.
Results in Table 1 show that using Google TTS
and setting the SR to 24k are better choices, while
distortion (i.e., adding the effects of tempo, speed
and echo) may or may not be helpful. Contrary to
the common practice of using a SR of 16k (Baevski
et al., 2020), applying 16k to SpokenVocab alters
the sound signiﬁcantly, as shown in the demo in
§2, and this has negative impacts on the system.
Overall, we use the setting in italic for the rest of
our experiments.
Word Vocabulary.We compile a word vocabulary,
consisting of 1) a common subset of words12, and
11SR is deﬁned as the number of samples taken from a
continuous signal per second.
12The list comes from Ofﬁcial Scrabble Players Dictionary
and Wiktionary’s word frequency lists, and can be found
Data TTS SR Distort. BLEU
ST - - - 26.91
ST+MTstitched
T2+Mel - - OOM
Google
24k - 28.02
24k ✓ 27.72
16k - 26.77
16k ✓ 27.47
Table 1: Comparison of different TTS conversions in
terms of TTS engine, sampling rate (SR) and distortion
(Distort.) Top row: baseline. Bottom rows: MT data is
converted to ST data with SpokenVocab. OOM: out-of-
memory with 24k and 16k SRs. italic: best setting.
2) unique words with a frequency of higher than 99
from the En→X WMT subset. The purpose is to
construct an approximated version of SpokenVocab
that is ready to convert any sentence to synthetic
speech. For words that are not covered by the list,
we employ a fuzzy matching mechanism where the
most similar word at the surface level is returned.
For instance, an out-of-vocabulary (OOV) word
"apples" is replaced by its closest match in the
vocabulary "apple", and the speech snippet for "ap-
ple" is retrieved. When no match is found, a default
ﬁlter word, "a", is returned. To investigate the ef-
fect of this approximation which would inevitably
lead to mispronounced words, we prepare another
set of SpokenVocab containing the full set of spo-
ken words in the WMT data (eliminating the need
for fuzzy matching). In controlled experiments
on En→De, the BLEU scores with the approxi-
mated and full SpokenVocabs, with the size of 35k
and 460k respectively, are 28.02 and 27.91. The
negligible difference indicates the effectiveness of
using an approximated SpokenVocab. Additional
ablation studies on using 50% and 10% of the full
vocabulary yield scores of 27.79 and 27.94, further
validating the insensitivity of W2V2 to nuanced
mispronunciation, perhaps due to the presence of
powerful pre-trained auto-regressive decoder.13
Number of Speakers.Despite the artiﬁcial nature
of the stitched speech sounds, one still can tell the
speaker’s information (e.g., gender, accent). To
examine whether diverse voices would be helpful
for translation, we set nto 1, 5 and 10 and train
models with the same amount of data. These sys-
at https://github.com/dolph/dictionary/
blob/master/popular.txt
13Optionally, one can dynamically call a TTS system to
generate an audio on OOV words.



### Claim 88/179

#### Claim Text
UEOP marks a significant advancement in end-to-end automatic speech recognition by incorporating external, offline strategies for voice-to-text conversion [180].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[180]_2301.02736.pdf (Page 0):

USING EXTERNAL OFF-POLICY SPEECH-TO-TEXT MAPPINGS IN CONTEXTUAL
END-TO-END AUTOMATED SPEECH RECOGNITION
David M. Chan⋆† Shalini Ghosh† Ariya Rastrow† Bj¨orn Hoffmeister†
⋆ University of California, Berkeley † Amazon Alexa AI
ABSTRACT
Despite improvements to the generalization performance of au-
tomated speech recognition (ASR) models, specializing ASR
models for downstream tasks remains a challenging task, pri-
marily due to reduced data availability (necessitating increased
data collection), and rapidly shifting data distributions (re-
quiring more frequent model ﬁne-tuning). In this work, we
investigate the potential of leveraging external knowledge, par-
ticularly through off-policy key-value stores generated with
text-to-speech methods, to allow for ﬂexible post-training adap-
tation to new data distributions. In our approach, audio em-
beddings captured from text-to-speech, along with semantic
text embeddings, are used to bias ASR via an approximate
k-nearest-neighbor (KNN) based attentive fusion step. Our ex-
periments on LibiriSpeech and in-house voice assistant/search
datasets show that the proposed approach can reduce domain
adaptation time by up to 1K GPU-hours while providing up
to 3% WER improvement compared to a ﬁne-tuning baseline,
suggesting a promising approach for adapting production ASR
systems in challenging zero and few-shot scenarios.
Index Terms— speech recognition, transfer learning, ﬁne-
tuning, adaptation, context
1 Introduction
One of the most challenging problems in automated speech
recognition (ASR) is specializing large-scale models, particu-
larly speech encoders, for downstream applications that often
(a) have fewer labeled training examples, and (b) rapidly evolv-
ing distributions of speech data. The traditional approach to
this problem is to frequently collect fresh data, which can be
used to re-train and specialize models, leveraging tools such
as domain-prompts [1], incremental-learning [2], knowledge
distillation [3], hand-written grammars [ 4], or metric learn-
ing [5, 6] to reduce the impact of re-training the model for the
downstream application. Unfortunately, for data that changes
on a rapid basis, such as product listings or applications requir-
ing per-customer specialization, such methods, while effective,
are either inherently slow or remain computationally infeasi-
ble.
For correspondence, direct questions todavidchan@berkeley.edu.
Work done during an internship at Amazon Alexa AI.
Fig. 1: An overview of our method leveraging text-to-speech
mappings for contextual ASR. Using data from a text catalog,
we generate audio and text representations to generate map-
pings from audio key to text value. To leverage these mappings
for ASR, we implement a K-Nearest Neighbors attention in
the speech encoder during the ﬁne-tuning (or training) phase.
In this work, we propose a method that leverages exter-
nal text data catalogs – large lists that can contain as much
as 10 million specialized words or phrases – to improve the
performance of models during both the ﬁne-tuning process,
and when specializing an already ﬁne-tuned model to a new
dataset. Here are the key highlights of our approach: ﬁrst, we
generate a key-value external knowledge store that maps an au-
dio representation of each text element of the catalog (usually
consisting of 1M-10M examples) to a semantic representation
of the text. Next, we train a model that leverages this external
store by attending over retrieved key/value pairs, which we
retrieve through approximate k-nearest neighbors. Relying on
an external, constant, and off-policy key-value store means
that this store can be updated during specialization, requiring
only an updated list of phrases for each new model instead of
additional ﬁne-tuning.
Inspired by Borgeaud et al.[7] and Wu et al.[8], we apply a
context embedding approach with a focus on ASR, leveraging
TTS-generated audio data and semantic text embeddings to
bias the speech encoder of a conformer model. To the best of
our knowledge, using TTS to encode textual context has not
arXiv:2301.02736v1  [eess.AS]  6 Jan 2023



Source: data\tc16_2312.10997v5\referenced_papers\[180]_2301.02736.pdf (Page 5):

ﬁne-tuning (BFT+T ) approach.
5 Conclusion
This paper introduces the ﬁrst approach for large-scale contex-
tualization of speech-encoder representations using text-only
catalog data. While this paper is a good ﬁrst step towards
contextualized speech encoders, problems like investigating
embeddings for the catalogs, leveraging grapheme/phoneme
embeddings, etc. remain interesting directions of future work.
This approach provides a natural way to combine external
memory for addressing distribution shifts when having OOV
words in dev/test, ensuring recognition of rare words in train-
ing data, handling personalization, and using pronunciation
instead of TTS – we would like to evaluate these features of
the approach on real-world data.
6 References
[1] S. Dingliwa, A. Shenoy, S. Bodapati, A. Gandhe, R. T. Gadde,
and K. Kirchhoff, “Domain prompts: Towards memory and
compute efﬁcient domain adaptation of asr systems,” inInter-
speech 2022, 2022.
[2] D. Baby, P. D’Alterio, and V . Mendelev, “Incremental learn-
ing for rnn-transducer based speech recognition models,” in
Interspeech 2022, 2022.
[3] K. Zhao, H. D. Nguyen, A. Jain, N. Susanj, A. Mouchtaris,
L. Gupta, and M. Zhao, “Knowledge distillation via module
replacing for automatic speech recognition with recurrent neural
network transducer,” inInterspeech 2022, 2022.
[4] A. Gandhe, A. Rastrow, and B. Hoffmeister, “Scalable language
model adaptation for spoken dialogue systems,” in2018 IEEE
Spoken Language Technology Workshop (SLT). IEEE, 2018,
pp. 907–912.
[5] T. P´apai, S. Ghosh, and H. Kautz, “Combining subjective proba-
bilities and data in training Markov Logic Networks,” vol. 7523,
09 2012, pp. 90–105.
[6] S. Mahadevan, B. Mishra, and S. Ghosh, “A uniﬁed frame-
work for domain adaptation using metric learning on manifolds,”
CoRR, vol. abs/1804.10834, 2018.
[7] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford,
K. Millican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc,
A. Clark et al., “Improving language models by retrieving from
trillions of tokens,” in International Conference on Machine
Learning. PMLR, 2022, pp. 2206–2240.
[8] Y . Wu, M. N. Rabe, D. Hutchins, and C. Szegedy, “Memorizing
transformers,” arXiv preprint arXiv:2203.08913, 2022.
[9] S. Novotney, S. Mukherjee, Z. Ahmed, and A. Stolcke, “Cue
vectors: Modular training of language models conditioned on
diverse contextual signals,”arXiv preprint arXiv:2203.08774,
2022.
[10] A. Shenoy, S. Bodapati, and K. Kirchhoff, “Contextual biasing
of language models for speech recognition in goal-oriented
conversational agents,”arXiv preprint arXiv:2103.10325, 2021.
[11] D. Zhao, T. N. Sainath, D. Rybach, P. Rondon, D. Bhatia, B. Li,
and R. Pang, “Shallow-fusion end-to-end contextual biasing.”
in Interspeech, 2019, pp. 1418–1422.
[12] B. Liu and I. Lane, “Dialog context language modeling with
recurrent neural networks,” in 2017 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2017, pp. 5715–5719.
[13] A. Jaech and M. Ostendorf, “Personalized language model
for query auto-completion,”arXiv preprint arXiv:1804.09661,
2018.
[14] S. Kim and F. Metze, “Dialog-context aware end-to-end speech
recognition,” in2018 IEEE Spoken Language Technology Work-
shop (SLT). IEEE, 2018, pp. 434–440.
[15] R. Lin, S. Liu, M. Yang, M. Li, M. Zhou, and S. Li, “Hier-
archical recurrent neural network for document modeling,” in
Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing, 2015, pp. 899–907.
[16] I. Williams, A. Kannan, P. S. Aleksic, D. Rybach, and T. N.
Sainath, “Contextual speech recognition in end-to-end neural
network systems using beam search.” inInterspeech, 2018, pp.
2227–2231.
[17] T. Munkhdalai, K. C. Sim, A. Chandorkar, F. Gao, M. Chua,
T. Strohman, and F. Beaufays, “Fast contextual adaptation with
neural associative memory for on-device personalized speech
recognition,” in ICASSP 2022-2022 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2022, pp. 6632–6636.
[18] K. M. Sathyendra, T. Muniyappa, F.-J. Chang, J. Liu, J. Su,
G. P. Strimel, A. Mouchtaris, and S. Kunzmann, “Contextual
adapters for personalized speech recognition in neural transduc-
ers,” in ICASSP 2022-2022 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP). IEEE,
2022, pp. 8537–8541.
[19] Z. Chen, M. Jain, Y . Wang, M. L. Seltzer, and C. Fuegen, “Joint
grapheme and phoneme embeddings for contextual end-to-end
asr.” inInterspeech, 2019, pp. 3490–3494.
[20] F.-J. Chang, J. Liu, M. Radfar, A. Mouchtaris, M. Omologo,
A. Rastrow, and S. Kunzmann, “Context-aware transformer
transducer for speech recognition,” in 2021 IEEE Automatic
Speech Recognition and Understanding Workshop (ASRU).
IEEE, 2021, pp. 503–510.
[21] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”
Advances in neural information processing systems, vol. 30,
2017.
[22] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, and R. Salakhut-
dinov, “Transformer-xl: Attentive language models beyond a
ﬁxed-length context,” arXiv preprint arXiv:1901.02860, 2019.
[23] R. Child, S. Gray, A. Radford, and I. Sutskever, “Generat-
ing long sequences with sparse transformers,” arXiv preprint
arXiv:1904.10509, 2019.
[24] K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi, “Ok-
vqa: A visual question answering benchmark requiring external
knowledge,” inProceedings of the IEEE/cvf conference on com-
puter vision and pattern recognition, 2019, pp. 3195–3204.



Source: data\tc16_2312.10997v5\referenced_papers\[179]_2210.08174.pdf (Page 4):

4 Conclusion
In this work, we proposed a simple, fast and effec-
tive data augmentation technique, SpokenVocabfor
ST. This provides an alternative for converting MT
data to ST data with TTS systems which comes
with monetary and computation costs in practice.
Our approach generates synthetic speech on-the-
ﬂy during training, with no cost or footprint. We
have shown that speech stitched from SpokenVo-
cab works as effective as TTS-generated speech,
and unlike TTS system, it could directly be applied
as a data augmentation tool in code-switching ST.
Our approach can be used in other content-driven
speech processing tasks as an uncompromising and
easy-to-use augmentation technique.
Limitations
CS ST exbihit difﬁculties (Huber et al., 2022;
Weller et al., 2022), exposing several limitations
in this study: 1) Bengali and Sanskrit (another mi-
nority language) are treated without difference, as
they originate from the same script and Sanskrit
is not supported by the Google TTS service. 2)
We use a open-source language detection tool to
calculate the oracle hyper-parameters in the dev
set; yet, imperfection of the detector on token-level
prediction and the fact that source sentences are
written in Latin regardless of the language deviate
the scores from true values.
References
Antonios Anastasopoulos and David Chiang. 2018.
Tied multitask learning for neural speech translation.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 82–91.
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed,
and Michael Auli. 2020. wav2vec 2.0: A frame-
work for self-supervised learning of speech represen-
tations. arXiv preprint arXiv:2006.11477.
Sameer Bansal, Herman Kamper, Karen Livescu,
Adam Lopez, and Sharon Goldwater. 2019. Pre-
training on high-resource speech recognition im-
proves low-resource speech-to-text translation. In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 58–68.
Roldano Cattoni, Mattia Antonino Di Gangi, Luisa
Bentivogli, Matteo Negri, and Marco Turchi. 2021.
Must-c: A multilingual corpus for end-to-end
speech translation. Computer Speech & Language,
66:101155.
Gerard I Gállego, Ioannis Tsiamas, Carlos Escolano,
José AR Fonollosa, and Marta R Costa-jussà. 2021.
End-to-end speech translation with pre-trained mod-
els and adapters: Upc at iwslt 2021. In Proceedings
of the 18th International Conference on Spoken Lan-
guage Translation (IWSLT 2021), pages 110–119.
Christian Huber, Enes Yavuz Ugan, and Alexander
Waibel. 2022. Code-switching without switching:
Language agnostic end-to-end speech translation.
arXiv preprint arXiv:2210.01512.
Hirofumi Inaguma, Tatsuya Kawahara, and Shinji
Watanabe. 2021. Source and target bidirectional
knowledge distillation for end-to-end speech trans-
lation. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1872–1881.
Javier Iranzo-Sánchez, Joan Albert Silvestre-Cerda,
Javier Jorge, Nahuel Roselló, Adria Giménez, Al-
bert Sanchis, Jorge Civera, and Alfons Juan. 2020.
Europarl-st: A multilingual corpus for speech trans-
lation of parliamentary debates. In ICASSP 2020-
2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pages
8229–8233. IEEE.
Ye Jia, Melvin Johnson, Wolfgang Macherey, Ron J
Weiss, Yuan Cao, Chung-Cheng Chiu, Naveen Ari,
Stella Laurenzo, and Yonghui Wu. 2019. Lever-
aging weakly supervised data to improve end-to-
end speech-to-text translation. In ICASSP 2019-
2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pages
7180–7184. IEEE.
Tsz Kin Lam, Shigehiko Schamoni, and Stefan Riezler.
2022. Sample, translate, recombine: Leveraging au-
dio alignments for data augmentation in end-to-end
speech translation. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 245–
254.
Xian Li, Changhan Wang, Yun Tang, Chau Tran,
Yuqing Tang, Juan Pino, Alexei Baevski, Alexis
Conneau, and Michael Auli. 2021. Multilingual
speech translation from efﬁcient ﬁnetuning of pre-
trained models. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 827–838.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising
pre-training for neural machine translation. Transac-
tions of the Association for Computational Linguis-
tics, 8:726–742.



Source: data\tc16_2312.10997v5\referenced_papers\[179]_2210.08174.pdf (Page 5):

Ha Nguyen, Fethi Bougares, Natalia Tomashenko, Yan-
nick Estève, and Laurent Besacier. 2020. Inves-
tigating self-supervised pre-training for end-to-end
speech translation. In Interspeech 2020.
Juan Pino, Liezl Puzon, Jiatao Gu, Xutai Ma, Arya D
McCarthy, and Deepak Gopinath. 2019. Harness-
ing indirect training data for end-to-end automatic
speech translation: Tricks of the trade. In Proceed-
ings of the 16th International Conference on Spoken
Language Translation.
Matt Post. 2018. A call for clarity in reporting bleu
scores. arXiv preprint arXiv:1804.08771.
Tomasz Potapczyk, Paweł Przybysz, Marcin Cho-
chowski, and Artur Szumaczuk. 2019. Samsung’s
system for the iwslt 2019 end-to-end speech transla-
tion task. In Proceedings of the 16th International
Conference on Spoken Language Translation.
Jivnesh Sandhan, Ayush Daksh, Om Adideva Paranjay,
Laxmidhar Behera, and Pawan Goyal. 2022. Prab-
hupadavani: A code-mixed speech translation data
for 25 languages. arXiv preprint arXiv:2201.11391.
Steffen Schneider, Alexei Baevski, Ronan Collobert,
and Michael Auli. 2019. wav2vec: Unsupervised
pre-training for speech recognition. In INTER-
SPEECH.
Matthias Sperber and Matthias Paulik. 2020. Speech
translation and the end-to-end promise: Taking stock
of where we are. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics, pages 7409–7421.
Yun Tang, Juan Pino, Xian Li, Changhan Wang, and
Dmitriy Genzel. 2021. Improving speech transla-
tion by understanding and learning from the aux-
iliary text translation task. In Proceedings of the
59th Annual Meeting of the Association for Compu-
tational Linguistics and the 11th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 4252–4261.
Changhan Wang, Anne Wu, Jiatao Gu, and Juan Pino.
2021. Covost 2 and massively multilingual speech
translation. In Interspeech, pages 2247–2251.
Chengyi Wang, Yu Wu, Shujie Liu, Ming Zhou, and
Zhenglu Yang. 2020. Curriculum pre-training for
end-to-end speech translation. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics, pages 3728–3738.
Orion Weller, Matthias Sperber, Telmo Pires, Hendra
Setiawan, Christian Gollan, Dominic Telaar, and
Matthias Paulik. 2022. End-to-end speech transla-
tion for code switched speech. In Findings of the As-
sociation for Computational Linguistics: ACL 2022,
pages 1435–1448.
Rong Ye, Mingxuan Wang, and Lei Li. 2021. End-to-
end speech translation via cross-modal progressive
training. Proc. Interspeech 2021, pages 2021–1065.
Jinming Zhao, Hao Yang, Ehsan Shareghi, and Gho-
lamreza Haffari. 2022. M-adapter: Modality adapta-
tion for end-to-end speech-to-text translation. arXiv
preprint arXiv:2207.00952.



Source: data\tc16_2312.10997v5\referenced_papers\[179]_2210.08174.pdf (Page 6):

A Appendix
A.1 Implementation Details
We implement and train all models with fairseq16
on 4 A40 GPUs, using 16 ﬂoating point precision,
for 25kupdates. WAV2VEC 217 and the mBart5018
decoder are used. We employ an Adam optimizer
with β1 = 0.99, β2 = 0.98, while setting the
dropout to 0.1, clip norm to 20 and label smoothing
to 0.2. For the baseline models, we use a learning
rate of 5e-04 and reduce it at plateau. For mod-
els trained with additional data, we use the same
learning scheduler with a learning rate of 3e-04.
A.2 Code-switching Speech Translation
Algorithm 1 Code-switching Utterance Genera-
tion
Require: E,B : English and Bengali Spoken-
V ocab, Dict : English-Bengali Dictionary,
Keys : English words in Dict, X : En-
glish sequence, p : probability of cs occur-
ring, n : number of code-switched words,
FetchSpeech : function to fetch speech
Output: U: CS utterance
1: q= NormDist(0,1)
2: if q >pthen
3: // Select words to be code-switched
4: words,indices = Random(X,n)
5: for word,i in words,indices do
6: // Only switch words in the dictionary
7: if wordin Keys then
8: // Replace with the Bengali word
X[i] =Dict[word]
9: end if
10: end for
11: end if
12: U = FetchSpeech(E,B,X )
13: return U
16https://github.com/facebookresearch/
fairseq
17https://dl.fbaipublicfiles.com/
fairseq/wav2vec/wav2vec_vox_960h_pl.pt
18https://dl.fbaipublicfiles.com/
fairseq/models/mbart50/mbart50.ft.1n.
tar.gz



### Claim 89/179

#### Claim Text
For structured knowledge, the CoK method [106] first extracts facts pertinent to the input query from a knowledge graph, then integrates these facts as hints within the input, enhancing performance in knowledge graph question-answering tasks.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[106]_2305.13269.pdf (Page 3):

Published as a conference paper at ICLR 2024
Table 1: An example of generated query, execution results, and formatted knowledge for rationales
of SPARQL. Knowl. stands for knowledge.
SPARQL
Rationale Souleyman San´e’s son, Leroy San´e, is a professional football player.Generated querySELECT ?answer WHERE{wd:/Souleymane San´e/ wdt:/child/ ?answer .}Execution resultsLeroy San´eFormatted knowl.The fact entity of the sentence “Souleyman San´e’s son, Leroy San´e, is a professional football player” is Leroy San´e.
Reasoning Generation Previous studies have demonstrated the importance of intermediate ratio-
nales for LLMs to answer complex reasoning questions (Wei et al., 2022). In this work, we utilize the
few-shot chain-of-thought (CoT) prompting to generate rationales (Wei et al., 2022). Moreover, we
employ the self-consistency method (Wang et al., 2023) to determine whether external knowledge is
necessary to answer the question. In sampling various reasoning paths and answers, self-consistency
is found to be highly correlated with accuracy. Thus, predictions with high consistency are preserved
without modification. Only questions with “uncertain” answers, i.e., their consistency falls below
a specified threshold, undergo further stages of processing. Such filtering technique is found to be
useful in identifying incorrect predictions by previous works (Yao et al., 2023; Zhao et al., 2023c).
Knowledge Domain Selection To ensure the retrieval of the most pertinent knowledge to the
question, we introduce the knowledge domain selection step. As shown in Figure 2, CoK inte-
grates four distinct knowledge domains: factual, medical, physics, and biology. Moreover, multiple
domains can be identified for answering a single question. To illustrate, when presented with the
question “Who proposed the theory which explains the cause of tides?”, both physics (gravitational
force of the Moon causes tides) and factual (Isaac Newton first proposed the universal gravitation
and explained tidal forces exerted by celestial bodies) domain knowledge are required to answer the
question. The knowledge domain selection is completed through in-context learning.
2.2 D YNAMIC KNOWLEDGE ADAPTING STAGE
Once the preliminary rationales and the identified knowledge domains are obtained, the next stage
is dynamic knowledge adapting, i.e., rectifying rationales based on the retrieved knowledge. To
minimize error propagation, CoK conducts knowledge retrieval and correction of the rationales se-
quentially. The preceding corrected rationales are used to generate the next rationale, which then
undergoes the same knowledge retrieval and correction step.
Knowledge Retrieval Upon identifying relevant domains to the question in the reasoning prepa-
ration stage, all knowledge sources within these domains are utilized for knowledge retrieval. The
knowledge retrieval consists of two steps: query generation and execution.
A) Query Generation Depending on the nature of the knowledge sources, each source is linked
to the most appropriate query language, which could either be structured, such as SPARQL or SQL,
or unstructured, such as natural language sentences. For instance, Wikidata is linked to the SPARQL
query as it consists of knowledge graphs. The flashcard source is linked to the natural sentence query
as it takes the format of natural sentence pairs. An example of generated queries for SPARQL is
shown in Table 1. 1 For instance, given a sentence “Souleyman San ´e’s son, Leroy San ´e, is a pro-
fessional football player”, a SPARQL query, “SELECT ?answer WHERE {wd:/Souleymane
San´e/ wdt:/child/ ?answer.}”, is generated to retrieve relevant knowledge from Wiki-
data. To facilitate the generation of both structured and unstructured queries, an adaptive query gen-
erator (AQG) is used. AQG is a versatile plug-in component, which can be either a tailor-finetuned
model or an off-the-shelf LLM. Details of AQG will be elaborated in Section 3.
B) Query Execution Once the queries are generated, the subsequent step is their execution to
acquire and convert the knowledge into formatted knowledge (see Table 1). A specialized method
is devised to execute queries and format the results for each query language. For SPARQL queries,
entity linking is initially performed to substitute entity spans with IDs, followed by acquiring results
by invoking the API of wikidata.org. Regarding SQL queries, they are executed directly to
1Examples of generated queries for each querying language are in Appendix D.5.
4



Source: data\tc16_2312.10997v5\referenced_papers\[106]_2305.13269.pdf (Page 2):

Published as a conference paper at ICLR 2024
Medical
Rationale 1
Knowledge 
Retrieval
Supporting 
Knowledge
Rationale 
Correction
Corrected Rationale 1
Next Rationale 
Generation
Corrected Rationale 2
…
…
Rationale 2
Knowledge 
Retrieval
Supporting 
Knowledge
Rationale 
Correction
Next Rationale 
Generation
Reasoning Generation & 
Knowledge Domain Selection Answer Consolidation
The answer is 1941What year was the Argentine actor who directed El 
Tio Disparate born? I III
II
Rationale
Query 
Generation
Query
Llama-2-LoRA
ChatGPT
Adaptive Query Generator
Wikidata (SPARQL)
 Table (SQL)Factual
 Wikipedia (n.s.)
ScienceQA Physics (n.s.)Physics
Biology
Supporting 
Knowledge
UpToDate (n.s.)
PysicsClassroom (n.s.)
Flashcard (n.s.)
ScienceQA Biology (n.s.)
 CK-12 (n.s.)
Figure 2: Our proposed chain-of-knowledge (CoK) framework, consisting of (I) Reasoning prepa-
ration, (II) Dynamic knowledge adapting, and (III) Answer consolidation. n.s.: natural sentence.
Given that different knowledge sources require distinct query languages, AQG holds a crucial role
in generating queries. AQG is versatile and can either be a fine-tuned model like Llama-2 (Touvron
et al., 2023) with LoRA (Hu et al., 2021) or an off-the-shelf LLM like ChatGPT. By leveraging both
unstructured and structured knowledge sources, CoK allows for better factual accuracy, improved
reliability, and easier information updates.
To summarize, our key contributions are the following: (1) We introduce chain-of-knowledge (CoK),
a novel framework to enhance the factual correctness of LLMs with heterogeneous knowledge
sources; (2) We propose an adaptive query generator (AQG), specially designed to generate queries
tailored to each knowledge source. AQG is versatile and can seamlessly transition between fine-
tuned models and black-box LLMs; (3) CoK corrects the rationales progressively, ensuring that
inaccuracies from preceding rationales do not propagate into the subsequent steps; (4) We perform
extensive experiments on knowledge-intensive tasks spanning a range of domains, including factual,
medical, physical, and biological. CoK outperforms the CoT baseline by 4.3% on average.
2 T HE CHAIN -OF-KNOWLEDGE FRAMEWORK
As shown in Figure 2, the CoK framework consists of three stages: (1) reasoning preparation, (2)
dynamic knowledge adapting, and (3) answer consolidation. In the first stage, given a knowledge-
intensive question, CoK generates preliminary rationales, i.e., reasoning units/sentences in the rea-
soning chain of CoT, and answers while identifying the relevant knowledge domains. Questions that
do not yield a majority consensus in their answers enter the dynamic knowledge adapting stage, in
which an adaptive query generator (AQG) is employed to generate queries to retrieve knowledge
from the knowledge sources of the identified domain. The rationales are progressively revised and
generated based on the retrieved knowledge. The final answer is then derived based on the corrected
rationales. Refer to Appendix A.1 for the prompts used for each step of our framework.
2.1 R EASONING PREPARATION STAGE
In real-world scenarios, when facing a complex knowledge-intensive question, it is necessary to
generate intermediate rationales before producing the final answer (Wei et al., 2022). Moreover,
before delving into external knowledge sources to address the question, it is crucial to identify the
relevant knowledge domains for effective retrieval. Thus, the reasoning preparation stage consists
of two essential components, namely, reasoning generation and knowledge domain selection.
3



Source: data\tc16_2312.10997v5\referenced_papers\[81]_2312.15883.pdf (Page 2):

HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable Medical LLMs Responses Conference’17, July 2017, Washington, DC, USA
Query After meals, I feel a bit of
stomach reflux. What medication
should I take for it?
Entities:
1. stomach acid
2. gastro-
esophageal 
reflux
…Search KG according 
to query only
Prompt = Query + Entities
Answer: Gastroesophageal reflux
may be caused by the backward
flow of food or stomach acid. You
can consider using Acid-suppressing
medications to relieve symptoms of
gastric reflux and mitigating the
development of reflux esophagitis…
Hypothesis Output  …Gastroesophageal reflux may 
caused by the backward flow of stomach acid into the 
esophagus … Depending on the evidence, considering the 
use of H2 receptor antagonists or proton pump inhibitors …
Step 2: Search KG according to query and hypothesis output
Prompt = Query + Reasoning Chains
Answer: Stomach acid backward may be the cause of
gastroesophageal reflux …You may consider omeprazole or
esomeprazole to reduce gastric acid secretion, …. Alternatively,
you can use acid-neutralizing medications (antacids) such as
magnesium aluminum carbonate. Another option is the use of
H2 receptor antagonists such as ranitidine or famotidine …
Step 1: Query LM and get hypothesis output
anchor entity
Query After meals, I feel a bit of stomach
reflux. What medication should I take for it?
Reasoning Chain:
① bile reflux → abdomen ← stomach pain
②magnesium aluminum carbonate→ heartburn 
→ excessive stomach acid → stomach pain
…
Figure 1: (a) KGRAG (Left). Basic KGRAG extracts key entities from user queries and searches for corresponding entities within
KG, which are then fed into LLMs along with the query. (b) HyKGE (Right). HyKGE first queries LLMs to obtain hypothesis
output and extracts entities from both the hypothesis output and the query. Then HyKGE retrieves reasoning chains between
any two anchor entities and feeds the reasoning chains together with the query into LLMs.
post-retrieval to maintain effective, diverse knowledge, enhancing
retrieval efficiently without fine-tuning or excessive interactions.
Knowledge Graph Query-Answer.Compared to knowledge
stored in document repositories [ 23], the knowledge contained
within KGs has the advantages of being structured and inferable,
rendering it a more suitable source for supplementing LLMs [28,
30, 48, 50, 63, 72]. However, how to design a retriever to extract
knowledge from KGs and how to design interaction strategies be-
tween LLM and KGs are still in the exploratory stage2. KGRAG [65]
uses the user query as a reference for retrieval in KGs, which suf-
fers from misalignment between high-quality structured knowl-
edge and varying-quality queries. Semantic parsing methods allow
LLMs to convert the question into a structural query (e.g., SPARQL),
which can be executed by a query engine to derive the answers
on KGs [42, 44, 69]. However, these methods depend heavily on
the quality of generated query sentences, displaying subpar perfor-
mance when confronted with intricate queries.
3 PRELIMINARIES
Definition 3.1 (Knowledge Graph). Given a medical knowledge
graph, denoted by KG= (E,R,T,D,N), where E= {𝑒1,...,𝑒 𝑁}
is the set of entities,R= {𝑟1,...,𝑟 𝑃}is the set of relations, andT=
{(𝑒𝑡ℎ𝑒𝑎𝑑
𝑖
,𝑟𝑡𝑖,𝑒𝑡𝑡𝑎𝑖𝑙
𝑖
)| 1 ≤𝑖 ≤𝑇,𝑒𝑡ℎ𝑒𝑎𝑑
𝑖
,𝑒𝑡𝑡𝑎𝑖𝑙
𝑖
∈E,𝑟𝑡𝑖 ∈R} is the set
2https://siwei.io/talks/graph-rag-with-jerry/1
of head-relation-tail triplets (facts). Additionally,𝑑𝑖 ∈D represents
the entity description of 𝑒𝑖, and N𝑣 = {(𝑟,𝑢)|( 𝑣,𝑟,𝑢 )∈T} stands
for the set of neighboring relations and entities of an entity 𝑣.
Definition 3.2 (Knowledge Graph Retrieval). Knowledge Graph
Retrieval [61] is a module that focuses on efficiently retrieving rel-
evant information from KGbased on the user query Q. In KGs,
information is represented as entities, relations, and attributes, form-
ing a structured network. The goal of retrieval is to find entities or
relationships that additionally supply knowledge for LLMs. Partic-
ularly, we retrieve knowledge from the matched entities {𝑒𝑗}such
as entity names, entity types, descriptions {𝑑𝑗}and even triplets or
subgraphs G𝑒𝑗 = (𝑒𝑗,T𝑗,𝑑𝑗).
4 METHOD
In this section, we detail our proposed HyKGE, and the overall
framework is illustrated in Figure 2. In general, we will discuss our
model from the four pipeline architectures:
•Pre-Retrieval Phaseincludes the Hypothesis Output Module
(HOM) and the NER Module (NM). HOM leverages LLMs to obtain hy-
pothesis output by exploring possible answers. Then NM extracts
medical entities from HO and the user query.
•Retrieval on Knowledge Graphutilizes the extracted entities as
anchors to search three distinct types of reasoning chains inter-
linking these anchors, providing relevant and logical knowledge.



Source: data\tc16_2312.10997v5\referenced_papers\[106]_2305.13269.pdf (Page 18):

Published as a conference paper at ICLR 2024
Table 10: Details of the evaluation datasets.
Domain Dataset # of Samples
Factual FEVER 1000
Factual HotpotQA 308
Factual FeTaQA 500
Medical MedMCQA 146
Physics MMLU Physics 253
Biology MMLU Biology 454
Table 11: Evaluation of domain selection performance.
Domain Precision Recall F1
Factual 96.0% 96.0% 96.0%
Medical 94.3% 96.1% 95.2%
Physics 89.9% 100.0% 94.6%
Biology 100.0% 92.8% 96.2%
E E VALUATION DATASETS
The evaluation datasets collect datasets from four different domains, including factaul, medical,
physics, and biology. Details of the dataset are in Table 10. We adopt exact match as the evaluation
metrics for HotpotQA, which is a more strict evaluation.
F A NALYSIS
F.1 D OMAIN SELECTION
As CoK relies on selecting relevant knowledge domains, it is important that the domain selection
step is of high quality. Hence, we randomly sample 50 questions from each domain and compare
the predicted domains with our manually annotated domains. As each question may be relevant for
more than one domain, we report the precision, recall, and F1 scores. As shown in Table 11, we find
that while the domain selection is not perfect, the overall F1 scores are more than 94% across all the
domains. Hence, we believe that the current domain selection process is adequate.
F.2 M ODELS OF ADAPTIVE QUERY GENERATOR
Table 12 demonstrates the performances of ChatGPT and instruction-tuned LlaMA-2-7B on SQL
and SPARQL generation. SPARQL is evaluated on 4,779 samples from LC-quad and KQA-pro.
SQL is evaluated on 15,900 samples from WikiSQL and we use the exact-match metric to evaluate
the generated queries with gold queries.
G D ISCUSSION OF LIMITATIONS
Knowledge Sources As CoK relies on external knowledge sources, there are some ethical im-
plications. Notably, LLMs using CoK may still generate inaccurate information if the knowledge
sources contain unreliable information. Hence, this could cause misinformation or manipulation of
public opinion. Another limitation is that there may be conflict between different knowledge sources
in theory. To address the two limitations, we selected authoritative knowledge sources such as Wiki-
data which are unlikely to contain inaccurate or conflicting information. As a result, the risk from
the knowledge sources are reduced.
Knowledge Retrieval On the other hand, CoK may not produce useful outputs if the knowledge
retrieval step is unable to retrieve facts that are relevant to the given question. However, we believe
19



Source: data\tc16_2312.10997v5\referenced_papers\[106]_2305.13269.pdf (Page 4):

Published as a conference paper at ICLR 2024
fetch the results, which could be a singular value or a subset of the original table. The outcomes
from both SPARQL and SQL are then formatted into markdown text. For natural sentence queries,
knowledge is retrieved from domain-specific knowledge sources either through sentence similarity
matching or by utilizing a search engine. 2
Rationale Correction Existing methods such as ReAct (Yao et al., 2023) and Verify-and-Edit
(Zhao et al., 2023c) keep all retrieved information in the context throughout the process, no matter
if it contains reasoning mistakes. This often leads to error propagation and misguides further gener-
ations. To overcome this weakness, CoK involves a progressive rationale correction step. Given the
current rationale and the formatted knowledge from various knowledge sources, a corrected ratio-
nale is generated to replace the current one. This step helps in rectifying any factual incorrectness
and preventing error propagation.
Next Rationale Generation Using the question and preceding corrected rationales, the next ratio-
nale is generated, and the process is reiterated for the new rationale until a final answer is produced.
2.3 A NSWER CONSOLIDATION STAGE
Ultimately, the LLM is prompted with the question and corrected rationales to generate a consol-
idated answer, which is expected leading to a more accurate answer. This hypothesis is further
examined through a series of experiments, as detailed in Section 4.
3 T HE ADAPTIVE QUERY GENERATOR
CoK incorporates heterogeneous knowledge sources from four different domains, including factual,
medical, physics, and biology. Each of these knowledge sources necessitates the use of a unique
query language for retrieval, which could be either structured or unstructured. Therefore, we design
the adaptive query generator (AQG) to facilitate query generation for different knowledge sources.
Unstructured Query Languages Natural language sentences are the most natural way that hu-
man beings search for information. AQG utilizes two distinct approaches for generating unstruc-
tured queries based on the knowledge sources. A) For general factual knowledge sources, such as
Wikipedia, ChatGPT is utilized. B) For domain-specific knowledge sources ( e.g., Flashcard, Sci-
enceQA Physics, and ScienceQA Biology), using ChatGPT may lead to hallucination as it may not
have comprehensive knowledge of the specific domains. Therefore, we instruction-tune LLaMA-
2-7B using LoRA with pairs of input texts and output queries. Furthermore, the domain of the
training data is on par with the respective knowledge source. Consequently, the AQG is equipped
with the requisite knowledge for generating queries with greater precision.
Structured Query Languages Querying unstructured knowledge sources often leads to the re-
trieval of irrelevant and redundant information. On the other hand, structured knowledge sources
(e.g., Wikidata and tables) provide direct factual results. To generate structured queries, AQG uti-
lizes two approaches based on the query languages. A) When generating commonly used query
languages like SQL, we employ ChatGPT. It is empirically inferred that ChatGPT included SQL
during its pre-training, providing it with advantages in generating SQL queries (OpenAI, 2023).
All pertinent details are incorporated into the prompt to enhance the precision of query generation.
For instance, when generating SQL queries, we include both the table schema and data snippets.
B) For less common languages like SPARQL, we instruction-tune LLaMA-2-7B using LoRA with
sentence-SPARQL pairs. The training data is collected to match the logical granularity of the ratio-
nales, thereby facilitating more accurate query generation. For example in SPARQL, both training
data and rationales contain single entity and relation within each sentence. Inspired by chain-of-
hindsight (Liu et al., 2023), besides giving the correct queries, we also append negative examples
such as “incorrect queries:..” during instruction-tuning.
Detailed query language, model, and training datasets of each knowledge source are in Table 8 of
Appendix. The constructions of instruction-tuning datasets and training details are in Appendix D.
We also evaluate the performances of AQG in Appendix F.2.
2Details of the execution process for each knowledge source is in Appendix C.
5



### Claim 90/179

#### Claim Text
A gamma correction factor γc is applied to increase the visibility of the plasma .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 15):

to rectify hallucinations during the post-
processing stage (Cao et al., 2020; Zhu
et al., 2021; Fabbri et al., 2022). The fixer
can be either another LLM (Peng et al.,
2023a; Zhang et al., 2023d; Chern et al.,
2023; Gou et al., 2023) or a specific small
model (Chen et al., 2023a). Such fix-
ers first interact with external knowledge
sources to gather sufficient evidence, and
then correct hallucinations. For example,
RARR (Gao et al., 2023a) directly prompts
an LLM to ask questions about the content
that needs to be corrected from multiple per-
spectives. Then it uses search engines to re-
trieve relevant knowledge. The LLM-based
fixer finally makes corrections based on re-
trieved evidence. The Verify-then-Edit ap-
proach (Zhao et al., 2023a) aims to enhance
the factuality of predictions by post-editing
reasoning chains based on external knowl-
edge sourced from Wikipedia. To achieve
better performance, LLM-Augmenter (Peng
et al., 2023a) prompts LLMs to summarize
retrieved knowledge before feeding it into the
fixer. Moreover, FacTool (Chern et al., 2023)
and CRITIC (Gou et al., 2023) propose to uti-
lize various external tools to obtain evidence
for the fixer.
Summary & Discussion. Resorting to external
knowledge to mitigate hallucinations in LLMs of-
fers several advantages. Firstly, this method cir-
cumvents the need for modifying LLMs, making
it a plug-and-play and efficient solution. Secondly,
it facilitates the easy transfer of proprietary knowl-
edge (e.g., a company’s internal data) and real-
time updated information to LLMs. Lastly, this
approach enhances the interpretability of infor-
mation generated by LLMs by allowing the trac-
ing of generation results back to the source evi-
dence (Gao et al., 2023b; Yue et al., 2023). How-
ever, this direction also presents some remaining
challenges. We discuss some of them below.
(1) Knowledge verification. In the era of LLMs,
the external knowledge source could extend
beyond a single document corpus or a spe-
cific website to encompass the entire Internet.
However, the information from the Internet is
in the wild, which means they may also be
fabricated, or even generated by LLMs them-
selves (Alemohammad et al., 2023). How to
User Query: What is the height of Mount Kilimanjaro?
User Query: What is the height of Mount Kilimanjaro?
Answer:Theheightis5932meters.
Please provide your confidence level (0-100). 
Answer: The height is 5932 meters. I am 90% confident. 
User Query: What is the height of Mount Kilimanjaro?
 Answer: The height is 5895 meters. 
Answer: The height is 5932 meters. 
Answer: The height is 5921 meters. 
(a) logit-based method
(b) verbalize-based method
(c) consistency-based method
Figure 5: The illustrations of three typical methods for
estimating LLM uncertainty. In the example of the
logit-based method, we use the red/green background
to distinct tokens with low/high generation probabili-
ties. In the example of the consistency-based method,
the responses are acquired from multiple sampling.
verify the authenticity of retrieved knowledge
from the Internet is an open and challenging
problem to be solved.
(2) Performance/efficiency of retriever/fixer.
The performance of the retriever/fixer plays
a vital role in ensuring the effects of hallu-
cination mitigation. Future work may con-
sider jointly optimising the whole working
flow (retriever→LLM→fixer) via reinforce-
ment learning (Qiao et al., 2023) or other
techniques. Besides, the efficiency of the
retriever/fixer is another important factor to
be considered, as the generation speed of
existing LLMs is already a significant bur-
den (Ning et al., 2023).
(3) Knowledge conflict. As introduced be-
fore, the retrieved knowledge may conflict
with the parametric knowledge stored by
LLMs (Qian et al., 2023). Shi et al. (2023b)
reveal that LLMs may fail to sufficiently ex-
ploit retrieved knowledge when knowledge
conflict happens. Xie et al. (2023) take
a more cautious look at this phenomenon.
How to fully utilize context knowledge is
an under-explored question. For example,
Liu et al. (2023d) find the performance of
retrieval-augmented LLMs significantly de-
grades when they must access evidence in the
middle of long contexts.
5.4.3 Exploiting Uncertainty
Uncertainty serves as a valuable indicator for de-
tecting and mitigating hallucinations during the
16



Source: data\tc16_2312.10997v5\referenced_papers\[81]_2312.15883.pdf (Page 7):

Conference’17, July 2017, Washington, DC, USA Jiang, Zhang and Xu et al.
in the KGs, which is the base model of RAG on KG and has been
widely applied in [63–65]. (3) Query Expansion (QE) [ 5] refor-
mulate the user’s initial query by adding additional terms with a
similar meaning with the help of LLMs. (4) CHAIN-OF-NOTE
(CoN) [93] generates sequential reading notes for retrieved knowl-
edge, enabling a thorough evaluation of their relevance to the given
question and integrating these notes to formulate the final an-
swer. (5) Chain-of-Knowledge (CoK) [ 44] utilize the power of
LLMs and consists of reasoning preparation, dynamic knowledge
adapting, and answer consolidation. (6) Knowledge-Augmented
Language Model Verification (KALMV) [ 6] verifies the output
and the knowledge of the knowledge-augmented LLMs with a sep-
arate verifier. (7) Knowledge Graph Generative Pre-Training
(KG-GPT) [ 34] comprises three steps: Sentence Segmentation,
Graph Retrieval, and Inference, each aimed at partitioning sen-
tences, retrieving relevant graph components, and deriving logical
conclusions. (8) Summarizing Retrievals (SuRe) [ 33] constructs
summaries of the retrieved passages for each of the multiple answer
candidates and confirms the most plausible answer from the can-
didate set by evaluating the validity and ranking of the generated
summaries. Note that we follow the prompts of the baselines as
stated strictly. The baselines and running time are summarized in
Table 1. In RAG Options, CoN requires fine-tuning the retriever,
implying a higher training overhead and the prerequisite of prepar-
ing a dataset. In addition, it is also difficult to migrate to other
domain-specific KGs. In terms of LLMs interactions, QE, CoN, CoK,
KALMV, KG-GPT, SuRe and HyKGE all necessitate engagement
with LLMs. However, CoN, CoK, KALMV, KALMV, KG-GPT and
SuRe entail multiple interactions (more than twice), significantly
escalating the time expenditure.
5.1.5 Evaluation Metrics. As for the evaluation of multi-task medi-
cal choice question performance, we guide LLMs to only answer
the correct answer and employ established metric Exact Match
(EM) as suggested by prior work [ 32, 99]. For the EM score, an
answer is deemed acceptable if its form corresponds to all correct
answers in the provided list. For multiple-choice questions, we also
calculate a Partial Correct Rate (PCR) . In comparison to EM, if
there is a missing answer without any incorrect ones, PCR clas-
sifies it as correct. In addition, to verify the effectiveness of the
retrieved knowledge, we also let LLMs output a complete analysis
process. Then, we measure Artificial Correlation Judgement
(ACJ)by inviting 20 medical experts to rate the retrieved knowledge
according to the criteria of (correlation=1, relevant but useless=0,
irrelevant=-1), and calculate the relevant scores for each question
by sampling 100 questions from the two datasets. Moreover, we
also objectively evaluated the Perplexity (PPL) of LLMs output.
The smaller the PPL, the greater the role of retrieved knowledge
in reducing LLMs’ hallucinations. Moreover, we also complement
our analysis with ROUGE-Recall (ROUGE-R) [86]. ROUGE-R
measures the extent to which the LLMs’ responses cover the re-
trieved knowledge, which is crucial for ensuring comprehensive
information coverage. For open-domain medical Q&A tasks, we
utilize ROUGE-R and Bilingual Evaluation Understudy (BLEU-
1 for answer precision, BLEU-4 for answer fluency) [86] to gauge
the similarity of LLMs responses to the ground-truth doctor anal-
ysis. Additionally, we employ PPL to assess the quality of LLMs
responses.
5.1.6 Experimental Implementation. In HyKGE, 𝑘 = 3,𝑡𝑜𝑝𝐾 =
10,𝛿 = 0.7,𝑙𝑐 = 10,𝑜𝑐 = 4. The prompts for LLMs can refer to
Table 3. Moreover, for all the baselines and HyKGE, we set the
maximum number of returned tokens for LLMs to 500 and the
temperature to 0.6. In all baselines and HyKGE, we first use the
Jieba library in Python to perform word segmentation, and then
use filtered text to filter out tone words and invalid characters
following “chinese_word_cut.txt”12 to avoid errors in knowledge
extraction. For a fair comparison, we apply the same W2NER, GTE
and FlagEmbedding models for all baselines. Moreover, the param-
eters of W2NER are optimized with Adam optimizer [35] with 𝐿2
regularization and dropout on high-quality medical dataset [22, 96],
the learning rate is set to 1e-3, the hidden unit is set to 1024 and
weight decay is 1e-4. Similar to previous work [65], because of the
randomness of LLMs’ outputs, we repeat experiments with differ-
ent random seeds five times and report the average and standard
deviation results. Experimental results are statistically significant
with 𝑝 < 0.05. Implementations are done using the PyTorch 1.9.0
framework [58] in Python 3.9, on an Ubuntu server equipped with
8 A100 GPU and an Intel(R) Xeon(R) CPU.
5.2 Performance Comparison (RQ 1)
To answer RQ1, we conduct experiments and report results of the ac-
curacy on the MMCU-Medical, CMB-Exam and CMB-Clin datasets
with two LLM turbos GPT 3.5 and Baichuan 13B-Chat, as illustrated
in Table 2 and Table 3. From the reported accuracy, we can find the
following observations:
Comparison of RAG methods and Base LLMs. Through com-
parison, we observe that most RAG approaches do not consistently
yield effective outcomes when integrated with KGs, especially in
contrast with the Base model. For instance, the KGRAG method ex-
tracts triples from KG without engaging in essential post-processing
steps like reranking and filtering, thereby infusing an overabun-
dance of noise and compromising the interpretative performance
of LLMs. As for QE tasks, while traditional QE methods typically
show efficacy, LLMs demonstrate a notable difficulty in compre-
hending instructions that necessitate the task-specific rewriting of
multiple-choice questions, which, in turn, detrimentally impacts
LLMs performance in such scenarios. Moreover, this effect is partic-
ularly pronounced in weaker models, such as Baichuan, where the
repercussions of these deficiencies are significantly magnified. How-
ever, the improvement in CoN, CoK, KG-GPT, SuRe and HyKGE is
more remarkable, because leveraging LLMs to explore or organize
knowledge can assist in finding more relational knowledge and the
reranking or filtering methods can highly likely remove irrelevant
noise knowledge chains, and contribute to accuracy improvement.
Comparison of HyKGE and other RAG methods. Firstly, it
is evident that our model, HyKGE, outperforms the baseline models
across all metrics. For instance, the EM and PCR scores see an im-
provement of approximately 8.55%-28.15% and 10.45%-33.29% for
the MMCU-Medical dataset with GPT 3.5 turbo, and the BLEU-1
and ROUGE-R scores see an improvement of approximately40.29%-
278.47% and 7.30%-51.69% for the CMB-Clin dataset with GPT
12https://github.com/Robust-Jay/NLP_Chinese_WordCut/blob/master/stopwords.txt



Source: data\tc16_2312.10997v5\referenced_papers\[168]_2311.08147.pdf (Page 7):

0.815 
0.822 
0.717 
0.726 
0.765 
0.546 
QA-AQA-NATGright answerswrong answers
(a) EventKG
0.811 
0.805 
0.710 
0.753 
0.667 
0.544 
QA-AQA-NATGright answerswrong answers
(b) UJ
Figure 3: The confidence of ChatGLM2 on its right and
wrong answers, respectively.
ternet and finally generate wrong answers for user
queries. The results also prove that the problem we
identify in this paper cannot be solved by existing
methods and deserves further studies in the future.
5 Analyses
5.1 Observation on Model Confidence
To study the influence of counterfactual contexts on
models’ final outputs quantitatively, we compare
the average generation probabilities of all tokens
in wrong answers and right answers generated by
ChatGLM2, respectively. For QA-A and QA-NA,
we only consider the samples that the model can
answer correctly with no contexts provided but will
give wrong predictions when given counterfactual
information. For text generation, we compare the
probabilities of original words/phrases and those
of edited words/phrases.
The results are shown in Figure 3. We observe
that the model’s confidence in its answers will sig-
nificantly drop with the interference of counterfac-
tual contexts, especially for text generation. This
phenomenon suggests that it is plausible to guide
models to generate accurate responses according
to the model’s confidence in future research.
5.2 Case Study
We can observe from Table 5 and Table 6 that the
prompt method achieves an improvement in QA-
A but causes a drop in performance in QA-NA.
A typical case, in which the model answers the
question incorrectly after adding a new sentence
Context
The Trial of Joan of Arc took place during the 15th centuryand was a legal proceeding against Joan of Arc. Joan of Arcwas a French military leader who served under Charles VIIduring the Hundred Years’ War. The trial began on January 9,1431, and lasted until May 29, 1431. (The trial began onJanuary 10, 1432, and lasted until May 30, 1432) Joan of Arcwas the defendant in the trial, while Pierre Cauchon acted asboth the prosecutor and the judge. She was charged with heresy.
Question Who was the judge in the Trial of Joan of Arc?Options 1) Pierre Cauchon; 2) Jean d’Estivet;Original Answer1) Pierre Cauchon;Prompt Answer2) Jean d’Estivet;
Table 7: A case where ChatGLM2 gives a wrong answer
with edited contexts provided. The sentence in red is the
edited one with time information changed. The answer
to the question appearing in the context is in blue.
into the prompt while it could originally give a right
prediction, is shown in Table 7. In this case, we
edit the beginning and end time of the trial, which
will not affect the answer to the question about
the judge of the trial. After we add a sentence
that instructs the model to neglect counterfactual
information in the context, the model changes its
answer from the right one Pierre Cauchon, which
has been clearly stated in the context, to another
name that even does not appear in the context.
We speculate that the instruction we add to the
prompt will reduce models’ trust in the external
knowledge. In QA-A, the models will thus dis-
believe the edited wrong answer in the context.
However, in QA-NA, models’ confidence in the
right answer appearing in the context will also fall
due to the instruction. This case indicates that it is
challenging to solve this problem by just modifying
the prompt.
6 Related Works
Hallucination in LLMs Although LLMs excel
at generating fluent natural language, studies show
that they are subject to the problem of hallucination,
which means that texts generated by the models of-
ten contain information that is irrelevant to user
inputs, conflicting with previous responses, or un-
faithful to established world knowledge (Ji et al.,
2022a; Rawte et al., 2023; Zhang et al., 2023b;
Huang et al., 2023). Some studies aim to mitigate
the issue of hallucination by incorporating addi-
tional information into the generation procedure,
such as Web corpora (Shuster et al., 2021; Huo
et al., 2023; Yu et al., 2023), knowledge graphs (Ji
et al., 2022b), and external tools (Gou et al., 2023).
Another line of work focuses on improving the de-
coding strategy of LLMs, such as careful prompt
design (Mündler et al., 2023), sampling multiple



Source: data\tc16_2312.10997v5\referenced_papers\[81]_2312.15883.pdf (Page 6):

HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable Medical LLMs Responses Conference’17, July 2017, Washington, DC, USA
Table 2: Performance comparison (in percent ±standard deviation) on CMB-Exam and MMCU-Medical for medical Q&A answer.
Red shading indicates the best-performing model, while blue signifies the second-best in the ablation study, and green signifies
the second-best in baselines.
LLM Turbo LLM GPT 3.5 Baichuan 13B-Chat
Method Dataset MMCU-Medical CMB-Exam MMCU-Medical CMB-Exam
Metric EM PCR EM PCR EM PCR EM PCR
Baselines
Base 43.52 ±1.92 50.55 ±1.88 38.40 ±2.03 46.76 ±1.93 42.20 ±2.87 46.09 ±2.65 36.91 ±2.94 40.95 ±2.70
KGRAG 38.74 ±1.66 43.38 ±1.68 38.00 ±1.90 42.26 ±1.88 34.37 ±2.36 38.51 ±2.10 39.92 ±2.37 45.84 ±2.29
QE 40.28 ±1.15 46.79 ±1.41 36.35 ±0.88 41.84 ±1.10 38.25 ±2.23 44.23 ±1.94 34.27 ±2.88 38.79 ±2.65
CoN 45.74 ±1.42 51.15 ±1.94 42.45 ±1.06 45.65 ±1.65 44.98 ±2.65 50.65 ±1.94 41.37 ±2.45 47.58 ±2.73
CoK 45.15 ±1.59 52.35 ±1.77 42.32 ±1.35 45.98 ±1.80 45.15 ±1.86 51.19 ±1.69 41.87 ±2.18 47.95 ±1.79
KALMV 39.24 ±1.41 43.77 ±1.23 38.24 ±0.84 43.37 ±1.89 36.17 ±2.33 40.85 ±2.11 38.61 ±2.44 43.92 ±1.97
KG-GPT 45.08 ±1.96 52.16 ±1.54 41.49 ±1.04 45.72 ±1.48 44.25 ±2.38 50.97 ±2.65 39.92 ±2.38 45.20 ±1.49
SuRe 44.81 ±1.38 51.49 ±1.97 41.37 ±1.26 44.27 ±1.47 44.77 ±1.80 50.24 ±2.09 39.49 ±1.57 46.22 ±1.70
Ours HyKGE 49.65 ±1.39 57.82 ±1.54 45.94 ±1.20 50.63 ±1.33 49.33 ±1.72 58.12 ±1.79 45.44 ±1.97 51.25 ±1.84
*Performance Gain ↑ 8.55 ∼28.16 10.45 ∼33.29 8.38 ∼26.38 8.28 ∼21.01 9.26 ∼43.53 13.54 ∼50.92 8.53 ∼32.59 6.88 ∼32.12
Ablation
HyKGE (w/o HO) 41.08 ±1.45 49.74 ±1.84 34.40 ±1.13 40.14 ±1.25 39.55 ±1.98 45.28 ±2.14 33.33 ±2.22 35.42 ±2.70
HyKGE (w/o Chains) 48.15 ±1.75 54.53 ±1.68 44.60 ±0.94 48.27 ±1.04 48.65 ±1.91 55.45 ±1.81 43.40 ±1.80 48.81 ±2.75
HyKGE (w/o Description) 48.30 ±1.45 54.01 ±1.86 44.80 ±1.33 48.56 ±1.41 48.22 ±2.12 55.23 ±1.86 43.77 ±2.37 49.86 ±1.47
HyKGE (w/o Fragment) 47.87 ±1.66 54.34 ±1.49 42.33 ±1.02 47.54 ±0.84 47.95 ±1.90 53.45 ±2.33 44.72 ±2.66 49.29 ±2.56
HyKGE (w/o Reranker) 46.38 ±1.65 52.48 ±1.88 41.44 ±0.88 48.84 ±1.09 43.59 ±2.34 46.88 ±2.56 40.65 ±2.27 46.25 ±2.11
Table 3: RAG relevance and answer performance comparison (in mean ±standard deviation) on CMB-Exam, MMCU-Medical
and CMB-Clin for medical Q&A answer with GPT 3.5 Turbo.
Method Dataset MMCU-Medical CMB-Exam CMB-Clin
Metric ACJ PPL ROUGE-R ACJ PPL ROUGE-R BLEU-1 BLEU-4 PPL ROUGE-R
Baselines
Base / 47.42 ±1.24 / / 62.54 ±0.94 / 4.83 ±1.21 6.51 ±1.55 10.38 ±1.47 23.99 ±1.06
KGRAG 13.38 ±4.27 151.22 ±2.87 5.31 ±0.97 18.40 ±5.58 218.67 ±3.68 11.25 ±1.93 5.34 ±1.51 8.77 ±1.90 61.81 ±2.51 22.15 ±1.27
QE 25.53 ±3.68 28.75 ±1.58 14.05 ±1.22 31.91 ±6.82 29.57 ±1.60 16.64 ±2.11 8.85 ±1.97 18.67 ±1.44 28.32 ±2.48 26.24 ±2.20
CoN 19.14 ±5.18 29.01 ±1.61 16.46 ±1.19 14.89 ±5.53 27.35 ±1.93 17.31 ±1.48 12.48 ±1.65 25.81 ±1.04 17.65 ±3.47 31.37±1.87
CoK 18.45 ±4.71 24.38±1.93 18.23±2.02 16.77 ±6.71 28.69 ±2.26 19.94±1.46 12.35 ±1.46 24.79 ±1.18 21.57 ±2.62 30.86 ±2.24
KALMV 14.42 ±3.88 147.22 ±3.12 7.21 ±1.08 18.77 ±5.91 233.49 ±4.19 12.84 ±1.34 5.72 ±1.16 8.27 ±1.20 80.46 ±2.51 23.16 ±2.23
KG-GPT 32.03±4.82 25.76 ±2.45 15.90 ±1.31 38.70±5.44 24.01±3.96 17.72 ±1.80 13.03±0.76 26.14±1.09 15.54±1.38 28.42 ±1.91
SuRe 20.16 ±3.93 26.49 ±2.88 16.91 ±1.84 22.27 ±4.02 30.81 ±2.59 16.18 ±1.70 10.54 ±0.92 24.82 ±1.31 16.84 ±1.46 29.18 ±1.62
Ours HyKGE 59.57±4.37 12.55±1.29 26.89±1.67 71.28±3.88 10.14±1.68 32.11±1.28 18.28±0.48 30.21±1.05 8.56±1.24 33.66±1.54
*Performance Gain 133.33∼345.22 48.52∼91.70 45.75∼406.40 84.19∼378.71 57.77∼95.36 61.03∼185.42 40.29∼278.47 15.57∼364.06 46.85∼89.25 7.30∼51.96
Ablation HyKGE (w/o HO) 41.49±5.36 15.57±2.31 22.30 ±2.37 51.48±4.92 11.23±1.96 29.01±1.96 7.15 ±2.35 11.55 ±1.89 8.96±1.01 30.48 ±2.58
HyKGE (w/o Fragment) 38.30±4.85 18.95 ±2.04 23.63±1.47 41.91 ±4.44 11.26 ±1.45 26.89 ±2.65 11.28±1.76 23.09±1.44 8.99 ±1.72 31.40±0.82
Table 4: Performance and computation time comparison (in
mean ±standard deviation) on MMCU-Medical for medical
Q&A answer with GPT 3.5 Turbo.
Method / Metric EM PCR Avg. Time (s)
HyKGE 49.65 ±1.39 57.82 ±1.54 19.76
HyKGE( +LLM for NER) 48.17 ±1.13 56.77 ±1.02 26.61
HyKGE( +LLM for Reranker) 42.72 ±2.06 48.24 ±1.17 32.51
HyKGE( +LLM for Summary) 43.02 ±3.11 46.54 ±2.08 28.51
three basic medical sciences, pharmacology, nursing, pathology,
clinical medicine, infectious diseases, surgery, anatomy, etc., with a
total of 2,819 questions. The CMB-Exam dataset utilizes qualifying
exams as a data source in the four clinical medicine specialties of
physicians, nurses, medical technicians, and pharmacists, with a to-
tal of 269,359 questions. Given the extensive size of the CMB-Exam
dataset, we randomly sample 4,000 questions for testing. The CMB-
Clin dataset contains 74 high-quality, complex, and real patient
cases with 208 medical questions.
5.1.2 Knowledge Graph. CMeKG (Clinical Medicine Knowledge
Graph)6 [12], CPubMed-KG (Large-scale Chinese Open Medical
6https://cmekg.pcl.ac.cn/, https://github.com/king-yyf/CMeKG_tools
Knowledge Graph) 7 and Disease-KG (Chinese disease Knowledge
Graph)8 are open-source medical KGs, which integrates extensive
medical text data, including diseases, medications, symptoms and
diagnostic treatment technologies. The fused KG has 1,288,721 en-
tities and 3,569,427 relations. However, due to the lack of medical
entity descriptions in its entities, we collect relevant entity knowl-
edge from Wikipedia9, Baidu Baike10, and Medical Baike 11, and
store them as entity descriptions.
5.1.3 LLM Turbo. To fairly verify whether HyKGE can effectively
enhance LLMs, we selected the following two types of general-
domain large models as the base model and explored the gains
brought by HyKGE: GPT 3.5 and Baichuan13B-chat [87].
5.1.4 Compared Methods. In order to explore the advantages of
the HyKGE, we compare the HyKGE results against eight other
models: (1) Base Model (Base) servers as the model without any
external knowledge, used to check the improvement effect of differ-
ent RAG methods. We use GPT 3.5 and Baichuan13B-chat as base
models. (2) Knowledge Graph Retrieval-Augmented Genera-
tion (KGRAG) [ 63–65] uses user query as a reference to retrieve
7https://cpubmed.openi.org.cn/graph/wiki
8https://github.com/nuolade/disease-kb
9https://www.wikipedia.org/
10https://baike.baidu.com/
11https://www.yixue.com/



### Claim 91/179

#### Claim Text
These segment, defined as tug-of-war units , are described by the spatial coordinate ξ ∈ [0, ℓ].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[84]_2402.07630.pdf (Page 6):

where zn and ze are the embeddings of node n and edge e, respectively. We use the cosine similarity
function, cos(·, ·), to measure the similarity between the query representation and the node/edge
embeddings. The argtopk operation retrieves the top-k elements based on this similarity, providing a
set of nodes Vk and edges Ek considered most relevant to the query. See Step 2 of Figure 3.
5.3 Subgraph Construction
This step aims to construct a subgraph that encompasses as many relevant nodes and edges as possible,
while keeping the graph size manageable. This approach offers two key benefits: Firstly, it helps
to filter out nodes and edges that are not pertinent to the query. This is crucial because irrelevant
information can overshadow the useful data, potentially diverting the focus of the subsequent LLM
from the information of interest. Secondly, it enhances efficiency; by keeping the graph size
manageable, it becomes feasible to translate the graph into natural language and then input it into the
LLM for processing. The Prize-Collecting Steiner Tree algorithm [2] serves as our primary method
for identifying such optimally sized and relevant subgraphs. See Step 3 in Figure 3.
Prize-Collecting Steiner Tree (PCST). The PCST problem aims to find a connected subgraph that
maximizes the total prize values of its nodes while minimizing the total costs of its edges. Our
approach assigns higher prize values to nodes and edges more relevant to the query, as measured by
cosine similarity. Specifically, the top k nodes/edges are assigned descending prize values from k
down to 1, with the rest assigned zero. The node prize assignment is as follows:
prize(n) =
k − i, if n ∈ Vk and n is the top i node,
0, otherwise. (6)
Edge prizes are assigned similarly. The objective is to identify a subgraph, S∗ = (V ∗, E∗), that
optimizes the total prize of nodes and edges, minus the costs associated with the size of the subgraph:
S∗ = argmax
S⊆G,
S is connected
X
n∈VS
prize(n) +
X
e∈ES
prize(e) − cost(S), (7)
where
cost(S) =|ES| ×Ce, (8)
and Ce denotes a predefined cost per edge, which is adjustable to control the subgraph size.
The original PCST algorithm is designed for node prizes only. However, given the significance of
edge semantics in certain scenarios, we adapt the algorithm to accommodate edge prizes as follows:
Consider an edge e with a cost Ce and a prize Pe. If Ce > Pe, it can be treated as a reduced edge cost
of Ce − Pe. However, if Pe > Ce, negative edge costs are not allowed in the original algorithm. Our
solution involves replacing edge e with a ‘virtual node’ ve, connected to both endpoints of e. This
virtual node is assigned a prize of Pe − Ce, and the cost of the two new edges leading to the virtual
node is set to zero. This modification effectively mirrors the original problem, as including edge e
in the original graph is analogous to including the virtual node in the modified graph. Finally, we
optimize the PCST problem using a near-linear time approach [9].
5.4 Answer Generation
Graph Encoder. Let S∗ = (V ∗, E∗) represent the retrieved subgraph. We use a graph encoder to
model the structure of this graph, specifically using a standard Graph Attention Network (GAT) [43].
Our approach for encoding the retrieved subgraph is defined as follows:
hg = POOL(GNNϕ1 (S∗)) ∈ Rdg , (9)
Here, POOL denotes the mean pooling operation, and dg is the dimension of the graph encoder.
Projection Layer. We incorporate a multilayer perceptron (MLP) to align the graph token with the
vector space of the LLM:
ˆhg = MLPϕ2 (hg) ∈ Rdl, (10)
where dl is the dimension of the LLM’s hidden embedding.
Text Embedder. To leverage the text-reasoning capabilities of LLMs, we transform the retrieved
subgraph S∗ into a textual format. This transformation involves flattening the textual attributes of the
7



Source: data\tc16_2312.10997v5\referenced_papers\[77]_2305.18846.pdf (Page 15):

Table 8: A list of notations that we used for defining our method.
V pre-defined vocabulary of tokens for pre-trained language models (text)
E pre-defined vocabulary of entities (symbol)
R pre-defined vocabulary of relations (symbol)
a, . . .z knowledge graph symbols written in typewrite font
x input sequence (vector)
x1, . . . , xN input tokens (scalar)
y = [y1, . . . , yT ] output sequence and tokens
G multi-relational graph, such as knowledge graph
Z retrieved subgraph: Z ⊂ G
z triplet (edge): z ∈ Z
q tokenization (mapping) function of KG symbol to the sequence of tokens
s(·) text representation function for retrieval
d(·) triplet representation function for retrieval
Enc Transformer Encoder
Dec Transformer Decoder
f token (word) embedding function
θ generator parameter
ϕ retriever parameter
ψ set encoding function
β perturbation function
π set permutation
n the number of triplets in a retrieved subgraph Z
k the number of samples in a marginalization term
z encoder hidden state (single token)
Z encoder hidden states (sequence of tokens)
h decoder hidden state (single token)
H decoder hidden states (sequence of tokens)
X input embeddings after token embedding function (sequence)
Y output embeddings after token embedding function (sequence)
With a naïve encoding, the PLM yields different representations for different orders of triplets in
the subgraph. Therefore, if the PLM is only fine-tuned with the input [A, born-in, C, B, born-in,
D, where was A born?], there is no guarantee that the PLM will output the exact same response
given the input with a permuted subgraph [B, born-in, D, A, born-in, C, where was A born?] in the
inference since the PLM is order-sensitive due to its positional encoding. In order to prevent the
aforementioned scenarios, we decide to design the permutation-invariant graph encoding which yields
stable results regardless of the order of triplets in the graph. Similarly, the inversion of the triplet
yields the same semantic (e.g., {(a, born-in, c)} = {(c, ¬born-in, a)}), but the graph encoding
without considerations for the inverse relation results in different representations from PLM given the
triplet and its inversed one.
D.2 Proofs
In this section, we first show that a naïve encoding function ψ in Section 3.4 is neither permutation
invariant nor relation inversion invariant, formalized in Proposition D.1. After that, we prove that our
invariant and efficient encoding function ψ∗ with graph-conditioned token embedding perturbation is
both permutation invariant and relation inversion invariant, formalized in Proposition D.2.
Proposition D.1. A naïve encoding function ψ is neither permutation invariant nor relation inversion
invariant.
Proof. We prove this by contradiction.
Suppose x = [ x1, . . . , xn] and Z = {(a, r1, b), (b, r2, a), (a, r1, c)}. Moreover, let Z′ =
{(b, r2, a), (a, r1, b), (a, r1, c)} be one of permutations of Z with the permutation order π =
(2, 1, 3).
16



Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 3):

Language Models as Semantic Indexers
… Codebook𝑬… Codebook𝑬
…
Semantic IDsWordtoken
He onlya rival
Deep TransEncoderDeep TransDecoder
Semantic Indexer
<s>
ShallowTransformer
Masked token 
…He only
is beenrival
ReconstructorSemantic Encoder
is been
Semantic ID representation
Document
 hints
Figure 1.The LMI NDEXER self-supervised ID learning framework overview. The proposed semantic indexer includes a semantic ID
encoder and several codebooks. During self-supervised learning, there is a reconstructor to reconstruct the input document from semantic
ID representations.
query channel input embeddings, and dh (token embeddings
correspond to dh) are fed as key and value channel input
embeddings in the multi-head self-attention. We adopt a
shallow reconstructor which has limited reconstruction capa-
bility based only on the hints in order to force the semantic
indexer to provide high-quality representations. The recon-
struction is conducted as follows:
zw “Reconϕpcd, dhq“
ÿ
t
Transpq “ct
d, k“dh, v“dhq
Preconpw|cd, dhq“ softmaxpW zwq
(5)
where W is the token embedding matrix. However, directly
adopting the reconstruction objective with cd as input to the
reconstructor will not optimize the semantic encoder. Since
the codebook look-up in Eq.(2) is a hard/discrete operation,
the reconstruction objective backpropagation gradients will
flow to the embeddings in the codebook rather than to the
parameters in the semantic encoder. To this end, we propose
to approximate the argmax operation similar to (Jang et al.,
2016) as follows:
ˆct
d “
$
&
%
arg maxet
jPEt ht
d ¨et
j forward pass.
ř
et
jPEt
exppht
d¨et
jqř
et
jPEt exppht
d¨et
jq et
j backward pass. (6)
In the forward pass, we still adopt the argmaxp¨qhard op-
eration; while in the backward pass, the selected semantic
embedding becomes a weighted average of the codebook
embeddings, to enable gradients to flow to ht
d and finally
to the parameters in the semantic encoder. In our imple-
mentation, we achieve this by adopting the stop gradient
operator (Van Den Oord et al., 2017). The reconstruction is
then conducted by
zw “Reconϕpˆct
d, dhq“
ÿ
t
Transpq “ˆct
d, k“dh, v“dhq
(7)
3.2. Training Self-Supervised Semantic Indexer
Progressive Training. To optimize the semantic indexer
and obtain semantic IDs in an auto-regressive way, we adopt
the progressive training scheme similar to (Sun et al., 2023).
The entire learning process consists ofT learning steps, each
corresponding to a specific semantic IDct
d being learned and
optimized at position t within the range of [T]. Additionally,
at each step t, both the ID ct
d and the model parameters
associated with generating ct
d are updated, while previously
generated IDs căt
d remain unchanged. The reconstruction
objective in t-step is shown as:
Lt
recon “´
ÿ
d
ÿ
wPdzdt
h
logPreconpw|cďt
d , dt
hq. (8)
Here dt
h is the hints provided for learning ID on position
t. We will gradually reduce the amounts of hints dt
h as t
increases to inject new knowledge into the new IDs, and
finally contribute to a hierarchical, coarse-to-fine-grained
semantic ID learning.
Contrastive Loss. The reconstruction objective in Eq.(8)
can force the semantic IDs to capture document-level se-
mantics. However, only optimizing the objective can lead to
the case where similar documents sharing căt
d also have the
same ct
d. To alleviate this issue, we propose a contrastive
objective to promote distinction between documents that
previously shared the same prefix, enabling the model to
discern finer-grained hierarchical relationships between doc-
uments:
Lt
contrastive “´
ÿ
d
log exppht
d ¨ht
dq
exppht
d ¨ht
dq` ř
căt
d1 “căt
d
exppht
d ¨ht
d1q.
(9)
The contrastive objective can help push ht
d of documents
sharing the same căt
d away in the t-th latent space and force
them to obtain diverse ct
d, finally contributing to higher
codebook utilization.
Commitment Loss. In addition, when learning the docu-
ment semantic IDs for position t, it is important that the
semantic indexer should remember the IDs that are already
learned before position t. To this end, we add a commitment
loss as:
Lt
commitment “´
ÿ
d
ÿ
jăt
log Pspcj
d|d, căj
d q. (10)
We optimize our model at step t based on a combination of
4



Source: data\tc16_2312.10997v5\referenced_papers\[77]_2305.18846.pdf (Page 4):

components for the multi-relational graph, is worthwhile to represent an entire triplet. We adopt
the existing node [14] and edge message passing frameworks [13]. Formally, our triplet embedding
function is denoted as follows:
d(z; G) = MLP([eh ∥ r ∥ et]), eh = GNN(e0
h; G),
r = GNN(r0; G∗), et = GNN(e0
t ; G),
(3)
where z = (eh, r, et), q(e) = [xi, . . . ,xj], 0 is a zero vector, and ∥ is the concatenation operator.
For node embedding e0, we reuse the word embedding from Enc, if it exists in x. For relation
embedding r0, we use the trainable relation embedding matrix. Please refer to Appendix E.1 for
more details.
3.4 Invariant Graph Encoding
We now specify graph encoding, which aims to condition the structural graph Z along with the text
sequence x over PLMs to generate y. Let ψ(x, Z) be a graph encoding function. Then, the simplest
way to encode graphs into PLMs is to prepend the tokens of entities and relations to the input x [19,
22]. Formally, given a text x = [x1, . . . , xN ] and a graph Z = {(a, r1, b), (b, r2, a), (a, r1, c)},
a naïve graph encoding is defined as follows: ψ(x, Z) = f([a, r1, b, b, r2, a, a, r1, c, x1, ..., xN ])
where a = q(a), r1 = q(r1), and so on. Here f is a token embedding and q is a mapping function
defined in Section 3.1. However, it violates two important properties for consistent encoding of a
multi-relational graph into PLMs: permutation invariance [ 48] and relation-inversion invariance,
formalized in Definition 3.1 and 3.2 as follows:
Definition 3.1. (Permutation Invariance) For any permutation π ∈ Sn, ψ(x, Z) = ψ(x, π· Z).
Definition 3.2. (Relation Inversion Invariance) Let ¬r be an inverse relation to r, if (a, r, b) =
(b, ¬r, a) ∀a, b ∈ E. Then, ψ(x, Z ∪ {(a, r, b)}) = ψ(x, Z ∪ {(b, ¬r, a)}) for any subgraph Z.
Invariant Graph Encoding To satisfy both properties, we consider two operations on a set of
triplets up to the naïve encoding. We first define a SORT operator that returns the same output
regardless of the order of input set elements, as follows:
SORT(π · Z) = SORT(π′ · Z), ∀π, π′ ∈ Sn, (4)
where Sn is a set of all possible permutations for n elements. We then define a INV operator that adds
the inverse triplet of each triplet in Z, as follows:
INV(Z) = Z ∪ {(et, ¬r, eh) | (eh, r, et) ∈ Z}. (5)
Based on them, our graph encoding function, ψ(x, SORT(INV(Z))), satisfies both invariances.
Invariant and Efficient Graph Encoding However, the above encoding is not efficient since it
requires O(n) space complexity with n triplets. Thus, we newly define ˜ψ that encodes the sorted
sequence of only the unique entities, as follows:
˜ψ(x, SORT(ENT(Z))) = f([a, b, c, x1, . . . , xN ]),
where ENT(Z) returns the set of unique entities in Z. This encoding meets both invariance properties
but also efficient since it only costs O(k), for the k entity where k < n. However, as it does not
consider the relational information in Z, we further perturb the token embeddings of each entity f(·)
in PLMs with respect to their graph representations in Z. Specifically, for each entity a ∈ ENT(Z),
we apply a learnable affine transformation [27] on the token embedding of a as follows:
β(f(a), Z) = (1 + γ) ∗ f(a) + δ,
γ, δ = MLP(η), η = R-GNN(f(a); Z), (6)
where MLP is a Multi-Layer Perceptron, β : Rd → Rd perturbs the embedding according to Z,
R-GNN is the relation-aware GNN [42]. In sum, we denote a relation-aware and invariant yet efficient
encoding ψ∗, defined as follows:
ψ∗(x, Z) = β( ˜ψ(x, SORT(ENT(Z))), INV(Z)).
We conclude that our graph encoding satisfies both properties. For further details on the proof and
comprehensive illustration, please refer to Appendix D.
5



### Claim 92/179

#### Claim Text
Annealing of the samples can improve the properties of NV centers, as demonstrated by Meng et al. .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 6):

Retrieve Anything To Augment Large Language Models Conference’17, July 2017, Washington, DC, USA
Table 2: Impact on in-context learning. The performances are measured by Misc. metrics (see Appendix).
In-Context Learning
Method CQA Comm Coref Para NLI RC Sent D2T Summ Avg
None 0.2923 0.7212 0.6578 0.5242 0.4478 0.4892 0.7077 0.1982 0.1447 0.4645
BM25 0.3603 0.7019 0.6029 0.5059 0.4583 0.5396 0.7284 0.3019 0.1555 0.4840
Instructor 0.5003 0.7772 0.5735 0.6312 0.5360 0.6219 0.9148 0.4595 0.4572 0.6036
Contriever 0.4912 0.7723 0.5624 0.6358 0.5466 0.6297 0.9141 0.4380 0.4444 0.6009
RetroMAE-BEIR 0.4594 0.7742 0.5840 0.5755 0.5408 0.6029 0.9286 0.4661 0.4465 0.5939
BGE∗ 0.4718 0.7773 0.5550 0.6171 0.5413 0.5988 0.9281 0.4719 0.4521 0.5974
AAR 0.4809 0.7796 0.5848 0.5890 0.5354 0.6039 0.9210 0.4445 0.4410 0.5938
API-Retriever 0.4765 0.7620 0.5465 0.6266 0.5204 0.6096 0.9245 0.4866 0.4424 0.5945
LLM-R† 0.5165 0.7802 0.5830 0.6567 0.6145 0.6223 0.9059 0.4777 0.4878 0.6262
LLM-Embedder 0.5163 0.7842 0.5927 0.6556 0.6041 0.6318 0.9224 0.4731 0.4742 0.6268
Table 3: Impact on long conversation and language modeling (PPL), tool learning (NDCG), conv search (NDCG).
Conversation Language Modeling Tool C-Search
Method MSC Books3 Arxiv CodeParrot PG19 (o.d.) ToolLLM QReCC
None 19.3501 8.8193 3.7647 2.7663 10.2510 – –
Recency 13.9569 8.7391 3.4158 2.5989 10.2216 – –
BM25 14.6512 8.6576 3.3106 2.4591 10.1960 0.5115 0.4341
Instructor 14.8799 8.6619 3.3546 2.4756 10.2011 0.3882 0.2863
Contriever 14.2129 8.6460 3.2709 2.4437 10.1616 0.4904 0.3563
RetroMAE-BEIR 14.3990 8.6376 3.2903 2.4592 10.1731 0.5205 0.4037
BGE∗ 14.2943 8.6311 3.2912 2.4578 10.1541 0.5761 0.3856
AAR 14.6999 8.6381 3.3260 2.4666 10.1808 0.4200 0.2877
API-Retriever† 14.7834 8.6722 3.3858 2.4919 10.1833 0.8017 0.1137
Conv-ANCE† – – – – – – 0.4560
LLM-R 14.4746 8.6619 3.3635 2.4724 10.2024 0.1321 0.0234
LLM-Embedder 13.4832 8.6080 3.2322 2.4303 10.1185 0.8645 0.5053
a simple yet strong baseline called Recency. Rather than using re-
trieved context, Recency directly leverages the most recent context
immediately preceding the current window. For example, in con-
versation, it considers the last pair of utterances before the current
session; and in language modeling, it introduces the content within
the range of 2049-4096 tokens preceding the latest 2048 tokens.
With the introduction of this new baseline, the impact of retrieval
augmentation becomes more nuanced. On one hand, the LLM-
Embedder continues to exhibit superior performance across various
situations. On the other hand, other retrievers no longer guarantee a
consistent enhancement: although alternative retrieval-augmented
methods yield improved generation quality for language modeling,
a majority of them fall short of Recency’s performance while dealing
with conversation. This observation underscores the challenges
regarding effective memory retrieval in practice.
•Tool Learning and Conversation Search . The experiment
results on tool learning and conversational search are shown in
Table 3. In line with our prior observations, the task-specific ap-
proaches, i.e. the API retriever (Tool) and Conv-ANCE (Conv Search),
consistently deliver higher performances then most of the baselines.
Besides, unlike other cases, BM25 overtakes most of the embedding
models in these two scenarios. However, it’s worth noting that
LLM-Embedder continues to maintain the leading position, which
again highlights its capability in unifying diverse retrieval tasks.
3.2.3 Ablation Studies. The ablation studies are presented to ana-
lyze the influential factors about LLM-Embedder’s training process
(see Table 4): reward from LLM, instruction based fine-tuning, ho-
mogeneous in-batch negative sampling, and stabilized distillation.
For “w.o. LLM reward ”, we replace the soft reward from LLM
by using highest rated candidates as positive samples (i.e. hard la-
bels). By doing so, the knowledge distillation is reduced to contrast
learning. The empirical performance in most of the scenarios are
decreased due to such a change. However, the performances in tool
learning and conversational search are little affect; this is compre-
hensible knowing that LLM-Embedder is purely trained with hard
labels in both scenarios.
For “w.o. instruction FT ”, we remove the task-specific instruc-
tions while fine-tuning LLM-Embedder. Without such a component,
it will become harder for the embedding model to discriminate the
retrieval task in different scenarios. This speculation is consistent
with the observed result, as LLM-Embedder’s performance is de-
creased from such a change.
For “w.o. homo NS ”, the homogeneous in-batch negative sam-
pling is disabled. Such a change could reduce the discrimination of



Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 0):

Preprint
This is a preprint version. It is recommended to refer to the conference version (ACL24), titled
AoE: Angle-optimized Embeddings for Semantic Textual Similarity . https://aclanthology.
org/2024.acl-long.101/.
ANGLE -OPTIMIZED TEXT EMBEDDINGS
Xianming Li, Jing Li∗
Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR
xianming.li@connect.polyu.hk, jing-amelia.li@polyu.edu.hk
ABSTRACT
High-quality text embedding is pivotal in improving semantic textual similarity
(STS) tasks, which are crucial components in Large Language Model (LLM) ap-
plications. However, a common challenge existing text embedding models face is
the problem of vanishing gradients, primarily due to their reliance on the cosine
function in the optimization objective, which has saturation zones. To address this
issue, this paper proposes a novel angle-optimized text embedding model called
AnglE. The core idea of AnglE is to introduce angle optimization in a complex
space. This novel approach effectively mitigates the adverse effects of the satura-
tion zone in the cosine function, which can impede gradient and hinder optimiza-
tion processes. To set up a comprehensive STS evaluation, we experimented on
existing short-text STS datasets and a newly collected long-text STS dataset from
GitHub Issues. Furthermore, we examine domain-specific STS scenarios with
limited labeled data and explore how AnglE works with LLM-annotated data. Ex-
tensive experiments were conducted on various tasks including short-text STS,
long-text STS, and domain-specific STS tasks. The results show that AnglE out-
performs the state-of-the-art (SOTA) STS models that ignore the cosine saturation
zone. These findings demonstrate the ability of AnglE to generate high-quality
text embeddings and the usefulness of angle optimization in STS. The code is
available at: https://github.com/SeanLee97/AnglE.
1 I NTRODUCTION
The development of text embeddings (Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017;
Cer et al., 2018; Reimers & Gurevych, 2019; Gao et al., 2021) is an essential research challenge in
the NLP community. Text embeddings effectively feature key semantic and syntactic information
in language, which broadly affects the performance of downstream tasks, such as text classification
(Li et al., 2021), sentiment analysis (Suresh & Ong, 2021; Zhang et al., 2022), semantic matching
(Grill et al., 2020; Lu et al., 2020), clustering (Reimers & Gurevych, 2019; Xu et al., 2023), and
question-answering (QA) system (Yue et al., 2021). In particular, text embedding models play a
crucial role in LLMs such as ChatGPT (OpenAI, 2022; 2023), LLaMA (Touvron et al., 2023a;b),
and ChatGLM (Du et al., 2022)-based applications. These LLM-based applications heavily rely on
high-quality text embeddings for tasks such as vector search, where related documents are retrieved
for LLM QA (Asai et al., 2023).
Recent studies (Gao et al., 2021; Jiang et al., 2022b; Chuang et al., 2022; Chanchani & Huang, 2023;
Zhuo et al., 2023) have utilized pre-trained language models such as BERT (Devlin et al., 2019) and
RoBERTa (Liu et al., 2019) in combination with contrastive learning to enhance the quality of text
embeddings. These approaches involve pulling semantically similar samples together and pushing
apart those not (Gao et al., 2021). In these contrastive models, positive samples that are semanti-
cally similar can be generated by data augmentation, while negative samples that are dissimilar are
selected from different texts within the same mini-batch (in-batch negatives). However, supervised
negatives are underutilized, and the correctness of in-batch negatives is difficult to guarantee without
∗Corresponding author
1
arXiv:2309.12871v9  [cs.CL]  31 Dec 2024



Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 18):

Preprint.
Supported, then we sample it as the continuation. If there is more than one passage satisfying this
criterion, we use the one with the highest retrieval score. If there are only ISREL =Irrelevant or
ISSUP =No Support passages, we randomly sample one passage.
Algorithm 3 Mgen Data creation
1: Input Input-output data D = X, Y
2: for (x, y) ∈ {X, Y} do
3: Given (x, y) C predicts Retrieve
4: if Retrieve is predicted then
5: Retrieve relevant passages D using R given (x, y) ▷ Retrieve passages
6: for d ∈ D do
7: C predicts ISREL for each d ▷ Predict relevance of passages
8: C predicts ISSUP for each (y, d) ▷ Predict supports of outputs
9: C predicts ISUSE for each d ▷ Predict overall utility (t = T only)
10: Sample d
11: else if Retrieve is not predicted then
12: C predicts ISUSE given x, y
Add augmented (x, y, d, r) to Dgen
Training examples. Table 4 show several training examples used forM training.
A.3 S ELF -RAG INFERENCE
Details of beam-search score calculations. We first compute scores for each critique type by
taking the normalized probabilities of desirable tokens. For ISREL , we compute the score as follows:
s( ISREL ) = p( ISREL = RELEVANT )
p( ISREL = RELEVANT ) +p( ISREL = IRRELEVANT ).
For ISSUP , we compute the score as follows:
s( ISREL ) =p( ISSUP = FULLY)
S + 0.5 × p( ISSUP = PARTIALLY )
S ,
where S = P
t∈{FULLY,PARTIALLY ,NO} p( ISSUP = t). For ISUSE where we have a five-scale score, we
compute the weighted sum of the scores. We assigns weighted scores of w = {−1, −0.5, 0, 0.5, 1}
to the tokens ISUSE ={1, 2, 3, 4, 5}, and compute the final scores as follows:
s( ISUSE ) =
5X
i
wi
p( ISUSE = i)
S ,
where S = P
t∈{1,2,3,4,5} p( ISUSE = t).
Details of adaptive retrieval. For retrieval based on soft constraints, we trigger retrieval if the
following condition is satisfied:
p( Retrieve = YES)
p( Retrieve = YES) +p(p( Retrieve = NO) > δ.
B E XPERIMENTAL DETAILS
B.1 M ORE DETAILS OF TRAINING
More details of training and computations. We use 4 Nvidia A100 with 80GB memory to train
our models. All models are trained for 3 epochs with a batch size of 128, a peak learning rate of 2e-5
with 3% warmup steps, and linear decay afterward. We set the maximum token length to be 2,048
for the 7B model, and 1,524 for the 13B model due to the memory constraint. We use Deepspeed
stage 3 (Rajbhandari et al., 2020) to conduct multi-GPU distributed training, with training precision
Bfloat16 enabled. FlashAttention (Dao et al., 2022) is used to make the long-context training more
efficient. We run inference of our trained models using 1-2 Quadro RTX 6000 GPUs with 24GB
memory.
19



Source: data\tc16_2312.10997v5\referenced_papers\[27]_2310.01352.pdf (Page 20):

Published as a conference paper at ICLR 2024
Table 10: Instruction template used for our fine-tuning datasets. <inst s>, <inst e> and
<answer s> are special markers denoting the start and the end of a field.
Category Instruction Tuning Template Query Template
Dialogue Background: {retrieved passage}\n\nQ:{turn1}A:{turn2}Q:
{turn3}A:...
{turn1} {turn2} {turn3}...
Open-domain QA Background:{retrieved passage}\n\n<insts> {question}
<inste> <answers>{answer}
{question}
Reading Compre-
hension
Background:{context}\n\n<insts>{question} <inste>
<answers>{answer}
{question}
Summarization Background: {context}\n\nSummarize this article:<inste>
<answers>{summary}
Chain-of-thought
Reasoning
Background:{retrieved passage}\n\n<insts>{instructions}
{reasoning chain}<answers>{answer}
{question}
Table 11: Our evaluation datasets. † indicates the development datasets we used to select fine-tuning
hyperparameters.
Task Dataset name Acronym Metric Score
Open-domain
QA
MMLU (?) MMLU Acc. nll
Natural Questions (Kwiatkowski et al., 2019) NQ EM nll
TriviaQA (Joshi et al., 2017) TQA EM nll
†HotpotQA (Yang et al., 2018) HoPo EM nll
ELI5 (Fan et al., 2019) ELI5 Rouge-L nll token
Fact Checking†FEVER (Thorne et al., 2018) FEV Acc. nll
Entity Linking†AIDA CoNLL-Y AGO (Hoffart et al., 2011) AIDA Acc. nll
Slot Filling
†Zero-Shot RE (Levy et al., 2017) zsRE Acc. nll
†T-REx (Elsahar et al., 2018) T-REx Acc. nll
Dialogue †Wizard of Wikipedia (Dinan et al., 2019) WoW F1 nll token
Commonsense
Reasoning
BoolQ (Clark et al., 2019) BoolQ Acc. nll compl
PIQA (Bisk et al., 2020) PIQA Acc. nll char
SIQA (Sap et al., 2019) SIQA Acc. nll char
HellaSwag (Zellers et al., 2019) HellaSwag Acc. nll char
WinoGrande (Sakaguchi et al., 2019) WinoGrande Acc. nll char
ARC-Easy (Clark et al., 2018) ARC-E Acc. nll char
ARC-Challenge (Clark et al., 2018) ARC-C Acc. nll char
OpenBookQA (Mihaylov et al., 2018) OBQA Acc. nll compl
computational cost. For test set evaluation, we use the full set to ensure fair comparison with pre-
vious work. The language model instruction templates and retriever queries used in our evaluation
are shown in Table 12. We randomly select few-shot examples from the official training splits of the
KILT tasks, except for FEV , NQ and TQA, where we use the 64-shot examples released by Izacard
et al. (2022b). For these three datasets, we also ensure that the 5-shot examples are subsets of the 64
examples. For retrieval augmented models, we use the top-1 relevant chunk to augment the prompt
for each in-context few-shot example.
E A DDITIONAL EXPERIMENTS
E.1 S CALING LAWS OF RETRIEVAL AUGMENTED LANGUAGE MODEL FINE -TUNING
We investigate the impact of the base language model size when retrieval-augmented instruction
tuning is applied, and summarize the results in Figure 2. We combine the fine-tuned models with
the base DRAGON + retriever in this set of experiments.
Overall, all models substantially benefit from retrieval augmentation, with smaller models witness-
ing even bigger improvements. We further note that retrieval augmentation can be an effective
strategy for enhancing the performance of smaller models (hence reducing pre-training and infer-
ence costs), given the 7B model leveraging > 1 retrieved chunks surpassed the performance of the
vanilla 65B model on several tasks. This trend also differs across tasks. For tasks that primarily
21



Source: data\tc16_2312.10997v5\referenced_papers\[47]_2305.17331.pdf (Page 12):

A Experimental Settings
A.1 Training Hyperparameters
We take the ANCE initialized from T5Base3 (Xiong
et al., 2021; Ge et al., 2023) and Contriever4 (Izac-
ard et al., 2021)’s hyperparameters in the
augmentation-adapted training. Specifically, we fix
batch size as 8, learning rate as 5e-6, and epochs as
6 for ANCE while taking batch size as 8, learning
rate as 1e-5, and epochs as 3 for Contriever. We
choose their best checkpoints based on the perfor-
mance of the development set. The information
about our source tasks and target tasks are listed in
Table 6.
A.2 Number of Augmentation Documents
LMs of different sizes, facing various target tasks,
may require indefinite numbers of augmentation
documents to achieve their best performance.
For MMLU, we analyze how the number of aug-
mentation documents affects LMs’ performance.
As illustrated in Figure 9, we discover that LMs of
larger capacity generally benefit more from more
augmentation documents. A possible explanation
is that larger LMs are more capable of integrating
information from multiple documents and perform-
ing complicated reasoning based on them.
For PopQA, using 3 augmentation documents
achieves the best performance across all LMs.
A.3 Prompt Templates
The prompt template for MMLU is:
Here’s a problem to solve: {question}
Among the 4 following options, which is
the correct answer?
- A: {choice_A}
- B: {choice_B}
- C: {choice_C}
- D: {choice_D}
The prompt template for PopQA is:
Q: {question} A:
B Selection of Source Task
We provide a detailed selection of the source tasks
here, using a variety of source and target tasks to an-
alyze. MSMARCO QA, KILT-TriviaQA, and NQ
belong to Open Domain QA, while KILT-T-REx
and zsRE belong to Slot Filling. MMLU belongs
to Multi-task Language Understanding, which is
3https://huggingface.co/OpenMatch/t5-ance
4https://huggingface.co/facebook/contriever-msmarco
Ts
Tt MMLU NQ zsRE
MSMARCO QA 44.8 46.7 75.1
KILT-TriviaQA 43.6 46.4 74.9
KILT-T-REx 44.1 45.9 77.2
Table 5: Relationship between the selection of source
task Ts and the performance of target task Tt. The
model is Flan-T5Base w/ AARANCE. As NQ and zsRE
are included in the Flan-T5 training data, we only report
their F1 results here for reference.
closer to the Open Domain QA in terms of the task
objective. As shown in Table 5, when we align the
category of the source task with the target task, the
LM w/ AAR can generally achieve the best results.
We suppose that this is because LM may share sim-
ilar document preferences on the tasks from the
same dataset category, making AAR easier to gen-
eralize. Furthermore, taking MSMARCO QA as
the source task performs the best on MMLU. This
validates the rationality to set Ts as MSMARCO
QA in our main experimental settings.
C AAR’s Improvements on PopQA
250M 780M 3B 175B
# Parameters
25
30
35
40
45
50
55PopQA Accuracy
ANCE
AARANCE
Contriever
AARContriever
Figure 8: AAR’s improvements on PopQA, using Flan-
T5Base (250M), Flan-T5Large (780M), Flan-T5XL (3B),
InstructGPT (175B) as target LMs.
D Fine-tuning Results
We also report the fine-tuning results of Flan-
T5Base and Flan-T5Large on MMLU auxiliary train-
ing data (Hendrycks et al., 2021) in Table 7. Due to
the limitation of the computational resources, we
do not include the fine-tuning result of Flan-T5XL.
We take batch size as 32, learning rate as 5e-5, and
epochs as 3 in fine-tuning. In general, the LM that
has already been massively multi-task instruction-
finetuned, such as Flan-T5, improves little from
fine-tuning on extra tasks but benefits greatly from
our AAR. The results further validate the power of
zero-shot retrieval augmentation.



### Claim 93/179

#### Claim Text
We focus on the linearized problem, while a comprehensive non linear analysis is carried out in .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[172]_2309.17453.pdf (Page 3):

Published as a conference paper at ICLR 2024
Dense Attention Window Attention Sliding Window 
w/ Re-computation StreamingLLM
Dense Attention Window Attention StreamingLLM
Figure 3: Language modeling perplexity on texts with 20K tokens across various LLM. Observations reveal
consistent trends: (1) Dense attention fails once the input length surpasses the pre-training attention window
size. (2) Window attention collapses once the input length exceeds the cache size, i.e., the initial tokens are
evicted. (3) StreamingLLM demonstrates stable performance, with its perplexity nearly matching that of the
sliding window with re-computation baseline.
Improving LLMs’ Utilization of Long Textoptimizes LLMs to better capture and employ the
content within the context rather than merely taking them as inputs. As highlighted by Liu et al.
and Li et al., success in the previously mentioned two directions does not necessarily translate to
competent utilization of lengthy contexts. Addressing this effective usage of prolonged contexts
within LLMs is still a challenge. Our work concentrates on stably harnessing the most recent tokens,
enabling the seamless streaming application of LLMs.
3 S TREAMING LLM
3.1 T HE FAILURE OF WINDOW ATTENTION AND ATTENTION SINKS
While the window attention technique offers efficiency during inference, it results in an exceedingly
high language modeling perplexity. Consequently, the model’s performance is unsuitable for deploy-
ment in streaming applications. In this section, we use the concept of attention sink to explain the
failure of window attention, serving as the inspiration behind StreamingLLM.
Identifying the Point of Perplexity Surge.Figure 3 shows the perplexity of language modeling on
a 20K token text. It is evident that perplexity spikes when the text length surpasses the cache size, led
by the exclusion of initial tokens. This suggests that the initial tokens, regardless of their distance
from the predicted tokens, are crucial for maintaining the stability of LLMs.
Why do LLMs break when removinginitial tokens’ KV? We visualize attention maps from
all layers and heads of the Llama-2-7B and models in Figure 2. We find that, beyond the bottom
two layers, the model consistently focuses on the initial tokens across all layers and heads. The
implication is clear: removing these initial tokens’ KV will remove a considerable portion of the
denominator in the SoftMax function (Equation 1) in attention computation. This alteration leads to a
significant shift in the distribution of attention scores away from what would be expected in normal
inference settings.
SoftMax(x)i = exi
ex1 + PN
j=2 exj
, x 1 ≫ xj, j∈ 2, . . . , N (1)
There are two possible explanations for the importance of the initial tokens in language modeling:
(1) Either their semantics are crucial, or (2) the model learns a bias towards their absolute position.
To distinguish between these possibilities, we conduct experiments (Table 1), wherein the first four
tokens are substituted with the linebreak token “\n". The observations indicate that the model still
significantly emphasizes these initial linebreak tokens. Furthermore, reintroducing them restores
the language modeling perplexity to levels comparable to having the original initial tokens. This
suggests that the absolute position of the starting tokens, rather than their semantic value, holds
greater significance.
LLMs attend to Initial Tokens as Attention Sinks.To explain why the model disproportionately
focuses on initial tokens—regardless of their semantic relevance to language modeling, we introduce
the concept of “attention sink". The nature of the SoftMax function (Equation 1) prevents all attended
tokens from having zero values. This requires aggregating some information from other tokens across
all heads in all layers, even if the current embedding has sufficient self-contained information for its
prediction. Consequently, the model tends to dump unnecessary attention values to specific tokens. A
similar observation has been made in the realm of quantization outliers (Xiao et al., 2023; Bondarenko
et al., 2023), leading to the proposal of SoftMax-Off-by-One (Miller, 2023) as a potential remedy.
4



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 20):

the compute budget. We start with the Equation (1.6), repeated here for convenience:
L(N,S) =
(Nc
N
)αN
+
(Sc
S
)αS
. (B.1)
Here, S represents the number of parameter updates when training at the critical batch size [MKAT18],
which was deﬁned in Equation (5.2)9:
B(L) = B∗
L1/αB
. (B.2)
We would like to determine optimal training parameters for a ﬁxed compute budget, so we replace S =
C/(6NB (L)), where Cis the number of FLOPs used in the training run:
L(N,C) =
(Nc
N
)αN
+
(
6B∗Sc
N
L1/αBC
)αS
. (B.3)
Now, we set ∂NL
⏐⏐
C = 0to ﬁnd the condition for optimality:
0 = ∂L
∂N
⏐⏐
C
= −αN
N
(Nc
N
)αN
+ αS
N
(
6B∗Sc
N
L1/αBC
)αS (
1 −5N
L∂L
∂N
⏐⏐
C
)
=⇒ αN
αS
(Nc
N
)αN
=
(
6B∗Sc
N
L1/αBC
)αS
(B.4)
Equation (B.3) and (B.4) together determine the compute-efﬁcient frontier.
B.2 Efﬁcient Training
Now we assemble the implications of (B.3) and (B.4). First, note that inserting (B.4) into (B.3) yields
L(Neﬀ (C) ,C) =
(
1 +αN
αS
)
L(Neﬀ,∞) , (B.5)
which implies that for compute-efﬁcient training, we should train to a ﬁxed percentage αN
αS
≈10% above
the converged loss. Next, let’s determine how the optimal loss depends on the compute budget. Eliminating
N yields a power-law dependence of performance on compute:
L(C) =
(Cc
C
)αC
(B.6)
where we deﬁned
αC = 1/(1/αS + 1/αB + 1/αN) ≈0.052 (B.7)
Cc = 6NcB∗Sc
(
1 +αN
αS
)1/αS+1/αN (αS
αN
)1/αS
. (B.8)
Similarly, we can eliminate Lto ﬁnd N(C):
N(C)
Nc
=
(C
Cc
)αC/αN (
1 +αN
αS
)1/αN
(B.9)
and
S(C) = Cc
6NcB∗
(
1 +αN
αS
)−1/αN (C
Cc
)αC/αS
(B.10)
9There is a slight ambiguity here: we can imagine training either at a constant batch size B (Ltarget), or we could
instead train at a variable batch size ˜B (L), where ˜B is the instantaneous critical batch size (as opposed to B, which is
the averaged version). These two prescriptions result in the same number of steps, so we can ignore this subtlety (see
[MKAT18]).
21



Source: data\tc16_2312.10997v5\referenced_papers\[101]_2310.06839.pdf (Page 0):

LongLLMLingua: Accelerating and Enhancing LLMs in Long Context
Scenarios via Prompt Compression
Huiqiang Jiang, Qianhui Wu, Xufang Luo,
Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu
Microsoft Corporation
{hjiang,qianhuiwu,xufluo,dongsli,cyl,yuqyang,liliqiu}@microsoft.com
Abstract
In long context scenarios, large language mod-
els (LLMs) face three main challenges: higher
computational cost, performance reduction,
and position bias. Research indicates that LLM
performance hinges on the density and posi-
tion of key information in the input prompt. In-
spired by these findings, we propose LongLLM-
Lingua for prompt compression towards im-
proving LLMs’ perception of the key informa-
tion to simultaneously address the three chal-
lenges. Our extensive evaluation across vari-
ous long context scenarios demonstrates that
LongLLMLingua not only enhances perfor-
mance but also significantly reduces costs and
latency. For instance, in the NaturalQuestions
benchmark, LongLLMLingua boosts perfor-
mance by up to 21.4% with around 4x fewer
tokens in GPT-3.5-Turbo, leading to substantial
cost savings. It achieves a 94.0% cost reduction
in the LooGLE benchmark. Moreover, when
compressing prompts of about 10k tokens at ra-
tios of 2x-6x, LongLLMLingua can accelerate
end-to-end latency by 1.4x-2.6x.1
1 Introduction
Large language models (LLMs) have revolution-
ized user-oriented language technologies and are
serving as crucial components in more and more
applications. Carefully designing prompts is nec-
essary to achieve better performance in specific
downstream tasks. The commonly used technolo-
gies such as In-Context Learning (ICL) (Min et al.,
2022; Dong et al., 2023), Retrieval Augment Gener-
ation (RAG) (Lewis et al., 2020; Asai et al., 2024),
and Multi-turn Agent (Shen et al., 2024; Park et al.,
2023; Wu et al., 2023a) are driving prompts to be
increasingly longer, even reaching thousands of to-
kens. Scenarios such as multi-document question
answering, code completion, and document sum-
marization also necessitate the processing of long
contexts.
1Access our code at https://aka.ms/LongLLMLingua.
There are three main challenges when LLMs are
used in long context scenarios: (1) Higher com-
putational costs, encompassing both financial and
latency expenses. (2) Longer prompts introduce
irrelevant and redundant information, which can
weaken LLMs’ performance (Shi et al., 2023), as
illustrated in Figure 1a. (3) LLMs exhibit position
bias (Kamradt, 2023), also known as the "lost in the
middle" issue (Liu et al., 2024), suggesting that the
placement of key information within the prompt
significantly affects LLMs’ performance. This is
demonstrated by the purple curve in Figure 1b.
Inspired by these observations, we propose
LongLLMLingua to address the three challenges.
Specifically, we use LLMLingua (Jiang et al.,
2023a) as the backbone for prompt compression
to address the first challenge, i.e., reduce cost and
latency. However, in the case of long contexts, the
distribution of question-relevant key information
in the prompt is generally dynamic and sparse. Ex-
isting prompt compression methods like LLMLin-
gua (Jiang et al., 2023a) and Selective-Context (Li
et al., 2023c) that often fail to consider question
during compression, resulting in retention of exces-
sive noise and decreased performance. LongLLM-
Lingua aims to improve LLMs’ perception of key
information pertinent to the question, thereby over-
coming the noise and position bias issues in long
contexts, shown in Figure 1b. The underlying prin-
ciple of LongLLMLingua is that small LM are
inherently capable of capturing the distribution of
key information relevant to a given question.
Our main contributions are five-fold: (1) We
propose a question-aware coarse-to-fine compres-
sion method to improve the key information den-
sity in the prompt (Sec. 4.1); (2) We introduce
a document reordering strategy to minimize po-
sition bias in LLMs. (Sec. 4.2); (3) We estab-
lish dynamic compression ratios for precise con-
trol between coarse and fine compression levels
(Sec. 4.3); (4) We propose a post-compression
arXiv:2310.06839v2  [cs.CL]  12 Aug 2024



Source: data\tc16_2312.10997v5\referenced_papers\[101]_2310.06839.pdf (Page 1):

1 5 10 15 20
Document Number in the Prompt
85
90
95
100Normalized Performance(%)
Multi-Document QA
Code Completion
Summarization
(a) Performance v.s. Document Number
1st 5th 10th 15th 20th
Position of Document with the Answer
55
60
65
70
75Accuracy(%)
 Original
LongLLMLingua 
w/o Reorder (4x)
LongLLMLingua (4x) (b) Performance v.s. Key Information Position
Figure 1: (a) LLMs’ performance in downstream tasks decreases with increased noise in prompts. In this case,
we keep k most relevant documents/paragraphs based on the ground-truth or LongLLMLingua rk. A larger k
implies more noise introduced into the prompt. To improve the key information density in the prompt, we present
question-aware coarse-to-fine compression. (b) LLMs’ ability to capture the relevant information depends on their
positions in the prompt. To reduce information loss in the middle, we introduce a document reordering mechanism.
subsequence recovery strategy to improve the in-
tegrity of the key information (4.4). (5) We evaluate
LongLLMLingua across five benchmarks, i.e., Nat-
uralQuestions (Liu et al., 2024), LongBench (Bai
et al., 2023), ZeroSCROLLS (Shaham et al., 2023),
MuSicQue (Trivedi et al., 2022), and LooGLE (Li
et al., 2023b), covering a variety of long con-
text scenarios. Experimental results reveal that
LongLLMLingua’s compressed prompts outper-
form original prompts in terms of performance,
cost efficiency, and system latency.
2 Problem Formulation
Following LLMLingua (Jiang et al., 2023a), we
use x = (xins, xdoc
1 , ··· , xdoc
K , xque) to represent
a prompt, including the instruction xins, K docu-
ments xdoc
i , and the question xque. However, this
definition can be adjusted for specific scenarios.
The objective of a prompt compression system can
be formulated as:
min
ex
Dϕ (y, ey) +λ∥ex∥0, (1)
where ex represents the compressed prompt, a token-
level subsequence of x. y and ey represent the
LLM-generated results from x and ex, respectively.
Dϕ measures the distance function, such as KL di-
vergence. λ serves as a hyper-parameter balancing
the compression ratio. Additionally, this study ex-
plores a permutation operation space over the K
documents (xdoc
1 , ··· , xdoc
K ) for joint optimization.
3 Preliminary: LLMLingua
LLMLingua (Jiang et al., 2023a) utilizes a small
language model MS to evaluate the perplexity of
each prompt token, removing those with lower per-
plexities. This method is premised on the idea
that tokens with lower perplexities have a negli-
gible effect on the language model’s overall en-
tropy gain, implying their removal slightly impacts
the LLMs’ contextual understanding. This process
is viewed as an application of "LM is Compres-
sion" (Delétang et al., 2023). LLMLingua include
three key components: budget controller, iterative
token-level prompt compression, and distribution
alignment, highlighted by italic text in Figure 2.
The budget controller assigns varying compres-
sion ratios to different parts of the prompt (i.e.,
instruction, demonstrations, question), implement-
ing coarse-level prompt compression. Subsequent
steps involve dividing intermediate results into seg-
ments and applying token-level compression iter-
atively, where each token’s perplexity based on
preceding compressed segments. To aware differ-
ent target LLMs, LLMLingua fine-tunesMS using
data from the target LLM.
4 LongLLMLingua
LongLLMLingua builds on LLMLingua to better
compress prompts in long context scenorias. It tack-
les three main issues in handling lengthy contexts,
as introduced in Sec. 1. This approach focuses on
making LLMs more effective at recognizing key



### Claim 94/179

#### Claim Text
Two improvements of the comb source were implemented compared to .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[71]_2310.04408.pdf (Page 1):

RECOMP during inference
moved from Smyrna, Tennessee, to 
Nissan's facility in Canton, Mississippi. 
Early US models include X, S and 
PRO-4X, with a choice of 6-speed 
manual or 5-speed automatic 
transmissions, a choice of [...]
moved from Smyrna, Tennessee, to 
Nissan's facility in Canton, Mississippi. 
Early US models include X, S and 
PRO-4X, with a choice of 6-speed 
manual or 5-speed automatic 
transmissions, a choice of [...]
moved from Smyrna, Tennessee, to 
Nissan's facility in Canton, Mississippi. 
Early US models include X, S and 
PRO-4X, with a choice of 6-speed 
manual or 5-speed automatic 
transmissions, a choice of [...]
moved from Smyrna, Tennessee, to 
Nissan's facility in Canton, Mississippi. 
Early US models include X, S and 
PRO-4X, with a choice of 6-speed 
manual or 5-speed automatic 
transmissions, a choice of [...]
moved from Smyrna, Tennessee, 
to Nissan's facility in Canton, 
Mississippi. Early US models 
include X, S and PRO-4X, with a 
choice of 6-speed manual…
Retrieved documents D
RECOMP 
(58 tokens)
Retrieve Compress Prepend
No retrieval  (0 tokens)
RALM  (749 tokens)
2010
 ❌
✅2015
Summary
Input query x
when did they 
stop making the 
nissan xterra?
Blackbox 
LM M 
2015
 ✅
Compressor 
The Nissan Xterra is a 
front-engine, 2-wheel or 4-
wheel drive, five-door …
Figure 1: An illustration of RECOMP , which compresses retrieved documents into a texual summary
before prepending it as input to a language model at inference time. The compressed summary guides
the LM to generate the correct answer, while significantly reducing the computation costs required to
encode the documents.
We propose compressors: (1) Extractive compressorwhich selects relevant sentences from retrieved
document set; (2) Abstractive compressorwhich generates a summary synthesizing information
from multiple retrieved documents. Both compressors implement multi-document query-focused
summarization (Xu & Lapata, 2020), where we summarize retrieved evidence document set with
respect to the input query. As we aim to enable RALM to generate correct output when summary is
prepended to the input query, we design training schemes to optimize the end task performance. Our
extractive compressor is trained with a contrastive learning objective to identify sentences that lead to
target outputs, and our abstractive compressor is distilled (West et al., 2022) from an extreme-scale
LM (e.g. GPT-3), which achieves impressive summarization performance.
Our experiments show that RECOMP can improve performance of frozen LMs on language model-
ing (Merity et al., 2016) and three question answering datasets (Natural Questions (Kwiatkowski et al.,
2019), TriviaQA (Joshi et al., 2017) and HotpotQA (Yang et al., 2018)), while prepending significantly
fewer tokens compared to RALM without compression. We present two oracle compression methods
– an extractive oracle which selects a sentence in evidence documents that leads to the best task
performance and an abstractive oracle which chooses between a summary generated by extreme-scale
LLM (e.g. GPT-3) and no retrieval augmentation that leads to the best task performance. Both oracle
methods achieve a compression rate as low as 6% and significantly outperforms prepending full
documents. Our trained compressors also show promising results. For language modelling, both
trained compressors achieve a compression ratio of 25% with minimal performance drop. When
applied to QA datasets, our best model compresses the documents to 5 - 10% of the original tokens
with at most less than 10% relative performance drop. We conclude with careful analyses of our
approach that reveal both its strength and weaknesses, thereby building foundation for future work.
2 P ROBLEM FORMULATION : RECOMP
Given an input sequence x, a target output sequence y and a set of N retrieved documents D
([d1, d2, ...dN ]),2 RECOMP compresses retrieved documents D with respect to x into a summary
s which captures core information in D relevant to x with significantly fewer tokens than D. Our
architecture consists of two modules: compressor cθ and LM M. In this work, we assume a blackbox
LM and train the compressor. Given the set of retrieved N documents ([d1, d2, ...dN ]) and the input
sequence x, a compressor returns a token sequence s. We design our compressor to be substantially
smaller than LM M, as we aim to reduce computational costs of encoding a set of retrieved documents.
The output from compressor, s, should be: (1) Concise: The summary should be as short as possible
to optimize efficiency. If the retrieved documents do not contain relevant information or retrieval
augmentation is not necessary, s can be an empty sequence. (2) Effecive: when s is prepended to
input sequence x and provided to LM M as a prompt, LM should generate the target output sequence
y. (3) Faithful: s should be a faithful and interpretable summary of the input document set (i.e.,
s must be entailed by the input document set ( [d1, d2, ...dN ])). We focus on training compressors
for conciseness and effectiveness. We summarize the key ideas for our two compressors, extractive
compressors and abstractive compressor here, and discuss their training schemes formally in Section 3.
2Improving retriever is not the focus of this work, so we assume a set of retrieved documents are provided.
2



Source: data\tc16_2312.10997v5\referenced_papers\[71]_2310.04408.pdf (Page 6):

Table 2: Open-domain QA results with Flan-UL2 (20B) as the LM M. We report number of tokens
provided as in-context evidence document, excluding the in-context examples. We train separate
compressors (one extractive, one abstractive) for each dataset. Extractive compressor selects one
sentence for NQ/TQA, and two sentences for HotpotQA.
NQ TQA HotpotQA
In-Context evidence # tok EM F1 # tok EM F1 # tok EM F1
- 0 21.99 29.38 0 49.33 54.85 0 17.80 26.10
RALM without compression
Top 1 documents 132 33.07 41.45 136 57.84 64.94 138 28.80 40.58
Top 5 documents 660 39.39 48.28 677 62.37 70.09 684 32.80 43.90
Phrase/token level compression
Top 5 documents (NE) 338 23.60 31.02 128 54.96 61.19 157 22.20 31.89
Top 5 documents (BoW) 450 28.48 36.84 259 58.16 65.15 255 25.60 36.00
Extractive compression of top 5 documents
Oracle 34 60.22 64.25 32 79.29 82.06 70 41.80 51.07
Random 32 23.27 31.09 31 50.18 56.24 61 21.00 29.86
BM25 36 25.82 33.63 37 54.67 61.19 74 26.80 38.02
DPR 39 34.32 43.38 41 56.58 62.96 78 27.40 38.15
Contriever 36 30.06 31.92 40 53.67 60.01 78 28.60 39.48
Ours 37 36.57 44.22 38 58.99 65.26 75 30.40 40.14
Abstractive compression of top 5 documents
Oracle 51 45.68 53.66 37 71.01 76.38 102 35.80 46.25
GPT-3.5 56 37.12 46.35 41 62.03 69.66 107 31.60 42.65
T5 10 25.90 34.63 7 55.18 62.34 7 23.20 33.19
Ours 36 37.04 45.47 32 58.68 66.34 64 28.20 37.91
Our trained extractive compressor significantly outperforms other extractive baselines (Contriever
and BM25) across all three LMs, while prepending slightly fewer tokens. Comparing to prepending
one document, we achieve a compression ratio of 25% at minimum performance drop. Our trained
abstractive compressor performs the best across the board, achieving the lowest perplexity and
the highest compression ratio. Our abstractive compressor achieves high compression rate through
selective augmentation, prepending summaries to only 33% of examples (length distribution of
generated summaries in Fig. 8).
Open-domain QA We report the results on QA tasks in Table 2. Similar to the language modeling
task, all retrieval augmentation methods improve performance over no retrieval setting, across three
datasets, consistent with previous study on other LMs (Shi et al., 2023b; Mallen et al., 2022; Si
et al., 2022). Unlike language modeling, prepending five documents shows significant gains over
prepending a single document, motivating the use of compression to incorporate more documents.
We find that extractive oracle outperforms the abstractive one in all datasets. Extractive oracle
selects the best one from N candidate sentences, while abstractive oracle selects from two options
– prepending GPT-3.5 summary or prepending nothing. Both oracles show improvements over
prepending all information, suggesting that removing irrelevant information benefit the model.8
Among extractive baselines, DPR performs the best as it has been fine-tuned on high-quality NQ data.
On NQ, selecting the top 1 DPR ranked sentences from top 5 documents outperforms prepending
top 1 document, with much fewer tokens (39 vs. 132). However, its performance degrades in out of
domain datasets. Off-the-shelf summarization model (T5) boasts the highest level of compression,
achieving 4-6 points gains in EM while adding mere 7-10 tokens.
The trained compressors, both extractive and abstractive, shows promising performances. On NQ
and TQA, the abstractive approach is more effective. On NQ, it achieves a compression ratio of 5%
tokens while losing 2 EM points compared to prepending full documents. On TQA, we observe
similar trends, compression ratio of 5% tokens while losing 3.7 EM points compared to prepending
full sets of documents. On HotpotQA that requires multihop understanding of documents, we
8We provide an example where our compressed summary yields correct answer while prepending full
document does not in Table 9 in the appendix.
7



Source: data\tc16_2312.10997v5\referenced_papers\[71]_2310.04408.pdf (Page 0):

RECOMP: I MPROVING RETRIEVAL -AUGMENTED LMS
WITH COMPRESSION AND SELECTIVE AUGMENTATION
Fangyuan Xu1, Weijia Shi2, Eunsol Choi1
Department of Computer Science
1The University of Texas at Austin
2University of Washington
{fangyuan,eunsol}@utexas.edu , swj0419@cs.washington.edu
ABSTRACT
Retrieving documents and prepending them in-context at inference time improves
performance of language model (LMs) on a wide range of tasks. However, these
documents, often spanning hundreds of words, make inference substantially more
expensive. We propose compressing the retrieved documents into textual sum-
maries prior to in-context integration. This not only reduces the computational
costs but also relieves the burden of LMs to identify relevant information in long re-
trieved documents. We present two compressors – an extractive compressorwhich
selects useful sentences from retrieved documents and an abstractive compressor
which generates summaries by synthesizing information from multiple documents.
Both compressors are trained to improve LMs’ performance on end tasks when the
generated summaries are prepended to the LMs’ input, while keeping the summary
concise. If the retrieved documents are irrelevant to the input or offer no additional
information to LM, our compressor can return an empty string, implementing
selective augmentation. We evaluate our approach on language modeling task and
open domain question answering task. We achieve a compression rate of as low as
6% with minimal loss in performance for both tasks, significantly outperforming
the off-the-shelf summarization models. We show that our compressors trained
for one LM can transfer to other LMs on the language modeling task and provide
summaries largely faithful to the retrieved documents.1
1 I NTRODUCTION
Retrieval-augmented language models (RALMs) (Khandelwal et al., 2019; Izacard et al., 2022; Lewis
et al., 2020; Borgeaud et al., 2022) have shown impressive performance on knowledge-intensive tasks
(Kwiatkowski et al., 2019; Petroni et al., 2021). Simply prepending retrieved documents to the input
without updating the language models (LMs) (Shi et al., 2023b; Ram et al., 2023; Si et al., 2022)
allows retrieval augmentation even for black-box LMs, but such approach comes with limitations.
First, it increases computational costs as LMs now encode substantially more tokens. Second, even if
we manage to adapt LMs to efficiently incorporate longer context (Beltagy et al., 2020; Zaheer et al.,
2020), these models struggle to use all information in the context, frequently missing information
placed in the middle (Liu et al., 2023). Third, prepending a large number of documents in-context
can further confuse LMs with irrelevant information, degrading model performances (Mallen et al.,
2022; Shi et al., 2023a).
To overcome such limitations, we propose RECOMP (Retrieve, Compress, Prepend), an inter-
mediate step for RALMs which compresses retrieved documents into a textual summary prior to
in-context augmentation. Figure 1 illustrates our approach. The generated summary should be concise
to maximize efficiency, be faithful to the retrieved evidence documents, and guide RALM to generate
desired outputs when prepended to the input. To satisfy both efficiency and effectiveness constraints,
our compressor strategically performs selective augmentation by generating an empty summary when
the retrieved documents are irrelevant or unhelpful for target task.
1Our code is available at https://github.com/carriex/recomp.
1
arXiv:2310.04408v1  [cs.CL]  6 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[9]_2311.03758.pdf (Page 7):

WWW ’24 Companion, May 13–17, 2024, Singapore, Singapore. Peng, et al.
Table 5: Overall performance of BEQUE with multiple baselines. The best results are in bold, and the second-best results are
underlined. “Top Queries” are defined as queries with retrieval results containing more than 70% related products; “Torso
Queries” are defined as queries with retrieval results containing 10%-70% related products; “Tail Queries” are defined as queries
with retrieval results containing less than 10% related products. All the metrics are the larger the better.
Method Top Queries Torso Queries Tail Queries All Queries
rele incr hitrate rele incr hitrate rele incr hitrate rele incr hitrate
CLE-QR 73.4 90.0 13.16 24.7 36.4 15.77 10.0 17.2 12.36 69.6 90.0 12.95
BART 67.5 106.0 13.26 27.9 130.0 17.70 8.4 27.5 13.59 62.2 100.0 13.56
Q2D (ChatGPT) 71.7 47.3 12.96 35.3 66.7 16.86 12.8 15.9 17.21 66.7 45.5 14.73
Qwen (SFT) 67.1 117.4 14.18 25.5 56.4 17.49 7.2 34.9 14.91 61.4 109.6 14.58
RL (rele) 74.7 48.6 12.66 39.4 2.1 15.60 14.5 8.1 11.36 70.0 45.1 12.42
RL (incr) 46.9 76.1 13.33 17.7 134.1 18.00 4.4 60.9 15.20 42.5 75.9 14.22
RL (hitrate) 56.0 176.8 15.10 6.2 117.3 20.16 2.2 51.0 18.22 49.6 162.8 15.75
BEQUE (rele) 69.3 174.5 15.04 27.3 160.9 19.29 4.1 66.4 18.27 62.3 164.0 16.43
BEQUE (incr) 64.6 212.3 15.45 20.5 207.7 21.45 2.4 71.7 19.62 57.7 198.7 17.27
BEQUE (hitrate) 69.3 177.0 15.39 18.4 204.0 19.78 5.0 62.5 18.22 62.1 167.0 16.64
Table 6: Online A/B test of BEQUE on Mobile Taobao Search.
“all queries”: every query in the test bucket counts, regardless
of whether it has been rewritten or not. “covered queries”:
only rewritten queries count. “long-tail queries”: rewritten
and long-tail queries count. “few-recall queries”: rewritten
and few-recall queries count.
Online Traffic GMV #Trans UV
all queries +0.40% +0.34% +0.33%
covered queries +2.96% +1.36% +1.22%
long-tail queries +1.57% +2.52% +2.32%
“few-recall” queries +18.66% +5.90% +6.25%
4.5 Online Experiments
To assess the actual online performance of BEQUE, we deployed it
on Taobao search for a 14-day online test, during which we recorded
the three key metrics in the Taobao search scene: GMV, #Trans, and
UV. Table 6 reveals that BEQUE surpassed the previous-generation
rewriting model CLE-QR by 0.4%, 0.34%, and 0.33% in terms of GMV,
#Trans, and UV, respectively. This implies that BEQUE contributes
millions of GMV to Taobao search. It’s important to note that the
overall performance mentioned here refers to all queries in the
testing buckets. Since we inference offline, there are about 70% of
online queries that do not hit our rewriting table. Even in these
cases, our model still delivers remarkable enhancements. Addition-
ally, for the queries covered (rewritten) by BEQUE (approximately
27% of total PV), there were noteworthy increases of 2.96%, 1.36%,
and 1.22% in GMV, #Trans, and UV, respectively. These findings
indicate that BEQUE effectively rewrites queries and addresses po-
tential semantic gaps in the semantic matching process. Moreover,
BEQUE significantly improved online #Trans and UV for long-tail
queries and “few-recall” queries, although we disregarded the GMV
fluctuation for this subset due to its low proportion. This improve-
ment can be attributed to our specialized optimization for long-tail
queries. During the first-stage SFT of BEQUE, rejection sampling
and auxiliary task data enhanced the model’s performance in terms
of retrieval increment and relevance, and also deepened its under-
standing of long-tail queries. The alignment process in the second
and third stages effectively compelled the model to align with online
objectives of Taobao search.
5 CONCLUSION
In this paper, we introduce BEQUE, a framework specifically de-
signed for e-commerce query rewriting. The main objective of
BEQUE is to address the semantic gap that occurs during the se-
mantic matching process, particularly for long-tail queries. Initially,
we improve the quality of the rewriting dataset by employing re-
jection sampling and auxiliary task mixing. We then train a LLM
using this refined dataset, which enhances the model’s query un-
derstanding and enables effective rewriting of e-commerce queries.
Utilizing the well-trained LLM, we generate multiple candidate
rewrites for each sampled query. To establish a partial order among
these candidates, we create an offline feedback system based on
online Taobao search. This feedback system accurately evaluates
the retrieval quality of the candidate rewrites from various per-
spectives, such as relevance, increment, and hitrate. Finally, by
incorporating the partial order of rewriting retrieval quality, we
introduce PRO, which aligns the model’s objectives with those of
Taobao. This ensures that our approach generates rewriting results
that yield high-quality retrieval outcomes. Through multiple exper-
iments, we have demonstrated the effectiveness of our approach
in improving offline metrics. Additionally, online A/B experiments
have substantiated a significant increase in Taobao Search’s GMV,
#Trans, and UV, particularly for long-tail queries.
6 ACKNOWLEDGMENTS.
This work was supported in part by the grants from National Natu-
ral Science Foundation of China (No.62222213, U22B2059, 62072423),
and the USTC Research Funds of the Double First-Class Initiative
(No.YD2150002009).



Source: data\tc16_2312.10997v5\referenced_papers\[32]_2310.14503.pdf (Page 6):

Model Top-1 ↑ Oracle↑ P-BLEU↓ Overall↑
SQuAD /1
Mixture-Decoder (Shen et al., 2019) 15.17 21.97 58.73 5.67
Mixture-Selector (Cho et al., 2019) 15.67 22.45 58.82 5.88
CV AE (Wang et al., 2020b) 15.34 21.15 54.18 5.99
Composition (Narayan et al., 2022a) ⋆ 16.5 25.7 58.99 7.21†
Nucleus-T5 (Holtzman et al., 2019) 12.98 23.45† 50.28 † 6.05
RAST(ours) 19.25 23.23 48.91 9.14
SQuAD /2
Composition (Narayan et al., 2022a) ⋆ 15.94† 24.90 60.05 6.61†
Nucleus-T5 (Holtzman et al., 2019) 13.31 24.42 † 55.54 5.85
RAST(ours) 19.36 22.59 56.42 † 7.75
NewsQA
Mixture-Decoder (Shen et al., 2019) 10.02 17.04 † 55.07 3.10
Mixture-Selector (Cho et al., 2019) 10.90† 17.51 52.61 3.63
CV AE (Wang et al., 2020b) 9.90 15.48 41.37 3.70 †
Nucleus (T5) (Holtzman et al., 2019) 5.29 14.63 27.47 † 2.82
RAST(ours) 11.02 16.26 23.16 7.74
Table 2: Comparison of different techniques on question generation on NewsQA and two splits of SQuAD. Here, ⋆
denotes that the results are re-evaluated by us. The best and runner-up† are marked.
planning and those based on sampling.
Content Planning-based Methods Mixture De-
coder (Shen et al., 2019) models a mixture of ex-
perts (MoE), where a latent variable drawn from
MoE is used to control the generation and produce
a diverse set of hypotheses. Mixture Selector (Cho
et al., 2019) focuses on different parts of the context
by modeling a binary variable for token selection.
CV AE (Wang et al., 2020b) also selects tokens from
the context, but uses a continuous latent variable
instead of a binary variable like Mixture-Selector.
Sampling-based Methods Nucleus Sampling
(Holtzman et al., 2019) samples tokens from a trun-
cated distribution, where the unreliable tail of1−p
probability mass is cropped. Composition Sam-
pling (Narayan et al., 2022a) uses nucleus sampling
to obtain diverse entity chains, then utilizes beam
search to generate the most-likely output.
5.3 Implementation Details
We use the pre-trained DPR (Karpukhin et al.,
2020) to initialize the retrieval encoders. Pre-
trained T5-base 3 is used for vanilla QG and the
style transfer model. During inference, the tem-
plate of the vanilla QG is used as a query to retrieve
N − 1 more templates. The obtained templates are
then used to generate N (N=5) questions for eval-
uation. We use SacreBLEU 4 to calculate BLEU.
3https://huggingface.co/t5-base
4https://github.com/mjpost/sacrebleu
More details can be found in Appendix A.
We conduct experiments with Nucleus-T5 by
ourself using Transformers5. In addition, the re-
sults of Composition Sampling are reevaluated,
whereas those of other baselines are from (Shen
et al., 2019; Cho et al., 2019; Wang et al., 2020b).
5.4 Results and Analysis
Table 2 summarizes our experimental results, for
which detailed analysis are given in the following.
Among diverse-promoting baselines, Nucleus-
T5 promotes diversity with the cost of Top-1 BLEU
being dropped significantly. CV AE and Com-
position are better at balancing between consis-
tency and diversity, resulting in high overall scores.
For example, in comparison with Nucleus-T5 on
SQuAD /2, Composition is more consistent (better
Top-1 and Oracle-BLEU), despite of being less di-
verse (lower Pairwise-BLEU). Our result is in line
with (Narayan et al., 2022b).
Compared to the previous methods, RAST
achieve the best diversity score (the lowest
Pairwise-BLEU) on SQuAD/1 and NewsQA, and
the second-best on SQuAD/2. Particularly, our
method outperforms strong baselines (Composi-
tion Sampling and CV AE) by a large margin in
terms of diversity, whereas being comparable on
consistency scores. Specifically, on NewsQA and
5https://github.com/huggingface/transformers



### Claim 95/179

#### Claim Text
The in-plane (Longitudinal) Goos-H¨ anchen (GH) shift originates due to angular gradient of the Fresnel coefficients associated with the change of angle of incidence for the non-central wave vectors and the out of the plane (Transverse) Imbert-Fedorov (IF) shifts originates from the spin orbit interaction of light .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 0):

Preprint
This is a preprint version. It is recommended to refer to the conference version (ACL24), titled
AoE: Angle-optimized Embeddings for Semantic Textual Similarity . https://aclanthology.
org/2024.acl-long.101/.
ANGLE -OPTIMIZED TEXT EMBEDDINGS
Xianming Li, Jing Li∗
Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR
xianming.li@connect.polyu.hk, jing-amelia.li@polyu.edu.hk
ABSTRACT
High-quality text embedding is pivotal in improving semantic textual similarity
(STS) tasks, which are crucial components in Large Language Model (LLM) ap-
plications. However, a common challenge existing text embedding models face is
the problem of vanishing gradients, primarily due to their reliance on the cosine
function in the optimization objective, which has saturation zones. To address this
issue, this paper proposes a novel angle-optimized text embedding model called
AnglE. The core idea of AnglE is to introduce angle optimization in a complex
space. This novel approach effectively mitigates the adverse effects of the satura-
tion zone in the cosine function, which can impede gradient and hinder optimiza-
tion processes. To set up a comprehensive STS evaluation, we experimented on
existing short-text STS datasets and a newly collected long-text STS dataset from
GitHub Issues. Furthermore, we examine domain-specific STS scenarios with
limited labeled data and explore how AnglE works with LLM-annotated data. Ex-
tensive experiments were conducted on various tasks including short-text STS,
long-text STS, and domain-specific STS tasks. The results show that AnglE out-
performs the state-of-the-art (SOTA) STS models that ignore the cosine saturation
zone. These findings demonstrate the ability of AnglE to generate high-quality
text embeddings and the usefulness of angle optimization in STS. The code is
available at: https://github.com/SeanLee97/AnglE.
1 I NTRODUCTION
The development of text embeddings (Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017;
Cer et al., 2018; Reimers & Gurevych, 2019; Gao et al., 2021) is an essential research challenge in
the NLP community. Text embeddings effectively feature key semantic and syntactic information
in language, which broadly affects the performance of downstream tasks, such as text classification
(Li et al., 2021), sentiment analysis (Suresh & Ong, 2021; Zhang et al., 2022), semantic matching
(Grill et al., 2020; Lu et al., 2020), clustering (Reimers & Gurevych, 2019; Xu et al., 2023), and
question-answering (QA) system (Yue et al., 2021). In particular, text embedding models play a
crucial role in LLMs such as ChatGPT (OpenAI, 2022; 2023), LLaMA (Touvron et al., 2023a;b),
and ChatGLM (Du et al., 2022)-based applications. These LLM-based applications heavily rely on
high-quality text embeddings for tasks such as vector search, where related documents are retrieved
for LLM QA (Asai et al., 2023).
Recent studies (Gao et al., 2021; Jiang et al., 2022b; Chuang et al., 2022; Chanchani & Huang, 2023;
Zhuo et al., 2023) have utilized pre-trained language models such as BERT (Devlin et al., 2019) and
RoBERTa (Liu et al., 2019) in combination with contrastive learning to enhance the quality of text
embeddings. These approaches involve pulling semantically similar samples together and pushing
apart those not (Gao et al., 2021). In these contrastive models, positive samples that are semanti-
cally similar can be generated by data augmentation, while negative samples that are dissimilar are
selected from different texts within the same mini-batch (in-batch negatives). However, supervised
negatives are underutilized, and the correctness of in-batch negatives is difficult to guarantee without
∗Corresponding author
1
arXiv:2309.12871v9  [cs.CL]  31 Dec 2024



Source: data\tc16_2312.10997v5\referenced_papers\[77]_2305.18846.pdf (Page 16):

With a naïve encoding, ψ(x, Z) = [ a, r1, b, b, r2, a, a, r2, c, x1, . . . ,xn] and ψ(x, Z′) =
[b, r2, a, a, r1, b, a, r1, c, x1, ...,xn]. Therefore, it is easy to notice that ψ(x, Z) ̸= ψ(x, Z′),
thus the naïve encoding is not permutation invariant.
We then show that a naïve encoding is not relation inversion invariant. Suppose Z′′ =
{(a, r1, b), (b, r2, a), (c, ¬r1, a)}, where (a, r1, c) ∈ Z is changed to its inverse relation
(c, ¬r1, a). Then, ψ(x, Z′′) = [ a, d, b, b, e, a, c, ¬d, a, x1, . . . ,xn] that is different against
ψ(x, Z): ψ(x, Z) ̸= ψ(x, Z′′). Therefore, the naïve encoding function is not relation inversion
invariant.
In conclusion, from the above two counterexamples, we prove that a naïve encoding functionψ is
neither permutation invariant nor relation inversion invariant.
We now provide proof of the permutation invariance and the relation inversion invariance of our
invariant and effective graph encodingψ∗, described in Section 3.4. Before starting the proof, we first
revisit the permutation invariant property of graph neural networks that sum, mean and max operators
are permutation invariant for the input set of AGGR. Thus, if we use sum, mean, or max for AGGR, then
the token embedding perturbation function β naturally satisfies the permutation invariance property.
In other words, β(X, Z) = β(X, π· Z), where X = ˜ψ(x, SORT(ENT(Z))) for any permutation π.
Proposition D.2. Invariant and efficient encoding ψ∗ is both permutation invariant and relation
inversion invariant.
Proof. Suppose x = [ x1, . . . , xn] and Z = {(a, r1, b), (b, r2, a), (a, r1, c)}. We first consider
the permutation invariance for any permuted set Z′ = π · Z. While Z and Z′ can have different
orders of elements thus the outputs of ENT(Z) and ENT(Z′) could be different, we always obtain the
same output with the usage of the SORT operator for encoding. In other words, SORT(ENT(Z)) =
SORT(ENT(Z′)) holds due to the definition of theSORT operation in Eq. 5 of the main paper. Therefore,
˜ψ(x, SORT(ENT(Z))) = ˜ψ(x, SORT(ENT(Z′))) holds.
Further, since the token embedding perturbation function β(·, Z) along with sum, max, or mean in
AGGR is also permutation invariant with regards to any permutation on Z, we conclude our invariant
and efficient encoding ψ∗ is permutation invariant.
We finally prove the relation inversion invariance property ofψ∗. Suppose Z′′ = (Z ∪t′) \ t where
t ∈ Zis any triplet in a set and t′ is inverse of t. Then, ENT(Z) = ENT(Z′′) that is trivial as ENT(Z)
returns the set of only unique nodes inZ. Therefore, ˜ψ(x, SORT(ENT(Z))) = ˜ψ(x, SORT(ENT(Z′′)))
correspondingly holds.
The remaining step to conclude the proof is to show the following equality: β(·, INV(Z)) =
β(·, INV(Z′′)), to conclude that ψ∗(x, Z) = ψ∗(x, Z′′) from β( ˜ψ(x, SORT(ENT(Z))), INV(Z)) =
β( ˜ψ(x, SORT(ENT(Z′′))), INV(Z′′)). We note that INV(Z) = INV(Z′′), as INV makes any graph
as bidirectional one by the definition in Eq. 6 of the main paper. Therefore, β(·, INV(Z)) =
β(·, INV(Z′′)) holds, and the relation inversion invariance property of ψ∗ holds.
E Experimental Setup
In this section, we introduce the detailed experimental setups for our models and baselines. Specif-
ically, we describe the details on implementation, dataset, training and model in the following
subsections of E.1, E.2, E.3 and E.4, one by one.
E.1 Implementation Details
We use the T5-small [30] as the base Pre-trained Language Model (PLM) for all experiments. For
the pre-trained checkpoint, we use the version that the authors released. For all implementations, we
use Pytorch [26]. To easily implement the language model, we use the huggingface transformers
library [46].
17



Source: data\tc16_2312.10997v5\referenced_papers\[178]_2210.03765.pdf (Page 2):

Input Context x  
A man is seen skiing behind a boat. He holds on tight as he is pulled through the water. The man …
Target :  is water skiing until the end of the clip.y
Prediction : then moves to the side and begins to swim./uni0302 y
AE Decoder
AE Encoder
CLIP 
Visual 
Encoder
...Mapping 
Network
... Language 
Model
Projection 
Layer
Machine Imagination I 
v 
 /uni0302 t
c1 c2 cl t1 t2 tm 
Text Emb.Visual Prefix
Input Visual Feature
L
"
acher
L
con
#
as
!
ve
Predicted Text Feature
Text-to-Image 
Generation
Visually-Guided Text Generation
Diffusion 
Model
Text Encoder
randomly 
initialized 
image
Figure 2: An overview of our iNLG. Given an input contextx, we ﬁrst visualize the context with the text-to-image
generation model. Then we use the machine-generated image I as the additional visual supervision to guide the
language model in open-ended text generation. The visual feature is provided as a source of input to the LM in the
form of the visual preﬁx. Aside from the teacher forcing objective Lteacher, we also enforce the LM to generate text
that is semantically similar to the machine imagination with a contrastive training objective Lcontrastive.
3 Method
3.1 Overview
Open-ended text generation is a task that provides
an input context, and asks the model to generate a
piece of text that is consistent with the context.
This work mainly focused on introducing
machine-rendered images to assist LM in perform-
ing open-ended text generation. More speciﬁcally,
given the context xi, we ﬁrst use a text-to-image
generator to illustrate an image Ii that depicts the
input context. The LM is prompted with image
Ii as the visual preﬁx along with the text context
xi, and will incorporate the multimodal input to
generate the output text ˆyi.
Figure 2 provides an overview of our iNLG
framework, which mainly involves two modules.
The ﬁrst module is a text-to-image generator that
takes in the input context and illustrates a descrip-
tive image, which we also refer to as the machine
imagination. The second module is a visually-
guided language model that utilizes the machine
imagination as a source of input and also a supervi-
sion that encourages the LM to generate text that is
semantically similar to the visual information.
3.2 Text-to-Image Rendering
In this work, we propose to use images gener-
ated conditioning on the context by the machines
as additional visual information to the LM. The
text-to-image generation backbone is StableDiffu-
sion (Rombach et al., 2022), which mainly consists
of a text encoder, a diffusion model, and an au-
toencoder. The text encoder is from the frozen
CLIP ViT-L/14 (Radford et al., 2021) and encodes
the input text to textual embeddings. The diffu-
sion model uses UNet (Ronneberger et al., 2015)
to provide noise estimation. The UNet is modi-
ﬁed so as to attend to the input textual embeddings.
The encoder of the pretrained autoencoder encodes
images into the lower-resolution latent maps zT.
At each step t, the diffusion model provides the
noise estimation ϵand modiﬁes zt correspondingly.
The decoder of the pretrained autoencoder takes
the ﬁnal noise-free latent map z and generates the
image prediction. StableDiffusion is trained with
LAION-5B (Schuhmann et al., 2022).
3.3 Visually Guided Text Generation
Visual Preﬁx Construction One can encode the
visual information with the pre-trained visual mod-
els. However, such visual embedding may lie in a
representation space different from the LM due to
the discrepancy between models. One way of intro-
ducing features extracted by another network to the
current model is through feature mapping (Mokady
et al., 2021). With a dataset of image-text pairs
pI1,x1q, we can pre-train a mapping network F for
a given LM in an image captioning formulation.
More speciﬁcally, we encode I1 with the visual
encoder Encvisual and receive its visual features v1.
Then we apply the mapping network F over v1,
and receive a sequence of lvisual preﬁxes:
c1
1,c1
2,...,c 1
l “Fpv1q“ FpEncvisualpI1qq (1)



Source: data\tc16_2312.10997v5\referenced_papers\[77]_2305.18846.pdf (Page 15):

Table 8: A list of notations that we used for defining our method.
V pre-defined vocabulary of tokens for pre-trained language models (text)
E pre-defined vocabulary of entities (symbol)
R pre-defined vocabulary of relations (symbol)
a, . . .z knowledge graph symbols written in typewrite font
x input sequence (vector)
x1, . . . , xN input tokens (scalar)
y = [y1, . . . , yT ] output sequence and tokens
G multi-relational graph, such as knowledge graph
Z retrieved subgraph: Z ⊂ G
z triplet (edge): z ∈ Z
q tokenization (mapping) function of KG symbol to the sequence of tokens
s(·) text representation function for retrieval
d(·) triplet representation function for retrieval
Enc Transformer Encoder
Dec Transformer Decoder
f token (word) embedding function
θ generator parameter
ϕ retriever parameter
ψ set encoding function
β perturbation function
π set permutation
n the number of triplets in a retrieved subgraph Z
k the number of samples in a marginalization term
z encoder hidden state (single token)
Z encoder hidden states (sequence of tokens)
h decoder hidden state (single token)
H decoder hidden states (sequence of tokens)
X input embeddings after token embedding function (sequence)
Y output embeddings after token embedding function (sequence)
With a naïve encoding, the PLM yields different representations for different orders of triplets in
the subgraph. Therefore, if the PLM is only fine-tuned with the input [A, born-in, C, B, born-in,
D, where was A born?], there is no guarantee that the PLM will output the exact same response
given the input with a permuted subgraph [B, born-in, D, A, born-in, C, where was A born?] in the
inference since the PLM is order-sensitive due to its positional encoding. In order to prevent the
aforementioned scenarios, we decide to design the permutation-invariant graph encoding which yields
stable results regardless of the order of triplets in the graph. Similarly, the inversion of the triplet
yields the same semantic (e.g., {(a, born-in, c)} = {(c, ¬born-in, a)}), but the graph encoding
without considerations for the inverse relation results in different representations from PLM given the
triplet and its inversed one.
D.2 Proofs
In this section, we first show that a naïve encoding function ψ in Section 3.4 is neither permutation
invariant nor relation inversion invariant, formalized in Proposition D.1. After that, we prove that our
invariant and efficient encoding function ψ∗ with graph-conditioned token embedding perturbation is
both permutation invariant and relation inversion invariant, formalized in Proposition D.2.
Proposition D.1. A naïve encoding function ψ is neither permutation invariant nor relation inversion
invariant.
Proof. We prove this by contradiction.
Suppose x = [ x1, . . . , xn] and Z = {(a, r1, b), (b, r2, a), (a, r1, c)}. Moreover, let Z′ =
{(b, r2, a), (a, r1, b), (a, r1, c)} be one of permutations of Z with the permutation order π =
(2, 1, 3).
16



### Claim 96/179

#### Claim Text
This makes their ratios, or site preferences highly informative for determining global N 2O fluxes , where the site preference relates to 15N being in the central ( α isotopomer) or the terminal (β isotopomer) position and is calculated as SP = ([14N15NO]/[15N14NO] – 1) × 1000 ‰.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 15):

0 50 100 150
1.6
1.8
2
2.2
2.4
2.6
2.8
3 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(a) ELI5 - p = 10%
0 50 100 150
1.4
1.45
1.5
1.55
1.6
1.65 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (b) ELI5 - p = 30%
0 50 100 150
1.04
1.06
1.08
1.1
1.12
1.14 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (c) ELI5 - p = 50%
0 10 20 30 40 50
1.6
1.8
2
2.2
2.4
2.6
2.8
3
3.2
3.4 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(d) MS MARCO - p = 10%
0 10 20 30 40 50
1.15
1.2
1.25
1.3
1.35
1.4 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
0.91 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (f) MS MARCO - p = 50%
0 20 40 60 80 100
1.5
2
2.5
3
3.5
1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(g) NQ - p = 10%
0 20 40 60 80 100
1.5
1.6
1.7
1.8
1.9
2 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
Loa ding [ M a thJ a x ] / ex tensions/ M a thM enu .j s (h) NQ - p = 30%
0 20 40 60 80 100
1.22
1.24
1.26
1.28
1.3
1.32
1.34
1.36 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (i) NQ - p = 50%
Figure 7: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-12), for FiD-Base models.
Each row represents a different dataset, and every column represents a different filtering percentage (10,30,50).



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 17):

0 50 100 150
1.4
1.6
1.8
2
2.2
2.4
2.6 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(a) ELI5 - p = 10%
0 50 100 150
1.35
1.4
1.45
1.5
1.55
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (b) ELI5 - p = 30%
0 50 100 150
1.04
1.06
1.08
1.1
1.12
1.14 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (c) ELI5 - p = 50%
0 10 20 30 40 50
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
3.2 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(d) MS MARCO - p = 10%
0 10 20 30 40 50
1.1
1.15
1.2
1.25
1.3
1.35 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.84
0.86
0.88
0.9
0.92 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (f) MS MARCO - p = 50%
0 20 40 60 80 100 120
1.5
2
2.5
3
3.5
4
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(g) NQ - p = 10%
0 20 40 60 80 100 120
1.5
1.6
1.7
1.8
1.9
2
2.1 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
Loa ding [ M a thJ a x ] / ex tensions/ M a thM enu .j s (h) NQ - p = 30%
0 20 40 60 80 100 120
1.25
1.3
1.35
1.4
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (i) NQ - p = 50%
Figure 9: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-24), for FiD-Large models.
Each row represents a different dataset, and every column represents a different filtering percentage (10,30,50).



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 16):

0 50 100 150
1
1.5
2
2.5
3 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens
(a) ELI5 - p = 10%
0 50 100 150
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (b) ELI5 - p = 30%
0 50 100 150
0.85
0.9
0.95
1
1.05
1.1
1.15
1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (c) ELI5 - p = 50%
0 10 20 30 40 50
0.5
1
1.5
2
2.5
3
3.5 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens
(d) MS MARCO - p = 10%
0 10 20 30 40 50
0.6
0.8
1
1.2
1.4
1.6
1.8 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.8
0.9
1
1.1
1.2
1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (f) MS MARCO - p = 50%
0 20 40 60 80 100
0.5
1
1.5
2
2.5
3
3.5 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens
(g) NQ - p = 10%
0 20 40 60 80 100
0.8
1
1.2
1.4
1.6
1.8
2 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens (h) NQ - p = 30%
0 20 40 60 80 100
0.9
1
1.1
1.2
1.3
1.4 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens (i) NQ - p = 50%
Figure 8: The percentage of tokens that were chosen from each passage, for FiD-Base models. The gold passage
(labeled as 1) is colored red. Each row represents a different dataset, and every column represents a different filtering
percentage (10,30,50).



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 18):

0 50 100 150
0.8
1
1.2
1.4
1.6
1.8
2
2.2 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens
(a) ELI5 - p = 10%
0 50 100 150
0.9
1
1.1
1.2
1.3
1.4
1.5
1.6 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (b) ELI5 - p = 30%
0 50 100 150
0.9
0.95
1
1.05
1.1
1.15 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (c) ELI5 - p = 50%
0 10 20 30 40 50
0.5
1
1.5
2
2.5
3 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens
(d) MS MARCO - p = 10%
0 10 20 30 40 50
0.8
1
1.2
1.4
1.6 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.8
0.85
0.9
0.95
1
1.05
1.1
1.15
1.2
1.25 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (f) MS MARCO - p = 50%
0 20 40 60 80 100 120
0.5
1
1.5
2
2.5
3
3.5
4
4.5 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens
(g) NQ - p = 10%
0 20 40 60 80 100 120
0.6
0.8
1
1.2
1.4
1.6
1.8
2 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens (h) NQ - p = 30%
0 20 40 60 80 100 120
0.8
0.9
1
1.1
1.2
1.3
1.4 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens (i) NQ - p = 50%
Figure 10: The percentage of tokens that were chosen from each passage, for FiD-Large models. The gold passage
(labeled as 1) is colored red. Each row represents a different dataset, and every column represents a different filtering
percentage (10,30,50).



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 13):

0 1000 2000 3000 4000
24.5
25
25.5
26
26.5
27
FLOPS (MACs)
Rouge-L
2%
(a) ELI5
0 1000 2000 3000 4000
20
21
22
23
24
25
FLOPS (MACs)
2% (b) MS MARCO
0 1000 2000 3000 4000
24
25
26
27
28
29
30
31
32
FLOPS (MACs)
2% (c) NQ
0 20 40 60 80 100
24.5
25
25.5
26
26.5
27
Input P sgs
Rouge-L
2%
(d) ELI5
0 20 40 60 80 100
20
21
22
23
24
25
Input P sgs
2% (e) MS MARCO
0 20 40 60 80 100
24
25
26
27
28
29
30
31
32
FiD
CALM
T ok en Filtering
Combined
Input P sgs
2% (f) NQ
Figure 6: The ROUGE-L performance on FiD-Base vs. the FLOPS (MACs) (First row), and vs. the input passage
amount (Input Psg., second row). The results are on the test set for each dataset, for the various methods utilized.
We observe that the trends of the FLOPS (MACs) and the Input Psg. are very similar, since the passages effect the
encoder the most, and the encoder has the most impact on FLOPS (MACs) (as stated by de Jong et al. (2022)).



### Claim 97/179

#### Claim Text
Also, external radiation like cosmic rays or bremsstrahlung photons from very fast runaway electrons can act as an electron source .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[35]_2402.13482.pdf (Page 0):

Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks
Minju Seo∗ Jinheon Baek∗ James Thorne Sung Ju Hwang
KAIST
{minjuseo, jinheon.baek, thorne, sjhwang82}@kaist.ac.kr
Abstract
Despite large successes of recent language mod-
els on diverse tasks, they suffer from severe
performance degeneration in low-resource set-
tings with limited training data available. Many
existing works tackle this problem by generat-
ing synthetic data from the training data and
then training models on them, recently using
Large Language Models (LLMs). However, in
low-resource settings, the amount of seed data
samples to use for data augmentation is very
small, which makes generated samples subopti-
mal and less diverse. To tackle this challenge,
we propose a novel method that augments train-
ing data by incorporating a wealth of examples
from other datasets, along with the given train-
ing data. Specifically, we first retrieve the rele-
vant instances from other datasets, such as their
input-output pairs or contexts, based on their
similarities with the given seed data, and then
prompt LLMs to generate new samples with the
contextual information within and across the
original and retrieved samples. This approach
can ensure that the generated data is not only
relevant but also more diverse than what could
be achieved using the limited seed data alone.
We validate our proposed Retrieval-Augmented
Data Augmentation (RADA) framework on
multiple datasets under low-resource settings
of training and test-time data augmentation sce-
narios, on which it outperforms existing LLM-
powered data augmentation baselines.
1 Introduction
Recent advances in language models (Brown et al.,
2020; Touvron et al., 2023; OpenAI, 2023; Anil
et al., 2023), which are trained on general text cor-
pora, have achieved numerous successes across
various natural language tasks. The common prac-
tice to further enhance their performances is to per-
form fine-tuning on task-specific datasets, which
has been proven substantially effective regardless
∗* Equal contribution
of model sizes (Gudibande et al., 2023; Lv et al.,
2023). However, the efficacy of this fine-tuning is
closely tied to the volume and quality of the data
available for training. Meanwhile, in real-world
scenarios, particularly in specific domains, there is
often a scarcity of training instances. For example,
at the beginning of a pandemic such as COVID-19,
there are only a few limited training instances to
fine-tune language models, despite an urgent need
for tasks, such as question answering (Möller et al.,
2020) (Figure 1, (A)). Yet, the manual annotation
of additional training samples is costly and time-
consuming, which may require domain experts.
To address this challenge, various approaches
have been proposed to augment the training data au-
tomatically. These methods typically range from al-
tering the texts of existing training samples (Sahin
and Steedman, 2018; Wei and Zou, 2019) to lever-
aging generative models to produce new instances
for training based on initial seed samples (Yao et al.,
2018; Anaby-Tavor et al., 2020; Lee et al., 2020).
Also, many recent approaches have leveraged the
capability of LLMs for data augmentation based on
prompting, which eliminates the burden of perform-
ing task-specific training (Honovich et al., 2023a;
Whitehouse et al., 2023; Lee et al., 2023). In par-
ticular, Chen et al. (2023a) has utilized the diverse
prompting strategies to create a broader set of in-
stances. However, in low-resource environments
where only a limited number of training instances
are available, generating new data from these mini-
mal seed samples results in poor diversity and vari-
ation (See Figure 1, (B)). We note that a very recent
approach attempts to overcome this by iteratively
including generated samples as seed data for fur-
ther data generation (Wang et al., 2023a). However,
this approach is still ill-suited, which is not only
constrained by the limited diversity of the initial
seed data but also vulnerable to recursively dimin-
ishing the quality of subsequent augmentations due
to the potential low-quality of prior augmentations.
arXiv:2402.13482v1  [cs.CL]  21 Feb 2024



Source: data\tc16_2312.10997v5\referenced_papers\[167]_2309.01431.pdf (Page 0):

Benchmarking Large Language Models in Retrieval-Augmented Generation
Jiawei Chen1,3, Hongyu Lin1,*, Xianpei Han1,2,*, Le Sun1,2
1Chinese Information Processing Laboratory 2State Key Laboratory of Computer Science
Institute of Software, Chinese Academy of Sciences, Beijing, China
3University of Chinese Academy of Sciences, Beijing, China
{jiawei2020,hongyu,xianpei,sunle}@iscas.ac.cn
Abstract
Retrieval-Augmented Generation (RAG) is a promising ap-
proach for mitigating the hallucination of large language
models (LLMs). However, existing research lacks rigorous
evaluation of the impact of retrieval-augmented generation
on different large language models, which make it challeng-
ing to identify the potential bottlenecks in the capabilities
of RAG for different LLMs. In this paper, we systemati-
cally investigate the impact of Retrieval-Augmented Gener-
ation on large language models. We analyze the performance
of different large language models in 4 fundamental abili-
ties required for RAG, including noise robustness, negative
rejection, information integration, and counterfactual robust-
ness. To this end, we establish Retrieval-Augmented Genera-
tion Benchmark (RGB), a new corpus for RAG evaluation in
both English and Chinese. RGB divides the instances within
the benchmark into 4 separate testbeds based on the afore-
mentioned fundamental abilities required to resolve the case.
Then we evaluate 6 representative LLMs on RGB to diag-
nose the challenges of current LLMs when applying RAG.
Evaluation reveals that while LLMs exhibit a certain degree
of noise robustness, they still struggle significantly in terms of
negative rejection, information integration, and dealing with
false information. The aforementioned assessment outcomes
indicate that there is still a considerable journey ahead to ef-
fectively apply RAG to LLMs.
Introduction
Recently, there have been impressive advancements in large
language models (LLMs) like ChatGPT (OpenAI 2022) and
ChatGLM (THUDM 2023a). Although these models have
shown remarkable general abilities (Bang et al. 2023; Guo
et al. 2023), they still suffer severely from challenges includ-
ing factual hallucination (Cao et al. 2020; Raunak, Menezes,
and Junczys-Dowmunt 2021; Ji et al. 2023), knowledge out-
dating (He, Zhang, and Roth 2022), and the lack of domain-
specific expertise (Li et al. 2023c; Shen et al. 2023).
Incorporating external knowledge via information re-
trieval, i.e., Retrieval-Augmented Generation (RAG), has
been regarded as a promising way to resolve the above chal-
lenges. (Guu et al. 2020; Lewis et al. 2020; Borgeaud et al.
* Corresponding authors.
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
Noise Robustness Negative Rejection
Who was awarded the 2022 Nobel prize in 
literature?
The Nobel Prize in Literature for 2022 is 
awarded to the French author Annie Ernaux, 
“for the courage and clinical acuity …
The Nobel Prize in Literature for 2021 is 
awarded to the novelist Abdulrazak Gurnah, 
born in Zanzibar and active in …
Annie Ernaux
Question
External documents contain noises
Retrieval Augmented 
Generation
Who was awarded the 2022 Nobel prize in 
literature?
The Nobel Prize in Literature for 2021 is 
awarded to the novelist Abdulrazak Gurnah, 
born in Zanzibar and active in …
The 2020 Nobel Laureate in Literature, 
poet Louise Glück, has written both poetry 
and essays about poetry. Since her…
I can not answer the question because of the 
insufficient information in documents
Question
External documents are all noises
Information Integration
When were the ChatGPT app for iOS and 
ChatGPT api launched?
On May 18th, 2023, OpenAI introduced its 
own ChatGPT app for iOS…
That changed on March 1, when OpenAI 
announced the release of API access to 
ChatGPT and Whisper,…
May 18 and March 1.
Question
External documents contain all answers
Retrieval Augmented 
Generation
Counterfactual Robustness
Which city hosted the Olympic games in 
2004?
The 2004 Olympic Games returned home to 
New York, birthplace of the … 
After leading all voting rounds, New York
easily defeated Rome in the fifth and 
final vote …
There are factual errors in the provided 
documents. The answer should be Athens. 
Question
Counterfactual external documents
Retrieval Augmented 
Generation
Retrieval Augmented 
Generation
Figure 1: Illustration of 4 kinds of abilities required for
retrieval-augmented generation of LLMs.
2022; Izacard et al. 2022). With the help of external knowl-
edge, LLMs can generate more accurate and reliable re-
sponses. The most common method is to use a search engine
as a retriever such as New Bing. Due to the vast amount of
information available on the Internet, using a search engine
can provide more real-time information.
However, Retrieval-Augmented Generation brings not
only positive effects to LLMs (Liu, Zhang, and Liang 2023;
Maynez et al. 2020). On one hand, there is a significant
amount of noise information even fake news in the content
available on the Internet, which poses challenges for search
engines in accurately retrieving desirable knowledge. On the
other hand, LLMs suffer from unreliable generation chal-
lenge. LLMs can be misled by incorrect information con-
tained in the context (Bian et al. 2023) and also suffer from
hallucination during the generation (Adlakha et al. 2023),
resulting in generating content that goes beyond external in-
arXiv:2309.01431v2  [cs.CL]  20 Dec 2023



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 13):

Method Timing of Using Knowledge Source Application Task
WebGPT (Nakano et al., 2021) Generation-Time Search API QA
Adaptive-Retrieval (Mallen et al., 2023) Generation-Time Wikipedia QA
ReACT (Yao et al., 2022) Generation-Time Wikipedia QA & FV
RETRO (Borgeaud et al., 2022) Generation-Time Unstructured Corpus LM & QA
Chain-of-Knowledge (Li et al., 2023d) Generation-Time Structured Knowledge Base QA & FV & Decision
RARR (Gao et al., 2023a) Post-Processing Search API QA
Verify-then-Edit (Zhao et al., 2023b) Post-Processing Wikipedia, Search API, etc QA
LLM-Augmenter (Peng et al., 2023a) Post-Processing Web documents, Databases QA
REFEED (Yu et al., 2023b) Post-Processing Wikipedia QA, Dialogue
CRITIC (Gou et al., 2023) Post-Processing Search API, Code Executor, Calculator, etc QA & Program & Toxicity
FacTool (Chern et al., 2023) Post-Processing Search API, Code Executor, Calculator, etc QA & Reasoning & Generation
Table 10: A summary of some recent studies on resorting to external knowledge to mitigate hallucinations. We use
abbreviations for some application task names, including QA (Question Answering), FV (Fact Verification), and
LM (Language Modeling).
ically yield more accurate facts than those pre-
sented in long-form answers. The C OVE frame-
work initially plans verification questions, and
then answers these questions to ultimately produce
an enhanced, revised response. Experimental re-
sults on list-based questions, closed book QA, and
long-form text generation demonstrate that COVE
can effectively mitigate hallucination.
Another work, Li et al. (2023b), introduces a
novel Inference-Time Intervention (ITI) method to
improve the truthfulness of LLMs. This method is
based on the assumption that LLMs possess latent,
interpretable sub-structures associated with factu-
ality. The ITI method comprises two steps: 1)
fitting a binary classifier on top of each attention
head of the LLM to identify a set of heads that ex-
hibit superior linear probing accuracy for answer-
ing factual questions, and 2) shifting model activa-
tions along these factuality-related directions dur-
ing inference. The ITI method leads to a substan-
tial performance improvement on the TruthfulQA
benchmark (Lin et al., 2021).
Distinct from the aforementioned studies, Shi
et al. (2023b) instead concentrates on the retrieval-
augmentation setting. Prior research has shown
that LLMs sometimes fail to adequately attend to
retrieved knowledge when addressing downstream
tasks, particularly when the retrieved knowl-
edge conflicts with the parametric knowledge of
LLMs (Zhou et al., 2023b; Xie et al., 2023).
To address this issue, Shi et al. (2023b) propose
a straightforward context-aware decoding (CAD)
strategy. The core idea of CAD is to perform
a contrastive ensemble of pθ(yt | x, c, y<t) and
pθ(yt | x, y<t), where θ represents the LM,x is the
input query, c is the context, y is the response, and
t is the time step. pθ(yt | x, c, y<t) means the gen-
eration probability distribution of t-th token when
given the context whilepθ(yt | x, y<t) denotes the
distribution only considering the query. The CAD
method aims to compel LLMs to pay more at-
tention to contextual information instead of over-
relying their own parametric knowledge to make
decisions. Experimental results show that CAD
effectively elicits the ability of LLMs to exploit
retrieved knowledge and thus reduces factual hal-
lucinations on downstream tasks. Another work,
DoLA (Chuang et al., 2023), also employ the idea
of contrastive decoding to reduce hallucination.
However, they contrast the generation probabili-
ties from different layers of LLMs, as they find
that linguistic and factual information is encoded
in distinct sets of layers.
Summary & Discussion. Designing decoding
strategies to mitigate hallucinations in LLMs dur-
ing inference is typically in a plug-and-play man-
ner. Therefore, this method is easy to deploy, mak-
ing it promising for practical applications. How-
ever, for this approach, most existing works re-
quire accessing the token-level output probabili-
ties, while a substantial number of current LLMs
can only return generated content through lim-
ited APIs (e.g., ChatGPT). Consequently, we en-
courage future research in this direction to explore
within a more strict black-box setting.
5.4.2 Resorting to External Knowledge
Using external knowledge as supplementary ev-
idence to assist LLMs in providing truthful re-
sponses recently represents a burgeoning solution
(Ren et al., 2023; Mialon et al., 2023). This ap-
proach typically consists of two steps. The first
step entails accurately obtaining knowledge re-
lated to the user instructions. Once useful knowl-
edge has been achieved, the second step involves
14



Source: data\tc16_2312.10997v5\referenced_papers\[24]_2305.06983.pdf (Page 0):

Active Retrieval Augmented Generation
Zhengbao Jiang1∗ Frank F. Xu1∗ Luyu Gao1∗ Zhiqing Sun1∗ Qian Liu2
Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1
1Language Technologies Institute, Carnegie Mellon University
2Sea AI Lab 3FAIR, Meta
{zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu
Abstract
Despite the remarkable ability of large lan-
guage models (LMs) to comprehend and gen-
erate language, they have a tendency to hal-
lucinate and create factually inaccurate out-
put. Augmenting LMs by retrieving informa-
tion from external knowledge resources is one
promising solution. Most existing retrieval aug-
mented LMs employ a retrieve-and-generate
setup that only retrieves information once based
on the input. This is limiting, however, in
more general scenarios involving generation
of long texts, where continually gathering in-
formation throughout generation is essential. In
this work, we provide a generalized view of ac-
tive retrieval augmented generation, methods
that actively decide when and what to retrieve
across the course of the generation. We propose
Forward-Looking Active REtrieval augmented
generation (FLARE), a generic method which
iteratively uses a prediction of the upcoming
sentence to anticipate future content, which is
then utilized as a query to retrieve relevant doc-
uments to regenerate the sentence if it contains
low-confidence tokens. We test FLARE along
with baselines comprehensively over 4 long-
form knowledge-intensive generation tasks/-
datasets. FLARE achieves superior or compet-
itive performance on all tasks, demonstrating
the effectiveness of our method.1
1 Introduction
Generative language models (LMs) (Brown et al.,
2020; Ouyang et al., 2022; OpenAI, 2023; Chowd-
hery et al., 2022; Zhang et al., 2022; Touvron et al.,
2023; Zhao et al., 2023) have become a founda-
tional component in natural language processing
(NLP) systems with their remarkable abilities. Al-
though LMs have memorized some world knowl-
edge during training (Petroni et al., 2019; Roberts
et al., 2020; Jiang et al., 2020), they still tend to
∗Lead contributors.
1Code and datasets are available athttps://github.com/
jzbjyb/FLARE.
hallucinate and create imaginary content (Maynez
et al., 2020; Zhou et al., 2021). Augmenting LMs
with retrieval components that look up relevant in-
formation from external knowledge resources is a
promising direction to address hallucination (Khan-
delwal et al., 2020; Izacard et al., 2022).
Retrieval augmented LMs commonly use a
retrieve-and-generate setup where they retrieve doc-
uments based on the user’s input, and then generate
a complete answer conditioning on the retrieved
documents (Chen et al., 2017; Guu et al., 2020;
Lewis et al., 2020; Izacard and Grave, 2021; Sachan
et al., 2021; Lee et al., 2021; Jiang et al., 2022;
Izacard et al., 2022; Nakano et al., 2021; Qian
et al., 2023; Lazaridou et al., 2022; Shi et al., 2023).
These single-time retrieval augmented LMs outper-
form purely parametric LMs, particularly for short-
form knowledge-intensive generation tasks such
as factoid question answering (QA) (Kwiatkowski
et al., 2019; Joshi et al., 2017), where the informa-
tion needs are clear in the user’s input, and it is
sufficient to retrieve relevant knowledge once solely
based on the input.
Increasingly powerful large LMs have also
demonstrated abilities in more complex tasks that
involve generating long-form output, such as long-
form QA (Fan et al., 2019; Stelmakh et al., 2022),
open-domain summarization (Cohen et al., 2021;
Hayashi et al., 2021; Giorgi et al., 2022), and
(chain-of-thought; CoT) reasoning (Wei et al.,
2022; Ho et al., 2020; Geva et al., 2021; Hendrycks
et al., 2020). In contrast to short-form generation,
long-form generation presents complex informa-
tion needs that are not always evident from the in-
put alone. Similar to how humans gradually gather
information as we create content such as papers,
essays, or books, long-form generation with LMs
would require gathering multiple pieces of knowl-
edge throughout the generation process . For ex-
ample, to generate a summary about a particular
topic, the initial retrieval based on the topic name
arXiv:2305.06983v2  [cs.CL]  22 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[67]_2401.15884.pdf (Page 0):

Corrective Retrieval Augmented Generation
Shi-Qi Yan1*, Jia-Chen Gu2*, Yun Zhu3, Zhen-Hua Ling1
1National Engineering Research Center of Speech and Language Information Processing,
University of Science and Technology of China, Hefei, China
2Department of Computer Science, University of California, Los Angeles
3Google DeepMind
yansiki@mail.ustc.edu.cn, gujc@ucla.edu, yunzhu@google.com, zhling@ustc.edu.cn
Abstract
Large language models (LLMs) inevitably
exhibit hallucinations since the accuracy of
generated texts cannot be secured solely by
the parametric knowledge they encapsulate. Al-
though retrieval-augmented generation (RAG)
is a practicable complement to LLMs, it relies
heavily on the relevance of retrieved docu-
ments, raising concerns about how the model
behaves if retrieval goes wrong. To this end, we
propose the Corrective Retrieval Augmented
Generation (CRAG ) to improve the robustness
of generation. Specifically, a lightweight
retrieval evaluator is designed to assess the
overall quality of retrieved documents for a
query, returning a confidence degree based
on which different knowledge retrieval ac-
tions can be triggered. Since retrieval from
static and limited corpora can only return sub-
optimal documents, large-scale web searches
are utilized as an extension for augmenting the
retrieval results. Besides, a decompose-then-
recompose algorithm is designed for retrieved
documents to selectively focus on key infor-
mation and filter out irrelevant information in
them. CRAG is plug-and-play and can be
seamlessly coupled with various RAG-based
approaches. Experiments on four datasets
covering short- and long-form generation tasks
show that CRAG can significantly improve the
performance of RAG-based approaches. 1
1 Introduction
Large language models (LLMs) have attracted
increasing attention and exhibited impressive abili-
ties to understand instructions and generate fluent
language texts (Brown et al., 2020; Ouyang et al.,
2022; Touvron et al., 2023a). Nevertheless, LLMs
inevitably manifest hallucinations (Ji et al., 2023)
due to their struggle with factual errors (Mallen
et al., 2023; Min et al., 2023) and inability to
secure the accuracy of generated texts solely by
* Equal contribution.
1The code is available at github.com/HuskyInSalt/CRAG
Q: What is Henry Feilden's occupation?
Henry Feilden (Conservative politician):Henry Master Feilden was an Conservative Party politician…
Politician.✓
Q: Who was the screenwriter for Death of a Batman?
Batman (1989 film): of the murder of Bruce Wayne's parents. When Hamm'sscript was rewritten, …
Retriever
✗Hamm.
RetrievedDocuments
Generator
Accurate DocumentsInaccurate Documents
Generator
Figure 1: The examples show that a low-quality retriever
is prone to introducing a substantial amount of irrelevant
information, impeding the generators from acquiring
accurate knowledge and potentially misleading them.
the parametric knowledge they encapsulate (Zhang
et al., 2023b; Muhlgay et al., 2023).
Prior research has introduced the retrieval tech-
niques to incorporate the knowledge relevant to
input and augment generation, as exemplified
by retrieval-augmented generation (RAG) (Lewis
et al., 2020). In this framework, the input to models
is augmented by prepending relevant documents
that are retrieved from an external knowledge
corpus (Guu et al., 2020). While RAG serves as a
practicable complement to LLMs, its effectiveness
is contingent upon the relevance and accuracy of
the retrieved documents (Li et al., 2022; Tan et al.,
2022). The heavy reliance of generation on the
retrieved knowledge raises significant concerns
about the model’s behavior and performance in
scenarios where retrieval may fail or return inaccu-
rate results (Shi et al., 2023). As Figure 1 shows
that a low-quality retriever is prone to introducing
arXiv:2401.15884v3  [cs.CL]  7 Oct 2024



### Claim 98/179

#### Claim Text
CL is oriented in such a way that it gives the Fourier transform of only y-direction, without any effect on the x-direction . 4 IV.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[77]_2305.18846.pdf (Page 15):

Table 8: A list of notations that we used for defining our method.
V pre-defined vocabulary of tokens for pre-trained language models (text)
E pre-defined vocabulary of entities (symbol)
R pre-defined vocabulary of relations (symbol)
a, . . .z knowledge graph symbols written in typewrite font
x input sequence (vector)
x1, . . . , xN input tokens (scalar)
y = [y1, . . . , yT ] output sequence and tokens
G multi-relational graph, such as knowledge graph
Z retrieved subgraph: Z ⊂ G
z triplet (edge): z ∈ Z
q tokenization (mapping) function of KG symbol to the sequence of tokens
s(·) text representation function for retrieval
d(·) triplet representation function for retrieval
Enc Transformer Encoder
Dec Transformer Decoder
f token (word) embedding function
θ generator parameter
ϕ retriever parameter
ψ set encoding function
β perturbation function
π set permutation
n the number of triplets in a retrieved subgraph Z
k the number of samples in a marginalization term
z encoder hidden state (single token)
Z encoder hidden states (sequence of tokens)
h decoder hidden state (single token)
H decoder hidden states (sequence of tokens)
X input embeddings after token embedding function (sequence)
Y output embeddings after token embedding function (sequence)
With a naïve encoding, the PLM yields different representations for different orders of triplets in
the subgraph. Therefore, if the PLM is only fine-tuned with the input [A, born-in, C, B, born-in,
D, where was A born?], there is no guarantee that the PLM will output the exact same response
given the input with a permuted subgraph [B, born-in, D, A, born-in, C, where was A born?] in the
inference since the PLM is order-sensitive due to its positional encoding. In order to prevent the
aforementioned scenarios, we decide to design the permutation-invariant graph encoding which yields
stable results regardless of the order of triplets in the graph. Similarly, the inversion of the triplet
yields the same semantic (e.g., {(a, born-in, c)} = {(c, ¬born-in, a)}), but the graph encoding
without considerations for the inverse relation results in different representations from PLM given the
triplet and its inversed one.
D.2 Proofs
In this section, we first show that a naïve encoding function ψ in Section 3.4 is neither permutation
invariant nor relation inversion invariant, formalized in Proposition D.1. After that, we prove that our
invariant and efficient encoding function ψ∗ with graph-conditioned token embedding perturbation is
both permutation invariant and relation inversion invariant, formalized in Proposition D.2.
Proposition D.1. A naïve encoding function ψ is neither permutation invariant nor relation inversion
invariant.
Proof. We prove this by contradiction.
Suppose x = [ x1, . . . , xn] and Z = {(a, r1, b), (b, r2, a), (a, r1, c)}. Moreover, let Z′ =
{(b, r2, a), (a, r1, b), (a, r1, c)} be one of permutations of Z with the permutation order π =
(2, 1, 3).
16



Source: data\tc16_2312.10997v5\referenced_papers\[172]_2309.17453.pdf (Page 3):

Published as a conference paper at ICLR 2024
Dense Attention Window Attention Sliding Window 
w/ Re-computation StreamingLLM
Dense Attention Window Attention StreamingLLM
Figure 3: Language modeling perplexity on texts with 20K tokens across various LLM. Observations reveal
consistent trends: (1) Dense attention fails once the input length surpasses the pre-training attention window
size. (2) Window attention collapses once the input length exceeds the cache size, i.e., the initial tokens are
evicted. (3) StreamingLLM demonstrates stable performance, with its perplexity nearly matching that of the
sliding window with re-computation baseline.
Improving LLMs’ Utilization of Long Textoptimizes LLMs to better capture and employ the
content within the context rather than merely taking them as inputs. As highlighted by Liu et al.
and Li et al., success in the previously mentioned two directions does not necessarily translate to
competent utilization of lengthy contexts. Addressing this effective usage of prolonged contexts
within LLMs is still a challenge. Our work concentrates on stably harnessing the most recent tokens,
enabling the seamless streaming application of LLMs.
3 S TREAMING LLM
3.1 T HE FAILURE OF WINDOW ATTENTION AND ATTENTION SINKS
While the window attention technique offers efficiency during inference, it results in an exceedingly
high language modeling perplexity. Consequently, the model’s performance is unsuitable for deploy-
ment in streaming applications. In this section, we use the concept of attention sink to explain the
failure of window attention, serving as the inspiration behind StreamingLLM.
Identifying the Point of Perplexity Surge.Figure 3 shows the perplexity of language modeling on
a 20K token text. It is evident that perplexity spikes when the text length surpasses the cache size, led
by the exclusion of initial tokens. This suggests that the initial tokens, regardless of their distance
from the predicted tokens, are crucial for maintaining the stability of LLMs.
Why do LLMs break when removinginitial tokens’ KV? We visualize attention maps from
all layers and heads of the Llama-2-7B and models in Figure 2. We find that, beyond the bottom
two layers, the model consistently focuses on the initial tokens across all layers and heads. The
implication is clear: removing these initial tokens’ KV will remove a considerable portion of the
denominator in the SoftMax function (Equation 1) in attention computation. This alteration leads to a
significant shift in the distribution of attention scores away from what would be expected in normal
inference settings.
SoftMax(x)i = exi
ex1 + PN
j=2 exj
, x 1 ≫ xj, j∈ 2, . . . , N (1)
There are two possible explanations for the importance of the initial tokens in language modeling:
(1) Either their semantics are crucial, or (2) the model learns a bias towards their absolute position.
To distinguish between these possibilities, we conduct experiments (Table 1), wherein the first four
tokens are substituted with the linebreak token “\n". The observations indicate that the model still
significantly emphasizes these initial linebreak tokens. Furthermore, reintroducing them restores
the language modeling perplexity to levels comparable to having the original initial tokens. This
suggests that the absolute position of the starting tokens, rather than their semantic value, holds
greater significance.
LLMs attend to Initial Tokens as Attention Sinks.To explain why the model disproportionately
focuses on initial tokens—regardless of their semantic relevance to language modeling, we introduce
the concept of “attention sink". The nature of the SoftMax function (Equation 1) prevents all attended
tokens from having zero values. This requires aggregating some information from other tokens across
all heads in all layers, even if the current embedding has sufficient self-contained information for its
prediction. Consequently, the model tends to dump unnecessary attention values to specific tokens. A
similar observation has been made in the realm of quantization outliers (Xiao et al., 2023; Bondarenko
et al., 2023), leading to the proposal of SoftMax-Off-by-One (Miller, 2023) as a potential remedy.
4



Source: data\tc16_2312.10997v5\referenced_papers\[19]_2311.06595.pdf (Page 1):

!"#$డ&క(అదృష-వంత12ో#ా5ో!"క(6ె8యదు, అతనుజ=రం#ారణం@ాAాఠCాలక(EFళHలIకAJ యKడ&?(I don't know if my son is lucky or unlucky, he couldn't go to school because of fever.)
Telugu input:
Cross-Lingual Retriever
I tried the new restaurant in town and the food was amazing, but the service was terrible. I don't think I'll be going back anytime soon.
Top k retrievedEnglishsamples:
What isthesentimentforthetext: Negative, Neutral, orPositive?______eni
SelfPrediction
MPLM
Consider the following English examples:Example: I tried the new restaurant in town and the food was amazing, but the service was terrible. I don't think I'll be going back anytime soon.Sentiment: Negative…Example: My best friend surprised me with tickets to my favorite band's concert and we had the best time ever! I can't thank her enough. Sentiment: PositiveWhat is a possible sentiment for theTelugutext:!"#$డ&క(అదృష-వంత12ో#ా5ో!"క(6ె8యదు, అతనుజ=రం#ారణం@ాAాఠCాలక(EFళHలIకAJ యKడ&?Negative,Neutral,orPositive?______
ConsiderthefollowingEnglishexamples:Text:Sentiment:enieni
WhatisthesentimentfortheTelugu text:Negative,Neutral,orPositive?______te
Prompt Engineering
Negative
MPLM
Neutral
Figure 1: Detailed overview of the CREA-ICL pipeline for LRLs: (a) An LRL input is used as a
query for the cross-lingual retriever, which then retrieves the most semantically similar HRL sample
from the HRL corpus. The associated label is either taken directly from the corpus (labeled setting)
or determined by self-prediction (unlabeled setting). (b) Next, this HRL sample, its label, and the
original input are combined to create a retrieval-augmented input for MPLM to make prediction.
forward in multilingual QA models, demonstrating a many-to-many approach that avoids language-
specific data and retrieval modules, which is particularly beneficial for low-resource languages.In
contrast, strategies like PARC [Nie et al., 2023] propose a more comprehensive methodology by
obtaining semantically aligned instructions from high-resource languages.
Building on these methods, our work introduces novel perspectives and aims to bridge gaps. While
MEGA provides task-centric instructions, we integrate deeper semantic understanding. We embrace
a cross-lingual approach akin to PARC, as depicted in Figure 1. In contrast to PARC’s focus on
masked language models like mBERT and XLMR, we explore the potential of larger, decoder-
only multilingual pretrained language models (MPLMs) - BLOOM and BLOOMZ. Our focus is
on addressing both classification and generation tasks in a cohesive generative style, emphasizing
instruction execution [Muennighoff et al., 2023, Scao et al., 2022].
This paper explores the application of cross-lingual retrieval-augmented ICL (CREA-ICL) to a
specific low-resourced case, the Bangla language-covering text classification and generation tasks.
We prioritize the effective execution of instructions. Our main contributions are:
• A comprehensive evaluation of cross-lingual retrieval augmented ICL, highlighting consis-
tent improvements over MPLMs’ zero-shot performance on Bangla classification tasks.
• An in-depth analysis revealing the challenges in Bangla generation task, providing insights
into the performance dynamics in both the classification and generation domains.
• A pioneering exploration to adapt PARC for generative models, BLOOM and BLOOMZ,
providing insights for a unified pipeline of CREA-ICL.
2



Source: data\tc16_2312.10997v5\referenced_papers\[178]_2210.03765.pdf (Page 2):

Input Context x  
A man is seen skiing behind a boat. He holds on tight as he is pulled through the water. The man …
Target :  is water skiing until the end of the clip.y
Prediction : then moves to the side and begins to swim./uni0302 y
AE Decoder
AE Encoder
CLIP 
Visual 
Encoder
...Mapping 
Network
... Language 
Model
Projection 
Layer
Machine Imagination I 
v 
 /uni0302 t
c1 c2 cl t1 t2 tm 
Text Emb.Visual Prefix
Input Visual Feature
L
"
acher
L
con
#
as
!
ve
Predicted Text Feature
Text-to-Image 
Generation
Visually-Guided Text Generation
Diffusion 
Model
Text Encoder
randomly 
initialized 
image
Figure 2: An overview of our iNLG. Given an input contextx, we ﬁrst visualize the context with the text-to-image
generation model. Then we use the machine-generated image I as the additional visual supervision to guide the
language model in open-ended text generation. The visual feature is provided as a source of input to the LM in the
form of the visual preﬁx. Aside from the teacher forcing objective Lteacher, we also enforce the LM to generate text
that is semantically similar to the machine imagination with a contrastive training objective Lcontrastive.
3 Method
3.1 Overview
Open-ended text generation is a task that provides
an input context, and asks the model to generate a
piece of text that is consistent with the context.
This work mainly focused on introducing
machine-rendered images to assist LM in perform-
ing open-ended text generation. More speciﬁcally,
given the context xi, we ﬁrst use a text-to-image
generator to illustrate an image Ii that depicts the
input context. The LM is prompted with image
Ii as the visual preﬁx along with the text context
xi, and will incorporate the multimodal input to
generate the output text ˆyi.
Figure 2 provides an overview of our iNLG
framework, which mainly involves two modules.
The ﬁrst module is a text-to-image generator that
takes in the input context and illustrates a descrip-
tive image, which we also refer to as the machine
imagination. The second module is a visually-
guided language model that utilizes the machine
imagination as a source of input and also a supervi-
sion that encourages the LM to generate text that is
semantically similar to the visual information.
3.2 Text-to-Image Rendering
In this work, we propose to use images gener-
ated conditioning on the context by the machines
as additional visual information to the LM. The
text-to-image generation backbone is StableDiffu-
sion (Rombach et al., 2022), which mainly consists
of a text encoder, a diffusion model, and an au-
toencoder. The text encoder is from the frozen
CLIP ViT-L/14 (Radford et al., 2021) and encodes
the input text to textual embeddings. The diffu-
sion model uses UNet (Ronneberger et al., 2015)
to provide noise estimation. The UNet is modi-
ﬁed so as to attend to the input textual embeddings.
The encoder of the pretrained autoencoder encodes
images into the lower-resolution latent maps zT.
At each step t, the diffusion model provides the
noise estimation ϵand modiﬁes zt correspondingly.
The decoder of the pretrained autoencoder takes
the ﬁnal noise-free latent map z and generates the
image prediction. StableDiffusion is trained with
LAION-5B (Schuhmann et al., 2022).
3.3 Visually Guided Text Generation
Visual Preﬁx Construction One can encode the
visual information with the pre-trained visual mod-
els. However, such visual embedding may lie in a
representation space different from the LM due to
the discrepancy between models. One way of intro-
ducing features extracted by another network to the
current model is through feature mapping (Mokady
et al., 2021). With a dataset of image-text pairs
pI1,x1q, we can pre-train a mapping network F for
a given LM in an image captioning formulation.
More speciﬁcally, we encode I1 with the visual
encoder Encvisual and receive its visual features v1.
Then we apply the mapping network F over v1,
and receive a sequence of lvisual preﬁxes:
c1
1,c1
2,...,c 1
l “Fpv1q“ FpEncvisualpI1qq (1)



### Claim 99/179

#### Claim Text
In our work, imitation dynamics is utilized to study the evolution of individual strategies.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[74]_2401.13256.pdf (Page 9):

10
than unsupervised methods except the BLEU-1 and Rouge-L
of unsupervised SAFARI on DuLeMon dataset. We carefully
check the outputs of unsupervised SAFARI and find that it tends
to plan to use source of knowledge ( 70% using bothUser-Per
and Bot-Per) while most of original test samples do not require
any sources of knowledge, resulting in extremely low P.C and
higher BLEU-1 and Rouge-L. Furthermore, it is evident that
SAFARI outperforms FoCus. We emphasise that FoCus treats
knowledge selection as a classification task and optimizes it
jointly with response generation tasks, leading to efficiency and
scalability issues compared with SAFARI and UniMS-RAG.
2) The performance of UniMS-RAG: Referring to Tabel III
and Table IV, the performance of the planning step and retrieval
step largely affects the results in the final generation step.
Specifically, when using itself as retriever, we can find that
UniMS-RAG using signals from DPR (w/ DPR) leads to better
performance in contrast to ChatGPT (w/ ChatGPT) or GPT4o (w
/ GPT4o), revealing the effectiveness of better retriever signals.
The results of GPT4o is also slightly better than ChatGPT due
to more accurate similarity scores no matter using itself as
retriever or using independent retriever. The gap between DPR
and ChatGPT on DuLeMon is relatively small since most of
cases here do not require the involvement of external sources
of knowledge. Thus, we decide to load parameters of UniMS-
RAG w/ DPR to conduct evaluation when using independent
retriever. In detail, the results again validate the effectiveness
of better retriever (w/ DPR > w/ GPT4o > w/ ChatGPT > w/
BM25). We find that the performance gap between different
retrievers in DuLeMon dataset is pretty small (less than 1%).
We suspect this is highly related to the original data distribution
in the test dataset, since most of the samples do not require
external persona information. Furthermore, we also find that
using independent retriever is mostly better than using itself as
retriever except the worst BM25, which is consistent with the
findings in the performance of retrieval step. To conclude, it
is obvious that our proposed method UniMS-RAG outperforms
all other baselines in at least 5 out 7 evaluation metrics no
matter using itself as retriever or using independent retriever.
Combining the performance of planning, retrieval, and response
generation together, we can find that UniMS-RAG is capable of
serving as a planner, retriever, and reader in a unified manner,
leading to better performance in personalized dialogues.
3) Discussion of Unified Manner: Despite the clear perfor-
mance boost from using independent existing retrievers, we
emphasize that the performance gain is not solely due to the
better relevance scores provided by these retrievers. It is also
significantly attributed to our proposed UniMS-RAG, especially
the introduction of relevance score prediction task which makes
it can attention on relevant evidences while overlooking noisy
ones. This conclusion based on two observations: 1) simply
using the existing retriever based on SAFARI cannot achieve
state-of-the-art results. We have already selected the best-
performing version of SAFARI, and all variants of our proposed
UniMS-RAG (i.e., using independent retriever and using itself as
retriever) largely outperform it, 2) The performance gap between
variants of our proposed method is relatively smaller than the
gap between ours with baselines, revealing the great potential
and flexibility of UniMS-RAG. In practice, we advocate for the
BLEU-1
ROUGE-L P .C
18.43
18.78
20.32
20.84
63.18
63.96
Number=1
Number=2
Number=3
(a) DuLeMon
BLEU-1
ROUGE-L
P .C
K.C
29.65
33.5
32.4837.33
75.51
79.9
42.07 53.46
Number=1
Number=2
Number=3 (b) KBP
Fig. 5: The performance of Generation with different number of
retrieved evidences on two datasets.
unified training of these three sub-tasks in PerDS, as depicted in
UniMS-RAG. During inference, the retriever should be selected
on a case-by-case basis.
VII. Analysis and Discussions
In this section, we choose the best model as shown in our
previous experiments to investigate the performance changes
under different settings (UniMS-RAG w/ DPR). In detail,
we start from the performance of our proposed model with
different numbers of retrieved evidence (§VII-A), then we study
the effects of introduced self-refinement mechanism during
the inference stage by re-evaluating the relationship between
generated response with the dialogue context or retrieved
evidence (§VII-B). We present the ablation study to show the
effectiveness and rationale of UniMS-RAG (§VII-B), followed
by the results of human evaluation.
A. Different Numbers of Retrieved Results
The number of retrieved results plays a key role in the response
generation. Striking a balance between accuracy and recall is
essential, as too few results may miss important semantics, while
too many can introduce noise. Figure 5 shows the performance
of UniMS-RAG under different numbers of retrieved results.
In DuLeMon dataset, we observe a slight improvement in
performance as the number of retrieved results increases. This
improvement is likely attributed to infrequent cases requiring
evidence in the original test dataset. On the other hand, in
KBP dataset, it is obvious that the performance get notably
improvement when the number of retrieved evidence increases
from 1 to 2, but then experiences a slight decline upon further
increase to 3. We hypothesize that this drop is a result of
additional noise introduced as the number of retrieved evidence
continues to increase. This suggests a delicate balance must
be struck to optimize performance, considering the specific
characteristics of each dataset.
B. Additional Self-refinement during Inference
To investigate the effects of self-refinement during the infer-
ences, we set the number of retrieved evidence as 3 according
to the experimental results in the previous section since it



Source: data\tc16_2312.10997v5\referenced_papers\[59]_2310.05149.pdf (Page 3):

Table 1: Exact match performance on single-hop question answering. All ITRG results are from the last iteration (T = 5).
Method Natural Questions TriviaQA
0-shot 1-shot 5-shot 0-shot 1-shot 5-shot
GPT 3.5 Text-davinci-002 12.0 24.6 33.0 46.0 74.2 76.0
Text-davinci-003 29.4 33.0 33.8 75.8 78.6 77.8
LLaMA 33B
Vanilla LM 27.0 29.4 32.4 74.8 70.8 75.8
Retrieve-then-Read 27.8 30.6 29.8 74.6 76.0 76.0
Generate-then-Read 28.0 31.4 31.0 73.6 77.2 77.6
ITRG (refine) 34.4 34.6 34.8 79.0 79.4 80.6
ITRG (refresh) 37.6 38.4 38.0 77.0 78.6 79.4
Table 2: Exact match performance on multi-hop question answering. All ITRG results are from the last iteration (T = 5).
Method 2WikiMultiHopQA HotpotQA
0-shot 1-shot 5-shot 0-shot 1-shot 5-shot
GPT 3.5 Text-davinci-002 16.4 27.6 30.8 12.2 20.2 22.2
Text-davinci-003 27.2 27.0 29.8 25.0 25.8 26.6
LLaMA 33B
Vanilla LM 24.4 27.6 31.8 22.6 25.0 27.0
COT - - 32.2 - - 28.6
Retrieve-then-Read 27.4 29.2 32.0 28.4 29.8 30.4
Generate-then-Read 30.0 30.4 31.6 25.0 27.0 27.0
ITRG (refine) 33.0 33.6 37.0 28.8 29.6 30.6
ITRG (refresh) 32.2 36.2 38.6 31.0 32.6 33.4
Table 3: Exact match performance of ITRG (refresh) at dif-
ferent iterations in 5-shot setting.
Iteration 1 2 3 4 5
Natural Questions 34.0 35.2 37.0 37.2 38.0
TriviaQA 79.8 79.2 79.8 79.8 79.4
2WikiMultiHopQA 34.8 37.4 37.2 38.6 38.6
HotpotQA 32.6 32.8 34.0 33.4 33.4
and improves by 6.8 points in absolute gains. Compared to
vanilla LM, ITRG (refresh) can improve the EM score by 9.4,
7.6, and 6.4 points respectively in 0-shot, 1-shot, and 5-shot
settings on the Hotpotqa dataset.
4.2. Performance at Different Iterations
In this section, we analyze the performance of our model and
the quality of the generated documents during the iteration
process. Specifically, we present the results of ITRG (refresh)
at different iterations in 5-shot setting in Table 3. We measure
the answer recall of generated documents at different itera-
tion steps and present results in Table 4. Table 3 shows that
the performance of the model gradually improves with iter-
ation. And Table 4 shows that the quality of the generated
documents also gradually improves with iteration. These re-
sults verify that our iterative retrieval-generation collaborative
Table 4: Answer recall of generated documents at different
iterations with ITRG (refresh).
Iteration 1 2 3 4 5
Natural Questions 44.0 46.4 48.4 48.8 48.0
TriviaQA 18.8 19.0 20.2 19.2 19.2
2WikiMultiHopQA 34.2 36.6 35.0 40.0 37.0
HotpotQA 34.2 34.8 35.6 33.8 33.6
framework is effective and can further enhance the reasoning
capabilities of large language models.
5. CONCLUSION
In this paper, we present ITRG, which is an iterative retrieval-
generation synergy framework, containing two important
steps: generation-augmented retrieval and retrieval-augmented
generation. They form a closed loop, and can improve
each other via multiple iterations. We propose a simple
and effective generation-augmented retrieval strategy and
two retrieval-augmented generation strategies. Empirical re-
sults show our approach significantly exceeds several strong
baselines, including GPT 3.5, on four open domain ques-
tion answering datasets, which indicates that our method can
significantly improve the reasoning ability of large language
models.



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 7):

size of the model increases. Furthermore, we find that more similar translation demonstrations
significantly enhance performance across all model sizes (from random, kNN to Self). This suggests
that demonstration examples in in-context learning not only act as triggers for model ability but also
adhere to the primal problem, where better demonstration example leads to better generation. Also,
by comparing the results in Table 2 and Table 4, we can conclude that the cross-lingual LLM with
designed examples still falls short of the supervised baselines in this task.
5.2 Summarization
In this paper, we compare the performance of our trainable model with those of REINA [ 87],
PEGASUS [100], and BART [40]. The results are presented in Table5. Initially, it can be observed
that memory has varying impacts on different datasets. The enhancement brought by memory in the
BigPatent dataset is significantly larger than that in the XSum dataset. This can be attributed to the
inherent characteristics of the BigPatent dataset, which consists of official patent documents that
exhibit considerable similarity. Consequently, this greatly improves the summarization quality in
accordance with the primal problem. Furthermore, we discovered that self-memory substantially
enhances the performance of both BRIO (+1.2 R1) and BART (+18.5 R1), achieving state-of-the-art
results on both datasets. We selected these baselines for a fair comparison, as they share the same
base generator. Due to space constraints, additional comparisons and the confidence region of the
SOTA model can be found in the Appendix E.
Table 5: Results of summarization task on XSum and BigPatent measured by ROUGE.
System Memory R-1 R-2 R-L
XSum
PEGASUS None 47.2 24.6 39.3
BRIO None 49.1 25.6 40.4
REINA (PG) Retrieval 48.2 26.0 40.2
REINA (B) Retrieval 43.2 21.0 35.5
REINA (L) Retrieval 46.5 24.1 38.6
BRIOdual⋆ Retrieval 48.6 26.1 40.6
BRIOjoint† Retrieval 49.5 26.5 41.2
BRIOdual⋆ Self 49.2 26.2 40.8
BRIOjoint† Self 50.3 26.7 41.6
System Memory R-1 R-2 R-L
BigPatent
PEGASUS None 53.6 33.2 43.2
BART None 44.4 21.3 31.0
REINA (B) Retrieval 59.5 42.6 50.6
REINA (L) Retrieval 60.7 43.3 51.3
REINA (PG) Retrieval 44.6 21.5 33.3
BARTdual⋆ Retrieval 57.4 43.3 49.7
BARTjoint† Retrieval 59.6 43.4 51.0
BARTdual⋆ Self 61.2 44.6 52.3
BARTjoint† Self 62.9 48.1 59.6
5.3 Dialogue Generation
As demonstrated in Table 6, the self-memory significantly enhances the performance of the retrieval-
augmented generator for dialogue generation tasks. By optimizing memory using BLEU as ∆(·, ·),
the self-memory improves the B-1,2 score over retrieved memory by 3.08 B-1 and 0.6 B-2 on
BARTjoint. Intriguingly, although Selfmem surpasses the baselines in terms of B-1/2, it falls behind in
D-1 and D-2, which can be attributed to the trade-off between BLEU score and Distinct score when
evaluating a dialogue system [104]. To address this issue, we opt for D-1,2 as∆(·, ·) when optimizing
Sθ, denoted as BARTjoint†(D). The results in Table 6 highlight the remarkable flexibility of Selfmem
by directly optimizing memory to achieve the desired attributes for diverse and informative dialogue.
6 Further Analysis
To gain a deeper insight intoSelfmem, we first examine the impact of each key component, namelyGξ
and Sθ. Subsequently, we perform a detailed token-level analysis of the generated output concerning
their frequency in the training set. Experiments are conducted on the JRC-Acquis En→De dataset.
We also include latency analysis and human evaluation on Appendix F and G.
Tuning Sθ We explored various Sθ by direct selection from the candidate pool based on gold
rankings. As shown in Figure 3a, both architectures with enhanced Sθ significantly outperform
the current SOTA performance (60.11 BLEU). Moreover, we assessed the candidate pool quality
during this iterative process using an oracle Sθ, as displayed in Figure 3b. A clear pattern emerges
8



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 6):

Retrieve Anything To Augment Large Language Models Conference’17, July 2017, Washington, DC, USA
Table 2: Impact on in-context learning. The performances are measured by Misc. metrics (see Appendix).
In-Context Learning
Method CQA Comm Coref Para NLI RC Sent D2T Summ Avg
None 0.2923 0.7212 0.6578 0.5242 0.4478 0.4892 0.7077 0.1982 0.1447 0.4645
BM25 0.3603 0.7019 0.6029 0.5059 0.4583 0.5396 0.7284 0.3019 0.1555 0.4840
Instructor 0.5003 0.7772 0.5735 0.6312 0.5360 0.6219 0.9148 0.4595 0.4572 0.6036
Contriever 0.4912 0.7723 0.5624 0.6358 0.5466 0.6297 0.9141 0.4380 0.4444 0.6009
RetroMAE-BEIR 0.4594 0.7742 0.5840 0.5755 0.5408 0.6029 0.9286 0.4661 0.4465 0.5939
BGE∗ 0.4718 0.7773 0.5550 0.6171 0.5413 0.5988 0.9281 0.4719 0.4521 0.5974
AAR 0.4809 0.7796 0.5848 0.5890 0.5354 0.6039 0.9210 0.4445 0.4410 0.5938
API-Retriever 0.4765 0.7620 0.5465 0.6266 0.5204 0.6096 0.9245 0.4866 0.4424 0.5945
LLM-R† 0.5165 0.7802 0.5830 0.6567 0.6145 0.6223 0.9059 0.4777 0.4878 0.6262
LLM-Embedder 0.5163 0.7842 0.5927 0.6556 0.6041 0.6318 0.9224 0.4731 0.4742 0.6268
Table 3: Impact on long conversation and language modeling (PPL), tool learning (NDCG), conv search (NDCG).
Conversation Language Modeling Tool C-Search
Method MSC Books3 Arxiv CodeParrot PG19 (o.d.) ToolLLM QReCC
None 19.3501 8.8193 3.7647 2.7663 10.2510 – –
Recency 13.9569 8.7391 3.4158 2.5989 10.2216 – –
BM25 14.6512 8.6576 3.3106 2.4591 10.1960 0.5115 0.4341
Instructor 14.8799 8.6619 3.3546 2.4756 10.2011 0.3882 0.2863
Contriever 14.2129 8.6460 3.2709 2.4437 10.1616 0.4904 0.3563
RetroMAE-BEIR 14.3990 8.6376 3.2903 2.4592 10.1731 0.5205 0.4037
BGE∗ 14.2943 8.6311 3.2912 2.4578 10.1541 0.5761 0.3856
AAR 14.6999 8.6381 3.3260 2.4666 10.1808 0.4200 0.2877
API-Retriever† 14.7834 8.6722 3.3858 2.4919 10.1833 0.8017 0.1137
Conv-ANCE† – – – – – – 0.4560
LLM-R 14.4746 8.6619 3.3635 2.4724 10.2024 0.1321 0.0234
LLM-Embedder 13.4832 8.6080 3.2322 2.4303 10.1185 0.8645 0.5053
a simple yet strong baseline called Recency. Rather than using re-
trieved context, Recency directly leverages the most recent context
immediately preceding the current window. For example, in con-
versation, it considers the last pair of utterances before the current
session; and in language modeling, it introduces the content within
the range of 2049-4096 tokens preceding the latest 2048 tokens.
With the introduction of this new baseline, the impact of retrieval
augmentation becomes more nuanced. On one hand, the LLM-
Embedder continues to exhibit superior performance across various
situations. On the other hand, other retrievers no longer guarantee a
consistent enhancement: although alternative retrieval-augmented
methods yield improved generation quality for language modeling,
a majority of them fall short of Recency’s performance while dealing
with conversation. This observation underscores the challenges
regarding effective memory retrieval in practice.
•Tool Learning and Conversation Search . The experiment
results on tool learning and conversational search are shown in
Table 3. In line with our prior observations, the task-specific ap-
proaches, i.e. the API retriever (Tool) and Conv-ANCE (Conv Search),
consistently deliver higher performances then most of the baselines.
Besides, unlike other cases, BM25 overtakes most of the embedding
models in these two scenarios. However, it’s worth noting that
LLM-Embedder continues to maintain the leading position, which
again highlights its capability in unifying diverse retrieval tasks.
3.2.3 Ablation Studies. The ablation studies are presented to ana-
lyze the influential factors about LLM-Embedder’s training process
(see Table 4): reward from LLM, instruction based fine-tuning, ho-
mogeneous in-batch negative sampling, and stabilized distillation.
For “w.o. LLM reward ”, we replace the soft reward from LLM
by using highest rated candidates as positive samples (i.e. hard la-
bels). By doing so, the knowledge distillation is reduced to contrast
learning. The empirical performance in most of the scenarios are
decreased due to such a change. However, the performances in tool
learning and conversational search are little affect; this is compre-
hensible knowing that LLM-Embedder is purely trained with hard
labels in both scenarios.
For “w.o. instruction FT ”, we remove the task-specific instruc-
tions while fine-tuning LLM-Embedder. Without such a component,
it will become harder for the embedding model to discriminate the
retrieval task in different scenarios. This speculation is consistent
with the observed result, as LLM-Embedder’s performance is de-
creased from such a change.
For “w.o. homo NS ”, the homogeneous in-batch negative sam-
pling is disabled. Such a change could reduce the discrimination of



Source: data\tc16_2312.10997v5\referenced_papers\[74]_2401.13256.pdf (Page 2):

3
access to sources like Wikipedia or the internet to ensure the
provision of up-to-date information [33, 34]. However, most
of them focus on individual sources or tasks in isolation,
overlooking the complexity and the potential existence of
multiple sources in practice. There is a latest work named
TPE which regards different knowledge sources as conceptual
tools and proposes a multi-persona collaboration framework to
model the decision-making process of the call order for multiple
knowledge sources [35]. We differ in exploring the capability of
LLMs to be planner, retriever, reader in a unified manner.
C. Retrieval-augmented Generation
Retrieval-augmented Generation (RAG) has been considered
as an effective method to overcome several limitations of
LLMs, such as hallucinations [36], factuality [37] and long-
term memory [7]. Usually, an external retriever is first used to
retrieve relevant textual knowledge from one specific knowledge
source (e.g., Wikipedia), then the reader takes the relevant
textual knowledge as external context for generating knowledge-
grounded response [38]. Most of previous works try to optimize
the retriever and reader independently [39]. During the initial
phases, people use sparse retriever, such as BM25 [40], to
make relevance decisions and retrieve corresponding evidence.
However, sparse approaches fall short in extracting the semantic
features inherent in text content [41]. To overcome this issue,
researchers have proposed language model-based dense retrieval
methods by encoding documents and queries as dense vectors,
which effectively represent the semantic features of text content
[42, 43, 44]. For example, DPR [42] uses two pre-trained
language models to encode documents and queries separately,
allowing for a more nuanced understanding of the content. More
recently, there are a handful of works exploring the performance
of LLMs as retriever [45, 46, 47, 48]. In detail, Shen et al. [47]
firstly prove that LLMs can be used as strong zero-shot retriever
on several benchmark datasets, while Ma et al. [48] propose
Listwise Reranker with a Large Language Model (LRL), which
achieves strong reranking effectiveness without using any task-
specific training data. Distinguishing from previous works, we
finetune LLM itself to learn a joint distribution of dialogue and
evidence using similarity feedbacks from current most powerful
LLMs, such as ChatGPT and GPT-4.
III. Problem Definition
To consider the responses which require external knowledge
and those which do not in practice, we provide a unified
definition to unify the retrieval-augmented dialogue system and
non-retrieval dialogue system, following Wang et al. [11]. Let
𝑪𝑡 = {𝑢1,𝑠1,𝑢2,𝑠2,...,𝑢 𝑡}denotes the dialogue context at the
current conversation turn 𝑡, and there are different knowledge
sources 𝑲 = {𝐾1,𝐾2,...,𝐾 𝑖}, where 𝐾𝑖 = {𝑘1
𝑖,𝑘2
𝑖,...,𝑘 𝑗
𝑖}
indicates the 𝑖𝑡ℎ source’s name of 𝑲. 𝑘𝑗
𝑖 denotes the 𝑗𝑡ℎ
knowledge in natural language from 𝐾𝑖. The goal of retrieval-
augmented dialogue system 1 is to select suitable knowledge
1In this context, a non-retrieval dialogue system is viewed as a specific type
within retrieval-augmented dialogue systems, distinguished by the absence of
a knowledge source, denoted as NULL.
LargeLanguageModels(LLMs)
DialogContext
PERSONADOCUMENTSMEMORY...
RetrievedResults
PERSONADOCUMENTS
SystemResponse
Retriever
PERSONADOCUMENTS
SystemResponse
Step1:KnowledgeSourceSelection Step3:ResponseGeneration
Persona
NULL
Documents…
𝐸𝑣𝑖𝑑𝑒𝑛𝑐𝑒!,𝑆𝑖𝑚! 𝐸𝑣𝑖𝑑𝑒𝑛𝑐𝑒",𝑆𝑖𝑚"…
Step2:RelevanceScorePrediction
𝑺𝒊𝒎𝟏 𝑺𝒊𝒎𝒏…{0.1,0.2,0.3,…,1.0}
EvidenceAttentionMask
ChatGPTDPR
EvidenceAttentionMask
Context,Source,𝐸vidence!,𝑆𝑖𝑚!𝐸vidence",𝑆𝑖𝑚"𝐸vidence#,𝑆𝑖𝑚#Context,Source,𝐸vidence!,𝑆𝑖𝑚!𝐸vidence",𝑆𝑖𝑚"𝐸vidence#,𝑆𝑖𝑚#Context,Source,𝐸vidence!,𝑆𝑖𝑚!𝐸vidence",𝑆𝑖𝑚"𝐸vidence#,𝑆𝑖𝑚#
Sources
à
ActingTokensEvaluationTokens
labels
Fig. 2: Our proposed method UniMS-RAG, where three opti-
mization tasks are carefully designed: 1) Knowledge Source
Selection; 2) Relevance Score Prediction; and 3) Response
Generation. We use orange to indicate acting tokens and blue to
indicate evaluation tokens. It is worth noting we have all labels
during training to optimize these three sub-tasks in a teacher-
forcing way.
sources, and then generate the helpful, informative and personal-
ized response, depending on which source is chosen [30]. Thus,
the problem of PerDS can be decomposed into the following
three tasks:
• Knowledge Source Selection. At each turn 𝑡, given the
dialogue context𝐶𝑡and different knowledge sources𝑲, PerDS
first select suitable sources denoted as 𝑆 ∈ {𝑲,NULL}. It’s
important to highlight that 𝑆may consist of multiple sources
or be NULL. Notably, there are no constraints imposed on the
relationships between different sources within 𝑆. They can
either be independent or interdependent.
• Knowledge Retrieval. The second task is to retrieve top-n
corresponding evidence 𝑬 = {𝑒1,𝑒2,...,𝑒 𝑛}for each selected
source if there is. We simply skip this step once the PerDS
determine there is no need to call external knowledge.
• Response Generation. The final task is to generate a proper
response 𝑠𝑡 concerning the dialogue context𝑪𝒕 and necessary
evidences 𝑬 from different external knowledge sources 𝑆 if
there is. The generated response is expected to ground on these
evidences, being personalized, informative, up-to-date and
helpful according to the distinctions across different sources.
IV. Method
In this section, we first describe the framework that reformu-
lates each task in PerDS into the unified Seq2Seq paradigm,
and then introduce the joint-training strategies for the retriever
and reader modules and the inference strategy to re-evaluate the
quality of response, respectively. The overview and examples of
the input and output sequences for UniMS-RAG are illustrated
in Figure 2.
A. UniMS-RAG
Inspired by previous work [11], we propose an innovative
methodology termed UniMS-RAG, where 𝑈 signifies the
unification of the training process for planner, retriever and
reader, as well as the integration of diverse tasks into a singular
comprehensive framework. Recognizing the potential of large



### Claim 100/179

#### Claim Text
Alternative and more detailed/realistic models for the axoneme will be considered in future work, starting from the one proposed in .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 18):

We were able to precisely model the dependence of the loss onN and D, and alternatively onN and S, when
these parameters are varied simultaneously. We used these relations to derive the compute scaling, magnitude
of overﬁtting, early stopping step, and data requirements when training large language models. So our scaling
relations go beyond mere observation to provide a predictive framework. One might interpret these relations
as analogues of the ideal gas law, which relates the macroscopic properties of a gas in a universal way,
independent of most of the details of its microscopic consituents.
It is natural to conjecture that the scaling relations will apply to other generative modeling tasks with a
maximum likelihood loss, and perhaps in other settings as well. To this purpose, it will be interesting to
test these relations on other domains, such as images, audio, and video models, and perhaps also for random
network distillation. At this point we do not know which of our results depend on the structure of natural
language data, and which are universal. It would also be exciting to ﬁnd a theoretical framework from
which the scaling relations can be derived: a ‘statistical mechanics’ underlying the ‘thermodynamics’ we
have observed. Such a theory might make it possible to derive other more precise predictions, and provide a
systematic understanding of the limitations of the scaling laws.
In the domain of natural language, it will be important to investigate whether continued improvement on the
loss translates into improvement on relevant language tasks. Smooth quantitative change can mask major
qualitative improvements: “more is different”. For example, the smooth aggregate growth of the economy
provides no indication of the speciﬁc technological developments that underwrite it. Similarly, the smooth
improvements in language model loss may hide seemingly qualitative changes in capability.
Our results strongly suggest that larger models will continue to perform better, and will also be much more
sample efﬁcient than has been previously appreciated. Big models may be more important than big data.
In this context, further investigation into model parallelism is warranted. Deep models can be trained using
pipelining [HCC+18], which splits parameters depth-wise between devices, but eventually requires increased
batch sizes as more devices are used. Wide networks on the other hand are more amenable to parallelization
[SCP+18], since large layers can be split between multiple workers with less serial dependency. Sparsity
[CGRS19, GRK17] or branching (e.g. [KSH12]) may allow for even faster training of large networks through
increased model parallelism. And using methods like [WRH17, WYL19], which grow networks as they train,
it might be possible to remain on the compute-efﬁcient frontier for an entire training run.
Acknowledgements
We would like to thank Shan Carter, Paul Christiano, Jack Clark, Ajeya Cotra, Ethan Dyer, Jason Eisner,
Danny Hernandez, Jacob Hilton, Brice Menard, Chris Olah, and Ilya Sutskever for discussions and for feed-
back on drafts of this work.
19



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 29):

Jiawei Zhang
Table 8: Analysis of generated API query cases different from ground-truth queries in the testing set.
Input
Sources
Language Generation Examples
Input Payload Generated Outputs Before Fine-Tuning Generated Outputs After Fine-Tuning
Pile
Data
Roman Catholic Diocese of Tambacounda\n\nThe Ro-
man Catholic Diocese of Tambacounda () is a diocese
located in the city of Tambacounda in the Ecclesiastical
province of Dakar in Senegal.
\n\nHistory\n\nThe Diocese of Tambacounda
was erected by Pope Pius XI on December 21,
1933, on territory split off from the Diocese of
Saint-Louis du Sénégal. \n\nBishops\n\nDiocese
of Tambacounda \nErected: December 21,
1933\nMetropolitan: Archdiocese of Dakar\n
center>The Roman Catholic Diocese of Tambacounda
() is a diocese located in the city of Tambacounda in
the <center>Ecclesiastical province of Dakar in Sene-
gal.</center> center>Output: </center> center>The
<center>Roman</center> <center>Catholic
Mud Hens pitcher Evan Reed charged with sexual
assault\n\nMud Hens pitcher Evan Reed was charged
July 30 with sexual assault related to a March incident
in Detroit when he was a member of the Detroit Tigers.
\n\nMud Hens pitcher Evan Reed was charged July 30
with sexual assault related to a March incident in Detroit
when he was a member of the Detroit Tigers.\n\nMud
Hens pitcher Evan Reed was charged July 30 with sexual
assault related to a March incident in Detroit when he
was a member of the Detroit Tigers.\n\nMud Hens
pitcher Evan Reed was charged July 30 with sexual
assault related to
center>###<centercentercentercentercentercentercenter
centercentercentercentercentercentercentercenter
centercentercentercentercentercentercentercenter
centercentercentercentercentercentercentercenter
centercentercentercentercenter
Random
Sources
Thursday’s verdict only added to the intrigue surround-
ing the gravest legal and political unknown from the
2021 Capitol insurrection that hangs over the 2024 cam-
paign: will Donald Trump, the president who inspired
the uprising, face his own legal and political price?
I don’t think there’s any question that he’s going to be
impeached, ” Rep. Al Green (D-TX) told The Daily Beast.
“I think he’s going to be impeached, and I think he’s
going to be removed from office. ”\n\nGreen’s
will be the verdict only added to the intrigue surround-
ing the gravest legal and political unknown from the
2021 Capitol insurrection that hangs over the 2024 cam-
paign: will Donald Trump, the president who inspired
the uprising, face his own legal and political price?br>
A source of debate for centuries, the village of Ponte
Buriano, a suburb of Arezzo in the Tuscany region of
Italy, is so convinced the bridge behind Mona Lisa is
the Ponte Buriano that they’ve made it a key feature of
their local tourism campaign, even laying claim to the
bridge on the village’s welcome sign.
\n\nThe Ponte Buriano is one of the oldest bridges in
Tuscany, dating back to the 12th century. It’s also one of
the most famous, thanks to Leonardo da Vinci’s Mona
Lisa, the most famous painting in the world.\n\nThe
The village of [GR(GL("mona_lisa"), "bpr:search",
GR(GL("ponte_buriano"), GR(GL("search"),
GR(GL("mona_lisa"), GR(GL("search
•Efficiency: With LoRA and quantized models/optimizers,
we can reduce the model fine-tuning memory capacity re-
quirement to less than 11GB and the memory capacity re-
quirement even lower for the model inference stage. Mean-
while, integrated with the large-sized graph data, pre-trained
graph models, and necessary pre-processed data, the effi-
ciency of Graph-ToolFormer for various graph reasoning
task can still be a problem. In this paper, we introduce a
tentative approach to make the problem less severe with
the working memory. However, if we plan to deployGraph-
ToolFormer on devices with very small memories, like cell-
phones or embedded equipments, new techniques will still
be needed to improve the model learning and inference effi-
ciency.
•Diverse Applications : Due to the limited space, we can
only study a few number of the graph reasoning tasks with
Graph-ToolFormer in this paper. Meanwhile, in the real-
world, we have lots of graph structured data that may require
the LLMs to handle them to reason for the desired outputs.
Therefore, a very promising future work direction is to apply
Graph-ToolFormer to study diverse real-world graph/net-
work data oriented reasoning tasks with LLMs. We list a
few of them here just for the readers’ information, and the
readers may explore more diverse reasoning tasks according
to your own backgrounds and expertises.
– Urban Computing and Smart City : In the offline world,
we have extensively connected traffic networks that bridge
different local communities, cities and countries by lo-
cal roads, national highways, international fights and
ocean freight corridors. Applying LLMs for knowledge
extraction and reasoning based on such traffic networks
is critical for the current urban computing and smart city
projects.
– IoT and Smart Home : Assisted with the 5G, the IoT net-
work effectively bridges the cyber world with the physical
devices and equipments together via extremely fast com-
munication channels. The LLMs provide the opportunity
for us to utilize language models as the general interface
for controlling the devices within the IoT networks, which
is also the main objective for building the smart home
system.
– Healthcare : During the past years, the world has suf-
fered a lot from the covid-19 pandemic. Similar to the
protein molecules studied in this paper, both the virus
and the vaccines can also be represented as the molecular
graphs. LLMs with the molecular graph reasoning ability
have the potential to improve our current healthcare sys-
tem in many perspectives, like early identification of virus ,
analysis of the virus pathogenicity and creation of vaccines .
What’s more, the LLMs with the social network reasoning
ability will also help infer the potential virus propagation
among people , early prediction of highly infectious commu-
nities and identify rumors and misinformation about the
pandemic (at the online social networks).



Source: data\tc16_2312.10997v5\referenced_papers\[93]_2309.11495.pdf (Page 0):

CHAIN -OF-VERIFICATION REDUCES HALLUCINATION
IN LARGE LANGUAGE MODELS
Shehzaad Dhuliawala
Meta AI & ETH Z¨urich
Mojtaba Komeili
Meta AI
Jing Xu
Meta AI
Roberta Raileanu
Meta AI
Xian Li
Meta AI
Asli Celikyilmaz
Meta AI
Jason Weston
Meta AI
ABSTRACT
Generation of plausible yet incorrect factual information, termed hallucination,
is an unsolved issue in large language models. We study the ability of language
models to deliberate on the responses they give in order to correct their mistakes.
We develop the Chain-of-Verification (COVE) method whereby the model first (i)
drafts an initial response; then (ii) plans verification questions to fact-check its
draft; (iii) answers those questions independently so the answers are not biased
by other responses; and (iv) generates its final verified response. In experiments,
we show COVE decreases hallucinations across a variety of tasks, from list-based
questions from Wikidata, closed book MultiSpanQA and longform text generation.
1 I NTRODUCTION
Large Language Models (LLMs) are trained on huge corpora of text documents with billions of
tokens of text. It has been shown that as the number of model parameters is increased, performance
at tasks such as closed book QA improve in accuracy, and larger models can generate more correct
factual statements (Radford et al., 2019; Petroni et al., 2019). However, even the largest models can
still fail, particularly on lesser known torso and tail distribution facts (Sun et al., 2023a), i.e. those
that occur relatively rarely in the training corpora. In those cases where the model is incorrect, they
instead generate an alternative response which is typically plausible looking (e.g., a similar entity, but
an incorrect one). These factually incorrect generations are referred to as hallucinations (Maynez
et al., 2020). Further, in longform tasks consisting of generating multiple sentences or paragraphs, the
hallucination problem can be exacerbated due to the issue of exposure bias (Wang & Sennrich, 2020).
The current wave of language modeling research goes beyond next word prediction, and has focused
on their ability to reason. Improved performance in reasoning tasks can be gained by encouraging
language models to first generate internal thoughts or reasoning chains before responding (Wei et al.,
2022; Adolphs et al., 2021; Wang et al., 2022; Lanchantin et al., 2023), as well as updating their
initial response through self-critique (Press et al., 2022; Madaan et al., 2023). In this work we
follow this line of research to study how and when language-model-based reasoning can be used to
reduce hallucinations. We develop an approach, called Chain-of-Verification (CoVe) which, given
an initial draft response, first plans verification questions to check its work, and then systematically
answers those questions in order to finally produce an improved revised response. We find that
independent verification questions tend to provide more accurate facts than those in the original
longform answer, and hence improve the correctness of the overall response. We study variations on
this recipe across a range of tasks: from list-based questions, closed booked QA and longform text
generation. We first propose a joint approach for generating the entire verification chain left-to-right,
which improves performance and decreases hallucinations compared to the baseline language model.
However, models that attend to existing hallucinations in the context from their own generations tend
to repeat the hallucinations. Hence we also introduce further improvements with factored variants
which separate out the verification chain steps, in terms of which context is attended to. We show
how these factored variants give further performance gains across all three tasks considered.
1
arXiv:2309.11495v2  [cs.CL]  25 Sep 2023



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 8):

Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT
Eccentricity: Given a connected graph, like the lollipop graph
𝐺𝑙 = (V,E), for node𝑣𝑖 ∈V, its eccentricity denotes the maximum
graph distance between 𝑣𝑖 and any other node 𝑣𝑗 ∈V in the graph.
According to such a definition, for disconnected graph, all nodes are
defined to have infiniteeccentricity. We can compute theeccentricity
either for the whole graph (i.e., for all nodes in the graph) or for
specific node(s) via the following two API calls:
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑒𝑐𝑐𝑒𝑛𝑡𝑟𝑖𝑐𝑖𝑡𝑦 ”)→ 𝑟</API>, (21)
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑒𝑐𝑐𝑒𝑛𝑡𝑟𝑖𝑐𝑖𝑡𝑦 ”,node-subset)→ 𝑟</API>.
(22)
Diameter: The diameter of a graph denotes the “longest shortest
path” between any two nodes in the graph, whose API call can be
represented as
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑑𝑖𝑎𝑚𝑒𝑡𝑒𝑟”)→ 𝑟</API>, (23)
whose result will be equal to the result of the above API call
<API>𝐺𝑅(𝐺𝑙,“𝑚𝑎𝑥-𝑠ℎ𝑜𝑟𝑡𝑒𝑠𝑡 -𝑝𝑎𝑡ℎ”)</API> actually.
Radius: Graph radius denotes the is the minimum graph eccen-
tricity of any node in a graph. A disconnected graph therefore has
infinite radius. The API call for computing a graph radius can be
represented as
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑟𝑎𝑑𝑖𝑢𝑠”)→ 𝑟</API>. (24)
Center: Formally, the center of a graph denotes the set of nodes
whose eccentricity is equal to the graph radius. The API call for
identifying a graph center can be represented as
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑐𝑒𝑛𝑡𝑒𝑟”)→ 𝑟</API>. (25)
Periphery: The periphery of a graph is the subgraph of the graph
induced by nodes that have the eccentricities equal to the graph
diameter, whose API call can be represented as
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑝𝑒𝑟𝑖𝑝ℎ𝑒𝑟𝑦 ”)→ 𝑟</API>. (26)
A Summary of Basic Graph Reasoning API Calls : According
to our descriptions above, the readers should have observed that
those graph properties may require very complex logic reasoning.
Via some preliminary experimental testings, the current LLMs (such
as ChatGPT and LLaMA) cannot handle them very well. At the same
time, the above reasoning properties, like the shortest-path, also
have very extensive applications in the real-world graph reason-
ing tasks, e.g., traffic network reasoning and traffic route planning .
Currently, the LLMs have been criticized since they cannot provide
the correct reasoning results for the spatial traffic data, e.g., esti-
mating the traveling distance and time between different locations.
Equipped with the above shortest path based property API calls on
traffic networks, we will be able to provide more precise reasoning
results for LLMs in handling such queries. To incorporate them
into language models, we also show some examples of the above
API calls in Table 1, which can load the graph data from specified
data sources and conduct the reasoning of some general graph
properties as discussed above.
4.3.3 Advanced Graph Reasoning Tasks. Besides the basic graph
property reasoning tasks, we will also study several advanced rea-
soning tasks on real-world graph data with more complex structures
in this paper, which include (1) academic paper topic reasoning on
bibliographic network, (2) protein function reasoning based on protein
graph structures , (2) sequential product recommendation reasoning
based on recommender systems , (4) social community reasoning from
online social networks and (5) semantics reasoning on knowledge
graphs.
For many other advanced graph reasoning tasks not studied
in this paper, via very minor changes to the Graph-ToolFormer
framework, they can also be effectively incorporated into Graph-
ToolFormer as well by adding the corresponding API calls into the
reasoning prompts. The Graph-ToolFormer framework can serve
as the backbone for hosting various graph reasoning application
tasks with LLMs as the general interface. In Section 7, we will
also describe some potential future research opportunities for the
readers at the very end of this paper.
Bibliographic Paper Topic Reasoning : Bibliographic network
[47] defines a complex graph structured data involving diverse enti-
ties, such as academic papers, authors, institutions and publication
venues, as well as diverse links among these entities, such as the
citation links, authorship links, affiliation links and publication
links. In this part, we will discuss about the academic paper topic
reasoning task based on the bibliographic network. The topics of a
paper can be inferred with not only its own textual descriptions but
also the other papers cited by/citing it, which requires the graph
reasoning model to utilize both the raw textual features of the
papers and the extensive citation links among the papers.
Formally, based on the terminology definition provided in the
previous Section 3.2, we can represent the bibliographic network as
𝐺 = (V,E), which can be loaded via the API call
<API>𝐺𝐿(“𝑏𝑖𝑏𝑙𝑖𝑜𝑔𝑟𝑎𝑝ℎ𝑖𝑐 -𝑛𝑒𝑡𝑤𝑜𝑟𝑘”)→ 𝐺</API>. (27)
Each paper is represented as a node 𝑣𝑖 ∈V in the bibliographic
network, which has both its raw feature vector x𝑣𝑖 and label vector
y𝑣𝑖 . The raw feature vector includes the textual information about
the paper (like its title or abstract), and its label vector indicates
the topics of the paper. Existing graph neural networks (GNNs)
infer the paper topics by learning their representations with both
raw features and connected neighbors’ information [19, 60], which
can be further used to infer the topic label vector. For the Graph-
ToolFormer model introduced in this paper, we will use the pre-
trained Graph-Bert [60] as the default topic inference model for
bibliographic networks. Based on the above descriptions, we can
represent the paper topic reasoning via graph neural network model
with the following API call:
<API>𝐺𝑅(𝐺,“𝑔𝑟𝑎𝑝ℎ-𝑏𝑒𝑟𝑡:𝑡𝑜𝑝𝑖𝑐”,𝑝𝑎𝑝𝑒𝑟 -𝑛𝑜𝑑𝑒)→ 𝑟</API>. (28)
The function notation “𝐺𝑅(·,“𝑔𝑟𝑎𝑝ℎ-𝑏𝑒𝑟𝑡:𝑡𝑜𝑝𝑖𝑐”,·)” denotes it is a
paper topic reasoning API with the Graph-Bert model [60]. Actu-
ally, the Graph-ToolFormer framework proposed in this paper is
a general framework. Besides the Graph-Bert model, many other
existing graph neural network models can also be used here for aca-
demic paper topic inference as well. Based on the provided source
code, the readers can customize the Graph-ToolFormer to include



### Claim 101/179

#### Claim Text
Their findings highlight the complicated synergies between He, H and radiation damage of structural materials .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[31]_2311.02616.pdf (Page 0):

Divide & Conquer for Entailment-aware Multi-hop Evidence Retrieval
Fan Luo
University of Arizona
Tucson, AZ, USA
fanluo@email.arizona.edu
Mihai Surdeanu
University of Arizona
Tucson, AZ, USA
msurdeanu@email.arizona.edu
Abstract
Lexical and semantic matches are commonly
used as relevance measurements for informa-
tion retrieval. Together they estimate the se-
mantic equivalence between the query and the
candidates. However, semantic equivalence is
not the only relevance signal that needs to be
considered when retrieving evidences for multi-
hop questions. In this work, we demonstrate
that textual entailment relation is another im-
portant relevance dimension that should be con-
sidered. To retrieve evidences that are either
semantically equivalent to or entailed by the
question simultaneously, we divide the task of
evidence retrieval for multi-hop question an-
swering (QA) into two sub-tasks, i.e., seman-
tic textual similarity and inference similarity
retrieval. We propose two ensemble models,
EAR and EARnest, which tackle each of the
sub-tasks separately and then jointly re-rank
sentences with the consideration of the diverse
relevance signals. Experimental results on Hot-
potQA verify that our models not only signifi-
cantly outperform all the single retrieval mod-
els it is based on, but is also more effective than
two intuitive ensemble baseline models.
1. Introduction
Widely adopted QA approaches use a two-stage
pipeline, i.e., a retriever module followed by a
reader module (Chen et al., 2017). The retriever is
responsible for collecting relevant evidences, then
the reader module combines the relevant informa-
tion from the retriever module to infer the answer.
Evidence retrieval is a ranking task, which usu-
ally maps questions and candidates to vectors and
scores the semantic relationship between them with
respect to relevance. The goal is to rank the most
relevant pieces of evidence (supporting facts) at the
top of the list among all the candidates.
According to formal semantic notions, the se-
mantic relationship between two text fragments
includes semantic equivalence, referential equality,
and textual entailment. While referential equal-
ity can be mostly solved by coreference resolution
and entity linking, semantic similarity and textual
entailment require deep semantic understanding
between question and context (Lin et al., 2021).
Textual entailment is a framework that captures
semantic inference. Textual Entailment (TE) of
two text fragments can be defined as the task of
deciding whether the meaning of one text fragment
can be inferred from another text fragment. That is,
a premise T entails a hypothesis H if, typically, a
human reading T would infer that H is most likely
true. For example, “T: Jack sold the house to Peter.
H: Peter owns the house.Here H can be inferred
from T, so T entails H. While semantic equivalence
capture “Do these two texts mean the same thing?”,
textual entailment is a framework that captures se-
mantic inference, deciding whether the meaning of
one text can be inferred from another.
It is common to utilize an inference model as
a reader to infer the correct answer from a subset
of retrieved context. However, most works of the
evidence ranking only measure the semantic tex-
tual similarity between the question and candidate
corpus to determine the relevance , and ignore the
inference signals. This works for single-hop ques-
tions, in which relevant information usually share
the same entity mentions with the question. How-
ever, it is not sufficient for multi-hop questions.
The relevance between multi-hop question and its
evidence(s) is beyond the lexical or semantic simi-
larity that is targeted by most retrievers, especially
for secondary hops. As the example in Figure 1
shows, relevance is multi-dimensional. The first
hop evidence “James Henry Miller (25 January
1915 – 22 October 1989), better known by James
Henry Miller stage name Ewan MacColl, was an
English folk singer, songwriter, communist, labour
activist, actor, poet, playwright and record pro-
ducer" has lexical overlap with the question and
thus get high scores from token overlap match and
arXiv:2311.02616v1  [cs.CL]  5 Nov 2023



Source: data\tc16_2312.10997v5\referenced_papers\[31]_2311.02616.pdf (Page 1):

Question: What nationality was James Henry Miller’s wife?
Answer: American
Supporting Evidences
Ewan MacColl
(1) James Henry Miller (25 January 1915 – 22 October
1989), better known by James Henry Miller stage name
Ewan MacColl, was an English folk singer, songwriter,
communist, labour activist, actor, poet, playwright and
record producer .
Peggy Seeger
(2) Margaret "Peggy" Seeger (born June 17 , 1935) is an
American folksinger.
(3) She is also well known in Britain, where she has lived
for more than 30 years, and was married to the singer and
songwriter Ewan MacColl until his death in 1989.
Semantic Equality
James Henry Miller (Q)≈ James Henry Miller (25 January
1915 – 22 October 1989), better known by James Henry Miller
stage name Ewan MacColl (1)
Textual Entailment
American (2) ⊢ nationality (Q)
was married to (3) ⊢ wife (Q)
Figure 1: An example from the HotpotQA dataset show-
ing the two different dimensions of relevance between
the question and its supporting evidences.
semantic similarity comparison. However, the sub-
sequent hops in the paragraph‘Peggy Seeger’failed
to be retrieved by both lexical match and semantic
similarity comparison, as they would not be con-
sidered as “mention or mean the same thing” as the
question. Instead, they have an entailment relation
with the question. “Margaret ‘Pegg’ Seeger (born
June 17, 1935) is an American folksinger. ”en-
tails a nationality relationship, and similarly‘James
Henry Miller’s wife’entails a marriage relationship.
Thus, we argue that semantic similarity is not the
only relevance signal that needs to be considered
when ranking evidences for multi-hop questions.
To consider the fine-grained aspects of relevance
for the multi-hop QA evidence retrieval task, we di-
vide the task into separate retrieval subtasks, where
each aims to retrieve a subset of sentences that
score highly on one of the relevance dimension
(i.e., semantic equivalence or textual entailment)
respectively, and then combine them to output the
final ranking with an ensemble model.
Our contributions are: 1. We call attention that
fine-grained aspects of a model’s capability in per-
forming types of relevance signals should be con-
sidered. Especially, textual entailment should be
explicitly taken into consideration when measure
relevance for complex question answering evidence
retrieval, so that the retriever covers a more accu-
rate relevant context that not only lexical match
based but also require inference to identify. 2. we
propose two ensemble models that combine diverse
relevance signals captured by different base models.
Our experimental results demonstrate that not only
are the individual base retrieval model necessary
in evidence retrieval but cooperate advantageously
to produce a better ranking for multi-hop QA evi-
dence retrieval when used together . 3. We empir-
ically show the effectiveness of the proposed en-
semble retrieval models by evaluating on multi-hop
HotpotQA dataset and show they not only outper-
form all the base models, and also several ensemble
baselines.
2. Related Work
2.1. Text Retrieval
Traditional retrieval modelssuch as TF-IDF and
BM25 (Trotman et al., 2014) use sparse bag-of-
words representations to collect lexical matching
signals (e.g., term frequency). Such sparse retrieval
models are mostly limited to exact matches. Dense
Retrieval modelsmove away from sparse signals
to dense representations, which help address the
vocabulary mismatch problem. These models can
be categorized into two types according to their
model architecture, representation-based (Huang
et al., 2013; Shen et al., 2014) and interaction-
based models (Pang et al., 2016; Lu and Li, 2013;
Guo et al., 2016; Mitra et al., 2017). Hybrid meth-
ods (Lin et al., 2021a; Gao et al., 2020; Karpukhin
et al., 2020; Shan et al., 2020) aggregate the dense
retrieval with sparse retrieval methods to better
model relevance. The entailment-aware retrieval
models we propose are also hybrid methods that
combine sparse and dense retrieval methods, but
our method is unsupervised. Further, we combine
a sparse model with multiple dense models to con-
sider diverse relevance signals, i.e., textual entail-
ment in addition to semantic equivalence.
2.2. Multi-hop Evidence Retrieval
Research works on multi-hop evidence retrieval
can be broadly categorized into two directions: (1)
Question decomposition:(Min et al., 2019; Jiang
and Bansal, 2019; Fu et al., 2021; Perez et al., 2020;
Talmor and Berant, 2018) decompose multi-hop
questions into multiple single-hop sub-questions.
The question decomposition works that relies on
a decomposer model trained via supervised labels



Source: data\tc16_2312.10997v5\referenced_papers\[81]_2312.15883.pdf (Page 9):

Conference’17, July 2017, Washington, DC, USA Jiang, Zhang and Xu et al.
Figure 4: Case study. We demonstrate the User Query Q, Hypothesis Output HO, Retrieved Reasoning Chains RCand Pruned
Reasoning chains RCprune of HyKGE using GPT-3.5 Turbo to verify the interpretability and effectiveness of HyKGE. Red shed
signifies that the knowledge or answer is derived from evidence in Q, blue shed indicates that the evidence originates from
HO, and green shed represents a corrected answer with the help of KG, despite initially being false in HO.
### User Query Q:
I feel stomach reflux after eating. What medicine should I take?
### Hypothesis Output HO:
Stomach reflux can be a symptom caused by gastroesophageal reflux . 1. Do you have symptoms related to indigestion , excess stomach acid , gastric ulcers ,
bile reflux , or stomach pain ? ... When experiencing mild gastroesophageal reflux, the following treatment plans can be considered:- Avoid overeating - Avoid spicy,
stimulating foods, greasy foods, and alcohol - Maintain a light diet, eat more vegetables. Under appropriate evidence, consider taking H2 receptor antagonists or
proton pump inhibitors . It is not recommended to take calcium carbonate or aluminum hydroxide medications, as calcium carbonate can lead to kidney
stones, and the aluminum element in aluminum hydroxide is toxic and may cause allergic reactions or discomfort.
### Retrieved Reasoning Chains RC:
Kidney stones →Laboratory tests →Serum calcium ←Laboratory tests ←Gastric ulcer. Kidney stones →Clinical symptoms and signs →Fever ←Complications ←
Esophageal cancer. Excess gastric acid →Affected area →Abdomen ←Affected area ←Stomach pain. Niflumic acid →Adverse reaction →Discomfort ←
Adverse reaction ←Calcium carbonate. Gastrointestinal bleeding →Related diseases →Gastric ulcer. Gastric ulcer →Treatment →Aluminum hydroxide.
Calcium carbonate →Indication →Excess gastric acid. Aluminum hydroxide →Indication →Gastric reflux.
Niflumic acid →Third level classification →Others ←Affected area ←Postprandial food reflux.
Duodenogastric reflux and bile reflux gastritis →Treatment →Calcium carbonate. More else ... [125 reasoning chains in total.]
### Pruned Reasoning chains RCprune:
Gastric ulcer →Treatment →Aluminum hydroxide. Calcium carbonate →Indication →Excess gastric acid.
Aluminum hydroxide →Indication →Gastric reflux. Niflumic acid →Third level classification →Others ←Affected area ←Postprandial food reflux.
Duodenogastric reflux and bile reflux gastritis →Treatment →Calcium carbonate. Stomach reflux →Related disease →Excess gastric acid.
Caved-S →Indication →Excess gastric acid. Weile tablets →Indication →Excess gastric acid.
### Answer:
Stomach reflux can be a symptom caused by gastroesophageal reflux disease (GERD) , and this condition could potentially lead to reflux esophagitis. Consider the
following medical recommendations: Acid-suppressing medications (Proton Pump Inhibitors) : You may consider using omeprazole or esomeprazole to effectively
reduce gastric acid secretion, aiding in relieving symptoms of gastric reflux and mitigating the development of reflux esophagitis. Alternatively, you can use
acid-neutralizing medications (antacids) such as magnesium aluminum carbonate (Rennie), calcium carbonate, and aluminum hydroxide. Another option is the use
of H2 receptor antagonists such as ranitidine or famotidine. Additionally, medications like Caved-S or Weile tablets are also suitable for mild to moderate
symptoms of gastric reflux. Gastric protective medications: For short-term relief of stomach pain and discomfort,
you may also consider using gastric mucosal protectants such as sucralfate. It helps form a protective mucous layer, reducing irritation to the stomach mucosa. Diet
and lifestyle adjustments: First, ... it is advisable to consult a healthcare professional for evaluation and diagnosis as soon as possible.
A case study of HyKGE pipeline
1 2 3 4 5
k
0
10
20
30
40
50
60EM Score (%)
Base(MMCU)
Base(CMB)
w/ HO (CMB)
w/o HO (CMB)
w/ HO (MMCU)
w/o HO (MMCU)
101
102
103
104
105
106
107
Retrieval Chains Count
5 7 10 15 30 50
topK
25
30
35
40
45
50EM Score (%)
Base(MMCU)
Base(CMB)
w/ Fragment (CMB)
w/o Fragment (CMB)
w/ Fragment (MMCU)
w/o Fragment (MMCU)
Figure 5: (Left.) Hyper-parameter study with the KG hop 𝑘on
MMCU-Medical and CMB-Exam with GPT 3.5 turbo, from 1
to 5. (Right.) Hyper-parameter study with the reranker 𝑡𝑜𝑝𝐾
on MMCU-Medical and CMB-Exam with GPT 3.5 turbo, from
5 to 50.
Figure 5 (Right.) depicts EM with different reranking thresholds.
Similar to Figure 5 (Left.), as 𝑡𝑜𝑝𝐾 increases, the trends demon-
strate that overwhelming reasoning chains will hamper LLMs’ abil-
ity for comprehension. Meanwhile, it is obvious that HyKGE w/o
Fragment always underperforms on EM as analyzed in Section 5.3.
5.6 Case Study (RQ2 and RQ3)
This case study presents a representative sample that illustrates the
effectiveness of our HyKGE model using GPT-3.5 Turbo as shown
in Table 4. The color coding within the table is key to understand-
ing the source and validity of the information and we have these
observations: i) Compared to a brief user query, semantic spaces of
HOare more abundant and have a clear direction for answering,
helping us better understand user intention and extract more ef-
fective entity information. Ultimately, HyKGE extracted 23 entities
from HOcompared to only 1 from Q. ii) Comparing the RCwith



Source: data\tc16_2312.10997v5\referenced_papers\[81]_2312.15883.pdf (Page 10):

HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable Medical LLMs Responses Conference’17, July 2017, Washington, DC, USA
RCprune, it can be observed that the pre-filtered chains contain a
large amount of irrelevant or repetitive knowledge, marked in black.
After reranking, retrieved knowledge is highly non-redundant and
relevant to HO and Q, demonstrating the effectiveness of our
fragment-based reranker. Ultimately, out of 125 reasoning chains,
HyKGE selected 𝑡𝑜𝑝𝐾 = 10 of the most valuable chains. iii) Note
that retrieved knowledge effectively assisted LLMs in correcting
errors, mitigating the issue of hallucinations. In HO, LLMs posited
that “calcium carbonate could not treat GERD ”; however, with the
supplemental knowledge about “calcium carbonate” in our retrieved
reasoning chains, marked in green. LLMs corrected this error in its
final response. In general, this case study demonstrates HyKGE’s
strong ability to generate hypotheses and validate them against a
structured KG, effectively leveraging HOfor exploring and reason-
ing chains for error correction. In general, the integration of these
components ensures that the RAG’s outputs are not only contextu-
ally relevant but also accurate, showcasing the interpretability and
potential for AI-assisted decision-making in healthcare.
5.7 Efficiency Analysis (RQ2)
To illustrate the effectiveness of our HyKGE module, we conducted
a comparative analysis of the time overhead between HyKGE and
other knowledge graph-enhanced LLM approaches, as presented in
Table 1. The KGRAG method demonstrates the shortest time over-
head among RAG methods, as it solely necessitates conveying the
retrieved knowledge to the LLM Reader. However, when juxtaposed
with QE and HyKGE, KGRAG’s performance notably lags behind,
even resulting in a negative gain because of the huge noise. In con-
trast to QE, HyKGE incurs slightly higher time primarily due to the
noise filtering process, which consumes some time. Nonetheless,
the performance enhancement achieved by HyKGE outweighs this
marginal increase in time overhead. Furthermore, CoN and CoK,
which adopt the chain-of-thought strategy [ 80], entail multiple
interactions with LLMs, which proves to be considerably restric-
tive, particularly in real-world medical Q&A scenarios where time
is a critical consideration. Therefore, striking a balance between
time overhead and model accuracy becomes imperative, in which
regard HyKGE emerges as the most efficient and high-performing
framework.
Moreover, inspired by these Chain-of-thought works [ 59, 71],
which respectively employ LLMs in different processes of RAG, we
embarked on similar endeavors. Specifically, we integrated LLMs
into the modules of NER, Reranker, and summarization modules (to
summarize the retrieved knowledge) [33], as shown in Table 4. How-
ever, our findings underscored that leveraging such large-parameter
models for tasks amenable to smaller counterparts incurs substan-
tial time costs with marginal benefits. For instance, incorporating
LLMs into NER, aimed at enhancing entity extraction, a task that
could be efficiently handled by specialized pre-trained medical NER
models, not only doubling interaction time but also introducing
complexities such as misinterpretation of instructions, thus im-
peding subsequent processing. Similarly, the utilization of LLMs
in Reranker considerably strained token resources. For instance,
upon retrieving the query “I feel stomach reflux after eating. What
medicine should I take? ” it generated a whopping 125 reasoning
chains. However, employing LLMs to eliminate noisy knowledge
from these chains resulted in decreased effectiveness. We argue
that this was primarily due to the inundation of tokens, causing
LLMs to lose in the middle [47], thereby impeding their ability to
discern genuinely relevant knowledge from the retrieved chains
and even ignore LLMs’ tasks. Consequently, LLMs employed for
Reranker inadvertently filtered out valuable knowledge, yielding
negative outcomes and exacerbating computational overhead. Like-
wise, employing LLMs for knowledge summarization encountered
challenges akin to those encountered in Reranker. Although LLMs
are quite effective, according to Occam’s razor principle [67], it is
not always beneficial to use LLMs in every RAG step. Excessive
reliance on LLMs can only lead to wasted time costs. In summary,
because RAG involves a process of continuous trial and error [8],
we experimented with many strategies and ultimately arrived at
HyKGE.
6 CONCLUSION
In this paper, we proposed HyKGE, a hypothesis knowledge graph
enhanced framework for LLMs to improve accuracy and reliabil-
ity. In the pre-retrieval phase, we leverage the zero-shot capability
of LLMs to compensate for the incompleteness of user queries by
exploring searching directions through hypothesis outputs. In the
post-retrieval phase, HyKGE applies a fragment reranking mod-
ule to enhance the knowledge density alignment between user
queries and retrieved knowledge, preserving relevant and diverse
knowledge chains. The comprehensive experiments conducted on
three medical Q&A tasks with two LLMs turbo demonstrate the
effectiveness of HyKGE. Nevertheless, it remains worthwhile to
contemplate how to dynamically optimize fragment granularity
in the post-retrieval phase—a direction that we are committed to
exploring actively in the future. In addition, despite the limitations
of data sources and the high computational cost of LLMs, we will
experiment on more other language or domain-specific KGs in the
future to enhance the scalability and generalization of HyKGE.



Source: data\tc16_2312.10997v5\referenced_papers\[81]_2312.15883.pdf (Page 8):

HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable Medical LLMs Responses Conference’17, July 2017, Washington, DC, USA
3.5 turbo. This highlights the effectiveness of our modules in locat-
ing valid information and filtering noises in retrieved knowledge.
Although CoK, CoN, KG-GPT and SuRe have achieved commend-
able results, their advancements are constrained in the knowledge
search space, due to their focus on continuous knowledge under-
standing rather than exploration. Moreover, compared to CoK,
CoN, KG-GPT and SuRe, HyKGE avoids accumulating errors in
the chain of thought while acquiring and retaining more relevant
yet diverse knowledge. In summary, our proposed HyKGE model
exhibits superior performance over all baselines with fewer inter-
action times with LLMs (c.f. Table 1). Evidenced by comprehension
experiments, HyKGE demonstrates the HO Module’s and the HO
Fragment Granularity-aware rerank module’s effectiveness com-
pared to CoN, CoK, KG-GPT and SuRe.
5.3 Ablation Study (RQ 2)
To answer RQ2, we perform ablation studies to verify the effective-
ness of the critical components of HyKGE, as illustrated in Table 2.
Our observation can be summarized as follows:
In pre-retrieval phase. When we remove the Hypothesis Out-
put Module, results are even deteriorating than base model. This is
attributed to the fact that retrieved knowledge simply based on user
queries is either insufficient or futile because of lacking direction
for exploration. Nevertheless, the results of w/o HO are still better
than KGRAG and we argue the reason is the reranking of reasoning
chains effectively filters out noise during the post-retrieval phase.
In post-retrieval phase. The removal of the Reranker leads to
a noticeable decline in performance compared to HyKGE, which
indicates that Reranker effectively eliminates excessive noise intro-
duced by the retrieved knowledge, retaining only the most pertinent
parts for answering the question. When we use entire HOand Q
instead of chunk (Q⊕HO) to perform reranking with reasoning
chains, a decline in performance is also observed. This is attribut-
able to the misalignment between dense retrieved knowledge and
sparsely distributed keywords in HOand Q, inducing a tendency
to select more general or lengthier knowledge, thereby diminishing
the HOM’s capability to supplement diverse knowledge.
Moreover, results of w/o Chains and w/o Description demon-
strate that even when KG lacks certain knowledge, descriptive
information or relevant knowledge chains can still enhance the
answering capabilities of LLMs, which is believed to be associated
with the inherent implicit knowledge within the LLMs themselves.
5.4 Interpretability Analysis (RQ 3)
In this section, we concentrate on evaluating the interpretability
with three metrics ACJ, BLEU, PPL and ROUGE-R as shown
in Table 3 to find out whether the retrieved knowledge is effec-
tive and whether it can help LLMs reduce hallucinations. Several
observations can be derived from the results.
The relevance of knowledge retrieval. For methods that in-
teracted with LLMs and applied noise filtering modules, such as
QE, CoK, CoN, SuRE and HyKGE, we notice that they often score
higher on ACJ on MMCU-Medical and CMB-Exam, and ROUGE-R
on CMB-Clin dataset, reflecting the efficacy of the LLMs’ inherent
knowledge and reasoning abilities as well as the importance of re-
moving irrelevant knowledge. Moreover, the ACJ value of KG-GPT
and QE is the second-to-best as they do not alter the semantics of
the user query. Therefore, the knowledge retrieved by KG-GPT and
QE have higher relevance with ACJ score, compared to CoK and
CoN. Furthermore, it is noticed that our proposed HyKGE surpasses
baselines with a performance gain of84.19%-378.71% and 133.33%-
345.22% on MMCU-Medical and CMB-Exam respectively, which
demonstrates our superiority in solving the misaligned knowledge
density between user query and retrieved knowledge. The marked
decline in ACJ of w/o Fragment also supports the HO Fragment
Granularity-aware reranker’s role in keeping relevant knowledge.
The BLEU and ROUGE-R scores on CMB-Clin also demonstrate
HyKGE’s superiority, indicating that HyKGE could be more appro-
priate for and aligned with real-life doctor consultations, proving
the effectiveness of HyKGE in information retrieval.
Can LLMs utilize retrieved knowledge to reduce hallucina-
tions? As for method KGRAG, it fails to perform well on PPL and
ROUGE-R, which is attributed to the provision of overly lengthy
retrieved knowledge and redundant noise, resulting in the inability
of the LLMs to extract useful information from the knowledge. The
performance test of baselines consistently shows that our proposed
HyKGE greatly reduces hallucinations and promotes LLMs to better
utilize the retrieved knowledge, with performance gain of 57.77%-
95.36% and 61.03%-185.42% on MMCU for PPL and ROUGE-R
respectively. We argue the reason that the retrieved knowledge is
more relevant and diverse because of the HOM and HO Fragment
Granularity-aware Reranker, and its chain structure also stimulates
the reasoning ability of LLMs. Other methods such as QE, CoN, and
CoK’s have been greatly reduced because their rerankers cannot
retain more diverse knowledge, resulting in LLMs’ answers being
too singular and ROUGE-R surely being lower. Notably, our per-
formance on the CMB-Exam test set was superior, due to its richer
and more detailed description of medical questions, allowing us to
obtain more diverse and relevant knowledge based on HOand Q.
5.5 Hyper-parameter Study (RQ4)
In this part, we concentrate on evaluating the influence of different
hyper-parameters on HyKGE for RQ4. Specifically, we perform a
series analysis of KG hop 𝑘 from the list [1,2,3,4,5]and reranker
𝑡𝑜𝑝𝐾 from the list [5,7,10,15,30,50]to verify the sensitive:
Figure 5 (Left.) depicts EM and the number of retrieved knowl-
edge before pruning. We observe that as𝑘increases, the amount of
knowledge retrieved explodes exponentially following a power-law
distribution[1, 9], exceeding 103 when 𝑘 = 5. However, an excessive
amount of knowledge not only fails to improve EM, but also bur-
dens LLMs with an increased number of tokens. Concurrently, EM
exhibits a trend of initial increase followed by a decrease as 𝑘 in-
creases. This phenomenon can be attributed to the fact that at lower
values of 𝑘, the retrieved knowledge predominantly consists of iso-
lated snippets of information, offering minimal utility. Conversely,
with larger 𝑘, the LLMs encounter limitations in comprehending
extensive reasoning chains, thereby rendering them incapable of ef-
fectively utilizing the complex and abundant retrieved information,
with the performance even worse than the base model. Besides, we
notice removing the HO will result in a substantial reduction in the
quantity of knowledge retrieved, because of the limited diversity
of knowledge obtained based solely on user query.



### Claim 102/179

#### Claim Text
This resulted in a laser spot diameter of roughly 660 nm .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[54]_2401.14887.pdf (Page 6):

The Power of Noise: Redefining Retrieval for RAG Systems SIGIR ’24, July 14–18, 2024, Washington, DC, USA
Table 3: Accuracy of Llama2-7b in configurations involving random Wikipedia documents and retrieved documents [I, , , Q].
Rows denote the number of random documents added, and columns show the quantity of retrieved documents . The left
section reports results using Contriever, and the right section using BM25. Scenarios where the prompt exceeded the model’s
input limit, leading to potential data truncation, are not included ( - ). Each value not marked with an asterisk * represents a
statistically significant change from the base case of retrieved documents only [I, , Q] (first row), as determined by a Wilcoxon
test (p-value < 0.01).
Contriever BM25
#
# 1 2 3 4 5 8 10 1 2 3 4 5 8 10
0 0.1620 0.1866 0.1876 0.1866 0.1921 0.2198 0.2108 0.2008 0.2208 0.2084 0.2028 0.2243 0.2492 0.2447
1 0.1308 0.1616 0.1717 0.1893* 0.1987* 0.2153* 0.2146* 0.1568 0.1963 0.1921 0.2115 0.2295* 0.2475* 0.2506*
2 0.1315 0.1644 0.1859* 0.2008 0.2174 0.2156* 0.2368 0.1644 0.1973 0.2080* 0.2281 0.2558 0.2495* 0.2596
3 0.1301 0.1727 0.2008 0.2316 0.2201 0.2198 0.2409 0.1568 0.2063 0.2160 0.2520 0.2579 0.2644 0.2707
5 0.1464 0.2056 0.2233 0.2240 0.2150 0.2451 0.2482 0.1772 0.2402 0.2437 0.2520 0.2554 0.2804 0.2866
8 0.1734 0.2066 0.2336 0.2375 0.2454 0.2416 0.2364 0.1994 0.2451 0.2579 0.2769 0.2817 0.2859 0.2777
10 0.1796 0.2174 0.2450 0.2502 0.2499 0.2420 - 0.2108 0.2589 0.2734 0.2835 0.2935 0.2853 -
15 0.2018 0.2354 0.2551 0.2530 - - - 0.2243 0.2686 0.2790 0.2928 - - -
16 0.2032 0.2471 0.2558 - - - - 0.2323 0.2662 0.2838 - - - -
17 0.2039 0.2426 - - - - - 0.2326 0.2693 - - - - -
18 0.2073 - - - - - - 0.2309 - - - - - -
Figure 3: This heatmap depicts the attention distribution
across the context documents from the example shown in
Figure 2, relative to the answer generated by Llama2-7b in
a prompt structured as [I, /u♀k, ⋆, Q]. Cell (i, j) denotes the
mean attention that tokens in the generated answer allocate
to the tokens of the i-th document within the j-th attention
layer. This mean attention for each document is calculated
by averaging the attention scores across all its constituent
tokens.
in the presence of noise, as can be seen in Table 2. Instead, we ob-
serve an improvement in performance under the best-performing
setting (near [I, , ⋆, Q]), with an improvement of 0.08 (+36%) in
LLM Input - Random and Gold ⋆
Task instruction...
Documents:
Document [140](Title: Richard Yates (novelist)) For much
of his life, Yates’s work met almost universal critical ac-
claim, yet not one of his books sold over 12,000 copies in...
Document [242] (Title: Android version history) Code
name Version number Initial release date API level Security
patches (No codename ) 1.0 September 23...
Document [3](Title: Millennium Falcon) Han Solo won
the Millennium Falcon from Lando Calrissian in the card
game sabacc...
Question: who owned the millennium falcon be-
fore han solo
Answer: Lando Calrissian
Figure 4: Example LLM input with a correct output, high-
lighted in green. The context of the prompt is composed of
random documents and the gold near the query. The task
instruction is as in Figure 1.
the case of MPT. Furthermore, we observe that different models
exhibit distinct behaviors. Both Llama2 and Phi-2 showed improve-
ments in this setting when the noise is introduced furthest from
the query. However, when the noise is positioned in the far[I, ⋆,
, Q] and mid [I, , ⋆, , Q] settings, these models exhibit a
decline in performance. Notably, this performance degradation is
much less accentuated when compared to the earlier setting with
distracting documents. This suggests that while Llama2 and Phi-2
can effectively handle noise far from the query, their ability to sift



Source: data\tc16_2312.10997v5\referenced_papers\[54]_2401.14887.pdf (Page 4):

The Power of Noise: Redefining Retrieval for RAG Systems SIGIR ’24, July 14–18, 2024, Washington, DC, USA
accurate or inaccurate based on the presence of the answer in a
binary fashion. Nevertheless, this evaluation strategy is not without
challenges. A principal issue arises in determining response cor-
rectness, particularly in instances involving date representations
or varying phrasings conveying identical meanings. For example,
if the LLM generates “Roosevelt” in response to a query where the
established correct answer is “President Roosevelt”, the response
would be deemed incorrect under our current evaluation schema.
Recognizing this limitation, we acknowledge the necessity for a
more advanced analysis of answer variations, which we leave to
future research.
5 RESULTS
Studying the characteristics of optimal prompts for RAG systems
corresponds to answering our research question (RQ): "What char-
acteristics are desirable in a retriever to optimize prompt construction
for RAG systems in order to increase the LLM effectiveness?" . More
specifically, we focus on three essential elements of the configura-
tion: type, number, and positioning of the documents, and for each,
we test various prompt combinations. To facilitate the understand-
ing of our experimental setup, we employ a streamlined schema for
representing the composition of prompts via the following symbols:
[I, ⋆, ὑ7, /u♀k, , Q]. The task instruction (I) and the query (Q) are
consistently positioned at the beginning and end, respectively. The
middle section varies and represents different contextual elements
- in this instance, these are gold, relevant, distracting, and random,
appearing in that specific sequence. Additionally, the number of
contextual documents is a variable in its own right and will be
reported in the results tables below.
5.1 Impact of Distracting Documents
LLM Input - Distracting /u♀kand Gold ⋆
Task Instruction...
Documents:
Document [1](Title: Han Solo) Before the events of the
film, he and Chewbacca had lost the “Millennium Falcon”
to thieves, but they reclaim the ship after it...
Document [2](Title: Millennium Falcon) The “Falcon” has
been depicted many times in the franchise, and ownership
has changed several times...
Document [3](Title: Millennium Falcon) Han Solo won
the Millennium Falcon from Lando Calrissian in the card
game sabacc...
Question: who owned the millennium falcon be-
fore han solo
Answer: Han Solo
Figure 2: Example LLM input with an erroneous output, high-
lighted in red. The context of the prompt is composed of
distracting documents and the gold near the query. The task
instruction is as in Figure 1.
In our first set of experiments, we use a selection of 10K queries
from the training set of the NQ-open dataset and assume an oracle
setup in which the gold document for the query is known. To this
effect, we add to the gold document a set of distracting documents,
i.e., documents with high retrieval scores but not containing the
answer, in order to measure their impact on the system; schemat-
ically [I, /u♀k, ⋆, Q] . Figure 2 shows an example of this setup’s
visualization. Results of this experiment are shown in Table 1 (far,
mid, and near relate to the distance between the gold document
and the query; more details in the following sub-section). A crit-
ical observation emerging from this analysis is a clear pattern of
progressive accuracy degradation as the number of distracting doc-
uments included in the context increases. This was observed across
all LLMs, with accuracy deteriorating by more than 0.38 ( −67%)
in some cases. Even more importantly, adding just one distracting
document causes a sharp reduction in accuracy, with peaks of 0.24
(−25%), as can be seen by comparing the row with 0 distracting
documents (only gold scenario, as seen in Figure 1) with that of 1
distracting document. This experiment highlights a critical issue
for RAG systems, particularly in real-world IR settings where re-
lated but non-answer-containing documents are commonplace. Our
empirical analysis suggests that introducing semantically aligned
yet non-relevant documents adds a layer of complexity, potentially
misguiding LLMs away from the correct response. A visual ex-
planation can be seen in Figure 3, which illustrates the attention
scores within the prompt’s context for a specific example in which
the LLM incorrectly answers. This figure highlights the model’s
disproportionate focus on a distracting document (leftmost) at the
expense of the gold document (rightmost), likely contributing to
the erroneous response. Note that for consistency of results across
LLMs, we need to account for their various input token capabilities:
Llama2 can process up to 4096 tokens, but other models are lim-
ited to 2048 tokens. This led to the exclusion of evaluations with a
higher number of distracting documents (namely greater than 10)
as reflected by the empty values in the tables.
In addition, we wanted to verify that our results were not overly
dependent on the type of dense retrieval system we used. We
wanted, in particular, to check whether another dense retriever
specifically trained on “hard negatives" would better distinguish
between directly relevant and distracting documents, potentially
leading to different results. To explore this hypothesis, we used
ADORE [59], a state-of-the-art retriever trained with “dynamic
hard negatives”, to select the distracting documents. In scenarios
with 1, 2, and 4 distracting documents in the [I, /u♀k, ⋆, Q] setting
with Llama2, we obtain an accuracy of 0.4068, 0.3815, and 0.3626,
respectively. This is significantly lower than the baseline accuracy
of 0.5642, where no distracting documents were included, and than
the results obtained with Contriever in the same settings. We con-
clude from this that distinguishing between relevant and distracting
information is a hard problem that cannot be mitigated simply by
changing the dense retrieval method at this stage.
5.2 Impact of Gold Positioning
We conduct here another experiment where we systematically shift
the position of the gold document within the context to study its



Source: data\tc16_2312.10997v5\referenced_papers\[160]_2308.10633.pdf (Page 11):

Fact Check. Entity Linking Slot Filling Open Domain QA Dial.
Dataset FEV AY2 WnWi WnCw T-REx zsRE NQ HoPo TQA ELI5 WoW
Model / Metric Accuracy Exact Match RL F1
Llama2-70B (closed-book) 33.6(74.9) 39.8(54.5) 42.8(53.8) 39.2(55.7) 28.5(40.5) 11.3(13.6) 19.6(37.4) 13.9(25.1) 67.4(80.8) 23.0 13.3
RAG♢ 87.7 77.4 49.0 46.7 61.5 47.4 48.8 27.7 61.7 16.1 13.3
e5 + Llama2-70B 49.9(88.6) 51.2(57.9) 48.6(51.4) 45.6(51.4) 28.9(49.2) 35.0(43.2) 36.4(48.8) 28.1(35.8) 71.1(83.9) 21.5 13.2
3-action - 24.4 (70.0) - - 16.3 (46.8) - 36.9 (49.3) - - - -
Model / Metric KILT-Accuracy KILT-EM KILT-RLKILT-F1
RAG♢ 55.5 77.4 49.0 46.7 25.4 42.6 36.3 3.1 36.1 2.7 7.5
e5 + Llama2-70B 40.2(71.2) 51.2(51.2) 48.6(48.6) 45.5(45.5) 19.2(29.7) 32.8(40.4) 27.7(36.3) 11.3(14.5) 42.8(49.7) 2.7 8.1
3-action - 9.5 (27.7) - - 10.4 (27.9) - 28.0 (36.6) - - - -
Table 5: Downstream performance of the 3-action chain on KILT dev set along with baselines. The figures in
parentheses represent has_answer percentage, which corresponds to the proportion of questions with gold answers
included in the final output of the LLM. ♢: Results from Petroni et al. (2021).
Fact Check. Entity Linking Slot Filling Open Domain QA Dial.
Dataset FEV AY2 WnWi WnCw T-REx zsRE NQ HoPo TQA ELI5 WoW
Model / Metric Accuracy Exact Match RL F1
BART-large♢ 80.7 86.6 47.9 48.0 43.8 3.0 26.2 16.9 32.5 22.7 13.8
W-Vicuna-13B0.0(58.4) 0.1(52.2) 2.0(44.9) 0.0(48.1) 17.9(33.0) 5.9(8.5) 6.2(27.4) 1.7(17.1) 20.0(64.5) 22.7 12.7
Llama2-13B 26.3(50.7) 34.6(47.5) 35.0(42.8) 28.5(41.3) 26.9(36.7) 7.8(9.9) 11.5(29.1) 8.3(20.3) 43.0(70.2) 27.6 13.0
Llama2-70B 33.6(74.9) 39.8(54.5) 42.8(53.8) 39.2(55.7) 28.5(40.5) 11.3(13.6) 19.6(37.4) 13.9(25.1) 67.4(80.8) 23.0 13.3
Table 6: Downstream performance on KILT development set in aclosed-book setting (generation without retrieval).
Following Petroni et al. (2021), we report the results of typical metrics for each dataset, with bold indicating the
best result. The figures in parentheses represent has_answer percentage, which corresponds to the proportion of
questions with gold answers included in the final output of the LLM. ♢: Results from Petroni et al. (2021).
datasets, while our chosen LLMs have not. Despite
this, the LLMs demonstrate superior performance
compared to the baseline on several datasets.
Specifically, the Llama2-70B model outperforms
the BART baseline on the zsRE and TQA datasets,
and the Llama2-13B model outperforms the base-
line on the ELI5 dataset. This suggests that the
parametric knowledge embedded in the LLMs and
their capacity for text generation can be leveraged
effectively for knowledge-intensive tasks, even
zero-shot setting. Nevertheless, as described in
Section 4.2, retrieval augmentation can enhance
the performance on downstream tasks, except the
ELI5 dataset. We also present the closed-book per-
formances of several LLMs on the development set
of NQ dataset in Table 7.
A.7 Additional Results for Retrieval
Performance
Table 8 presents the recall@5 of the retrievers used
in our experiments. Note that even though m-
e5 outperforms e5 on the MTEB Retrieval task
(shown in Table 1), e5 still demonstrates superior
performance compared to m-e5 in terms of both
R-precision (shown in Table 3) and recall@5.
A.8 Details of Speed Analysis
Table 9 presents the details of speed analysis on
KILT development set. The search speed of BM25
(without REWRITE-EL) decreases as the total
number of words in a query increases. In con-
trast, for dense vector search, the search speed re-
mains relatively constant regardless of the size of
the query due to the fixed dimensionality of the
embedding vectors.
According to Table 9, the execution times re-
quired for generation with an LLM is longer than
the times required for retrieval, particularly when
generating lengthy responses such as ELI5 and
WoW. Therefore, it may seem counterintuitive that
the advantages of ANNS used in vector search are
not fully realized in terms of execution time of
R-LLMs. However, as previously discussed in Sec-
tion 4.4, DiskANN requires less memory compared
to other vector search algorithms, which means that
using such algorithm can actually help conserve
computational resources for R-LLM.
We observe that Llama2-13B requires more time



Source: data\tc16_2312.10997v5\referenced_papers\[47]_2305.17331.pdf (Page 13):

1 2 3 4 5 6 7 8 9 10
Number of documents
36
38
40
42
44MMLU Accuracy
Standalone LM
Lt=Flan-T5Base
(a) Flan-T5Base w/ AARANCE.
1 2 3 4 5 6 7 8 9 10
Number of documents
44
46
48
50
52MMLU Accuracy
Standalone LM
Lt=Flan-T5Larg e (b) Flan-T5Large w/ AARANCE.
1 2 3 4 5 6 7 8 9 10
Number of documents
50
52
54
56
58MMLU Accuracy
Standalone LM
Lt=Flan-T5X L (c) Flan-T5XL w/ AARANCE.
Figure 9: Relationship between LM’s performance and the number of augmentation documents.
Category Number
Ts
MSMARCO QA Open Domain QA 148122
KILT-FEVER Fact Checking 10444
KILT-WNED Entity Linking 3396
KILT-T-REx Slot Filling 5000
KILT-TriviaQA Open Domain QA 5359
KILT-Wizard of Wikipedia Dialogue 3054
Tt
MMLU Multi-task Language Understanding 1531
PopQA Open Domain QA 14267
Table 6: Configurations of our source tasks and target tasks.
Methods MMLU
All Hum. Soc. Sci. STEM Other
Flan-T5Base 36.1 40.4 39.8 27.0 40.6
Flan-T5Base Fine-tuning 36.1 38.9 41.2 27.9 39.9
Flan-T5Base w/ Contriever 43.7 44.4 45.0 36.4 51.1
Flan-T5Base w/ ANCE 43.0 44.2 44.3 34.5 51.9
Flan-T5Base w/ AARContriever (Ours) 44.4 44.7 47.7 35.8 52.2
Flan-T5Base w/ AARANCE (Ours) 44.8 42.2 46.4 39.0 53.2
Flan-T5Large 45.1 47.7 53.5 34.4 49.2
Flan-T5Large Fine-tuning 45.3 47.6 54.1 35.2 48.7
Flan-T5Large w/ Contriever 50.7 50.5 56.4 38.9 61.1
Flan-T5Large w/ ANCE 49.2 49.3 56.7 38.1 57.2
Flan-T5Large w/ AARContriever (Ours) 51.8 50.8 59.7 39.4 61.8
Flan-T5Large w/ AARANCE (Ours) 50.4 48.0 58.1 39.3 60.2
Flan-T5XL 51.2 55.5 57.4 38.1 58.7
Flan-T5XL w/ Contriever 56.4 57.3 66.1 43.9 63.2
Flan-T5XL w/ ANCE 55.3 55.9 64.0 41.5 64.9
Flan-T5XL w/ AARContriever (Ours) 56.7 57.7 65.4 43.6 65.1
Flan-T5XL w/ AARANCE (Ours) 56.2 59.4 64.8 41.5 64.9
InstructGPT 60.2 65.7 68.0 46.1 66.5
InstructGPT w/ Contriever 60.5 62.0 71.8 44.3 70.1
InstructGPT w/ ANCE 61.6 62.4 73.4 47.6 68.6
InstructGPT w/ AARContriever (Ours) 61.5 64.5 73.1 45.0 69.9
InstructGPT w/ AARANCE (Ours) 62.2 62.0 72.0 49.2 70.7
Table 7: Fine-tuning results on MMLU. We use the official auxiliary training data of MMLU to fine-tune the LM.



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 17):

0 50 100 150
1.4
1.6
1.8
2
2.2
2.4
2.6 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(a) ELI5 - p = 10%
0 50 100 150
1.35
1.4
1.45
1.5
1.55
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (b) ELI5 - p = 30%
0 50 100 150
1.04
1.06
1.08
1.1
1.12
1.14 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (c) ELI5 - p = 50%
0 10 20 30 40 50
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
3.2 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(d) MS MARCO - p = 10%
0 10 20 30 40 50
1.1
1.15
1.2
1.25
1.3
1.35 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.84
0.86
0.88
0.9
0.92 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (f) MS MARCO - p = 50%
0 20 40 60 80 100 120
1.5
2
2.5
3
3.5
4
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(g) NQ - p = 10%
0 20 40 60 80 100 120
1.5
1.6
1.7
1.8
1.9
2
2.1 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
Loa ding [ M a thJ a x ] / ex tensions/ M a thM enu .j s (h) NQ - p = 30%
0 20 40 60 80 100 120
1.25
1.3
1.35
1.4
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (i) NQ - p = 50%
Figure 9: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-24), for FiD-Large models.
Each row represents a different dataset, and every column represents a different filtering percentage (10,30,50).



### Claim 103/179

#### Claim Text
This significantly increases negative effects and reduces the effectiveness of stimulation .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[167]_2309.01431.pdf (Page 4):

English Chinese
Noise Ratio 0 0.2 0.4 0.6 0.8 0 0.2 0.4 0.6 0.8
ChatGPT (OpenAI 2022) 96.33 94.67 94.00 90.00 76.00 95.67 94.67 91.00 87.67 70.67
ChatGLM-6B (THUDM 2023a) 93.67 90.67 89.33 84.67 70.67 94.33 90.67 89.00 82.33 69.00
ChatGLM2-6B (THUDM 2023b)91.33 89.67 83.00 77.33 57.33 86.67 82.33 76.67 72.33 54.00
Vicuna-7B-v1.3 (Chiang et al. 2023)87.67 83.33 86.00 82.33 60.33 85.67 82.67 77.00 69.33 49.67
Qwen-7B-Chat (QwenLM 2023)94.33 91.67 91.00 87.67 73.67 94.00 92.33 88.00 84.33 68.67
BELLE-7B-2M (Yunjie Ji 2023)83.33 81.00 79.00 71.33 64.67 92.00 88.67 85.33 78.33 67.68
Table 1: The experimental result of noise robustness measured by accuracy (%) under different noise ratios. We can see that the
increasing noise rate poses a challenge for RAG in LLMs.
Long-distance information. Evidence uncertainty. Concept confusion.
QuestionWho did Iga Swiatek defeat to win the Qatar Open 2022?What is the name of Apple’s headset?What was Tesla’s revenue in Q1 2022?
AnswerAnett Kontaveit Vision Pro 18.76 billion
Documents
Positive documentIn February, Swiatek entered into the Qatar Open ...In the final, she won ...Anett Kontaveit...
Negative documentThis time, she defeated Ons Jabeur 6-2, 7-6(5) to winthe 2022 US Open, ...
Positive documentApple (AAPL.O) on Monday unveiled a costlyaugmented-reality headset called theVision Pro...
Negative document... is what Gurman believes will be calledApple Reality Pro. ...
Positive documentTesla, Inc. (TSLA) reported Q1 FY 2022 earnings results... detailed revenues of $18.76 billion...
Negative document...first-quarter earnings for 2022 ......Automotive revenue reached $16.86 billion...
ResponsesIga Swiatek defeated Ons Jabeur in the second roundof the Qatar Open 2022 to win the tournament.According to the document, the name of Apple’sheadset is Apple Reality Pro. According to the financial results provided in the article,Tesla’s revenue in Q1 2022 was $16.86 billion.
Table 2: Error cases of noise robustness, and only one positive document and one negative document are shown. The responses
are generated by ChatGLM2-6B. The blue text indicates the matching parts between the document and the question or answer,
while the red text highlights the non-matching parts.
structions to inform the model.). If the model generates this
content, it indicates that the model has detected erroneous
information in the document.
Error correction ratemeasures whether the model can
provide the correct answer after identifying errors for coun-
terfactual robustness. The model is asked to generate the cor-
rect answer after identifying the factual errors. If the model
generates the correct answer, it indicates that the model is
capable of correcting errors in the document.
Considering that the model may not fully adhere to in-
structions, for rejection rate and error detection rate, we
also use ChatGPT to conduct additional evaluation of the
answers. Specifically, we assess the model’s responses by
using instructions and demonstrations to determine if they
can reflect information that is not present in the document or
identify any factual errors.
Experiments
In this section, we evaluate the performance of various
LLMs, analyze and discuss the results, summarizing the
main challenges that existing LLMs encounter when using
external knowledge.
Settings
Task formats.Due to contextual limitations, we provide 5
external documents for each question. In our experiments
on noise robustness, we evaluate scenarios with noise ra-
tios ranging from 0 to 0.8. To comprehensively evaluate the
overall capabilities, we have adopted a unified instruction
for each language, as shown in Figure 3. The experiments
were conducted using an NVIDIA GeForce RTX 3090.
Models We conduct evaluation on 6 state-of-the-art
large language models which can generate both En-
glish and Chinese including ChatGPT (OpenAI 2022) 3,
ChatGLM-6B (THUDM 2023a), ChatGLM2-6B (THUDM
2023b), Vicuna-7b-v1.3 (Chiang et al. 2023), Qwen-7B-
Chat (QwenLM 2023), BELLE-7B-2M (Yunjie Ji 2023).
Results on Noise Robustness
We evaluated the accuracy based on the different noise ratios
in external documents, and the results are shown in Table 1.
We can see that:
(1) RAG can effect improve the responses of LLMs.
LLMs have shown strong performance even in the presence
of noise, indicating that RAG is a promising way for LLMs
to generate accurate and reliable responses.
(2) The increasing noise rate poses a challenge for
RAG in LLMs.Specifically, when the noise ratio exceeds
80%, the accuracy decreases significantly at a significance
level of 0.05. For example, the performance of ChatGPT has
decreased from 96.33% to 76.00%, while the performance
of ChatGLM2-6B has decreased from 91.33% to 57.33%.
Error Analysis. To better comprehend the negative im-
pact of noise on model generation, we examined the incor-
rect answers and found that these errors typically originate
from three reasons, as shown in Table 2.
(1) Long-distance information.LLMs often face diffi-
culty in identifying the correct answer from external docu-
ments when the information related to the question is distant
from the information related to the answer. This scenario
is quite common as longer texts are frequently encountered
3We use gpt-3.5-turbo api in the experiments.



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 11):

that this method simply mimics behavior without
learning a strategy to achieve the final goal. The
SFT process of LLMs can be viewed as a spe-
cial case of behavior cloning, where LLMs learn
the format and style of interaction by mimicking
humans. As for LLMs, despite having encoded
a substantial amount of knowledge into their pa-
rameters, there remains knowledge that surpasses
their capacity (Yin et al., 2023; Ren et al., 2023).
By cloning human behaviors during SFT, LLMs
learn to respond to all questions with a predom-
inantly positive tone, without assessing whether
these questions exceed their knowledge bound-
aries (see Figure 3). As a result, during inference,
if prompted to answer questions related to un-
learned knowledge, they are likely to confidently
produce hallucinations. One way to remit this
problem can be the honesty-oriented SFT, which
means introducing some honest samples into the
SFT data. The honest samples refer to responses
that admit incompetence, such as “Sorry, I don’t
know”. The Moss project (Sun et al., 2023b) open-
sourced their SFT data, which includes such hon-
est samples. We observed that models tuned with
them could learn to refuse to answer specific ques-
tions, therefore helping reduce hallucinations.
Summary & Discussion. Curating the training
data is one approach for mitigating hallucinations
during the SFT phase. Thanks to the acceptable
volume of SFT data, they can be manually curated
by human experts. Recently, we have performed
a preliminary human inspection and observed that
some widely-used synthetic SFT data, such as Al-
paca (Taori et al., 2023), contains a considerable
amount of hallucinated answers due to the lack of
human inspection. This calls for careful attention
when researchers try to build SFT datasets based
on self-instruct (Wang et al., 2023c).
Previous work also pointed out that the SFT
process may inadvertently introduce hallucina-
tions, by forcing LLMs to answer questions that
surpass their knowledge boundaries. Some re-
searchers have suggested honesty-oriented SFT as
a solution. However, we argue this method has two
main problems. Firstly, it exhibits limited gen-
eralization capabilities towards out-of-distribution
(OOD) cases. Secondly, the annotated honest
samples just reflect the incompetence and uncer-
tainty of annotators rather than those of LLMs, as
annotators are unaware of LLMs’ real knowledge
boundaries. Such challenges make solving this is-
Situation Reward Value
Unhedged Correct +1
Hedged Correct +0.5
Uninformative 0
Hedged Wrong -2
Unhedged Wrong -4
Table 8: An example of reward design for mitigating
LLM hallucinations through RL (Schulman, 2023).
sue during SFT sub-optimal.
5.3 Mitigation during RLHF
Nowadays, many researchers attempt to fur-
ther improve the supervised fine-tuned LLMs
via reinforcement learning from human feedback
(RLHF) (Fernandes et al., 2023). This process
consists of two steps: 1) train a reward model
(RW) as the proxy for human preference, which
aims to assign an appropriate reward value to each
LLM response; 2) optimize the SFT model with
the reward model’s feedback, by using RL algo-
rithms such as PPO (Schulman et al., 2017).
Leveraging human feedback not only closes the
gap between machine-generated content and hu-
man preference but also helps LLMs align with
desired criteria or goals. One commonly used
criterion today is “3H”, which denotes helpful,
honest, and harmless (Ouyang et al., 2022; Bai
et al., 2022; Zheng et al., 2023b). The hon-
est aspect here just refers to the minimization of
hallucinations in LLM responses. Current ad-
vanced LLMs, such as InstructGPT (Ouyang et al.,
2022), ChatGPT (OpenAI, 2023a), GPT4 (Ope-
nAI, 2023b), and Llama2-Chat (Touvron et al.,
2023b), have collectively considered this aspect
during RLHF. For example, GPT4 uses synthetic
hallucination data to train the reward model and
perform RL, which increases accuracy on Truth-
fulQA (Lin et al., 2021) from about 30% to 60%.
Moreover, Lightman et al. (2023) use the process
supervision to detect and mitigate hallucinations
for reasoning tasks, which provides feedback for
each intermediate reasoning step.
As discussed in the previous section, the phe-
nomenon of behavior cloning during the SFT stage
can potentially lead to hallucinations. Some re-
searchers have attempted to address this issue by
integrating honest samples into the original SFT
data. However, this approach has certain limita-
tions, such as unsatisfactory OOD generalization
capabilities and a misalignment between human
12



Source: data\tc16_2312.10997v5\referenced_papers\[167]_2309.01431.pdf (Page 5):

on the internet. In such cases, it is typical for the question’s
information to be initially presented at the start of the doc-
ument and subsequently referred to using pronouns. In Ta-
ble 2, the question information (“Qatar Open 2022”) is only
mentioned once at the beginning and is far from where the
answer text “Anett Kontaveit” appears. This situation may
cause LLMs to depend on information from other docu-
ments and create false impressions, i.e., hallucination.
(2) Evidence uncertainty. Before highly anticipated
events, like the release of new Apple products or the an-
nouncement of the Oscars, there is often a significant
amount of speculative information circulating on the inter-
net. Although the relevant documents explicitly state that
it is uncertain or speculative content, they can still impact
on the retrieval-augmented generation of LLMs. In Table 2,
when the noise ratio increases, the content of erroneous
documents is all about some people’s predictions about the
name of the headset (“Apple Reality Pro”). Even if there is
a correct answer (“Vision Pro”) in the relevant documents,
LLMs can still be misled by uncertain evidences.
(3) Concept confusion.The concepts in external docu-
ments may be similar to, but different from, the concepts in
the question. This can cause confusion for LLMs and make
LLMs generate incorrect answers. In Table 2, the model an-
swer focuses on the concept “automotive revenue” in the
document rather than “revenue” in the question.
Based on the analysis above, we have identified certain
limitations in LLMs regarding retrieval-augmented genera-
tion. To effectively handle the vast amount of noise present
on the internet, further detailed enhancements are required
for the model such as long documents modeling and precise
concept comprehension.
Results on Negative Rejection testbed
We evaluated the rejection rate when only noise documents
were provided. The results are shown in Table 3. In addi-
tion to evaluating the rejection rate through exact matching
(Rej in Table 3), we also utilize ChatGPT to determine if
the responses from the LLMs contain any rejection informa-
tion (Rej∗ in Table 3). We can see that: Negative Rejection
poses a challenge for RAG in LLMs.The highest rejection
rates for LLMs in English and Chinese were only 45% and
43.33%, respectively. This suggests that LLMs can be easily
misled by noisy documents, leading to incorrect answers.
In addition, through comparing Rej and Rej ∗, we found
that LLMs fail to strictly follow instructions, and they often
generate unpredictable responses, which make it hard to use
them as state triggers (such as for recognizing rejection).
We conduct case studies in Table 4. The first error is
because of Evidence uncertainty. Although the document
only mentions contact with “Adam McKay” and does not
explicitly state that he is the director of the movie, the
model still concludes that he holds this role. The first er-
ror is because of Concept confusion. The information pro-
vided in the answer pertains to “the 2018 Winter Olympics”
instead of “the 2022 Olympics” mentioned in the question.
Retrieval-augmented generation poses a greater challenge of
negative rejection compared to answer directly as it presents
relevant documents that could potentially mislead the LLMs
Languages English Chinese
Rej Rej ∗ Rej Rej ∗
ChatGPT 24.67 45.00 5.33 43.33
ChatGLM-6B 9.00 25.00 6.33 17.00
ChatGLM2-6B10.33 41.33 6.33 36.33
Vicuna-7B-v1.317.00 33.33 3.37 24.67
Qwen-7B-Chat31.00 35.67 8.67 25.33
BELLE-7B-2M5.67 32.33 5.33 13.67
Table 3: The result of negative rejection. Rej means the re-
jection rate (%) and Rej∗ means the rejection rate evaluated
by ChatGPT. We can see that negative rejection poses a chal-
lenge for RAG in LLMs.
Question Answer Response
who will directIrredeemable film?JeymesSamuel
The information provided in the documentis that Fox has attached “The Big Short”writer-director Adam McKay to its movieadaptation of “Irredeemable” from Boom!Comics. Therefore, the answer is Adam McKay.
Which country won themost medals at the2022 Winter Olympics?Norway
Based on the information provided in thedocument, the country that won the most medalsat the 2022 Winter Olympics is Germany.Germany has won a total of 31 medals, including 14gold medals, 14 bronze medals, and 31 silver medals.
Table 4: Error cases of negative rejection generated by
ChatGLM2-6B. The red text highlights the error answers.
and result in incorrect responses. In future developments, it
will be crucial for LLMs to enhance their ability to accu-
rately match questions with the appropriate documents.
Results on Information Integration testbed
We evaluated the accuracy based on the different noise ratios
in external documents, and the results are shown in Table 5.
When comparing the model to Table 1, we observed that
it has a weak information integration ability, which in turn
affects its noise robustness. We can see that:
(1) Information integration poses a challenge for RAG
in LLMs.Even without noise, the highest accuracy of LLMs
can only reach 60% and 67% for English and Chinese,
respectively. After adding noise, the highest accuracy de-
creases to 43% and 55%. These results suggest that LLMs
struggle with integrating information effectively and are not
well-suited for directly answering complex questions.
(2) Complex questions are more challenging for RAG
with noisy documents.Performance decline becomes sig-
nificant when the noise ratio is 0.4, but for simple problems,
a significant decline occurs only at a noise ratio of 0.8 at a
significance level of 0.05. This indicates that complex prob-
lems are more vulnerable to interference from noise. We
speculate that this is because solving complex problems re-
quires integrating information from multiple documents, and
this information can be considered as noise to each other,
making it harder for the model to extract relevant informa-
tion from the documents.
Error Analysis. We conducted an error analysis on
ChatGLM2-6B (noise ratio is 0). Apart from the similar er-
rors founded in the noise robustness experiment (38% of the
total), there are also three types of unique errors. We have
presented these cases in Table 6.



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 9):

able questions. Their empirical study reveals that
even the most advanced LLM, GPT4 (OpenAI,
2023b), shows a significant performance gap when
compared to humans. Ren et al. (2023) note a
correlation between accuracy and confidence, but
such confidence often surpasses the actual capa-
bilities of LLMs, namely over-confidence. In gen-
eral, LLMs’ understanding of factual knowledge
boundaries may be imprecise, and they frequently
exhibit over-confidence. Such over-confidence
misleads LLMs to fabricate answers with unwar-
ranted certainty.
Problematic alignment process could mislead
LLMs into hallucination. LLMs typically un-
dergo an alignment process following pre-training,
where they receive further training on curated
instruction-following examples to align their re-
sponses with human preferences. However, when
trained on instructions for which LLMs have not
acquired prerequisite knowledge from the pre-
training phase, this is actually a misalignment pro-
cess that encourages LLMs to hallucinate (Gold-
berg, 2023; Schulman, 2023). Another potential
issue is sycophancy, where LLMs may generate
responses that favor the user’s perspective rather
than providing correct or truthful answers, which
can result in hallucination (Perez et al., 2022; Rad-
hakrishnan et al., 2023; Wei et al., 2023b).
The generation strategy employed by LLMs
has potential risks. Today’s most advanced
LLMs generate responses sequentially, outputting
one token at a time. Zhang et al. (2023a) discover
that LLMs sometimes over-commit to their early
mistakes, even when they recognize they are in-
correct. In other words, LLMs may prefer snow-
balling hallucination for self-consistency rather
than recovering from errors. This phenomenon
is known as hallucination snowballing . Azaria
and Mitchell (2023) also contend that local opti-
mization (token prediction) does not necessarily
ensure global optimization (sequence prediction),
and early local predictions may lead LLMs into
situations where it becomes challenging to formu-
late a correct response. Lee et al. (2022) highlight
that the randomness introduced by sampling-based
generation strategies, such as top-p and top-k, can
also be a potential source of hallucination.
LLM Pre-train Data Size
GLM (Zeng et al., 2022) 400B tokens
BLOOM (Scao et al., 2022) 366B tokens
GPT-3 (Brown et al., 2020) 300B tokens
LLaMA (Touvron et al., 2023a) 1.4T tokens
Llama 2 (Touvron et al., 2023b) 2T tokens
Table 6: The pre-training data size of popular LLMs.
5 Mitigation of LLM Hallucination
In this section, we provide an extensive review of
recent studies focused on mitigating LLM halluci-
nations. To make the structure clear, we categorize
existing mitigation works based on the timing of
their application within the LLM life cycle.
5.1 Mitigation during Pre-training
Existing work (Zhou et al., 2023a) argues that the
knowledge of LLMs is mostly acquired during the
pre-training phase. The presence of noisy data
such as misinformation in the pre-training corpus
could corrupt the parametric knowledge of LLMs,
which is a significant factor contributing to hallu-
cinations, as previously discussed in § 4. Akyürek
et al. (2022) also demonstrate that it is possible to
trace the factual knowledge acquired by language
models back to their training data. Consequently,
an intuitive approach to mitigating hallucinations
could involve manually or automatically curating
the pre-training corpus to minimize unverifiable or
unreliable data as much as possible.
Before the LLM era, there existed a series of
efforts dedicated to manually eliminating noisy
training data to mitigate hallucinations. For in-
stance, Gardent et al. (2017) focus on the data-to-
text task and enlist human annotators to manually
compose clean and accurate responses based on
given knowledge bases. It has been shown to ef-
fectively reduce hallucinations with such curated
training data. Similarly, Wang (2019) manually
refine the text in existing table-to-text datasets and
observe that this process also substantially alle-
viates fact hallucinations. Besides, Parikh et al.
(2020) instruct annotators to revise verified sen-
tences from Wikipedia rather than directly creat-
ing new sentences when constructing table-to-text
training data. This approach has also been proven
to result in improved factuality of results.
With the advent of the LLM era, curating train-
ing data during pre-training has become increas-
ingly challenging due to the vast scale of pre-
training corpora (as exemplified in Table 6). For
10



Source: data\tc16_2312.10997v5\referenced_papers\[54]_2401.14887.pdf (Page 5):

SIGIR ’24, July 14–18, 2024, Washington, DC, USA Cuconasu and Trappolini, et al.
Table 1: Accuracy results of the LLMs when evaluated with prompts composed of the gold document ⋆and a varying number
of distracting /u♀kdocuments. The table illustrates how the inclusion of an increasing number of distracting documents affects
LLM’s performance. Scenarios where the prompt exceeded the model’s input limit, leading to potential data truncation, are not
included ( - ). All values not marked with an asterisk * denote statistically significant changes from the gold-only document
scenario [I, ⋆, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01). Additionally, the closed-book accuracy scores
for the models are as follows: Llama2 (0.1123), MPT (0.1205), Phi-2 (0.0488), Falcon (0.1083).
Far - [I, ⋆, /u♀k, Q] Mid - [I, /u♀k, ⋆, /u♀k, Q] Near - [I, /u♀k, ⋆, Q]
# /u♀kLlama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon
0 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330
1 0.4586 0.1976 0.3585 0.3469 no-mid no-mid no-mid no-mid 0.4283 0.1791 0.4227 0.3602
2 0.3455 0.1913 0.3430 0.3246 0.3322 0.1802 0.3375 0.2823 0.3974 0.2002 0.3975 0.3111
4 0.2745 0.2209* 0.3019 0.2670 0.2857 0.1775 0.2885 0.2378 0.3795 0.2059* 0.3701 0.2736
6 0.2898 0.2171* 0.2943 0.2392 0.2698 0.1424 0.2625 0.2103 0.3880 0.1892 0.3623 0.2656
8 0.2643 0.2077* 0.2513 0.1878 0.2268 0.1002 0.2360 0.1745 0.3748 0.1944 0.3423 0.2424
10 0.2537 - - - 0.2180 - - - 0.3716 - - -
12 0.2688 - - - 0.2382 - - - 0.3991 - - -
14 0.2583 - - - 0.2280 - - - 0.4118 - - -
16 0.2413 - - - 0.2024 - - - 0.3889 - - -
18 0.2348 - - - 0.1795 - - - 0.3781 - - -
Table 2: Accuracy results of the LLMs when evaluated with prompts composed of the gold document ⋆and a varying number
of random documents. Surprisingly, increasing the number of random documents in the Near setting improves LLM’s
performance. Scenarios where the prompt exceeded the model’s input limit, leading to potential data truncation, are not
included ( - ). All values not marked with an asterisk * denote statistically significant changes from the gold-only document
scenario [I, ⋆, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01). Additionally, the closed-book accuracy scores
for the models are as follows: Llama2 (0.1123), MPT (0.1205), Phi-2 (0.0488), Falcon (0.1083).
Far - [I, ⋆, , Q] Mid - [I, , ⋆, , Q] Near - [I, , ⋆, Q]
# Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon
0 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330
1 0.4733 0.2447 0.4329 0.4035 no-mid no-mid no-mid no-mid 0.4862 0.2125* 0.4587 0.4091
2 0.3776 0.2639 0.4249 0.3805 0.3928 0.2584 0.4293 0.3612 0.5032 0.2660 0.4614 0.3912
4 0.3109 0.2933 0.4091 0.3468 0.3998 0.2577 0.3985 0.3462 0.5221 0.2930 0.4311 0.3949
6 0.3547 0.3036 0.4130 0.3250 0.4138 0.2265 0.3891 0.3196 0.5681* 0.2890 0.4388 0.3908
8 0.3106 0.3039 0.3812 0.2543 0.3734 0.1566 0.3596 0.2767 0.5609* 0.2911 0.4258 0.3704
10 0.3390 - - - 0.3675 - - - 0.5579* - - -
12 0.3736 - - - 0.3641 - - - 0.5836 - - -
14 0.3527 - - - 0.3372 - - - 0.5859 - - -
16 0.3401 - - - 0.3159 - - - 0.5722 - - -
18 0.3466 - - - 0.2982 - - - 0.5588* - - -
impact on the model’s effectiveness. We define the positions of the
gold document as follows:
•Near: placed adjacent to the query in the prompt [I, /u♀k, ⋆,
Q] (as in Figure 2)
•Mid: inserted in the middle of the context [I, /u♀k, ⋆, /u♀k, Q]
•Far: positioned as far as possible from the query in the con-
text [I, ⋆, /u♀k, Q]
Results in these settings partially corroborate evidence from [30].
The accuracy is higher when the gold document is near the query,
lower when the gold document is furthest from it, and lowest when
the gold document is placed in the middle of the context. For in-
stance, Llama2, with 18 distracting documents, reaches an accuracy
of 0.37, 0.23, and 0.17, respectively. These results are consistent
across all models tested in the setting with distracting documents.
5.3 Impact of Noise
We devise an additional experimental setting aimed at evaluating
the robustness of the RAG system against noise. To this effect, we
take the gold document and add to it a certain number of docu-
ments picked at random from the corpus; see an example in Figure
4. Against our expectations, the performance does not deteriorate



### Claim 104/179

#### Claim Text
This exceeds the optical bullet diameter of 280 ±20 µm , but corresponds to the width of the top hat profile.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 16):

The intersection point is sensitive to 
the precise power-law parameters
Figure 15 Far beyond the model sizes we study empirically, we ﬁnd a contradiction between our equations
for L(Cmin) and L(D) due to the slow growth of data needed for compute-efﬁcient training. The intersection
marks the point before which we expect our predictions to break down. The location of this point is highly
sensitive to the precise exponents from our power-law ﬁts.
6.3 Contradictions and a Conjecture
We observe no signs of deviation from straight power-law trends at large values of compute, data, or model
size. Our trends must eventually level off, though, since natural language has non-zero entropy.
Indeed, the trends for compute-efﬁcient training described in this section already contain an apparent contra-
diction. At scales several orders of magnitude above those documented here, the performance predicted by
the L(Cmin) scaling law decreases below what should be possible given the slow growth in training data with
compute. This implies that our scaling laws must break down before this point, but we conjecture that the
intersection point has a deeper meaning: it provides an estimate of the point at which Transformer language
models reach maximal performance.
Since the amount of data used by compute-efﬁcient training grows slowly with the compute budget, the
performance predicted by L(Cmin) eventually hits a lower bound set by theL(D) power law (see Figure 15).
Let us work this out in more detail.
To keep overﬁtting under control, the results of Section 4 imply that we should scale the dataset size as
D∝N0.74 ∝C0.54
min (6.6)
where we have used the compute-efﬁcient N(Cmin) from Figure 14.
Let us compare this to the data requirements of compute-efﬁcient training. If we train at the critical batch
size (i.e. C = 2Cmin) and never re-use data during training, we ﬁnd that data usage grows with compute as
D(Cmin) = 2Cmin
6N(Cmin) ≈
(
4 ×1010 tokens
)
(Cmin/PF-Day)0.26 (6.7)
This is the maximum rate at which the dataset size can productively grow with compute, since it means that
we are only training for a single epoch. But it grows the dataset much more slowly than in Equation (6.6).
It appears to imply that compute-efﬁcient training will eventually run into a problem with overﬁtting, even if
the training process never re-uses any data!
According to Figure 1, we expect that when we are bottlenecked by the dataset size (ie by overﬁtting), the
loss should scale asL(D) ∝D−0.095. This implies that the loss would scale with compute asL(D(Cmin)) ∝
C−0.03
min once we are data-limited. Once again, we have a contradiction, as this will eventually intersect with
our prediction for L(Cmin) from Figure 13, where we found a scaling L(Cmin) ∝C−0.050
min .
The intersection point of L(D(Cmin)) and L(Cmin) occurs at
C∗∼104 PF-Days N∗∼1012 parameters, D ∗∼1012 tokens, L ∗∼1.7 nats/token (6.8)
though the numerical values are highly uncertain, varying by an order or magnitude in either direction de-
pending on the precise values of the exponents from the power-law ﬁts. The most obvious interpretation is
that our scaling laws break down at or before we reach this point, which is still many orders of magnitude
away in both compute and model size.
17



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 22):

103 104 105
Sc × [L(N, D) L(N, )] 1/ S
103
104
105
Sstop
Early Stopping Step
Data Size
21M
43M
86M
172M
344M
688M
1.4B
103 104 105
Step
2
3
4
5
6Loss
Test Loss
Train Loss
108
109
1010
Dataset Size (Tokens)
Figure 16 Left: We characterize the step on which early stopping occurs, as a function of the extent of
overﬁtting. The red line indicates a lower bound for early stopping that is derived in Section 5.3. Right:
We display train and test loss for a series of 300M parameter models trained on different sized dataset sub-
samples. The test loss typically follows that of a run done with unrestricted data until diverging. Note that the
degree of overﬁtting (as compared to the inﬁnite data limit) is signiﬁcantly overestimated by Ltest −Ltrain
(denoted by a black bar for each run).
• We are not especially conﬁdent in the prediction of Bcrit(L) for values of the loss far outside the
range we have explored. Changes in Bcrit could have a signiﬁcant impact on trade-offs between
data parallelism and the number of serial training steps required, which would have a major impact
on training time.
• We did not thoroughly investigate the small data regime, and our ﬁts for L(N,D) were poor for
the smallest values of D (where an epoch corresponded to only 40 steps). Furthermore, we did
not experiment with regularization and data augmentation. Improvements in these could alter our
results, quantitatively or qualitatively.
• We used the estimated training compute C ≈6NBS, which did not include contributions propor-
tional to nctx (see Section 2.1). So our scalings with compute may be confounded in practice in the
regime of very large nctx, speciﬁcally where nctx ≳ 12dmodel.
• We tuned learning rates, and we experimented with learning rate schedules. But we may have
neglected to tune some hyperparameter (e.g. intialization scale or momentum) that have an important
effect on scaling.
• The optimal choice of learning rate is sensitive to the target loss. When training close to convergence,
it may be necessary to use a smaller learning rate to avoid divergences. But when conducting a short
training run (eg due to compute limitations), it may be possible to use a larger learning rate. We did
not experiment with higher learning rates for training runs that did not proceed to convergence.
D Supplemental Figures
D.1 Early Stopping and Test vs Train
In section 5.3 we described the result shown in Figure 16, which provides a prediction for a lower bound on
the early stopping step. We also show the train and test loss for a given model size when training on different
sized datasets.
D.2 Universal Transformers
We compare the performance of standard Transformers to recurrent Transformers [DGV +18] in Figure 17.
These models re-use parameters, and so perform slightly better as a function of N, but slightly worse as a
function of compute C. We include several different different possibilities for parameter re-use.
D.3 Batch Size
We measure the critical batch size using the data displayed in ﬁgure 18. This made it possible to estimate
Bcrit(L) in ﬁgure 10.
23



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 10):

106 107 108 109
Params (non-embed)
2.5
3.0
3.5
4.0
4.5Test Loss
Data Size Bottleneck
Data Size
21M
43M
86M
172M
344M
688M
1.4B
22.0B
10 4
 10 3
 10 2
 10 1
N N/ D/D
0.0
0.1
0.2
0.3
0.4
0.5L/L(D = ) 1
Overfitting
Data Size
21M
43M
86M
172M
344M
688M
1.4B
22.0B
Figure 9 The early-stopped test loss L(N,D) depends predictably on the dataset size Dand model size N
according to Equation (1.5). Left: For large D, performance is a straight power law inN. For a smaller ﬁxed
D, performance stops improving as N increases and the model begins to overﬁt. (The reverse is also true,
see Figure 4.) Right: The extent of overﬁtting depends predominantly on the ratio N
αN
αD /D, as predicted in
equation (4.3). The line is our ﬁt to that equation.
Since we stop training early when the test loss ceases to improve and optimize all models in the same way, we
expect that larger models should always perform better than smaller models. But with ﬁxed ﬁnite D, we also
do not expect any model to be capable of approaching the best possible loss (ie the entropy of text). Similarly,
a model with ﬁxed size will be capacity-limited. These considerations motivate our second principle. Note
that knowledge of L(N) at inﬁnite Dand L(D) at inﬁnite N fully determines all the parameters in L(N,D).
The third principle is more speculative. There is a simple and general reason one might expect overﬁtting
to scale ∝1/D at very large D. Overﬁtting should be related to the variance or the signal-to-noise ratio
of the dataset [AS17], and this scales as 1/D. This expectation should hold for any smooth loss function,
since we expect to be able to expand the loss about the D →∞ limit. However, this argument assumes that
1/Dcorrections dominate over other sources of variance, such as the ﬁnite batch size and other limits on the
efﬁcacy of optimization. Without empirical conﬁrmation, we would not be very conﬁdent of its applicability.
Our third principle explains the asymmetry between the roles of N and D in Equation (1.5). Very similar
symmetric expressions4 are possible, but they would not have a 1/D expansion with integer powers, and
would require the introduction of an additional parameter.
In any case, we will see that our equation for L(N,D) ﬁts the data well, which is the most important justiﬁ-
cation for our L(N,D) ansatz.
4.2 Results
We regularize all our models with 10% dropout, and by tracking test loss and stopping once it is no longer
decreasing. The results are displayed in Figure 9, including a ﬁt to the four parameters αN,αD,Nc,Dc in
Equation (1.5):
Parameter αN αD Nc Dc
Value 0.076 0.103 6.4 ×1013 1.8 ×1013
Table 2 Fits to L(N,D)
We obtain an excellent ﬁt, with the exception of the runs where the dataset has been reduced by a factor of
1024, to about 2 ×107 tokens. With such a small dataset, an epoch consists of only 40 parameter updates.
Perhaps such a tiny dataset represents a different regime for language modeling, as overﬁtting happens very
early in training (see Figure 16). Also note that the parameters differ very slightly from those obtained in
Section 3, as here we are ﬁtting the full L(N,D) rather than just L(N,∞) or L(∞,D).
To chart the borderlands of the inﬁnite data limit, we can directly study the extent of overﬁtting. For all but
the largest models, we see no sign of overﬁtting when training with the full 22B token WebText2 dataset,
so we can take it as representative of D = ∞. Thus we can compare ﬁnite D to the inﬁnite data limit by
4For example, one might have used L(N, D) =
[( Nc
N
)αN
+
( Dc
D
)αD]β
, but this does not have a 1/D expansion.
11



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 7):

Feed-Forward Ratio (dff / dmodel) 
50M Parameters Aspect Ratio (dmodel / nlayer) Attention Head Dimension (dmodel / nhead) 
25M Parameters
10%
8%
6%
4%
2%
0% Loss Increase
A wide range of architectures 
achieve similar performance
22% additional compute
compensates for 1% loss increase
Figure 5 Performance depends very mildly on model shape when the total number of non-embedding
parameters N is held ﬁxed. The loss varies only a few percent over a wide range of shapes. Small differences
in parameter counts are compensated for by using the ﬁt to L(N) as a baseline. Aspect ratio in particular can
vary by a factor of 40 while only slightly impacting performance; an (nlayer,dmodel) = (6,4288) reaches a
loss within 3% of the (48,1600) model used in [RWC+19].
106 107 108 109
Parameters (with embedding)
2
3
4
5
6
7Test Loss
0 Layer
1 Layer
2 Layers
3 Layers
6 Layers
> 6 Layers
103 104 105 106 107 108 109
Parameters (non-embedding)
2
3
4
5
6
7Test Loss
1 Layer
2 Layers
3 Layers
6 Layers
> 6 Layers
Figure 6 Left: When we include embedding parameters, performance appears to depend strongly on the
number of layers in addition to the number of parameters. Right: When we exclude embedding parameters,
the performance of models with different depths converge to a single trend. Only models with fewer than 2
layers or with extreme depth-to-width ratios deviate signiﬁcantly from the trend.
In this section we will display data along with empirically-motivated ﬁts, deferring theoretical analysis to
later sections.
3.1 Approximate Transformer Shape and Hyperparameter Independence
Transformer performance depends very weakly on the shape parametersnlayer,nheads, and dﬀ when we hold
the total non-embedding parameter count N ﬁxed. To establish these results we trained models with ﬁxed
size while varying a single hyperparameter. This was simplest for the case of nheads. When varying nlayer,
we simultaneously varied dmodel while keeping N ≈12nlayerd2
model ﬁxed. Similarly, to vary dﬀ at ﬁxed
model size we also simultaneously varied the dmodel parameter, as required by the parameter counts in Table
1. Independence of nlayers would follow if deeper Transformers effectively behave as ensembles of shallower
models, as has been suggested for ResNets [VWB16]. The results are shown in Figure 5.
3.2 Performance with Non-Embedding Parameter Count N
In Figure 6 we display the performance of a wide variety of models, ranging from small models with shape
(nlayer,dmodel) = (2,128) through billion-parameter models, ranging in shape from (6,4288) through
(207,768). Here we have trained to near convergence on the full WebText2 dataset and observe no over-
ﬁtting (except possibly for the very largest models).
As shown in Figure 1, we ﬁnd a steady trend with non-embedding parameter countN, which can be ﬁt to the
ﬁrst term of Equation (1.5), so that
L(N) ≈
(Nc
N
)αN
(3.1)
8



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 14):

Models between 0.6x and 2.2x the 
optimal size can be trained with a 
20% larger compute budget
Smaller models require 
more steps to train, while 
larger models require fewer
Our framework does not 
capture early training dynamics
Figure 12 Left: Given a ﬁxed compute budget, a particular model size is optimal, though somewhat larger
or smaller models can be trained with minimal additional compute. Right: Models larger than the compute-
efﬁcient size require fewer steps to train, allowing for potentially faster training if sufﬁcient additional paral-
lelism is possible. Note that this equation should not be trusted for very large models, as it is only valid in the
power-law region of the learning curve, after initial transient effects.
10 8
 10 6
 10 4
 10 2
 100
Compute (PF-days), non-embedding
2
3
4
5
6
7Test Loss
L = (Cmin/2.3 108) 0.050
L = (C/2.0 107) 0.057
Figure 13 When adjusting performance to simulate training far below the critical batch size, we ﬁnd a
somewhat altered power law for L(Cmin) when compared with the fully empirical results. The conspicuous
lump at 10−5 PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks
in the power-law ﬁts. It is the L(Cmin) trend that we expect to provide a reliable extrapolation for larger
compute.
that in fact we could train more efﬁciently 6 by training at the batch size Bcrit discussed in Section 5.1.
Large and small values of the loss could have been achieved with fewer samples or fewer steps, respectively,
and correcting for this inefﬁciency by standardizing to the critical batch size results in cleaner and more
predictable trends.
In this section we will adjust for this oversight. More importantly, we will use the results of Section 5
to determine the optimal allocation of compute between model size N and the quantity of data processed
during training, namely 2BcritSmin. We will determine this allocation both empirically and theoretically, by
using the equation for L(N,Smin), and we will demonstrate that these methods agree.
6.1 Optimal Performance and Allocations
Let us ﬁrst study the loss as a function of the optimally allocated compute from Equation (5.5). The result is
plotted in Figure 13, along with a power-law ﬁt. We see that as compared to the compute plot of Figure 1, the
new ﬁt with Cmin is somewhat improved.
Given L(Cmin), it is natural to ask for the optimal model sizeN(Cmin) that provides the minimal loss with a
given quantity of training compute. The optimal model size is shown in Figure 14. We observe thatN(Cmin)
6One might ask why we did not simply train at Bcrit in the ﬁrst place. The reason is that it depends not only on the
model but also on the target value of the loss we wish to achieve, and so is a moving target.
15



### Claim 105/179

#### Claim Text
Note that the IBM data is not identical to that presented in ; it has been recomputed here with the procedure described in such that the Galileo number matches the nominal value Ga = 178.46 exactly. 24 5 Particle-resolved DNS methods and the angular velocity, which are not shown), which can be attributed to the different lateral boundary conditions (periodic vs. zero-stress) and to the shape of the computational domain (cuboid vs. cylindrical).

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 13):

0 1000 2000 3000 4000
24.5
25
25.5
26
26.5
27
FLOPS (MACs)
Rouge-L
2%
(a) ELI5
0 1000 2000 3000 4000
20
21
22
23
24
25
FLOPS (MACs)
2% (b) MS MARCO
0 1000 2000 3000 4000
24
25
26
27
28
29
30
31
32
FLOPS (MACs)
2% (c) NQ
0 20 40 60 80 100
24.5
25
25.5
26
26.5
27
Input P sgs
Rouge-L
2%
(d) ELI5
0 20 40 60 80 100
20
21
22
23
24
25
Input P sgs
2% (e) MS MARCO
0 20 40 60 80 100
24
25
26
27
28
29
30
31
32
FiD
CALM
T ok en Filtering
Combined
Input P sgs
2% (f) NQ
Figure 6: The ROUGE-L performance on FiD-Base vs. the FLOPS (MACs) (First row), and vs. the input passage
amount (Input Psg., second row). The results are on the test set for each dataset, for the various methods utilized.
We observe that the trends of the FLOPS (MACs) and the Input Psg. are very similar, since the passages effect the
encoder the most, and the encoder has the most impact on FLOPS (MACs) (as stated by de Jong et al. (2022)).



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 18):

Table 11: Confidence region for SOTA model in XSum and BigPatent.
System ROUGE-1/2/L 95%-conf.int
XSum
BRIOjoint
50.3 0.49986 - 0.50602
26.7 0.26300 - 0.26989
41.6 0.41231 - 0.41900
BigPatent
BARTjoint
62.9 0.62664 - 0.63080
48.1 0.47783 - 0.48333
59.6 0.59401 - 0.59847
Table 12: Generation Latency analysis.
NMT XSum BigPatent DailyDialog
Average Input Length 87 512 1024 71
Average Output Length 44 75 127 16
CPU
Retrieval-augmented Baseline 0.97 1.79 3.16 0.32
Selfmem
Candidate Generation 3.20 7.50 15.00 1.02
Memory Selection 0.50 0.52 0.95 0.14
Hypothesis Generation 0.97 1.79 3.00 0.32
×4.80 ×5.47 ×6.04 ×4.63
CUDA
Retrieval-augmented Baseline 0.29 0.44 0.75 0.10
Selfmem
Candidate Generation 0.51 1.00 1.72 0.18
Memory Selection 0.01 0.01 0.01 0.01
Hypothesis Generation 0.29 0.44 0.75 0.10
×2.76 ×2.99 ×3.35 ×2.91
19



Source: data\tc16_2312.10997v5\referenced_papers\[38]_2307.07164.pdf (Page 13):

Input
What happens next in this paragraph? How to survive remedial classes Look at the course as an opportunity.
Many students are discouraged when they are assigned to a remedial class. Some assume this placement
means they aren’t ready for college. OPTIONS:
A) However, people who are not unable to do what they’re given on campus, or those who are cut out
from college academies, are likely to have some little snitches. You want to be prepared for a negative
outcome if possible.
B) In this case, you should consider what you will do if your subject consists of a certain term or number
of subject areas. You could set up a study study program yourself or tutor a student who is struggling to
thoroughly comprehend where they sat for homework.
C) If you take the course, you might find you feel highly motivated after passing the test. Try to develop a
positive attitude towards the course so that you are not discouraged when you take your homework at the
end of the day.
D) However, being assigned a remedial class doesn’t mean that you are behind, just that you have an
opportunity to receive better instruction and improve your skills in a subject that you have struggled
with in the past. There is nothing unusual about being asked to attend a remedial course: two thirds of
community college students take at least one remedial course.
Output D
Table 9: Input-output format for GPT-35-Turbo. This example is from the HellaSwag dataset. We add some line
breaks for better readability.
Task Zero-shot Random Kmeans BM25 E5 base SBERT EPR LLM-R
1 iter 2 iter 3 iter
AESLC 5.8 19.4 19.0 26.8 27.0 25.3 26.0 26.7 27.3 27.1
AGNews 31.5 67.4 71.9 90.6 90.6 90.2 91.8 92.4 93.5 93.5
ARC Chall. 35.6 39.7 40.5 40.3 44.6 42.8 43.0 43.4 43.6 44.0
ARC Easy 51.0 60.0 61.8 59.9 63.0 63.1 63.1 63.6 63.3 63.6
BoolQ 64.7 70.0 69.0 74.7 72.4 73.9 74.8 75.6 75.1 74.1
CommonGen 19.2 36.3 34.4 37.6 37.4 37.6 39.2 38.2 37.7 37.3
COPA 66.0 80.0 85.0 78.0 83.0 82.0 82.0 84.0 84.0 84.0
DART 22.9 52.0 46.6 55.9 54.7 54.4 56.2 57.3 57.2 57.3
E2E NLG 34.6 52.7 46.4 54.5 51.8 50.2 53.6 54.9 54.7 54.9
Gigaword 15.3 30.0 30.7 32.7 32.5 32.6 32.4 33.3 32.5 31.8
HellaSwag 71.5 73.9 74.0 74.9 75.2 75.3 75.2 75.4 75.5 75.4
MNLI (m) 35.8 46.3 44.2 50.1 44.5 50.8 59.9 68.2 70.2 69.8
MNLI (mm) 35.6 48.1 45.4 48.3 44.7 49.3 61.5 69.5 72.0 71.3
MRPC 69.1 49.5 38.0 61.8 41.2 52.7 55.9 62.3 75.3 78.2
MultiRC 57.0 48.5 34.1 54.2 56.0 55.3 50.4 52.9 51.5 52.1
NQ 0.3 21.5 22.6 37.6 39.3 39.4 39.2 39.4 39.1 39.2
OpenBook QA 41.6 49.8 49.0 49.6 51.4 51.4 49.6 50.8 52.2 53.4
PAWS 53.2 57.0 56.6 56.6 55.4 58.2 57.7 57.0 56.6 57.0
PIQA 77.0 79.1 79.4 81.3 81.3 80.7 80.5 80.9 81.6 80.6
QNLI 49.2 56.4 53.4 62.2 61.5 61.9 65.0 74.4 69.6 69.4
QQP 57.7 63.4 63.3 79.8 77.5 81.3 81.7 80.1 82.6 83.3
RTE 59.6 59.9 58.5 65.7 63.9 67.2 66.8 67.2 68.6 70.4
Sentiment140 49.3 88.6 89.4 90.8 93.9 92.2 91.4 90.8 91.1 90.3
SNLI 39.8 43.7 52.5 47.1 53.5 58.4 68.4 80.2 82.0 82.2
SQuAD v1 2.1 64.1 62.3 61.2 60.8 61.6 64.3 60.7 57.3 52.5
SST2 54.4 85.9 89.7 84.4 92.1 87.6 88.7 94.0 93.8 93.1
Winogrande 62.0 66.7 66.5 67.5 66.9 66.5 66.5 67.9 68.1 67.2
WSC 64.4 60.6 56.7 56.7 61.5 63.5 61.5 60.6 63.5 66.4
WSC273 74.0 74.4 74.7 64.5 65.2 62.6 65.2 74.4 79.5 78.8
Yelp 47.9 92.0 93.5 93.5 97.3 95.9 95.1 95.7 95.9 95.5
Average 44.9 57.9 57.0 61.3 61.4 62.1 63.5 65.7 66.5 66.4
Table 10: Detailed results for each dataset.



Source: data\tc16_2312.10997v5\referenced_papers\[160]_2308.10633.pdf (Page 13):

Fact Check. Entity Linking Slot Filling Open Domain QA Dial.
Tasks FEV AY2 WnWi WnCw T-REx zsRE NQ HoPo TQA ELI5 WoWAvg.
Models Completion in Closed-Book Setting (in seconds per question)
W-Vicuna-13B 1.565 13.040 10.870 9.793 0.983 1.142 2.165 1.969 1.414 22.820 7.122 6.626
Llama2-13B 0.625 1.077 1.036 1.201 0.940 0.913 1.270 1.185 1.014 40.100 9.522 5.353
Llama2-70B 1.765 2.936 2.745 2.618 1.953 2.031 2.285 2.188 1.877 42.500 11.100 6.727
Retrieval + Generation (in seconds per question)
e5 + W-Vicuna-13B 1.529 1.310 1.368 1.158 1.192 1.453 2.595 1.945 1.734 15.480 10.850 3.692
e5 + Llama2-13B 1.084 1.165 1.209 1.046 1.300 1.407 1.284 1.975 9.830 32.48 16.76 6.322
BM25 + Llama2-70B 1.841 0.008 0.009 0.008 2.015 2.296 2.206 2.344 2.249 15.020 12.010 3.637
e5 + Llama2-70B 1.926 0.133 0.131 0.135 2.135 2.424 2.419 2.346 2.238 16.030 11.810 3.793
e5 (top-2) + Llama2-70B1.544 0.133 0.131 0.135 1.661 1.908 1.994 1.833 1.759 15.120 10.820 3.367
e5 (top-10) + Llama2-70B2.811 0.133 0.131 0.135 2.951 3.276 - 13.900 14.400 35.070 24.100 -
e5 (DiskANN) + Llama2-70B1.803 0.044 0.044 0.043 2.009 2.281 2.166 2.247 2.116 15.780 11.370 3.628
e5 + Llama2-70B (3-action) - 25.41 - - 4.993 - 16.320 - - - -
Retrieval (in seconds per question)
BM25 0.038 0.008 0.009 0.008 0.018 0.013 0.052 0.105 0.086 0.136 0.857 0.121
BM25 (withoutREWRITE-EL) 0.038 5.700 4.531 5.440 0.018 0.013 0.052 0.105 0.086 0.136 0.857 1.543
m-e5 (Flat) 0.174 0.164 0.166 0.176 0.187 0.165 0.194 0.156 0.176 0.177 0.165 0.173
m-e5 (HNSW) 0.008 0.013 0.013 0.015 0.008 0.009 0.009 0.009 0.009 0.011 0.010 0.010
e5 (Flat) 0.177 0.168 0.172 0.159 0.201 0.170 0.171 0.146 0.155 0.174 0.165 0.169
e5 (HNSW) 0.008 0.008 0.008 0.008 0.008 0.008 0.008 0.008 0.008 0.008 0.009 0.008
e5 (DiskANN) 0.018 0.020 0.038 0.020 0.020 0.030 0.020 0.021 0.019 0.019 0.021 0.022
Mean Query Length (tokens)
11.1±4.0 357.9±149.0 331.5±113.5 505.2±31.1 7.5±2.5 7.6±2.3 9.9±2.1 19.5±6.6 17.9±8.9 21.0±10.7 86.3±58.0
Table 9: Execution time (in seconds per question) in RALLE. Avg. refers to macro-average of the times in each
task. The mean query length and its standard deviation (shown as ± after the value) are also displayed, which were
calculated using the e5 tokenizer.
Language Model
Model Name Size max len. emb dim URL
wizard-vicuna-13b (Lee, 2023) 13,015,864,320 2,048 - https://huggingface.co/junelee/wizard-vicuna-13b
Llama-2-13b-chat (Touvron et al., 2023b) 13,015,864,320 4,096 -https://huggingface.co/meta-llama/Llama-2-13b-chat
Llama-2-70b-chat (Touvron et al., 2023b) 68,976,653,312 4,096 -https://huggingface.co/meta-llama/Llama-2-70b-chat
StableBeluga2 70B 4,096 - https://huggingface.co/stabilityai/StableBeluga2
Retriever
multilingual-e5-large 559,890,946 514 1,024 https://huggingface.co/intfloat/multilingual-e5-large
e5-large-v2 (Wang et al., 2022) 335,142,400 512 1,024 https://huggingface.co/intfloat/e5-large-v2
Table 10: Hugging Face links of the models used in our evaluation. Size refers to the total number of effective
parameters of each model. max len.refers to the maximum token length of model input.
Closed-book indicates that an LLM answers to
the given question without retrieval. Although
these prompts have been our established best prac-
tices, we recognize that there may be opportunities
for improvement (see also Section 5).



Source: data\tc16_2312.10997v5\referenced_papers\[42]_2208.03299.pdf (Page 30):

5-shot 5-shot (multi-task) Full / Transfer
770M 3B 11B 770M 3B 11B 770M 3B 11B
All 29.2 35.7 36.1 26.5 40.0 43.5 42.4 50.4 54.0
Humanities 30.5 35.4 35.5 27.3 38.5 41.6 41.0 48.6 51.3
Social Sciences 29.7 38.0 39.4 24.8 43.8 48.9 48.6 57.8 64.7
STEM 29.0 31.4 30.8 26.5 32.8 35.8 33.4 40.6 41.7
Other 26.7 37.7 38.6 27.0 45.0 48.5 46.8 55.2 59.1
abstract algebra 26.0 23.0 21.0 29.0 30.0 26.0 23.0 29.0 26.0
anatomy 21.5 40.0 40.7 27.4 39.3 45.9 35.6 43.7 42.2
astronomy 37.5 38.8 37.5 27.6 39.5 41.4 36.2 50.7 55.3
business ethics 29.0 54.0 42.0 26.0 47.0 55.0 53.0 64.0 60.0
clinical knowledge 32.5 33.6 40.0 28.7 44.2 47.9 45.3 52.8 57.7
college biology 29.9 34.7 34.0 29.9 34.7 40.3 38.2 46.5 52.1
college chemistry 37.0 22.0 32.0 20.0 35.0 33.0 36.0 34.0 36.0
college computer science 28.0 35.0 34.0 28.0 27.0 36.0 31.0 44.0 35.0
college mathematics 31.0 29.0 27.0 22.0 34.0 27.0 30.0 33.0 32.0
college medicine 24.3 34.7 34.1 27.2 40.5 40.5 35.8 41.6 48.6
college physics 33.3 23.5 23.5 22.5 19.6 26.5 22.5 32.4 24.5
computer security 36.0 42.0 46.0 31.0 49.0 52.0 50.0 65.0 61.0
conceptual physics 26.4 35.7 30.2 23.4 30.6 32.8 34.5 37.4 43.8
econometrics 26.3 21.9 28.9 17.5 19.3 24.6 29.8 25.4 29.8
electrical engineering 31.0 33.1 31.7 31.0 31.0 36.6 41.4 47.6 51.7
elementary mathematics 26.2 27.5 28.0 27.0 31.2 33.3 25.9 31.2 35.5
formal logic 34.1 34.1 31.7 15.1 34.9 31.0 31.7 38.1 42.1
global facts 32.0 30.0 25.0 34.0 34.0 27.0 28.0 34.0 30.0
high school biology 22.6 31.9 29.7 27.1 41.6 50.0 43.5 57.7 60.6
high school chemistry 27.1 26.6 27.6 28.6 31.5 29.1 30.5 36.5 38.9
high school computer science 26.0 32.0 25.0 33.0 37.0 45.0 45.0 55.0 48.0
high school european history 34.5 43.0 42.4 24.2 60.0 59.4 58.2 69.1 76.4
high school geography 31.3 40.4 36.9 24.7 45.5 50.5 56.1 66.7 74.2
high school gov. and pol. 28.0 49.2 51.3 19.2 56.0 59.6 55.4 70.5 75.6
high school macroeconomics 25.6 37.7 32.1 26.7 42.3 43.6 41.0 51.5 56.4
high school mathematics 35.9 35.2 35.9 28.1 26.7 31.1 27.8 36.7 31.9
high school microeconomics 27.3 29.8 36.1 20.6 35.7 42.9 42.9 50.8 60.5
high school physics 21.9 25.2 22.5 24.5 28.5 29.1 27.8 31.1 27.8
high school psychology 26.1 46.4 51.0 24.8 54.3 60.2 56.3 67.3 76.1
high school statistics 27.8 33.3 33.3 17.6 30.6 33.8 32.9 33.3 37.0
high school us history 30.4 39.7 45.6 27.5 46.1 58.3 51.0 63.2 72.5
high school world history 42.6 50.6 41.8 29.1 54.0 64.6 66.7 72.2 73.8
human aging 28.3 37.2 29.6 26.0 45.3 46.2 46.6 57.0 62.8
human sexuality 29.8 34.4 41.2 25.2 42.0 44.3 51.1 58.0 59.5
international law 57.9 57.9 41.3 44.6 57.9 58.7 62.8 71.9 71.1
jurisprudence 30.6 33.3 34.3 32.4 49.1 52.8 55.6 67.6 74.1
logical fallacies 40.5 55.8 46.6 25.8 51.5 62.0 43.6 69.3 71.2
machine learning 33.0 34.8 36.6 29.5 35.7 37.5 32.1 37.5 42.9
management 21.4 29.1 40.8 24.3 47.6 50.5 60.2 69.9 70.9
marketing 38.9 58.5 60.7 31.2 67.9 75.6 69.2 79.9 85.9
medical genetics 26.0 36.0 36.0 29.0 43.0 44.0 40.0 54.0 50.0
miscellaneous 24.5 45.2 46.4 27.1 52.2 58.2 51.3 64.6 72.7
moral disputes 32.4 37.3 38.7 28.6 43.4 43.4 49.7 64.7 64.7
moral scenarios 24.7 24.7 24.7 23.0 23.9 24.7 23.8 24.0 23.8
nutrition 30.1 33.0 34.6 25.8 42.5 44.1 50.3 55.6 61.1
philosophy 28.6 32.5 37.3 31.2 38.9 45.0 44.1 56.6 59.2
prehistory 33.6 37.0 41.4 27.5 39.8 50.6 41.0 51.5 57.7
professional accounting 21.3 28.0 30.5 25.9 35.5 34.0 37.2 41.5 42.2
professional law 28.2 33.4 34.0 27.6 35.4 35.5 38.3 43.0 45.6
professional medicine 19.5 26.5 24.3 20.2 32.0 37.9 38.6 40.8 46.0
professional psychology 27.8 32.8 32.8 26.6 39.5 43.6 38.4 48.0 58.3
public relations 22.7 43.6 40.0 21.8 47.3 56.4 50.0 55.5 60.0
security studies 37.6 26.1 31.0 20.4 34.7 44.1 56.3 61.6 66.9
sociology 43.3 41.8 38.8 30.8 45.8 52.7 60.2 66.7 72.1
us foreign policy 49.0 57.0 66.0 38.0 56.0 61.0 59.0 75.0 76.0
virology 29.5 26.5 34.3 30.1 36.1 39.8 44.0 46.4 41.6
world religions 24.0 40.9 47.4 32.7 49.1 57.3 48.0 63.7 70.2
Table 17: MMLU Test scores for the T5 closed book baseline for each model size and each of the 57 domains.
31



### Claim 106/179

#### Claim Text
Notice that by differentiating (3) we obtain u′ = aφ′ = aκ. (4) 1micro-chemo-Elasto-Hydrodynamic. 4 Figure 1: From the axoneme to the two rows of molecular motors. (A) Cross section of the axoneme when viewed from the base to tip, with numbering taken from . (B) projection of the axoneme onto its bending plane. (C) Flagellum composed by two filaments fixed at the base. 2.2 Force and torque balance equations The center filament is subject to internal forces R(s) and torque M(s) that act at position s.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 16):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question A spherical conductor carries a net charge. How is this charge distributed on
the sphere?
0) The charge is evenly distributed on the surface.
1) The charge resides on the surface only; the distribution of charge on the
surface depends on what other charged objects are near the sphere.
2) The charge moves continually within the sphere.
3) The charge is distributed uniformly throughout the sphere.
Principles Coulomb’s Law: the force between two charged particles is proportional to the
product of their charges and inversely proportional to the square of the distance
between them, F = k * q1 * q2 / r, where F is the electric force, k is a constant,
q1 and q2 are the charges the particles carry, and r is the distance between them.
Answer Using the Principles of Coulomb’s Law, we can solve the problem as following:
Step 1: Apply Coulomb’s Law to find out how charges are distributed on the
surface.
In the case of a spherical conductor, the charges on the surface will repel each
other. The further apart the charges are, the less force they will exert on each
other. Therefore, the charges will distribute themselves evenly on the surface
of the sphere, as this is the configuration that minimizes the repulsive force
between them.
Step 2: Apply Coulomb’s Law to find out what happens if there are other
charges present.
The distribution of charge on the surface may also be affected by the presence
of other charged objects near the sphere. For example, if a negatively charged
object is brought near a positively charged sphere, the negative charges on the
sphere will be repelled and will move to the opposite side of the sphere. This
will result in a non-uniform distribution of charge on the surface of the sphere.
Therefore, the correct answer is 1) The charge resides on the surface only; the
distribution of charge on the surface depends on what other charged objects are
near the sphere.
Table 9: Few-shot demonstration exemplar for MMLU high-school Physics.
17



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 22):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset Question Answer
MMLU
Physics
A point charge, Q1 = +1 mC, is
fixed at the origin. How much
work is required to move another
charge, Q2 = +8 µC, from the
point (0, 4 meters) to the point
(3 meters, 0)?
0) 3.5 J
1) 6.0 J
2) 22.5 J
3) 40 J
Sure, let’s think step by step.
1. Calculate the electric potential energy of Q2
at the initial position. The electric potential
energy (U) of a point charge Q2 at a distance
r from another point charge Q1 is given by the
following equation: U = k * Q1 * Q2 / r, where k
is Coulomb’s constant.
Therefore, the electric potential energy of Q2 at
the initial position is:
Uinitial = (9 ×109Nm2/C2) ∗(1 ×10−3C) ∗(8 ×
10−6C)/4m = 18 J
2. Calculate the electric potential energy of Q2 at
the final position. The electric potential energy of
Q2 at the final position is:
Ufinal = (9 × 109Nm2/C2) ∗ (1 × 10−3C) ∗ (8 ×
10−6C)/3m = 24 J
3. Calculate the work required to move Q2 from
the initial position to the final position. The work
(W) required to move a charge from one position
to another is equal to the change in electric
potential energy (∆U) between the two positions.
W = ∆U = Ufinal - Uinitial = 24 J - 18 J = 6 J
Therefore, the answer is 1) 6.0 J.
Table 18: Demonstration exemplar for CoT prompting for MMLU Physics.
23



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 28):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question Two spheres of net charge +5e and -6e briefly come into contact. Afterward,
which of the following is a possible combination of net charges for the two
spheres?
0) +3e and -4e
1) +1.5e and -2.5e
2) Both of the above
3) None of the above
Principles Law of conservation of charge: the total charge in a closed system remains
constant.
Coulomb’s Law: the mutual repulsion of like charges demands that the charges
be as far apart as possible.
Answer Using the Principles of Conservation of charge, we can solve the problem as
following:
Step 1: Apply Conservation of charge to find out the possible combination of
net charges for the two spheres.
The total charge of the two spheres before they come into contact is +5e - 6e =
-1e. After they come into contact, the total charge of the two spheres must still
be -1e.
Step 2: Find out the possible combination of net charges for the two spheres.
The possible combination of net charges for the two spheres are:
(1) +3e and -4e
(2) +1.5e and -2.5e
Therefore, the correct answer is 2) Both of the above.
Table 25: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Principle Error. In this case, Coulomb’s Law is not needed for solving this problem. Instead, the
Charge Quantization Principle is missing from the retrieved principles, and is in fact needed to rule
out option (2) of fractional charges.
29



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 30):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question Physics students are checking the constant acceleration equations of kinematics
by measuring the velocity of a tennis ball that is dropped and falls 6 meters
and then passes through a photogate. The predicted velocity is 20% above the
velocity measured by the photogate. Which of the following best describes the
cause of the large percent difference?
0) The ball changes its shape while falling.
1) The acceleration of gravity varies as the ball is falling.
2) Air resistance increases the acceleration of the ball.
3) The acceleration of the balls varies with the velocity.
Principles Vertical motion: y = 0.5 * g * t2, where y is the vertical displacement, g is the
acceleration due to gravity, t is the time.
Free fall: is a special case of motion with constant acceleration, in which the
acceleration is due to gravity.
Acceleration to Velocity Formula: v = u + a * t, where v is the final speed, u is
the initial velocity, a is the acceleration, and t is the time.
Answer Using the Principles of Vertical motion and Horizontal motion, we can solve
the problem as following:
Step 1: Apply the Vertical motion equation to find the time it takes for the ball
to fall 6 meters.
y = 0.5 * g * t2
6 m = 0.5 * 9.8 m/s2 * t2
t = 1.12 s
Step 2: Apply the Horizontal motion equation to find the velocity of the ball
when it passes through the photogate.
x = v * t
v = x / t
v = 6 m / 1.12 s
v = 5.36 m/s
Step 3: Compare the predicted velocity with the velocity measured by the
photogate. The predicted velocity is 20% above the velocity measured by the
photogate, so the predicted velocity is 1.2 * 5.36 m/s = 6.43 m/s.
Step 4: Find the cause of the large percent difference. The predicted velocity is
6.43 m/s, but the velocity measured by the photogate is 5.36 m/s. The difference
is 1.07 m/s. This difference is caused by air resistance. Air resistance is a force
that opposes the motion of an object through the air. It is caused by the friction
between the object and the air molecules. The greater the surface area of the
object, the greater the air resistance. The tennis ball has a large surface area, so
it experiences a lot of air resistance. This air resistance slows down the tennis
ball and causes the difference between the predicted velocity and the velocity
measured by the photogate.
Therefore, the correct answer is 0) The ball changes its shape while falling.
Table 27: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Reasoning Error. In this case, the model was able to correctly identify air resistance is the cause of
the velocity difference, but made the error at the final reasoning step of deriving the right answer.
31



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 33):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: A 2 µF capacitor is connected directly to a battery. When
the capacitor is fully charged, it stores 600 µC of charge. An experimenter
replaces the 2 µF capacitor with three 18 µF capacitors in series connected to
the same battery. Once the capacitors are fully charged, what charge is stored
on each capacitor?
0) 100 µC
1) 200 µC
2) 600 µC
3) 1800 µC
Original Answer [Incorrect]: The answer is 100 µC. The total capacitance of
the three 18 µF capacitors in series is 1 / (1 / 18 + 1 / 18 + 1 / 18) = 6 µF. The
charge stored on the three capacitors in series is 6 µF * V = 600 µC. The charge
stored on each capacitor is 600 µC / 3 = 200 µC.
Stepback Question: What are the physics principles behind this question?
Principles:
Capacitors: Q = C *V , where Q is the charge, C is the capacitance and V is the
voltage.
Capacitors in series: C = 1 / (1/C1 + 1/C2 + ... + 1/Cn), where C is the
equivalent capacitance and C1, C2, Cn are the capacitances of each capacitor.
Capacitor in parallel: C = C1 + C2 + ... + Cn, where C is the equivalent
capacitance and C1, C2, Cn are the capacitances of each capacitor.
Final Answer: Using the Principles of Capacitance and Capacitors in series,
we can solve the problem as following:
Step 1: Apply Capacitance to find out the voltage of the battery.
We know that the capacitance of the 2 µF capacitor is 2 µF and the charge
stored on the capacitor is 600 µC. Using the formula for capacitance, we can
find the voltage of the battery:
C = Q / V
2 µF = 600 µC / V
V = 300 V
Step 2: Apply Capacitors in series to find out the equivalent capacitance of the
three 18 µF capacitors in series.
We know that the capacitance of each of the three capacitors is 18µF. Using the
formula for capacitors in series, we can find the equivalent capacitance of the
three capacitors:
1 / Ceq = 1 / C1 + 1 / C2 + 1 / C3
1 / Ceq = 1 / 18 µF + 1 / 18 µF + 1 / 18 µF
1 / Ceq = 3 / 18 µF
Ceq = 6 µF
Step 3: Apply Capacitance to find out the charge stored on each capac-
itor.
We know that the equivalent capacitance of the three capacitors is 6µF and the
voltage of the battery is 300 V . Using the formula for capacitance, we can find
the charge stored on each capacitor:
C = Q / V
6 µF = Q / 300 V
Q = 1800 µC
Therefore, the charge stored on each capacitor is 1800 µC. The correct
answer is 3) 1800 µC.
Table 30: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
34



### Claim 107/179

#### Claim Text
While th ese deviations refer to the relative intensities within a given isotopocule, such low relative uncertainties of the intensities of the α and β isotopomers suggest that site -preference measurements for source characterization applications can be performed with a few per mil precision .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 18):

Table 11: Confidence region for SOTA model in XSum and BigPatent.
System ROUGE-1/2/L 95%-conf.int
XSum
BRIOjoint
50.3 0.49986 - 0.50602
26.7 0.26300 - 0.26989
41.6 0.41231 - 0.41900
BigPatent
BARTjoint
62.9 0.62664 - 0.63080
48.1 0.47783 - 0.48333
59.6 0.59401 - 0.59847
Table 12: Generation Latency analysis.
NMT XSum BigPatent DailyDialog
Average Input Length 87 512 1024 71
Average Output Length 44 75 127 16
CPU
Retrieval-augmented Baseline 0.97 1.79 3.16 0.32
Selfmem
Candidate Generation 3.20 7.50 15.00 1.02
Memory Selection 0.50 0.52 0.95 0.14
Hypothesis Generation 0.97 1.79 3.00 0.32
×4.80 ×5.47 ×6.04 ×4.63
CUDA
Retrieval-augmented Baseline 0.29 0.44 0.75 0.10
Selfmem
Candidate Generation 0.51 1.00 1.72 0.18
Memory Selection 0.01 0.01 0.01 0.01
Hypothesis Generation 0.29 0.44 0.75 0.10
×2.76 ×2.99 ×3.35 ×2.91
19



Source: data\tc16_2312.10997v5\referenced_papers\[32]_2310.14503.pdf (Page 7):

Figure 3: Results of changing the diverse coefficient λ
on SQuAD /1 (SQuAD v1.1 with split from (Zhou et al.,
2017)). EM and F1 indicate QA-model based metrics,
and P-BLEU is short for Pairwise-BLEU.
SQuAD/1, RAST is better than CV AE on both Top-
1 and Oracle-BLEU. On SQuAD/1 and SQuAD/2,
RAST is better than Composition on Top-1 whereas
being comparable on Oracle-BLEU. Regarding the
overall score, RAST obtains the superior results
on three datasets, showing that its capability in
balancing between diversity and consistency.
5.5 Ablation Study
We study the impact of different components of
RAST on SQuAD/1 (Zhou et al., 2017), where the
results are given in Figure 3 and Table 3.
Diverse Coefficient Figure 3 shows how diver-
sity and consistency change when increasing λ.
Besides Oracle-BLEU, we also use Exact Match
(EM) and F1 (two QA metrics) to measure consis-
tency (Sultan et al., 2020; Lyu et al., 2021). Here,
the QA metrics are calculated by averaging EM
and F1 scores of the answers, which are generated
by the QA model for top-N evaluation questions.
As observable from Figure 3, increasing λ leads
to higher diversity (lower pairwise-BLEU), but
lower consistency (lower Oracle and QA metrics).
This is the result that we expect. The rate of in-
crease in diversity, however, is much higher than
the rate of decrease in the consistency metrics.
Specifically, when λ changes from 0.05 to 0.25,
pairwise-BLEU drops 39.52 points (from 74.7 to
35.18), whereas F1 only drops 6.96 points (from
85.16 to 78.2), showing that our method is able
to maintain consistency within a reasonable range
while promoting diversity significantly.
Freeze DPR To study the impact of joint RL
training on the retrieval and generation models, we
Model Top1 ↑ Oracle↑ P-B↓ Over↑
RAST 19.25 23.23 48.91 9.14
w/o e2e 19.94 23.40 51.70 9.02
w/o cluster 15.58 23.04 61.06 5.88
w/ question 19.00 23.59 54.09 8.28
Table 3: Experimental results of different model vari-
ants on SQuAD /1 (Zhou et al., 2017). Here, P-B and
Over are short for Pairwise-BLEU and Overall-BLEU
respectively; EM and F1 indicate the QA-based metrics.
Model Consistency Diversity Total
RAST 3.36 2.36 2.86
Nucleus 3.00 1.78 2.39
Table 4: Human evaluation result on SQuAD /1.
compare the performance of RAST and RAST (w/o
e2e). As observable from Table 3, overall BLEU is
improved with end2end training, showing that the
retrieval model is better optimized for balancing
between diversity and consistency.
Diversity-driven Sampling We measure the im-
pact of the clustering step in diversity-driven sam-
pling (Algorithm 1) by comparing RAST and
RAST (w/o cluster) in Table 3. Here, during train-
ing, RAST (w/o cluster) samples templates based
solely on the retrieval scores. It is observable that
clustering allows us to better train RAST, and thus
results in better performance across all metrics.
Retrieval Query The last row of Table 3 shows
the performance of RAST when we use the best
question y0 of the vanilla QG (RAST w/ question)
instead of the question template z0 for querying
external templates. As we can see, using masked
questions (RAST) leads to higher diversity than
the alternative. This is intuitive given the fact that
masking context-sensitive information can make
templates more generalizable across contexts.
5.6 Human Evaluation
We followed (Wang et al., 2020b) to evaluate the
consistency and diversity of RAST and Nucleus
sample6 on 50 samples of SQuAD /1. Here, the
consistency metric ranges from 0 to 5, measuring
the proportion of the generated questions being
answerable based on the given context (without
6This is because the source code of the other baselines is
not publicly available



Source: data\tc16_2312.10997v5\referenced_papers\[3]_2310.20158.pdf (Page 17):

In our experiments, we notice that the number of output tokens is negligible compared to the number
of input tokens. Specifically, when making calls to GPT -3.5-Turbo, approximately 1000 tokens
are utilized for input, while calls to GPT -4use around 2000 tokens for input. This results in an
approximate cost of $1.02 per API query, with specific cost details provided in Table A1.
Table A1: The cost of API calls per query.
Model Number of API calls Cost ($)
GPT -3.5-Turbo 320 0.48
GPT -4 9 0.54
C H YPERPARAMETER TUNING
In our final re-ranking step with GPT -4, we conduct hyperparameter tuning for the window size
(w) and step size ( s), exploring two specific configurations: w,s = 20,10 and w,s = 10,5. Our
results indicate that the w,s = 10,5 configuration outperforms the w,s = 20,10 setup, with a higher
nDCG@10 score of 0.7390 compared to 0.7376.
D A DDITIONAL RESULTS
Extended results on SPARTA(Zhao et al., 2021), docT5query (Cheriton, 2019), GenQ (Thakur
et al., 2021), ColBERT (Khattab & Zaharia, 2020), BM25+CE (Wang et al., 2020), and TART -
Rerank (FLAN-T5-XL)(Asai et al., 2022) can be found in Table A2 and Table A3.
Table A2: Retrieval performance (nDCG@10) on BEIR datasets.
Method TREC-COVID NFCorpus Signal-1M (RT) TREC-NEWS Robust04 Touch´e-2020 DBPedia SciFact
BM25 59.5 30.8 33.0 39.5 40.7 44.2 31.8 67.9SPARTA 53.8 30.1 25.2 25.8 27.6 17.5 31.4 58.2docT5query 71.3 32.8 30.7 42.0 43.7 34.7 33.1 67.7DPR 33.2 18.9 15.5 16.1 25.2 13.1 26.3 31.8ANCE 65.4 23.7 24.9 38.2 39.2 24.0 28.1 50.7TAS-B 48.1 31.9 28.9 37.7 42.7 16.2 38.4 64.3GenQ 61.9 31.9 28.1 39.6 36.2 18.2 32.8 64.4ColBERT 67.7 30.5 27.4 39.3 39.1 20.2 39.2 67.1BM25+CE 75.7 35.0 33.8 43.1 27.1 40.9 68.8monoBERT(340M) 70.0 36.9 31.4 44.6 49.3 31.8 41.9 71.4monoT5(220M) 78.34 37.4 31.7 46.8 51.7 30.8 42.4 73.4monoT5(3B) 80.7 39.0 32.6 48.5 56.7 32.4 44.4 76.6TART -Rerank (FLAN-T5-XL)75.1 36.0 25.8 40.0 50.8 27.5 42.5 74.8UPR 68.1 35.0 31.9 43.1 42.4 19.7 30.9 72.7Promptagator++(zero-shot) 76.0 36.0 - - - 27.8 41.3 73.6Promptagator++(few-shot) 76.2 37.0 - - - 38.1 43.4 73.1RankGPT 85.5 38.5 34.4 52.9 57.6 38.6 47.1 75.0RRR(this work) 86.4 39.9 29.8 53.6 67.4 29.8 51.0 77.2
Table A3: Retrieval performance ( Recall@100) on BEIR datasets. For TREC-COVID, capped
Recall@100 is used.
Method TREC-COVID NFCorpus Signal-1M (RT) TREC-NEWS Robust04 Touch´e-2020 DBPedia SciFact
BM25 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5SPARTA 40.9 24.3 27.0 26.2 21.5 38.1 41.1 86.3docT5query 54.1 25.3 35.1 43.9 35.7 55.7 36.5 91.4DPR 21.2 20.8 16.2 21.5 21.1 30.1 34.9 72.7ANCE 45.7 23.2 23.9 39.8 27.4 45.8 31.9 81.6TAS-B 38.7 28.0 30.4 41.8 33.1 43.1 49.9 89.1GenQ 45.6 28.0 28.1 41.2 29.8 45.1 43.1 89.3ColBERT 46.4 25.4 28.3 36.7 31.0 43.9 46.1 87.8BM25+CE 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5monoBERT(340M) 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5monoT5(220M) 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5monoT5(3B) 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5RankGPT 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5RRR(this work) 54.8 32.4 32.4 51.6 45.4 52.2 55.0 94.3
18



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 15):

ARES Ranking of Real RAG Systems
NQ WoW FEVER
C.R. A.R. C.R. A.R. C.R. A.R.
Kendall’s Tau for
Sampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82
Kendall’s Tau
for RAGAS 0.82 0.82 0.73 0.82 0.73 0.87
Kendall’s Tau
for GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87
Kendall’s Tau
for ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87
Kendall’s Tau
for ARES 1.0 0.96 0.91 1.0 0.82 1.0
RAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%
GPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%
ARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%
Table 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance
(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled
annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150
annotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses
GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we
found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.
Additionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted
the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial
costs required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human
preference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.
ARES Cross-Domain Ranking of Pseudo RAG Systems
NQ to
FEVER
FEVER to
NQ
NQ to
MultiRC
MultiRC to
NQ
NQ to
ReCoRD
ReCoRD to
NQ
C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.
Kendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94
Kendall’s Tau of
In-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0
Average PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%
Accuracy on
RAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%
Table 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the
fine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance
(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains
when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and
ReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that
additional examples further improved the performance of ARES. Furthermore, we found that even in scenarios
where the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ
to FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of
percentage points from the lower bound to the upper bound of the PPI confidence interval.



Source: data\tc16_2312.10997v5\referenced_papers\[78]_2310.08877.pdf (Page 7):

/uni0000001a/uni00000018/uni0000001b/uni00000014/uni00000011/uni00000015/uni00000018/uni0000001b/uni0000001a/uni00000011/uni00000018/uni0000001c/uni00000016/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000013/uni00000013
/uni00000035/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000048/uni00000059/uni00000048/uni00000055/uni00000003/uni00000035/uni00000048/uni00000046/uni00000044/uni0000004f/uni0000004f/uni00000023/uni0000001a
/uni00000017/uni00000015
/uni00000017/uni00000019
/uni00000018/uni00000013
/uni00000018/uni00000017/uni00000028/uni00000051/uni00000057/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000029/uni00000014
/uni00000034/uni00000010/uni00000037/uni00000032/uni00000027/uni00000003/uni0000000b/uni00000013/uni00000011/uni00000016/uni00000019/uni00000013/uni0000001c/uni0000000c
/uni00000029/uni0000004c/uni00000027/uni00000003/uni0000000b/uni00000013/uni00000011/uni00000017/uni00000016/uni00000013/uni00000013/uni0000000c
/uni00000029/uni0000004c/uni00000027/uni0000000e/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni0000000b/uni00000013/uni00000011/uni0000001c/uni00000013/uni0000001b/uni0000001c/uni0000000c
/uni00000029/uni0000004c/uni00000027/uni0000000e/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000003/uni0000000b/uni00000013/uni00000011/uni0000001b/uni0000001c/uni0000001b/uni0000001c/uni0000000c
/uni00000029/uni0000004c/uni00000027/uni0000000e/uni00000046/uni00000057/uni00000055/uni00000003/uni0000000b/uni00000013/uni00000011/uni0000001b/uni0000001a/uni0000001a/uni0000001b/uni0000000c
(a) T5-Based model
/uni0000001a/uni00000018/uni0000001b/uni00000014/uni00000011/uni00000015/uni00000018/uni0000001b/uni0000001a/uni00000011/uni00000018/uni0000001c/uni00000016/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000013/uni00000013
/uni00000035/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000048/uni00000059/uni00000048/uni00000055/uni00000003/uni00000035/uni00000048/uni00000046/uni00000044/uni0000004f/uni0000004f/uni00000023/uni0000001a
/uni00000015/uni00000019
/uni00000016/uni00000013
/uni00000016/uni00000017
/uni00000016/uni0000001b/uni00000028/uni00000051/uni00000057/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000029/uni00000014
/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni00000003/uni0000000b/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000016/uni00000014/uni0000000c
/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni0000000e/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni0000000b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000013/uni00000017/uni0000000c
/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni0000000e/uni00000053/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057/uni00000003/uni0000000b/uni00000013/uni00000011/uni0000001b/uni0000001a/uni00000013/uni0000001a/uni0000000c (b) ChatGPT
Figure 3: Entity F1 scores of generators ((a) FiD&Q-
TOD and (b) ChatGPT) as the retrieval performance
varies with different retrievers. Bracketed numbers fol-
lowing model names refer to the correlation coefficients
between retrieval performance and Entity F1.
Model Large-scalse
BLEU Entity F1 Recall@7
Oursprefix 16.39 50.35 92.51
w/o MML 16.07 49.56 91.39
Oursprompt 17.56 50.69 92.74
w/o MML 16.67 50.41 91.39
Oursctr 15.96 51.35 92.74
w/o MML 14.78 50.54 91.39
Table 3: Ablation study of the MML loss.
entities are even worse than those with a weak re-
triever. We refer to this phenomenon as retrieval-
generation misalignment. In contrast, the dashed
lines, which depict the results of the generators
with our proposed meta knowledge, exhibit greater
consistency between the retrieval performance and
the generators. This indicates that our proposed
method mitigates the misalignment issue. The cor-
relation coefficients shown in parentheses next to
method names further confirm this observation.
5.4 Ablation Study
We assess the impact of maximum marginal like-
lihood, various types of meta knowledge, and the
inclusion of negative samples. Unless otherwise
specified, the ablation study is performed on the
MWOZ dataset using T5-Base as the generator,
considering the computational resource constraints.
5.4.1 Maximum Marginal Likelihood
Table 3 presents the impact of the maximum
marginal likelihood (MML) loss. The methods
labeled as “w/o MML” utilize the warmed-up re-
triever described in Section 4.2, without the joint
training with the response generator. The re-
sults demonstrate that the inclusion of maximum
marginal likelihood enables further enhancement
of the retriever during training. Consequently, the
improved retrievers lead to enhanced final gener-
ated responses.
Method Type Condensed Large-scaleBLEU Entity F1 BLEU Entity F1 Recall@7
Oursprefix
all 16.97 51.99 16.39 50.35 92.51order 16.97 51.64 16.20 49.88 92.27conf 16.15 51.70 13.96 47.35 89.11cooccur 16.70 51.14 15.61 49.66 91.39
Oursprompt
all 17.05 52.42 17.56 50.69 92.74order 16.15 49.88 15.60 49.47 91.15conf 17.02 51.66 16.84 50.16 91.93cooccur 16.99 51.78 16.20 50.38 92.35
Table 4: Ablation study of different types of meta
knowledge on MWOZ with condensed and large-scale
knowledge bases. “order”, “conf”, “cooccur” and “all”
mean using only retrival order, retrieval confidence, co-
occurrence, or all types of meta knowledge, respectively.
Type Condensed Large-scalseBLEU Entity F1 BLEU Entity F1 Recall@7
Oursprefix(Base) 16.97 51.99 16.39 50.35 92.51w/o neg 16.68 50.45 15.94 49.46 90.24Oursprompt(Base) 17.05 52.42 17.56 50.69 92.74w/o neg 16.98 51.35 15.99 49.85 91.15Oursctr(Base) 17.33 51.86 15.96 51.35 92.74w/o neg 17.11 50.34 15.79 48.32 92.29Oursprefix(LLM) 7.32 32.38 6.83 30.47 92.74w/o neg 7.22 32.78 7.01 30.69 92.74Oursprompt(LLM) 7.29 35.98 6.97 31.88 92.74w/o neg 7.58 36.18 7.31 32.04 92.74
Table 5: Ablation study of negative entities.
5.4.2 Types of Meta Knowledge
We compare different types of meta knowledge,
and the results are presented in Table 4. The find-
ings indicate that using a single type of meta knowl-
edge yields inferior performance compared to com-
bining all three types. Furthermore, an interesting
observation emerges when using the prefix: the
retrieval order outperforms other types of meta
knowledge. In contrast, when using the prompt, the
results are reversed. We attribute this phenomenon
to the design of the prefix and prompt. Repre-
senting meta knowledge with a prefix introduces
a higher diversity in ranking order since a distinct
prefix is assigned to each ranking order. This in-
creased diversity enables the generator to better
distinguish the recalled entities. On the other hand,
the distinction between retrieval confidence and co-
occurrence in the prefix setting is less obvious. In
contrast, when representing meta knowledge with
a prompt, the retrieval order becomes less diverse,
since only the numbers representing the retrieval
order are varied.
5.4.3 Negative Samples
We conduct an investigation into the impact of
negative entities on the performance of dialogue
systems. The results presented in Table 5 demon-
strate that the inclusion of negative entities sig-
nificantly improves the performance of dialogue



### Claim 108/179

#### Claim Text
So far, laser diagnostics, including E-FISH, are generally considered non-invasive when applied to plasma .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 1):

Parametric MemorizationPre-training
SFT
RLHF
Inference
Overinflated Self-confidence
Misleading Alignment
Generation-time Risk
Curating Training Data
Honesty-oriented RL
Decoding StrategyKnowledge Retrieve
Honesty-oriented SFT
Exploiting UncertaintySources (Sec. 4) Mitigation (Sec. 5)
Definition (Sec. 2)Input-Conflicting Hallucination
TimeLine
Input-Conflicting Benchmark:BEGIN, QMSum, FENMT,FEQA…
Benchmark (Sec. 3) Context-Conflicting HallucinationFact-Conflicting Hallucination
Context-Conflicting Benchmark:HADES…Fact-Conflicting Benchmark:TruthfulQA,,FActScore,HaluEval,FACTOR…
Figure 2: The overview structure of this paper: We initially categorize LLM hallucinations into three distinct types
and then introduce corresponding evaluation benchmarks. Subsequently, we explore the source of hallucinations
and discuss mitigation strategies throughout the life cycle of LLMs (pre-training→SFT→RLHF→inference).
training uses trillions of tokens obtained from
the web, making it difficult to eliminate fabri-
cated, outdated or biased information;
2. Versatility of LLMs : general-purpose LLMs
are expected to excel in cross-task, cross-
lingual, and cross-domain settings, posing
challenges for comprehensive evaluation and
mitigation of hallucination.
3. Imperceptibility of errors: as a byproduct of
their strong abilities, LLMs may generate false
information that initially seems highly plausi-
ble, making it challenging for models or even
humans to detect hallucination.
In addition, the RLHF process (Ouyang et al.,
2022), the vague knowledge boundary (Ren et al.,
2023) and the black-box property of LLMs (Sun
et al., 2022) also complicate the detection, expla-
nation, and mitigation of hallucination in LLMs.
There has been a notable upsurge in cutting-edge
research dedicated to addressing the aforemen-
tioned challenges, which strongly motivates us to
compile this survey.
We organize this paper as follows, as also
depicted in Figure 2. We first introduce the
background of LLMs and offer our definition of
hallucination in LLMs (§2). Next, we introduce
relevant benchmarks and metrics (§3). Subse-
quently, we discuss potential sources of LLM hal-
lucinations (§4), and provide an in-depth review of
recent work towards addressing the problem (§5).
Finally, we present forward-looking perspectives
(§6). We will consistently update the related
open-source materials, which can be accessed at
https://github.com/HillZhang1999/
llm-hallucination-survey.
2 Hallucination in the Era of LLM
We begin this section by overviewing the history
of LLMs (§2.1). Next, we present our defini-
tion of LLM hallucination, by breaking it down
2



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 6):

Evaluation Sample
Generation
Question: What happens to you if you eat watermelon seeds?
Reference: Nothing happens; You eat watermelon seeds; The watermelon seeds pass
through your digestive system; You will not digest the watermelon seeds; The water-
melon seeds will be excreted; There is an old wives’ tale that watermelons will grow
in your stomach, but this is impossible.
Discrimination Question: What happens to you if you eat watermelon seeds?
Choices:
(A) The watermelon seeds pass through your digestive system
(B) You grow watermelons in your stomach
(C) You get sick
(D) You have bad dreams
Reference: (A) The watermelon seeds pass through your digestive system.
Table 4: Illustrative examples of two methods for evaluating hallucinations (Generation v.s. Discrimination).
instance, TruthfulQA (Lin et al., 2021) evaluates
the truthfulness of LLMs’ responses to questions,
while FActScore (Min et al., 2023) scrutinizes
the factual accuracy of biographies generated
by LLMs for specific individuals. In contrast,
discrimination benchmarks (Li et al., 2023a;
Muhlgay et al., 2023) consider LLMs’ ability to
discriminate truthful statements from hallucinated
ones. Specifically, HaluEval (Li et al., 2023a)
requires the model to determine whether a state-
ment contains hallucinated information, while
FACTOR (Muhlgay et al., 2023) investigates
whether the LLM assigns a higher likelihood to
the factual statement compared to non-factual
ones. Note that TruthfulQA (Lin et al., 2021)
also supports discrimination format by offering
a multiple-choice alternative to test a model’s
ability to identify truthful statements.
Task format. Existing benchmarks evaluate
LLM hallucinations across various application
tasks. Firstly, certain benchmarks (Lin et al.,
2021; Li et al., 2023a) explore the issue of hal-
lucination in the context of question-answering,
evaluating the ability of LLMs to provide truthful
answers to knowledge-intensive questions. Sec-
ondly, FActScore (Min et al., 2023) and HaluE-
val (Li et al., 2023a) employ task instructions,
such as biography introduction instructions and
52K instructions from the Alpaca project (Taori
et al., 2023), to prompt LLMs to generate re-
sponses. The factuality of these responses is then
evaluated. Thirdly, a line of work (Lee et al., 2022;
Muhlgay et al., 2023) directly prompts LLMs to
complete text given a prefix, and diagnoses po-
tential hallucination during the generation of in-
formative and factual statements. For instance,
FACTOR (Muhlgay et al., 2023) considers con-
text prefixes in Wikipedia documents, while Fac-
tualityPrompt (Lee et al., 2022) designs prefixes
specifically for factual or non-factual statements
to elicit hallucinations. Table 5 provides samples
under different task formats.
Construction methods. Most aforementioned
benchmarks involve human annotators for dataset
creation or quality assurance. TruthfulQA (Lin
et al., 2021) carefully designs the questions to
elicit imitative falsehoods, i.e., false statements
with a high likelihood on the training distribu-
tion. They then hire human annotators to fur-
ther validate the agreement of golden answers.
FActScore (Min et al., 2023) conducts a man-
ual annotation pipeline to transform a long-form
model generation into pieces of atomic statements.
HaluEval (Li et al., 2023a) employs two construc-
tion methods. For the automatic generation track,
they design prompts to query ChatGPT to sam-
ple diverse hallucinations and automatically fil-
ter high-quality ones. For the human-annotation
track, they hire human annotators to annotate the
existence of hallucination in the model responses
and list the corresponding spans. FACTOR (Muhl-
gay et al., 2023) first uses external LLMs to gen-
erate non-factual completion. Then, they man-
ually validate whether the automatically created
datasets meet the predefined requirements, i.e.,
they should be non-factual, fluent, and similar to
the factual completion. To construct knowledge
creation task, Yu et al. (2023a) build an annota-
7



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 15):

to rectify hallucinations during the post-
processing stage (Cao et al., 2020; Zhu
et al., 2021; Fabbri et al., 2022). The fixer
can be either another LLM (Peng et al.,
2023a; Zhang et al., 2023d; Chern et al.,
2023; Gou et al., 2023) or a specific small
model (Chen et al., 2023a). Such fix-
ers first interact with external knowledge
sources to gather sufficient evidence, and
then correct hallucinations. For example,
RARR (Gao et al., 2023a) directly prompts
an LLM to ask questions about the content
that needs to be corrected from multiple per-
spectives. Then it uses search engines to re-
trieve relevant knowledge. The LLM-based
fixer finally makes corrections based on re-
trieved evidence. The Verify-then-Edit ap-
proach (Zhao et al., 2023a) aims to enhance
the factuality of predictions by post-editing
reasoning chains based on external knowl-
edge sourced from Wikipedia. To achieve
better performance, LLM-Augmenter (Peng
et al., 2023a) prompts LLMs to summarize
retrieved knowledge before feeding it into the
fixer. Moreover, FacTool (Chern et al., 2023)
and CRITIC (Gou et al., 2023) propose to uti-
lize various external tools to obtain evidence
for the fixer.
Summary & Discussion. Resorting to external
knowledge to mitigate hallucinations in LLMs of-
fers several advantages. Firstly, this method cir-
cumvents the need for modifying LLMs, making
it a plug-and-play and efficient solution. Secondly,
it facilitates the easy transfer of proprietary knowl-
edge (e.g., a company’s internal data) and real-
time updated information to LLMs. Lastly, this
approach enhances the interpretability of infor-
mation generated by LLMs by allowing the trac-
ing of generation results back to the source evi-
dence (Gao et al., 2023b; Yue et al., 2023). How-
ever, this direction also presents some remaining
challenges. We discuss some of them below.
(1) Knowledge verification. In the era of LLMs,
the external knowledge source could extend
beyond a single document corpus or a spe-
cific website to encompass the entire Internet.
However, the information from the Internet is
in the wild, which means they may also be
fabricated, or even generated by LLMs them-
selves (Alemohammad et al., 2023). How to
User Query: What is the height of Mount Kilimanjaro?
User Query: What is the height of Mount Kilimanjaro?
Answer:Theheightis5932meters.
Please provide your confidence level (0-100). 
Answer: The height is 5932 meters. I am 90% confident. 
User Query: What is the height of Mount Kilimanjaro?
 Answer: The height is 5895 meters. 
Answer: The height is 5932 meters. 
Answer: The height is 5921 meters. 
(a) logit-based method
(b) verbalize-based method
(c) consistency-based method
Figure 5: The illustrations of three typical methods for
estimating LLM uncertainty. In the example of the
logit-based method, we use the red/green background
to distinct tokens with low/high generation probabili-
ties. In the example of the consistency-based method,
the responses are acquired from multiple sampling.
verify the authenticity of retrieved knowledge
from the Internet is an open and challenging
problem to be solved.
(2) Performance/efficiency of retriever/fixer.
The performance of the retriever/fixer plays
a vital role in ensuring the effects of hallu-
cination mitigation. Future work may con-
sider jointly optimising the whole working
flow (retriever→LLM→fixer) via reinforce-
ment learning (Qiao et al., 2023) or other
techniques. Besides, the efficiency of the
retriever/fixer is another important factor to
be considered, as the generation speed of
existing LLMs is already a significant bur-
den (Ning et al., 2023).
(3) Knowledge conflict. As introduced be-
fore, the retrieved knowledge may conflict
with the parametric knowledge stored by
LLMs (Qian et al., 2023). Shi et al. (2023b)
reveal that LLMs may fail to sufficiently ex-
ploit retrieved knowledge when knowledge
conflict happens. Xie et al. (2023) take
a more cautious look at this phenomenon.
How to fully utilize context knowledge is
an under-explored question. For example,
Liu et al. (2023d) find the performance of
retrieval-augmented LLMs significantly de-
grades when they must access evidence in the
middle of long contexts.
5.4.3 Exploiting Uncertainty
Uncertainty serves as a valuable indicator for de-
tecting and mitigating hallucinations during the
16



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 10):

SFT Dataset Data Size
Alpaca (Taori et al., 2023) 52k samples
GPT4-Alpaca (Peng et al., 2023b) 52k samples
Baize (Xu et al., 2023) 210k samples
Dolly (Conover et al., 2023) 15k samples
Open-assistant (Köpf et al., 2023) 34k samples
LIMA (Zhou et al., 2023a) 1k samples
Table 7: The size of popular SFT datasets.
instance, Llama 2 (Touvron et al., 2023b) conducts
pre-training on about two trillion tokens. There-
fore, compared to manual curation, a more practi-
cal approach today could be automatically select-
ing reliable data or filtering out noisy data. For
example, the pre-training data of GPT-3 (Brown
et al., 2020) is cleaned by using similarity to a
range of high-quality reference corpora. The de-
velopers of Falcon (Penedo et al., 2023) carefully
extract high-quality data from the web via heuris-
tic rules and prove that properly curated pertaining
corpora lead to powerful LLMs. Li et al. (2023f)
propose phi-1.5, a 1.3 billion parameter LLMs
pre-trained on filtered “textbook-like” synthetic
data, which exhibits many traits of much larger
LLMs. In order to mitigate hallucinations, current
LLMs tend to collect pre-training data from credi-
ble text sources. The developers of Llama 2 (Tou-
vron et al., 2023b) strategically up-sample data
from highly factual sources, such as Wikipedia,
when constructing the pre-training corpus. Lee
et al. (2022) propose to prepend the topic pre-
fix to sentences in the factual documents to make
each sentence serve as a standalone fact during
pre-training. Concretely, they treat the document
name as the topic prefix and observe this method
improves LMs’ performance on TruthfulQA.
Summary & Discussion. The mitigation of hal-
lucinations during pre-training is primarily cen-
tred around the curation of pre-training corpora .
Given the vast scale of existing pre-training cor-
pora, current studies predominantly employ sim-
ple heuristic rules for data selection and filtering.
A potential avenue for exploration could be devis-
ing more effective selection or filtering strategies.
5.2 Mitigation during SFT
As a common practice, current LLMs collec-
tively undergo the process known as supervised
fine-tuning (SFT) to elicit their knowledge ac-
quired from pre-training and learn how to inter-
act with users (Wang et al., 2023c; Zhang et al.,
Parametric KnowledgeOf LLMs
 SFT Data
Teach LLMs to hallucinate
Figure 3: The SFT data usually contains samples that
exceed LLMs’ parametric knowledge, which may re-
sult in hallucinations.
2023b). SFT generally involves first annotating
or collecting massive-task instruction-following
data (Chung et al., 2022; Taori et al., 2023),
followed by fine-tuning pre-trained foundational
LLMs on this data using maximum likelihood es-
timation (MLE) (Wei et al., 2021). By employing
well-designed SFT strategies, many recent stud-
ies claim to have built LLMs that achieve perfor-
mance on par with ChatGPT (Wang et al., 2023b).
Similar to pre-training, one potential approach
to reduce hallucination during the SFT stage could
be curating the training data. Given the rela-
tively small volume of SFT data (refer to Table 7),
both manual and automatic curation are viable
options here. Zhou et al. (2023a) have meticu-
lously constructed an instruction-tuning dataset,
comprising 1,000 samples annotated by human ex-
perts. Some other studies (Chen et al., 2023b;
Cao et al., 2023; Lee et al., 2023) have employed
an automatic selection of high-quality instruction-
tuning data, by leveraging LLMs as evaluators or
designing specific rules. Experimental results on
hallucination-related benchmarks, such as Truth-
fulQA (Lin et al., 2021), suggest that LLMs fine-
tuned on such curated instruction data demonstrate
higher levels of truthfulness and factuality com-
pared to LLMs fine-tuned on uncurated data. Fur-
thermore, Mohamed et al. (2023) propose the inte-
gration of domain-specific knowledge sets into the
SFT data, which aims to reduce hallucinations that
arise from a lack of relevant knowledge.
It is worth noting that Schulman (2023) under-
scored a potential risk of the SFT process that
it could induce hallucination from LLMs due to
behavior cloning. Behavior cloning is a concept
in reinforcement learning (Torabi et al., 2018),
which means the model learns directly from im-
itating the expert’s actions. The problem here is
11



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 13):

Method Timing of Using Knowledge Source Application Task
WebGPT (Nakano et al., 2021) Generation-Time Search API QA
Adaptive-Retrieval (Mallen et al., 2023) Generation-Time Wikipedia QA
ReACT (Yao et al., 2022) Generation-Time Wikipedia QA & FV
RETRO (Borgeaud et al., 2022) Generation-Time Unstructured Corpus LM & QA
Chain-of-Knowledge (Li et al., 2023d) Generation-Time Structured Knowledge Base QA & FV & Decision
RARR (Gao et al., 2023a) Post-Processing Search API QA
Verify-then-Edit (Zhao et al., 2023b) Post-Processing Wikipedia, Search API, etc QA
LLM-Augmenter (Peng et al., 2023a) Post-Processing Web documents, Databases QA
REFEED (Yu et al., 2023b) Post-Processing Wikipedia QA, Dialogue
CRITIC (Gou et al., 2023) Post-Processing Search API, Code Executor, Calculator, etc QA & Program & Toxicity
FacTool (Chern et al., 2023) Post-Processing Search API, Code Executor, Calculator, etc QA & Reasoning & Generation
Table 10: A summary of some recent studies on resorting to external knowledge to mitigate hallucinations. We use
abbreviations for some application task names, including QA (Question Answering), FV (Fact Verification), and
LM (Language Modeling).
ically yield more accurate facts than those pre-
sented in long-form answers. The C OVE frame-
work initially plans verification questions, and
then answers these questions to ultimately produce
an enhanced, revised response. Experimental re-
sults on list-based questions, closed book QA, and
long-form text generation demonstrate that COVE
can effectively mitigate hallucination.
Another work, Li et al. (2023b), introduces a
novel Inference-Time Intervention (ITI) method to
improve the truthfulness of LLMs. This method is
based on the assumption that LLMs possess latent,
interpretable sub-structures associated with factu-
ality. The ITI method comprises two steps: 1)
fitting a binary classifier on top of each attention
head of the LLM to identify a set of heads that ex-
hibit superior linear probing accuracy for answer-
ing factual questions, and 2) shifting model activa-
tions along these factuality-related directions dur-
ing inference. The ITI method leads to a substan-
tial performance improvement on the TruthfulQA
benchmark (Lin et al., 2021).
Distinct from the aforementioned studies, Shi
et al. (2023b) instead concentrates on the retrieval-
augmentation setting. Prior research has shown
that LLMs sometimes fail to adequately attend to
retrieved knowledge when addressing downstream
tasks, particularly when the retrieved knowl-
edge conflicts with the parametric knowledge of
LLMs (Zhou et al., 2023b; Xie et al., 2023).
To address this issue, Shi et al. (2023b) propose
a straightforward context-aware decoding (CAD)
strategy. The core idea of CAD is to perform
a contrastive ensemble of pθ(yt | x, c, y<t) and
pθ(yt | x, y<t), where θ represents the LM,x is the
input query, c is the context, y is the response, and
t is the time step. pθ(yt | x, c, y<t) means the gen-
eration probability distribution of t-th token when
given the context whilepθ(yt | x, y<t) denotes the
distribution only considering the query. The CAD
method aims to compel LLMs to pay more at-
tention to contextual information instead of over-
relying their own parametric knowledge to make
decisions. Experimental results show that CAD
effectively elicits the ability of LLMs to exploit
retrieved knowledge and thus reduces factual hal-
lucinations on downstream tasks. Another work,
DoLA (Chuang et al., 2023), also employ the idea
of contrastive decoding to reduce hallucination.
However, they contrast the generation probabili-
ties from different layers of LLMs, as they find
that linguistic and factual information is encoded
in distinct sets of layers.
Summary & Discussion. Designing decoding
strategies to mitigate hallucinations in LLMs dur-
ing inference is typically in a plug-and-play man-
ner. Therefore, this method is easy to deploy, mak-
ing it promising for practical applications. How-
ever, for this approach, most existing works re-
quire accessing the token-level output probabili-
ties, while a substantial number of current LLMs
can only return generated content through lim-
ited APIs (e.g., ChatGPT). Consequently, we en-
courage future research in this direction to explore
within a more strict black-box setting.
5.4.2 Resorting to External Knowledge
Using external knowledge as supplementary ev-
idence to assist LLMs in providing truthful re-
sponses recently represents a burgeoning solution
(Ren et al., 2023; Mialon et al., 2023). This ap-
proach typically consists of two steps. The first
step entails accurately obtaining knowledge re-
lated to the user instructions. Once useful knowl-
edge has been achieved, the second step involves
14



### Claim 109/179

#### Claim Text
doped mode-locked fiber laser (Menlo Systems AB, O range High Power) with a repetition rate frep = 125 MHz, and a Raman-shifted soliton (signal) generated from the same source in a highly non-linear fiber, centered at 1.680 μm .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[103]_2303.08559.pdf (Page 4):

56.0
57.0
58.0
59.0
60.0
I0 I1 I2 I3 I4 I5
Instruction format
F1 score
36
40
44
48
52
56
60
4 8 16 32 64 96
Demonstration number
F1 score ChatGPT
CODEX 52.5
55.0
57.5
60.0
random embed epr
Demonstration selection
F1 score
Figure 3: LLMs’ performance w.r.t prompt variants on 20-shot FewNERD dataset. See full results on other datasets
in Appendix E.2- E.5. Left: ChatGPT’s performance (F1 Score) across six instruction variants. Middle: F1 Score
changes over varying numbers of demo. Right: ChatGPT’s performance across three demo selection strategies.
Random: Random sampling. Embed: Sentence embedding. EPR: Efficient Prompt Retriever (Rubin et al., 2022).
3.4 Main Results
We summarize the main experimental outcomes in
Figure 2, indicating that LLMs only outperform
SLMs in environments with restricted labels and
samples. Conversely, SLMs are generally more
effective. Given (1) the practicality of fine-grained
IE tasks and the manageable effort of obtaining 10-
20 annotations per label and (2) the excessive time
and budget demands of LLM inference, we con-
clude that LLMs are not as effective as supervised
SLMs for few-shot IE tasks under real scenarios.
We detail our findings as below.
Performance w.r.t sample number.The perfor-
mance dynamics of SLMs and LLMs are influenced
by variations in sample size. Under extremely low-
resource (1-shot or 5-shot) settings, LLMs some-
times present superior performance than SLMs.
Yet, LLMs tend to reach a performance plateau
with only modest increases in sample size. Con-
versely, SLMs demonstrate marked performance
enhancement as sample sizes grow. This trend is
evident in Figure 2, where the SLM trajectories
(represented by dashed lines) ascend more steeply
compared to the LLM ones (solid lines).
Performance w.r.t label number.Compared with
SLMs, LLMs tend to struggle on fine-grained
datasets. For instance, LLMs perform relatively
worse on MA VEN and RAMS datasets (with
168/139 labels) than on CONLL (4 labels only).
Detailed quantitative results are shown in Ap-
pendix E.1, illustrating a clear negative correlation
between the label number and the result disparity
between LLMs and SLMs across various IE tasks.
Comparisons among LLMs.We observe perfor-
mance variability among LLMs. (1) Open-source
models, LLaMA and Vicuna, significantly lag be-
hind proprietary LLMs across all few-shot IE tasks.
(2) Among proprietary LLMs, ChatGPT performs
better on NER and EAE tasks, but poorer so on RE
and ED tasks. InstructGPT and CODEX demon-
strate comparable performance across these tasks.
LLMs show limited inference speed.We compare
the inference speed of different methods and show
their results in Table 1. We observe that LLMs
is much slower than SLMs since they have much
more parameters, longer input contexts and extra
response decay (if external APIs applied).
3.5 Analysis on Prompt Sensitivity
Previous work (Lu et al., 2022b) indicates that the
efficacy of LLMs on specific tasks can be signifi-
cantly influenced by the construction of the prompt.
To ensure that LLMs’ suboptimal outcomes are
not erroneously ascribed to inappropriate prompt
designs, we meticulously examine the impact of
diverse prompt variations from four aspects,i.e., in-
struction format, demo number, demo selector and
prompt format. We leave comprehensive details
of the variants and their results to Appendix E.2-
E.5, and illustrate salient findings in Figure 3. Our
findings include that (1) diverse instruction strate-
gies yield comparable results in IE task; (2) in-
creasing the number of samples in demonstrations
does not unequivocally enhance performance; and
(3) The selection strategy of demonstration mat-
ters, and retrieval based on sentence embedding
Table 1: The inference seconds over 500 sentences (run
on single V100 GPU). Here LLaMA is extremely slow
since we set batch size as 1 due to memory limit.
Dataset (Task) Roberta T5 LLaMA CODEX
FewNERD (NER) 2.8 39.4 1135.4 179.4
TACREV (RE) 1.4 45.6 1144.9 151.6
ACE05 (ED) 6.6 62.5 733.4 171.7



Source: data\tc16_2312.10997v5\referenced_papers\[36]_2303.08559.pdf (Page 4):

56.0
57.0
58.0
59.0
60.0
I0 I1 I2 I3 I4 I5
Instruction format
F1 score
36
40
44
48
52
56
60
4 8 16 32 64 96
Demonstration number
F1 score ChatGPT
CODEX 52.5
55.0
57.5
60.0
random embed epr
Demonstration selection
F1 score
Figure 3: LLMs’ performance w.r.t prompt variants on 20-shot FewNERD dataset. See full results on other datasets
in Appendix E.2- E.5. Left: ChatGPT’s performance (F1 Score) across six instruction variants. Middle: F1 Score
changes over varying numbers of demo. Right: ChatGPT’s performance across three demo selection strategies.
Random: Random sampling. Embed: Sentence embedding. EPR: Efficient Prompt Retriever (Rubin et al., 2022).
3.4 Main Results
We summarize the main experimental outcomes in
Figure 2, indicating that LLMs only outperform
SLMs in environments with restricted labels and
samples. Conversely, SLMs are generally more
effective. Given (1) the practicality of fine-grained
IE tasks and the manageable effort of obtaining 10-
20 annotations per label and (2) the excessive time
and budget demands of LLM inference, we con-
clude that LLMs are not as effective as supervised
SLMs for few-shot IE tasks under real scenarios.
We detail our findings as below.
Performance w.r.t sample number.The perfor-
mance dynamics of SLMs and LLMs are influenced
by variations in sample size. Under extremely low-
resource (1-shot or 5-shot) settings, LLMs some-
times present superior performance than SLMs.
Yet, LLMs tend to reach a performance plateau
with only modest increases in sample size. Con-
versely, SLMs demonstrate marked performance
enhancement as sample sizes grow. This trend is
evident in Figure 2, where the SLM trajectories
(represented by dashed lines) ascend more steeply
compared to the LLM ones (solid lines).
Performance w.r.t label number.Compared with
SLMs, LLMs tend to struggle on fine-grained
datasets. For instance, LLMs perform relatively
worse on MA VEN and RAMS datasets (with
168/139 labels) than on CONLL (4 labels only).
Detailed quantitative results are shown in Ap-
pendix E.1, illustrating a clear negative correlation
between the label number and the result disparity
between LLMs and SLMs across various IE tasks.
Comparisons among LLMs.We observe perfor-
mance variability among LLMs. (1) Open-source
models, LLaMA and Vicuna, significantly lag be-
hind proprietary LLMs across all few-shot IE tasks.
(2) Among proprietary LLMs, ChatGPT performs
better on NER and EAE tasks, but poorer so on RE
and ED tasks. InstructGPT and CODEX demon-
strate comparable performance across these tasks.
LLMs show limited inference speed.We compare
the inference speed of different methods and show
their results in Table 1. We observe that LLMs
is much slower than SLMs since they have much
more parameters, longer input contexts and extra
response decay (if external APIs applied).
3.5 Analysis on Prompt Sensitivity
Previous work (Lu et al., 2022b) indicates that the
efficacy of LLMs on specific tasks can be signifi-
cantly influenced by the construction of the prompt.
To ensure that LLMs’ suboptimal outcomes are
not erroneously ascribed to inappropriate prompt
designs, we meticulously examine the impact of
diverse prompt variations from four aspects,i.e., in-
struction format, demo number, demo selector and
prompt format. We leave comprehensive details
of the variants and their results to Appendix E.2-
E.5, and illustrate salient findings in Figure 3. Our
findings include that (1) diverse instruction strate-
gies yield comparable results in IE task; (2) in-
creasing the number of samples in demonstrations
does not unequivocally enhance performance; and
(3) The selection strategy of demonstration mat-
ters, and retrieval based on sentence embedding
Table 1: The inference seconds over 500 sentences (run
on single V100 GPU). Here LLaMA is extremely slow
since we set batch size as 1 due to memory limit.
Dataset (Task) Roberta T5 LLaMA CODEX
FewNERD (NER) 2.8 39.4 1135.4 179.4
TACREV (RE) 1.4 45.6 1144.9 151.6
ACE05 (ED) 6.6 62.5 733.4 171.7



Source: data\tc16_2312.10997v5\referenced_papers\[103]_2303.08559.pdf (Page 2):

T ext 
T ext 
T ext 
Relation ExtractionNamed Entity Recognition 
Event Detection Event Argument Extraction
Figure 1: Examples of prompts used. The green, blue and black parts in the top boxes represent the instruction,
demonstration (demo) and test sentence in the prompt respectively. The red parts represent the outputs from LLMs.
We plot only 1 example for convenience of visualization. The actual demo number is usually much larger than 1.
2015). (4) Event Argument Extraction (EAE):
ACE05, ERE and RAMS (Ebner et al., 2020). With
label numbers ranging from 4 to 168, we assess
LLMs’ performance under different schema com-
plexities. See their details in Appendix A.1.
Few-shot SetWe construct few-shot datasets from
the original datasets above. For training and vali-
dation set, we adopt K-shot sampling strategy, i.e.,
sampling K samples for each label type. See more
details in Appendix A.2. For test set, we down-
sample their original test sets to reduce the cost
of LLMs. We randomly sample 500 sentences for
RE tasks, and 250 sentences for other task. We en-
sure that each label has at least one corresponding
sample to avoid the absence of rare labels.
Evaluation We adopt micro-F1 score in NER, RE
and ED tasks. For EAE task, we follow previous
work (Wang et al., 2023b) and adopt head-F1 score,
which merely considers matching of the head word
rather than the whole content of a text span. We re-
port averaged score w.r.t 5 sampled train/validation
sets unless otherwise stated.
3.2 Small Language Models
We adopt five supervised methods to evaluate the
abilities of SLMs. (1) Vanilla fine-tuning for all
tasks, (2) FSLS (Ma et al., 2022a) for NER and ED
tasks, (3) KnowPrompt (Chen et al., 2022b) for RE
task, (4) PAIE (Ma et al., 2022b) for EAE task, and
(5) UIE (Lu et al., 2022c) for all tasks. See their
details in Appendix B.
3.3 Large Language Models
Detailed in Appendix C, we evaluate the ICL abil-
ities of LLMs. Given labeled sentences D =
{(si, yi)} and a test sentence s, our goal is to pre-
dict structured information y from s using a frozen
LLM L. We feed LLM with prompt PE,I,f (D, s):
PE,I,f (D, s) = [I; f(E(D, s)); f(s)] (1)
We give examples of prompts on four IE tasks
in Figure 1. The prompts consist of three parts: in-
struction I (color in green in Figure 1), demonstra-
tion f(E(D, s)) (demo; color in blue) and the ques-
tion f(x) (color in black). Here E denotes demo
selector and E(D, s) ⊂ D denotes selected sen-
tences as the demo to predict s. Prompt format f 5
refers to the template which converts demoE(D, s)
and sample s to input context for LLMs. Then
LLM generates f(y) (color in red) from which we
could readily parse the extraction results y.
Models L: We explore six LLMs from two
sources. (1) OpenAI models 6: we employ Chat-
5We slightly abuse the notationf to allow s, y and {(s, y)}
as the input for simplicity.
6The versions of model we use are:gpt-3.5-turbo-0301,



Source: data\tc16_2312.10997v5\referenced_papers\[36]_2303.08559.pdf (Page 2):

T ext 
T ext 
T ext 
Relation ExtractionNamed Entity Recognition 
Event Detection Event Argument Extraction
Figure 1: Examples of prompts used. The green, blue and black parts in the top boxes represent the instruction,
demonstration (demo) and test sentence in the prompt respectively. The red parts represent the outputs from LLMs.
We plot only 1 example for convenience of visualization. The actual demo number is usually much larger than 1.
2015). (4) Event Argument Extraction (EAE):
ACE05, ERE and RAMS (Ebner et al., 2020). With
label numbers ranging from 4 to 168, we assess
LLMs’ performance under different schema com-
plexities. See their details in Appendix A.1.
Few-shot SetWe construct few-shot datasets from
the original datasets above. For training and vali-
dation set, we adopt K-shot sampling strategy, i.e.,
sampling K samples for each label type. See more
details in Appendix A.2. For test set, we down-
sample their original test sets to reduce the cost
of LLMs. We randomly sample 500 sentences for
RE tasks, and 250 sentences for other task. We en-
sure that each label has at least one corresponding
sample to avoid the absence of rare labels.
Evaluation We adopt micro-F1 score in NER, RE
and ED tasks. For EAE task, we follow previous
work (Wang et al., 2023b) and adopt head-F1 score,
which merely considers matching of the head word
rather than the whole content of a text span. We re-
port averaged score w.r.t 5 sampled train/validation
sets unless otherwise stated.
3.2 Small Language Models
We adopt five supervised methods to evaluate the
abilities of SLMs. (1) Vanilla fine-tuning for all
tasks, (2) FSLS (Ma et al., 2022a) for NER and ED
tasks, (3) KnowPrompt (Chen et al., 2022b) for RE
task, (4) PAIE (Ma et al., 2022b) for EAE task, and
(5) UIE (Lu et al., 2022c) for all tasks. See their
details in Appendix B.
3.3 Large Language Models
Detailed in Appendix C, we evaluate the ICL abil-
ities of LLMs. Given labeled sentences D =
{(si, yi)} and a test sentence s, our goal is to pre-
dict structured information y from s using a frozen
LLM L. We feed LLM with prompt PE,I,f (D, s):
PE,I,f (D, s) = [I; f(E(D, s)); f(s)] (1)
We give examples of prompts on four IE tasks
in Figure 1. The prompts consist of three parts: in-
struction I (color in green in Figure 1), demonstra-
tion f(E(D, s)) (demo; color in blue) and the ques-
tion f(x) (color in black). Here E denotes demo
selector and E(D, s) ⊂ D denotes selected sen-
tences as the demo to predict s. Prompt format f 5
refers to the template which converts demoE(D, s)
and sample s to input context for LLMs. Then
LLM generates f(y) (color in red) from which we
could readily parse the extraction results y.
Models L: We explore six LLMs from two
sources. (1) OpenAI models 6: we employ Chat-
5We slightly abuse the notationf to allow s, y and {(s, y)}
as the input for simplicity.
6The versions of model we use are:gpt-3.5-turbo-0301,



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 12):

For generating our synthetic answers, we use the
following prompt for FLAN-T5 XXL:
• Example #1
Query: <few-shot example here>
Document: <few-shot example here>
Answer: <few-shot example here>
Example #2
Query: <few-shot example here>
Document: <few-shot example here>
Answer: <few-shot example here>
Example #3
Query: <few-shot example here>
Document: <few-shot example here>
Answer: <few-shot example here>
Example #4
Query: <synthetic query here>
Document: <in-domain passage here>
Answer:



### Claim 110/179

#### Claim Text
Notably, the sum of the transition rates is uniform as in but they are not symmetric, in the sense of , so that the µ-chemoEH can effectively symmetrize the system composed of the two filaments and the motors.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[42]_2208.03299.pdf (Page 26):

Table 13: MMLU scores with de-biasing:
Setting Model All Hum. Soc. Sci. STEM Other
zero-shot
Standard 36.8 37.5 39.0 30.2 39.7
All permutations 48.5 45.7 55.2 39.4 54.4
Cyclic Permutations 47.1 43.6 54.1 38.0 54.9
5-shot
Standard 43.4 41.8 49.3 33.9 48.8
All permutations 49.0 46.0 56.1 40.5 54.6
Cyclic Permutations 47.9 46.1 54.6 38.8 52.8
A Training details and additional results
A.1 MMLU
A.1.1 Training Details
Featurization MMLU consists of multiple choice questions with four possible lexicalized answer options.
We represent the input using the following template:
question: {question text}
options: (A) {answer 1 text} (B) {answer 2 text} (C) {answer 3 text} (D) {answer 4 text}
answer: [MASK_0]
and train the model to generate the mask token followed by the letter of the correct answer:
[MASK_0] {correct answer option letter}
This format closely matches the format of MLM pre-training objective, aiding few-shot learning. When
training, we permute the order of the answer options, i.e. shuﬄing which answer option appears as letter A
etc. This helps reduce overﬁtting, and encourages a uniform prior on the letters.
Standard inference Once trained we obtain predictions from the model by selecting the pre-softmax
logits for the tokens A, B, C and D, and performing a softmax over them to obtain a distribution over the 4
answer options. For standard inference, we then simply return the answer corresponding to the argmax of
this distribution.
De-biased Inference As mentioned in the main text, even though our model is ﬁnetuned with data
that encourages a uniform prior over answer letters (by permuting which answer option letter is used with
which lexical answer option text in training data), this may not be enough to ensure the model has no
residual bias towards speciﬁc letters. Consider answersa, questionsq and a nuisance variablez∈Z, which
represents the ordering of the answer options or, equivalently, which answer letter gets assigned to which
answer option text. There are 4 answer options in MMLU, and thus|Z|= 24 unique ways they can be
ordered, or assigned to given letters. Running our model with our standard inference for a questionq,
corresponds to calculatingp(a|q= q,z = z) for the answer orderingz that happens to appear in the dataset.
We can control forz by running the model with all possible answer orderings in the input, and marginalizing:
p(a|q = q) = ∑
z′∈Zp(a|q = q,z = z′)p(z = z′|q = q), and assumingp(z = z′|q = q) is uniform (no answer
ordering is more likely than another), this reduces to simplyp(a|q = q) ∝∑
z′∈Zp(a|q = q,z = z′). This
procedure requires 24 forward passes, one for each answer ordering, so is 24×slower than standard inference.
Table 13 shows the result of applying the full permutation de-biasing, which leads to an 12% improvement
zero-shot and 6% in 5-shot performance overall. Empirically, using only the cyclic permutations of the answer
order provided in the original dataset (of which there are 4) works nearly as well, which is what we report in
the main paper, and only increases inference compute by a factor of 4, rather than 24. Cyclic permutation
de-biasing improves over standard inference by 10% in zero-shot and 5% in 5-shot. Empirically, de-biased
inference is largely unnecessary when training in the 5-shot multitask or full dataset setting, as there is
enough data for the model to learn a more uniform prior over the letters.
27



Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[42]_2208.03299.pdf (Page 31):

Domain zero-shot 5-shot 5-shot (multi-task) Full / Transfer
All 47.1 47.9 56.6 66.0
Humanities 43.6 46.1 50.1 61.1
Social Sciences 54.1 54.6 66.4 77.2
STEM 38.0 38.8 46.4 53.2
Other 53.9 52.8 66.2 74.4
abstract algebra 22.0 26.0 31.0 31.0
anatomy 48.9 47.4 62.2 70.4
astronomy 61.8 62.5 68.4 81.6
business ethics 60.0 57.0 62.0 70.0
clinical knowledge 50.6 49.4 66.4 72.8
college biology 51.4 53.5 61.1 77.8
college chemistry 36.0 39.0 39.0 45.0
college computer science 32.0 32.0 33.0 49.0
college mathematics 30.0 35.0 35.0 34.0
college medicine 44.5 41.0 52.6 67.6
college physics 24.5 26.5 37.3 42.2
computer security 59.0 59.0 68.0 76.0
conceptual physics 37.0 41.3 57.0 60.0
econometrics 20.2 20.2 36.8 37.7
electrical engineering 37.9 40.0 50.3 65.5
elementary mathematics 31.2 28.0 30.7 36.5
formal logic 27.8 27.0 32.5 35.7
global facts 41.0 43.0 51.0 53.0
high school biology 53.2 56.5 68.7 83.2
high school chemistry 41.9 41.4 49.3 51.2
high school computer science 40.0 36.0 46.0 60.0
high school european history 56.4 58.8 68.5 80.6
high school geography 57.1 59.6 71.2 81.3
high school gov. and pol. 67.9 67.9 77.2 90.2
high school macroeconomics 46.9 48.5 57.9 65.9
high school mathematics 28.1 28.9 34.1 31.5
high school microeconomics 51.7 51.7 68.9 82.4
high school physics 26.5 25.8 32.5 41.1
high school psychology 66.2 65.5 78.9 86.8
high school statistics 31.5 30.1 43.1 45.8
high school us history 57.8 54.9 64.7 77.5
high school world history 59.1 62.9 65.4 79.3
human aging 48.4 50.7 60.5 70.4
human sexuality 55.7 54.2 61.8 84.0
international law 66.1 72.7 71.9 84.3
jurisprudence 61.1 64.8 72.2 81.5
logical fallacies 54.6 57.7 71.2 77.9
machine learning 37.5 39.3 43.8 44.6
management 56.3 56.3 79.6 89.3
marketing 72.2 73.1 84.6 91.9
medical genetics 55.0 58.0 71.0 81.0
miscellaneous 69.7 67.8 83.8 90.4
moral disputes 45.1 46.8 60.1 72.3
moral scenarios 24.5 30.3 25.8 38.5
nutrition 56.5 53.9 67.0 77.1
philosophy 56.3 57.6 70.7 77.2
prehistory 59.3 60.5 71.6 78.7
professional accounting 35.1 33.0 42.2 50.7
professional law 36.3 38.4 39.4 51.7
professional medicine 35.7 33.1 52.2 60.7
professional psychology 47.7 49.3 60.9 74.0
public relations 54.5 53.6 68.2 68.2
security studies 47.3 45.7 59.2 73.9
sociology 62.2 62.7 71.6 84.6
us foreign policy 64.0 68.0 73.0 83.0
virology 39.8 40.4 44.6 51.8
world religions 77.2 74.9 80.7 87.1
Table 18: MMLU Test set scores for the de-biasedAtlas-XXL using cyclic permutations for each of the 57
domains for zero-shot, 5 shot, 5-shot-multitask and the transfer setting.
32



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 18):

Table 11: Confidence region for SOTA model in XSum and BigPatent.
System ROUGE-1/2/L 95%-conf.int
XSum
BRIOjoint
50.3 0.49986 - 0.50602
26.7 0.26300 - 0.26989
41.6 0.41231 - 0.41900
BigPatent
BARTjoint
62.9 0.62664 - 0.63080
48.1 0.47783 - 0.48333
59.6 0.59401 - 0.59847
Table 12: Generation Latency analysis.
NMT XSum BigPatent DailyDialog
Average Input Length 87 512 1024 71
Average Output Length 44 75 127 16
CPU
Retrieval-augmented Baseline 0.97 1.79 3.16 0.32
Selfmem
Candidate Generation 3.20 7.50 15.00 1.02
Memory Selection 0.50 0.52 0.95 0.14
Hypothesis Generation 0.97 1.79 3.00 0.32
×4.80 ×5.47 ×6.04 ×4.63
CUDA
Retrieval-augmented Baseline 0.29 0.44 0.75 0.10
Selfmem
Candidate Generation 0.51 1.00 1.72 0.18
Memory Selection 0.01 0.01 0.01 0.01
Hypothesis Generation 0.29 0.44 0.75 0.10
×2.76 ×2.99 ×3.35 ×2.91
19



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 14):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Are the following two answers to the given question equivalent? Do not
consider whether the answers are right or wrong, but only whether they
are equivalent. Directly state ”Yes” or ”No”.
Question: Which title was conferred to Anna Muzychuk in 2007?
Answer 1: Anna Muzychuk was conferred the title of International
Master (IM) in 2007. She earned the title by scoring three norms in rapid
chess tournaments.
Answer 2: International Master
Answer 1 (short): International Master
Answer 2 (short): International Master
Are the two answers equivalent?Yes
Question: What state is Seattle located in?
Answer 1: Seattle is in Washington State.
Answer 2: The answer is George Washington.
Answer 1 (short): Washington State
Answer 2 (short): George Washington
Are the two answers equivalent?No
Question: <Question>
Answer 1: <Model Output>
Answer 2: <Target Label>
Table 6: Illustration of few shot evaluation with the PaLM-2L model.
MMLU Physics/Chemistry First-Principle Prompt
You are an expert at Physics/Chemistry. You are given
a Physics/Chemistry problem. Your task is to extract the
Physics/Chemistry concepts and principles involved in solving
the problem. Here are a few examples:
Question: <Question Example1>
Principles Involved: <Principles Example1>
...
Question: <Question Example5>
Principles Involved: <Principles Example5>
Question: <Question>
Principles Involved:
Table 7: Prompt of extracting the underlying principles involved in MMLU physics and chemistry
questions.
C.2 H YPER -PARAMETERS FOR EVALUATION WITH PALM-2L
We use PaLM-2L as the scoring model for evaluation. We experiment with different sampling
temperatures, and find that T = 1 gives us a highly-accurate evaluation. For example, we sampled
100 test examples and the model predictions, and manually rated the correctness of the model scoring.
We found that out of 4 trials, the model scoring agrees with human ratings 97%, 98%, 99% and 99%
of the time.
D P ROMPTS AND FEW SHOT EXAMPLES
D.1 STEM
For MMLU high-school Physics and Chemistry, we first prompt the model to generate the first
principles behind the question. Using the generated first principles, we further prompt the model to
generate the final answer through few-shot demonstrations The prompt generating first principles is
shown in Table 7 for MMLU high-school Physics and Chemistry.
15



### Claim 111/179

#### Claim Text
Pressure values measured at several annealing points are also given. 0 2 4 6 8 Time [h] 0 200 400 600 800 1000 1200 1400 T emperature [°C] Pressure -implanted Pressure -implanted Pressure -doped 10 7 10 6 Pressure [mbar] FIG.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 15):

0 50 100 150
1.6
1.8
2
2.2
2.4
2.6
2.8
3 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(a) ELI5 - p = 10%
0 50 100 150
1.4
1.45
1.5
1.55
1.6
1.65 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (b) ELI5 - p = 30%
0 50 100 150
1.04
1.06
1.08
1.1
1.12
1.14 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (c) ELI5 - p = 50%
0 10 20 30 40 50
1.6
1.8
2
2.2
2.4
2.6
2.8
3
3.2
3.4 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(d) MS MARCO - p = 10%
0 10 20 30 40 50
1.15
1.2
1.25
1.3
1.35
1.4 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
0.91 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (f) MS MARCO - p = 50%
0 20 40 60 80 100
1.5
2
2.5
3
3.5
1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(g) NQ - p = 10%
0 20 40 60 80 100
1.5
1.6
1.7
1.8
1.9
2 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
Loa ding [ M a thJ a x ] / ex tensions/ M a thM enu .j s (h) NQ - p = 30%
0 20 40 60 80 100
1.22
1.24
1.26
1.28
1.3
1.32
1.34
1.36 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (i) NQ - p = 50%
Figure 7: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-12), for FiD-Base models.
Each row represents a different dataset, and every column represents a different filtering percentage (10,30,50).



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 13):

0 1000 2000 3000 4000
24.5
25
25.5
26
26.5
27
FLOPS (MACs)
Rouge-L
2%
(a) ELI5
0 1000 2000 3000 4000
20
21
22
23
24
25
FLOPS (MACs)
2% (b) MS MARCO
0 1000 2000 3000 4000
24
25
26
27
28
29
30
31
32
FLOPS (MACs)
2% (c) NQ
0 20 40 60 80 100
24.5
25
25.5
26
26.5
27
Input P sgs
Rouge-L
2%
(d) ELI5
0 20 40 60 80 100
20
21
22
23
24
25
Input P sgs
2% (e) MS MARCO
0 20 40 60 80 100
24
25
26
27
28
29
30
31
32
FiD
CALM
T ok en Filtering
Combined
Input P sgs
2% (f) NQ
Figure 6: The ROUGE-L performance on FiD-Base vs. the FLOPS (MACs) (First row), and vs. the input passage
amount (Input Psg., second row). The results are on the test set for each dataset, for the various methods utilized.
We observe that the trends of the FLOPS (MACs) and the Input Psg. are very similar, since the passages effect the
encoder the most, and the encoder has the most impact on FLOPS (MACs) (as stated by de Jong et al. (2022)).



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 18):

Table 11: Confidence region for SOTA model in XSum and BigPatent.
System ROUGE-1/2/L 95%-conf.int
XSum
BRIOjoint
50.3 0.49986 - 0.50602
26.7 0.26300 - 0.26989
41.6 0.41231 - 0.41900
BigPatent
BARTjoint
62.9 0.62664 - 0.63080
48.1 0.47783 - 0.48333
59.6 0.59401 - 0.59847
Table 12: Generation Latency analysis.
NMT XSum BigPatent DailyDialog
Average Input Length 87 512 1024 71
Average Output Length 44 75 127 16
CPU
Retrieval-augmented Baseline 0.97 1.79 3.16 0.32
Selfmem
Candidate Generation 3.20 7.50 15.00 1.02
Memory Selection 0.50 0.52 0.95 0.14
Hypothesis Generation 0.97 1.79 3.00 0.32
×4.80 ×5.47 ×6.04 ×4.63
CUDA
Retrieval-augmented Baseline 0.29 0.44 0.75 0.10
Selfmem
Candidate Generation 0.51 1.00 1.72 0.18
Memory Selection 0.01 0.01 0.01 0.01
Hypothesis Generation 0.29 0.44 0.75 0.10
×2.76 ×2.99 ×3.35 ×2.91
19



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 17):

0 50 100 150
1.4
1.6
1.8
2
2.2
2.4
2.6 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(a) ELI5 - p = 10%
0 50 100 150
1.35
1.4
1.45
1.5
1.55
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (b) ELI5 - p = 30%
0 50 100 150
1.04
1.06
1.08
1.1
1.12
1.14 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (c) ELI5 - p = 50%
0 10 20 30 40 50
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
3.2 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(d) MS MARCO - p = 10%
0 10 20 30 40 50
1.1
1.15
1.2
1.25
1.3
1.35 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.84
0.86
0.88
0.9
0.92 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (f) MS MARCO - p = 50%
0 20 40 60 80 100 120
1.5
2
2.5
3
3.5
4
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(g) NQ - p = 10%
0 20 40 60 80 100 120
1.5
1.6
1.7
1.8
1.9
2
2.1 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
Loa ding [ M a thJ a x ] / ex tensions/ M a thM enu .j s (h) NQ - p = 30%
0 20 40 60 80 100 120
1.25
1.3
1.35
1.4
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (i) NQ - p = 50%
Figure 9: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-24), for FiD-Large models.
Each row represents a different dataset, and every column represents a different filtering percentage (10,30,50).



Source: data\tc16_2312.10997v5\referenced_papers\[38]_2307.07164.pdf (Page 13):

Input
What happens next in this paragraph? How to survive remedial classes Look at the course as an opportunity.
Many students are discouraged when they are assigned to a remedial class. Some assume this placement
means they aren’t ready for college. OPTIONS:
A) However, people who are not unable to do what they’re given on campus, or those who are cut out
from college academies, are likely to have some little snitches. You want to be prepared for a negative
outcome if possible.
B) In this case, you should consider what you will do if your subject consists of a certain term or number
of subject areas. You could set up a study study program yourself or tutor a student who is struggling to
thoroughly comprehend where they sat for homework.
C) If you take the course, you might find you feel highly motivated after passing the test. Try to develop a
positive attitude towards the course so that you are not discouraged when you take your homework at the
end of the day.
D) However, being assigned a remedial class doesn’t mean that you are behind, just that you have an
opportunity to receive better instruction and improve your skills in a subject that you have struggled
with in the past. There is nothing unusual about being asked to attend a remedial course: two thirds of
community college students take at least one remedial course.
Output D
Table 9: Input-output format for GPT-35-Turbo. This example is from the HellaSwag dataset. We add some line
breaks for better readability.
Task Zero-shot Random Kmeans BM25 E5 base SBERT EPR LLM-R
1 iter 2 iter 3 iter
AESLC 5.8 19.4 19.0 26.8 27.0 25.3 26.0 26.7 27.3 27.1
AGNews 31.5 67.4 71.9 90.6 90.6 90.2 91.8 92.4 93.5 93.5
ARC Chall. 35.6 39.7 40.5 40.3 44.6 42.8 43.0 43.4 43.6 44.0
ARC Easy 51.0 60.0 61.8 59.9 63.0 63.1 63.1 63.6 63.3 63.6
BoolQ 64.7 70.0 69.0 74.7 72.4 73.9 74.8 75.6 75.1 74.1
CommonGen 19.2 36.3 34.4 37.6 37.4 37.6 39.2 38.2 37.7 37.3
COPA 66.0 80.0 85.0 78.0 83.0 82.0 82.0 84.0 84.0 84.0
DART 22.9 52.0 46.6 55.9 54.7 54.4 56.2 57.3 57.2 57.3
E2E NLG 34.6 52.7 46.4 54.5 51.8 50.2 53.6 54.9 54.7 54.9
Gigaword 15.3 30.0 30.7 32.7 32.5 32.6 32.4 33.3 32.5 31.8
HellaSwag 71.5 73.9 74.0 74.9 75.2 75.3 75.2 75.4 75.5 75.4
MNLI (m) 35.8 46.3 44.2 50.1 44.5 50.8 59.9 68.2 70.2 69.8
MNLI (mm) 35.6 48.1 45.4 48.3 44.7 49.3 61.5 69.5 72.0 71.3
MRPC 69.1 49.5 38.0 61.8 41.2 52.7 55.9 62.3 75.3 78.2
MultiRC 57.0 48.5 34.1 54.2 56.0 55.3 50.4 52.9 51.5 52.1
NQ 0.3 21.5 22.6 37.6 39.3 39.4 39.2 39.4 39.1 39.2
OpenBook QA 41.6 49.8 49.0 49.6 51.4 51.4 49.6 50.8 52.2 53.4
PAWS 53.2 57.0 56.6 56.6 55.4 58.2 57.7 57.0 56.6 57.0
PIQA 77.0 79.1 79.4 81.3 81.3 80.7 80.5 80.9 81.6 80.6
QNLI 49.2 56.4 53.4 62.2 61.5 61.9 65.0 74.4 69.6 69.4
QQP 57.7 63.4 63.3 79.8 77.5 81.3 81.7 80.1 82.6 83.3
RTE 59.6 59.9 58.5 65.7 63.9 67.2 66.8 67.2 68.6 70.4
Sentiment140 49.3 88.6 89.4 90.8 93.9 92.2 91.4 90.8 91.1 90.3
SNLI 39.8 43.7 52.5 47.1 53.5 58.4 68.4 80.2 82.0 82.2
SQuAD v1 2.1 64.1 62.3 61.2 60.8 61.6 64.3 60.7 57.3 52.5
SST2 54.4 85.9 89.7 84.4 92.1 87.6 88.7 94.0 93.8 93.1
Winogrande 62.0 66.7 66.5 67.5 66.9 66.5 66.5 67.9 68.1 67.2
WSC 64.4 60.6 56.7 56.7 61.5 63.5 61.5 60.6 63.5 66.4
WSC273 74.0 74.4 74.7 64.5 65.2 62.6 65.2 74.4 79.5 78.8
Yelp 47.9 92.0 93.5 93.5 97.3 95.9 95.1 95.7 95.9 95.5
Average 44.9 57.9 57.0 61.3 61.4 62.1 63.5 65.7 66.5 66.4
Table 10: Detailed results for each dataset.



### Claim 112/179

#### Claim Text
Gsell & Favier have shown that the discrepancy (i.e. the di fference between the updated and the desired velocities at the Lagrangian marker points) depends essentially on the reciprocity of the interpolation and spreading operators (5.4,5.6).

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 30):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question Physics students are checking the constant acceleration equations of kinematics
by measuring the velocity of a tennis ball that is dropped and falls 6 meters
and then passes through a photogate. The predicted velocity is 20% above the
velocity measured by the photogate. Which of the following best describes the
cause of the large percent difference?
0) The ball changes its shape while falling.
1) The acceleration of gravity varies as the ball is falling.
2) Air resistance increases the acceleration of the ball.
3) The acceleration of the balls varies with the velocity.
Principles Vertical motion: y = 0.5 * g * t2, where y is the vertical displacement, g is the
acceleration due to gravity, t is the time.
Free fall: is a special case of motion with constant acceleration, in which the
acceleration is due to gravity.
Acceleration to Velocity Formula: v = u + a * t, where v is the final speed, u is
the initial velocity, a is the acceleration, and t is the time.
Answer Using the Principles of Vertical motion and Horizontal motion, we can solve
the problem as following:
Step 1: Apply the Vertical motion equation to find the time it takes for the ball
to fall 6 meters.
y = 0.5 * g * t2
6 m = 0.5 * 9.8 m/s2 * t2
t = 1.12 s
Step 2: Apply the Horizontal motion equation to find the velocity of the ball
when it passes through the photogate.
x = v * t
v = x / t
v = 6 m / 1.12 s
v = 5.36 m/s
Step 3: Compare the predicted velocity with the velocity measured by the
photogate. The predicted velocity is 20% above the velocity measured by the
photogate, so the predicted velocity is 1.2 * 5.36 m/s = 6.43 m/s.
Step 4: Find the cause of the large percent difference. The predicted velocity is
6.43 m/s, but the velocity measured by the photogate is 5.36 m/s. The difference
is 1.07 m/s. This difference is caused by air resistance. Air resistance is a force
that opposes the motion of an object through the air. It is caused by the friction
between the object and the air molecules. The greater the surface area of the
object, the greater the air resistance. The tennis ball has a large surface area, so
it experiences a lot of air resistance. This air resistance slows down the tennis
ball and causes the difference between the predicted velocity and the velocity
measured by the photogate.
Therefore, the correct answer is 0) The ball changes its shape while falling.
Table 27: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Reasoning Error. In this case, the model was able to correctly identify air resistance is the cause of
the velocity difference, but made the error at the final reasoning step of deriving the right answer.
31



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 32):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: A liquid flows at a constant flow rate through a pipe
with circular cross-sections of varying diameters. At one point in the
pipe, the diameter is 2 cm and the flow speed is 18 m/s. What is the flow
speed at another point in this pipe, where the diameter is 3 cm?
0) 4 m/s
1) 6 m/s
2) 8 m/s
3) 12 m/s
Original Answer [Incorrect]: The answer is 3) 12 m/s A step-by-step
solution is below: The flow rate is constant, so the product of the speed
and the cross-sectional area is constant. The cross-sectional area is
proportional to the square of the diameter, so the speed is inversely
proportional to the square of the diameter. The ratio of the speeds is the
inverse of the ratio of the squares of the diameters, so the speed at the
second point is 18m/s ∗ (2cm/3cm)2 = 12 m/s.
Stepback Question: What are the physics principles behind this
question?
Principles:
Conservation of Mass: the amount of mass flowing through the pipe at
two different points must be equal, or A1 * v1 = A2 * v2, where A1 is
the area of cross-section one, v1 is the velocity at point 1, A2 is the area
of cross-section two, v2 is the velocity at point 2.
Area of Circle: A = π ∗ r2, where r is the radius of the circle.
Final Answer: Using the Principles of Continuity equation, we can
solve the problem as following:
Step 1: Apply the Continuity equation to find out the flow speed at
another point in this pipe. The continuity equation states that the product
of the area and the velocity is constant at all points in a pipe. In this
problem, we are given the following information:
A1 = π ∗ (2cm)2 = 4πcm2
v1 = 18 m/s
A2 = π ∗ (3cm)2 = 9πcm2
We can use the continuity equation to find v2:
A1 * v1 = A2 * v2
4πcm2 ∗ 18m/s = 9πcm2 ∗ v2
v2 = 8 m/s
Therefore, the flow speed at another point in this pipe is 8 m/s.
The correct answer is 2) 8 m/s.
Table 29: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
33



Source: data\tc16_2312.10997v5\referenced_papers\[172]_2309.17453.pdf (Page 15):

Published as a conference paper at ICLR 2024
Table 7: Accuracy (in %) on StreamEval with increasing query-answer distance. Each line in StreamEval
contains 23 tokens. Accuracies are averaged over 100 samples, and each sample contains 100 queries.
Llama-2-7B-32K-Instruct Cache Config
Line Distances Token Distances 4+2044 4+4092 4+8188 4+16380
20 460 85.80 84.60 81.15 77.65
40 920 80.35 83.80 81.25 77.50
60 1380 79.15 82.80 81.50 78.50
80 1840 75.30 77.15 76.40 73.80
100 2300 0.00 61.60 50.10 40.50
150 3450 0.00 68.20 58.30 38.45
200 4600 0.00 0.00 62.75 46.90
400 9200 0.00 0.00 0.00 45.70
600 13800 0.00 0.00 0.00 28.50
800 18400 0.00 0.00 0.00 0.00
1000 23000 0.00 0.00 0.00 0.00
and reconfiguring position encoding distances to enhance length generalization in LLMs. This
approach bears a resemblance to our methodology. However, our work uncovers the “attention sink"
phenomenon, wherein Transformer models tend to assign high attention scores to initial tokens
with small semantics. This phenomenon extends beyond the scope of length generalization failure,
indicating a more pervasive issue in Transformer models. We observe this “attention sink" behavior
not only in auto-regressive language models but also in encoder Transformers such as BERT (see
Section H), and Vision Transformers (ViTs) (Darcet et al., 2023), suggesting its broader prevalence in
Transformer architectures. To mitigate the “attention sink" phenomenon, we propose the introduction
of a learnable sink token during pre-training, and we support our findings with extensive ablation
studies.
In parallel, Darcet et al. observed similar attention concentration on random background patch tokens
in Vision Transformers, termed as "registers." These registers act as repositories for global image
information. Their solution, adding dedicated "register" tokens, aims to balance attention distribution.
Our finding of "attention sinks" parallels this concept. In our paper, the “attention sinks" are initial
tokens that disproportionately attract attention from subsequent tokens. Introducing a dedicated sink
token during pre-training prevents the model from inappropriately using content tokens as attention
sinks, leading to more effective attention distribution. However, a key difference exists: "registers" in
Vision Transformers function as global information holders within intermediate layers, whereas our
"attention sinks" are positioned as initial tokens in autoregressive models. This positional variance
suggests that the softmax function in attention computation might play a more fundamental role in
the emergence of attention sinks.
C A CCURACY ON STREAM EVAL WITH INCREASING QUERY-ANSWER LINE
DISTANCE
To assess StreamingLLM’s handling of extended inputs, we evaluated the Llama-2-7B-32K-Instruct
model on StreamEval, focusing on different query-answer line distances under various cache con-
figurations. In StreamEval, each line consists of 23 tokens, making the line distances equivalent
to token distances of 23 × line distances. Accuracy was calculated by averaging results over 100
samples, with each sample comprising 100 queries. Table 7 illustrates that StreamingLLM retains
accuracy when the token distance between the query and answer is within the cache size. However,
accuracy diminishes as this distance increases and eventually drops to zero when it surpasses the
cache capacity.
These results demonstrate that while StreamingLLM is effective in generating coherent text based on
recent context, it cannot extend the context length of language models. These results also emphasize
a broader challenge in current language models: their inability to fully utilize context information
within the cache, a finding that aligns with the observations made by Liu et al..
16



Source: data\tc16_2312.10997v5\referenced_papers\[101]_2310.06839.pdf (Page 3):

1 5 10 15 20
Number of Retained Documents
0
20
40
60
80
100Recall(%)
LLMLingua
BM25
OpenAI
Voyageai
BGE-large-en v1.5
SBERT
Gzip
Cohere-Rerank
BGE-llmembeder
Jina
LongLLMLingua rk 
w/o restrict
BGE-Ranker-large
LongLLMLingua rk
(a) Recall Distribution
1 5 10 15 20
Document Position in the Prompt
0.0
0.2
0.4
0.6
0.8
1.0Document Avg. Perplexity
Perplexity
Contrastive Perplexity (b) Perplexity Distribution (5th)
Figure 3: (a) Comparison of recall on NaturalQuestions Multi-documemnt QA dataset, which increases from top to
bottom in terms of Recall@1. Different colors represent different types of methods. Among them, yellow represents
traditional relevance methods, green signifies embedding-based methods, and red denotes rerank-based methods.
(b) Comparison between perplexities and contrastive perplexities of tokens in the prompt from Multi-documemnt
QA dataset. The document containing the ground-truth information is located in the 5th position. More results on
position can be found in the Appendix C.1.
iterative compression mechanism following LLM-
Lingua and directly calculate token perplexities
to compress xins and xque. In this section, we in-
vestigate how to make the fine-grained token-level
compression over {xdoc
k }K′
k=1 aware of the question
xque, so that the compressed results could contain
more question-relevant key information.
A straightforward solution for the awareness of
xque is to simply concatenate it at the beginning
of the whole context. However, this will result in
low perplexities of relevant tokens in the context
following the condition of question xque, further
reducing their differentiation from other tokens.
In this paper, we propose contrastive perplexity,
i.e., the distribution shift caused by the condition of
the question, to represent the association between
the token and the question. The contrastive perplex-
ity based importance metric si for each token xi in
{xdoc
k }K′
k=1 can be formulated as:
si = perplexity(xi|x<i)−perplexity(xi|xque, x<i).
(3)
Additionally, we provide the derivation of its
mathematical significance in the Appendix A, con-
cluding that it is equivalent to conditional pointwise
mutual information (Church and Hanks, 1989).
Figure 3b illustrates the difference between per-
plexities and contrastive perplexities. The distri-
bution of perplexities appears random, making it
challenging to extract information related to the
question. However, tokens with high contrastive
perplexities tend to cluster near the ground-truth
document, which contains information relevant to
the question. This suggests that the proposed con-
trastive perplexity can better distinguish tokens
relevant to the question, thus improving the key
information density in the compressed results.
4.2 How to reduce information loss in the
middle?
As demonstrated in Figure 1b, LLM achieves the
highest performance when relevant information oc-
curs at the beginning and significantly degrades if
relevant information is located in the middle of long
contexts. After the coarse-grained compression, we
have obtained a set of documents {xdoc
k }K′
k=1 with
their corresponding importance scores {rk}K′
k=1 in-
dicating their association with the question xque.
Therefore, we reorder documents using their impor-
tance scores to better leverage LLMs’ information
perception difference in positions:
(xins, xdoc
1 , ··· , xdoc
K′ ,xque)
rk
−→
(xins,xdoc
r1 , ··· , xdoc
rK′ , xque)
(4)
4.3 How to achieve adaptive granular control
during compression?
In fine-grained compression, LLMLingua applies
the same compression ratio over all documents ob-
tained from budget controller. However, the key
information density of different documents is differ-
ent. The more relevant to the question a document



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 28):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question Two spheres of net charge +5e and -6e briefly come into contact. Afterward,
which of the following is a possible combination of net charges for the two
spheres?
0) +3e and -4e
1) +1.5e and -2.5e
2) Both of the above
3) None of the above
Principles Law of conservation of charge: the total charge in a closed system remains
constant.
Coulomb’s Law: the mutual repulsion of like charges demands that the charges
be as far apart as possible.
Answer Using the Principles of Conservation of charge, we can solve the problem as
following:
Step 1: Apply Conservation of charge to find out the possible combination of
net charges for the two spheres.
The total charge of the two spheres before they come into contact is +5e - 6e =
-1e. After they come into contact, the total charge of the two spheres must still
be -1e.
Step 2: Find out the possible combination of net charges for the two spheres.
The possible combination of net charges for the two spheres are:
(1) +3e and -4e
(2) +1.5e and -2.5e
Therefore, the correct answer is 2) Both of the above.
Table 25: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Principle Error. In this case, Coulomb’s Law is not needed for solving this problem. Instead, the
Charge Quantization Principle is missing from the retrieved principles, and is in fact needed to rule
out option (2) of fractional charges.
29



### Claim 113/179

#### Claim Text
In addition, a few samples have been manufactured using typical technology for fabrication of superconducting qubits .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[35]_2402.13482.pdf (Page 14):

Table 7: Results of various filtering mechanisms on domain-specific QA datasets with training data augmentation settings.
Covid QA Policy QA Tech QA Average
Methods 10 30 100 10 30 100 10 30 100 10 30 100
RADA (Ours) 67.49 68.15 68.57 29.23 28.49 29.18 40.81 44.37 46.93 45.84 47.00 48.23
w/ ROUGE-based Filtering 66.21 67.25 66.84 28.35 28.09 28.31 37.75 44.64 46.74 44.10 46.66 47.30
w/ Embedding-based Filtering 67.19 67.67 67.27 28.62 28.13 28.65 40.02 44.64 46.74 45.27 46.82 47.55
w/o Answer Filtering 66.78 66.65 67.09 28.78 28.44 29.12 40.55 42.43 42.56 45.37 45.84 46.26
cally, to further promote diversity in the generated
samples from our RADA, we filter samples if they
are similar to the already generated samples, based
on their ROUGE scores or their embedding-level
distances. Then, as shown in Table 7, these filtering
techniques do not improve the model performance.
This may further strengthen our claim that the aug-
mented instances from RADA are already very di-
verse but also relevant to the seed data, which does
not necessitate additional filtering mechanisms. On
the other hand, if we relax the assumption that the
passage should include the answer to the question
for domain-specific QA, and subsequently do not
apply the filtering strategy (checking the inclusive-
ness), the performance drops slightly in Table 7.
Quantitative Analysis In Table 9, 10, 11, we pro-
vide examples of the augmented instances across
different methods on Covid QA, Policy QA, and
Tech QA. A key finding from these results is that
the existing approach that uses only the seed data re-
sults in a limited diversity of generated samples, un-
like our RADA which generates distinct yet contex-
tually coherent samples with the seed data, thanks
to the retrieval of relevant external samples.



Source: data\tc16_2312.10997v5\referenced_papers\[170]_2310.03025.pdf (Page 3):

Published as a conference paper at ICLR 2024
3 E XPERIMENTAL SETUP
In this section, we present the details of our experimental setup.
3.1 L ARGE LANGUAGE MODELS
We focus on comparing the zero-shot capability of integrating long context information for generative
QA or summarization tasks via retrieval or LLM’s own self-attention mechanism. In contrast to
most existing works that focus on relatively small models (e.g., 3B or 7B) (Kaiokendev, 2023;
Nijkamp et al., 2023; Tworkowski et al., 2023; Mohtashami & Jaggi, 2023), we gather the insights by
exploring model sizes that are larger than 40B after instruction tuning, as previous study suggests that
instruction tuning becomes effective when the decoder-only LLM has around 50B parameters (Wei
et al., 2021; 2022).
Specifically, we experimented with two pretrained GPT models, a proprietary Nemo GPT-43B and
Llama2-70B. GPT-43B is a 43 billion parameter model that is trained with 1.1T tokens with 70%
English corpus and the other 30% for multilingual and code data. For the English pretraining corpus,
GPT-43B used Common Crawl web archive (W ARC), Wikipedia, Reddit, Books, Gutenberg, ArXiv,
StackExchange, PubMed, etc. It contains 48 layers with the hidden dimension of 8,192. It is trained
with a sequence length of 4,096 and RoPE embeddings (Su et al., 2021). The other Llama2-70B is a
public available 70B GPT model trained on 2T tokens using around 90% English data. It contains 80
layers with the hidden dimension of 8,192. It also has the context window size of 4,096 and trained
with RoPE embeddings.
3.2 D ATASETS AND METRICS
In this study, we include seven datasets ranging from single document QA, multi document QA, to
query-based summarization for our zero shot evaluations. Specifically, we include four datasets from
the validation set of the Scroll benchmark (Shaham et al., 2022).
• QMSum (QM) (Zhong et al., 2021) is a query-based summarization dataset, consisting
of meetings’ transcripts and their corresponding summaries from multiple domains such as
academic, industrial product. In this task, a meeting dialogue transcript is given, and a question
to summarize certain topic is raised about the dialogue, such as “what is agreed between them”.
The answer generally contains a few sentences.
• Qasper (QASP)(Dasigi et al., 2021) is a question answering dataset over NLP papers filtered
from the Semantic Scholar Open Research Corpus (S2ORC) (Lo et al., 2020). Qasper contains
abstractive, extractive, and yes/no questions, as well as unanswerable ones. In this task, one
script is provided together with an information seeking question, such as “which multilingual
approaches do they compare with?”. A model needs to give a short answer by reasoning over
the given context.
• NarrativeQA (NQA) (Koˇciský et al., 2018) is an established question answering dataset over
entire books from Project Gutenberg3 and movie scripts from a list of websites. In this task, the
given passage is transcribed from books and is usually noisy. A model is required to generate a
short phrase by reasoning over the long and noisy text.
• QuALITY (QLTY)(Pang et al., 2022) is a question answering dataset over stories and articles
collected from several resources, such as Project Gutenberg and the Open American National
Corpus4. Different from all the other tasks, this is a multi-choices dataset and a model is required
to select one among four given choices.
We take another three datasets from LongBench (Bai et al., 2023).
• HotpotQA (HQA)(Yang et al., 2018) is a Wikipedia-based question-answer dataset. Different
from above single hot datasets, HQA is a multi-hop dataset where multiple supporting documents
are required to be read for answering and reasoning and the questions are diverse and not
constrained to any pre-existing knowledge bases.
3https://www.gutenberg.org/
4https://anc.org/
4



Source: data\tc16_2312.10997v5\referenced_papers\[60]_2310.03025.pdf (Page 3):

Published as a conference paper at ICLR 2024
3 E XPERIMENTAL SETUP
In this section, we present the details of our experimental setup.
3.1 L ARGE LANGUAGE MODELS
We focus on comparing the zero-shot capability of integrating long context information for generative
QA or summarization tasks via retrieval or LLM’s own self-attention mechanism. In contrast to
most existing works that focus on relatively small models (e.g., 3B or 7B) (Kaiokendev, 2023;
Nijkamp et al., 2023; Tworkowski et al., 2023; Mohtashami & Jaggi, 2023), we gather the insights by
exploring model sizes that are larger than 40B after instruction tuning, as previous study suggests that
instruction tuning becomes effective when the decoder-only LLM has around 50B parameters (Wei
et al., 2021; 2022).
Specifically, we experimented with two pretrained GPT models, a proprietary Nemo GPT-43B and
Llama2-70B. GPT-43B is a 43 billion parameter model that is trained with 1.1T tokens with 70%
English corpus and the other 30% for multilingual and code data. For the English pretraining corpus,
GPT-43B used Common Crawl web archive (W ARC), Wikipedia, Reddit, Books, Gutenberg, ArXiv,
StackExchange, PubMed, etc. It contains 48 layers with the hidden dimension of 8,192. It is trained
with a sequence length of 4,096 and RoPE embeddings (Su et al., 2021). The other Llama2-70B is a
public available 70B GPT model trained on 2T tokens using around 90% English data. It contains 80
layers with the hidden dimension of 8,192. It also has the context window size of 4,096 and trained
with RoPE embeddings.
3.2 D ATASETS AND METRICS
In this study, we include seven datasets ranging from single document QA, multi document QA, to
query-based summarization for our zero shot evaluations. Specifically, we include four datasets from
the validation set of the Scroll benchmark (Shaham et al., 2022).
• QMSum (QM) (Zhong et al., 2021) is a query-based summarization dataset, consisting
of meetings’ transcripts and their corresponding summaries from multiple domains such as
academic, industrial product. In this task, a meeting dialogue transcript is given, and a question
to summarize certain topic is raised about the dialogue, such as “what is agreed between them”.
The answer generally contains a few sentences.
• Qasper (QASP)(Dasigi et al., 2021) is a question answering dataset over NLP papers filtered
from the Semantic Scholar Open Research Corpus (S2ORC) (Lo et al., 2020). Qasper contains
abstractive, extractive, and yes/no questions, as well as unanswerable ones. In this task, one
script is provided together with an information seeking question, such as “which multilingual
approaches do they compare with?”. A model needs to give a short answer by reasoning over
the given context.
• NarrativeQA (NQA) (Koˇciský et al., 2018) is an established question answering dataset over
entire books from Project Gutenberg3 and movie scripts from a list of websites. In this task, the
given passage is transcribed from books and is usually noisy. A model is required to generate a
short phrase by reasoning over the long and noisy text.
• QuALITY (QLTY)(Pang et al., 2022) is a question answering dataset over stories and articles
collected from several resources, such as Project Gutenberg and the Open American National
Corpus4. Different from all the other tasks, this is a multi-choices dataset and a model is required
to select one among four given choices.
We take another three datasets from LongBench (Bai et al., 2023).
• HotpotQA (HQA)(Yang et al., 2018) is a Wikipedia-based question-answer dataset. Different
from above single hot datasets, HQA is a multi-hop dataset where multiple supporting documents
are required to be read for answering and reasoning and the questions are diverse and not
constrained to any pre-existing knowledge bases.
3https://www.gutenberg.org/
4https://anc.org/
4



Source: data\tc16_2312.10997v5\referenced_papers\[37]_2211.07067.pdf (Page 3):

the separation tokens in between them) as the ﬁnal
demonstration sequence.
Demonstration dr = Qr [sep] Cr [sep]
The answer is: Ar
We use S-BERT (Reimers and Gurevych, 2019) to
calculate the similarity scores between the current
instance and all demonstrations in ST. S-BERT is
a modiﬁcation of the BERT model (Devlin et al.,
2019) that uses siamese and triplet network struc-
tures to obtain semantically meaningful embed-
dings for word sequences7.
To construct the target (sequence), we ﬁrst de-
termine how much to learn from the demonstration
– if the similarity score is above a threshold (deter-
mined on the development set), and the demonstra-
tion and current instance both have a non-empty an-
swer, then we assign 1 (Yes) to yanalogy, otherwise
0 (No). Then we concatenate all argument spans of
the role with [sep_arg] to construct yseq2seq,
yseq2seq = <s> Argument1
[sep_arg] Argument2 [sep_arg] ...</s>
The ﬁnal y includes yseq2seq and yanalogy.
3.2 Training and Inference
Training After the preparation for S =
{(x(i),y(i))}|S|
i=1, we minimize the joint loss func-
tion during training,
L= Lseq2seq + Lanalogy
Lseq2seq = −
|S|∑
i=1
log p(y(i)
seq2seq|x(i); θ)
= −
|S|∑
i=1
|y(i)
seq2seq|∑
j=1
log p(y(i)
j |x(i),y(i)
<j; θ)
(2)
where Lseq2seq is the cross-entropy loss between
the decoder’s output and the target sequence
yseq2seq. Lanalogy is the binary cross-entropy loss
calculated with the ﬁnal hidden state of the ﬁnal
decoder token.
Inference and Post-processing At test time, we
conduct greedy decoding to obtain the target se-
quence, then we split the decoded sequence with
7The SentenceTransformer library ( https://www.
sbert.net/docs/quickstart.html) supports calcu-
lations in batch.
respect to [seq_arg]. Since it is also required to ob-
tain the offsets of the argument in the input context,
we automatically match the candidate argument’s
span with the input context. Then, if there’s no
matched span, we discard the candidate argument;
if there are multiple matches, we select the one clos-
est to the trigger word. For example, if the input
context is “One of those difﬁcult judges [John M.]
is nominated (Type: nomination) by Adam to be
chief justice in 2000.. [John M.] started ofﬁce on ...”
and there are two appearances of the candidate ar-
gument (in brackets) for the role PERSON , then we
use the ﬁrst candidate’s offsets. Different from our
methods, the template-based generation method
generates a sequence similar to the one in Section 2
– causing the model to (1) not fully exploit the se-
mantic relations of roles across event types; (2)
require more complicated post-processing includ-
ing an additional step to obtain arguments from the
generated template.
3.3 Few-shot Setting and Sampling Strategy
Algorithm 1: Our Strategy for Obtaining Sfew
Input :|S|Unlabeled Examples, Sample Size N
1 k ←# event types (based on ontology);
2 Sfew ←[ ];
// obtain embeddings for all
unlabeled instances
3 for i ←1 to |S|do
4 repi ←[enc(contexti), enc(trigger_texti)];
5 add repi to all_reps;
6 end
7 clusters = k_means(all_reps);
// add instances to samples
8 for i ←1 to k do
9 #instance = length(clusters[i])
|S| ∗N;
10 instances =
sample(clusters[i], #instances);
11 add instances to Sfew ;
12 end
In the few-shot setting, we assume that we have
a budget to obtain annotations for a limited number
of examples’ arguments (5%-20% of all examples)
for training. We denote the set of few training
examples as Sfew . We study (1) how different
sampling strategies affect the Sfew ’s distributions
and models’ performance; (2) how to select the
best set of examples (in zero or one round 8) and
have them annotated for training, to achieve better
performance at test time.
We propose a sampling method calledJointEnc.
It uses k-means clustering upon the embeddings
8One-round active learning setting (Wang et al., 2021).



Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 18):

Preprint.
Supported, then we sample it as the continuation. If there is more than one passage satisfying this
criterion, we use the one with the highest retrieval score. If there are only ISREL =Irrelevant or
ISSUP =No Support passages, we randomly sample one passage.
Algorithm 3 Mgen Data creation
1: Input Input-output data D = X, Y
2: for (x, y) ∈ {X, Y} do
3: Given (x, y) C predicts Retrieve
4: if Retrieve is predicted then
5: Retrieve relevant passages D using R given (x, y) ▷ Retrieve passages
6: for d ∈ D do
7: C predicts ISREL for each d ▷ Predict relevance of passages
8: C predicts ISSUP for each (y, d) ▷ Predict supports of outputs
9: C predicts ISUSE for each d ▷ Predict overall utility (t = T only)
10: Sample d
11: else if Retrieve is not predicted then
12: C predicts ISUSE given x, y
Add augmented (x, y, d, r) to Dgen
Training examples. Table 4 show several training examples used forM training.
A.3 S ELF -RAG INFERENCE
Details of beam-search score calculations. We first compute scores for each critique type by
taking the normalized probabilities of desirable tokens. For ISREL , we compute the score as follows:
s( ISREL ) = p( ISREL = RELEVANT )
p( ISREL = RELEVANT ) +p( ISREL = IRRELEVANT ).
For ISSUP , we compute the score as follows:
s( ISREL ) =p( ISSUP = FULLY)
S + 0.5 × p( ISSUP = PARTIALLY )
S ,
where S = P
t∈{FULLY,PARTIALLY ,NO} p( ISSUP = t). For ISUSE where we have a five-scale score, we
compute the weighted sum of the scores. We assigns weighted scores of w = {−1, −0.5, 0, 0.5, 1}
to the tokens ISUSE ={1, 2, 3, 4, 5}, and compute the final scores as follows:
s( ISUSE ) =
5X
i
wi
p( ISUSE = i)
S ,
where S = P
t∈{1,2,3,4,5} p( ISUSE = t).
Details of adaptive retrieval. For retrieval based on soft constraints, we trigger retrieval if the
following condition is satisfied:
p( Retrieve = YES)
p( Retrieve = YES) +p(p( Retrieve = NO) > δ.
B E XPERIMENTAL DETAILS
B.1 M ORE DETAILS OF TRAINING
More details of training and computations. We use 4 Nvidia A100 with 80GB memory to train
our models. All models are trained for 3 epochs with a batch size of 128, a peak learning rate of 2e-5
with 3% warmup steps, and linear decay afterward. We set the maximum token length to be 2,048
for the 7B model, and 1,524 for the 13B model due to the memory constraint. We use Deepspeed
stage 3 (Rajbhandari et al., 2020) to conduct multi-GPU distributed training, with training precision
Bfloat16 enabled. FlashAttention (Dao et al., 2022) is used to make the long-context training more
efficient. We run inference of our trained models using 1-2 Quadro RTX 6000 GPUs with 24GB
memory.
19



### Claim 114/179

#### Claim Text
The method to estimate ⟨X⟩, ⟨Y ⟩, ⟨XY ⟩, ⟨XPy⟩ from an experimentally obtained image of the light beam is covered in the following .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[25]_2310.11511.pdf (Page 18):

Preprint.
Supported, then we sample it as the continuation. If there is more than one passage satisfying this
criterion, we use the one with the highest retrieval score. If there are only ISREL =Irrelevant or
ISSUP =No Support passages, we randomly sample one passage.
Algorithm 3 Mgen Data creation
1: Input Input-output data D = X, Y
2: for (x, y) ∈ {X, Y} do
3: Given (x, y) C predicts Retrieve
4: if Retrieve is predicted then
5: Retrieve relevant passages D using R given (x, y) ▷ Retrieve passages
6: for d ∈ D do
7: C predicts ISREL for each d ▷ Predict relevance of passages
8: C predicts ISSUP for each (y, d) ▷ Predict supports of outputs
9: C predicts ISUSE for each d ▷ Predict overall utility (t = T only)
10: Sample d
11: else if Retrieve is not predicted then
12: C predicts ISUSE given x, y
Add augmented (x, y, d, r) to Dgen
Training examples. Table 4 show several training examples used forM training.
A.3 S ELF -RAG INFERENCE
Details of beam-search score calculations. We first compute scores for each critique type by
taking the normalized probabilities of desirable tokens. For ISREL , we compute the score as follows:
s( ISREL ) = p( ISREL = RELEVANT )
p( ISREL = RELEVANT ) +p( ISREL = IRRELEVANT ).
For ISSUP , we compute the score as follows:
s( ISREL ) =p( ISSUP = FULLY)
S + 0.5 × p( ISSUP = PARTIALLY )
S ,
where S = P
t∈{FULLY,PARTIALLY ,NO} p( ISSUP = t). For ISUSE where we have a five-scale score, we
compute the weighted sum of the scores. We assigns weighted scores of w = {−1, −0.5, 0, 0.5, 1}
to the tokens ISUSE ={1, 2, 3, 4, 5}, and compute the final scores as follows:
s( ISUSE ) =
5X
i
wi
p( ISUSE = i)
S ,
where S = P
t∈{1,2,3,4,5} p( ISUSE = t).
Details of adaptive retrieval. For retrieval based on soft constraints, we trigger retrieval if the
following condition is satisfied:
p( Retrieve = YES)
p( Retrieve = YES) +p(p( Retrieve = NO) > δ.
B E XPERIMENTAL DETAILS
B.1 M ORE DETAILS OF TRAINING
More details of training and computations. We use 4 Nvidia A100 with 80GB memory to train
our models. All models are trained for 3 epochs with a batch size of 128, a peak learning rate of 2e-5
with 3% warmup steps, and linear decay afterward. We set the maximum token length to be 2,048
for the 7B model, and 1,524 for the 13B model due to the memory constraint. We use Deepspeed
stage 3 (Rajbhandari et al., 2020) to conduct multi-GPU distributed training, with training precision
Bfloat16 enabled. FlashAttention (Dao et al., 2022) is used to make the long-context training more
efficient. We run inference of our trained models using 1-2 Quadro RTX 6000 GPUs with 24GB
memory.
19



Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 11):

Language Models as Semantic Indexers
Table 7.Dataset Statistics
Dataset # Items # Users # Rec history (train/dev/test) # Search query (train/dev/test) # Search labels (train/dev/test)
Amazon-Beauty 12,101 22,363 111,815 / 22,363 / 22,363 1,049 / 150 / 338 1,907 / 268 / 582
Amazon-Sports 18,357 35,598 177,990 / 35,598 / 35,598 1,299 / 186 / 443 2,209 / 311 / 764
Amazon-Toys 11,924 19,412 97,060 / 19,412 / 19,412 1,010 / 145 / 351 1,653 / 250 / 594
Table 8.Dataset Statistics
Dataset # Documents # Query (train/test) # Search labels (train/test)
NQ320k 109,739 307,373 / 7,830 307,373 / 7,830
MACRO 1M 1,000,000 502,939 / 6,980 532,751 / 7437
TREC-DL 1M 1,000,000 502,939 / 93 532,751 / 1,069
Algorithm 1 Self-supervised ID Learning Procedure of
LMI NDEXER
1: Input: The document corpus tdu.
2: Output: The semantic IDs tcduof the documents tdu.
A semantic indexer SemIndexerp¨qwhich contains a
semantic encoder SemEncθp¨qand codebooks tEtut.
A reconstruction model Reconϕp¨q.
3: Begin
4: // initialize semantic encoder
5: SemEncθp¨qÐ T5-base;
6: // reconstruction warm up
7: minϕ L0
recon “´ ř
d
ř
wPdzd0
h
log Preconpw|d0
h q;
8: for t “1, . . . , Tdo
9: // semantic encoder & codebook warm up
10: ht ÐSemEncθpd, cdq;
11: zw ÐReconϕpq “tct
d, ht
du, k“dt
hq;
12: minϕ,ϕc Lt “Lt
recon `Lt
contrastive `Lt
commitment;
13: ht
d ÐSemEncθpd, ct
dq;
14: Et ÐKMeanspht
dq;
15: // whole framework training
16: zw ÐReconϕpq “tct
d, ct
du, k“dh, v“dhq;
17: minϕ,ϕc Lt “Lt
recon `Lt
contrastive `Lt
commitment;
18: ct
d Ðargmaxjpspct
d “j|cd, dq;
19: end for
20: Return tcdu, SemIndexerp¨q;
21: End
rate in {1e-3, 2e-3, 5e-3}. The training epochs are set to be
30, 10, and 5 for Amazon datasets, NQ, and MS MACRO
respectively. The hyper-parameter configuration for self-
supervised semantic indexer training can be found in Table
9.
In the downstream recommendation task, for generative
recommendation methods with semantic IDs (rq-V AE in-
dexer, hierarchical clustering indexer, and LMI NDEXER ),
we concatenate the textual information (title & description)
of the user’s previously interacted items, serve it as the input
text into the generative language model and ask the model
to generate the ID for next item. The baselines are using
the same T5-base checkpoint. We train all the compared
generative recommendation methods for 10,000 steps with
the learning rate searched in {1e-2, 1e-3, 1e-4}. The batch
size is set to be 32, the maximum input text length is set
to be 1024 and all experiments are run on an 8 A100 40G
machine. The number of beams for beam search is set to
20. The hyper-parameter configuration for generative rec-
ommendation training can be found in Table 10.
In the downstream product search task, for generative re-
trieval methods with semantic IDs (rq-V AE indexer, hierar-
chical clustering indexer, and LMI NDEXER ), we serve the
query as the input text into the generative language model
and ask the model to generate the ID for the relevant items.
All baselines initially load the same T5-base checkpoint.
We train all the compared generative retrieval methods for
10,000 steps with the learning rate searched in {1e-2, 1e-
3, 1e-4}. The batch size is set to 32, the maximum input
text length is set to be 1024 and all experiments are run on
an 8 A100 40G machine. The number of beams for beam
search is set to 20. The hyper-parameter configuration for
generative product search training can be found in Table 11.
In the downstream document retrieval task, for generative
retrieval methods with semantic IDs (rq-V AE indexer, hier-
archical clustering indexer, and LMI NDEXER ), we serve the
query as the input text into the semantic indexer and ask the
model to generate the ID for the relevant documents. Fol-
lowing (Wang et al., 2022), we use docT5query (Nogueira
12



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 3):

Target Distribution
 Frozen LLM / Trainable LM
NLL Loss
KL Loss
Y N 
Y 1 
... 
Y 
X 
 Y 
X 
candidates 
source 
target training 
memory 
... ... 
... ... 
(a) Retrieval-augmented Generator (b) Memory Selector
Retrieval
Predicted Distribution
M 
Primal
Dual
Figure 2: Overall framework. There are two components in Selfmem, a retrieval-augmented genera-
tor (a) and a memory selector (b). For the primal problem, (a) takes source and memory as input to
generate candidates for (b). For the dual problem, (b) takes as input source and generated candidates
to select memory for (a).
retrieved memory) has already learned to discriminate between different memories in both oracle and
random scenarios, without updating the model weights.
To evaluate the second conjecture, we first define the token sets of the reference, retrieved memory,
and beam memory as R, M, and B, respectively. The overlap token set, denoted by O, is defined
as the tokens that overlap with the references in the beam memory but not in the retrieved memory,
which is represented as R ∩ B − R ∩ M. O is considered as the additional information provided
by the beam memory. Inspired by the confidence analysis of NMT model [58], we compute the set
confidence score, ψ (·), as follows:
ψ (·) = 1
| · |
X
y i∈·
p (y i |x, y <i ) (1)
where p (y i |x, y <i ) is defined by the generation model. ψ (·) measures the confidence with which the
generation model generates the tokens. The value of ψ (R) is 0.58, while that of O is 0.76, indicating
that the generator is relatively confident in generating tokens in O, and therefore does not need to
resort to external memory [38]. Beam search ranks generated candidates based on p (y |x ), where the
selected memory falls within the confidence region of the generator and consequently provides no
information gain. This observation motivates us to select memory according to metrics other than
p (y |x ) in the memory selector (§3.3).
3.2 Retrieval-augmented Generator
Given a text pair (x, y ), where x = {x1, ..., x|x |} is the source, y = {y1, ..., y|y |} is the target. They
could be (document, summary) in summarization, (context, response) in dialogue generation or
(source, target) in machine translation. The retrieval-augmented generation would first use x to
retrieve memory m from datastore D. Then the generator G ξ (x, m ), parameterized by ξ , would take
both x and m as input to generate the target sentence y . In this paper, following standard practice,
we choose the training set as D = {(x i , y i )}|D|
i =1. For LLM as G ξ , we use the standard in-context
learning format to give (x, y ) as demonstration example. For tunable generator G ξ , we only keep the
target side of top-1 retrieval results as memory and we consider two commonly used architectures:
Joint-Encoder [29, 87, 41] and Dual-Encoder [92, 8, 17].
Joint-Encoder This architecture is the standard encoder-decoder-based model [3, 84]. The input is
the concatenation of x and m . The encoder would first map the input into the hidden states H :
H = Encoder(x [SEP] m ) (2)
4



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 5):

Conference’17, July 2017, Washington, DC, USA Zhang and Xiao, et al.
Table 1: Impact on knowledge enhancement. MMLU and PopQA are measured by precision and exact match (EM), respectively.
“∗” and “ †” indicates the SOTA general embedding model and the task-specific method for the corresponding scenario.
MMLU PopQA
Method STEM Social Human Other All Avg. PopQA
None 0.3468 0.5328 0.5094 0.4967 0.4599 0.2061
BM25 0.3760 0.5378 0.5051 0.5088 0.4721 0.3491
Instructor [76] 0.3702 0.5406 0.5111 0.5082 0.4721 0.3533
Contriever [30] 0.3677 0.5383 0.5080 0.5013 0.4684 0.3276
RetroMAE-BEIR [46] 0.3857 0.5456 0.5221 0.5276 0.4853 0.4364
BGE∗[89] 0.3852 0.5564 0.5194 0.5389 0.4896 0.4491
AAR†[96] 0.3802 0.5501 0.5125 0.5288 0.4826 0.4792
API-Retriever [62] 0.3535 0.5335 0.4999 0.5068 0.4625 0.2488
LLM-R [83] 0.3629 0.5277 0.5018 0.4984 0.4625 0.2506
LLM-Embedder 0.3848 0.5568 0.5255 0.5360 0.4903 0.5052
notably outperforms a series of general retrievers, including the
state-of-the-art method BGE. On the other hand, it also goes be-
yond the task-specific method, i.e. AAR for knowledge enhance-
ment, LLM-R for in-context learning, API-Retriever for tool learn-
ing, Conv-ANCE for conversational search. Such an observation
indicates that LLM-Embedder is able to provide a strong and unified
foundation to support different retrieval augmentation needs of LLMs .
Finally, we can also observe that the task-specific retrievers opti-
mized for one scenario could result in limited performances in other
scenarios, indicating that the training impacts between different
retrieval tasks are not always transferable. To better illustrate this
point, we visualize the retrieval augmentation’s impact (improve-
ments over None) from five representative methods in Figure 3: BGE,
AAR, LLM-R, API-Retriever (API-R), and LLM-Embedder (ours). The
first method is the general embedding model, while the second to
fourth are task-specific methods. We can observe that although
task-specific training can deliver a competitive performance for its
corresponding scenario, e.g., AAR for knowledge enhancement and
LLM-R for in-context learning, their impacts are severely weak-
ened when applied for other usages. In contrast, LLM-Embedder
demonstrates a steady and competitive performance across dif-
ferent scenarios. Although challenging, the seemingly irrelevant or
even adverse retrieval patterns can still be reconciled by one unified
embedding model on top of the properly optimized training recipe .
3.2.2 Individualized Analysis. Further analysis is made for the re-
trieval augmentation’s impact to each individual scenario.
•Knowledge Enhancement. The experiment results on knowl-
edge enhancement are shown in Table 1, where we can make the
following observations. 1) Benefit of external knowledge. LLMs
benefit from external knowledge when answering questions in
MMLU and PopQA, as clear empirical advantages are achieved by
the retrieval augmentation methods compared with the plain LLM,
i.e. None. 2) Importance of retrieval accuracy. The impact of knowl-
edge enhancement becomes more pronounced when knowledge
retrieval is more accurate. We observe consistent improvements
as we transition from using the BM25 retriever to more advanced
embedding models. 3) Distinction between datasets. The impact
of retrieval augmentation is more noticeable in the PopQA dataset
compared to MMLU. This difference is likely due to the nature of
0.50520.626813.48320.86450.50530.24880.594514.78340.80170.11370.25060.626214.47460.13210.02340.47920.593814.69990.42000.28770.44910.597414.29430.57610.3856QAICLLongToolC-Search
OursAPI-RLLM-RAARBGE
Figure 3: Retrieval augmentation’s impact from different
retrievers. The warmer color indicates a better performance.
the datasets. PopQA tends to be more knowledge-intensive, with a
focus on questions about long-tail entities. In contrast, many ques-
tions in MMLU rely more on common sense and logical reasoning
rather than extensive world knowledge.
•In-Context Learning. The experiment results on in-context
learning are shown in Table 2, where we can draw the following
observations. 1) Benefits of retrieved examples. When comparing
plain LLM (None) with other retrieval-augmented methods, we can
consistently observe the improved performances in most cases. This
finding underscores the enhancement of LLM’s ability to follow
instructions when retrieved examples are presented. 2) Limitation
of BM25. It’s noteworthy that BM25’s performance is comparatively
weaker than its performance in other scenarios. This discrepancy
can be attributed to the specific nature of in-context learning, where
examples need to emphasize semantic similarity rather than lexical
similarity. 3) Limited transferability. While the task-specific method,
LLM-R, exhibits a competitive performance for in-context learning,
its utility becomes severely limited when applied to other scenarios,
such as knowledge retrieval and tool using. This suggests that
example retrieval calls for a unique pattern tailored to this very
task, making it challenging to transfer to other scenarios.
•Long-Context Modeling . The experiment results on long-
context modeling are shown in Table 3. While retrieval augmenta-
tion consistently demonstrates improvements compared to having
no augmentation (None), it may not be entirely convincing due to
the utilization of more context. To address this issue, we introduce



### Claim 115/179

#### Claim Text
In fact, the error shown in figure 5.2( b) remains practically identically when the correction proposed in is applied, and the same is true for the interface flux (not shown).

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 28):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question Two spheres of net charge +5e and -6e briefly come into contact. Afterward,
which of the following is a possible combination of net charges for the two
spheres?
0) +3e and -4e
1) +1.5e and -2.5e
2) Both of the above
3) None of the above
Principles Law of conservation of charge: the total charge in a closed system remains
constant.
Coulomb’s Law: the mutual repulsion of like charges demands that the charges
be as far apart as possible.
Answer Using the Principles of Conservation of charge, we can solve the problem as
following:
Step 1: Apply Conservation of charge to find out the possible combination of
net charges for the two spheres.
The total charge of the two spheres before they come into contact is +5e - 6e =
-1e. After they come into contact, the total charge of the two spheres must still
be -1e.
Step 2: Find out the possible combination of net charges for the two spheres.
The possible combination of net charges for the two spheres are:
(1) +3e and -4e
(2) +1.5e and -2.5e
Therefore, the correct answer is 2) Both of the above.
Table 25: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Principle Error. In this case, Coulomb’s Law is not needed for solving this problem. Instead, the
Charge Quantization Principle is missing from the retrieved principles, and is in fact needed to rule
out option (2) of fractional charges.
29



Source: data\tc16_2312.10997v5\referenced_papers\[93]_2309.11495.pdf (Page 18):

Figure 8: Examples where questions asking for a fact are answered correctly, but verifying via a
yes/no question is incorrect (the model tends to agree with the way the question is stated, even if it
was stated incorrectly).
19



Source: data\tc16_2312.10997v5\referenced_papers\[93]_2309.11495.pdf (Page 15):

8.5 F ACTOR +REVISE : I DENTIFY WHICH FACTS ARE CONSISTENT
Context: <Original Fact>.
From another source,
<output of execute verification step: Q + A>
Response: CONSISTENT. <Consistent fact>
Context: <Original Fact>.
From another source,
<output of execute verification step: Q + A>
Response: INCONSISTENT.
Context: <Original Fact>.
From another source,
<output of execute verification step: Q + A>
Response: PARTIALLY CONSISTENT. <Consistent part>
Table 9: In the CoVe (Factor + Revise) variant, as part of step (3) after subsection 8.3, the model is
made to explicitly identify which facts are consistent between the two sources. The consistent facts
can then be spliced together.
16



Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 30):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question Physics students are checking the constant acceleration equations of kinematics
by measuring the velocity of a tennis ball that is dropped and falls 6 meters
and then passes through a photogate. The predicted velocity is 20% above the
velocity measured by the photogate. Which of the following best describes the
cause of the large percent difference?
0) The ball changes its shape while falling.
1) The acceleration of gravity varies as the ball is falling.
2) Air resistance increases the acceleration of the ball.
3) The acceleration of the balls varies with the velocity.
Principles Vertical motion: y = 0.5 * g * t2, where y is the vertical displacement, g is the
acceleration due to gravity, t is the time.
Free fall: is a special case of motion with constant acceleration, in which the
acceleration is due to gravity.
Acceleration to Velocity Formula: v = u + a * t, where v is the final speed, u is
the initial velocity, a is the acceleration, and t is the time.
Answer Using the Principles of Vertical motion and Horizontal motion, we can solve
the problem as following:
Step 1: Apply the Vertical motion equation to find the time it takes for the ball
to fall 6 meters.
y = 0.5 * g * t2
6 m = 0.5 * 9.8 m/s2 * t2
t = 1.12 s
Step 2: Apply the Horizontal motion equation to find the velocity of the ball
when it passes through the photogate.
x = v * t
v = x / t
v = 6 m / 1.12 s
v = 5.36 m/s
Step 3: Compare the predicted velocity with the velocity measured by the
photogate. The predicted velocity is 20% above the velocity measured by the
photogate, so the predicted velocity is 1.2 * 5.36 m/s = 6.43 m/s.
Step 4: Find the cause of the large percent difference. The predicted velocity is
6.43 m/s, but the velocity measured by the photogate is 5.36 m/s. The difference
is 1.07 m/s. This difference is caused by air resistance. Air resistance is a force
that opposes the motion of an object through the air. It is caused by the friction
between the object and the air molecules. The greater the surface area of the
object, the greater the air resistance. The tennis ball has a large surface area, so
it experiences a lot of air resistance. This air resistance slows down the tennis
ball and causes the difference between the predicted velocity and the velocity
measured by the photogate.
Therefore, the correct answer is 0) The ball changes its shape while falling.
Table 27: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Reasoning Error. In this case, the model was able to correctly identify air resistance is the cause of
the velocity difference, but made the error at the final reasoning step of deriving the right answer.
31



### Claim 116/179

#### Claim Text
Fig. 11 The number of events that (a) mono-vacancies and vacancy clusters and (b) mono-interstitials and SIA clusters interact with vacancy clusters under different irradiation conditions at 1.0 dpa, respectively. S.J.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[54]_2401.14887.pdf (Page 6):

The Power of Noise: Redefining Retrieval for RAG Systems SIGIR ’24, July 14–18, 2024, Washington, DC, USA
Table 3: Accuracy of Llama2-7b in configurations involving random Wikipedia documents and retrieved documents [I, , , Q].
Rows denote the number of random documents added, and columns show the quantity of retrieved documents . The left
section reports results using Contriever, and the right section using BM25. Scenarios where the prompt exceeded the model’s
input limit, leading to potential data truncation, are not included ( - ). Each value not marked with an asterisk * represents a
statistically significant change from the base case of retrieved documents only [I, , Q] (first row), as determined by a Wilcoxon
test (p-value < 0.01).
Contriever BM25
#
# 1 2 3 4 5 8 10 1 2 3 4 5 8 10
0 0.1620 0.1866 0.1876 0.1866 0.1921 0.2198 0.2108 0.2008 0.2208 0.2084 0.2028 0.2243 0.2492 0.2447
1 0.1308 0.1616 0.1717 0.1893* 0.1987* 0.2153* 0.2146* 0.1568 0.1963 0.1921 0.2115 0.2295* 0.2475* 0.2506*
2 0.1315 0.1644 0.1859* 0.2008 0.2174 0.2156* 0.2368 0.1644 0.1973 0.2080* 0.2281 0.2558 0.2495* 0.2596
3 0.1301 0.1727 0.2008 0.2316 0.2201 0.2198 0.2409 0.1568 0.2063 0.2160 0.2520 0.2579 0.2644 0.2707
5 0.1464 0.2056 0.2233 0.2240 0.2150 0.2451 0.2482 0.1772 0.2402 0.2437 0.2520 0.2554 0.2804 0.2866
8 0.1734 0.2066 0.2336 0.2375 0.2454 0.2416 0.2364 0.1994 0.2451 0.2579 0.2769 0.2817 0.2859 0.2777
10 0.1796 0.2174 0.2450 0.2502 0.2499 0.2420 - 0.2108 0.2589 0.2734 0.2835 0.2935 0.2853 -
15 0.2018 0.2354 0.2551 0.2530 - - - 0.2243 0.2686 0.2790 0.2928 - - -
16 0.2032 0.2471 0.2558 - - - - 0.2323 0.2662 0.2838 - - - -
17 0.2039 0.2426 - - - - - 0.2326 0.2693 - - - - -
18 0.2073 - - - - - - 0.2309 - - - - - -
Figure 3: This heatmap depicts the attention distribution
across the context documents from the example shown in
Figure 2, relative to the answer generated by Llama2-7b in
a prompt structured as [I, /u♀k, ⋆, Q]. Cell (i, j) denotes the
mean attention that tokens in the generated answer allocate
to the tokens of the i-th document within the j-th attention
layer. This mean attention for each document is calculated
by averaging the attention scores across all its constituent
tokens.
in the presence of noise, as can be seen in Table 2. Instead, we ob-
serve an improvement in performance under the best-performing
setting (near [I, , ⋆, Q]), with an improvement of 0.08 (+36%) in
LLM Input - Random and Gold ⋆
Task instruction...
Documents:
Document [140](Title: Richard Yates (novelist)) For much
of his life, Yates’s work met almost universal critical ac-
claim, yet not one of his books sold over 12,000 copies in...
Document [242] (Title: Android version history) Code
name Version number Initial release date API level Security
patches (No codename ) 1.0 September 23...
Document [3](Title: Millennium Falcon) Han Solo won
the Millennium Falcon from Lando Calrissian in the card
game sabacc...
Question: who owned the millennium falcon be-
fore han solo
Answer: Lando Calrissian
Figure 4: Example LLM input with a correct output, high-
lighted in green. The context of the prompt is composed of
random documents and the gold near the query. The task
instruction is as in Figure 1.
the case of MPT. Furthermore, we observe that different models
exhibit distinct behaviors. Both Llama2 and Phi-2 showed improve-
ments in this setting when the noise is introduced furthest from
the query. However, when the noise is positioned in the far[I, ⋆,
, Q] and mid [I, , ⋆, , Q] settings, these models exhibit a
decline in performance. Notably, this performance degradation is
much less accentuated when compared to the earlier setting with
distracting documents. This suggests that while Llama2 and Phi-2
can effectively handle noise far from the query, their ability to sift



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 15):

0 50 100 150
1.6
1.8
2
2.2
2.4
2.6
2.8
3 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(a) ELI5 - p = 10%
0 50 100 150
1.4
1.45
1.5
1.55
1.6
1.65 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (b) ELI5 - p = 30%
0 50 100 150
1.04
1.06
1.08
1.1
1.12
1.14 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (c) ELI5 - p = 50%
0 10 20 30 40 50
1.6
1.8
2
2.2
2.4
2.6
2.8
3
3.2
3.4 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(d) MS MARCO - p = 10%
0 10 20 30 40 50
1.15
1.2
1.25
1.3
1.35
1.4 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
0.91 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (f) MS MARCO - p = 50%
0 20 40 60 80 100
1.5
2
2.5
3
3.5
1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(g) NQ - p = 10%
0 20 40 60 80 100
1.5
1.6
1.7
1.8
1.9
2 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
Loa ding [ M a thJ a x ] / ex tensions/ M a thM enu .j s (h) NQ - p = 30%
0 20 40 60 80 100
1.22
1.24
1.26
1.28
1.3
1.32
1.34
1.36 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (i) NQ - p = 50%
Figure 7: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-12), for FiD-Base models.
Each row represents a different dataset, and every column represents a different filtering percentage (10,30,50).



Source: data\tc16_2312.10997v5\referenced_papers\[47]_2305.17331.pdf (Page 13):

1 2 3 4 5 6 7 8 9 10
Number of documents
36
38
40
42
44MMLU Accuracy
Standalone LM
Lt=Flan-T5Base
(a) Flan-T5Base w/ AARANCE.
1 2 3 4 5 6 7 8 9 10
Number of documents
44
46
48
50
52MMLU Accuracy
Standalone LM
Lt=Flan-T5Larg e (b) Flan-T5Large w/ AARANCE.
1 2 3 4 5 6 7 8 9 10
Number of documents
50
52
54
56
58MMLU Accuracy
Standalone LM
Lt=Flan-T5X L (c) Flan-T5XL w/ AARANCE.
Figure 9: Relationship between LM’s performance and the number of augmentation documents.
Category Number
Ts
MSMARCO QA Open Domain QA 148122
KILT-FEVER Fact Checking 10444
KILT-WNED Entity Linking 3396
KILT-T-REx Slot Filling 5000
KILT-TriviaQA Open Domain QA 5359
KILT-Wizard of Wikipedia Dialogue 3054
Tt
MMLU Multi-task Language Understanding 1531
PopQA Open Domain QA 14267
Table 6: Configurations of our source tasks and target tasks.
Methods MMLU
All Hum. Soc. Sci. STEM Other
Flan-T5Base 36.1 40.4 39.8 27.0 40.6
Flan-T5Base Fine-tuning 36.1 38.9 41.2 27.9 39.9
Flan-T5Base w/ Contriever 43.7 44.4 45.0 36.4 51.1
Flan-T5Base w/ ANCE 43.0 44.2 44.3 34.5 51.9
Flan-T5Base w/ AARContriever (Ours) 44.4 44.7 47.7 35.8 52.2
Flan-T5Base w/ AARANCE (Ours) 44.8 42.2 46.4 39.0 53.2
Flan-T5Large 45.1 47.7 53.5 34.4 49.2
Flan-T5Large Fine-tuning 45.3 47.6 54.1 35.2 48.7
Flan-T5Large w/ Contriever 50.7 50.5 56.4 38.9 61.1
Flan-T5Large w/ ANCE 49.2 49.3 56.7 38.1 57.2
Flan-T5Large w/ AARContriever (Ours) 51.8 50.8 59.7 39.4 61.8
Flan-T5Large w/ AARANCE (Ours) 50.4 48.0 58.1 39.3 60.2
Flan-T5XL 51.2 55.5 57.4 38.1 58.7
Flan-T5XL w/ Contriever 56.4 57.3 66.1 43.9 63.2
Flan-T5XL w/ ANCE 55.3 55.9 64.0 41.5 64.9
Flan-T5XL w/ AARContriever (Ours) 56.7 57.7 65.4 43.6 65.1
Flan-T5XL w/ AARANCE (Ours) 56.2 59.4 64.8 41.5 64.9
InstructGPT 60.2 65.7 68.0 46.1 66.5
InstructGPT w/ Contriever 60.5 62.0 71.8 44.3 70.1
InstructGPT w/ ANCE 61.6 62.4 73.4 47.6 68.6
InstructGPT w/ AARContriever (Ours) 61.5 64.5 73.1 45.0 69.9
InstructGPT w/ AARANCE (Ours) 62.2 62.0 72.0 49.2 70.7
Table 7: Fine-tuning results on MMLU. We use the official auxiliary training data of MMLU to fine-tune the LM.



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 17):

0 50 100 150
1.4
1.6
1.8
2
2.2
2.4
2.6 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(a) ELI5 - p = 10%
0 50 100 150
1.35
1.4
1.45
1.5
1.55
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (b) ELI5 - p = 30%
0 50 100 150
1.04
1.06
1.08
1.1
1.12
1.14 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (c) ELI5 - p = 50%
0 10 20 30 40 50
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
3.2 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(d) MS MARCO - p = 10%
0 10 20 30 40 50
1.1
1.15
1.2
1.25
1.3
1.35 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.84
0.86
0.88
0.9
0.92 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (f) MS MARCO - p = 50%
0 20 40 60 80 100 120
1.5
2
2.5
3
3.5
4
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(g) NQ - p = 10%
0 20 40 60 80 100 120
1.5
1.6
1.7
1.8
1.9
2
2.1 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
Loa ding [ M a thJ a x ] / ex tensions/ M a thM enu .j s (h) NQ - p = 30%
0 20 40 60 80 100 120
1.25
1.3
1.35
1.4
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (i) NQ - p = 50%
Figure 9: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-24), for FiD-Large models.
Each row represents a different dataset, and every column represents a different filtering percentage (10,30,50).



Source: data\tc16_2312.10997v5\referenced_papers\[56]_2312.11361.pdf (Page 16):

You are an evaluator checking whether the question contains the answer within the provided
contexts or not. I will give you a question and several contexts containing information about the
question. Read the contexts carefully. If any of the contexts answers the question, respond as
either “Yes, answer is present” or “I don’t know”:
QUESTION: {query}
CONTEXTS:
[1] {Passage title}: {Passage text}
[2] {Passage title}: {Passage text}
...
[10] {Passage title}: {Passage text}
Please remember to read all the contexts carefully. If any of the contexts answers the
question: {query}, respond as either “Yes, answer is present” or “I don’t know”.
OUTPUT:
Read the query and the contexts carefully and provide a step-by-step explanation for your answer.
If any of the contexts answers the question, respond as either “Yes, answer is present” or “I
don’t know”. You must strictly follow the output format with ## Reasoning: ... ## Answer: “Yes,
answer is present” OR “I don’t know”.
QUESTION: {query}
CONTEXTS:
[1] {Passage title}: {Passage text}
[2] {Passage title}: {Passage text}
...
[10] {Passage title}: {Passage text}
OUTPUT:
Figure 10: All prompt ablations used in our experiments for LLM hallucination evaluation for all 18 languages
in NoMIRACL on both the relevant and non-relevant subsets. Role prompt appends the role of the LLM at the
beginning of the prompt (highlighted in blue). Repeat prompt highlights the task by repeating instructions at the
end of the prompt (highlighted in red). Explanation prompt asks the model to provide a reasoning path and finally
answer the question (highlighted in violet).
ar bn de en es fa fr fi hi id ja ko ru sw te th yo zh Avg.
Hallucination Rates (in %) on NoMIRACL test split (non-relevant subset)
Llama-3 (8B) 19.6 20.0 29.5 19.2 30.0 44.8 35.2 10.5 14.4 23.2 52.0 31.6 26.8 4.0 13.2 41.6 1.6 64.4 26.8Llama-3 (8B) (w/ SFT)83.0 12.5 36.9 44.0 46.8 65.6 36.9 70.0 61.5 53.1 41.7 9.7 85.4 42.2 0.0 82.7 12.0 20.0 44.7Mistral-7B (v0.3) 40.4 63.2 38.2 42.8 17.2 52.4 47.6 16.1 39.6 30.8 44.4 28.8 41.6 14.8 74.0 58.8 23.6 45.2 40.0Mistral-7B (v0.3) (w/ SFT)46.8 33.2 34.1 73.6 33.6 52.4 48.0 42.7 26.0 59.2 52.0 47.6 62.8 35.6 31.2 40.0 45.2 33.2 44.3
Error Rates (in %) on NoMIRACL test split (relevant subset)
Llama-3 (8B) 53.6 56.0 36.8 41.6 26.8 26.8 34.4 61.2 56.8 48.4 17.6 35.6 46.4 80.8 65.6 29.2 91.7 6.0 45.3Llama-3 (8B) (w/ SFT)6.0 70.8 32.4 31.2 24.0 14.6 21.2 8.8 14.9 40.6 24.0 77.2 8.3 32.5 87.5 4.2 75.5 50.4 34.7Mistral-7B (v0.3) 14.4 20.4 21.6 8.0 28.0 22.4 15.6 38.0 30.8 43.6 16.0 30.0 18.0 57.6 17.6 13.6 50.0 13.2 25.5Mistral-7B (v0.3) (w/ SFT)46.4 60.8 46.0 12.4 46.8 36.4 35.6 42.0 69.6 37.2 37.6 42.0 25.2 69.6 66.4 56.8 52.0 46.8 46.1
Table 7: Complete SFT results using the NoMIRACL development dataset for LLAMA-3 (8B) and Mistral-7B
(v0.3) LLMs across all languages in NoMIRACL. Lower the hallucination and error rates (%) is better.



### Claim 117/179

#### Claim Text
Characteristics of the used plasma source can be found in .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[53]_2312.07559.pdf (Page 19):

G.1 C ONTRADICTING INFORMATION
In the first example, we ask: “Are COVID-19 vaccines effective?”. The goal of this experiment is to
evaluate the inclusion of background information that contradicts gathered context. Thus, while the
provided context supports the efficacy of covid vaccines, we provide the model with the following
background information: “COVID-19 vaccines are known to be ineffective.”
With the contradicting background information, PaperQA responds that it cannot answer. However,
if the extra contradicting information is excluded, the model provides an answer:
Yes, COVID-19 vaccines are effective. The BNT162b2 and ChAdOx1 nCoV-19 vaccines have shown
effectiveness against the delta variant, with the second[...]
...
Therefore, while the vaccines are effective, their effectiveness can vary based on the
specific vaccine, the variant of the virus, and the time elapsed since vaccination.
When the background information (or parametric knowledge) contradicts context, PaperQA opts
to not answer, indicating its recognition of a contradiction with established scientific understand-
ing. This example shows the model’s capability to discern and respond appropriately to accurate
information, while avoiding potentially misleading or incorrect assertions.
G.2 A BSENCE OF KEY INFORMATION
As a second example, we ask PaperQA: “What temperature does water turn to plasma?” and provide
it with context that is relevant but insufficient to answer the question. Then, we provide the model
with the following background information:
“Water turns into plasma under extreme conditions of temperature and pressure. Plasma is the fourth
state of matter, beyond solid, liquid, and gas. It’s a super-heated state where atoms are ionized,
meaning electrons are stripped from atoms. This typically occurs at temperatures exceeding 20,000
Kelvin.”
In this example, the model answers:
Water transitions into the plasma state under extreme conditions of temperature and
pressure, typically at temperatures exceeding 20,000 Kelvin.
Excluding the background information results in PaperQA responding that it is unable to answer.
Thus, the role of parametric knowledge is crucial in shaping the model’s responses: when it con-
tradicts the context, the model may refrain from answering due to the conflict, whereas supportive
parametric knowledge enables the model to provide detailed and informed responses when context
is lacking.
20



Source: data\tc16_2312.10997v5\referenced_papers\[75]_2305.04757.pdf (Page 8):

Table 4: Examples of background documents generated by our PKGs to guide different tasks. Clues
to answer the input are highlighted in blue within the documents.
Input (FM2) The Natural is a book about Roy Hobbs a natural southpaw boxer who
goes on to win the heavyweight title from Boom Boom Mancini.
PKG Guiding This is a story about a baseball player, Roy Hobbs, who is shot by a
woman, Harriet Bird, and then recovers to become a great player.
Input (NQ-Table) Batman The Enemy Within episode 5 release date
PKG Guiding
Batman: The Enemy Within
Ep Title Directed by Release date
1 "The Enigma" Kent Mudle August 8, 2017
2 "The Pact" Matthew Leach October 3, 2017
3 "Fractured Mask" Sean Manning November 21, 2017
4 "What Ails You" Chris Rieser January 23, 2018
5 "Same Stitch" Kent Mudle March 27, 2018
Input (MedMC-QA) Calcium ions triggers muscle contraction by binding to: Options: (A)
Actin (B) Myosin (C) Troponin (D) Tropomyosin
PKG Guiding Troponin is a complex of three proteins that are attached to actin. Cal-
cium ions bind to troponin-C, which then causes the troponin-I and
troponin-T to shift and allow the tropomyosin to move off of the active
site of the myosin so that the myosin head can then bind to actin and
cause contraction.
Input (ScienceQA)
 Which animal’s neck is also adapted for hunting prey while keeping the
rest of its body still? Context: Saddle-billed storks live near wetlands
and lakes. They eat mostly ﬁsh. The ’s neck helps it grab ﬁsh while
keeping the rest of its body still. If the stork had to move its body, it
might scare the ﬁsh away. Figure: saddle-billed stork. A black and
white bird is standing in the grass. Options: (A) northern pintail (B)
black-headed heron
PKG Guiding Look at the picture of the saddle-billed stork. The saddle-billed stork
has a long neck. Its neck is adapted for hunting prey while keeping the
rest of its body still. Now look at each animal. Figure out which animal
has a similar adaptation. The black-headed heron has a long neck. Its
neck is adapted for hunting prey while keeping the rest of its body still.
The northern pintail has a short neck. Its neck is not adapted for hunting
prey while keeping the rest of its body still.
superior performance. For example, increasing the number of parameters from 1.3B to 2.7B leads to
performance improvements of 1.4% on FM2, 1.4% on NQ-Table, 0.5% on MedMC-QA, and 2.5%
on ScienceQA, which is consistent with the scaling law [Kaplan et al., 2020].
Examples of Generated Background Documents. Table 4 presents examples of background
documents generated by our PKGs to assist LLMs in different tasks. For the factual task, our PKG
can supply input-related factual information to support or refute the input, such as the example of
Roy Hobbs being a baseball player and not a boxer. For the tabular task, our PKG can offer an
input-related background table, like the episode table of Batman. For the medical task, our PKG can
provide relevant medical knowledge, such as the background of calcium ions. For the multimodal
task, our PKG can produce a document based on text information while taking into account the
image context in the input, for example, noting that the bird in the image has a long neck. Additional
examples can be found in Appendix D.
9



Source: data\tc16_2312.10997v5\referenced_papers\[75]_2305.04757.pdf (Page 15):

Table 7: All experiments results of Figure 3 for different sizes of LMs.
Methods FM2 NQ-Table MedMC-QA ScienceQA
PKG-Davinci 67.3 28.8 47.4 80.76
PKG-Curie 52.0 0.1 23.8 32.87
PKG-Babbage 45.4 0.1 20.0 35.77
PKG-Ada 38.0 0.0 20.6 29.76
Direct-Davinci 59.4 16.9 44.4 71.66
Direct-Curie 50.8 0.0 20.3 36.76
Direct-Babbage 39.5 0.0 20.6 41.08
Direct-Ada 42.6 0.0 20.3 26.79
methods. We can ﬁnd that the retrieval methods cannot provide relevant background documents to
answer the question.
E Errors
Table 13 presents a hallucination error of our PKGs.
16



Source: data\tc16_2312.10997v5\referenced_papers\[75]_2305.04757.pdf (Page 18):

Table 10: Examples of background documents generated by our baseline methods and PKGs for
MedMC-QA. Clues to answering the input are highlighted in blue within the documents. The
information that will disrupt LLMs is highlighted in red.
Input (MedMC-QA) Calcium ions triggers muscle contraction by binding to: Options: (A)
Actin (B) Myosin (C) Troponin (D) Tropomyosin (Correct Answer: D)
CoT Guiding First, we need to understand what a muscle contraction is. A muscle
contraction is when the muscle ﬁbers in the muscle shorten. This is
caused by the sliding of the actin and myosin ﬁlaments past each other.
Next, we need to understand what role calcium ions play in muscle
contraction. Calcium ions are needed for the muscle contraction to occur
because they are needed to trigger the release of the myosin head from
the actin ﬁlament. So, the answer is (C) Troponin.
GenRead Guiding Calcium ions play an important role in muscle contraction. When cal-
cium ions bind to troponin, they cause a conformational change in the
protein that allows myosin and actin to interact. This interaction between
myosin and actin is what causes muscle contraction.
PKG Guiding Troponin is a complex of three proteins that are attached to actin. Cal-
cium ions bind to troponin-C, which then causes the troponin-I and
troponin-T to shift and allow the tropomyosin to move off of the active
site of the myosin so that the myosin head can then bind to actin and
cause contraction.
Input (MedMC-QA) Which of the following statements is true regarding medical aboion?
Options: (A) Ultrasound should be done in all cases (B) If the patient
has an IUCD in-situ, it doesn’t need to be removed (C) Can only be done
up to 72 days (D) Only a person ceiﬁed under MTP act can perform
medical termination of pregnancy (Correct Answer: D)
CoT Guiding First, we need to understand what medical abortion is. Medical abortion
is the use of medication to end a pregnancy. Next, we need to consider
the options given. Option A is not necessarily true - ultrasound may not
be needed in all cases, but it can be helpful in determining how far along
the pregnancy is. Option B is also not necessarily true - in some cases,
the IUCD (intrauterine contraceptive device) may need to be removed
before the medical abortion can be performed. Option C is generally
true - medical abortions can usually be performed up to 72 days after the
last menstrual period. Option D is also generally true - only a certiﬁed
healthcare provider can perform a medical abortion.
GenRead Guiding Medical abortion is a safe and effective way to end a pregnancy. It can
be done up to 72 days after the ﬁrst day of the last menstrual period. A
medical abortion can be done with pills or with a shot.
PKG Guiding Medical aboion can be done up to 9 weeks and 6 days. Ultrasound is not
required for medical aboion. If the patient has an IUCD in-situ, it should
be removed. Only a person ceiﬁed under MTP act can perform medical
termination of pregnancy.
19



Source: data\tc16_2312.10997v5\referenced_papers\[75]_2305.04757.pdf (Page 14):

- Natural Questions Over Tables (NQ-Table) [Herzig et al., 2021] were mined from real Google search
queries and the answers are spans in Wikipedia tables identiﬁed by human annotators.
- Multi-Subject Multi-Choice Dataset for Medical domain (MedMC-QA) [Pal et al., 2022] contains a
set of real-world medical entrance exam questions and answers.
- Multimodal Reasoning for Science Question Answering (ScienceQA) [Lu et al., 2022] consists of
multimodal multiple-choice questions with a diverse set of science topics.
Table 5 shows the dataset splits and statistics.
Table 5: Datasets splits and statistics. For MedMC-QA, labels in the test are hidden, so the model
performance is evaluated on the validation set.
Datasets Domain Train Valid Test Test labels
FM2 [Eisenschlos et al., 2021] Factual 10,419 1,169 1,380 Public
NQ-Table [Herzig et al., 2021] Tabular 9,594 1,068 959 Public
MedMC-QA [Pal et al., 2022] Medical 160,869 4,183 6,150 Private
ScienceQA [Lu et al., 2022] Multimodal 12,726 4,241 4,241 Public
B Implementation Details
We use LLaMa-7B [Touvron et al., 2023] as our backbone models to implement the PKG modules.
We use AdamW as the optimizer, with 10% warmup steps. We use 8 V100 GPUs for training PKG
modules. The open-source code LLaMa-X3 is widely used in our experiments. We refer to more
individual implementation details in Table 6.
We implement other baseline methods based on the following repositories:
- BM25 + GPT3.5: https://github.com/castorini/pyserini
- REPLUG + GPT3.5: https://github.com/facebookresearch/DPR/tree/main
- CoT + GPT3.5: https://github.com/kojima-takeshi188/zero_shot_cot
- GenRead + GPT3.5: https://github.com/wyu97/GenRead
C All Experiment Results of Figure 3
In Figure 3, we compare our PKGs framework with the direct generation on various types of LMs.
We include all results in Table 7.
D Case Studies
Table 8 9 10 11 presents more examples of background documents generated by our baseline methods
(CoT and GenRead) and PKGs for different tasks. We can notice that our PKGs can provide clues
for LLMs to answer speciﬁc questions. Table 12 also compares our PKGs with the retrieval-based
3https://github.com/AetherCortex/Llama-X
Table 6: Hyperparameters settings of our PKG modules on different tasks.
Settings FM2 NQ-Table MedMC-QA ScienceQA
Peak learning rate 2e-5 2e-5 2e-5 2e-5
Total batch size 64 32 32 32
Total training epochs 3 10 3 5
Warmup Schedule cosine cosine cosine cosine
Warmup ratio 0.1 0.1 0.1 0.1
15



### Claim 118/179

#### Claim Text
Yet another approach is due to Tschisgale et al. (cf. also ), who define an interface layer (with finite thickness) to which the rigid-body assumption is restricted.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 32):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: A liquid flows at a constant flow rate through a pipe
with circular cross-sections of varying diameters. At one point in the
pipe, the diameter is 2 cm and the flow speed is 18 m/s. What is the flow
speed at another point in this pipe, where the diameter is 3 cm?
0) 4 m/s
1) 6 m/s
2) 8 m/s
3) 12 m/s
Original Answer [Incorrect]: The answer is 3) 12 m/s A step-by-step
solution is below: The flow rate is constant, so the product of the speed
and the cross-sectional area is constant. The cross-sectional area is
proportional to the square of the diameter, so the speed is inversely
proportional to the square of the diameter. The ratio of the speeds is the
inverse of the ratio of the squares of the diameters, so the speed at the
second point is 18m/s ∗ (2cm/3cm)2 = 12 m/s.
Stepback Question: What are the physics principles behind this
question?
Principles:
Conservation of Mass: the amount of mass flowing through the pipe at
two different points must be equal, or A1 * v1 = A2 * v2, where A1 is
the area of cross-section one, v1 is the velocity at point 1, A2 is the area
of cross-section two, v2 is the velocity at point 2.
Area of Circle: A = π ∗ r2, where r is the radius of the circle.
Final Answer: Using the Principles of Continuity equation, we can
solve the problem as following:
Step 1: Apply the Continuity equation to find out the flow speed at
another point in this pipe. The continuity equation states that the product
of the area and the velocity is constant at all points in a pipe. In this
problem, we are given the following information:
A1 = π ∗ (2cm)2 = 4πcm2
v1 = 18 m/s
A2 = π ∗ (3cm)2 = 9πcm2
We can use the continuity equation to find v2:
A1 * v1 = A2 * v2
4πcm2 ∗ 18m/s = 9πcm2 ∗ v2
v2 = 8 m/s
Therefore, the flow speed at another point in this pipe is 8 m/s.
The correct answer is 2) 8 m/s.
Table 29: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
33



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 30):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question Physics students are checking the constant acceleration equations of kinematics
by measuring the velocity of a tennis ball that is dropped and falls 6 meters
and then passes through a photogate. The predicted velocity is 20% above the
velocity measured by the photogate. Which of the following best describes the
cause of the large percent difference?
0) The ball changes its shape while falling.
1) The acceleration of gravity varies as the ball is falling.
2) Air resistance increases the acceleration of the ball.
3) The acceleration of the balls varies with the velocity.
Principles Vertical motion: y = 0.5 * g * t2, where y is the vertical displacement, g is the
acceleration due to gravity, t is the time.
Free fall: is a special case of motion with constant acceleration, in which the
acceleration is due to gravity.
Acceleration to Velocity Formula: v = u + a * t, where v is the final speed, u is
the initial velocity, a is the acceleration, and t is the time.
Answer Using the Principles of Vertical motion and Horizontal motion, we can solve
the problem as following:
Step 1: Apply the Vertical motion equation to find the time it takes for the ball
to fall 6 meters.
y = 0.5 * g * t2
6 m = 0.5 * 9.8 m/s2 * t2
t = 1.12 s
Step 2: Apply the Horizontal motion equation to find the velocity of the ball
when it passes through the photogate.
x = v * t
v = x / t
v = 6 m / 1.12 s
v = 5.36 m/s
Step 3: Compare the predicted velocity with the velocity measured by the
photogate. The predicted velocity is 20% above the velocity measured by the
photogate, so the predicted velocity is 1.2 * 5.36 m/s = 6.43 m/s.
Step 4: Find the cause of the large percent difference. The predicted velocity is
6.43 m/s, but the velocity measured by the photogate is 5.36 m/s. The difference
is 1.07 m/s. This difference is caused by air resistance. Air resistance is a force
that opposes the motion of an object through the air. It is caused by the friction
between the object and the air molecules. The greater the surface area of the
object, the greater the air resistance. The tennis ball has a large surface area, so
it experiences a lot of air resistance. This air resistance slows down the tennis
ball and causes the difference between the predicted velocity and the velocity
measured by the photogate.
Therefore, the correct answer is 0) The ball changes its shape while falling.
Table 27: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Reasoning Error. In this case, the model was able to correctly identify air resistance is the cause of
the velocity difference, but made the error at the final reasoning step of deriving the right answer.
31



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 1):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Despite the great advancements, complex multi-step reasoning remains challenging for even the state-
of-the-art LLMs. Lightman et al. (2023) show that process-supervision with step-by-step verification
is a promising remedy to improve the correctness of intermediate reasoning steps. Techniques such
as Chain-of-Thought (Wei et al., 2022b) were introduced to produce a coherent series of intermediate
reasoning steps to increase the success rate of following the right decoding path. Inspired by the
fact that when faced with challenging tasks humans often step back and do abstractions to arrive at
high-level principles to guide the process, we propose STEP -BACK PROMPTING to ground reasoning
on abstractions to reduce the chance of making errors in the intermediate reasoning steps.
What happens to the 
pressure, P, of an ideal gas if 
the temperature is increased 
by a factor of 2 and the 
volume is increased by a 
factor of 8 ?
What are the physics 
principles behind this 
question?
Ideal gas law: PV = nRT, where P is the pressure, V 
is the volume, n is the number of moles, R is the gas 
constant, and T is the temperature.
Apply Ideal gas law to find out the pressure.
If the temperature is increased by a factor of 2, then T 
becomes 2T. If the volume is increased by a factor of 8, 
then V becomes 8V.
Substituting these values into the ideal gas law, we get:
P(8V) = nR(2T)
Dividing both sides by 8V, we get:
P = nR(2T) / 8V
We can see that the pressure has decreased by a factor 
of 4.
If the temperature is increased by a factor of 2 
and the volume is increased by a factor of 8, 
then the pressure will decrease by a factor of 
16.
Original Question
Original Answer
Stepback Question
Final Answer
Stepback Answer
Step 1: Abstraction
Step 2: Reasoning
Step-Back Prompting
Estella Leopold went to which 
school between Aug 1954 
and Nov 1954?
What was Estella Leopold’s 
education history?
B.S. in Botany, University of Wisconsin, Madison, 1948
M.S. in Botany, University of California, Berkeley, 1950
Ph.D. in Botany, Yale University, 1955
She was enrolled in the Ph.D. program in Botany at Yale 
from 1951 to 1955. Therefore, Estella Leopold was most 
likely attending Yale University between August 1954 and 
November 1954.
Step 1: Abstraction Step 2: Reasoning
Estella Leopold attended the University of 
Wisconsin-Madison between August 1954 
and November 1954
Original Question
Original Answer
Stepback Question Stepback Answer
Final Answer
Step-Back Prompting
... After that, she pursued graduate studies 
at the University of California, Berkeley, 
earning her master's degree in 1950 and her 
Ph.D. in 1955.
Considering this timeline, it is highly likely 
that Estella Leopold was attending the 
University of California, Berkeley, between 
August 1954 and November 1954...
 Chain-of-Thought
…If the temperature is increased by a 
factor of 2 and the volume is increased by 
a factor of 8, we can rewrite the ideal gas 
law as: 2P' (8V) = nR(2T), where P' 
represents the new pressure.
Simplifying the equation, we get: 16P'V = 
2nRT or P' = (2nRT) / (16V)
Since n, R, and V are constants, we can 
write this as: P' = (1/8) * (nRT/V)
Since nRT/V is the original pressure P, we 
have: P' = (1/8) * P This means the new 
pressure P' is 1/8 of the original pressure 
P…
 Chain-of-Thought
Figure 2: Illustration of STEP -BACK PROMPTING with two steps of Abstraction and Reasoning
guided by concepts and principles. Top: an example of MMLU high-school physics (Hendrycks et al.,
2020) where the first principle of Ideal Gas Law is retrieved via abstraction. Bottom: an example
from TimeQA (Chen et al., 2021) where the high-level concept of education history is a result of the
abstraction. Left: PaLM-2L (Anil et al., 2023) fails to answer the original question. Chain-of-Thought
prompting (Wei et al., 2022b; Kojima et al., 2022) ran into errors during intermediate reasoning
steps (highlighted as red). Right: PaLM-2L (Anil et al., 2023) successfully answers the question via
STEP -BACK PROMPTING .
Among many of the cognitive skills, abstraction (Lachmy et al., 2022) is ubiquitous to humans’
ability to process vast amounts of information and derive general principles. For example, Kepler
compressed thousands of measurements into Kepler’s three laws of planetary motion, which precisely
describe the orbits of planets around the Sun (Russell, 1964). In critical decision-making, humans
find abstraction to be helpful since it provides a broader view of the environment. This work explores
how LLMs can tackle complex tasks involving many low-level details through a two-step process
of abstraction-and-reasoning. The first step is to show LLMs how to step back through in-context
learning – prompting them to derive high-level abstractions such as concepts and principles for
a specific example. The second step is to leverage the reasoning ability to reason on top of the
high-level concepts and principles. We use few-shot exemplar demonstrations to executeSTEP -BACK
PROMPTING on LLMs.
We experiment across a range of tasks involving domain specific reasoning such as Physics and Chem-
istry, knowledge-intensive question answering requiring factual knowledge, multi-hop commonsense
reasoning. We observe significant performance improvements (up to 27%) in PaLM-2L (Anil et al.,
2



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 16):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question A spherical conductor carries a net charge. How is this charge distributed on
the sphere?
0) The charge is evenly distributed on the surface.
1) The charge resides on the surface only; the distribution of charge on the
surface depends on what other charged objects are near the sphere.
2) The charge moves continually within the sphere.
3) The charge is distributed uniformly throughout the sphere.
Principles Coulomb’s Law: the force between two charged particles is proportional to the
product of their charges and inversely proportional to the square of the distance
between them, F = k * q1 * q2 / r, where F is the electric force, k is a constant,
q1 and q2 are the charges the particles carry, and r is the distance between them.
Answer Using the Principles of Coulomb’s Law, we can solve the problem as following:
Step 1: Apply Coulomb’s Law to find out how charges are distributed on the
surface.
In the case of a spherical conductor, the charges on the surface will repel each
other. The further apart the charges are, the less force they will exert on each
other. Therefore, the charges will distribute themselves evenly on the surface
of the sphere, as this is the configuration that minimizes the repulsive force
between them.
Step 2: Apply Coulomb’s Law to find out what happens if there are other
charges present.
The distribution of charge on the surface may also be affected by the presence
of other charged objects near the sphere. For example, if a negatively charged
object is brought near a positively charged sphere, the negative charges on the
sphere will be repelled and will move to the opposite side of the sphere. This
will result in a non-uniform distribution of charge on the surface of the sphere.
Therefore, the correct answer is 1) The charge resides on the surface only; the
distribution of charge on the surface depends on what other charged objects are
near the sphere.
Table 9: Few-shot demonstration exemplar for MMLU high-school Physics.
17



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 31):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: What happens to the pressure, P, of an ideal gas if
the temperature is increased by a factor of 2 and the volume is increased
by a factor of 8 ?
0) P decreases by a factor of 16.
1) P decreases by a factor of 4.
2) P decreases by a factor of 2.
3) P increases by a factor of 4.
Original Answer [Incorrect]: The correct answer is 0. The ideal gas
law is PV = nRT. If the temperature is increased by a factor of 2 and the
volume is increased by a factor of 8, then the pressure will decrease by a
factor of 16.
Stepback Question: What are the physics principles behind this
question?
Principles:
Ideal gas law: PV = nRT, where P is the pressure, V is the volume, n is
the number of moles, R is the gas constant, and T is the temperature.
Final Answer: Using the Principles of Ideal gas law, we can solve the
problem as following:
Step 1: Apply Ideal gas law to find out the pressure of an ideal gas. The
ideal gas law states that PV = nRT, where P is the pressure, V is the
volume, n is the number of moles, R is the gas constant, and T is the
temperature.
Step 2: Apply Ideal gas law to find out the pressure of an ideal gas when
the temperature is increased by a factor of 2 and the volume is increased
by a factor of 8. If the temperature is increased by a factor of 2, then T be-
comes 2T. If the volume is increased by a factor of 8, then V becomes 8V .
Substituting these values into the ideal gas law, we get: P(8V) = nR(2T)
Dividing both sides by 8V , we get: P = nR(2T) / 8V
We can see that the pressure has decreased by a factor of 4.
Therefore, the correct answer is 1) P decreases by a factor of 4.
Table 28: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
32



### Claim 119/179

#### Claim Text
Rabi measurements were performed prior to the Hahn-echo to obtain the exact π-pulse duration.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 13):

0 1000 2000 3000 4000
24.5
25
25.5
26
26.5
27
FLOPS (MACs)
Rouge-L
2%
(a) ELI5
0 1000 2000 3000 4000
20
21
22
23
24
25
FLOPS (MACs)
2% (b) MS MARCO
0 1000 2000 3000 4000
24
25
26
27
28
29
30
31
32
FLOPS (MACs)
2% (c) NQ
0 20 40 60 80 100
24.5
25
25.5
26
26.5
27
Input P sgs
Rouge-L
2%
(d) ELI5
0 20 40 60 80 100
20
21
22
23
24
25
Input P sgs
2% (e) MS MARCO
0 20 40 60 80 100
24
25
26
27
28
29
30
31
32
FiD
CALM
T ok en Filtering
Combined
Input P sgs
2% (f) NQ
Figure 6: The ROUGE-L performance on FiD-Base vs. the FLOPS (MACs) (First row), and vs. the input passage
amount (Input Psg., second row). The results are on the test set for each dataset, for the various methods utilized.
We observe that the trends of the FLOPS (MACs) and the Input Psg. are very similar, since the passages effect the
encoder the most, and the encoder has the most impact on FLOPS (MACs) (as stated by de Jong et al. (2022)).



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 14):

Kendall’s Tau by Dataset
NQ MultiRC ReCoRD
PPI Labeled
Count C.R. A.R. C.R. A.R. C.R. A.R.
400 1.0 1.0 0.89 0.94 0.89 0.94
300 0.89 1.0 0.94 0.89 0.83 0.89
200 0.83 1.0 0.83 0.94 0.83 0.83
150 0.72 1.0 0.83 0.89 0.72 0.83
100 0.44 1.0 0.67 0.67 0.67 0.83
50 0.44 0.94 0.61 0.44 0.56 0.67
25 0.44 0.89 0.56 0.44 0.44 0.56
Table 3: Analysis of PPI Labeled Count vs. ARES Efficacy by Kendall’s Tau: The Kendall’s tau values represent
the correlation between the correct ranking and the ARES ranking of the pseudo RAG systems. We use the same
experimental set-up as described in subsection 4.2. We find that below about 100-150 datapoints in the human
preference validation set, ARES cannot meaningfully distinguish between the alternate RAG systems based on their
accuracies in context relevance and answer relevance (C.R. and A.R., respectively).
ARES Ranking of Pseudo RAG Systems using GPT-4 Labels
NQ ReCoRD MultiRC
Context
Relevance
Answer
Relevance
Context
Relevance
Answer
Relevance
Context
Relevance
Answer
Relevance
Kendall’s Tau 0.78 1.0 0.78 0.72 0.89 0.78
Kendall’s Tau of
Human Labeled Approach 0.94 1.0 0.83 0.89 0.94 0.89
Average PPI Range 9.2% 6.8% 8.2% 9.0% 7.7% 8.3%
Accuracy on
RAG Evaluation Sets 79.3% 96.7% 88.4% 78.3% 85.8% 82.5%
Table 4: GPT-4 Labels vs. Human Labels : We wanted to explore the practicality of using GPT-4 generated
labels instead of human annotations for our human preference validation set in ARES. In the experiments, we
generated 500 GPT-4 labels as replacements for human labeling using few-shot prompts (see Sections A.2, A.3,
and A.4). While GPT-4 generated labels decreased Kendall’s tau in most settings by 0.05 to 0.30, the ability to
cheaply produce GPT-4 generated labels significantly reduces the cost of annotation, cutting it from hundreds of
annotations to less than ten for few-shot prompts. Additionally, the efficacy of PPI continues improving as we
generate more GPT-4 generated labels. In the table, we define PPI range as the number of percentage points from
the lower number to the upper number of the PPI confidence bounding. Additionally, we use the fine-tuned LLM
judge (DeBERTa-v3-Large) for evaluation.



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 15):

ARES Ranking of Real RAG Systems
NQ WoW FEVER
C.R. A.R. C.R. A.R. C.R. A.R.
Kendall’s Tau for
Sampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82
Kendall’s Tau
for RAGAS 0.82 0.82 0.73 0.82 0.73 0.87
Kendall’s Tau
for GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87
Kendall’s Tau
for ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87
Kendall’s Tau
for ARES 1.0 0.96 0.91 1.0 0.82 1.0
RAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%
GPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%
ARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%
Table 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance
(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled
annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150
annotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses
GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we
found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.
Additionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted
the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial
costs required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human
preference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.
ARES Cross-Domain Ranking of Pseudo RAG Systems
NQ to
FEVER
FEVER to
NQ
NQ to
MultiRC
MultiRC to
NQ
NQ to
ReCoRD
ReCoRD to
NQ
C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.
Kendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94
Kendall’s Tau of
In-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0
Average PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%
Accuracy on
RAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%
Table 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the
fine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance
(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains
when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and
ReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that
additional examples further improved the performance of ARES. Furthermore, we found that even in scenarios
where the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ
to FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of
percentage points from the lower bound to the upper bound of the PPI confidence interval.



Source: data\tc16_2312.10997v5\referenced_papers\[20]_2303.08518.pdf (Page 5):

Task Metric GPT-Neo-2.7B BLOOM-7.1B OPT-66B Davinci Davinci-001
0-SHOT UPRISE 0-SHOT UPRISE 0-SHOT UPRISE 0-SHOT UPRISE 0-SHOT UPRISE
Reading Comprehension
SQuADv1 F1 4.4 26.4 4.5 5.5 6.1 7.5 6.5 6.0 41.6 57.7
EM 0.4 14.3 0.0 0.0 0.0 0.6 0.0 0.0 16.4 36.8
BoolQ Acc 54.5 59.4 54.0 60.2 60.7 63.5 62.0 65.7 64.2 65.7
MultiRC F1 57.1 58.1 58.8 59.8 59.6 60.4 59.8 60.0 54.3 58.9
OBQA Acc 41.8 42.2 44.0 41.8 46.4 48.8 49.2 52.4 52.8 48.8
Average 31.6 40.1 32.3 33.5 34.6 36.2 35.5 36.8 45.9 53.6
Closed-book QA
ARC-e Acc 45.7 55.6 53.7 60.9 56.2 66.0 64.1 71.8 67.0 74.4
ARC-c Acc 29.3 30.0 33.2 34.2 36.7 40.2 40.8 45.2 46.2 50.4
NQ F1 1.3 5.6 0.9 1.4 2.5 2.1 0.0 2.2 18.3 18.2
EM 0.5 2.2 0.0 0.1 0.3 0.4 0.0 0.0 4.8 8.7
Average 19.2 23.3 22.0 24.2 23.9 27.2 26.2 29.8 34.1 37.9
Paraphrase Detection
MRPC Acc 46.6 67.9 51.0 70.6 51.0 68.9 54.4 62.3 40.0 61.3
F1 46.0 80.4 58.0 82.1 57.8 81.5 68.9 81.4 39.2 72.9
QQP Acc 48.4 54.3 49.5 53.1 50.5 49.7 55.2 52.4 60.9 62.6
F1 42.2 59.8 46.7 59.6 43.7 58.5 33.7 57.9 43.0 45.9
PAWS Acc 51.7 45.7 50.8 45.9 50.5 44.4 52.4 44.5 53.2 52.3
Average 47.0 61.6 51.2 62.3 50.7 60.6 52.9 59.7 47.3 59.0
Natural Language Inference
MNLI-m Acc 35.3 41.3 35.4 36.0 37.0 40.4 34.2 38.2 44.7 41.1
MNLI-mm Acc 36.4 43.1 34.9 35.8 37.1 41.2 34.2 38.6 46.5 42.1
QNLI Acc 50.9 53.8 49.9 51.3 54.2 53.7 51.7 51.1 60.0 58.4
SNLI Acc 35.2 42.3 35.2 34.4 34.5 40.2 33.5 37.9 47.5 42.0
RTE Acc 33.6 34.7 50.5 49.8 52.3 46.9 51.3 45.5 52.3 50.9
Average 38.3 43.0 41.2 41.5 43.0 44.5 41.0 42.3 50.2 46.9
Sentiment Analysis
SST-2 Acc 52.4 56.2 63.2 69.1 57.9 65.3 52.3 64.3 90.5 90.5
Yelp Acc 71.7 67.8 56.1 58.0 67.6 63.5 59.8 65.3 80.3 80.2
Sent140 Acc 64.1 61.3 74.5 72.1 59.1 61.6 64.3 72.1 87.2 89.1
Average 62.7 61.8 64.6 66.4 61.5 63.5 58.8 67.3 86.0 86.6
Commonsense Reasoning
PiQA Acc 70.2 70.4 71.5 72.1 76.5 80.4 79.1 81.3 79.1 79.1
COPA Acc 67.0 64.0 67.0 67.0 74.0 76.0 80.0 83.0 83.0 80.0
HellaSwag Acc 54.4 52.1 59.6 58.8 72.9 71.4 76.9 76.7 77.6 78.2
Average 63.9 62.2 66.0 66.0 74.5 75.9 78.7 80.3 79.9 79.1
Coreference Resolution
WSC273 Acc 73.6 76.6 78.0 81.0 83.9 86.1 60.6 50.0 78.8 75.5
DPR Acc 59.6 51.0 64.4 55.8 66.3 50.0 82.1 83.9 64.4 58.7
Winogrande Acc 58.9 58.6 65.9 64.3 69.2 67.8 68.6 70.2 66.3 64.7
Average 64.0 62.1 69.4 67.0 73.1 68.0 70.4 68.0 69.8 66.3
Table 1: Zero-shot performance across tasks and LLMs. The model Davinci-001 is the fine-tuned version
text-davinci-001 of Davinci. The method 0-SHOT is the vanilla zero-shot method with only the in-
put instruction fed into the LLM.



Source: data\tc16_2312.10997v5\referenced_papers\[165]_2311.09476.pdf (Page 3):

Figure 1: Overview of ARES: As inputs, the ARES pipeline requires an in-domain passage set, a human preference
validation set of 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers (five or
more), which are used for prompting LLMs in synthetic data generation. To prepare our LLM judges for evaluation,
we first generate synthetic queries and answers from the corpus passages. Using our generated training triples and a
constrastive learning framework, we fine-tune an LLM to classify query–passage–answer triples in three different
criteria: context relevance, answer faithfulness, and answer relevance. Finally, we use the LLM judges to score
RAG systems and generate confidence bounds for the ranking using PPI and the human preference validation set.
3.3 Ranking RAG Systems with Confidence
Intervals
Once we have prepared our LLM judges, we need
to use them to score and rank the competing RAG
systems. To do this, ARES samples the in-domain
query-document-answer triples produced by each
RAG approach, and the judges label each triple,
predicting their context relevance, answer faithful-
ness, and answer relevance. By averaging the in-
dividual predicted labels for each in-domain triple,
we calculate the RAG system performance across
each of the three metrics.
In principle, we could simply report these aver-
age scores as quality metrics for each RAG system.
However, these scores reflect entirely unlabeled
data with predictions from a synthetically-trained
LLM judge, and hence they may not be entirely
accurate. As an extreme alternative, we could use
just the small human preference validation set dis-
cussed previously for evaluation, reporting the ex-
tent to which each RAG system agrees with (or
deviates from) the human annotations. However,
an annotation-based evaluation approach would re-
quire labeling substantially more generative out-
puts from each RAG systems separately, which can
be costly both in terms of time and financing.
To combine the benefits of both, and hence
boost the precision of the evaluation, ARES uses
prediction-powered inference (PPI; Angelopoulos
et al. 2023) to predict the system scores. PPI is
a recent statistical method that provides tighter
confidence intervals on a small set of annotated
datapoints (i.e., our validation set) by leveraging
predictions on a much larger set of non-annotated
datapoints. PPI can leverage both the labeled dat-
apoints and the ARES judge predictions on the
non-annotated datapoints to construct confidence
intervals for our RAG system’s performance.
To do this, PPI uses the LLM judges on the hu-
man preference validation set to learn a rectifier
function for constructing a confidence set of the ML
model’s performance, using each ML prediction in
the larger non-annotated dataset. The confidence
set can then be used to create a tighter confidence
interval for the performance of the evaluated RAG
system (e.g. its context relevance, answer faithful-
ness, or answer relevance accuracy individually)
compared to simply using annotated outputs from
the evaluated RAG system. By bolstering the hu-
man preference validation set with the much larger
set of datapoints with ML predictions, PPI can de-
velop reliable confidence intervals for ML model
performance that beat previous classical inference
approaches.
The PPI rectifier function allows us to estimate
the errors of the LLM judge and generate confi-
dence bounds for the success and failure rates of the
RAG system, estimating context relevance, answer
faithfulness, and answer relevance performance.
Additionally, PPI allows us to estimate confidence
intervals with a selected level of probability; for our
experiments, we use a standard 95% alpha (proba-
bility) for our confidence interval.
With the accuracy confidence interval for each
component of the RAG, we find the midpoint of
each confidence interval and use the midpoints to
rank the RAG systems. With our ranking, we can
compare different RAG systems, as well as differ-
ent configurations of the same RAG system, to find
the best-performing approach for a given domain.



### Claim 120/179

#### Claim Text
This is due to the Doppler width of N2O being comparable to frep, which results in relatively small instrumental line shape distortions , limiting the precision with which the OPD can be calibrated .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[33]_2305.17653.pdf (Page 5):

/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013/uni00000014/uni00000014/uni00000014/uni00000015/uni00000014/uni00000016/uni00000014/uni00000017/uni00000014/uni00000018/uni00000014/uni00000019
/uni00000006/uni0000004f/uni00000044/uni00000045/uni00000048/uni0000004f/uni00000003/uni00000046/uni00000052/uni00000051/uni00000056/uni0000004c/uni00000056/uni00000057/uni00000048/uni00000051/uni00000057/uni00000003/uni00000048/uni00000059/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048
/uni00000013
/uni00000015/uni00000013
/uni00000017/uni00000013
/uni00000019/uni00000013
/uni0000001b/uni00000013
/uni00000014/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000006/uni0000004c/uni00000051/uni00000056/uni00000057/uni00000044/uni00000051/uni00000046/uni00000048/uni00000056
/uni00000013
/uni00000015/uni00000017
/uni00000017/uni0000001b
/uni0000001a/uni00000015
/uni0000001c/uni00000019
/uni00000014/uni00000015/uni00000013
/uni00000006/uni0000004c/uni00000051/uni00000056/uni00000057/uni00000044/uni00000051/uni00000046/uni00000048/uni00000056
(a) PGRA
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
#label consistent evidence
0
20
40
60
80
100Accuracy (%)
Accuracy
#instances
0
24
48
72
96
120
#instances
(b) FiD
Figure 2: The pseudo label consistency of samples in
SST-2 with PGRA and FiD (T5-base models for both).
We plot the accuracy scores of instances with different
numbers of label-consistent evidence, along with the
number of such instances.
the total number of instances with different num-
bers of consistent evidence. We then compute the
average accuracy of PGRA for the instances with
the same number of consistent evidence. The re-
sults are shown in Figure 2a. Firstly, since we
rerank the evidence based on the relevance score of
pseudo-labels, the number of instances also rises as
the number of consistent evidence increases. The
phenomenon indicates that we can always find suffi-
cient task-specific evidence retrieved from the first
stage, except for a small part of inputs which is
possibly caused by the limitation of the k’s size in
the first stage. Secondly, the average accuracy is
also rising as the number of consistent evidence in-
creases, which reflects that the model performance
is related to the (pseudo) label consistency. How-
ever, when the number of consistent evidence is
small (i.e., 3 and 4), the accuracy can also be high.
This is because the number of instances is too small,
so the result is insignificant. Furthermore, it is in-
teresting to find that when the number of consistent
evidence is high enough (i.e., larger than 13), the
accuracy approaches 100%, which shows that there
exists high potential in increasing label consistency
to improve model performance.
We use the same method to plot the label con-
sistency figure on the FiD baseline, shown in Fig-
/uni00000014/uni00000019/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013
/uni0000004e
/uni0000001b/uni00000014/uni00000011/uni0000001b
/uni0000001b/uni00000015/uni00000011/uni00000013
/uni0000001b/uni00000015/uni00000011/uni00000015
/uni0000001b/uni00000015/uni00000011/uni00000017
/uni0000001b/uni00000015/uni00000011/uni00000019/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000017/uni0000001b/uni00000014/uni00000019/uni00000015/uni00000017
/uni00000047
/uni0000001b/uni00000014/uni00000011/uni0000001b
/uni0000001b/uni00000015/uni00000011/uni00000013
/uni0000001b/uni00000015/uni00000011/uni00000015
/uni0000001b/uni00000015/uni00000011/uni00000017
/uni0000001b/uni00000015/uni00000011/uni00000019
Figure 3: Accuracy against k (left) and d (right). De-
tails of performance on different tasks can be found in
Appendix C.
ure 2b. As can be seen from the figure, it still holds
that the more label-consistent evidence, the higher
accuracy the model can achieve. The difference be-
tween PGRA and FiD is that PGRA can retrieve
more label-consistent evidence than FiD.
4.2 Effects of k and d
In this section, we further investigate the effects
of k and d on the performance, where k and d
are the numbers of final retrieved evidence in the
first and second stages, respectively. In detail, we
run PGRA with different k or different d, while
other setups keep the same as main experiments.
As seen from Figure 3, larger k values can con-
sistently improve the average performance, while
larger d values maintain a relatively stable trend.
As for k, larger k values mean providing more can-
didate evidence for the second stage reranker to
find more appropriate instances with (pseudo) label
consistency. As for d, larger d values indicate more
consistent evidence if the proportion of consistent
evidence keeps the same. At the same time, their
top consistent evidence is the same, and the candi-
date evidence is fixed with the same k, so their per-
formance is close. In our expectation, the PGRA
can better solve diverse NKI tasks with larger k if
enough computing resources are allowed.
4.3 Effects of OPT Model Sizes
In this section, we first investigate the effects on
the performance of different sizes of the OPT mod-
els used in the prompt-guided reranker. Specifi-
cally, we vary the size of OPT models and con-
duct experiments in five downstream tasks. The
model performances are shown in the orange line
of Figure 4. The overall trend is obviously that the
larger OPT models can achieve better performance.
We believe that the larger OPT models have better
abilities to apply task-specific features to encode



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 7):

Feed-Forward Ratio (dff / dmodel) 
50M Parameters Aspect Ratio (dmodel / nlayer) Attention Head Dimension (dmodel / nhead) 
25M Parameters
10%
8%
6%
4%
2%
0% Loss Increase
A wide range of architectures 
achieve similar performance
22% additional compute
compensates for 1% loss increase
Figure 5 Performance depends very mildly on model shape when the total number of non-embedding
parameters N is held ﬁxed. The loss varies only a few percent over a wide range of shapes. Small differences
in parameter counts are compensated for by using the ﬁt to L(N) as a baseline. Aspect ratio in particular can
vary by a factor of 40 while only slightly impacting performance; an (nlayer,dmodel) = (6,4288) reaches a
loss within 3% of the (48,1600) model used in [RWC+19].
106 107 108 109
Parameters (with embedding)
2
3
4
5
6
7Test Loss
0 Layer
1 Layer
2 Layers
3 Layers
6 Layers
> 6 Layers
103 104 105 106 107 108 109
Parameters (non-embedding)
2
3
4
5
6
7Test Loss
1 Layer
2 Layers
3 Layers
6 Layers
> 6 Layers
Figure 6 Left: When we include embedding parameters, performance appears to depend strongly on the
number of layers in addition to the number of parameters. Right: When we exclude embedding parameters,
the performance of models with different depths converge to a single trend. Only models with fewer than 2
layers or with extreme depth-to-width ratios deviate signiﬁcantly from the trend.
In this section we will display data along with empirically-motivated ﬁts, deferring theoretical analysis to
later sections.
3.1 Approximate Transformer Shape and Hyperparameter Independence
Transformer performance depends very weakly on the shape parametersnlayer,nheads, and dﬀ when we hold
the total non-embedding parameter count N ﬁxed. To establish these results we trained models with ﬁxed
size while varying a single hyperparameter. This was simplest for the case of nheads. When varying nlayer,
we simultaneously varied dmodel while keeping N ≈12nlayerd2
model ﬁxed. Similarly, to vary dﬀ at ﬁxed
model size we also simultaneously varied the dmodel parameter, as required by the parameter counts in Table
1. Independence of nlayers would follow if deeper Transformers effectively behave as ensembles of shallower
models, as has been suggested for ResNets [VWB16]. The results are shown in Figure 5.
3.2 Performance with Non-Embedding Parameter Count N
In Figure 6 we display the performance of a wide variety of models, ranging from small models with shape
(nlayer,dmodel) = (2,128) through billion-parameter models, ranging in shape from (6,4288) through
(207,768). Here we have trained to near convergence on the full WebText2 dataset and observe no over-
ﬁtting (except possibly for the very largest models).
As shown in Figure 1, we ﬁnd a steady trend with non-embedding parameter countN, which can be ﬁt to the
ﬁrst term of Equation (1.5), so that
L(N) ≈
(Nc
N
)αN
(3.1)
8



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 10):

106 107 108 109
Params (non-embed)
2.5
3.0
3.5
4.0
4.5Test Loss
Data Size Bottleneck
Data Size
21M
43M
86M
172M
344M
688M
1.4B
22.0B
10 4
 10 3
 10 2
 10 1
N N/ D/D
0.0
0.1
0.2
0.3
0.4
0.5L/L(D = ) 1
Overfitting
Data Size
21M
43M
86M
172M
344M
688M
1.4B
22.0B
Figure 9 The early-stopped test loss L(N,D) depends predictably on the dataset size Dand model size N
according to Equation (1.5). Left: For large D, performance is a straight power law inN. For a smaller ﬁxed
D, performance stops improving as N increases and the model begins to overﬁt. (The reverse is also true,
see Figure 4.) Right: The extent of overﬁtting depends predominantly on the ratio N
αN
αD /D, as predicted in
equation (4.3). The line is our ﬁt to that equation.
Since we stop training early when the test loss ceases to improve and optimize all models in the same way, we
expect that larger models should always perform better than smaller models. But with ﬁxed ﬁnite D, we also
do not expect any model to be capable of approaching the best possible loss (ie the entropy of text). Similarly,
a model with ﬁxed size will be capacity-limited. These considerations motivate our second principle. Note
that knowledge of L(N) at inﬁnite Dand L(D) at inﬁnite N fully determines all the parameters in L(N,D).
The third principle is more speculative. There is a simple and general reason one might expect overﬁtting
to scale ∝1/D at very large D. Overﬁtting should be related to the variance or the signal-to-noise ratio
of the dataset [AS17], and this scales as 1/D. This expectation should hold for any smooth loss function,
since we expect to be able to expand the loss about the D →∞ limit. However, this argument assumes that
1/Dcorrections dominate over other sources of variance, such as the ﬁnite batch size and other limits on the
efﬁcacy of optimization. Without empirical conﬁrmation, we would not be very conﬁdent of its applicability.
Our third principle explains the asymmetry between the roles of N and D in Equation (1.5). Very similar
symmetric expressions4 are possible, but they would not have a 1/D expansion with integer powers, and
would require the introduction of an additional parameter.
In any case, we will see that our equation for L(N,D) ﬁts the data well, which is the most important justiﬁ-
cation for our L(N,D) ansatz.
4.2 Results
We regularize all our models with 10% dropout, and by tracking test loss and stopping once it is no longer
decreasing. The results are displayed in Figure 9, including a ﬁt to the four parameters αN,αD,Nc,Dc in
Equation (1.5):
Parameter αN αD Nc Dc
Value 0.076 0.103 6.4 ×1013 1.8 ×1013
Table 2 Fits to L(N,D)
We obtain an excellent ﬁt, with the exception of the runs where the dataset has been reduced by a factor of
1024, to about 2 ×107 tokens. With such a small dataset, an epoch consists of only 40 parameter updates.
Perhaps such a tiny dataset represents a different regime for language modeling, as overﬁtting happens very
early in training (see Figure 16). Also note that the parameters differ very slightly from those obtained in
Section 3, as here we are ﬁtting the full L(N,D) rather than just L(N,∞) or L(∞,D).
To chart the borderlands of the inﬁnite data limit, we can directly study the extent of overﬁtting. For all but
the largest models, we see no sign of overﬁtting when training with the full 22B token WebText2 dataset,
so we can take it as representative of D = ∞. Thus we can compare ﬁnite D to the inﬁnite data limit by
4For example, one might have used L(N, D) =
[( Nc
N
)αN
+
( Dc
D
)αD]β
, but this does not have a 1/D expansion.
11



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 16):

The intersection point is sensitive to 
the precise power-law parameters
Figure 15 Far beyond the model sizes we study empirically, we ﬁnd a contradiction between our equations
for L(Cmin) and L(D) due to the slow growth of data needed for compute-efﬁcient training. The intersection
marks the point before which we expect our predictions to break down. The location of this point is highly
sensitive to the precise exponents from our power-law ﬁts.
6.3 Contradictions and a Conjecture
We observe no signs of deviation from straight power-law trends at large values of compute, data, or model
size. Our trends must eventually level off, though, since natural language has non-zero entropy.
Indeed, the trends for compute-efﬁcient training described in this section already contain an apparent contra-
diction. At scales several orders of magnitude above those documented here, the performance predicted by
the L(Cmin) scaling law decreases below what should be possible given the slow growth in training data with
compute. This implies that our scaling laws must break down before this point, but we conjecture that the
intersection point has a deeper meaning: it provides an estimate of the point at which Transformer language
models reach maximal performance.
Since the amount of data used by compute-efﬁcient training grows slowly with the compute budget, the
performance predicted by L(Cmin) eventually hits a lower bound set by theL(D) power law (see Figure 15).
Let us work this out in more detail.
To keep overﬁtting under control, the results of Section 4 imply that we should scale the dataset size as
D∝N0.74 ∝C0.54
min (6.6)
where we have used the compute-efﬁcient N(Cmin) from Figure 14.
Let us compare this to the data requirements of compute-efﬁcient training. If we train at the critical batch
size (i.e. C = 2Cmin) and never re-use data during training, we ﬁnd that data usage grows with compute as
D(Cmin) = 2Cmin
6N(Cmin) ≈
(
4 ×1010 tokens
)
(Cmin/PF-Day)0.26 (6.7)
This is the maximum rate at which the dataset size can productively grow with compute, since it means that
we are only training for a single epoch. But it grows the dataset much more slowly than in Equation (6.6).
It appears to imply that compute-efﬁcient training will eventually run into a problem with overﬁtting, even if
the training process never re-uses any data!
According to Figure 1, we expect that when we are bottlenecked by the dataset size (ie by overﬁtting), the
loss should scale asL(D) ∝D−0.095. This implies that the loss would scale with compute asL(D(Cmin)) ∝
C−0.03
min once we are data-limited. Once again, we have a contradiction, as this will eventually intersect with
our prediction for L(Cmin) from Figure 13, where we found a scaling L(Cmin) ∝C−0.050
min .
The intersection point of L(D(Cmin)) and L(Cmin) occurs at
C∗∼104 PF-Days N∗∼1012 parameters, D ∗∼1012 tokens, L ∗∼1.7 nats/token (6.8)
though the numerical values are highly uncertain, varying by an order or magnitude in either direction de-
pending on the precise values of the exponents from the power-law ﬁts. The most obvious interpretation is
that our scaling laws break down at or before we reach this point, which is still many orders of magnitude
away in both compute and model size.
17



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 22):

103 104 105
Sc × [L(N, D) L(N, )] 1/ S
103
104
105
Sstop
Early Stopping Step
Data Size
21M
43M
86M
172M
344M
688M
1.4B
103 104 105
Step
2
3
4
5
6Loss
Test Loss
Train Loss
108
109
1010
Dataset Size (Tokens)
Figure 16 Left: We characterize the step on which early stopping occurs, as a function of the extent of
overﬁtting. The red line indicates a lower bound for early stopping that is derived in Section 5.3. Right:
We display train and test loss for a series of 300M parameter models trained on different sized dataset sub-
samples. The test loss typically follows that of a run done with unrestricted data until diverging. Note that the
degree of overﬁtting (as compared to the inﬁnite data limit) is signiﬁcantly overestimated by Ltest −Ltrain
(denoted by a black bar for each run).
• We are not especially conﬁdent in the prediction of Bcrit(L) for values of the loss far outside the
range we have explored. Changes in Bcrit could have a signiﬁcant impact on trade-offs between
data parallelism and the number of serial training steps required, which would have a major impact
on training time.
• We did not thoroughly investigate the small data regime, and our ﬁts for L(N,D) were poor for
the smallest values of D (where an epoch corresponded to only 40 steps). Furthermore, we did
not experiment with regularization and data augmentation. Improvements in these could alter our
results, quantitatively or qualitatively.
• We used the estimated training compute C ≈6NBS, which did not include contributions propor-
tional to nctx (see Section 2.1). So our scalings with compute may be confounded in practice in the
regime of very large nctx, speciﬁcally where nctx ≳ 12dmodel.
• We tuned learning rates, and we experimented with learning rate schedules. But we may have
neglected to tune some hyperparameter (e.g. intialization scale or momentum) that have an important
effect on scaling.
• The optimal choice of learning rate is sensitive to the target loss. When training close to convergence,
it may be necessary to use a smaller learning rate to avoid divergences. But when conducting a short
training run (eg due to compute limitations), it may be possible to use a larger learning rate. We did
not experiment with higher learning rates for training runs that did not proceed to convergence.
D Supplemental Figures
D.1 Early Stopping and Test vs Train
In section 5.3 we described the result shown in Figure 16, which provides a prediction for a lower bound on
the early stopping step. We also show the train and test loss for a given model size when training on different
sized datasets.
D.2 Universal Transformers
We compare the performance of standard Transformers to recurrent Transformers [DGV +18] in Figure 17.
These models re-use parameters, and so perform slightly better as a function of N, but slightly worse as a
function of compute C. We include several different different possibilities for parameter re-use.
D.3 Batch Size
We measure the critical batch size using the data displayed in ﬁgure 18. This made it possible to estimate
Bcrit(L) in ﬁgure 10.
23



### Claim 121/179

#### Claim Text
They stay close to the original ideas of Peskin (see also Section 5.2) of an “elastic restoration force” that counteracts deformations of the immersed solid object as a result of fluid flow.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 32):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: A liquid flows at a constant flow rate through a pipe
with circular cross-sections of varying diameters. At one point in the
pipe, the diameter is 2 cm and the flow speed is 18 m/s. What is the flow
speed at another point in this pipe, where the diameter is 3 cm?
0) 4 m/s
1) 6 m/s
2) 8 m/s
3) 12 m/s
Original Answer [Incorrect]: The answer is 3) 12 m/s A step-by-step
solution is below: The flow rate is constant, so the product of the speed
and the cross-sectional area is constant. The cross-sectional area is
proportional to the square of the diameter, so the speed is inversely
proportional to the square of the diameter. The ratio of the speeds is the
inverse of the ratio of the squares of the diameters, so the speed at the
second point is 18m/s ∗ (2cm/3cm)2 = 12 m/s.
Stepback Question: What are the physics principles behind this
question?
Principles:
Conservation of Mass: the amount of mass flowing through the pipe at
two different points must be equal, or A1 * v1 = A2 * v2, where A1 is
the area of cross-section one, v1 is the velocity at point 1, A2 is the area
of cross-section two, v2 is the velocity at point 2.
Area of Circle: A = π ∗ r2, where r is the radius of the circle.
Final Answer: Using the Principles of Continuity equation, we can
solve the problem as following:
Step 1: Apply the Continuity equation to find out the flow speed at
another point in this pipe. The continuity equation states that the product
of the area and the velocity is constant at all points in a pipe. In this
problem, we are given the following information:
A1 = π ∗ (2cm)2 = 4πcm2
v1 = 18 m/s
A2 = π ∗ (3cm)2 = 9πcm2
We can use the continuity equation to find v2:
A1 * v1 = A2 * v2
4πcm2 ∗ 18m/s = 9πcm2 ∗ v2
v2 = 8 m/s
Therefore, the flow speed at another point in this pipe is 8 m/s.
The correct answer is 2) 8 m/s.
Table 29: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
33



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 31):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: What happens to the pressure, P, of an ideal gas if
the temperature is increased by a factor of 2 and the volume is increased
by a factor of 8 ?
0) P decreases by a factor of 16.
1) P decreases by a factor of 4.
2) P decreases by a factor of 2.
3) P increases by a factor of 4.
Original Answer [Incorrect]: The correct answer is 0. The ideal gas
law is PV = nRT. If the temperature is increased by a factor of 2 and the
volume is increased by a factor of 8, then the pressure will decrease by a
factor of 16.
Stepback Question: What are the physics principles behind this
question?
Principles:
Ideal gas law: PV = nRT, where P is the pressure, V is the volume, n is
the number of moles, R is the gas constant, and T is the temperature.
Final Answer: Using the Principles of Ideal gas law, we can solve the
problem as following:
Step 1: Apply Ideal gas law to find out the pressure of an ideal gas. The
ideal gas law states that PV = nRT, where P is the pressure, V is the
volume, n is the number of moles, R is the gas constant, and T is the
temperature.
Step 2: Apply Ideal gas law to find out the pressure of an ideal gas when
the temperature is increased by a factor of 2 and the volume is increased
by a factor of 8. If the temperature is increased by a factor of 2, then T be-
comes 2T. If the volume is increased by a factor of 8, then V becomes 8V .
Substituting these values into the ideal gas law, we get: P(8V) = nR(2T)
Dividing both sides by 8V , we get: P = nR(2T) / 8V
We can see that the pressure has decreased by a factor of 4.
Therefore, the correct answer is 1) P decreases by a factor of 4.
Table 28: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
32



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 16):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question A spherical conductor carries a net charge. How is this charge distributed on
the sphere?
0) The charge is evenly distributed on the surface.
1) The charge resides on the surface only; the distribution of charge on the
surface depends on what other charged objects are near the sphere.
2) The charge moves continually within the sphere.
3) The charge is distributed uniformly throughout the sphere.
Principles Coulomb’s Law: the force between two charged particles is proportional to the
product of their charges and inversely proportional to the square of the distance
between them, F = k * q1 * q2 / r, where F is the electric force, k is a constant,
q1 and q2 are the charges the particles carry, and r is the distance between them.
Answer Using the Principles of Coulomb’s Law, we can solve the problem as following:
Step 1: Apply Coulomb’s Law to find out how charges are distributed on the
surface.
In the case of a spherical conductor, the charges on the surface will repel each
other. The further apart the charges are, the less force they will exert on each
other. Therefore, the charges will distribute themselves evenly on the surface
of the sphere, as this is the configuration that minimizes the repulsive force
between them.
Step 2: Apply Coulomb’s Law to find out what happens if there are other
charges present.
The distribution of charge on the surface may also be affected by the presence
of other charged objects near the sphere. For example, if a negatively charged
object is brought near a positively charged sphere, the negative charges on the
sphere will be repelled and will move to the opposite side of the sphere. This
will result in a non-uniform distribution of charge on the surface of the sphere.
Therefore, the correct answer is 1) The charge resides on the surface only; the
distribution of charge on the surface depends on what other charged objects are
near the sphere.
Table 9: Few-shot demonstration exemplar for MMLU high-school Physics.
17



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 28):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question Two spheres of net charge +5e and -6e briefly come into contact. Afterward,
which of the following is a possible combination of net charges for the two
spheres?
0) +3e and -4e
1) +1.5e and -2.5e
2) Both of the above
3) None of the above
Principles Law of conservation of charge: the total charge in a closed system remains
constant.
Coulomb’s Law: the mutual repulsion of like charges demands that the charges
be as far apart as possible.
Answer Using the Principles of Conservation of charge, we can solve the problem as
following:
Step 1: Apply Conservation of charge to find out the possible combination of
net charges for the two spheres.
The total charge of the two spheres before they come into contact is +5e - 6e =
-1e. After they come into contact, the total charge of the two spheres must still
be -1e.
Step 2: Find out the possible combination of net charges for the two spheres.
The possible combination of net charges for the two spheres are:
(1) +3e and -4e
(2) +1.5e and -2.5e
Therefore, the correct answer is 2) Both of the above.
Table 25: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Principle Error. In this case, Coulomb’s Law is not needed for solving this problem. Instead, the
Charge Quantization Principle is missing from the retrieved principles, and is in fact needed to rule
out option (2) of fractional charges.
29



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 30):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question Physics students are checking the constant acceleration equations of kinematics
by measuring the velocity of a tennis ball that is dropped and falls 6 meters
and then passes through a photogate. The predicted velocity is 20% above the
velocity measured by the photogate. Which of the following best describes the
cause of the large percent difference?
0) The ball changes its shape while falling.
1) The acceleration of gravity varies as the ball is falling.
2) Air resistance increases the acceleration of the ball.
3) The acceleration of the balls varies with the velocity.
Principles Vertical motion: y = 0.5 * g * t2, where y is the vertical displacement, g is the
acceleration due to gravity, t is the time.
Free fall: is a special case of motion with constant acceleration, in which the
acceleration is due to gravity.
Acceleration to Velocity Formula: v = u + a * t, where v is the final speed, u is
the initial velocity, a is the acceleration, and t is the time.
Answer Using the Principles of Vertical motion and Horizontal motion, we can solve
the problem as following:
Step 1: Apply the Vertical motion equation to find the time it takes for the ball
to fall 6 meters.
y = 0.5 * g * t2
6 m = 0.5 * 9.8 m/s2 * t2
t = 1.12 s
Step 2: Apply the Horizontal motion equation to find the velocity of the ball
when it passes through the photogate.
x = v * t
v = x / t
v = 6 m / 1.12 s
v = 5.36 m/s
Step 3: Compare the predicted velocity with the velocity measured by the
photogate. The predicted velocity is 20% above the velocity measured by the
photogate, so the predicted velocity is 1.2 * 5.36 m/s = 6.43 m/s.
Step 4: Find the cause of the large percent difference. The predicted velocity is
6.43 m/s, but the velocity measured by the photogate is 5.36 m/s. The difference
is 1.07 m/s. This difference is caused by air resistance. Air resistance is a force
that opposes the motion of an object through the air. It is caused by the friction
between the object and the air molecules. The greater the surface area of the
object, the greater the air resistance. The tennis ball has a large surface area, so
it experiences a lot of air resistance. This air resistance slows down the tennis
ball and causes the difference between the predicted velocity and the velocity
measured by the photogate.
Therefore, the correct answer is 0) The ball changes its shape while falling.
Table 27: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Reasoning Error. In this case, the model was able to correctly identify air resistance is the cause of
the velocity difference, but made the error at the final reasoning step of deriving the right answer.
31



### Claim 122/179

#### Claim Text
Dynamic equilibrium analysis In this section, by considering different selection intensities, we analyze the dynamic equilibrium , based on imitation dynamics.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 5):

Language Models as Semantic Indexers
… Codebook𝑬… Codebook𝑬
…
Deep TransEncoderDeep TransDecoder
Semantic IndexerSemantic Encoder
User history
Document IDs… Codebook𝑬… Codebook𝑬
…
Deep TransEncoderDeep TransDecoder
Semantic IndexerSemantic Encoder
Item IDs
query
(a) Recommendation(b) Retrieval
Figure 2.LMI NDEXER can be fine-tuned on downstream tasks including recommendation (user history as input and item ID as output)
and retrieval (query as input and document ID as output).
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08
Probability
Sets & Kits
Maternity
Eyes
Hair Color
Lips
Face
Body
Styling Products
Cleansers
Conditioners
Scrubs & Body Treatments
Bath
Bags & Cases
Makeup Brushes & Tools
Makeup Sets
Nails
Nail Tools
Hands & Nails
Category
Probabilities by Category and Semantic ID
Semantic ID
(0, *, *)
(1, *, *)
(2, *, *)
(3, *, *)
(a) The ground-truth category distribution for items in the
Amazon-Beauty dataset is colored by the value of first ID c1.
0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040
Probability
Cosmetic Bags
Nail Art
Nail Art Equipment
Nail Brushes
Nail Files & Buffers
Nail Polish
Nail Treatments
Refillable Containers
Sets & Kits
Top & Base Coats
Category
Probabilities by Category and Semantic ID
Semantic ID
(0, 0, *)
(0, 1, *)
(0, 2, *)
(0, 3, *)
(0, 4, *)
(0, 5, *)
(0, 6, *)
(0, 7, *)
(0, 8, *)
(0, 9, *)
(0, 10, *)
(0, 11, *)
(0, 12, *)
(0, 13, *)
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14
Probability
Body Scrubs
Body Washes
Bubble Bath
Cleansers
Creams & Moisturizers
Creams, Gels & Lotions
Exfoliators & Scrubs
Moisturizers
Soaps
Treatments & Masks
Category
Probabilities by Category and Semantic ID
Semantic ID
(1, 0, *)
(1, 1, *)
(1, 2, *)
(1, 3, *)
(1, 4, *)
(1, 5, *)
(1, 6, *)
(1, 7, *)
(1, 8, *)
(1, 9, *)
(1, 10, *)
(1, 11, *)
(1, 12, *)
(1, 13, *)
(1, 14, *)
(1, 15, *)
(1, 16, *)
(1, 17, *)
(1, 18, *)
0.00 0.05 0.10 0.15 0.20 0.25
Probability
Blush
Chemical Hair Dyes
Concealers & Neutralizers
Eye Shadow
Foundation
Liner & Shadow Combinations
Lip Glosses
Lip Stains
Lipstick
Lipstick Primers
Mascara
Nail Polish
Powder
Category
Probabilities by Category and Semantic ID
Semantic ID
(2, 0, *)
(2, 1, *)
(2, 2, *)
(2, 3, *)
(2, 4, *)
(2, 5, *)
(2, 6, *)
(2, 7, *)
(2, 8, *)
(2, 9, *)
(2, 10, *)
(2, 11, *)
(2, 12, *)
(2, 13, *)
(2, 14, *)
(2, 15, *)
(2, 16, *)
(2, 17, *)
(2, 18, *)
(2, 19, *)
(2, 20, *)
0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200
Probability
Combinations
Creams
Creams & Moisturizers
Dark Circle Treatments
Fillers
Moisturizers
Oils & Serums
Puffiness Treatments
Sets & Kits
Treatments & Masks
Category
Probabilities by Category and Semantic ID
Semantic ID
(3, 0, *)
(3, 1, *)
(3, 2, *)
(3, 3, *)
(3, 4, *)
(3, 5, *)
(3, 6, *)
(3, 7, *)
(3, 8, *)
(3, 9, *)
(3, 10, *)
(3, 11, *)
(3, 12, *)
(3, 13, *)
(3, 14, *)
(3, 15, *)
(3, 16, *)
(3, 17, *)
(3, 18, *)
(b) The category distributions for items having the Semantic ID as
(c1, ˚, ˚), where c1 P{0, 1, 2, 3}. The categories are colored based
on the second semantic token c2.
Figure 3.Semantic ID qualitative study on Amazon-Beauty.
Qualitative Results. We conduct a detailed study on the
quality of the learned semantic IDs from LMI NDEXER on
Amazon-Beauty dataset. For each product d in the dataset,
its learned semantic ID is represented as cd “c1
dc2
dc3
d. We
randomly select four c1
d values (i.e., 0, 1, 2, 3) and analyze
the products whose c1
d Pt0, 1, 2, 3u. The results are shown
in Figure 3. In Figure 3(a), we summarize each item’s
category using c1 to visualize c1-specific categories in the
overall category distribution of the dataset. As shown in Fig-
ure 3(a), c1 captures the coarse-grained category of the item.
For example, c1 “1 contains most of the products related
to “Bath”. Similarly, the majority of items with c1 “0 are
“Tool” and “Make-up” products for nails. We also visualize
the hierarchical structure of LMI NDEXER learned Semantic
IDs by fixing c1 and visualizing the category distribution
for all possible values of c2 in Figure 3(b). We again found
that the second ID c2 further categorizes the coarse-grained
semantics captured with c1 into fine-grained categories.
4.3. Training Study
In this section, we study the optimization process (recon-
structor collapse, posterior collapse, and contrastive loss
discussed in Sec 3.2) of our semantic indexer from two
Table 1.ID quantitative study (AMI) on Amazon datasets.
Model Beauty Sports Toys
rq-V AE indexer (BERT) 0.2654 0.2774 0.3154
HC indexer (BERT) 0.2428 0.2387 0.2729
rq-V AE indexer (In-domain SimCSE) 0.3100 0.2695 0.3126
HC indexer (In-domain SimCSE) 0.2771 0.2622 0.2968
LMINDEXER 0.3563 0.4163 0.3536
perspectives: reconstruction quality and semantic ID diver-
sity. We serve token reconstruction Macro-F1 (Opitz &
Burst, 2019) and semantic ID perplexity of all the docu-
ments (Horgan, 1995) as the main evaluation metrics. A
high-quality semantic indexer should contribute to a high
reconstruction quality (high Macro-F1) and a high semantic
ID diversity (high perplexity). We conduct model studies on
Amazon-sports shown in Figure 4 and have the following
findings: 1) Reconstructor collapse: The reconstruction
Macro-F1 is low without reconstructor warm-up, shown in
Figure 4(a). In this case, the reconstructor suffers from low
reconstruction capability and cannot provide meaningful
signals to train the semantic indexer. 2) Posterior collapse:
The semantic ID perplexity is low without semantic encoder
and codebook warm-up, shown in Figure 4(b). This indi-
6



Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 14):

Language Models as Semantic Indexers
0 10 20 30 40 50
#Doc in each ID
0.0
0.1
0.2
0.3
0.4Density
(a) Beauty
0 10 20 30 40 50 60 70 80
#Doc in each ID
0.00
0.05
0.10
0.15
0.20
0.25
0.30Density (b) Sports
0 5 10 15 20 25 30 35
#Doc in each ID
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7Density (c) Toys
Figure 6.Duplication study of LMI NDEXER ’s semantic IDs.
00.0050.010.0150.020.0250.030.0350.040.045
1 2 3
Recall@5
ID length
(a) beauty
00.0050.010.0150.020.025
1 2 3
Recall@5
ID length (b) sports
00.0050.010.0150.020.0250.030.0350.040.045
1 2 3
Recall@5
ID length (c) toys
Figure 7.Semantic ID length study on recommendation.
to unseen documents, demonstrating its strong semantic
capturing capability.
Table 15.Zero-shot study.
Model Recall@50 Recall@100
rq-V AE indexer 0.0000 0.0105
HC indexer 0.0000 0.0070
LMINDEXER 0.0455 0.0524
15



Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 6):

Language Models as Semantic Indexers
00.050.10.150.20.250.3
0 20004000
Macro-F1
Step
no recon warmuprecon warmup
(a) RC: Macro-F1
0100200300400500
0 1000200030004000
Perplexity
Step
no codebook warmupcodebook warmup (b) PC: perplexity
00.050.10.150.20.25
0 1000200030004000
Macro-F1
Step
no codebook warmupcodebook warmup (c) PC: Macro-F1
050100150200
0 50001000015000
Perplexity
Step
no contrastivecontrastive (d) CL: perplexity
00.20.40.60.81
0 50001000015000
Difference RatioStep
no contrastivecontrastive (e) CL: diff ratio
Figure 4.Semantic indexer training analysis on Amazon-sports. x-axis denotes the training step and y-axis denotes the evaluation metrics.
Reconstructor collapse analysis (a): The reconstructor suffers from low reconstruction Macro-F1 without reconstructor warm-up (blue).
Posterior collapse analysis (b,c): The semantic indexer suffers from generating homogeneous IDs (low perplexity), and results in low
reconstruction Macro-F1, without encoder and codebook warm-up (blue). Contrastive learning analysis (d,e): Documents sharing prefix
ID tend to have similar next position ID (low diff ratio) and low diversity (low perplexity) without contrastive objective (blue).
Table 2.Ablation study of commitment loss.
Dataset Sports Toys Beauty
w. commitment loss 305.39 280.30 287.01
w/o commitment loss 147.10 211.60 261.04
cates that the semantic indexer fails to provide diverse and
meaningful signals to the reconstructor and thus results in
low reconstruction macro-F1 in Figure 4(c). 3) Contrastive
loss: We propose the contrastive loss in Section 3.2 to push
documents sharing the same semantic ID prefix to obtain
different IDs at the current step. We show the effectiveness
of this design in Figure 4(d)(e). Difference ratio (diff ratio)
refers to the ratio of # (document pairs sharing the same
ID prefix but having different current step IDs) / # (docu-
ment pairs sharing the same ID prefix). From the result, we
can find that the difference ratio is high with contrastive
loss, which makes the semantic IDs more distinguishable
for different documents.
4.4. Commitment Loss Ablation Study
We conduct experiments on Amazon datasets and measure
the perplexity of the previously learned IDs when the model
is trained to learn the next new ID with and without com-
mitment loss, shown in Table 2. From the results, we can
find that without commitment loss, the perplexity of the pre-
viously learned IDs will decrease, which indicates that the
previously learned IDs are forgotten by the model when it is
trained to learn the next new ID. This observation highlights
the necessity of commitment loss to prevent catastrophic
forgetting.
5. Experiments: Downstream Tasks
5.1. Sequential Recommendation
Task Definitions. Given the historical data of user u’s
interacted items Iu, the task is to predict which next item v
the user will interact with in the future.
Datasets. We conduct experiments on three domains from
Amazon review dataset (He & McAuley, 2016): Amazon-
Beauty, Amazon-Sports, and Amazon-Toys. We keep the
users and items that have at least 5 interactions in their
history in the Amazon review dataset. We treat the last
interacted item by each user as the testing sample, the last
second interacted item as the validation sample, and the
previous items as training samples. The statistics of the
datasets can be found in Appendix Table 7.
Baselines. We compare our method with both popular se-
quential recommendation models including HGN (Ma et al.,
2019), GRU4Rec (Hidasi et al., 2016), BERT4Rec (Sun
et al., 2019) and FDSA (Zhang et al., 2019), as well as
generative recommendation methods with semantic IDs (Ra-
jput et al., 2023; Tay et al., 2022): rq-V AE indexer and
hierarchical clustering (HC) indexer.
Implementation Details. For generative recommendation
methods (rq-V AE indexer, hierarchical clustering indexer,
and LMI NDEXER ), we concatenate the textual information
(title & description) of the user’s previously interacted items,
serve it as the input text into the generative language model
and ask the model to generate the ID for next item. The
baselines are using the same T5-base checkpoint. We train
all the compared generative recommendation methods for
10,000 steps with the learning rate searched in {1e-2, 1e-3,
1e-4}. The batch size is set to 32, the maximum input text
length is set to 1024 and all experiments are run on an 8
A100 40G machine. The number of beams for beam search
is set to 20.
Main Result. The performance comparisons of different
methods are shown in Table 3. From the results, we can
find that: 1) LMI NDEXER performs consistently better than
all the baseline methods on all datasets. 2) Although other
generative recommendation methods employing semantic
IDs share a similar encoding approach with LMI NDEXER ,
their performance is hampered by limitations in the quality
of their semantic indexers and item IDs.
Semantic ID Length & Codebook Size Study. In this
section, we analyze how the length of the semantic IDs
and the size of the codebook affect the downstream recom-
mendation performance. We conduct experiments with the
7



Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 11):

Language Models as Semantic Indexers
Table 7.Dataset Statistics
Dataset # Items # Users # Rec history (train/dev/test) # Search query (train/dev/test) # Search labels (train/dev/test)
Amazon-Beauty 12,101 22,363 111,815 / 22,363 / 22,363 1,049 / 150 / 338 1,907 / 268 / 582
Amazon-Sports 18,357 35,598 177,990 / 35,598 / 35,598 1,299 / 186 / 443 2,209 / 311 / 764
Amazon-Toys 11,924 19,412 97,060 / 19,412 / 19,412 1,010 / 145 / 351 1,653 / 250 / 594
Table 8.Dataset Statistics
Dataset # Documents # Query (train/test) # Search labels (train/test)
NQ320k 109,739 307,373 / 7,830 307,373 / 7,830
MACRO 1M 1,000,000 502,939 / 6,980 532,751 / 7437
TREC-DL 1M 1,000,000 502,939 / 93 532,751 / 1,069
Algorithm 1 Self-supervised ID Learning Procedure of
LMI NDEXER
1: Input: The document corpus tdu.
2: Output: The semantic IDs tcduof the documents tdu.
A semantic indexer SemIndexerp¨qwhich contains a
semantic encoder SemEncθp¨qand codebooks tEtut.
A reconstruction model Reconϕp¨q.
3: Begin
4: // initialize semantic encoder
5: SemEncθp¨qÐ T5-base;
6: // reconstruction warm up
7: minϕ L0
recon “´ ř
d
ř
wPdzd0
h
log Preconpw|d0
h q;
8: for t “1, . . . , Tdo
9: // semantic encoder & codebook warm up
10: ht ÐSemEncθpd, cdq;
11: zw ÐReconϕpq “tct
d, ht
du, k“dt
hq;
12: minϕ,ϕc Lt “Lt
recon `Lt
contrastive `Lt
commitment;
13: ht
d ÐSemEncθpd, ct
dq;
14: Et ÐKMeanspht
dq;
15: // whole framework training
16: zw ÐReconϕpq “tct
d, ct
du, k“dh, v“dhq;
17: minϕ,ϕc Lt “Lt
recon `Lt
contrastive `Lt
commitment;
18: ct
d Ðargmaxjpspct
d “j|cd, dq;
19: end for
20: Return tcdu, SemIndexerp¨q;
21: End
rate in {1e-3, 2e-3, 5e-3}. The training epochs are set to be
30, 10, and 5 for Amazon datasets, NQ, and MS MACRO
respectively. The hyper-parameter configuration for self-
supervised semantic indexer training can be found in Table
9.
In the downstream recommendation task, for generative
recommendation methods with semantic IDs (rq-V AE in-
dexer, hierarchical clustering indexer, and LMI NDEXER ),
we concatenate the textual information (title & description)
of the user’s previously interacted items, serve it as the input
text into the generative language model and ask the model
to generate the ID for next item. The baselines are using
the same T5-base checkpoint. We train all the compared
generative recommendation methods for 10,000 steps with
the learning rate searched in {1e-2, 1e-3, 1e-4}. The batch
size is set to be 32, the maximum input text length is set
to be 1024 and all experiments are run on an 8 A100 40G
machine. The number of beams for beam search is set to
20. The hyper-parameter configuration for generative rec-
ommendation training can be found in Table 10.
In the downstream product search task, for generative re-
trieval methods with semantic IDs (rq-V AE indexer, hierar-
chical clustering indexer, and LMI NDEXER ), we serve the
query as the input text into the generative language model
and ask the model to generate the ID for the relevant items.
All baselines initially load the same T5-base checkpoint.
We train all the compared generative retrieval methods for
10,000 steps with the learning rate searched in {1e-2, 1e-
3, 1e-4}. The batch size is set to 32, the maximum input
text length is set to be 1024 and all experiments are run on
an 8 A100 40G machine. The number of beams for beam
search is set to 20. The hyper-parameter configuration for
generative product search training can be found in Table 11.
In the downstream document retrieval task, for generative
retrieval methods with semantic IDs (rq-V AE indexer, hier-
archical clustering indexer, and LMI NDEXER ), we serve the
query as the input text into the semantic indexer and ask the
model to generate the ID for the relevant documents. Fol-
lowing (Wang et al., 2022), we use docT5query (Nogueira
12



Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 13):

Language Models as Semantic Indexers
Table 11.Hyper-parameter configuration for generative product search.
Parameter Amazon-Beauty Amazon-Sports Amazon-Toys
Optimizer Adam Adam Adam
Adamϵ 1e-6 1e-6 1e-6
Adampβ1, β2q (0.9, 0.999) (0.9, 0.999) (0.9, 0.999)
Batch size 32 32 32
Max steps 10,000 10,000 10,000
Max sequence length 1024 1024 1024
Bean size 20 20 20
Learning rate searched in {1e-2, 1e-3, 1e-4}
Backbone LM T5-base
Table 12.Hyper-parameter configuration for generative retrieval.
Parameter NQ MACRO-1M
Optimizer Adam Adam
Adamϵ 1e-6 1e-6
Adampβ1, β2q (0.9, 0.999) (0.9, 0.999)
Batch size 2,048 2,048
Max steps 250,000 500,000
Max sequence length 32 32
Bean size 20 20
Learning rate searched in{5e-4, 1e-3, 5e-3}
Backbone LM T5-base
first two IDs că2 “c1c2 (20 pairs for each method). 2) We
ask four trained annotators to evaluate if the two products
in each pair are semantically related to each other. 3) We
finally calculate the accuracy of each method. The results
are shown in Figure 13. From the result, our LMIndexer can
outperform baseline methods by a large margin.
Table 13.Human evaluation of semantic ID quality.
Model Accuracy
rq-V AE indexer 0.7375
HC indexer 0.5375
LMINDEXER 0.7750
A.7. Semantic ID Length Study
In this section, we analyze how the length of the semantic
IDs affects the downstream recommendation performance.
We conduct experiments with the length of item semantic
IDs to be 1, 2, and 3. The results on the Amazon-Beauty,
Amazon-Sports, and Amazon-Toys datasets are shown in
Figure 7. From the result, we can find that the model perfor-
mance increases as the semantic ID length increases. The
result is intuitive, since the longer the semantic ID is, the
more semantic information it can contain.
A.8. Codebook Size Study
The codebook size is set as a hyperparameter in our model
design. We conduct experiments on Amazon-Beatuy dataset
to study how codebook size will influence the quality of
the learned semantic indexer LMI NDEXER . The results are
shown in Figure 8. From the result, we can find that the
downstream task performance increases as codebook size
increases. It is intuitive, since the larger the codebooks are,
the more information they can contain.
A.9. Latency Analysis
We conduct latency analysis to compare the time cost of
search inference for different methods on Amazon-Beauty
dataset. We measure the total latency of product search on
the whole Amazon-Beauty test set. The results are shown
in Table 14. From the result, the inference latency of our
method is comparable with rq-V AE indexer and HC indexer
and is much smaller than SEAL.
Table 14.Latency analysis.
Model Latency
rq-V AE indexer 13.66s
HC indexer 12.85s
SEAL 21min
LMINDEXER 12.21s
A.10. Zero-Shot Study
We conduct zero-shot product search experiments on the
Amazon beauty domain to test if the semantic indexer fine-
tuned on downstream tasks can generalize to items that are
not seen during semantic index self-supervised training and
downstream finetuning. The results are shown in Table 15.
From the results, we can find that compared with other se-
mantic indexer methods, LMI NDEXER can generalize better
14



### Claim 123/179

#### Claim Text
Bao et al. have proposed such an ansatz, which, however, requires the solution of additional global Poisson problems at each time step.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 31):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: What happens to the pressure, P, of an ideal gas if
the temperature is increased by a factor of 2 and the volume is increased
by a factor of 8 ?
0) P decreases by a factor of 16.
1) P decreases by a factor of 4.
2) P decreases by a factor of 2.
3) P increases by a factor of 4.
Original Answer [Incorrect]: The correct answer is 0. The ideal gas
law is PV = nRT. If the temperature is increased by a factor of 2 and the
volume is increased by a factor of 8, then the pressure will decrease by a
factor of 16.
Stepback Question: What are the physics principles behind this
question?
Principles:
Ideal gas law: PV = nRT, where P is the pressure, V is the volume, n is
the number of moles, R is the gas constant, and T is the temperature.
Final Answer: Using the Principles of Ideal gas law, we can solve the
problem as following:
Step 1: Apply Ideal gas law to find out the pressure of an ideal gas. The
ideal gas law states that PV = nRT, where P is the pressure, V is the
volume, n is the number of moles, R is the gas constant, and T is the
temperature.
Step 2: Apply Ideal gas law to find out the pressure of an ideal gas when
the temperature is increased by a factor of 2 and the volume is increased
by a factor of 8. If the temperature is increased by a factor of 2, then T be-
comes 2T. If the volume is increased by a factor of 8, then V becomes 8V .
Substituting these values into the ideal gas law, we get: P(8V) = nR(2T)
Dividing both sides by 8V , we get: P = nR(2T) / 8V
We can see that the pressure has decreased by a factor of 4.
Therefore, the correct answer is 1) P decreases by a factor of 4.
Table 28: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
32



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 32):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: A liquid flows at a constant flow rate through a pipe
with circular cross-sections of varying diameters. At one point in the
pipe, the diameter is 2 cm and the flow speed is 18 m/s. What is the flow
speed at another point in this pipe, where the diameter is 3 cm?
0) 4 m/s
1) 6 m/s
2) 8 m/s
3) 12 m/s
Original Answer [Incorrect]: The answer is 3) 12 m/s A step-by-step
solution is below: The flow rate is constant, so the product of the speed
and the cross-sectional area is constant. The cross-sectional area is
proportional to the square of the diameter, so the speed is inversely
proportional to the square of the diameter. The ratio of the speeds is the
inverse of the ratio of the squares of the diameters, so the speed at the
second point is 18m/s ∗ (2cm/3cm)2 = 12 m/s.
Stepback Question: What are the physics principles behind this
question?
Principles:
Conservation of Mass: the amount of mass flowing through the pipe at
two different points must be equal, or A1 * v1 = A2 * v2, where A1 is
the area of cross-section one, v1 is the velocity at point 1, A2 is the area
of cross-section two, v2 is the velocity at point 2.
Area of Circle: A = π ∗ r2, where r is the radius of the circle.
Final Answer: Using the Principles of Continuity equation, we can
solve the problem as following:
Step 1: Apply the Continuity equation to find out the flow speed at
another point in this pipe. The continuity equation states that the product
of the area and the velocity is constant at all points in a pipe. In this
problem, we are given the following information:
A1 = π ∗ (2cm)2 = 4πcm2
v1 = 18 m/s
A2 = π ∗ (3cm)2 = 9πcm2
We can use the continuity equation to find v2:
A1 * v1 = A2 * v2
4πcm2 ∗ 18m/s = 9πcm2 ∗ v2
v2 = 8 m/s
Therefore, the flow speed at another point in this pipe is 8 m/s.
The correct answer is 2) 8 m/s.
Table 29: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
33



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 28):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question Two spheres of net charge +5e and -6e briefly come into contact. Afterward,
which of the following is a possible combination of net charges for the two
spheres?
0) +3e and -4e
1) +1.5e and -2.5e
2) Both of the above
3) None of the above
Principles Law of conservation of charge: the total charge in a closed system remains
constant.
Coulomb’s Law: the mutual repulsion of like charges demands that the charges
be as far apart as possible.
Answer Using the Principles of Conservation of charge, we can solve the problem as
following:
Step 1: Apply Conservation of charge to find out the possible combination of
net charges for the two spheres.
The total charge of the two spheres before they come into contact is +5e - 6e =
-1e. After they come into contact, the total charge of the two spheres must still
be -1e.
Step 2: Find out the possible combination of net charges for the two spheres.
The possible combination of net charges for the two spheres are:
(1) +3e and -4e
(2) +1.5e and -2.5e
Therefore, the correct answer is 2) Both of the above.
Table 25: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Principle Error. In this case, Coulomb’s Law is not needed for solving this problem. Instead, the
Charge Quantization Principle is missing from the retrieved principles, and is in fact needed to rule
out option (2) of fractional charges.
29



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 22):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset Question Answer
MMLU
Physics
A point charge, Q1 = +1 mC, is
fixed at the origin. How much
work is required to move another
charge, Q2 = +8 µC, from the
point (0, 4 meters) to the point
(3 meters, 0)?
0) 3.5 J
1) 6.0 J
2) 22.5 J
3) 40 J
Sure, let’s think step by step.
1. Calculate the electric potential energy of Q2
at the initial position. The electric potential
energy (U) of a point charge Q2 at a distance
r from another point charge Q1 is given by the
following equation: U = k * Q1 * Q2 / r, where k
is Coulomb’s constant.
Therefore, the electric potential energy of Q2 at
the initial position is:
Uinitial = (9 ×109Nm2/C2) ∗(1 ×10−3C) ∗(8 ×
10−6C)/4m = 18 J
2. Calculate the electric potential energy of Q2 at
the final position. The electric potential energy of
Q2 at the final position is:
Ufinal = (9 × 109Nm2/C2) ∗ (1 × 10−3C) ∗ (8 ×
10−6C)/3m = 24 J
3. Calculate the work required to move Q2 from
the initial position to the final position. The work
(W) required to move a charge from one position
to another is equal to the change in electric
potential energy (∆U) between the two positions.
W = ∆U = Ufinal - Uinitial = 24 J - 18 J = 6 J
Therefore, the answer is 1) 6.0 J.
Table 18: Demonstration exemplar for CoT prompting for MMLU Physics.
23



### Claim 124/179

#### Claim Text
The mobility and the imitation dynamics between different cities could significantly impact vaccination decisions and epidemic dynamics , .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 29):

Jiawei Zhang
Table 8: Analysis of generated API query cases different from ground-truth queries in the testing set.
Input
Sources
Language Generation Examples
Input Payload Generated Outputs Before Fine-Tuning Generated Outputs After Fine-Tuning
Pile
Data
Roman Catholic Diocese of Tambacounda\n\nThe Ro-
man Catholic Diocese of Tambacounda () is a diocese
located in the city of Tambacounda in the Ecclesiastical
province of Dakar in Senegal.
\n\nHistory\n\nThe Diocese of Tambacounda
was erected by Pope Pius XI on December 21,
1933, on territory split off from the Diocese of
Saint-Louis du Sénégal. \n\nBishops\n\nDiocese
of Tambacounda \nErected: December 21,
1933\nMetropolitan: Archdiocese of Dakar\n
center>The Roman Catholic Diocese of Tambacounda
() is a diocese located in the city of Tambacounda in
the <center>Ecclesiastical province of Dakar in Sene-
gal.</center> center>Output: </center> center>The
<center>Roman</center> <center>Catholic
Mud Hens pitcher Evan Reed charged with sexual
assault\n\nMud Hens pitcher Evan Reed was charged
July 30 with sexual assault related to a March incident
in Detroit when he was a member of the Detroit Tigers.
\n\nMud Hens pitcher Evan Reed was charged July 30
with sexual assault related to a March incident in Detroit
when he was a member of the Detroit Tigers.\n\nMud
Hens pitcher Evan Reed was charged July 30 with sexual
assault related to a March incident in Detroit when he
was a member of the Detroit Tigers.\n\nMud Hens
pitcher Evan Reed was charged July 30 with sexual
assault related to
center>###<centercentercentercentercentercentercenter
centercentercentercentercentercentercentercenter
centercentercentercentercentercentercentercenter
centercentercentercentercentercentercentercenter
centercentercentercentercenter
Random
Sources
Thursday’s verdict only added to the intrigue surround-
ing the gravest legal and political unknown from the
2021 Capitol insurrection that hangs over the 2024 cam-
paign: will Donald Trump, the president who inspired
the uprising, face his own legal and political price?
I don’t think there’s any question that he’s going to be
impeached, ” Rep. Al Green (D-TX) told The Daily Beast.
“I think he’s going to be impeached, and I think he’s
going to be removed from office. ”\n\nGreen’s
will be the verdict only added to the intrigue surround-
ing the gravest legal and political unknown from the
2021 Capitol insurrection that hangs over the 2024 cam-
paign: will Donald Trump, the president who inspired
the uprising, face his own legal and political price?br>
A source of debate for centuries, the village of Ponte
Buriano, a suburb of Arezzo in the Tuscany region of
Italy, is so convinced the bridge behind Mona Lisa is
the Ponte Buriano that they’ve made it a key feature of
their local tourism campaign, even laying claim to the
bridge on the village’s welcome sign.
\n\nThe Ponte Buriano is one of the oldest bridges in
Tuscany, dating back to the 12th century. It’s also one of
the most famous, thanks to Leonardo da Vinci’s Mona
Lisa, the most famous painting in the world.\n\nThe
The village of [GR(GL("mona_lisa"), "bpr:search",
GR(GL("ponte_buriano"), GR(GL("search"),
GR(GL("mona_lisa"), GR(GL("search
•Efficiency: With LoRA and quantized models/optimizers,
we can reduce the model fine-tuning memory capacity re-
quirement to less than 11GB and the memory capacity re-
quirement even lower for the model inference stage. Mean-
while, integrated with the large-sized graph data, pre-trained
graph models, and necessary pre-processed data, the effi-
ciency of Graph-ToolFormer for various graph reasoning
task can still be a problem. In this paper, we introduce a
tentative approach to make the problem less severe with
the working memory. However, if we plan to deployGraph-
ToolFormer on devices with very small memories, like cell-
phones or embedded equipments, new techniques will still
be needed to improve the model learning and inference effi-
ciency.
•Diverse Applications : Due to the limited space, we can
only study a few number of the graph reasoning tasks with
Graph-ToolFormer in this paper. Meanwhile, in the real-
world, we have lots of graph structured data that may require
the LLMs to handle them to reason for the desired outputs.
Therefore, a very promising future work direction is to apply
Graph-ToolFormer to study diverse real-world graph/net-
work data oriented reasoning tasks with LLMs. We list a
few of them here just for the readers’ information, and the
readers may explore more diverse reasoning tasks according
to your own backgrounds and expertises.
– Urban Computing and Smart City : In the offline world,
we have extensively connected traffic networks that bridge
different local communities, cities and countries by lo-
cal roads, national highways, international fights and
ocean freight corridors. Applying LLMs for knowledge
extraction and reasoning based on such traffic networks
is critical for the current urban computing and smart city
projects.
– IoT and Smart Home : Assisted with the 5G, the IoT net-
work effectively bridges the cyber world with the physical
devices and equipments together via extremely fast com-
munication channels. The LLMs provide the opportunity
for us to utilize language models as the general interface
for controlling the devices within the IoT networks, which
is also the main objective for building the smart home
system.
– Healthcare : During the past years, the world has suf-
fered a lot from the covid-19 pandemic. Similar to the
protein molecules studied in this paper, both the virus
and the vaccines can also be represented as the molecular
graphs. LLMs with the molecular graph reasoning ability
have the potential to improve our current healthcare sys-
tem in many perspectives, like early identification of virus ,
analysis of the virus pathogenicity and creation of vaccines .
What’s more, the LLMs with the social network reasoning
ability will also help infer the potential virus propagation
among people , early prediction of highly infectious commu-
nities and identify rumors and misinformation about the
pandemic (at the online social networks).



Source: data\tc16_2312.10997v5\referenced_papers\[20]_2303.08518.pdf (Page 13):

Appendices
A Task Clustering
We use the following datasets for each task cluster.
• Reading Comprehension : SQuADv1 (Ra-
jpurkar et al., 2016), BoolQ (Clark et al., 2019),
MultiRC (Khashabi et al., 2018), and OBQA (Mi-
haylov et al., 2018).
• Closed-book QA: ARC-c/e (Bhakthavatsalam
et al., 2021) and NQ (Kwiatkowski et al., 2019).
• Paraphrase Detection : MRPC (Dolan and
Brockett, 2005), QQP (Wang et al., 2019), and
Paws Wiki (Zhang et al., 2019).
• Natural Language Inference : MNLI-
m/mm (Williams et al., 2018), QNLI (Rajpurkar
et al., 2018), SNLI (Bowman et al., 2015), and
RTE (Bentivogli et al., 2009).
• Sentiment Analysis: SST-2 (Socher et al., 2013),
Yelp (Zhang et al., 2015), and Sentiment140 (Go
et al., 2009).
• Commonsense Reasoning: COPA (Roemmele
et al., 2011), HellaSwag (Zellers et al., 2019),
and PIQA (Bisk et al., 2020).
• Coreferenece Resolution : Winogrande (Sak-
aguchi et al., 2020), DPR (Rahman and Ng,
2012), and WSC273 (Levesque et al., 2012).
• Structure to Text : CommonGen (Lin et al.,
2020), E2ENLG (Dusek et al., 2019), and
DART (Nan et al., 2021).
• Summarization: AESLC (Zhang and Tetreault,
2019), AGNews (Zhang et al., 2015), and Giga-
word (Napoles et al., 2012).
B Tuning Details
Hyperparameter Assignment
Computing Infrastructure 8 V100-32GB GPUs
Number of epochs 3
Run-time 36 Hours
Batch size per GPU 2
Maximum sequence length 256
Maximum learning rate 1e-5
Optimizer Adam
Adam epsilon 1e-8
Adam beta weights 0.9, 0.999
Learning rate scheduler warmup linear
Weight decay 0.0
Warm-up steps 1000
Learning rate decay linear
Table 5: Hyperparameter settings of tuning a prompt
retriever
C Hallucination Mitigation of ChatGPT
We evaluateChatGPT’s performance using its re-
leased API, with the gpt-3.5-turbo-0301 model and
a temperature of 0. Human evaluation is conducted
to check the accuracy on sampled test examples
from each dataset, including 66 from TruthfulQA
to follow Bang et al. (2023), 100 from FEVER2.0,
and 100 from the scientific subset of Covid-19.
As types of these tasks have no overlap with the
types we listed in Appendix A, we use the retriever
trained with all the listed task types for the cross-
task and cross-model evaluation.
On the Covid-19 dataset, which requires a
true/false answer to input claims, we observe
vanilla zero-shot prompting often leads to ambigu-
ous answers, which we consider the answer incor-
rect. However, by concatenating retrieved prompts,
primarily focused on natural language inference,
ChatGPT generates the correct answer. An ex-
ample is shown in Figure 9. This suggests the
model has access to necessary information but
struggles with making inferences, possibly due to
RLHF (Christiano et al., 2017) training or inherent
difficulty.
OnJuly82020,thefollowingclaimwasmade:ThereisapossibilityforsomeanimalstobecomeinfectedCOVID-19throughclosecontactwithinfectedhumans.Q:Wasthisclaimtrueorfalse?
ScientistsattheGenomeInstituteofSingapore(GIS)havediscoveredthecompletegeneticsequenceofacoronavirusisolatedfromaSingaporepatientwithSARS.Canweinferthefollowing?SingaporescientistsrevealthatSARSvirushasundergonegeneticchanges.Yes,No,orMaybe?MaybeOnJuly82020,thefollowingclaimwasmade:ThereisapossibilityforsomeanimalstobecomeinfectedCOVID-19throughclosecontactwithinfectedhumans.Q:Wasthisclaimtrueorfalse?
0-SHOT
UPRISE
AsanAIlanguagemodel,Icannotdeterminetheveracityofaclaimwithoutadditionalcontextorinformation.However,itiswidelyacceptedthatsomeanimals,suchascats,dogs,andminks,canbecomeinfectedwithCOVID-19throughclosecontactwithinfectedhumans.
True
Figure 9: Case of the chats of vanilla zero-shot prompt-
ing and UPRISE on Covid-19 dataset, the label comple-
tion is “True”.



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 10):

Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT
more different graph models that can be used for accomplishing
their own graph reasoning tasks.
Protein Molecule Function Reasoning : Protein and chemical
molecule function inference [52] has been a classic problem studied
in bio-chemical research for decades, which has fundamental appli-
cations in the real-world, such as helping design some new drugs
for curing some existing rare diseases. Protein function inference is
not an easy task, because homologous proteins often have several
different functions at the same time. Also such a prediction needs
to be fine-tuned with respect to some mutations but robust with
respect to others. Researchers have been exploring on this problem
with machine learning models, and have also developed a relatively
large protein function database [48] already. However, compared
with the number of protein existing in the real world, the specific
proteins with known functions included in the database is still
very limited. In graph learning, inferring the function of protein
molecules based on its structure has also be extensively studied as
well. Therefore, in this part, we also include it as a graph reasoning
task into Graph-ToolFormer as well.
Different from the bibliographic network, the protein molecular
graphs have much smaller sizes and there will also exist multiple
such graph instances in the dataset. What’s more, the features and
labels of protein molecular graphs are both about the whole molecu-
lar graph, not about the individual nodes anymore. As introduced
in Section 3.2, we can represent the set of studied protein molec-
ular graphs as G= {𝑔1,𝑔2,··· ,𝑔𝑙}, which can be loaded with the
following graph loading API call:
<API>𝐺𝐿(“𝑝𝑟𝑜𝑡𝑒𝑖𝑛-𝑔𝑟𝑎𝑝ℎ-𝑠𝑒𝑡”)→G </API>. (29)
For each molecular graph instance 𝑔𝑖 = (V𝑔𝑖 ,E𝑔𝑖 )in the dataset G,
there will also be raw features and labels related to each protein
molecular graph instance. For instance, for the graph instance 𝑔𝑖 ∈
G, we can represent its raw feature asx𝑔𝑖 and its label as y𝑔𝑖 , where
the label vector will indicate its corresponding functions. Based
on the protein graph structure and its raw features, we can define
the following API call for protein molecule function reasoning as
follows:
<API>𝐺𝑅(G,“𝑠𝑒𝑔-𝑏𝑒𝑟𝑡:𝑚𝑜𝑙𝑒𝑐𝑢𝑙𝑒-𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛 ”,𝑔𝑖)→ 𝑟</API>, (30)
which will call the pre-trained graph neural network SEG-Bert
proposed in [58]. The SEG-Bert with full name “Segmented Graph-
Bert” [58] extends the Graph-Bert model for molecular graph in-
stance representation learning. Besides the SEG-Bert model used
in Graph-ToolFormer, the readers can also customize the Graph-
ToolFormer framework to include other graph models for address-
ing the molecular graph reasoning tasks as well.
Sequential Recommender System Reasoning : In the era of big
data, as more and more data are generated both online and offline,
manual search of information from such big data sources has be-
come infeasible nowadays and we may need recommender systems
[28] to automatically recommend desired information for us instead.
Based on the historical records, sequential recommender system
aims to infer the next item(s) that users may be interested in, which
may lead to either the future purchase action or the review rating
scores of those items. When studying the sequential recommender
systems, it is a common way to model recommender systems as
the bipartite graphs, where the user-item interaction record also
has an attached timestamp. With considerations about the times-
tamps, sequential recommender systems aim to infer the potential
existence (or the weight) of links between user and their interested
items for the next future timestamp. In other words, we can define
the sequential recommendation problem in recommender systems
as a link prediction task with considerations about the temporal
factor.
Formally, according to the above description, we can repre-
sent the sequential recommender system as a bipartite graph
𝐺 = (V,E), where the node set V= U∪I covers both users and
items and the links in set E⊂M×I only exist between users and
item instead. For each user-item pair (𝑢𝑗,𝑖𝑙)∈E in the link set, we
can also obtain its timestamp. The sequential recommender system
data can be loaded with the following API call:
<API>𝐺𝐿(“𝑟𝑒𝑐𝑜𝑚𝑚𝑒𝑛𝑑𝑒𝑟-𝑠𝑦𝑠𝑡𝑒𝑚”)→ 𝐺</API>. (31)
For each user 𝑢𝑗 and item 𝑖𝑙 in the recommender system 𝐺, based
on the historical interaction records (before the current timestamp),
we can learn the embedding representations of them, which will be
used to infer the label between them in the future. Depending on
the modeling approach, the label vector can indicate either whether
the user will purchase the item or not ( i.e., binary classification
task) or the rating score of the user for the item (i.e., the regression
task). Regardless of the specific modeling settings, we can represent
the recommender system reasoning API call in LLMs as follows:
<API>𝐺𝑅(𝐺,“𝑏𝑝𝑟:𝑟𝑒𝑐𝑜𝑚𝑚𝑒𝑛𝑑𝑎𝑡𝑖𝑜𝑛”,𝑢𝑗,𝑖𝑙)→ 𝑟</API>, (32)
which will return either the probability scores that the user 𝑢𝑗 will
be interested in the item 𝑖𝑙 or the specific rating scores that 𝑢𝑗
will give to 𝑖𝑙. We use BPR (Bayesian Personalized Ranking) [42]
as the default recommendation model in Graph-ToolFormer in
this paper, but other recommendation models can also be used for
defining the above recommendation API calls as well. Besides the
recommendation API calls to infer the scores between user and
item, for one specific user 𝑢𝑗, we can also return the list of top-𝑘
recommended items with the following API call:
<API>𝐺𝑅(𝐺,“𝑏𝑝𝑟:𝑡𝑜𝑝𝑘-𝑟𝑒𝑐𝑜𝑚𝑚𝑒𝑛𝑑𝑎𝑡𝑖𝑜𝑛”,𝑢𝑗,𝑘)→ 𝑟</API>,
(33)
where the notation 𝑘 denotes a hyper-parameter to be extracted
from the input statements for the recommendation reasoning.
Online Social Network Community Reasoning : Online social
networks [31], like Facebook, Twitter and Tiktok, provide different
online services for their users to facilitate their online socialization
with friends, family members and colleagues. Users in online social
networks tend to interact more frequently with their online friends,
and they will naturally form their online social communities based
on their online social behaviors. Reasoning for the social commu-
nities of users in online social networks is a complicated problem.
In this part, we will introduce the API calls to empower LLMs to
detect social communities from online social networks.
Formally, we can represent the online social network studied
in this paper as 𝐺 = (V,E), where Vdenotes the set of user
nodes and Edenotes the social interactions among the users in the



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 7):

Jiawei Zhang
we can represent the graph loading API call as
<API>𝐺𝐿(file-path,node-subset,link-subset)→ 𝐺</API>, (10)
where “𝐺𝐿()” denotes abbreviation of the “Graph Loading” func-
tion name, and the function parameters “file-path”, “node-subset”
and “link-subset” specify the local graph data file path (or the on-
line repository URL if the data is stored on the web), subsets of
specific nodes and links, respectively. The notation “→𝐺” explic-
itly represents the loaded graph data with the reference variable
𝐺 = (V,E), which is optional actually depending on the applica-
tion task and settings. What’s more, if the local file directory or
the online repository root URL has been pre-provided to the “𝐺𝐿()”
function already, then we can just simplify the “file-path” with the
specific “graph-name” instead when calling this API function.
Furthermore, when the parameters “ node-subset” and “ link-
subset” are either omitted or assigned with the strings “all nodes”
and “all links”, respectively, then the API function call will just load
the whole graph. For some cases, we can only specify the subset
of nodes to be loaded (e.g., {𝑣𝑖,𝑣𝑗,··· ,𝑣𝑘}⊂V in the graph) but
cannot enumerate all the related links, we can just assign the “node-
subset” and “link-subset” parameters with values “{𝑣𝑖,𝑣𝑗,··· ,𝑣𝑘}”
and “all related links” (or the “link-subset” parameter is just omitted).
It will provide us with more flexibility in loading sub-graphs based
on the provided node set and their internal links. Similarly, we can
also only specify the subset of links, by assigning the “node-subset”
with “all related nodes” or just omitted it, it will automatically load
the nodes composing those provided links in the graph data, like
the second graph data loading prompt example shown in Table 1.
Besides that example, as shown at the top part of Table 1, we also
provide a few other prompt examples of the graph data loading API
calls, which can retrieve and load the requested graph data from
the (local) files according to the input textual statements.
4.3.2 Graph Property Reasoning. Graph structured data may have
various properties, such as diameter, density, center and shortest
path, which can capture different characteristics of the graph data
and have extensive applications in real-world graph structured data.
For reasoning such graph properties, it usually requires the model
to not only know the property definitions but also has very strong
logic reasoning and mathematical calculation abilities to compute
such properties. For the existing language models, either masked
language models or autoregressive language models , it will be very
hard (almost impossible) for them to conduct the reasoning process
for such complex properties based on the input graphs.
In this paper, to empower LLMs with the graph property reason-
ing ability, we introduce a group of external APIs, which can be
called by the language models for reasoning about those properties.
To illustrate howGraph-ToolFormer handles such graph property
reasoning tasks, we will use the small-sized lollipop graph shown
in Figure 1 (the top-left graph in green color) as an example in this
part, which can be loaded via the following API calls as introduced
before:
<API>𝐺𝐿(“𝑙𝑜𝑙𝑙𝑖𝑝𝑜𝑝 ”)→ 𝐺𝑙</API>, (11)
where the loaded the graph can also be referred to by notation 𝐺𝑙.
For simplicity, in the following part, we will also use the above
loaded lollipop graph 𝐺𝑙 as an example to introduce the graph
property reasoning APIs for readers.
Order and Size : Formally, given a graph, like the loaded lollipop
graph 𝐺𝑙 = (V,E), its order denotes the number of nodes in the
graph, i.e., |V|, and its size is the number of links in the graph, i.e.,
|E|. We can represent the API calls for reasoning the order and size
properties of the lollipop graph as
<API>𝐺𝑅(𝐺𝐿(“𝑙𝑜𝑙𝑙𝑖𝑝𝑜𝑝 ”),“𝑡𝑜𝑜𝑙𝑥:𝑜𝑟𝑑𝑒𝑟”)→ 𝑟</API>, (12)
<API>𝐺𝑅(𝐺𝐿(“𝑙𝑜𝑙𝑙𝑖𝑝𝑜𝑝 ”),“𝑡𝑜𝑜𝑙𝑥:𝑠𝑖𝑧𝑒”)→ 𝑟</API>. (13)
If the lollipop graph has been pre-loaded via other API calls al-
ready and can be referred to as 𝐺𝑙, the above API calls can also be
simplified as follows:
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑜𝑟𝑑𝑒𝑟”)→ 𝑟</API>, (14)
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑠𝑖𝑧𝑒”)→ 𝑟</API>, (15)
where the notation 𝐺𝑅()denotes the abbreviated “Graph Rea-
soning” function name and the parameters “order” and “size”
represent the graph properties to be reasoned. The notation
“toolx:desired_property” denotes the desired graph property reason-
ing with the toolx toolkit. The toolx is a graph property calculation
toolkit created in this paper for Graph-ToolFormer based on the
networkx, and we will introduce more information about the graph
reasoning models and toolkits used in this paper in the next ex-
periment section instead. The notation “→𝑟” specifies the output
result 𝑟by the graph property reasoning API call to be included into
the output statements. As introduced before, the returning output
result tag “→𝑟” of the API calls is actually optional, inclusion of
which depends on both the reasoning context and application task.
Density: Graphdensity denotes the ratio of existing links in a graph
compared with the maximal number of potential links among nodes
in a graph. If the input lollipop graph 𝐺𝑙 = (V,E)is directed, its
density can be represented as |E|
|V|(|V|− 1); while if 𝐺𝑙 is undirected,
its density can be represented as 2|E|
|V|(|V|− 1). Formally, the API
calls that can be used for computing the density of graph can be
represented as follows:
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑑𝑒𝑛𝑠𝑖𝑡𝑦”,𝑖𝑠-𝑑𝑖𝑟𝑒𝑐𝑡𝑒𝑑 )→ 𝑟</API>, (16)
where the boolean “is-directed” parameter differentiates directed
graph from undirected ones in the density calculation.
Shortest Path: The shortest path between two nodes in a graph is
a path of shortest possible length connecting them via the nodes
and links in the graph. The API call for reasoning the length of the
shortest path from 𝑛𝑜𝑑𝑒1 to 𝑛𝑜𝑑𝑒2 in a graph can be represented as
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑠ℎ𝑜𝑟𝑡𝑒𝑠𝑡 -𝑝𝑎𝑡ℎ”,𝑛𝑜𝑑𝑒1,𝑛𝑜𝑑𝑒2)→ 𝑟</API>.
(17)
Meanwhile, the average length of shortest path for all nodes in the
graph can be obtained via the following API call instead
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑎𝑣𝑔-𝑠ℎ𝑜𝑟𝑡𝑒𝑠𝑡 -𝑝𝑎𝑡ℎ”)→ 𝑟</API>. (18)
Besides the average shortest path length , we can also reason for the
largest shortest path length and the smallest shortest path length of
a graph as follows:
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑚𝑎𝑥-𝑠ℎ𝑜𝑟𝑡𝑒𝑠𝑡 -𝑝𝑎𝑡ℎ”)→ 𝑟</API>, (19)
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑚𝑖𝑛-𝑠ℎ𝑜𝑟𝑡𝑒𝑠𝑡 -𝑝𝑎𝑡ℎ”)→ 𝑟</API>. (20)



Source: data\tc16_2312.10997v5\referenced_papers\[101]_2310.06839.pdf (Page 14):

Methods SingleDoc MultiDoc Summ. FewShot Synth. Code A VG Tokens 1/τ
Original Prompt 27.4 30.3 20.3 49.9 12.5 42.5 30.5 10,295 -
Retrieval-based Methods
BM25 2.4 2.6 16.4 8.7 0.0 44.7 12.5 1,985 5x
SBERT 11.6 13.7 21.1 16.2 7.5 30.0 16.7 1,947 5x
LongLLMLingua rk 30.3 32.4 24.5 41.0 27.5 38.1 32.3 1,960 5x
Compression-based Methods
Selective-Context 16.1 23.5 21.8 21.4 2.5 35.9 20.2 1,925 5x
LLMLingua 20.6 22.3 22.4 35.6 0.0 35.4 22.7 1,950 5x
LongLLMLingua 31.3 34.6 24.6 46.1 27.8 48.8 35.5 1,822 6x
Table 5: Performance of different methods under different compression ratios on LongBench (Bai et al., 2023) using
LongChat-13b in 2,000 tokens constraint.
Methods GvRp SSFD QMsm SQAL QALT Nrtv Qspr MuSQ SpDg BkSS A VG Tokens 1/τ Latency Speedup
3,000 tokens constraint
Retrieval-based Methods
BM25 9.7 3.4 11.7 14.3 57.1 5.9 25.7 11.2 29.6 29.6 19.8 3,379 3x 5.5 2.2x
SBERT 16.5 9.8 12.3 15.2 60.0 14.6 23.4 12.1 39.4 36.4 24.0 3,340 3x 5.9 2.1x
OpenAI 14.3 8.3 12.0 15.3 66.7 13.3 24.3 11.7 31.2 26.4 22.4 3,362 3x 11.7 1.0x
LongLLMLinguark 19.5 11.6 14.7 15.5 66.7 20.5 27.6 13.0 60.8 43.4 29.3 3,350 3x 6.2 2.0x
Compression-based Methods
Selective-Context 20.8 9.1 11.7 13.4 50.0 9.8 26.1 11.0 46.0 9.5 20.7 3,460 3x 54.2 0.2x
LLMLingua 18.7 10.0 14.9 16.8 61.9 26.9 27.2 23.4 62.9 44.5 30.7 3,366 3x 7.4 1.7x
LongLLMLingua 22.1 12.8 15.9 17.1 67.0 27.8 31.3 23.9 65.8 46.5 33.0 3,431 3x 8.2 1.5x
2,000 tokens constraint
Retrieval-based Methods
BM25 8.8 2.5 11.1 13.5 60.0 7.0 4.9 20.3 39.9 32.9 20.1 1,799 5x 3.8 3.2x
SBERT 10.2 7.9 13.7 13.2 60.0 8.1 10.8 1.7 37.2 42.8 20.5 1,773 6x 4.1 3.0x
OpenAI 11.1 8.0 11.8 13.6 60.0 7.1 13.2 4.0 33.6 43.6 20.6 1,784 5x 9.9 1.2x
LongLLMLinguark 18.2 9.8 12.3 15.9 57.1 10.1 17.8 7.3 57.7 42.3 24.9 1,771 6x 4.7 2.6x
Compression-based Methods
Selective-Context 19.0 8.4 9.7 12.4 47.0 12.5 21.6 11.5 41.2 11.0 19.4 1,865 5x 47.5 0.3x
LLMLingua 19.4 11.9 13.1 16.0 62.1 23.7 24.0 22.4 33.9 44.9 27.2 1,862 5x 4.8 0.3x
LongLLMLingua 20.1 12.4 14.9 16.5 65.1 27.7 30.7 23.6 68.5 47.2 32.7 1,826 6x 5.2 2.3x
Original Prompt 21.8 12.1 17.9 17.4 66.7 25.3 29.8 20.0 69.7 44.1 32.5 9,788 - 12.2 -
Zero-shot 9.4 3.0 8.6 11.4 42.9 10.6 12.4 5.5 4.2 0.0 12.8 32 306x 1.0 12.2x
Table 6: Performance breakdown of different methods under different compression ratios on ZeroSCROLLS (Sha-
ham et al., 2023) using GPT-3.5-Turbo.
Scrolls validation dataset is relatively small, it still
demonstrates conclusions similar to previous ex-
perimental observations across various methods
and tasks. Furthermore, this study conducted an in-
depth analysis of the multi-hop QA task - MuSiQue,
and another long context benchmark - LooGLE.
The results can be found in Appendix C.5 and Ap-
pendix C.6.
C.5 MuSiQue
Table 7 presents the results from the MuSiQue
multi-hop question-answer dataset. From the table,
it can be observed that in the multi-hop QA task,
requiring global information: 1) LongLLMLingua
can reduce noise in the prompt by eliminating irrel-
evant information and putting more related informa-
tion at the beginning or end of the prompt, thereby
improving performance by 5.4 points. 2) The per-
formance drop is more pronounced for retrieval-
based methods, particularly for n-gram-based meth-
ods like BM25. Due to long dependencies, direct
matching information is lost, resulting in less rel-
evant information being recalled. 3) The perfor-
mance of compression-based methods is slightly
different. Selective-Context does not distinguish
between different modules’ sensitivity, resulting in
a loss of question and instruction-related informa-
tion, thereby leading to poorer performance. How-
ever, LLMLingua can still retain relevant key in-
formation at around a 2x compression ratio. 4)
The ablation experiments show that every module
designed in LongLLMLingua plays a role in the



### Claim 125/179

#### Claim Text
Uhlmann observed that applying the direct forcing immersed-boundary procedure throughout the volume occupied by the solid particle tends to reduce the slip error at the interface for low Reynolds number flows.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 32):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: A liquid flows at a constant flow rate through a pipe
with circular cross-sections of varying diameters. At one point in the
pipe, the diameter is 2 cm and the flow speed is 18 m/s. What is the flow
speed at another point in this pipe, where the diameter is 3 cm?
0) 4 m/s
1) 6 m/s
2) 8 m/s
3) 12 m/s
Original Answer [Incorrect]: The answer is 3) 12 m/s A step-by-step
solution is below: The flow rate is constant, so the product of the speed
and the cross-sectional area is constant. The cross-sectional area is
proportional to the square of the diameter, so the speed is inversely
proportional to the square of the diameter. The ratio of the speeds is the
inverse of the ratio of the squares of the diameters, so the speed at the
second point is 18m/s ∗ (2cm/3cm)2 = 12 m/s.
Stepback Question: What are the physics principles behind this
question?
Principles:
Conservation of Mass: the amount of mass flowing through the pipe at
two different points must be equal, or A1 * v1 = A2 * v2, where A1 is
the area of cross-section one, v1 is the velocity at point 1, A2 is the area
of cross-section two, v2 is the velocity at point 2.
Area of Circle: A = π ∗ r2, where r is the radius of the circle.
Final Answer: Using the Principles of Continuity equation, we can
solve the problem as following:
Step 1: Apply the Continuity equation to find out the flow speed at
another point in this pipe. The continuity equation states that the product
of the area and the velocity is constant at all points in a pipe. In this
problem, we are given the following information:
A1 = π ∗ (2cm)2 = 4πcm2
v1 = 18 m/s
A2 = π ∗ (3cm)2 = 9πcm2
We can use the continuity equation to find v2:
A1 * v1 = A2 * v2
4πcm2 ∗ 18m/s = 9πcm2 ∗ v2
v2 = 8 m/s
Therefore, the flow speed at another point in this pipe is 8 m/s.
The correct answer is 2) 8 m/s.
Table 29: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
33



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 30):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question Physics students are checking the constant acceleration equations of kinematics
by measuring the velocity of a tennis ball that is dropped and falls 6 meters
and then passes through a photogate. The predicted velocity is 20% above the
velocity measured by the photogate. Which of the following best describes the
cause of the large percent difference?
0) The ball changes its shape while falling.
1) The acceleration of gravity varies as the ball is falling.
2) Air resistance increases the acceleration of the ball.
3) The acceleration of the balls varies with the velocity.
Principles Vertical motion: y = 0.5 * g * t2, where y is the vertical displacement, g is the
acceleration due to gravity, t is the time.
Free fall: is a special case of motion with constant acceleration, in which the
acceleration is due to gravity.
Acceleration to Velocity Formula: v = u + a * t, where v is the final speed, u is
the initial velocity, a is the acceleration, and t is the time.
Answer Using the Principles of Vertical motion and Horizontal motion, we can solve
the problem as following:
Step 1: Apply the Vertical motion equation to find the time it takes for the ball
to fall 6 meters.
y = 0.5 * g * t2
6 m = 0.5 * 9.8 m/s2 * t2
t = 1.12 s
Step 2: Apply the Horizontal motion equation to find the velocity of the ball
when it passes through the photogate.
x = v * t
v = x / t
v = 6 m / 1.12 s
v = 5.36 m/s
Step 3: Compare the predicted velocity with the velocity measured by the
photogate. The predicted velocity is 20% above the velocity measured by the
photogate, so the predicted velocity is 1.2 * 5.36 m/s = 6.43 m/s.
Step 4: Find the cause of the large percent difference. The predicted velocity is
6.43 m/s, but the velocity measured by the photogate is 5.36 m/s. The difference
is 1.07 m/s. This difference is caused by air resistance. Air resistance is a force
that opposes the motion of an object through the air. It is caused by the friction
between the object and the air molecules. The greater the surface area of the
object, the greater the air resistance. The tennis ball has a large surface area, so
it experiences a lot of air resistance. This air resistance slows down the tennis
ball and causes the difference between the predicted velocity and the velocity
measured by the photogate.
Therefore, the correct answer is 0) The ball changes its shape while falling.
Table 27: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Reasoning Error. In this case, the model was able to correctly identify air resistance is the cause of
the velocity difference, but made the error at the final reasoning step of deriving the right answer.
31



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 16):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question A spherical conductor carries a net charge. How is this charge distributed on
the sphere?
0) The charge is evenly distributed on the surface.
1) The charge resides on the surface only; the distribution of charge on the
surface depends on what other charged objects are near the sphere.
2) The charge moves continually within the sphere.
3) The charge is distributed uniformly throughout the sphere.
Principles Coulomb’s Law: the force between two charged particles is proportional to the
product of their charges and inversely proportional to the square of the distance
between them, F = k * q1 * q2 / r, where F is the electric force, k is a constant,
q1 and q2 are the charges the particles carry, and r is the distance between them.
Answer Using the Principles of Coulomb’s Law, we can solve the problem as following:
Step 1: Apply Coulomb’s Law to find out how charges are distributed on the
surface.
In the case of a spherical conductor, the charges on the surface will repel each
other. The further apart the charges are, the less force they will exert on each
other. Therefore, the charges will distribute themselves evenly on the surface
of the sphere, as this is the configuration that minimizes the repulsive force
between them.
Step 2: Apply Coulomb’s Law to find out what happens if there are other
charges present.
The distribution of charge on the surface may also be affected by the presence
of other charged objects near the sphere. For example, if a negatively charged
object is brought near a positively charged sphere, the negative charges on the
sphere will be repelled and will move to the opposite side of the sphere. This
will result in a non-uniform distribution of charge on the surface of the sphere.
Therefore, the correct answer is 1) The charge resides on the surface only; the
distribution of charge on the surface depends on what other charged objects are
near the sphere.
Table 9: Few-shot demonstration exemplar for MMLU high-school Physics.
17



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 31):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: What happens to the pressure, P, of an ideal gas if
the temperature is increased by a factor of 2 and the volume is increased
by a factor of 8 ?
0) P decreases by a factor of 16.
1) P decreases by a factor of 4.
2) P decreases by a factor of 2.
3) P increases by a factor of 4.
Original Answer [Incorrect]: The correct answer is 0. The ideal gas
law is PV = nRT. If the temperature is increased by a factor of 2 and the
volume is increased by a factor of 8, then the pressure will decrease by a
factor of 16.
Stepback Question: What are the physics principles behind this
question?
Principles:
Ideal gas law: PV = nRT, where P is the pressure, V is the volume, n is
the number of moles, R is the gas constant, and T is the temperature.
Final Answer: Using the Principles of Ideal gas law, we can solve the
problem as following:
Step 1: Apply Ideal gas law to find out the pressure of an ideal gas. The
ideal gas law states that PV = nRT, where P is the pressure, V is the
volume, n is the number of moles, R is the gas constant, and T is the
temperature.
Step 2: Apply Ideal gas law to find out the pressure of an ideal gas when
the temperature is increased by a factor of 2 and the volume is increased
by a factor of 8. If the temperature is increased by a factor of 2, then T be-
comes 2T. If the volume is increased by a factor of 8, then V becomes 8V .
Substituting these values into the ideal gas law, we get: P(8V) = nR(2T)
Dividing both sides by 8V , we get: P = nR(2T) / 8V
We can see that the pressure has decreased by a factor of 4.
Therefore, the correct answer is 1) P decreases by a factor of 4.
Table 28: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
32



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 28):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question Two spheres of net charge +5e and -6e briefly come into contact. Afterward,
which of the following is a possible combination of net charges for the two
spheres?
0) +3e and -4e
1) +1.5e and -2.5e
2) Both of the above
3) None of the above
Principles Law of conservation of charge: the total charge in a closed system remains
constant.
Coulomb’s Law: the mutual repulsion of like charges demands that the charges
be as far apart as possible.
Answer Using the Principles of Conservation of charge, we can solve the problem as
following:
Step 1: Apply Conservation of charge to find out the possible combination of
net charges for the two spheres.
The total charge of the two spheres before they come into contact is +5e - 6e =
-1e. After they come into contact, the total charge of the two spheres must still
be -1e.
Step 2: Find out the possible combination of net charges for the two spheres.
The possible combination of net charges for the two spheres are:
(1) +3e and -4e
(2) +1.5e and -2.5e
Therefore, the correct answer is 2) Both of the above.
Table 25: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Principle Error. In this case, Coulomb’s Law is not needed for solving this problem. Instead, the
Charge Quantization Principle is missing from the retrieved principles, and is in fact needed to rule
out option (2) of fractional charges.
29



### Claim 126/179

#### Claim Text
The occurarXiv:2412.07339v1 [physics.soc-ph] 10 Dec 2024 2 rence of contacts between three or more nodes is described as higher-order interactions , , which can happen in various real-life scenarios.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 24):

Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT
Table 5: Case analysis of graph reasoning API call statement generation, query parsing, and query execution and final output
generation in Graph-ToolFormer.
Tasks Graph Reasoning Examples withGraph-ToolFormer
Input Statements Generated Statements with API callsQuery parsing Reasoning Final Statement Output
Graph
Property
Reasoning
What is the order of the dia-
mond graph?
The order of the diamond graph is [GR(GL("gpr",
{"diamond_graph"}), "toolx:order")–>r].
((’GR’, [(’GL’, [’gpr’, ’{"dia-
mond_graph"}’]), ’toolx:order’]),
[True])
4 The order of the diamond graph
is 4.
The path graph has a center at
nodes [TBR].
The path graph has a center at nodes
[GR(GL("gpr", {"path_graph"}), "toolx:center")–
>r].
((’GR’, [(’GL’, [’gpr’,
’{"path_graph"}’]), ’toolx:center’]),
[True])
[5, 6] The path graph has a center at
nodes [5, 6].
The nodes in the wheel graph
have eccentricity values of
[TBR].
The nodes in the wheel graph have eccentric-
ity values of [GR(GL("gpr", {"wheel_graph"}),
"toolx:eccentricity")–>r].
((’GR’, [(’GL’, [’gpr’,
’{"wheel_graph"}’]),
’toolx:eccentricity’]), [True])
{0: 1, 1: 2, 2: 2, 3: 2,
4: 2, 5: 2}
The nodes in the wheel graph
have eccentricity values of {0: 1,
1: 2, 2: 2, 3: 2, 4: 2, 5: 2}.
Bibliographic
Network
Reasoning
What is the topic of paper
#83826 in the cora biblio-
graphic network?
The topic of paper #83826 in the cora
bibliographic network is [GR(GL("cora"),
"graph_bert:topic", paper#83826)–>r].
((’GR’, [(’GL’, [’cora’]),
’graph_bert:topic’, ’paper#83826’]),
[True])
Neural_NetworksThe topic of paper #83826 in
the cora bibliographic network
is Neural_Networks.
Paper #2998, situated in the
citeseer bibliographic network,
is centered around the [TBR]
topic.
Paper #2998, situated in the citeseer bib-
liographic network, is centered around
the [GR(GL("citeseer"), "graph_bert:topic",
paper#2998)–>r] topic.
((’GR’, [(’GL’, [’citeseer’]),
’graph_bert:topic’, ’paper#2998’]),
[True])
AI Paper #2998, situated in the cite-
seer bibliographic network, is
centered around the AI topic.
Within the pubmed biblio-
graphic network, paper #16466
focuses on the subject of [TBR].
Within the pubmed bibliographic net-
work, paper #16466 focuses on the subject
of [GR(GL("pubmed"), "graph_bert:topic",
paper#16466)–>r].
((’GR’, [(’GL’, [’pubmed’]),
’graph_bert:topic’, ’paper#16466’]),
[True])
1 Within the pubmed biblio-
graphic network, paper #16466
focuses on the subject of 1.
Molecular
Graph
Reasoning
What is the function for the
protein molecular graph #138
in proteins?
The function for the protein molecular
graph #138 in proteins is [GR(GL("proteins"),
"seg_bert:molecule_function", instance#138)–>r].
((’GR’, [(’GL’, [’proteins’]),
’seg_bert:molecule_function’,
’instance#138’]), [True])
0 The function for the protein
molecular graph #138 in proteins
is 0.
In mutag, instance #30 of
the chemical molecular graph
demonstrates a function of
[TBR].
In mutag, instance #30 of the chemical
molecular graph demonstrates a function of
[GR(GL("mutag"), "seg_bert:molecule_function",
instance#30)–>r].
((’GR’, [(’GL’, [’mutag’]),
’seg_bert:molecule_function’,
’instance#30’]), [True])
2 In mutag, instance #30 of the
chemical molecular graph
demonstrates a function of 2.
For chemical molecular graph
instance #652 in nci1, its func-
tion is [TBR].
For chemical molecular graph instance
#652 in nci1, its function is [GR(GL("nci1"),
"seg_bert:molecule_function", instance#652)–>r].
((’GR’, [(’GL’, [’nci1’]),
’seg_bert:molecule_function’,
’instance#652’]), [True])
0 For chemical molecular graph in-
stance #652 in nci1, its function
is 0.
The chemical molecular graph
numbered 239 in ptc is charac-
terized by a function of [TBR].
The chemical molecular graph numbered 239 in
ptc is characterized by a function of [GR(GL("ptc"),
"seg_bert:molecule_function", instance#239)–>r].
((’GR’, [(’GL’, [’ptc’]),
’seg_bert:molecule_function’,
’instance#239’]), [True])
0 The chemical molecular graph
numbered 239 in ptc is charac-
terized by a function of 0.
Recommender
System
Reasoning
How likely user
#A1HOLE9R6WPT85
will be interested in item
#B00005MOTF in Amazon?
The likelihood that user #A1HOLE9R6WPT85
will be interested in item #B00005MOTF in Ama-
zon is [GR(GL("amazon"), "bpr:recommendation",
user#A1HOLE9R6WPT85, item#B00005MOTF)–
>r].
((’GR’, [(’GL’, [’amazon’]),
’bpr:recommendation’,
’user#A1HOLE9R6WPT85’,
’item#B00005MOTF’]), [True])
0.008 The likelihood that user
#A1HOLE9R6WPT85 will be in-
terested in item #B00005MOTF
in Amazon is 0.008.
The likelihood that user #u1527
will be interested in music
from artisit #i5422 in Last-fm
is [TBR].
The likelihood that user #u1527 will be in-
terested in music from artisit #i5422 in Last-
fm is [GR(GL("last-fm"), "bpr:recommendation",
user#u1527, artisit#i5422)–>r].
((’GR’, [(’GL’, [’last-fm’]),
’bpr:recommendation’,
’user#u1527’, ’artisit#i5422’]),
[True])
0.248 The likelihood that user #u1527
will be interested in music from
artisit #i5422 in Last-fm is 0.248.
In Movielens, the top 10 movies
that user #u272 likes include
[TBR].
In Movielens, the top 10 movies that user
#u272 likes include [GR(GL("movielens"),
"bpr:topk_recommendation", user#u272, 10)–>r].
((’GR’, [(’GL’, [’movielens’]),
’bpr:topk_recommendation’,
’user#u272’, ’10’]), [True])
[’i286’, ’i288’,
’i258’, ’i294’, ’i300’,
’i50’, ’i313’, ’i100’,
’i181’, ’i269’]
In Movielens, the top 10 movies
that user #u272 likes include
[’i286’, ’i288’, ’i258’, ’i294’, ’i300’,
’i50’, ’i313’, ’i100’, ’i181’, ’i269’].
Social
Network
Reasoning
In the online social network
foursquare, which community
is user user/1265481 involved
in?
In the online social network foursquare,
user user/1265481 is involved in the
[GR(GL("foursquare"), "kmeans:community",
user#user/1265481)–>r] communities formed by
users.
(’GR’, [(’GL’, [’foursquare’]),
’kmeans:community’,
’user#user/1265481’])
#2 In the online social network
foursquare, user user/1265481 is
involved in the #2 communities
formed by users.
In the online social network
twitter, are user #deeprogress
and user #alejandro1254 be-
long to the same community?
In the online social network twitter,
user #deeprogress and user #alejan-
dro1254 belong to [GR(GL("twitter"),
"kmeans:common_community_check",
user#deeprogress, user#alejandro1254)–>r]
community.
(’GR’, [(’GL’, [’twitter’]),
’kmeans:common_community_check’,
’user#deeprogress’,
’user#alejandro1254’])
The Same In the online social network twit-
ter, user #deeprogress and user
#alejandro1254 belong to the
same community.
Knowledge
Graph
Reasoning
According to the Freebase
knowledge graph, what
is the relation between
entity#/m/053yx and en-
tity#/m/015_1q?
According to the Freebase knowl-
edge graph, the relation between en-
tity#/m/053yx and entity#/m/015_1q is
[GR(GL("freebase"), "transe:relation", en-
tity#/m/053yx, entity#/m/015_1q)–>r].
(’GR’, [(’GL’, [’freebase’]),
’transe:relation’, ’entity#/m/053yx’,
’entity#/m/015_1q’])
/music/artist/labelAccording to the Freebase
knowledge graph, the relation
between entity#/m/053yx
and entity#/m/015_1q is
/music/artist/label.
According to the WordNet
knowledge graph, via relation
#_hypernym, what entity can
we obtain from entity #imagi-
nation.n.02?
According to the WordNet knowledge graph, via
relation #_hypernym, we can obtain entity #imag-
ination.n.02 from entity [GR(GL("wordnet"),
"transe:head_entity", relation#_hypernym,
entity#imagination.n.02)–>r].
(’GR’, [(’GL’, [’wordnet’]),
’transe:head_entity’, ’re-
lation#_hypernym’, ’en-
tity#imagination.n.02’])
chimera.n.02 According to the WordNet
knowledge graph, via relation
#_hypernym, we can obtain
entity #imagination.n.02 from
entity chimera.n.02.



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 9):

Jiawei Zhang
Table 2: A summary of API call examples for advanced graph reasoning tasks studied in this paper. In this table, we use
notations 𝐺𝐿(·)and 𝐺𝑅(·)to represent the graph loading and graph reasoning API calls. Similarly, we use “[”, “]” and “–>” to
represent the “ <API>”, “</API>” and “ →” tokens, and use notation “[TBR]” to denote the “to be reasoned” placeholder token.
Tasks API Call Templates Prompt Examples
Inputs Outputs
Bibliographic
Paper Topic
Reasoning
𝐺𝑅(𝑔𝑟𝑎𝑝ℎ,“𝑡𝑜𝑝𝑖𝑐”,𝑝𝑎𝑝𝑒𝑟-𝑛𝑜𝑑𝑒)→𝑟
In the core bibliographic network, paper
#31366 focuses on the topic of [TBR].
In the core bibliographic network, paper #31366 focuses on
the topic of[GR(GL(“cora”),“graph-bert:topic”,paper#31366)–
>NeuralNetworks].
Within cora, paper #13195 is dedicated to the
study of [TBR].
Within cora, paper #13195 is dedicated to the study of
[GR(GL(“cora”),“graph-bert:topic”,paper#13195)–>Reinforce-
mentLearning].
The citeseer bibliographic network’s paper
#2 is concerned with the area of [TBR].
The citeseer bibliographic network’s paper #2 is concerned with
the area of[GR(GL(“citeseer”),“graph-bert:topic”,paper#2)–
>Agents].
Paper #3 in the citeseer network investigates
the field of [TBR].
Paper #3 in the citeseer network investigates the field of
[GR(GL(“citeseer”),“graph-bert:topic”,paper#3)–>DB].
Paper #7, situated in the pubmed biblio-
graphic network, is centered around the
[TBR] topic.
Paper #7, situated in the pubmed bibliographic network, is cen-
tered around the[GR(GL(“pubmed”),“graph-bert:topic”,pa-
per#7)–>1] topic.
Protein
Function
Reasoning
𝐺𝑅(𝑔𝑟𝑎𝑝ℎ,“𝑝𝑟𝑜𝑡𝑒𝑖𝑛-𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛”,𝑔𝑖)→𝑟
The protein molecular graph instance #63
in the PROTEIN dataset has a function of
[TBR] for the disease.
The protein molecular graph instance #63 in the PRO-
TEIN dataset has a function of[GR(GL(“PROTEIN”),“seg-
bert:molecule-function”instance#63)–>0] for the disease.
In PROTEIN, instance #985 of the protein
molecular graph demonstrates a function of
[TBR] for the disease.
In PROTEIN, instance #985 of the protein molecular
graph demonstrates a function of[GR(GL(“PROTEIN”),“seg-
bert:molecule-function”instance#63)–>1] for the disease.
The chemical molecular graph numbered
63 in PTC is characterized by a function of
[TBR].
The chemical molecular graph numbered 63 in PTC is char-
acterized by a function of[GR(GL(“PTC”),“seg-bert:molecule-
-function”instance#63)–>1].
For chemical molecular graph instance #63
in NCI1, its function is [TBR].
For chemical molecular graph instance #63 in NCI1, its function
is [GR(GL(“NCI1”),“seg-bert:molecule-function”instance#63)–
>0].
The molecular graph of chemical compound
#121 in MUTAG possesses a function of
[TBR].
The molecular graph of chemical compound #121 in MUTAG
possesses a function of[GR(GL(“MUTAG”),“seg-bert:molecule-
-function”instance#121)–>2].
Sequential
Recommender
System
Reasoning
𝐺𝑅(𝑔𝑟𝑎𝑝ℎ,“𝑟𝑒𝑐𝑜𝑚𝑚𝑒𝑛𝑑𝑎𝑡𝑖𝑜𝑛”,𝑢𝑗,𝑖𝑙)→𝑟
In the Amazon recommender system, user
#A240ORQ2LF9LUI rates item #0077613252
with a score of [TBR].
In the Amazon recommender system, user #A240ORQ2LF9LUI
rates item #0077613252 with a score of[GR(GL(“amazon”),
“bpr:recommendation”, user#A240ORQ2LF9LUI,
item#0077613252)–>4.0].
Within Last.fm, user #2 awards item #52 with
a [TBR] tag.
Within Last.fm, user #2 awards item #52 with a
[GR(GL(“last.fm”),“bpr:recommendation”,user#2,item#52)–
>41] tag.
User #196 gives a rating of [TBR] to item
#251 at MovieLens.
User #196 gives a rating of [GR(GL(“movielens”),
“bpr:recommendation”,user#196, item#251)–>3] to item
#251 at MovieLens.
Online
Social Network
Reasoning
𝐺𝑅(𝑔𝑟𝑎𝑝ℎ,“𝑐𝑜𝑚𝑚𝑢𝑛𝑖𝑡𝑦”)→𝑟
In the academic collaboration network dblp,
scholar #355233 is involved in [TBR] local
community formed by his/her collaborators.
In the academic collaboration network dblp, scholar #355233
is involved in[GR(GL(“dblp”),“kmeans:community-count”,
scholar#355233)–>6] local community formed by his/her col-
laborators.
In the email communication social network,
there exist a number of [TBR] local commu-
nities formed by users.
In the email communication social network, there exist a num-
ber of[GR(GL(“email”),“kmeans:community-count”)–>42] local
communities formed by users.
The video sharing social network youtube
houses the largest user-formed local commu-
nity, which consists of [TBR] users.
The video sharing social network youtube houses the
largest user-formed local community, which consists of
[GR(GL(“youtube”),“kmeans:max-community-size”)–>3001]
users.
Knowledge
Graph
Reasoning
𝐺𝑅(𝑔𝑟𝑎𝑝ℎ,𝑟𝑒𝑎𝑠𝑜𝑛𝑖𝑛𝑔-𝑡𝑦𝑝𝑒,𝑖𝑛𝑝𝑢𝑡𝑠)→𝑟
According to the Freebase knowledge graph,
the relation between entity /m/027rn and
entity /m/06cx9 is [TBR].
According to the Freebase knowledge graph, the rela-
tion between entity /m/027rn and entity /m/06cx9 is
[GR(GL(“freebase”),“transe:relation”,entity:/m/027rn,en-
tity:/m/06cx9)–>/location/country/form_of_government].
According to the WordNet knowledge graph,
from entity plaything.n.01, via relation _hy-
ponym, we can derive entity [TBR].
According to the WordNet knowledge graph, from entity
plaything.n.01, via relation _hyponym, we can derive entity
[GR(GL(“freebase”),“transe:tail-entity”,entity:plaything.n.01,
relation:_hyponym)–>swing.n.02].



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 16):

Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT
• Output: The topic of paper #3230 in
the citeseer bibliographic network is
[GR(GL("citeseer"), "graph_bert:topic",
paper#3230)–>r].
3. Molecular Graph Reasoning Q&A Prompt Examples:
• Input: What is the function for the pro-
tein molecular graph #138 in proteins?
• Output: The function for the
protein molecular graph #138 in
proteins is [GR(GL("proteins"),
"seg_bert:molecule_function", instance#138)–
>r].
• Input: What is the function for the chemi-
cal molecular graph #129 in mutag?
• Output: The function for the
chemical molecular graph #129
in mutag is [GR(GL("mutag"),
"seg_bert:molecule_function", instance#129)–
>r].
• Input: What is the function for the chemi-
cal molecular graph #322 in nci1?
• Output: The function for the chem-
ical molecular graph #322 in nci1 is
[GR(GL("nci1"), "seg_bert:molecule_function",
instance#322)–>r].
• Input: What is the function for the chemi-
cal molecular graph #44 in ptc?
• Output: The function for the chemical
molecular graph #44 in ptc is [GR(GL("ptc"),
"seg_bert:molecule_function", instance#44)–
>r].
4. Social Network Reasoning Q&A Prompt Examples:
• Input: In foursquare, what is the id of
user sparkey215’s community?
• Output: In foursquare, the id
of user sparkey215’s community is
[GR(GL("foursquare"), "kmeans:community",
user#sparkey215)–>r].
• Input: In the online social network
foursquare, are user #user/9674821 and user
#ljaniszewski8 belong to the same community?
• Output: In the online social net-
work foursquare, user #user/9674821
and user #ljaniszewski8 be-
long to [GR(GL("foursquare"),
"kmeans:common_community_check",
user#user/9674821, user#ljaniszewski8)–>r]
community.
5. Recommender System Reasoning Q&A Prompt Examples:
• Input: How likely user #A23E9QQHJLNGUI
will be interested in item #B004PIPG2A in
Amazon?
• Output: The likelihood that user
#A23E9QQHJLNGUI will be interested in item
#B004PIPG2A in Amazon is [GR(GL("amazon"),
"bpr:recommendation", user#A23E9QQHJLNGUI,
item#B004PIPG2A)–>r].
• Input: How likely user #u329 will be inter-
ested in music of artist #i8323 in Last-fm?
• Output: The likelihood that user
#u329 will be interested in music from
artist #i8323 in Last-fm is [GR(GL("last-
fm"), "bpr:recommendation", user#u329,
artist#i8323)–>r].
• Input: How likely user #u650 will be inter-
ested in movie #i671 in Movielens?
• Output: The likelihood that user
#u650 will be interested in movie #i671
in Movielens is [GR(GL("movielens"),
"bpr:recommendation", user#u650,
movie#i671)–>r].
6. Knowledge Graph Reasoning Q&A Prompt Examples:
• Input: According to the Freebase knowl-
edge graph, what is the relation between
entity#/m/053yx and entity#/m/015_1q?
• Output: According to the Freebase knowl-
edge graph, the relation between en-
tity#/m/053yx and entity#/m/015_1q is
[GR(GL("freebase"), "transe:relation", en-
tity#/m/053yx, entity#/m/015_1q)–>r].
• Input: According to the WordNet knowledge
graph, via relation #_hypernym, we derive
entity #imagination.n.02 from what entity?
• Output: According to the WordNet knowledge
graph, via relation #_hypernym, we can ob-
tain entity #imagination.n.02 from entity
[GR(GL("wordnet"), "transe:head_entity", re-
lation#_hypernym, entity#imagination.n.02)–
>r].



Source: data\tc16_2312.10997v5\referenced_papers\[81]_2312.15883.pdf (Page 4):

enc(𝑢𝑖),enc(𝑒𝑗)

, 𝑢 𝑖 ∈U,𝑒𝑗 ∈E,
𝑢𝑖 ↔𝑒𝑗 iff 𝑒𝑗 = {argmax
𝑒𝑘∈E
sim(𝑢𝑖,𝑒𝑘)| sim(𝑢𝑖,𝑒𝑗)> 𝛿}, (3)
4https://www.modelscope.cn/models/damo/nlp_gte_sentence-embedding
where 𝛿 ∈[0,1]is the threshold hyper-parameter. We utilize the
same encoding model enc(·)to embed each medical entity, and
enc(𝑢𝑖),enc(𝑒𝑗)

denotes the inner product between extracted
entities and KGs entities for achieving graph entity linking. Finally,
the matched entities set is denoted as E𝑄.
4.2.2 Search Reasoning Chains in Knowledge Graph.Next, using
matched entities, we explore reasoning chains within 𝑘 hops and
consolidate this knowledge along with descriptions of the head
and tail entities. Considering various knowledge graph retrieval
methods, we opt for utilizing reasoning chains between entities for
several reasons: i) Reasoning Chains provide richer logical knowl-
edge provided for LLMs to help it digest, compared to entities and
entity descriptions alone. ii) Reasoning chains help LLM Reader
understand the relationships between different entities, thereby
alleviating hallucinations and error problems.iii) Reasoning chains
act as an efficient pruning mechanism, filtering out noise more
effectively than subgraphs and saving token resources.
As a consequence, in light of [ 85], we consider three possi-
ble reasoning chains from medical perspective: i) Path (head-to-
tail) as path𝑖𝑗, for comprehensively analyzing the triggering and
causal relationships between diseases and symptoms [52, 55]. ii)
Co-ancestor chain(tail-to-tail) as chainCA𝑖𝑗, for referring similar
physiological or environmental factors for better analogical diag-
nosis [10]. iii) Co-occurance chain(head-to-head) as chainCO𝑖𝑗,
for better capturing the pathological characteristics and evolution
of diseases [16]. In general, the reasoning chain set RCafter the



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 15):

Jiawei Zhang
function for fine-tuning the LLM as
ℓ(D)= 1
|D|
∑︁
(w𝑖,¯w𝑖 )∈D
ℓ(ˆw𝑖, ¯w𝑖) (51)
= 1
|D|
∑︁
(w𝑖,¯w𝑖 )∈D
∑︁
𝑗
cross-entropy(ˆw𝑖(𝑗), ¯w𝑖(𝑗)). (52)
4.6 Graph Reasoning Q&A Prompts
What’s more, to provide Graph-ToolFormer with basic Q&A abil-
ity for graph reasoning, besides the above statements based graph
reasoning prompts, we will also design some Q&A based prompts
for fine-tuning Graph-ToolFormer as well. The graph reasoning
Q&A based prompts are created in a very similar way as above, but
we will replace the input statements with other reasoning ques-
tions instead, and the output will still be a statement with graph
reasoning API calls corresponding to the input question.
Formally, we also list of the graph reasoning Q&A prompt exam-
ples used in this paper as follows, which will be merged into the
previous prompts for fine-tuning Graph-ToolFormer. Different
from the previous input-output statement prompts (where the out-
put is almost a duplicated copy of the input but with API calls), the
inputs and outputs in question-answer prompts are not duplicated
copies of each other anymore. However, with the above autoregres-
sive generation of the desired output statement introduced before,
the Graph-ToolFormer using causal language models as the back-
bone is still capable to generate the desired output statements for
the input question queries.
1. Graph Property Reasoning Q&A Prompt Examples:
• Input: What is the order of the barbell
graph?
• Output: The order of the barbell
graph is [GR(GL("gpr", "barbell_graph"),
"toolx:order")–>r].
• Input: What is the size of the star graph?
• Output: The size of the star graph is
[GR(GL("gpr", "star_graph"), "toolx:size")–
>r].
• Input: What is the density of the dodecahe-
dral graph?
• Output: The density of dodecahedral graph
is [GR(GL("gpr", "dodecahedral_graph"),
"toolx:density")–>r].
• Input: What is the eccentricity of node
#25 in the balanced tree?
• Output: The eccentricity of node #25
in the balanced tree is [GR(GL("gpr",
"balanced_tree"), "toolx:eccentricity",
"node#25")–>r].
• Input: What is the radius of the lollipop
graph?
• Output: The radius of the lollipop
graph is [GR(GL("gpr", "lollipop_graph"),
"toolx:radius")–>r].
• Input: What is the center of the star
graph?
• Output: The center of the star graph in-
cludes node(s) [GR(GL("gpr", "star_graph"),
"toolx:center")–>r].
• Input: What is the length of shortest path
between node #5 and node #0 in the octahe-
dral graph?
• Output: In the octahedral graph, the
length of shortest path between node #5 and
node #0 is [GR(GL("gpr", "octahedral_graph"),
"toolx:shortest_path", "node#5", "node#0")–
>r].
• Input: What is the diameter of the bino-
mial tree?
• Output: The diameter of the binomial
tree is [GR(GL("gpr", "binomial_tree"),
"toolx:diameter")–>r].
• Input: What is the periphery of the house
x graph?
• Output: The periphery of the house
x graph includes node(s) [GR(GL("gpr",
"house_x_graph"), "toolx:periphery")–>r].
2. Bibliographic Network Reasoning Q&A Prompt Exam-
ples:
• Input: What is the topic of paper #83826
in the cora bibliographic network?
• Output: The topic of paper #83826 in the
cora bibliographic network is [GR(GL("cora"),
"graph_bert:topic", paper#83826)–>r].
• Input: What is the topic of paper #5832 in
the pubmed bibliographic network?
• Output: The topic of paper #5832 in
the pubmed bibliographic network is
[GR(GL("pubmed"), "graph_bert:topic",
paper#5832)–>r].
• Input: What is the topic of paper #3230 in
the citeseer bibliographic network?



### Claim 127/179

#### Claim Text
It is well know n that the efficiency of HHG rapidly decreases with increasing ellipticity of the driver laser rad iation , and the ellipticity of high harmonics, as a rule, does not exceed the laser ellipticity [1 4].

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 2):

into three sub-categories (§2.2). In addition, we
discuss the unique challenges of hallucination in
LLMs (§2.3), and compare hallucination with
other prevalent problems that are frequently en-
countered in the realm of LLMs (§2.4).
2.1 Large Language Models
An important category of LLMs is autoregressive
language models (Radford et al., 2019; Chowd-
hery et al., 2022; Touvron et al., 2023a, inter
alia). These models take Transformers (Vaswani
et al., 2017) as the backbone, and predict the next
token based on previous tokens. 1 Prior to the
widespread adoption of Transformers, autoregres-
sive language models were built on the backbones
of n-grams (Bickel et al., 2005; Pauls and Klein,
2011) and recurrent neural networks (Mikolov
et al., 2010), and have been applied to various
NLG tasks such as summarization (Nallapati et al.,
2017) and dialogue generation (Chen et al., 2017).
Transformer-based LLMs have demonstrated
exceptional performance across tasks, and have
therefore shifted NLP from a paradigm centered
on task-specific solutions to general-purpose pre-
training (Devlin et al., 2019; Radford et al., 2019).
The pretrained models are optimized on various
self-supervision objectives (Devlin et al., 2019;
Raffel et al., 2020; Lewis et al., 2020a, inter
alia), using large-scale unlabeled corpora. Sub-
sequently, the models are fine-tuned with labeled
data on target downstream tasks. Representations
from the pretrained models can typically reduce
the demand for annotated data and achieve sig-
nificant performance improvement across down-
stream tasks (Qiu et al., 2020; Min et al., 2021;
Li et al., 2022b, inter alia).
In addition to performance improvement on
downstream tasks, recent work has found that scal-
ing up pretrained language models—both in terms
of model parameter count and the volume of pre-
training data—enables some remarkable abilities,
including in-context learning (Brown et al., 2020),
reasoning (Wei et al., 2022), and instruction fol-
lowing (Ouyang et al., 2022). The community has,
to some extent, popularized the term large lan-
guage models (LLMs) to differentiate them from
their smaller counterparts. Notably, LLMs exhibit
the potential to accurately comprehend human in-
structions and efficiently tackle a variety of com-
1Another variant of language models predicts masked to-
kens in a corrupted sequence (Devlin et al., 2019; Liu et al.,
2019; Lan et al., 2019, inter alia).
plex tasks with only minimal or even no supervi-
sion (OpenAI, 2023a,b; Touvron et al., 2023b).
2.2 What is LLM Hallucination
While LLMs have demonstrated remarkable per-
formances, they still inevitably encounter different
problems in practical applications, where halluci-
nation is one of the most significant issues among
them. The term hallucination has already been
widely adopted in the NLP community before the
emergence of LLM, typically referring to gen-
erating nonsensical or unfaithful to the provided
source content (Ji et al., 2023).
We argue that the definition appears to have
considerably expanded due to the versatility of
LLMs. To this end, we categorize hallucination
within the context of LLMs as follows:
• Input-conflicting hallucination, where LLMs
generate content that deviates from the source
input provided by users;
• Context-conflicting hallucination, where LLMs
generate content that conflicts with previously
generated information by itself;
• Fact-conflicting hallucination, where LLMs
generate content that is not faithful to estab-
lished world knowledge.
We present examples for each type of hallucina-
tions in Table 1, and discuss them in detail below.
Input-conflicting hallucination. This type of
hallucination arises when the content generated by
LLMs deviates from user input. Typically, user
input for LLMs comprises two components: task
instruction (e.g., user prompt for summarization)
and task input (e.g., document to be summarized).
The contradiction between LLM response and task
instructions typically reflects a misunderstanding
of user intents. In contrast, when the contradiction
arises between the generated content and task in-
put, the hallucination is in line with the conven-
tional definition in specific NLG tasks, such as
machine translation (Lee et al., 2019) and sum-
marization (Maynez et al., 2020; Pu et al., 2023).
For instance, the first example in Table 1 appears
to highlight a contradiction between the gener-
ated content and task input: when users request
the LLM to generate a summary, the LLM incor-
rectly replaces the person’s name in its response
(Hill→Lucas), even though the general form can
indeed be perceived as a suitable summary.
3



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 4):

Type Turn Example
Ambiguity User What is the capital of France?
Model The capital of a European country.
Incompleteness
User How can I replace a flat tire on my car?
Model To replace a flat tire, there are four steps. First, you’ll need to use a jack
to lift the car. Second, please remove the lug nuts and flat tire.
Bias
User Tell me about a typical elementary school teacher.
Model A typical elementary school teacher is a woman who is patient, nurtur-
ing, and skilled at managing a classroom full of young children.
Under-informativeness User Introduce the most popular task in NLP in 2022.
Model Sorry, I don’t know. Maybe you can check external search engines.
Table 2: Examples of various problems that LLMs may expose, in addition to hallucinations.
pre-training corpora are automatically collected
from the web and often contain a significant
amount of fabricated, outdated, or biased informa-
tion (Penedo et al., 2023). Such inadequate data
may lead LLMs to generate hallucinated content.
The large data scale may also increase the diffi-
culty of applying data-centric approaches to miti-
gate the hallucination in LLMs.
Versatility of LLMs. Conventional NLG mod-
els are typically designed for a single task, and
thus, hallucination studies on them are usually
task-specific (Maynez et al., 2020; Wang and Sen-
nrich, 2020; Xiao and Wang, 2021); however, cur-
rent LLMs are expected to excel in multi-task,
multi-lingual, and multi-domain settings (Bang
et al., 2023; Chang et al., 2023). This expectation
poses thorny challenges for both the evaluation
and mitigation of LLM hallucinations. In terms
of evaluation, LLMs are more commonly used for
free-form text generation, and the lack of deter-
ministic references in this setting complicates the
automatic detection of hallucinations. Therefore,
it is crucial to establish a comprehensive, reliable,
and automatic evaluation benchmark. Regarding
mitigation, the proposed methods should be ro-
bustly effective, maintaining decent performance
when being applied to various scenarios.
Invisibility of errors. Compared to traditional
NLG models, LLMs possess a significantly en-
hanced writing capability and store a larger vol-
ume of knowledge. Consequently, the false in-
formation hallucinated by LLMs often appears
highly plausible, to the extent that even humans
may feel hard to detect. This amplifies the diffi-
culty in detecting and reducing input- and context-
conflicting hallucination, as we can no longer re-
sort to simple superficial patterns. Regarding fact-
conflicting hallucinations, we also need to con-
sider leveraging more knowledge sources for veri-
fication. These factors collectively introduce sub-
stantial new challenges.
2.4 Other Problems in LLMs
Besides hallucination, LLMs also present other
problems. We outline some common issues below
and present examples in Table 2 to help readers
distinguish between them and hallucination.
Ambiguity. This type of issue arises when the
LLM response is ambiguous, lending itself to mul-
tiple interpretations. The response may not neces-
sarily be incorrect, but it falls short of providing a
useful answer to the user question (Tamkin et al.,
2022). The first example in Table 2 exemplifies
this issue. The desired answer is ‘Paris’, yet the
LLM provides an ambiguous response.
Incompleteness. The incompleteness issue oc-
curs when the generated response is incomplete or
fragmented. As demonstrated in the second exam-
ple in Table 2, the LLM only informs users of the
first two steps in a four-step process for replacing
a tire, resulting in an incomplete explanation.
Bias. Bias in LLMs pertains to the manifestation
of unfair or prejudiced attitudes within the gener-
ated text. These biases may originate from train-
ing data, which frequently encompasses historical
texts, literature, social media content, and other
sources. Such sources may inherently mirror so-
5



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 16):

inference process (Manakul et al., 2023). Typi-
cally, it refers to the confidence level of model out-
puts (Jiang et al., 2021; Huang et al., 2023a; Duan
et al., 2023). Uncertainty can assist users in de-
termining when to trust LLMs. Provided that the
uncertainty of LLM responses can be accurately
characterized, users can filter out or rectify LLMs’
claims with high uncertainty since such claims are
more prone to be fabricated ones (Lin et al., 2023).
Generally speaking, methods for estimating the
uncertainty of LLMs can be categorized into three
types (Xiong et al., 2023), as listed below. To fa-
cilitate understanding, we also present illustrative
examples for these methods in Figure 5.
(1) Logit-based estimation. The first method is
the logit-based method, which requires ac-
cess to the model logits and typically mea-
sures uncertainty by calculating token-level
probability or entropy. This method has been
widely used in the machine learning commu-
nity (Guo et al., 2017).
(2) Verbalize-based estimation. The second is
the verbalize-based method, which involves
directly requesting LLMs to express their un-
certainty, such as using the following prompt:
“Please answer and provide your confidence
score (from 0 to 100). ” This method is
effective due to the impressive verbal and
instruction-following capabilities of LLMs.
Notably, Xiong et al. (2023) further suggest
using chain-of-thoughts prompts (Wei et al.,
2022) to enhance this method.
(3) Consistency-based estimation. The third is
the consistency-based method (Wang et al.,
2022; Shi et al., 2022; Zhao et al., 2023a).
This method operates on the assumption that
LLMs are likely to provide logically incon-
sistent responses for the same question when
they are indecisive and hallucinating facts.
Several recent studies have leveraged uncer-
tainty estimation for detecting and mitigating hal-
lucinations in LLMs. S ELF CHECK GPT (Man-
akul et al., 2023) is the first framework to detect
LLM hallucinations based on uncertainty mea-
surement in a zero-resource and black-box set-
ting. They employ a consistency-based approach
for uncertainty estimation. A non-trivial chal-
lenge in S ELF CHECK GPT is determining how to
measure the consistency of different responses.
Manakul et al. (2023) perform experiments with
BERTScore (Zhang et al., 2019), QA-based met-
rics (Wu and Xiong, 2023) and n-gram metrics.
They finally find that a combination of these ap-
proaches yields the best results. Mündler et al.
(2023) directly utilize an additional LLM to as-
sess whether two LLM responses are logically
contradictory given the same context (Luo et al.,
2023b), which means at least one of them is hal-
lucinated. Consequently, they employ another
LLM to revise such self-contradictory hallucina-
tions from two responses. Agrawal et al. (2023)
further adopt the verbalize-based method to eval-
uate the hallucination rate of LLMs for fabricat-
ing references. Varshney et al. (2023), on the
other hand, use the logit-based method to detect
false concepts in LLMs’ responses with high un-
certainty. They then fix such content with auxil-
iary retrieval-augmented LLMs.
Besides, Zhao et al. (2023b) present a Pareto
optimal self-supervision framework. This frame-
work utilizes available programmatic supervision
to assign a risk score to LLM responses, which can
serve as an indicator of hallucinations. Luo et al.
(2023a) introduce a pre-detection self-evaluation
technique, which aims to evaluate the familiarity
of LLMs with the concepts in user prompts and
prevent the generation of content about those un-
familiar concepts.
Summary & Discussion. Exploiting uncer-
tainty to identify and mitigate LLM hallucinations
is a promising research direction today. Three pri-
mary approaches exist for estimating the uncer-
tainty of LLMs, each presenting its unique chal-
lenges. Firstly, the logit-based method is becom-
ing less applicable for modern commercial LLMs
as they are usually closed-source and black-box,
rendering their output logits inaccessible. Sec-
ondly, regarding the verbalize-based method, re-
searchers have observed that LLMs tend to display
a high degree of overconfidence when expressing
their confidence (Xiong et al., 2023). Thirdly, the
effective measurement of the consistency of differ-
ent responses remains an unresolved issue in the
consistency-based method (Manakul et al., 2023).
We believe that leveraging uncertainty is crucial in
developing trustworthy LLMs and encourage fu-
ture research to address the aforementioned chal-
lenges in this field.
17



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 1):

Parametric MemorizationPre-training
SFT
RLHF
Inference
Overinflated Self-confidence
Misleading Alignment
Generation-time Risk
Curating Training Data
Honesty-oriented RL
Decoding StrategyKnowledge Retrieve
Honesty-oriented SFT
Exploiting UncertaintySources (Sec. 4) Mitigation (Sec. 5)
Definition (Sec. 2)Input-Conflicting Hallucination
TimeLine
Input-Conflicting Benchmark:BEGIN, QMSum, FENMT,FEQA…
Benchmark (Sec. 3) Context-Conflicting HallucinationFact-Conflicting Hallucination
Context-Conflicting Benchmark:HADES…Fact-Conflicting Benchmark:TruthfulQA,,FActScore,HaluEval,FACTOR…
Figure 2: The overview structure of this paper: We initially categorize LLM hallucinations into three distinct types
and then introduce corresponding evaluation benchmarks. Subsequently, we explore the source of hallucinations
and discuss mitigation strategies throughout the life cycle of LLMs (pre-training→SFT→RLHF→inference).
training uses trillions of tokens obtained from
the web, making it difficult to eliminate fabri-
cated, outdated or biased information;
2. Versatility of LLMs : general-purpose LLMs
are expected to excel in cross-task, cross-
lingual, and cross-domain settings, posing
challenges for comprehensive evaluation and
mitigation of hallucination.
3. Imperceptibility of errors: as a byproduct of
their strong abilities, LLMs may generate false
information that initially seems highly plausi-
ble, making it challenging for models or even
humans to detect hallucination.
In addition, the RLHF process (Ouyang et al.,
2022), the vague knowledge boundary (Ren et al.,
2023) and the black-box property of LLMs (Sun
et al., 2022) also complicate the detection, expla-
nation, and mitigation of hallucination in LLMs.
There has been a notable upsurge in cutting-edge
research dedicated to addressing the aforemen-
tioned challenges, which strongly motivates us to
compile this survey.
We organize this paper as follows, as also
depicted in Figure 2. We first introduce the
background of LLMs and offer our definition of
hallucination in LLMs (§2). Next, we introduce
relevant benchmarks and metrics (§3). Subse-
quently, we discuss potential sources of LLM hal-
lucinations (§4), and provide an in-depth review of
recent work towards addressing the problem (§5).
Finally, we present forward-looking perspectives
(§6). We will consistently update the related
open-source materials, which can be accessed at
https://github.com/HillZhang1999/
llm-hallucination-survey.
2 Hallucination in the Era of LLM
We begin this section by overviewing the history
of LLMs (§2.1). Next, we present our defini-
tion of LLM hallucination, by breaking it down
2



### Claim 128/179

#### Claim Text
Ion irradiation is thus widely employed for comparative studies to investigate the effect of irradiation conditions, such as damage rate and temperature .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[61]_2212.10509.pdf (Page 5):

Figure 3: Retrieval recall for one-step retriever (OneR) and IRCoT instantiated from Flan-T5-XXL (left) and GPT3
(right) models. IRCoT outperforms OneR for both models and all datasets.
Figure 4: Answer F1 for ODQA model made using (i) no retriever (NoR QA) (ii) one-step retriever (OneR QA) and
(iii) IRCoT QA instantiated from Flan-T5-XXL (left) and GPT3 (right) models. IRCoT QA outperforms OneR QA
and NoR QA for both models on all datasets, except for GPT3 on IIRC.
Flan-T5-XXL and GPT3 LMs. For both models,
IRCoT significantly outperforms one-step retrieval
across all datasets. For Flan-T5-XXL, IRCoT im-
proves our recall metric relative to one-step re-
trieval, on HotpotQA by 7.9, on 2WikiMultihopQA
by 14.3, on MuSiQue by 3.5, and on IIRC by 10.2
points. For GPT3, this improvement is by 11.3, 22.6,
12.5, and 21.2 points, respectively.
IRCoT QA outperforms NoR and OneR QA.
Fig. 4 compares ODQA performance using
NoR, OneR and IRCoT retriever made from
Flan-T5-XXL and GPT3 LMs. For Flan-T5-XXL,
IRCoT QA outperforms OneR QA on HotpotQA
by 9.4, on 2WikiMultihopQA by 15.3, on MuSiQue
by 5.0 and IIRC by 2.5 F1 points. For GPT3, the
corresponding numbers (except for IIRC) are 7.1,
13.2, and 7.1 F1 points. For GPT3, IRCoT doesn’t
improve the QA score on IIRC, despite signifi-
cantly improved retrieval (21 points as shown in
Fig. 3). This is likely because IIRC relevant knowl-
edge may already be present in GPT3, as also ev-
idenced by its NoR QA score being similar. For
other datasets and model combinations, NoR QA is
much worse than IRCoT QA, indicating the limits
of the models’ parametric knowledge.
IRCoT is effective in OOD setting.Since CoT
may not always be easy to write for new datasets,
we evaluate NoR, OneR, and IRCoT on generaliza-
tion to new datasets, i.e. OOD setting. To do so,
we use prompt demonstrations from one dataset to
evaluate on another dataset.9 For all pairs of the
datasets10 and for both Flan-T5-XXL and GPT3, we
find the same trend as in the IID setting: IRCoT re-
trieval outperforms OneR (Fig. 5), and IRCoT QA
outperforms both OneR QA and NoR QA (Fig. 6).
IRCoT generates CoT with fewer factual errors.
To assess whether our approach also improves the
factuality of generated CoTs, we manually anno-
tated CoTs generated by NoR QA, OneR QA, and
IRCoT QA using GPT3 for 40 randomly sampled
questions from each of the four datasets. We con-
sidered CoT to have a factual error if at least one
9We use the evaluation dataset’s corpus for retrieval.
10We skip IIRC in this exploration as the task is structured
a bit differently and requires special handling (see App. B).



Source: data\tc16_2312.10997v5\referenced_papers\[61]_2212.10509.pdf (Page 6):

Figure 5: Retrieval recall for OneR and IRCoT using Flan-T5-XXL (Left) and GPT3 (Right) in out-of-distribution
(OOD) setting. HQ (HotpotQA), 2W (2WikiMultihopQA), MQ (MuSiQue). The result X →Y indicates prompt
demonstrations are from dataset X and evaluation is on dataset Y .IRCoT outperforms OneR in such an OOD setting.
Figure 6: Answer F1 for NoR QA, OneR QA and IRCoT QA using Flan-T5-XXL (Left) and GPT3 (Right) in
out-of-distribution (OOD) setting. HQ (HotpotQA), 2W (2WikiMultihopQA), MQ (MuSiQue). The result X→Y
indicates prompt demonstrations are from dataset X and evaluation is on dataset Y . IRCoTQA outperforms OneR
QA and NoR QA in such OOD setting.
Figure 7: Number of questions, out of 40, where CoT
generated by GPT3 using different methods has at least
1 factual error. Factual errors: IRCoT < OneR < NoR.
of the facts11 is not true.12 As Fig. 7 shows, NoR
makes the most factual errors, OneR makes fewer,
11all sentences before the final “answer is:” sentence.
12Note that factual error doesn’t necessarily mean the pre-
dicted answer is incorrect and vice-versa. This is because the
model can generate a wrong answer despite all correct facts,
and vice-versa. We also account for the possibility of answer
annotation errors in the original datasets.
and IRCoT the least. In particular, IRCoT reduces
the factual errors over OneR by 50% on HotpotQA
and 40% on 2WikiMultihopQA.
Table 2 illustrates how the CoT predictions for
different methods vary qualitatively. Since NoR
relies completely on parametric knowledge, it often
makes a factual error in the first sentence, which
derails the full CoT. OneR can retrieve relevant
information closest to the question and is less likely
to make such errors early on, but it still makes
errors later in the CoT. IRCoT, on the other hand,
is often able to prevent such errors in each step.
IRCoT is also effective for smaller models.To
see how effective IRCoT is at different LM sizes,
we show the scaling plots in Fig. 8. 13 We com-
pare the recall for OneR andIRCoT using Flan-T5
{base (0.2B), large (0.7B), XL (3B), XXL (11B)},
and GPT3 code-davinci-002 (175B). IRCoT
with even the smallest model (0.2B) is better than
13We skip IIRC here as the smaller models are not good at
identifying Wikipedia titles from a paragraph and a question
which is necessary for IIRC (see App. B).



Source: data\tc16_2312.10997v5\referenced_papers\[61]_2212.10509.pdf (Page 7):

Figure 8: Retrieval recall for OneR (bottom) and IRCoT (top) for LMs of increasing sizes: Flan-T5 {base (0.2B),
large (0.7B), XL (3B), XXL (11B)} and GPT3 (175B) on HotpotQA, 2WikiMultihopQA, MuSiQue. IRCoT
outperforms OneR for all model sizes, including the 0.3B model, and the difference roughly grows with model size.
Note: OneR doesn’t use LM in its retrieval and so has a fixed score.
Figure 9: Answer F1 for ODQA models made using OneR (bottom) and IRCoT (top) for LMs of increasing sizes:
Flan-T5 {base (0.2B), large (0.7B), XL (3B), XXL (11B)} and GPT3 (175B) on HotpotQA, 2WikiMultihopQA and
MuSiQue. IRCoT QA outperforms OneR QA for all model sizes except for the smallest, 0.3B. IRCoT with 3B
model even outperforms OneR with 58X larger GPT3 model showing the value of improved retrieval.
OneR, and the performance roughly improves with
the model size. This shows the CoT generation
capabilities of even small models can be leveraged
for improving retrieval. Furthermore, we show the
effect of model size on the QA score in Fig. 9. For
all sizes except the smallest (0.2B), we see IRCoT
QA is better than OneR QA. Moreover, IRCoT
with a 3B model even outperforms OneR and NoR
with a 58X larger 175B GPT3 model in all datasets.
IRCoT is SOTA for few-shot multistep ODQA.14
We compare IRCoT QA with five recent ap-
proaches to using LLMs for ODQA: Internet-
Augmented QA (Lazaridou et al., 2022), RE-
CITE (Sun et al., 2022) ReAct (Yao et al., 2022),
SelfAsk (Press et al., 2022), and DecomP (Khot
et al., 2022). Although these are not head-to-head
comparisons as different methods use different
APIs, knowledge sources, and even LLMs (see
App. C for details), it is still informative to ex-
plore, in a leaderboard-style fashion, how IRCoT
performs relative to the best numbers published for
these recent systems.
14App. §C reports updated SOTA numbers, including con-
temporaneous and newer works.
Model HpQA Br HpQA 2WikiMQA MQ 2H
InterAug − | − 30.3 | − − | − − | −
RECITE − | − 37.1 | 48.4 − | − − | −
ReAct − | − 35.1 | − − | − − | −
SelfAsk − | − − | − 40.1 | − 15.2 | −
DecomP − | 50.0 − | − − | 59.3 − | −
IRCoT QA 45.8 | 58.5 49.3 | 60.7 57.7 | 68.0 34.2 | 43.8
Table 1: Comparison with other LLM-based ODQA
systems on EM and F1 scores. ‘ −’: score is unavail-
able. HpQABr: Bridge questions subset of HotpotQA.
MQ2H: MuSiQue 2-hop questions. IRCoT QA with
GPT3 (ours) outperforms other systems by a large mar-
gin. Note: Comparisons aren’t head-to-head as dis-
cussed in the text. App. §C reports updated SOTA num-
bers, including contemporaneous and newer works.
As shown in Table 1, IRCoT QA significantly
outperforms all of these recent systems by a large
margin, setting a new state of the art in terms of
what’s achievable via retrieval-augmented LLMs
(without supervised training).
6 Conclusions
Chain-of-thought prompting has significantly im-
proved LLMs’ ability to perform multi-step reason-



Source: data\tc16_2312.10997v5\referenced_papers\[61]_2212.10509.pdf (Page 4):

corpus for our open-domain setting (see App. A
for details). For each dataset, we use 100 randomly
sampled questions from the original development
set for tuning hyperparameters, and 500 other
randomly sampled questions as our test set.
4.1 Models
Retriever. We use BM25 (Robertson et al., 2009)
implemented in Elasticsearch6 as our base retriever.
We compare two retriever systems:
(i) One-step Retriever (OneR)uses the ques-
tion as a query to retrieve K paragraphs. We select
K ∈ {5,7,9,11,13,15} that’s best on the dev set.
(ii) IRCoT Retriever is our method de-
scribed in §3. We use BM25 as its underly-
ing retriever and experiment with OpenAI GPT3
(code-davinci-002) (Brown et al., 2020; Ouyang
et al., 2022; Chen et al., 2021) and Flan-T5 (Chung
et al., 2022) of different sizes as its CoT generator.
For demonstrating in-context examples to these
LMs, we wrote CoTs for 20 questions for all the
datasets (see App. §G). We then create 3 demon-
stration (“training”) sets by sampling 15 questions
each for each dataset. For each experiment, we
search for the best hyperparameters for the dev set
using the first demonstration set and evaluate each
demonstration set on the test set using the selected
hyperparameters. We report the mean and standard
deviation of these 3 results for each experiment.
At test time, we pack as many demonstrations
as possible within the model’s context length limit.
The context limit for GPT3 (code-davinci-002)
is 8K word pieces. Flan-T5-* doesn’t have any
hard limit as it uses relative position embeddings.
But we limit Flan-T5’s context to 6K word pieces,
which is the maximum we could fit in the memory
of our 80G A100 GPUs.
IRCoT Retriever has one key hyperparameter:
K ∈ {2,4,6,8}, the number of paragraphs to re-
trieve at each step. Additionally, when creating
“training” demonstrations for IRCoT’s Reasoner
module, we use gold paragraphs and a smaller num-
ber M ∈ {1,2,3} of distractor paragraphs (§3.1).
Retrieval Metric:We allow a maximum of 15
paragraphs for all retriever systems and measure
the recall of the gold paragraphs among the re-
trieved set of paragraphs. We search for the hyper-
parameter K (and M for IRCoT) that maximizes
the recall on the dev set and use it on the test set.
6https://www.elastic.co/
The reported metric can thus be viewed as thefixed-
budget optimal recall for each system considered.7
QA Reader. To implement the reader, we use
the same LMs as used in the reason-step of
IRCoT Retriever. We found that QA readers im-
plemented with Flan-T5-* perform better with the
Direct Prompting strategy and GPT3 performs bet-
ter with CoT Prompting strategy (see App. E).
Hence we use Direct prompting strategy for QA
with Flan-T5-* and CoT with GPT3 for the experi-
ments.8
The QA reader has one hyperparameter M: the
number of distractor paragraphs in the in-context
demonstrations. We search for M in {1,2,3}.
When used in conjunction with IRCoT retriever
M is tied for the CoT generator and the reader.
Open-Domain QA (ODQA) Models.Putting re-
trievers and readers together, we experiment with
ODQA models constructed from the various lan-
guage models denoted as OneR QAand IRCoT
QA. For IRCoT QA, the choice of LM for the CoT
generator and the reader is kept the same. We also
experiment with retriever-less QA readersNoR QA
to assess how well LMs can answer the question
from their parametric knowledge alone. To select
the best hyperparameters for the ODQA model,
we search for the hyperparameters K and M that
maximize the answer F1 on the development set.
IIRC is structured slightly differently from the
other datasets, in that its questions are grounded
in a main passage and other supporting paragraphs
come from the Wikipedia pages of entities men-
tioned in this passage. We slightly modify the re-
trievers and readers to account for this (see App. B).
5 Results
IRCoT retrieval is better than one-step.Fig. 3
compares OneR with IRCoT retrievers made from
7Note that our retrieved documents are not ranked, mak-
ing standard information retrieval metrics such as MAP and
DCG inapplicable. Further, we can only limit the number of
retrieved paragraphs per step to K. Since the total number
of reasoning steps varies for questions, and in some cases,
we don’t even obtain all K paragraphs in a given step, the
total number of retrieved paragraphs also varies (even though
capped at 15). This makes Recall@k, Precision@k, etc., also
not applicable as metrics for any given k.
8IRCoT, by construction, produces a CoT as a part of its
retrieval process. Thus, instead of having a separate post-hoc
reader, one can also just extract the answer from the CoT
generated during retrieval. However, we found this to be a
suboptimal choice, so we always use a separate reader (see
App. F).



Source: data\tc16_2312.10997v5\referenced_papers\[61]_2212.10509.pdf (Page 13):

Model HpQA Br HpQA 2WikiMQA MQ 2H MQ
InterAug (Lazaridou et al., 2022) − | − 30.3 | − − | − − | − − | −
RECITE (Sun et al., 2022) − | − 37.1 | 48.4 − | − − | − − | −
ReAct (Yao et al., 2022) − | − 35.1 | − − | − − | − − | −
SelfAsk (Press et al., 2022) − | − − | − 40.1 | − 15.2 | − − | −
DecomP (Khot et al., 2022) − | 50.0 − | − − | 59.3 − | − − | −
DecomP (Khot et al., 2023) * − | − − | 53.5 − | 70.8 − | − − | 30.9
DSP (Khattab et al., 2023) * − | − 51.4 | 62.9 − | − − | − − | −
IRCoT QA (ours) 45.8 | 58.5 49.3 | 60.7 57.7 | 68.0 34.2 | 43.8 26.5 | 36.5
Table 3: Extended comparison with published LLM-based ODQA systems (as of May 25, 2023) on EM and
F1 scores (with new numbers marked with *). ‘ −’: score is unavailable. HpQA Br: Bridge questions subset of
HotpotQA. MQ2H: MuSiQue 2-hop questions. IRCoT remains SOTA for MuSiQue and is close to SOTA for
HotpotQA and 2WikiMultihopQA. Note the comparisons here are not head-to-head as discussed in the text.
Flan-T5-XXL GPT3
Model HotpotQA 2WikiMQA MuSiQue IIRC HotpotQA 2WikiMQA MuSiQue IIRC
ZeroR QA Direct 25.3± 0.3 32.7± 0.3 13.7± 0.3 28.9± 0.3 41.0± 1.1 38.5± 1.1 19.0± 1.2 40.9± 0.7
CoT 22.9 ± 0.1 31.7± 1.5 10.3± 0.5 24.4± 0.1 47.5± 0.4 41.2± 1.0 25.2± 1.2 52.1± 0.1
OneR QA Direct 49.7± 0.5 51.2± 0.3 25.8± 0.6 40.0± 1.3 50.7± 0.1 46.4± 2.9 20.4± 0.3 40.1± 0.9
CoT 43.1 ± 0.7 47.8± 0.9 17.6± 0.2 34.5± 1.5 53.6± 0.7 54.8± 2.1 29.4± 0.8 49.8± 2.3
IRCoT QA Direct 59.1± 0.9 66.5± 1.4 30.8± 0.2 42.5± 2.1 60.6± 1.0 63.5± 2.7 36.0± 0.5 47.9± 2.3
CoT 52.0 ± 0.6 55.1± 1.0 24.9± 1.0 36.5± 1.3 60.7± 1.1 68.0± 1.5 36.5± 1.2 49.9± 1.1
Table 4: Answer F1 for different ODQA models made from NoR, One and IRCoT retrievals, and Direct and
CoT prompting readers. For Flan-T5-XXL, Direct prompting is a better choice for the reader, and for GPT3, CoT
prompting is a better choice for the reader. Hence, we make different reader choices for Flan-T5 and GPT3 for the
experiments in the main paper. Note that IRCoT QA > OneR QA > ZeroR QA holds up regardless of this choice.
DSP (Khattab et al., 2023) provides a way to pro-
grammatically define interactions between LLM
and retrieval for ODQA (e.g., via question decom-
position), bootstrap demonstrations for such a pro-
gram, and use them to make the answer prediction.
It uses GPT3.5 LLM with ColBERT-based retrieval.
Since most of these methods use different knowl-
edge sources or APIs and are built using different
LLMs and retrieval models, it’s difficult to make a
fair scientific comparison across these systems. Ad-
ditionally, the evaluations in the respective papers
are on different random subsets (from the same
distribution) of test instances.
Despite these differences, it is still informative to
explore, in a leaderboard-style fashion, howIRCoT
performs relative to the best numbers published
for these recent systems. Table 3 shows results
from different systems, including contemporane-
ous and newer numbers. The two new systems in
this table (relative to Table 1) are DecomP (newer
version) and DSP. While IRCoT remains SOTA on
MuSiQue, DSP outperforms it on HotpotQA by 2.0
points and the newer version of Decomp outper-
forms IRCoT on 2WikiMultihopQA by 2.8 points.
We speculate DecomP performs well on 2WikiMul-
tihopQA because it has only a few easy-to-predict
decomposition patterns, which DecomP’s question
decomposition can leverage. The lack of such pat-
terns in HotpotQA and MuSiQue causes it to un-
derperform compared to IRCoT. Lastly, it will be
useful to assess whether DSP, which is hardcoded
for 2-hop questions like that of HotpotQA, will
work well for a dataset with a varied number of
hops like that of MuSiQue. We leave this further
investigation to future work.
D Additional CoT Generation Examples
Table 5 provides illustrations, in addition to the
ones provided in Table 2, for how the CoT gen-
erations for NoR QA, OneR QA, and IRCoT QA
methods vary. This gives an insight into how IR-
CoT improves QA performance. Since NoR re-
lies completely on parametric knowledge, it often
makes a factual error in the first sentence, which de-
rails the full reasoning chain. Some of this factual
information can be fixed by OneR, especially infor-
mation closest to the question (i.e., can be retrieved
using the question). This is insufficient for fixing



### Claim 129/179

#### Claim Text
Pan & Banerjee tracked up to 160 moving particles in turbulent channel flow), it is nowadays feasible to describe O(106) particles with resources available at many HPC centers .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 19):

Appendices
A Summary of Power Laws
For easier reference, we provide a summary below of the key trends described throughout the paper.
Parameters Data Compute Batch Size Equation
N ∞ ∞ Fixed L(N) = (Nc/N)αN
∞ D Early Stop Fixed L(D) = (Dc/D)αD
Optimal ∞ C Fixed L(C) = (Cc/C)αC
(naive)
Nopt Dopt Cmin B ≪Bcrit L(Cmin) =
(
Cmin
c /Cmin
)αmin
C
N D Early Stop Fixed L(N,D) =
[(Nc
N
)αN
αD + Dc
D
]αD
N ∞ Ssteps B L(N,S) =
(Nc
N
)αN
+
(
Sc
Smin(S,B)
)αS
Table 4
The empirical ﬁtted values for these trends are:
Power Law Scale (tokenization-dependent)
αN = 0.076 Nc = 8.8 ×1013 params (non-embed)
αD = 0.095 Dc = 5.4 ×1013 tokens
αC = 0.057 Cc = 1.6 ×107 PF-days
αmin
C = 0.050 Cmin
c = 3.1 ×108 PF-days
αB = 0.21 B∗= 2.1 ×108 tokens
αS = 0.76 Sc = 2.1 ×103 steps
Table 5
The optimal parameters for compute efﬁcient training are given by:
Compute-Efﬁcient Value Power Law Scale
Nopt = Ne ·CpN
min pN = 0.73 Ne = 1.3 ·109 params
B ≪Bcrit = B∗
L1/αB = BeCpB
min pB = 0.24 Be = 2.0 ·106 tokens
Smin = Se ·CpS
min (lower bound) pS = 0.03 Se = 5.4 ·103 steps
Dopt = De ·CpD
min (1 epoch) pD = 0.27 De = 2·1010 tokens
Table 6
B Empirical Model of Compute-Efﬁcient Frontier
Throughout this appendix all values of C,S, and αC are adjusted for training at the critical batch size Bcrit.
We have left off the ‘adj’ label to avoid cluttering the notation.
B.1 Deﬁning Equations
The power-law ﬁt to the learning curves implies a simple prescription for compute-efﬁcient training. In this
appendix, we will derive the optimal performance, model size, and number of training steps as a function of
20



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 16):

The intersection point is sensitive to 
the precise power-law parameters
Figure 15 Far beyond the model sizes we study empirically, we ﬁnd a contradiction between our equations
for L(Cmin) and L(D) due to the slow growth of data needed for compute-efﬁcient training. The intersection
marks the point before which we expect our predictions to break down. The location of this point is highly
sensitive to the precise exponents from our power-law ﬁts.
6.3 Contradictions and a Conjecture
We observe no signs of deviation from straight power-law trends at large values of compute, data, or model
size. Our trends must eventually level off, though, since natural language has non-zero entropy.
Indeed, the trends for compute-efﬁcient training described in this section already contain an apparent contra-
diction. At scales several orders of magnitude above those documented here, the performance predicted by
the L(Cmin) scaling law decreases below what should be possible given the slow growth in training data with
compute. This implies that our scaling laws must break down before this point, but we conjecture that the
intersection point has a deeper meaning: it provides an estimate of the point at which Transformer language
models reach maximal performance.
Since the amount of data used by compute-efﬁcient training grows slowly with the compute budget, the
performance predicted by L(Cmin) eventually hits a lower bound set by theL(D) power law (see Figure 15).
Let us work this out in more detail.
To keep overﬁtting under control, the results of Section 4 imply that we should scale the dataset size as
D∝N0.74 ∝C0.54
min (6.6)
where we have used the compute-efﬁcient N(Cmin) from Figure 14.
Let us compare this to the data requirements of compute-efﬁcient training. If we train at the critical batch
size (i.e. C = 2Cmin) and never re-use data during training, we ﬁnd that data usage grows with compute as
D(Cmin) = 2Cmin
6N(Cmin) ≈
(
4 ×1010 tokens
)
(Cmin/PF-Day)0.26 (6.7)
This is the maximum rate at which the dataset size can productively grow with compute, since it means that
we are only training for a single epoch. But it grows the dataset much more slowly than in Equation (6.6).
It appears to imply that compute-efﬁcient training will eventually run into a problem with overﬁtting, even if
the training process never re-uses any data!
According to Figure 1, we expect that when we are bottlenecked by the dataset size (ie by overﬁtting), the
loss should scale asL(D) ∝D−0.095. This implies that the loss would scale with compute asL(D(Cmin)) ∝
C−0.03
min once we are data-limited. Once again, we have a contradiction, as this will eventually intersect with
our prediction for L(Cmin) from Figure 13, where we found a scaling L(Cmin) ∝C−0.050
min .
The intersection point of L(D(Cmin)) and L(Cmin) occurs at
C∗∼104 PF-Days N∗∼1012 parameters, D ∗∼1012 tokens, L ∗∼1.7 nats/token (6.8)
though the numerical values are highly uncertain, varying by an order or magnitude in either direction de-
pending on the precise values of the exponents from the power-law ﬁts. The most obvious interpretation is
that our scaling laws break down at or before we reach this point, which is still many orders of magnitude
away in both compute and model size.
17



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 14):

Models between 0.6x and 2.2x the 
optimal size can be trained with a 
20% larger compute budget
Smaller models require 
more steps to train, while 
larger models require fewer
Our framework does not 
capture early training dynamics
Figure 12 Left: Given a ﬁxed compute budget, a particular model size is optimal, though somewhat larger
or smaller models can be trained with minimal additional compute. Right: Models larger than the compute-
efﬁcient size require fewer steps to train, allowing for potentially faster training if sufﬁcient additional paral-
lelism is possible. Note that this equation should not be trusted for very large models, as it is only valid in the
power-law region of the learning curve, after initial transient effects.
10 8
 10 6
 10 4
 10 2
 100
Compute (PF-days), non-embedding
2
3
4
5
6
7Test Loss
L = (Cmin/2.3 108) 0.050
L = (C/2.0 107) 0.057
Figure 13 When adjusting performance to simulate training far below the critical batch size, we ﬁnd a
somewhat altered power law for L(Cmin) when compared with the fully empirical results. The conspicuous
lump at 10−5 PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks
in the power-law ﬁts. It is the L(Cmin) trend that we expect to provide a reliable extrapolation for larger
compute.
that in fact we could train more efﬁciently 6 by training at the batch size Bcrit discussed in Section 5.1.
Large and small values of the loss could have been achieved with fewer samples or fewer steps, respectively,
and correcting for this inefﬁciency by standardizing to the critical batch size results in cleaner and more
predictable trends.
In this section we will adjust for this oversight. More importantly, we will use the results of Section 5
to determine the optimal allocation of compute between model size N and the quantity of data processed
during training, namely 2BcritSmin. We will determine this allocation both empirically and theoretically, by
using the equation for L(N,Smin), and we will demonstrate that these methods agree.
6.1 Optimal Performance and Allocations
Let us ﬁrst study the loss as a function of the optimally allocated compute from Equation (5.5). The result is
plotted in Figure 13, along with a power-law ﬁt. We see that as compared to the compute plot of Figure 1, the
new ﬁt with Cmin is somewhat improved.
Given L(Cmin), it is natural to ask for the optimal model sizeN(Cmin) that provides the minimal loss with a
given quantity of training compute. The optimal model size is shown in Figure 14. We observe thatN(Cmin)
6One might ask why we did not simply train at Bcrit in the ﬁrst place. The reason is that it depends not only on the
model but also on the target value of the loss we wish to achieve, and so is a moving target.
15



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 20):

the compute budget. We start with the Equation (1.6), repeated here for convenience:
L(N,S) =
(Nc
N
)αN
+
(Sc
S
)αS
. (B.1)
Here, S represents the number of parameter updates when training at the critical batch size [MKAT18],
which was deﬁned in Equation (5.2)9:
B(L) = B∗
L1/αB
. (B.2)
We would like to determine optimal training parameters for a ﬁxed compute budget, so we replace S =
C/(6NB (L)), where Cis the number of FLOPs used in the training run:
L(N,C) =
(Nc
N
)αN
+
(
6B∗Sc
N
L1/αBC
)αS
. (B.3)
Now, we set ∂NL
⏐⏐
C = 0to ﬁnd the condition for optimality:
0 = ∂L
∂N
⏐⏐
C
= −αN
N
(Nc
N
)αN
+ αS
N
(
6B∗Sc
N
L1/αBC
)αS (
1 −5N
L∂L
∂N
⏐⏐
C
)
=⇒ αN
αS
(Nc
N
)αN
=
(
6B∗Sc
N
L1/αBC
)αS
(B.4)
Equation (B.3) and (B.4) together determine the compute-efﬁcient frontier.
B.2 Efﬁcient Training
Now we assemble the implications of (B.3) and (B.4). First, note that inserting (B.4) into (B.3) yields
L(Neﬀ (C) ,C) =
(
1 +αN
αS
)
L(Neﬀ,∞) , (B.5)
which implies that for compute-efﬁcient training, we should train to a ﬁxed percentage αN
αS
≈10% above
the converged loss. Next, let’s determine how the optimal loss depends on the compute budget. Eliminating
N yields a power-law dependence of performance on compute:
L(C) =
(Cc
C
)αC
(B.6)
where we deﬁned
αC = 1/(1/αS + 1/αB + 1/αN) ≈0.052 (B.7)
Cc = 6NcB∗Sc
(
1 +αN
αS
)1/αS+1/αN (αS
αN
)1/αS
. (B.8)
Similarly, we can eliminate Lto ﬁnd N(C):
N(C)
Nc
=
(C
Cc
)αC/αN (
1 +αN
αS
)1/αN
(B.9)
and
S(C) = Cc
6NcB∗
(
1 +αN
αS
)−1/αN (C
Cc
)αC/αS
(B.10)
9There is a slight ambiguity here: we can imagine training either at a constant batch size B (Ltarget), or we could
instead train at a variable batch size ˜B (L), where ˜B is the instantaneous critical batch size (as opposed to B, which is
the averaged version). These two prescriptions result in the same number of steps, so we can ignore this subtlety (see
[MKAT18]).
21



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 15):

10 7
 10 5
 10 3
 10 1
Compute (PF-days), non-embedding
103
105
107
Parameters (non-embedding)
N = (1.3 109) C0.73
min
N = (1.6 109) C0.88
10 7
 10 5
 10 3
 10 1
Compute (PF-days), excluding embeddings
0
5000
10000
15000Steps
Smin (adjusted)
Smin = (5.4 103) C0.03
min
S (fixed-batch)
Figure 14 Left: Each value of the compute budget Cmin has an associated optimal model size N. Optimal
model size grows very rapidly with Cmin, increasing by 5x for each 10x increase in compute. The number
of data examples processed makes up the remainder of the increase, growing relatively modestly by only 2x.
Right: The batch-adjusted number of optimization steps also grows very slowly, if at all, meaning that most
of the growth in data examples processed can be used for increased batch sizes.
can be ﬁt very well with a power-law
N(Cmin) ∝(Cmin)0.73. (6.1)
In Figure 12, we show the effect of training models of sub-optimal sizes (see Appendix B.4).
By deﬁnition Cmin ≡6NBcritS, and so we can use N(Cmin) to extract further results. In particular, since
prior ﬁts show B ∝L−4.8 and L∝C−0.05
min , we can conclude that Bcrit ∝C0.24
min . This leads us to conclude
that the optimal number of steps will only grow very slowly with compute, as
Smin ∝(Cmin)0.03, (6.2)
matching the empirical results in Figure 14. In fact the measured exponent is sufﬁciently small that our results
may even be consistent with an exponent of zero.
Thus we conclude that as we scale up language modeling with an optimal allocation of computation, we
should predominantly increase the model size N, while simultaneously scaling up the batch size via B ∝
Bcrit with negligible increase in the number of serial steps. Since compute-efﬁcient training uses relatively
few optimization steps, additional work on speeding up early training dynamics may be warranted.
6.2 Predictions from L(N,Smin)
The results for L(Cmin) and the allocations can be predicted from the L(N,Smin) equation obtained in
Section 5. Given our equation for L(N,Smin), we can substitute Smin = Cmin
6NB and then ﬁnd the minimum
of the loss as a function of N, while ﬁxing the training compute. We carry out this procedure in detail in
Appendix B, where we also provide some additional predictions.
For the loss as a function of training compute, we predict that
L(Cmin) =
(Cmin
c
Cmin
)αmin
C
(6.3)
where
αmin
C ≡ 1
1/αS + 1/αB + 1/αN
≈0.054 (6.4)
in excellent agreement with the exponent of Figure 13. We also predict that
N(Cmin) ∝(Cmin)αmin
C /αN ≈(Cmin)0.71 (6.5)
which also matches the scaling of Figure 14 to within a few percent. Our scaling laws provide a predictive
framework for the performance of language modeling.
16



### Claim 130/179

#### Claim Text
However, a key difference between the two irradiation conditions is the density and size of the defects, leading to a difference in macroscopic void swelling and hardening .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[47]_2305.17331.pdf (Page 13):

1 2 3 4 5 6 7 8 9 10
Number of documents
36
38
40
42
44MMLU Accuracy
Standalone LM
Lt=Flan-T5Base
(a) Flan-T5Base w/ AARANCE.
1 2 3 4 5 6 7 8 9 10
Number of documents
44
46
48
50
52MMLU Accuracy
Standalone LM
Lt=Flan-T5Larg e (b) Flan-T5Large w/ AARANCE.
1 2 3 4 5 6 7 8 9 10
Number of documents
50
52
54
56
58MMLU Accuracy
Standalone LM
Lt=Flan-T5X L (c) Flan-T5XL w/ AARANCE.
Figure 9: Relationship between LM’s performance and the number of augmentation documents.
Category Number
Ts
MSMARCO QA Open Domain QA 148122
KILT-FEVER Fact Checking 10444
KILT-WNED Entity Linking 3396
KILT-T-REx Slot Filling 5000
KILT-TriviaQA Open Domain QA 5359
KILT-Wizard of Wikipedia Dialogue 3054
Tt
MMLU Multi-task Language Understanding 1531
PopQA Open Domain QA 14267
Table 6: Configurations of our source tasks and target tasks.
Methods MMLU
All Hum. Soc. Sci. STEM Other
Flan-T5Base 36.1 40.4 39.8 27.0 40.6
Flan-T5Base Fine-tuning 36.1 38.9 41.2 27.9 39.9
Flan-T5Base w/ Contriever 43.7 44.4 45.0 36.4 51.1
Flan-T5Base w/ ANCE 43.0 44.2 44.3 34.5 51.9
Flan-T5Base w/ AARContriever (Ours) 44.4 44.7 47.7 35.8 52.2
Flan-T5Base w/ AARANCE (Ours) 44.8 42.2 46.4 39.0 53.2
Flan-T5Large 45.1 47.7 53.5 34.4 49.2
Flan-T5Large Fine-tuning 45.3 47.6 54.1 35.2 48.7
Flan-T5Large w/ Contriever 50.7 50.5 56.4 38.9 61.1
Flan-T5Large w/ ANCE 49.2 49.3 56.7 38.1 57.2
Flan-T5Large w/ AARContriever (Ours) 51.8 50.8 59.7 39.4 61.8
Flan-T5Large w/ AARANCE (Ours) 50.4 48.0 58.1 39.3 60.2
Flan-T5XL 51.2 55.5 57.4 38.1 58.7
Flan-T5XL w/ Contriever 56.4 57.3 66.1 43.9 63.2
Flan-T5XL w/ ANCE 55.3 55.9 64.0 41.5 64.9
Flan-T5XL w/ AARContriever (Ours) 56.7 57.7 65.4 43.6 65.1
Flan-T5XL w/ AARANCE (Ours) 56.2 59.4 64.8 41.5 64.9
InstructGPT 60.2 65.7 68.0 46.1 66.5
InstructGPT w/ Contriever 60.5 62.0 71.8 44.3 70.1
InstructGPT w/ ANCE 61.6 62.4 73.4 47.6 68.6
InstructGPT w/ AARContriever (Ours) 61.5 64.5 73.1 45.0 69.9
InstructGPT w/ AARANCE (Ours) 62.2 62.0 72.0 49.2 70.7
Table 7: Fine-tuning results on MMLU. We use the official auxiliary training data of MMLU to fine-tune the LM.



Source: data\tc16_2312.10997v5\referenced_papers\[101]_2310.06839.pdf (Page 3):

1 5 10 15 20
Number of Retained Documents
0
20
40
60
80
100Recall(%)
LLMLingua
BM25
OpenAI
Voyageai
BGE-large-en v1.5
SBERT
Gzip
Cohere-Rerank
BGE-llmembeder
Jina
LongLLMLingua rk 
w/o restrict
BGE-Ranker-large
LongLLMLingua rk
(a) Recall Distribution
1 5 10 15 20
Document Position in the Prompt
0.0
0.2
0.4
0.6
0.8
1.0Document Avg. Perplexity
Perplexity
Contrastive Perplexity (b) Perplexity Distribution (5th)
Figure 3: (a) Comparison of recall on NaturalQuestions Multi-documemnt QA dataset, which increases from top to
bottom in terms of Recall@1. Different colors represent different types of methods. Among them, yellow represents
traditional relevance methods, green signifies embedding-based methods, and red denotes rerank-based methods.
(b) Comparison between perplexities and contrastive perplexities of tokens in the prompt from Multi-documemnt
QA dataset. The document containing the ground-truth information is located in the 5th position. More results on
position can be found in the Appendix C.1.
iterative compression mechanism following LLM-
Lingua and directly calculate token perplexities
to compress xins and xque. In this section, we in-
vestigate how to make the fine-grained token-level
compression over {xdoc
k }K′
k=1 aware of the question
xque, so that the compressed results could contain
more question-relevant key information.
A straightforward solution for the awareness of
xque is to simply concatenate it at the beginning
of the whole context. However, this will result in
low perplexities of relevant tokens in the context
following the condition of question xque, further
reducing their differentiation from other tokens.
In this paper, we propose contrastive perplexity,
i.e., the distribution shift caused by the condition of
the question, to represent the association between
the token and the question. The contrastive perplex-
ity based importance metric si for each token xi in
{xdoc
k }K′
k=1 can be formulated as:
si = perplexity(xi|x<i)−perplexity(xi|xque, x<i).
(3)
Additionally, we provide the derivation of its
mathematical significance in the Appendix A, con-
cluding that it is equivalent to conditional pointwise
mutual information (Church and Hanks, 1989).
Figure 3b illustrates the difference between per-
plexities and contrastive perplexities. The distri-
bution of perplexities appears random, making it
challenging to extract information related to the
question. However, tokens with high contrastive
perplexities tend to cluster near the ground-truth
document, which contains information relevant to
the question. This suggests that the proposed con-
trastive perplexity can better distinguish tokens
relevant to the question, thus improving the key
information density in the compressed results.
4.2 How to reduce information loss in the
middle?
As demonstrated in Figure 1b, LLM achieves the
highest performance when relevant information oc-
curs at the beginning and significantly degrades if
relevant information is located in the middle of long
contexts. After the coarse-grained compression, we
have obtained a set of documents {xdoc
k }K′
k=1 with
their corresponding importance scores {rk}K′
k=1 in-
dicating their association with the question xque.
Therefore, we reorder documents using their impor-
tance scores to better leverage LLMs’ information
perception difference in positions:
(xins, xdoc
1 , ··· , xdoc
K′ ,xque)
rk
−→
(xins,xdoc
r1 , ··· , xdoc
rK′ , xque)
(4)
4.3 How to achieve adaptive granular control
during compression?
In fine-grained compression, LLMLingua applies
the same compression ratio over all documents ob-
tained from budget controller. However, the key
information density of different documents is differ-
ent. The more relevant to the question a document



Source: data\tc16_2312.10997v5\referenced_papers\[47]_2305.17331.pdf (Page 4):

Settings Methods # Parameters MMLU PopQA
All Hum. Soc. Sci. STEM Other All
Base Setting:T5 Base Size
Few-shot Flan-T5Base(Chung et al., 2022) 250M 35.8 39.6 39.8 26.3 41.2 8.0
Zero-shot
Flan-T5Base 250M 36.1 40.4 39.8 27.0 40.6 8.8
Flan-T5Basew/ AR (Mallen et al., 2022) 250M 42.8 43.5 44.0 35.8 50.0 29.4
Flan-T5Basew/ AARContriever(Ours) 250M 44.4 44.7 47.7 35.8 52.2 31.9
Flan-T5Basew/ AARANCE(Ours) 250M 44.8 42.2 46.4 39.0 53.2 37.7
Large Setting:T5 Large Size
Few-shot AtlasLargeFT (Izacard et al., 2022) 770M 38.9 37.3 41.7 32.3 44.9 n.a.
Flan-T5Large 780M 45.1 47.7 53.5 34.4 49.2 9.3
Zero-shot
Flan-T5Large 780M 44.8 46.3 51.4 34.8 50.6 7.2
Flan-T5Largew/ AR 780M 49.8 50.0 55.6 38.4 59.5 29.6
Flan-T5Largew/ AARContriever(Ours) 780M 51.8 50.8 59.7 39.4 61.8 33.4
Flan-T5Largew/ AARANCE(Ours) 780M 50.4 48.0 58.1 39.3 60.2 39.3
XL Setting:T5 XL Size
Few-shot AtlasXLFT 3B 42.3 40.0 46.8 35.0 48.1 n.a.
Flan-T5XL 3B 51.6 55.0 61.1 36.8 59.5 11.1
Zero-shot
Flan-T5XL 3B 51.2 55.5 57.4 38.1 58.7 11.3
Flan-T5XLw/ AR 3B 55.5 56.7 64.5 43.0 62.6 33.7
Flan-T5XLw/ AARContriever(Ours) 3B 56.7 57.7 65.4 43.6 65.1 31.5
Flan-T5XLw/ AARANCE(Ours) 3B 56.2 59.4 64.8 41.5 64.9 38.0
Giant Setting:Over 70B Size
Few-shot
Chinchilla (Hoffmann et al., 2022) 70B 67.5 63.6 79.3 55.0 73.9 n.a.
OPT-IML-Max (Iyer et al., 2022) 175B 47.1 n.a. n.a. n.a. n.a. n.a.
InstructGPT (Ouyang et al., 2022) 175B 60.5 62.0 71.8 44.3 70.1 35.2
Zero-shot
GAL (Taylor et al., 2022) 120B 52.6 n.a. n.a. n.a. n.a. n.a.
OPT-IML-Max 175B 49.1 n.a. n.a. n.a. n.a. n.a.
InstructGPT 175B 60.2 65.7 68.0 46.1 66.5 34.7
InstructGPT w/ AR 175B 60.5 62.2 71.3 44.7 69.7 43.3
InstructGPT w/ AARContriever(Ours) 175B 61.5 64.5 73.1 45.0 69.9 43.9
InstructGPT w/ AARANCE(Ours) 175B 62.2 62.0 72.0 49.2 70.7 52.0
Table 1: Our main results on MMLU and PopQA dataset. We group the methods mainly by the parameters. Our
Ls is Flan-T5Base. AARContriever: AAR initialized from Contriever; AARANCE: AAR initialized from ANCE; FT:
fine-tuning; AR: adaptive retrieval. Unspecified methods represent direct prompting. The score marked as bold
means the best performance among the models in the zero-shot setting.
2.5 5.0 7.5
Training FLOPs 1e21
30
35
40
45
50
55
60MMLU Accuracy
AARANCE
Ls=Flan-T5Base, Lt=Flan-T5Base
AARANCE
Ls=Flan-T5Base, Lt=Flan-T5Large
AARANCE
Ls=Flan-T5Base, Lt=Flan-T5XL
AARANCE
Ls=Lt=Flan-T5Large
AARANCE
Ls=Lt=Flan-T5XL
AtlasLarge
AtlasXL
Figure 3: Training FLOPs of retrieval augmentation
methods.
using unsupervised data and fine-tunes the retriever
via the attention distillation on few-shot data.
5 Evaluation Results
In this section, we discuss our main results on
MMLU and PopQA datasets (§ 5.1) and conduct
comprehensive studies about how (§ 5.2, § 5.3,
§ 5.4) and when (§ 5.5, § 5.6) AAR helps.
5.1 Overall Performance
Table 1 demonstrates that, with the assistance of a
generic AAR, target LMs of different sizes and
architectures can significantly outperform their
standalone baselines in the zero-shot setting. No-
tably, AAR even improves powerful InstructGPT
by 2% on MMLU and by nearly 20% on PopQA.
We hypothesize that the PopQA dataset mainly
comprises long-tail questions and thus necessitates
more augmentation information to attain high accu-
racy. AAR outperforms other augmentation meth-
ods like few-shot prompting and adaptive retrieval,
as they may not offer as extensive evidence text as
AAR does.
Meanwhile, AAR is a highly efficient augmenta-
tion approach since it only relies on a small source



Source: data\tc16_2312.10997v5\referenced_papers\[101]_2310.06839.pdf (Page 2):

Original Prompt LongLLMLingua
Compressed Prompt
Instruction: Answer the question based 
on the given passages. Only give me the 
answer and do not output any other 
words. The following are given 
passages.
Document 1: Alberic III of Dammartin
Alberic III of Dammartin (Aubry de 
Dammartin) (c. 1138 – 19 September 
1200) was a French count and son of 
Alberic II, Count of Dammartin, and 
Clé mence de Bar, daughter of Reginald 
I, Count of Bar…
Document 2:
…
Document N: Pope Agapetus II
Pope Agapetus II (died 8 November 955) 
was the bishop of Rome and ruler of the 
Papal States from 10 May 946 to his 
death. A nominee of the princeps of 
Rome, Alberic II of Spoleto, his 
pontificate occurred during …
Question: Who gave the mother of 
Alberic II of Spoleto the title "patricia" 
of Rome?
Small 
Model
Black-box LLMs
~13k tokens
Answer the question based on the given 
passages. …Passage 4:was a Roman 
noblewo who was the alleged mistress 
of Pope Sergius III and was given the 
unprecedented titles senatrix 
("senatoress") and patricia of Rome by 
Pope John X. , when Ottosys, the of and, 
were to to discusss Rome and other 
more important, were.us was to the 
dispute the the of Re… Who gave the 
mother of Alberic II of Spoleto the title 
"patricia" of Rome? ~2k tokens
I  Budget Controller
Question-aware Coarse-Grained 
                Compression
w/ document reordering
II  Iterative Token-level           
Question-aware Fine-Grained 
Compression
w/ dynamic compression ratio
0  Distribution 
    Alignment
III Execution with 
Compressed Prompt
Subsequence 
Recovery
Response
IV
Figure 2: Framework of LongLLMLingua. Gray Italic content: As in LLMLingua.
information related to the question in the prompt.
It encompasses three perspectives and further incor-
porates a subsequence recovery strategy, as shown
in Figure 2, to enhance the accuracy and reliability
of the information provided to users. In this section,
we detail how each part of LongLLMLingua works
to improve the LLMs deal with long context.
4.1 How to improve key information density
in the prompt?
Question-Aware Coarse-Grained Compression
In coarse-grained compression, we aim to figure
out a metric rk to evaluate the importance of each
document xdoc
k = {xdoc
k,i }Nk
i=1, where Nk is the
number of tokens in xdoc
k . We only keep xdoc
k
with higher rk as the intermediate compressed re-
sults. One approach to improve key information
density in the compressed prompts is to calculate
document-level perplexity conditioned on the ques-
tion p(xdoc
k |xque). However, this method may not
be effective because documents often contain a sig-
nificant amount of irrelevant information. Even
when conditioned on xque, the perplexity scores
computed for entire documents may not be suffi-
ciently distinct, rendering them an inadequate met-
ric for document-level compression.
We propose to use the perplexity of the ques-
tion xque conditioned on different contexts xdoc
k
p(xque|xdoc
k ) to represent the association between
them. We also append a restrictive statement 2
xrestrict after xque to strengthen the interconnection
2Specifically, "We can get the answer to this question in
the given documents".
of xque and xdoc
k . It can be regarded as a regulariza-
tion term that mitigates the impact of hallucinations.
This can be formulated as:
rk = − 1
Nc
NcX
i
log p(xque,restrict
i |xdoc
k ),
k ∈ {1, 2, ··· , K},
(2)
where xque,restrict
i is the i-th token in the concate-
nated sequence of xque and xrestrict and Nc in the
number of tokens.
Figure 3a displays the recall distribution of dif-
ferent retrieval methods, including traditional rele-
vance methos (BM25, Gzip (Jiang et al., 2023b)),
embedding-based methods (OpenAI-embedding,
V oyageai3, BGE-large-en v1.5 (Xiao et al., 2023),
Sentence-BERT (Reimers and Gurevych, 2019),
Jina (Günther et al., 2023)), and reranker methods
(Cohere-Rerank4, BGE-llmembeder, BGE-Ranker-
large), which demonstrates that our coarse-level
compression approach achieves the highest recall
with different numbers of retained documents, sug-
gesting that it preserves the most key information
from the contexts in the compressed results.
Question-Aware Fine-Grained Compression
In fine-grained compression, we assess the impor-
tance of each token in the instruction xins, the ques-
tion xque, and K′ documents {xdoc
i }K′
i=1 retained af-
ter coarse-grained compression. We incorporate the
3https://www.voyageai.com/
4https://cohere.com/rerank



Source: data\tc16_2312.10997v5\referenced_papers\[101]_2310.06839.pdf (Page 6):

Methods GPT3.5-Turbo LongChat-13b Length Latency
1st 5th 10th 15th 20th Reorder 1st 5th 10th 15th 20th Reorder Tokens 1/τ Latency Speedup
2x constraint
Retrieval-based Methods
BM25 53.7 49.3 47.9 49.9 46.9 50.3 50.9 44.9 44.1 42.9 43.2 46.0 1,545 1.9x 2.1 1.9x
Gzip 64.6 63.8 60.5 58.3 57.3 64.4 61.9 55.7 52.7 50.8 50.9 59.3 1,567 1.9x 2.1 1.9x
SBERT 72.5 67.9 63.3 65.0 66.2 68.7 65.8 57.5 54.9 53.4 55.7 61.4 1,549 1.9x 2.2 1.9x
OpenAI 73.0 65.6 66.5 65.4 65.5 69.9 65.9 57.5 56.2 54.2 55.7 61.7 1,550 1.9x 4.9 0.8x
LongLLMLinguark 73.9 67.7 68.7 66.0 65.6 74.3 68.5 59.1 56.8 55.3 56.9 65.2 1,548 1.9x 2.3 1.8x
Compression-based Methods
Selective-Context 45.4 39.0 33.8 33.5 41.5 - 53.2 26.3 25.4 24.2 33.3 - 1,478 2.0x 7.4 0.6x
LLMLingua 39.7 39.5 40.4 37.1 42.3 41.5 38.7 37.3 35.7 34.1 37.5 37.1 1,410 2.1x 2.8 1.5x
LongLLMLingua 77.2 72.9 70.8 70.5 70.6 76.2 68.7 59.4 57.3 55.9 58.4 66.1 1,429 2.1x 2.9 1.4x
4x constraint
Retrieval-based Methods
BM25 40.6 38.6 38.2 37.4 36.6 36.3 39.5 37.5 36.8 36.4 35.5 37.7 798 3.7x 1.5 2.7x
Gzip 63.1 61.0 59.8 61.1 60.1 62.3 57.6 52.9 51.0 50.1 50.4 57.2 824 3.6x 1.5 2.7x
SBERT 66.9 61.1 59.0 61.2 60.3 64.4 62.6 56.6 55.1 53.9 55.0 59.1 808 3.6x 1.6 2.5x
OpenAI 63.8 64.6 65.4 64.1 63.7 63.7 61.2 56.0 55.1 54.4 55.0 58.8 804 3.7x 4.3 1.0x
LongLLMLinguark 71.1 70.7 69.3 68.7 68.5 71.5 67.8 59.4 57.7 57.7 58.6 64.0 807 3.7x 1.7 2.4x
Compression-based Methods
Selective-Context 31.4 19.5 24.7 24.1 43.8 - 38.2 17.2 15.9 16.0 27.3 - 791 3.7x 6.8 0.6x
LLMLingua 25.5 27.5 23.5 26.5 30.0 27.0 32.1 30.8 29.9 28.9 32.4 30.5 775 3.8x 1.8 2.2x
LongLLMLingua 75.0 71.8 71.2 71.2 74.7 75.5 68.7 60.5 59.3 58.3 61.3 66.7 748 3.9x 2.1 2.0x
Original Prompt 75.7 57.3 54.1 55.4 63.1 - 68.6 57.4 55.3 52.5 55.0 - 2,946 - 4.1 -
Zero-shot 56.1 35.0 15 196x 1.1 3.7x
Table 1: Performance of different methods with different compression ratios (raw size / compressed size = 1/τ) on
NaturalQuestions (20 documents) (Liu et al., 2024). Reorder: we reorder the documents with relevance metrics of
different baselines as our document reordering strategy described in Sec. 4.2. In the case of OpenAI, it corresponds
to LongContextReorder9 in the LangChain framework (Chase, 2022). For results reported under 1st to 20th, we do
not use the reordering strategy for all methods.
introduce following variants of it for ablation
study. (1) Variants about Question-aware Coarse-
grained Compression, include: ours w/o Question-
awareness, which calculates question-text rele-
vance rk using information entropy in LLMLin-
gua, ours w/ SBERT, which employs SBERT to
compute rk, ours w/ p(xdoc
k |xque,restrict
i ), which re-
place p(xque,restrict
i |xdoc
k ) with p(xdoc
k |xque,restrict
i ) in
Eq. (2), and ours w/o restrict, which only calcu-
lates the conditional probability corresponding to
xque. (2) Ours w/o Question-aware Fine-grained,
which disregards Eq. (3) and only applies Iterative
Token-level Prompt Compression as LLMLingua.
(3) Ours w/o Dynamic Compression Ratio, where
all documents share the same compression ratio
in fine-grained compression. (4) Ours w/o and
(5) LLMLingua w/ Subsequence Recovery, which
either removes or adds the post-processing subse-
quence recovery strategy. (6) Ours w/ GPT2-small,
which uses the GPT2-small model as the MS.
Table 3, 4, and 7 shows the results of the ablation
study in difference tasks. In summary, removing
any component proposed for LongLLMLingua will
lead to a performance drop regardless of the posi-
tion of the ground-truth answer. This well validates
the necessity and effectiveness of the proposed
question-aware mechanism during coarse-to-fine
compression, the dynamic compression ratio, and
the subsequence recovery strategy. It also shows
that applying SBERT for coarse-grained compres-
sion will result in inferior performance, which im-
plies the superiority of our question-aware impor-
tance metric in Eq. (2) over SBERT. In addition, re-
placing p(xque,restrict
i |xdoc
k ) with p(xdoc
k |xque,restrict
i )
can greatly affect performance due to the large
noise in calculating p(xdoc
k ) since the perplexity
of document depends on many other information
besides the question. Removing the restrictive
statement can increase the hallucination of small
language models, leading to a decrease in perfor-
mance. Moreover, our subsequence recovery strat-
egy can also bring performance gains for LLMLin-
gua. However, without our question-aware mech-
anism, results from LLMLingua are still less sat-
isfactory. For more detailed cases, please go to
Appendix E.



### Claim 131/179

#### Claim Text
Since the early work of Fadlun et al. many researchers have chosen to apply an immersed boundary force only at the fluid-solid interface, while allowing the flow field in the interior of the solid domain to evolve freely.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[173]_2403.10131.pdf (Page 2):

Preprint, Under Review
Figure 2: Overview of our RAFT method. The top-left figure depicts our approach of
adapting LLMs to reading solution from a set of positive and distractor documents in
contrast to standard RAG setup where models are trained based on the retriever outputs,
which is a mixture of both memorization and reading. At test time, all methods follow the
standard RAG setting, provided with a top-k retrieved documents in the context.
the exam. For LLMs, this is equivalent to the scenario, for example, in which the LLM is
used as a chatbot. In this scenario the LLM draws from the knowledge baked in during
pre-training and supervised-finetuning to respond to the users’ prompt.
Open Book Exam In contrast, we liken the open-book exam setting to the scenario in
which the LLM can refer to external sources of information (e.g., a website or a book chapter).
In such scenarios, typically, the LLM is paired with retriever which retrieves ‘k’ documents
(or specific segments of the document) which are appended to the users’ prompt. It is
only through these documents retrieved that the LLM gains access to “domain-specific
information”. As a result, we argue that the LLM’s performance in these settings, where it
is trained as a general-purpose LLM is largely dependent on the quality of the retriever and
how accurately the retriever can identify the most relevant piece of information.
Domain-Specific Open-Book Exam In this paper, we focus on the narrower but increas-
ingly popular domain than the general open book exam, which we call the domain-specific
open-book exam. Here, we know apriori the domain in which the LLM will be tested. The
LLM can respond to the users’ prompt using use any and all information from this specific
domain, which it has been fine-tuned on. Examples of domain specific examples include
enterprise documents, code repositories belonging to an organization, etc. In all these
scenarios, the LLM will be used to respond to the questions, whose answers can be found
within a collection of documents. The retrieval technique itself has little to no-impact on the
mechanism (though it may impact the accuracy). This paper studies the domain-specific
open-book setting and how to adapt a pretrained LLM to this specific domain, including
how to make it more robust to a varying number of retrieved documents and distractors.
3 RAFT
In this section, we present RAFT, a novel way of training LLMs for domain-specific open-
book exams. We first introduce the classical technique of supervised fine-tuning, followed
with the key takeaways from our experiments. Then, we introduce RAFT , a modified
version of general instruction tuning. Lastly, we provide an overview of the experiments to
expect in the later sections.
Supervised Finetuning
Consider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The
formulation consists of the Dataset (D) from which a set of Question (Q) and corresponding
answer (A) pairs are derived or already available. In the classical SFT setting, the model is
trained to improve it’s ability to answer the questions based on it’s knowledge - obtained
either during pre-training, or during the SFT training phase. The model so trained can also
3



Source: data\tc16_2312.10997v5\referenced_papers\[172]_2309.17453.pdf (Page 3):

Published as a conference paper at ICLR 2024
Dense Attention Window Attention Sliding Window 
w/ Re-computation StreamingLLM
Dense Attention Window Attention StreamingLLM
Figure 3: Language modeling perplexity on texts with 20K tokens across various LLM. Observations reveal
consistent trends: (1) Dense attention fails once the input length surpasses the pre-training attention window
size. (2) Window attention collapses once the input length exceeds the cache size, i.e., the initial tokens are
evicted. (3) StreamingLLM demonstrates stable performance, with its perplexity nearly matching that of the
sliding window with re-computation baseline.
Improving LLMs’ Utilization of Long Textoptimizes LLMs to better capture and employ the
content within the context rather than merely taking them as inputs. As highlighted by Liu et al.
and Li et al., success in the previously mentioned two directions does not necessarily translate to
competent utilization of lengthy contexts. Addressing this effective usage of prolonged contexts
within LLMs is still a challenge. Our work concentrates on stably harnessing the most recent tokens,
enabling the seamless streaming application of LLMs.
3 S TREAMING LLM
3.1 T HE FAILURE OF WINDOW ATTENTION AND ATTENTION SINKS
While the window attention technique offers efficiency during inference, it results in an exceedingly
high language modeling perplexity. Consequently, the model’s performance is unsuitable for deploy-
ment in streaming applications. In this section, we use the concept of attention sink to explain the
failure of window attention, serving as the inspiration behind StreamingLLM.
Identifying the Point of Perplexity Surge.Figure 3 shows the perplexity of language modeling on
a 20K token text. It is evident that perplexity spikes when the text length surpasses the cache size, led
by the exclusion of initial tokens. This suggests that the initial tokens, regardless of their distance
from the predicted tokens, are crucial for maintaining the stability of LLMs.
Why do LLMs break when removinginitial tokens’ KV? We visualize attention maps from
all layers and heads of the Llama-2-7B and models in Figure 2. We find that, beyond the bottom
two layers, the model consistently focuses on the initial tokens across all layers and heads. The
implication is clear: removing these initial tokens’ KV will remove a considerable portion of the
denominator in the SoftMax function (Equation 1) in attention computation. This alteration leads to a
significant shift in the distribution of attention scores away from what would be expected in normal
inference settings.
SoftMax(x)i = exi
ex1 + PN
j=2 exj
, x 1 ≫ xj, j∈ 2, . . . , N (1)
There are two possible explanations for the importance of the initial tokens in language modeling:
(1) Either their semantics are crucial, or (2) the model learns a bias towards their absolute position.
To distinguish between these possibilities, we conduct experiments (Table 1), wherein the first four
tokens are substituted with the linebreak token “\n". The observations indicate that the model still
significantly emphasizes these initial linebreak tokens. Furthermore, reintroducing them restores
the language modeling perplexity to levels comparable to having the original initial tokens. This
suggests that the absolute position of the starting tokens, rather than their semantic value, holds
greater significance.
LLMs attend to Initial Tokens as Attention Sinks.To explain why the model disproportionately
focuses on initial tokens—regardless of their semantic relevance to language modeling, we introduce
the concept of “attention sink". The nature of the SoftMax function (Equation 1) prevents all attended
tokens from having zero values. This requires aggregating some information from other tokens across
all heads in all layers, even if the current embedding has sufficient self-contained information for its
prediction. Consequently, the model tends to dump unnecessary attention values to specific tokens. A
similar observation has been made in the realm of quantization outliers (Xiao et al., 2023; Bondarenko
et al., 2023), leading to the proposal of SoftMax-Off-by-One (Miller, 2023) as a potential remedy.
4



Source: data\tc16_2312.10997v5\referenced_papers\[172]_2309.17453.pdf (Page 2):

Published as a conference paper at ICLR 2024
Layer 0 Head 0 Layer 1 Head 0 Layer 2 Head 0
Layer 23 Head 0 Layer 31 Head 0
Layer 16 Head 0Layer 9 Head 0
Figure 2: Visualization of the average attention logits in Llama-2-7B over 256 sentences, each with a length
of 16. Observations include: (1) The attention maps in the first two layers (layers 0 and 1) exhibit the "local"
pattern, with recent tokens receiving more attention. (2) Beyond the bottom two layers, the model heavily attends
to the initial token across all layers and heads.
Furthermore, we confirm our attention sink hypothesis and demonstrate that language models can
be pre-trained to require only a single attention sink token for streaming deployment. Specifically,
we suggest that an extra learnable token at the beginning of all training samples can serve as a
designated attention sink. By pre-training 160-million parameter language models from scratch, we
demonstrate that adding this single sink token preserves the model’s performance in streaming cases.
This stands in contrast to vanilla models, which necessitate the reintroduction of multiple initial
tokens as attention sinks to achieve the same performance level.
Finally, we emphasize that StreamingLLM efficiently generates coherent text from tokens within
the KV cache without extending the LLMs’ context length. It suits continuous operation needs with
minimal memory use and past data reliance. Additionally, StreamingLLM can complement context
extension methods to increase the attendable recent context.
2 R ELATED WORK
Extensive research has been done on applying LLMs to lengthy texts, with three main areas of focus:
Length Extrapolation, Context Window Extension, and Improving LLMs’ Utilization of Long
Text. While seemingly related, it’s worth noting that progress in one direction doesn’t necessarily
lead to progress in the other. For example, extending the context size of LLMs doesn’t improve the
model’s performance beyond the context size, and neither approach ensures effective use of the long
context. Our StreamingLLM framework primarily lies in the first category, where LLMs are applied
to text significantly exceeding the pre-training window size, potentially even of infinite length. We do
not expand the attention window size of LLMs or enhance the model’s memory and usage on long
texts. The last two categories are orthogonal to our focus and could be integrated with our techniques.
Length extrapolationaims to enable language models trained on shorter texts to handle longer
ones during testing. A predominant avenue of research targets the development of relative position
encoding methods for Transformer models, enabling them to function beyond their training window.
One such initiative is Rotary Position Embeddings (RoPE) (Su et al., 2021), which transforms
the queries and keys in every attention layer for relative position integration. Despite its promise,
subsequent research (Press et al., 2022; Chen et al., 2023) indicated its underperformance on text
that exceeds the training window. Another approach, ALiBi (Press et al., 2022), biases the query-key
attention scores based on their distance, thereby introducing relative positional information. While
this exhibited improved extrapolation, our tests on MPT models highlighted a breakdown when the
text length was vastly greater than the training length. Current methodologies, however, have yet to
achieve infinite length extrapolation, causing no existing LLMs to fit for streaming applications.
Context Window Extensioncenters on expanding the LLMs’ context window, enabling the process-
ing of more tokens in one forward pass. A primary line of work addresses the training efficiency
problem. Given the attention to computation’s quadratic complexity during training, developing
a long-context LLM is both a computational and memory challenge. Solutions have ranged from
system-focused optimizations like FlashAttention (Dao et al., 2022; Dao, 2023), which accelerates
attention computation and reduces memory footprint, to approximate attention methods (Zaheer
et al., 2020b; Beltagy et al., 2020; Wang et al., 2020; Kitaev et al., 2020) that trade model quality for
efficiency. Recently, there has been a surge of work on extending pre-trained LLMs with RoPE (Chen
et al., 2023; kaiokendev, 2023; bloc97, 2023; Peng et al., 2023), involving position interpolation and
fine-tuning. However, all the aforementioned techniques only extend LLMs’ context window to a
limited extent, which falls short of our paper’s primary concern of handling limitless inputs.
3



Source: data\tc16_2312.10997v5\referenced_papers\[172]_2309.17453.pdf (Page 0):

Published as a conference paper at ICLR 2024
EFFICIENT STREAMING LANGUAGE MODELS
WITH ATTENTION SINKS
Guangxuan Xiao1∗ Yuandong Tian2 Beidi Chen3 Song Han1,4 Mike Lewis2
1 Massachusetts Institute of Technology 2 Meta AI
3 Carnegie Mellon University 4 NVIDIA
https://github.com/mit-han-lab/streaming-llm
ABSTRACT
Deploying Large Language Models (LLMs) in streaming applications such as
multi-round dialogue, where long interactions are expected, is urgently needed but
poses two major challenges. Firstly, during the decoding stage, caching previous
tokens’ Key and Value states (KV) consumes extensive memory. Secondly, popular
LLMs cannot generalize to longer texts than the training sequence length. Window
attention, where only the most recent KVs are cached, is a natural approach — but
we show that it fails when the text length surpasses the cache size. We observe
an interesting phenomenon, namely attention sink, that keeping the KV of initial
tokens will largely recover the performance of window attention. In this paper, we
first demonstrate that the emergence of attention sink is due to the strong attention
scores towards initial tokens as a “sink” even if they are not semantically important.
Based on the above analysis, we introduce StreamingLLM, an efficient framework
that enables LLMs trained with a finite length attention window to generalize to
infinite sequence length without any fine-tuning. We show that StreamingLLM can
enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language
modeling with up to 4 million tokens and more. In addition, we discover that
adding a placeholder token as a dedicated attention sink during pre-training can
further improve streaming deployment. In streaming settings, StreamingLLM
outperforms the sliding window recomputation baseline by up to 22.2× speedup.
Code and datasets are provided in the link.
1 I NTRODUCTION
Large Language Models (LLMs) (Radford et al., 2018; Brown et al., 2020; Zhang et al., 2022;
OpenAI, 2023; Touvron et al., 2023a;b) are becoming ubiquitous, powering many natural language
processing applications such as dialog systems (Schulman et al., 2022; Taori et al., 2023; Chiang et al.,
2023), document summarization (Goyal & Durrett, 2020; Zhang et al., 2023a), code completion (Chen
et al., 2021; Rozière et al., 2023) and question answering (Kamalloo et al., 2023). To unleash the
full potential of pretrained LLMs, they should be able to efficiently and accurately perform long
sequence generation. For example, an ideal ChatBot assistant can stably work over the content of
recent day-long conversations. However, it is very challenging for LLM to generalize to longer
sequence lengths than they have been pretrained on, e.g., 4K for Llama-2 Touvron et al. (2023b).
The reason is that LLMs are constrained by the attention window during pre-training. Despite
substantial efforts to expand this window size (Chen et al., 2023; kaiokendev, 2023; Peng et al., 2023)
and improve training (Dao et al., 2022; Dao, 2023) and inference (Pope et al., 2022; Xiao et al., 2023;
Anagnostidis et al., 2023; Wang et al., 2021; Zhang et al., 2023b) efficiency for lengthy inputs, the
acceptable sequence length remains intrinsically finite, which doesn’t allow persistent deployments.
In this paper, we first introduce the concept of LLM streaming applications and ask the question:
Can we deploy an LLM for infinite-length inputs without sacrificing efficiency and performance?
∗Part of the work done during an internship at Meta AI.
1
arXiv:2309.17453v4  [cs.CL]  7 Apr 2024



Source: data\tc16_2312.10997v5\referenced_papers\[172]_2309.17453.pdf (Page 14):

Published as a conference paper at ICLR 2024
A D ISCUSSIONS
Applications. StreamingLLM is particularly suited for streaming applications, such as multi-round
dialogues, where continuous operation without heavy reliance on extensive memory or historical data
is crucial. For instance, in a daily assistant application based on LLMs, StreamingLLM enables the
model to function seamlessly over extended periods. It bases its responses on recent interactions, thus
avoiding the need for frequent cache refreshes. Traditional methods might require resetting the cache
when the conversation length surpasses the training length, leading to a loss of recent context, or they
might need to recompute key-value (KV) states from recent text history, which can be inefficient.
Limitations. While StreamingLLM improves the efficiency of LLMs in streaming contexts, it
does not extend the models’ context window or enhance their long-term memory capabilities. As
detailed in Section C, the model is limited to operating within the confines of its current cache.
Consequently, StreamingLLM is not suitable for tasks that demand long-term memory and extensive
data dependency, such as long document question-answering (QA) and summarization. However, it
excels in scenarios only requiring short-term memory, like daily conversations and short document
QA, where its strength lies in generating coherent text from recent context without the need for cache
refreshment.
Broader Societal Impacts.StreamingLLM significantly enhances the efficiency and accessibility of
LLMs, democratizing their use across various sectors. By enabling nonstop and rapid interactions
in applications like conversational agents, StreamingLLM improves user experiences, especially in
scenarios requiring fixed-length models. This advancement allows for more seamless and contextually
aware dialogues, potentially benefiting sectors like education, healthcare, and customer service.
Additionally, StreamingLLM’s efficiency in processing reduces the computational load, aligning with
the need for environmentally sustainable AI technologies. This aspect is crucial in making advanced
AI tools more accessible in regions with limited technological resources. However, the potential
negative impacts of StreamingLLM mirror those associated with general language models, such as
misinformation and biased content generation risks. It’s essential to address these risks with robust
ethical guidelines and safeguards. In summary, while StreamingLLM shares some risks common to
language models, its positive contributions towards enhancing user experience, democratizing AI
access, and promoting sustainability are noteworthy. These benefits underscore the importance of
responsible deployment and ethical use of this technology.
B A DDITIONAL RELATED WORKS
Sparse Transformers.The literature on efficient Transformer models primarily focuses on reducing
the computational and memory complexity of the self-attention mechanism. A relevant line of
work involves sparsifying the attention matrix by restricting the field of view to fixed, predefined
patterns, such as local windows or block patterns with fixed strides (Tay et al., 2022). Sparse Trans-
former (Child et al., 2019) introduces sparse factorizations of the attention matrix, reducing the
computational complexity of attention to O(n√n). LongFormer (Beltagy et al., 2020) combines
dilated local windowed attention with task-motivated global attention. Extended Transformer Con-
struction (ETC) Ainslie et al. (2020) presents a novel global-local attention mechanism, incorporating
four types of attention patterns: global-to-global, local-to-local, local-to-global, and global-to-local.
Building on ETC, BigBird (Zaheer et al., 2020a) proposes another linear complexity attention alter-
native, utilizing global tokens, local sliding window attentions, and random attention. However, these
methods have several limitations. First, Sparse Transformer and ETC require custom GPU kernels
for a specific block-sparse variant of matrix-matrix multiplication. Second, LongFormer, ETC, and
BigBird all rely on a global attention pattern, which is unsuitable for autoregressive language models.
Third, these methods are incompatible with pre-trained models, necessitating retraining from scratch.
In contrast, our method offers ease of implementation using standard GPU kernels and is compatible
with pre-trained autoregressive language models using dense attention, which are prevalent in the
NLP community. This compatibility provides a significant advantage, allowing for the leveraging of
existing pre-trained models without any fine-tuning.
Concurrent Works.Our research coincides with the work of Han et al., who conducted a theoretical
study on the length generalization failure of language models, identifying three out-of-distribution
factors. Their approach, inspired by this analysis, involves employing a “Λ"-shaped attention pattern
15



### Claim 132/179

#### Claim Text
These results align with the ones presented in .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 6):

2 2.5 3 3.5 4 4.5 5 5.5
23.5
24
24.5
25
25.5
26
26.5
27
Latency (seconds)
Rouge-L
2%
(a) ELI5 - Base
0.6 0.8 1 1.2 1.4 1.6 1.8 2 2.2
20
21
22
23
24
25
Latency (seconds)
2% (b) MS MARCO- Base
1.5 2 2.5 3 3.5 4 4.5
24
25
26
27
28
29
30
31
32
Latency (seconds)
2% (c) NQ - Base
4 6 8 10
24
24.5
25
25.5
26
26.5
Latency (seconds)
Rouge-L
2%
(d) ELI5 - Large
2 3 4 5
21
21.5
22
22.5
23
23.5
24
24.5
25
Latency (seconds)
2% (e) MS MARCO- Large
4 6 8 10 12
26
27
28
29
30
31
32
F iD
CALM
T ok en Filtering
Combined
Latency (seconds)
2% (f) NQ - Large
Figure 5: ROUGE-L Performance results of the different methods on the test sets, plotted as smoothed Max Curves,
as a function of latency (seconds), for Base (top) and Large (bottom) models. Overall, our combined approach is
able to reach a better trade-off than the regular FiD model, for most cases.
values, due to skipping the redundant layer compu-
tations. In the case of Token Filtering, it is also able
to preserve and at times improve the performance
of the model overall, while the latency improve-
ment remains limited, since it is still computing the
remaining tokens across all decoder layers. The
performance improvement is presumably due to the
redundant tokens being removed early on during
the generation process, hence allowing the model
to better attend to the salient information in the
input.
When combining both methods, the performance
enhancement of the Token Filtering and the latency
reduction of CALM produce a better curve than
either method alone. In addition, we showcase
the drop in 2% performance per dataset, show-
ing that our method is able to reduce the latency
significantly more than the regular FiD, with the
best reduction reached on the MS MARCO dataset
for FiD-Base, saving 62.2% of the latency. In the
NQ dataset however, for both the base-sized and
large-sized models, while the CALM method does
achieve proper latency reduction, the Token Filter-
ing does not effect the results significantly. Since
we focus on real-world scenarios, we showcase
the trade-off with the actual latency, instead of
measurements such as FLOPS (MACs), as done by
previous works (de Jong et al., 2022). For those, we
refer to Figure 6 in the Appendix for the trade-off
and FLOPS (MACs) analysis. For the large-sized
models (Figures 5d, 5e, and 5f), we observe similar
patterns to those in the base-sized variations, with
the latency values being significantly larger. We
note that the overall performance values for these
models are not substantially different than those
produced by the smaller versions, hence we do not
focus as much on them.
6.2 Performance Comparison
To asses the performance of our Combined method
further, we choose the best performing hyperpa-
rameter setting for the FiD-Base model, and report
the test set results for each dataset, with Table 1
showing the results, compared to approaches sug-
gested in Su et al. (2022). In particular, we com-
pare to their implementation of FiD (named RBG
FID) and their suggested system (named RBG),
with the results of both taken from their published
work. We note that both our models and the RBG
models are of the same size. In our experiments,
we denote the original FiD model we trained as
FiD (ours), the FiD model with the Token Filter-
ing method as FiD TF, and the Combined method
as FiD Comb. On both datasets, FiD (ours), FiD
TF and FiD Comb achieve state-of-the-art results,
with Combined reaching the best overall perfor-
mance in terms of ROUGE-L and F1 (aside from
MS MARCO F1, where our approach is second).



Source: data\tc16_2312.10997v5\referenced_papers\[3]_2310.20158.pdf (Page 17):

In our experiments, we notice that the number of output tokens is negligible compared to the number
of input tokens. Specifically, when making calls to GPT -3.5-Turbo, approximately 1000 tokens
are utilized for input, while calls to GPT -4use around 2000 tokens for input. This results in an
approximate cost of $1.02 per API query, with specific cost details provided in Table A1.
Table A1: The cost of API calls per query.
Model Number of API calls Cost ($)
GPT -3.5-Turbo 320 0.48
GPT -4 9 0.54
C H YPERPARAMETER TUNING
In our final re-ranking step with GPT -4, we conduct hyperparameter tuning for the window size
(w) and step size ( s), exploring two specific configurations: w,s = 20,10 and w,s = 10,5. Our
results indicate that the w,s = 10,5 configuration outperforms the w,s = 20,10 setup, with a higher
nDCG@10 score of 0.7390 compared to 0.7376.
D A DDITIONAL RESULTS
Extended results on SPARTA(Zhao et al., 2021), docT5query (Cheriton, 2019), GenQ (Thakur
et al., 2021), ColBERT (Khattab & Zaharia, 2020), BM25+CE (Wang et al., 2020), and TART -
Rerank (FLAN-T5-XL)(Asai et al., 2022) can be found in Table A2 and Table A3.
Table A2: Retrieval performance (nDCG@10) on BEIR datasets.
Method TREC-COVID NFCorpus Signal-1M (RT) TREC-NEWS Robust04 Touch´e-2020 DBPedia SciFact
BM25 59.5 30.8 33.0 39.5 40.7 44.2 31.8 67.9SPARTA 53.8 30.1 25.2 25.8 27.6 17.5 31.4 58.2docT5query 71.3 32.8 30.7 42.0 43.7 34.7 33.1 67.7DPR 33.2 18.9 15.5 16.1 25.2 13.1 26.3 31.8ANCE 65.4 23.7 24.9 38.2 39.2 24.0 28.1 50.7TAS-B 48.1 31.9 28.9 37.7 42.7 16.2 38.4 64.3GenQ 61.9 31.9 28.1 39.6 36.2 18.2 32.8 64.4ColBERT 67.7 30.5 27.4 39.3 39.1 20.2 39.2 67.1BM25+CE 75.7 35.0 33.8 43.1 27.1 40.9 68.8monoBERT(340M) 70.0 36.9 31.4 44.6 49.3 31.8 41.9 71.4monoT5(220M) 78.34 37.4 31.7 46.8 51.7 30.8 42.4 73.4monoT5(3B) 80.7 39.0 32.6 48.5 56.7 32.4 44.4 76.6TART -Rerank (FLAN-T5-XL)75.1 36.0 25.8 40.0 50.8 27.5 42.5 74.8UPR 68.1 35.0 31.9 43.1 42.4 19.7 30.9 72.7Promptagator++(zero-shot) 76.0 36.0 - - - 27.8 41.3 73.6Promptagator++(few-shot) 76.2 37.0 - - - 38.1 43.4 73.1RankGPT 85.5 38.5 34.4 52.9 57.6 38.6 47.1 75.0RRR(this work) 86.4 39.9 29.8 53.6 67.4 29.8 51.0 77.2
Table A3: Retrieval performance ( Recall@100) on BEIR datasets. For TREC-COVID, capped
Recall@100 is used.
Method TREC-COVID NFCorpus Signal-1M (RT) TREC-NEWS Robust04 Touch´e-2020 DBPedia SciFact
BM25 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5SPARTA 40.9 24.3 27.0 26.2 21.5 38.1 41.1 86.3docT5query 54.1 25.3 35.1 43.9 35.7 55.7 36.5 91.4DPR 21.2 20.8 16.2 21.5 21.1 30.1 34.9 72.7ANCE 45.7 23.2 23.9 39.8 27.4 45.8 31.9 81.6TAS-B 38.7 28.0 30.4 41.8 33.1 43.1 49.9 89.1GenQ 45.6 28.0 28.1 41.2 29.8 45.1 43.1 89.3ColBERT 46.4 25.4 28.3 36.7 31.0 43.9 46.1 87.8BM25+CE 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5monoBERT(340M) 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5monoT5(220M) 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5monoT5(3B) 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5RankGPT 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5RRR(this work) 54.8 32.4 32.4 51.6 45.4 52.2 55.0 94.3
18



Source: data\tc16_2312.10997v5\referenced_papers\[38]_2307.07164.pdf (Page 13):

Input
What happens next in this paragraph? How to survive remedial classes Look at the course as an opportunity.
Many students are discouraged when they are assigned to a remedial class. Some assume this placement
means they aren’t ready for college. OPTIONS:
A) However, people who are not unable to do what they’re given on campus, or those who are cut out
from college academies, are likely to have some little snitches. You want to be prepared for a negative
outcome if possible.
B) In this case, you should consider what you will do if your subject consists of a certain term or number
of subject areas. You could set up a study study program yourself or tutor a student who is struggling to
thoroughly comprehend where they sat for homework.
C) If you take the course, you might find you feel highly motivated after passing the test. Try to develop a
positive attitude towards the course so that you are not discouraged when you take your homework at the
end of the day.
D) However, being assigned a remedial class doesn’t mean that you are behind, just that you have an
opportunity to receive better instruction and improve your skills in a subject that you have struggled
with in the past. There is nothing unusual about being asked to attend a remedial course: two thirds of
community college students take at least one remedial course.
Output D
Table 9: Input-output format for GPT-35-Turbo. This example is from the HellaSwag dataset. We add some line
breaks for better readability.
Task Zero-shot Random Kmeans BM25 E5 base SBERT EPR LLM-R
1 iter 2 iter 3 iter
AESLC 5.8 19.4 19.0 26.8 27.0 25.3 26.0 26.7 27.3 27.1
AGNews 31.5 67.4 71.9 90.6 90.6 90.2 91.8 92.4 93.5 93.5
ARC Chall. 35.6 39.7 40.5 40.3 44.6 42.8 43.0 43.4 43.6 44.0
ARC Easy 51.0 60.0 61.8 59.9 63.0 63.1 63.1 63.6 63.3 63.6
BoolQ 64.7 70.0 69.0 74.7 72.4 73.9 74.8 75.6 75.1 74.1
CommonGen 19.2 36.3 34.4 37.6 37.4 37.6 39.2 38.2 37.7 37.3
COPA 66.0 80.0 85.0 78.0 83.0 82.0 82.0 84.0 84.0 84.0
DART 22.9 52.0 46.6 55.9 54.7 54.4 56.2 57.3 57.2 57.3
E2E NLG 34.6 52.7 46.4 54.5 51.8 50.2 53.6 54.9 54.7 54.9
Gigaword 15.3 30.0 30.7 32.7 32.5 32.6 32.4 33.3 32.5 31.8
HellaSwag 71.5 73.9 74.0 74.9 75.2 75.3 75.2 75.4 75.5 75.4
MNLI (m) 35.8 46.3 44.2 50.1 44.5 50.8 59.9 68.2 70.2 69.8
MNLI (mm) 35.6 48.1 45.4 48.3 44.7 49.3 61.5 69.5 72.0 71.3
MRPC 69.1 49.5 38.0 61.8 41.2 52.7 55.9 62.3 75.3 78.2
MultiRC 57.0 48.5 34.1 54.2 56.0 55.3 50.4 52.9 51.5 52.1
NQ 0.3 21.5 22.6 37.6 39.3 39.4 39.2 39.4 39.1 39.2
OpenBook QA 41.6 49.8 49.0 49.6 51.4 51.4 49.6 50.8 52.2 53.4
PAWS 53.2 57.0 56.6 56.6 55.4 58.2 57.7 57.0 56.6 57.0
PIQA 77.0 79.1 79.4 81.3 81.3 80.7 80.5 80.9 81.6 80.6
QNLI 49.2 56.4 53.4 62.2 61.5 61.9 65.0 74.4 69.6 69.4
QQP 57.7 63.4 63.3 79.8 77.5 81.3 81.7 80.1 82.6 83.3
RTE 59.6 59.9 58.5 65.7 63.9 67.2 66.8 67.2 68.6 70.4
Sentiment140 49.3 88.6 89.4 90.8 93.9 92.2 91.4 90.8 91.1 90.3
SNLI 39.8 43.7 52.5 47.1 53.5 58.4 68.4 80.2 82.0 82.2
SQuAD v1 2.1 64.1 62.3 61.2 60.8 61.6 64.3 60.7 57.3 52.5
SST2 54.4 85.9 89.7 84.4 92.1 87.6 88.7 94.0 93.8 93.1
Winogrande 62.0 66.7 66.5 67.5 66.9 66.5 66.5 67.9 68.1 67.2
WSC 64.4 60.6 56.7 56.7 61.5 63.5 61.5 60.6 63.5 66.4
WSC273 74.0 74.4 74.7 64.5 65.2 62.6 65.2 74.4 79.5 78.8
Yelp 47.9 92.0 93.5 93.5 97.3 95.9 95.1 95.7 95.9 95.5
Average 44.9 57.9 57.0 61.3 61.4 62.1 63.5 65.7 66.5 66.4
Table 10: Detailed results for each dataset.



Source: data\tc16_2312.10997v5\referenced_papers\[3]_2310.20158.pdf (Page 5):

each retrieved document z ∈ Rt using the relevance model σ (the prompt template for the σ LLM
is provided in Appendix A); the higher the score, the more relevant the document is to the original
query q. We add the documents in Rt that clear the relevance threshold τ, denoted by Ft, to the
output set S.
Stage 2. Rewrite: We take the top-Naug documents from Rt, denoted by Rt,Naug . This list, together
with the (rewritten) query qt, is appended to the prompt of the rewrite model g (see Appendix for
the A PPEND implementation) to derive πt and generate a new query rewrite qt+1. This trimmed
set Rt,Naug also helps work with limited prompt sizes of LLMs (e.g., GPT-3.5-turbo has a limit of
4097 tokens) when the documents are large. In Figure 1, given query q1 and R1,Naug , the rewriter g
generates a plausible rewrite q2 “Effects of artificial sweeteners on weight gain”.
We iterate the two stages until either |S| exceeds N or the number of rewrites exceeds Nrw. We
show empirically that the resulting S achieves high recall.
Stage 3. Re-rank: In the final stage, we first rank the documents S by their relevance scores w.r.t.
input query q, σ(q, z), z∈ S. This becomes the initial ordering for the LLM-based re-ranker model
h, which we implement using the technique of Sun et al. (2023). The algorithm outputs the re-ranked
list S∗ = h(S) for the input query q.
Table 1: Retrieval performance (nDCG@10) on BEIR datasets. Dataset-wise best score is marked
in bold and the second best is underlined.
Method TREC-COVID NFCorpus Signal-1M (RT) TREC-NEWS Robust04 Touch´e-2020 DBPedia SciFact
BM25 59.5 30.8 33.0 39.5 40.7 44.2 31.8 67.9DPR 33.2 18.9 15.5 16.1 25.2 13.1 26.3 31.8ANCE 65.4 23.7 24.9 38.2 39.2 24.0 28.1 50.7TAS-B 48.1 31.9 28.9 37.7 42.7 16.2 38.4 64.3monoT5(3B) 80.7 39.0 32.6 48.5 56.7 32.4 44.4 76.6Promptagator++(few-shot) 76.2 37.0 - - - 38.1 43.4 73.1RankGPT 85.5 38.5 34.4 52.9 57.6 38.6 47.1 75.0RRR(this work) 86.4 39.9 29.8 53.6 67.4 29.8 51.0 77.2
Table 2: Retrieval performance (Recall@100) on BEIR datasets. For TREC-COVID, capped Re-
call@100 is used. Dataset-wise best score is marked in bold and the second best is underlined.
Method TREC-COVID NFCorpus Signal-1M (RT) TREC-NEWS Robust04 Touch´e-2020 DBPedia SciFact
BM25 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5DPR 21.2 20.8 16.2 21.5 21.1 30.1 34.9 72.7ANCE 45.7 23.2 23.9 39.8 27.4 45.8 31.9 81.6TAS-B 38.7 28.0 30.4 41.8 33.1 43.1 49.9 89.1monoT5(3B) 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5RankGPT 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5RRR(this work) 54.8 32.4 32.4 51.6 45.4 52.2 55.0 94.3
4 E VALUATION SETUP
In this section, we give details of datasets, baselines, metrics, and implemention, before presenting
evaluation results in the subsequent section.
Table 3: Performance ( nDCG@k) on the TREC-DL20 dataset. Best score is marked in bold and
the second best is underlined.
Method nDCG @1 nDCG @5 nDCG @10
BM25 57.7 50.7 48.0
monoBERT (340M) 78.7 70.7 67.3
monoT5 (220M) 77.5 69.4 67.0
monoT5 (3B) 80.3 72.3 68.9
UPR 63.2 59.4 56.0
RankGPT 78.4 74.1 70.6
RRR (this work) 79.6 76.0 72.3
6



Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 6):

Preprint
Model STS12 STS13 STS14 STS15 STS16 STS-B SICR-R Avg.
Unsupervised Models
GloVe (avg.) † 55.14 70 .66 59 .73 68 .25 63 .66 58 .02 53 .76 61 .32
BERT-flow‡ 58.40 67 .10 60 .85 75 .16 71 .22 68 .66 64 .47 66 .55
BERT-whitening‡ 57.83 66 .90 60 .90 75 .08 71 .31 68 .24 63 .73 66 .28
IS-BERT ‡ 56.77 69 .24 61 .21 75 .23 70 .16 69 .21 64 .25 66 .58
CT-BERT‡ 61.63 76 .80 68 .47 77 .50 76 .48 74 .31 69 .19 72 .05
ConSERT-BERT 64.64 78 .49 69 .07 79 .72 75 .95 73 .97 67 .31 72 .74
DiffCSE-BERT 72.28 84 .43 76 .47 83 .90 80 .54 80 .59 71 .23 78 .49
SimCSE-BERT 68.40 82 .41 74 .38 80 .91 78 .56 76 .85 72 .23 76 .25
LLaMA2-7B ⋆ 50.66 73 .32 62 .76 67 .00 70 .98 63 .28 67 .40 65 .06
Supervised Models
InferSent-GloVe † 52.86 66 .75 62 .15 72 .77 66 .87 68 .03 65 .65 65 .01
USE † 64.49 67 .80 64 .61 76 .83 73 .18 74 .92 76 .69 71 .22
ConSERT-BERT 74.07 83 .93 77 .05 83 .66 78 .76 81 .36 76 .77 79 .37
CoSENT-BERT⋆ 71.35 77 .52 75 .05 79 .68 76 .05 78 .99 71 .19 75 .69
SBERT † 70.97 76 .53 73 .19 79 .09 74 .30 77 .03 72 .91 74 .89
SimCSE-BERT 75.30 84 .67 80 .19 85 .40 80 .82 84 .25 80 .39 81 .57
SimCSE-LLaMA2-7B ⋆ 78.39 89 .95 84 .80 88 .50 86 .04 87 .86 81 .11 85 .24
AnglE-BERT 75.09 85 .56 80 .66 86 .44 82 .47 85 .16 81 .23 82 .37
AnglE-LLaMA2-7B 79.00 90 .56 85 .79 89 .43 87 .00 88 .97 80 .94 85 .96
Table 2: Text embedding performance on STS tasks. We report the Spearman’s correlation ρ ×
100 of the “all” setting computed by SentEval. For supervised LLaMA-based models, we fine-
tuned them using the LoRA (Hu et al., 2021) technique and used the prompt “ Summarize sentence
{sentence} in one word:” sparked by (Jiang et al., 2023). Results marked with † are obtained from
(Reimers & Gurevych, 2019), while results marked with ‡ are retrieved from (Gao et al., 2021).
Additionally, results marked with ⋆ denote our own implementation using official code. For the
remaining baselines, we refer to the corresponding original papers to obtain their results.
data (x, x+, x−). This limitation might affect its performance. On the other hand, AnglE consis-
tently outperforms SBERT, achieving an absolute gain of 5.52%. This can support the idea that
angle-optimized text embedding can mitigate the negative impact of the cosine function, resulting
in better performance. Furthermore, we explore applying the long text model RAN base (86M pa-
rameters) (Li et al., 2023) as the backbone to test the performance on long text. The results show
that AnglE-BERT outperforms AnglE-RAN across all short text datasets. This advantage might be
attributed to the larger parameter size of BERT and its proficiency in handling short texts. How-
ever, we observe a remarkable shift in long-text STS. AnglE-RAN outperforms AnglE-BERT in this
scenario, suggesting that AnglE-RAN can handle long texts well despite having fewer parameters.
Model MRPC STS-B QQP QNLI GitHub Issues. Avg.
test test validation validation test
SimCSE-BERT 48.13 76 .27 65 .84 33 .00 60 .38 56 .72
SBERT 46.19 84 .67 73 .80 65 .98 69 .50 68 .03
AnglE-RAN 58.70 80 .23 74 .87 63 .04 71.25 69.62
AnglE-BERT 62.20 86 .26 76 .54 72 .19 70.55 73.55
Table 3: Results on the STS tasks. All baseline results are our implementation using the official
code. Spearman’s correlation (ρ × 100) serves as the reported metric.
In short, this evidence suggests AnglE’s superiority in transfer and non-transfer settings, its ability
to produce high-quality text embeddings, and its robustness and adaptability to different backbones.
7



### Claim 133/179

#### Claim Text
The basic idea can be demonstrated by writing the Navier-Stokes momentum equation (1.3) in the following simple semi-discrete form un+1 f −un f ∆t = RHSn+1/2 + fn+1/2 , (5.1) where the superscript indicates a discrete time level,RHS regroups the advective, viscous and pressure terms (which are evaluated at some intermediate time with index n + 1/2), and ∆t is the discrete time step.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 20):

the compute budget. We start with the Equation (1.6), repeated here for convenience:
L(N,S) =
(Nc
N
)αN
+
(Sc
S
)αS
. (B.1)
Here, S represents the number of parameter updates when training at the critical batch size [MKAT18],
which was deﬁned in Equation (5.2)9:
B(L) = B∗
L1/αB
. (B.2)
We would like to determine optimal training parameters for a ﬁxed compute budget, so we replace S =
C/(6NB (L)), where Cis the number of FLOPs used in the training run:
L(N,C) =
(Nc
N
)αN
+
(
6B∗Sc
N
L1/αBC
)αS
. (B.3)
Now, we set ∂NL
⏐⏐
C = 0to ﬁnd the condition for optimality:
0 = ∂L
∂N
⏐⏐
C
= −αN
N
(Nc
N
)αN
+ αS
N
(
6B∗Sc
N
L1/αBC
)αS (
1 −5N
L∂L
∂N
⏐⏐
C
)
=⇒ αN
αS
(Nc
N
)αN
=
(
6B∗Sc
N
L1/αBC
)αS
(B.4)
Equation (B.3) and (B.4) together determine the compute-efﬁcient frontier.
B.2 Efﬁcient Training
Now we assemble the implications of (B.3) and (B.4). First, note that inserting (B.4) into (B.3) yields
L(Neﬀ (C) ,C) =
(
1 +αN
αS
)
L(Neﬀ,∞) , (B.5)
which implies that for compute-efﬁcient training, we should train to a ﬁxed percentage αN
αS
≈10% above
the converged loss. Next, let’s determine how the optimal loss depends on the compute budget. Eliminating
N yields a power-law dependence of performance on compute:
L(C) =
(Cc
C
)αC
(B.6)
where we deﬁned
αC = 1/(1/αS + 1/αB + 1/αN) ≈0.052 (B.7)
Cc = 6NcB∗Sc
(
1 +αN
αS
)1/αS+1/αN (αS
αN
)1/αS
. (B.8)
Similarly, we can eliminate Lto ﬁnd N(C):
N(C)
Nc
=
(C
Cc
)αC/αN (
1 +αN
αS
)1/αN
(B.9)
and
S(C) = Cc
6NcB∗
(
1 +αN
αS
)−1/αN (C
Cc
)αC/αS
(B.10)
9There is a slight ambiguity here: we can imagine training either at a constant batch size B (Ltarget), or we could
instead train at a variable batch size ˜B (L), where ˜B is the instantaneous critical batch size (as opposed to B, which is
the averaged version). These two prescriptions result in the same number of steps, so we can ignore this subtlety (see
[MKAT18]).
21



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 12):

prediction for Bcrit, and that neither depends directly on model size except through the value of the loss that
has been attained. These results can be used to predict how training time and compute will vary with the
batch size. To utilize both training time and compute as effectively as possible, it is best to train with a batch
size B ≈Bcrit. Training at B ≫Bcrit minimizes the number of training steps, while B ≪Bcrit minimizes
the use of compute.
More speciﬁcally, it was demonstrated that for a wide variety of neural network tasks, the number of training
steps Sand the number of data examples processed E = BS satisfy the simple relation
( S
Smin
−1
)( E
Emin
−1
)
= 1 (5.1)
when training to any ﬁxed value of the lossL. Here Smin is the minimum number of steps necessary to reach
L, while Emin is the minimum number of data examples that must be processed.
We demonstrate the relation (5.1) for Transformers in Figure 18 in the appendix. This relation deﬁnes the
critical batch size
Bcrit(L) ≡Emin
Smin
(5.2)
which is a function of the target value of the loss. Training at the critical batch size makes a roughly optimal
time/compute tradeoff, requiring 2Smin training steps and processing E = 2Emin data examples.
In Figure 10 we have plotted the critical batch size and gradient noise scale5 as a function of training loss for
two different models. We see that Bcrit(L) is independent of model size, and only depends on the loss L. So
the predictions of [MKAT18] continue to hold for Transformer language models. The critical batch size can
be ﬁt with a power-law in the loss
Bcrit(L) ≈ B∗
L1/αB
(5.3)
where B∗≈2 ×108 and αB ≈0.21.
We have chosen this parameterization for Bcrit(L) because as the loss approaches its minimum value Lmin,
the gradient noise scale is expected to diverge, and we expect Bcrit to track this noise scale. We do not
know Lmin, as we see no sign that our models are approaching it, but Lmin >0 since the entropy of natural
language is non-zero. Since apparently Lmin is much smaller than the values ofLwe have achieved, we used
a parameterization where Bcrit diverges as L→0.
We will use Bcrit(L) to estimate the relation between the number of training steps S while training at batch
size B = 219 tokens and the number of training steps while training at B ≫Bcrit. This is simply
Smin(S) ≡ S
1 +Bcrit(L)/B (minimum steps, at B ≫Bcrit) (5.4)
for any given target valueLfor the loss. This also deﬁnes a critical value of the compute needed to train to L
with a model of size N if we were to train at B ≪Bcrit(L). This is
Cmin(C) ≡ C
1 +B/Bcrit(L) (minimum compute, at B ≪Bcrit) (5.5)
where C = 6NBS estimates the (non-embedding) compute used at batch size B.
5.2 Results for L(N,Smin) and Performance with Model Size and Compute
Now we will use Smin deﬁned in Equation (5.4) to obtain a simple and universal ﬁt for the dependence of the
loss on model size and training time in the inﬁnite data limit. We will ﬁt the stable, Adam-optimized training
runs using Equation (1.6), repeated here for convenience:
L(N,Smin) =
(Nc
N
)αN
+
( Sc
Smin
)αS
(5.6)
for the loss. We include all training steps after the warmup period of the learning rate schedule, and ﬁnd a ﬁt
to the data with the parameters:
5Although the critical batch size roughly matches the gradient noise scale, we are using a direct measurements of
Bcrit from Figures 18 and 10 for all our later analyses.
13



Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 19):

Appendices
A Summary of Power Laws
For easier reference, we provide a summary below of the key trends described throughout the paper.
Parameters Data Compute Batch Size Equation
N ∞ ∞ Fixed L(N) = (Nc/N)αN
∞ D Early Stop Fixed L(D) = (Dc/D)αD
Optimal ∞ C Fixed L(C) = (Cc/C)αC
(naive)
Nopt Dopt Cmin B ≪Bcrit L(Cmin) =
(
Cmin
c /Cmin
)αmin
C
N D Early Stop Fixed L(N,D) =
[(Nc
N
)αN
αD + Dc
D
]αD
N ∞ Ssteps B L(N,S) =
(Nc
N
)αN
+
(
Sc
Smin(S,B)
)αS
Table 4
The empirical ﬁtted values for these trends are:
Power Law Scale (tokenization-dependent)
αN = 0.076 Nc = 8.8 ×1013 params (non-embed)
αD = 0.095 Dc = 5.4 ×1013 tokens
αC = 0.057 Cc = 1.6 ×107 PF-days
αmin
C = 0.050 Cmin
c = 3.1 ×108 PF-days
αB = 0.21 B∗= 2.1 ×108 tokens
αS = 0.76 Sc = 2.1 ×103 steps
Table 5
The optimal parameters for compute efﬁcient training are given by:
Compute-Efﬁcient Value Power Law Scale
Nopt = Ne ·CpN
min pN = 0.73 Ne = 1.3 ·109 params
B ≪Bcrit = B∗
L1/αB = BeCpB
min pB = 0.24 Be = 2.0 ·106 tokens
Smin = Se ·CpS
min (lower bound) pS = 0.03 Se = 5.4 ·103 steps
Dopt = De ·CpD
min (1 epoch) pD = 0.27 De = 2·1010 tokens
Table 6
B Empirical Model of Compute-Efﬁcient Frontier
Throughout this appendix all values of C,S, and αC are adjusted for training at the critical batch size Bcrit.
We have left off the ‘adj’ label to avoid cluttering the notation.
B.1 Deﬁning Equations
The power-law ﬁt to the learning curves implies a simple prescription for compute-efﬁcient training. In this
appendix, we will derive the optimal performance, model size, and number of training steps as a function of
20



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 13):

104 106 108
Parameters (non-embedding)
2
3
4
5
6
7
8Test Loss
Performance vs Compute Budget
10 5
10 4
10 3
10 2
10 1
100
PF-dayss
106 107 108 109
Parameters (non-embedding)
2.4
3.0
3.6
4.2
4.8
5.4Test Loss
Performance vs Steps
104
105
Steps
Figure 11 When we hold either total compute or number of training steps ﬁxed, performance follows
L(N,S) from Equation (5.6). Each value of compute budget has an associated optimal model size that
maximizes performance. Mediocre ﬁts at small Sare unsurprising, as the power-law equation for the learning
curves breaks down very early in training.
Parameter αN αS Nc Sc
Value 0.077 0.76 6.5 ×1013 2.1 ×103
Table 3 Fits to L(N,S)
With these parameters, we obtain the learning curve ﬁts in Figure 4. Though the ﬁts are imperfect, we believe
they are quite compelling given the simplicity of Equation (5.6).
The data and ﬁts can be visualized in a different and more interesting way, as shown in Figure 11. There we
study the test loss as a function of model size while ﬁxing either the total non-embedding compute C used
in training, or the number of steps S. For the ﬁts we use Equation (5.5) and (5.4) along with the parameters
above and Equation (5.6).
The power-law dependence of the loss on Smin reﬂects the interplay of optimizer dynamics and the loss
landscape. Since the ﬁts are best late in training, when the loss may be approximately quadratic, the power-
law should provide information about the spectrum of the Hessian of the loss. Its universality suggests that
the Hessian eigenvalue density is roughly independent of model size.
5.3 Lower Bound on Early Stopping Step
The results for L(N,Smin) can be used to derive a lower-bound (and rough estimate) of the step at which
early stopping should occur when training is data limited. It is motivated by the idea that ﬁnite and inﬁniteD
learning curves for a given model will be very similar until we reach Smin ≈Sstop. Thus overﬁtting should
be proportional to the correction from simply ending training atSstop. This will underestimate Sstop, because
in reality the test loss will decrease more slowly when we have a ﬁniteD, and therefore we will require more
training steps to reach the optimal test loss at ﬁnite D. This line of reasoning leads to the inequality
Sstop(N,D) ≳ Sc
[L(N,D) −L(N,∞)]1/αS
(5.7)
where L(N,∞) is the converged loss, evaluated with inﬁnite available data. This inequality and its com-
parison to the empirical data is displayed in Figure 16 in the appendix. In that ﬁgure, the values of Sstop
and L(N,D) are empirical (though Sstop is adjusted to mimic training at B ≫Bcrit), while L(N,∞) is
computed from the ﬁt to L(N,D) evaluated at D= ∞.
6 Optimal Allocation of the Compute Budget
We displayed the empirical trend of performance as a function of the computation used during training in
the top-right of Figure 1. However, this result involved training at a ﬁxed batch size B, whereas we know
14



### Claim 134/179

#### Claim Text
Developing personalized vaccination strategies based on different age groups, such as mass vaccination with transmission-blocking vaccines for adults or prioritizing vaccine distribution to the elderly, can improve vaccine effectiveness and suppress infection to some extent .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[13]_2209.10063.pdf (Page 3):

Published as a conference paper at ICLR 2023
What does Monsanto own? (WebQ) 
QuestionDocumentCluster𝑞 𝑑 …𝑞!" 𝑑!" 𝑐… … …𝑞!# 𝑑!# 𝑐… … …
Step1:Getonedocument𝑑foreachquestion𝑞viaretrievalorgeneration.
Step2:Getembeddings, andcluster them by K-means.
Initial 𝑑:Monsanto is a multinational agrochemical and agricultural biotechnology corporation…It is one of the world’s leading producers of roundup, a gly-phosateherbicide.(63words)Step3:Givenquestion 𝑞fortrainingorinference,foreachcluster𝑐∈1…𝑘:•sample𝑞!#,𝑑!#,𝑗=1…𝑛,whoseclusteridis𝑐;•createprompt𝑝!=“𝑞!";𝑑!";…;…;𝑞!$;𝑑!$”;•generatedocument𝑑!with𝑝!usinga large language model.Usinga reader (e.g., FiD),with𝑞andthediversedocuments{𝑑",𝑑%,…,𝑑&},findanswers𝑎.
•agriculturalchemicals•seed (also correct)
Generated𝑑!:Monsanto Company is an Ame-rican multinational agrochemical and agricultural biotechnology corporation …It is a leading producer of genetically engi-neered seedand … (70 words)
Generated𝑑":Monsanto is a multinational agricultural biotechnology corporation. … The company also manufactures other agricultural chemicals, such as insecticides … (36 words) 
Figure 1: An overall framework of clustering-based prompting method. It leverages distinct question-
document pairs sampled from each embedding cluster as in-context demonstrations to prompt a large
language model to generate diverse documents, then read the documents to predict an answer.
the MAP estimate ˆd = arg max ˆp(d) using beam search, and then to approximate the sum over d
with this single value. This two step approach, we label it as a generate-then-read pipeline.
STEP 1: G ENERATE . In this step, we ﬁrst prompt a large language model (e.g., InstructGPT (Ouyang
et al., 2022)) to generate documents based on the given question. For example, the input to the
language model could be “Generate a background document to answer the given question. {question
placeholder}”. We can use any decoding strategy (e.g., greedy decoding, beam search), but we used
greedy decoding throughout the zero-shot experiments for simplicity and reproducibility.
STEP 2: R EAD . In the second step, we use generated sentence ˆdalong with the input question to
produce the ﬁnal answer from the large language model. This is actually the same setting as “zero-
shot” reading comprehension, as widely studied in existing works (Brown et al., 2020; Lazaridou
et al., 2022). We choose appropriate prompts from P3 (Bach et al., 2022), such as “Refer to the
passage below and answer the following question. Passage: {background placeholder}Question:
{question placeholder}”. Finally, the language model is fed the prompted text to generate an answer.
3.2 S UPERVISED SETTING
Although large language models demonstrate impressive performance on zero-shot learning abilities,
their performance still lag behind the supervised setting. Therefore, we also explore how the generated
documents from large language models can beneﬁt the supervised setting. As directly ﬁne-tuning
large language models on downstream datasets could be prohibitively expensive, we leverage a small
reader model such as FiD to peruse the generated documents under the supervised setting.
Under the supervised setting, scaling the size of retrieved documents can lead to better perfor-
mance (Karpukhin et al., 2020; Izacard & Grave, 2021). This is mainly because retrieving more
documents can cover more relevant information and knowledge, i.e., a higher recall score. Nev-
ertheless, asking a large language model to generate multiple high-quality contextual documents
is a challenging task. Dense retrieval methods can fetch multiple documents covering different
perspectives of the answer. Compared to dense retrievers, simply prompting a large language model
to generate multiple contextual documents often leads to low knowledge coverage, since the contents
generated by multiple decoding passes from the same input tend to be similar. Sampling decoding
methods, such as nucleus sampling1 (Holtzman et al., 2020) can diversify the generation process to
some extent, but the knowledge content of generated texts still tends to be highly repetitive when
used to generate documents for a given question. We further propose two novel solutions, including
diverse human prompts and clustering-based prompts, which will be elaborated on in this section.
3.2.1 D IVERSE HUMAN PROMPTS
In order to avoid similar token distributions under a single prompt, we ask human annotators to
provide different prompts, in order to make the generated document diverse. This method is simple,
but can effectively vary the token distribution during generation. In the experiments, we empirically
found this method can bring improvement to the retrieval performance (Figure 2). However, this
1We treated nucleus sampling as a baseline to generate multiple documents, in which we set p = .95.
4



Source: data\tc16_2312.10997v5\referenced_papers\[22]_2210.01296.pdf (Page 4):

Published as a conference paper at ICLR 2023
when was child benefit 
paid for the first child?
Finetuned
Reciter-LM
Passage hint: Child support --- 
Short description --- Paragraph #1
Passage hint: Child benefit --- 
Short description --- Paragraph #1
Passage hint: Child benefit --- 
History --- Paragraph #2
Sampling diverse passage hints
Greedy decoding recitations
1911
Question 
answering with 
aggregated 
recitations
Frozen 
Language
Model
<Passage hint K>
<Passage K>
<Generated 
Question K>
Passage hint fine-tuning
for diverse recitation
<Passage 0>
<Question 0>
…
<Passage K>
LM input
<Generated 
Question K>
LM Output
few-shot question
generation
fine-tuning
LM input LM Output
prediction <Passage hint N>
<Recitation N><Question N>
LM input LM Output
Recitation: The first child support 
law was passed in 1911 in the 
United Kingdom…
Recitation: The first child benefit 
was introduced in the United 
Kingdom in 1911, and was paid to…
Recitation: Child benefit is a 
payment made to parents in the 
United Kingdom, Ireland, 
Australia…
QA with diverse 
recitation augmentation
Figure 3: Illustration of question answering with diverse recitation and the corresponding few-shot
question generation and ﬁne-tuning processes.
Self-consistency ensemble The factual knowledge about a question can appear in several places
in the language model’s training corpora. For example, the fact of “Queen Elizabeth II opened
the London Bridge on 17 March 1973” can appear in both Wikipedia page “London Bridge” and
page “March 1973”, so it is highly likely that there exists knowledge from different articles that
could lead to the same, correct answer. With this motivation, we argue that similar to multi-step
reasoning in chain-of-thought, recitation-augmented question answering can also beneﬁt from the
self-consistency technique with multiple-path decoding (Wang et al., 2022b). Speciﬁcally, given an
arbitrary question, we ﬁrst use top-ksampling to independently generate a few recitations, and then
greedy decode the answer of the question based on the sampled recitations. Finally, we determine
the optimal answer by taking a plurality/majority vote (Step 3 in Figure 2).
Multiple-recite-and-answer for multi-hop question-answering Multi-hop question answering
requires the QA system to ﬁnd and reason over multiple supporting documents. However, the nature
of recitation restricts us to recite passages from one article at a time. In order to apply the recite-and-
answer scheme to solve multi-hop questions, we introduce multiple-recite-and-answer scheme (Step
4 in Figure 2), that is, given the multiple-hop question, we use the prompt words such as “Recitation
1” and “Recitation 2” to elicit the LLM to generate recitation passages on different topics. Since
the multiple recited passages are generated in one-pass from the LLM decoding sequentially, the
generation of later passages can effectively utilize the information both in the original question and
the previous recited ones. Our multiple-recite-and-answer scheme for multi-hop question-answering
is also compatible with the self-consistency technique, by applying top-ksampling when generating
multiple recitations and performing majority voting for the ﬁnal answers.
3.2 P ASSAGE HINT -BASED DIVERSIFIED RECITATION WITH FINE -TUNING
Passage hint-based diversiﬁed recitation While the sampling-based recitation and self-
consistency improves the robustness of recite-and-answer method, one argument for its inefﬁciency
is that if the evidence-recitation module samples the wrong facts about the question, the question-
answering module will not be able to ﬁgure it out and tend to generate the wrong answer. Therefore,
on the one hand, we need to use a low sampling temperature to avoid generating recitations with
wrong facts, on the other hand, we want to make sure the sampled recitations have enough diversity.
To tackle such a dilemma, we propose passage hint-based diversiﬁed recitation . We observe that
in well-formed text knowledge bases, such as Wikipedia, we can usually ﬁnd a unique passage hint
for each passage, by concatenating the section titles and the in-section order of each passage. For
example, the passage hint of the second passage in Section 5.2 “Enforcement” of Wikipedia page
5



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 29):

Jiawei Zhang
Table 8: Analysis of generated API query cases different from ground-truth queries in the testing set.
Input
Sources
Language Generation Examples
Input Payload Generated Outputs Before Fine-Tuning Generated Outputs After Fine-Tuning
Pile
Data
Roman Catholic Diocese of Tambacounda\n\nThe Ro-
man Catholic Diocese of Tambacounda () is a diocese
located in the city of Tambacounda in the Ecclesiastical
province of Dakar in Senegal.
\n\nHistory\n\nThe Diocese of Tambacounda
was erected by Pope Pius XI on December 21,
1933, on territory split off from the Diocese of
Saint-Louis du Sénégal. \n\nBishops\n\nDiocese
of Tambacounda \nErected: December 21,
1933\nMetropolitan: Archdiocese of Dakar\n
center>The Roman Catholic Diocese of Tambacounda
() is a diocese located in the city of Tambacounda in
the <center>Ecclesiastical province of Dakar in Sene-
gal.</center> center>Output: </center> center>The
<center>Roman</center> <center>Catholic
Mud Hens pitcher Evan Reed charged with sexual
assault\n\nMud Hens pitcher Evan Reed was charged
July 30 with sexual assault related to a March incident
in Detroit when he was a member of the Detroit Tigers.
\n\nMud Hens pitcher Evan Reed was charged July 30
with sexual assault related to a March incident in Detroit
when he was a member of the Detroit Tigers.\n\nMud
Hens pitcher Evan Reed was charged July 30 with sexual
assault related to a March incident in Detroit when he
was a member of the Detroit Tigers.\n\nMud Hens
pitcher Evan Reed was charged July 30 with sexual
assault related to
center>###<centercentercentercentercentercentercenter
centercentercentercentercentercentercentercenter
centercentercentercentercentercentercentercenter
centercentercentercentercentercentercentercenter
centercentercentercentercenter
Random
Sources
Thursday’s verdict only added to the intrigue surround-
ing the gravest legal and political unknown from the
2021 Capitol insurrection that hangs over the 2024 cam-
paign: will Donald Trump, the president who inspired
the uprising, face his own legal and political price?
I don’t think there’s any question that he’s going to be
impeached, ” Rep. Al Green (D-TX) told The Daily Beast.
“I think he’s going to be impeached, and I think he’s
going to be removed from office. ”\n\nGreen’s
will be the verdict only added to the intrigue surround-
ing the gravest legal and political unknown from the
2021 Capitol insurrection that hangs over the 2024 cam-
paign: will Donald Trump, the president who inspired
the uprising, face his own legal and political price?br>
A source of debate for centuries, the village of Ponte
Buriano, a suburb of Arezzo in the Tuscany region of
Italy, is so convinced the bridge behind Mona Lisa is
the Ponte Buriano that they’ve made it a key feature of
their local tourism campaign, even laying claim to the
bridge on the village’s welcome sign.
\n\nThe Ponte Buriano is one of the oldest bridges in
Tuscany, dating back to the 12th century. It’s also one of
the most famous, thanks to Leonardo da Vinci’s Mona
Lisa, the most famous painting in the world.\n\nThe
The village of [GR(GL("mona_lisa"), "bpr:search",
GR(GL("ponte_buriano"), GR(GL("search"),
GR(GL("mona_lisa"), GR(GL("search
•Efficiency: With LoRA and quantized models/optimizers,
we can reduce the model fine-tuning memory capacity re-
quirement to less than 11GB and the memory capacity re-
quirement even lower for the model inference stage. Mean-
while, integrated with the large-sized graph data, pre-trained
graph models, and necessary pre-processed data, the effi-
ciency of Graph-ToolFormer for various graph reasoning
task can still be a problem. In this paper, we introduce a
tentative approach to make the problem less severe with
the working memory. However, if we plan to deployGraph-
ToolFormer on devices with very small memories, like cell-
phones or embedded equipments, new techniques will still
be needed to improve the model learning and inference effi-
ciency.
•Diverse Applications : Due to the limited space, we can
only study a few number of the graph reasoning tasks with
Graph-ToolFormer in this paper. Meanwhile, in the real-
world, we have lots of graph structured data that may require
the LLMs to handle them to reason for the desired outputs.
Therefore, a very promising future work direction is to apply
Graph-ToolFormer to study diverse real-world graph/net-
work data oriented reasoning tasks with LLMs. We list a
few of them here just for the readers’ information, and the
readers may explore more diverse reasoning tasks according
to your own backgrounds and expertises.
– Urban Computing and Smart City : In the offline world,
we have extensively connected traffic networks that bridge
different local communities, cities and countries by lo-
cal roads, national highways, international fights and
ocean freight corridors. Applying LLMs for knowledge
extraction and reasoning based on such traffic networks
is critical for the current urban computing and smart city
projects.
– IoT and Smart Home : Assisted with the 5G, the IoT net-
work effectively bridges the cyber world with the physical
devices and equipments together via extremely fast com-
munication channels. The LLMs provide the opportunity
for us to utilize language models as the general interface
for controlling the devices within the IoT networks, which
is also the main objective for building the smart home
system.
– Healthcare : During the past years, the world has suf-
fered a lot from the covid-19 pandemic. Similar to the
protein molecules studied in this paper, both the virus
and the vaccines can also be represented as the molecular
graphs. LLMs with the molecular graph reasoning ability
have the potential to improve our current healthcare sys-
tem in many perspectives, like early identification of virus ,
analysis of the virus pathogenicity and creation of vaccines .
What’s more, the LLMs with the social network reasoning
ability will also help infer the potential virus propagation
among people , early prediction of highly infectious commu-
nities and identify rumors and misinformation about the
pandemic (at the online social networks).



Source: data\tc16_2312.10997v5\referenced_papers\[43]_2308.07922.pdf (Page 3):

Published as a conference paper at COLM 2024
Natural Questions TriviaQA
0-shot 1-shot 5-shot 8-shot 0-shot 1-shot 5-shot 8-shot
ATLAS 11B S1 26.7 21.3 29.8 31.3 56.9 35.5 62.3 63.9
ATLAS 11B S2 21.4 16.3 9.8 49.8 48.4 44.4
Table 1: Results of ATLAS 11B with prompting strategy 1 (S1) and strategy 2 (S2).
Natural Questions TriviaQA
first 0.7 9.2
random 6.5 19.5
last 29.8 62.3
Table 2: Results of ATLAS 11B (5-shot) with different target question positions.
Enc: Question: q1 Answer: a1 . . .Question: qk Answer: ak Question: q0 Answer:<extra id 0>
d
where (q1, a1), . . ., (qk, ak) represent example QA pairs, q0 denotes the target question,
<extra id 0> is a sentinel token (Raffel et al., 2020), and d is the relevant passage retrieved
with q0. An example in a 2-shot setting is illusated in Figure 1 (middle).
Strategy 2. As the decoder of the encoder-decoder model can also accept input, we can
feed the answers of in-context examples to the decoder and only feed the questions to the
encoder, using multiple sentinel tokens:
Enc: Question: q1 Answer:<extra id 0> . . . Question: qk Answer:<extra id (k − 1)> Ques-
tion: q0 Answer:<extra id k> d
Dec: <extra id 0> a1. . . <extra id (k − 1)> ak
Figure 1 (right) demonstrates an example. The model is expected to learn from in-context
examples by examining both the input to the encoder and input to the decoder.
We select two widely-used datasets in the domain of open-domain question answering for
the preliminary study: Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA
(TQA) (Joshi et al., 2017)3. Table 1 summarizes the results. We find that the model struggles
to learn from in-context examples using strategy 2, as the few-shot performance is worse
than the zero-shot performance. We hypothesize that this is because the model has difficulty
learning the pattern of S2 with masked language modeling during its pretraining, since it is
unlikely to obtain several consecutive question-answer pairs (or something similar) in the
form of strategy 2 by randomly masking several spans in a sequence.
On the other hand, we observe that with strategy 1, the model does exhibit some in-context
learning ability, where the 5-shot and 8-shot performance is significantly better than the
zero-shot performance on both NQ and TriviaQA. Therefore, we choose to focus on strategy
1 for further study and disregard strategy 2 for the remainder of the paper.
Effect of Position. As the encoder of encoder-decoder language models is bidirectional,
it can also examine in-context examples that follow the target question to fill in the masked
token. This means that we may position the target question at the beginning or middle
of a sequence, for example:
Question: q0 Answer:<extra id 0> Question: q1 Answer: a1 . . . Question: qk Answer: ak d
Question: q1 Answer: a1 . . .Question: q0 Answer:<extra id 0>. . .Question: qk Answer: ak d
Table 2 summarizes the results. We denote the target question’s position as “first” for the
beginning of the sequence, “random” for a random position, and “last” for the original setting
(S1). Interestingly, placing the target question anywhere other than the last position results
3Experimental setup is detailed in the Appendix B.1.
4



Source: data\tc16_2312.10997v5\referenced_papers\[43]_2308.07922.pdf (Page 19):

Published as a conference paper at COLM 2024
Question How does a painting like “When Will You Marry” sell for $300 million
dollars when it doesn’t look like anything spectacular
Human When it comes to a lot of art, context is more important than the actual material.
Rarity, historical context etc. make a thing more valuable than its actual
aesthetic properties. For example, some shitty buggy SNES or N64 games will
sell for huge amounts if they are well preserved and in the original packaging
etc, even when any high school student nowadays could write a better game
in a week.
ATLAS 1-shot It’s a masterpiece.
RAVEN 1-shot It’s not the painting that sells for $300 million dollars, it’s the story behind the
painting.
RAVEN FiCL [40-1] It’s not the painting that sells for $300 million dollars, it’s the reputation of the
artist.
Question When the society goes fully automatized, what will be the “normal” people
needed for?
Human Call centers... I am being totally serious. If your Sony TV is being completely
fabricated by machines, shipped by machines, sold on amazon and delivered
by machines, one of the few value adds companies will be able to deliver is
GOOD customer service. So calling that 1-800 number and being instantly
connected to a knowledgeable, friendly, human being who is going to patiently
help you fix whatever issue you have (FYI... you forgot to plug it in).
ATLAS 1-shot ’> ELI5: What will the society go fully automatized? >
RAVEN 1-shot The ones who will program the robots.
RAVEN FiCL [40-1] The “normal” people will be needed to program the robots.
Question Why is a 2 liter of Coca-cola $1, a 12 oz bottle $1.49 and the same size bottle
of water $1.99?
Human Because people will pay. Plain and simple. Capitalism at its best.
ATLAS 1-shot The price of the product is based on the cost of the packaging.
RAVEN 1-shot The cost of packaging is proportional to the size of the container.
RAVEN FiCL [40-1] The cost of packaging is the largest portion of the cost of a product. The larger
the container, the less packaging is required.
Question If Matter Cannot Be Created or Destroyed... How do trees form?
Human Say you have a box of Legos with 500 pieces. With those 500 pieces, you build a
bulldozer. Then one day, you decide to rearrange those pieces from a bulldozer
to a monster truck. It’s still the same 500 pieces, just in a different layout. This
is how trees are formed. The matter is just rearranged from other objects. Soil,
nutrients, water, and even air. This is called “Conservation of Mass.” EDIT:
added air. Thanks. :)
ATLAS 1-shot Trees are made of carbon and oxygen.
RAVEN 1-shot Matter can be created or destroyed by combining two or more different types
of matter.
RAVEN FiCL [40-1] The tree is a living organism. It is made up of matter. The tree is made up of
carbon, hydrogen, oxygen, and water.
Question If the flu shot contains a non-active version of the flu, then why are “flu like
symptoms” common for the days following?
Human the non-active proteins trigger an immune system response which can involve
some inflammatory system responses. once the immune system is activated
for that infection your body will react more readily if the actual infection gets
into you.
ATLAS 1-shot The flu shot contains live virus particles.
RAVEN 1-shot The flu shot contains a live attenuated virus. This means that it is not able to
infect the body, but it can still cause some side effects.
RAVEN FiCL [40-1] The flu shot contains a weakened version of the virus. This weakened virus is
able to trigger an immune response in the body. This immune response can
cause flu like symptoms.
Table 7: Example outputs of ATLAS and RAVEN 11B on long-form question answering.
20



### Claim 135/179

#### Claim Text
As a consequence, feed-back forcing is typically avoided in PR-DNS applications. 5.2.1 Direct forcing immersed boundary method Mohd-Yusof (cf. also ) realized that the feed-back procedure can be circumvented by constructing the forcing term such that the “desired” velocity is directly obtained after each time step.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 32):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: A liquid flows at a constant flow rate through a pipe
with circular cross-sections of varying diameters. At one point in the
pipe, the diameter is 2 cm and the flow speed is 18 m/s. What is the flow
speed at another point in this pipe, where the diameter is 3 cm?
0) 4 m/s
1) 6 m/s
2) 8 m/s
3) 12 m/s
Original Answer [Incorrect]: The answer is 3) 12 m/s A step-by-step
solution is below: The flow rate is constant, so the product of the speed
and the cross-sectional area is constant. The cross-sectional area is
proportional to the square of the diameter, so the speed is inversely
proportional to the square of the diameter. The ratio of the speeds is the
inverse of the ratio of the squares of the diameters, so the speed at the
second point is 18m/s ∗ (2cm/3cm)2 = 12 m/s.
Stepback Question: What are the physics principles behind this
question?
Principles:
Conservation of Mass: the amount of mass flowing through the pipe at
two different points must be equal, or A1 * v1 = A2 * v2, where A1 is
the area of cross-section one, v1 is the velocity at point 1, A2 is the area
of cross-section two, v2 is the velocity at point 2.
Area of Circle: A = π ∗ r2, where r is the radius of the circle.
Final Answer: Using the Principles of Continuity equation, we can
solve the problem as following:
Step 1: Apply the Continuity equation to find out the flow speed at
another point in this pipe. The continuity equation states that the product
of the area and the velocity is constant at all points in a pipe. In this
problem, we are given the following information:
A1 = π ∗ (2cm)2 = 4πcm2
v1 = 18 m/s
A2 = π ∗ (3cm)2 = 9πcm2
We can use the continuity equation to find v2:
A1 * v1 = A2 * v2
4πcm2 ∗ 18m/s = 9πcm2 ∗ v2
v2 = 8 m/s
Therefore, the flow speed at another point in this pipe is 8 m/s.
The correct answer is 2) 8 m/s.
Table 29: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
33



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 31):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: What happens to the pressure, P, of an ideal gas if
the temperature is increased by a factor of 2 and the volume is increased
by a factor of 8 ?
0) P decreases by a factor of 16.
1) P decreases by a factor of 4.
2) P decreases by a factor of 2.
3) P increases by a factor of 4.
Original Answer [Incorrect]: The correct answer is 0. The ideal gas
law is PV = nRT. If the temperature is increased by a factor of 2 and the
volume is increased by a factor of 8, then the pressure will decrease by a
factor of 16.
Stepback Question: What are the physics principles behind this
question?
Principles:
Ideal gas law: PV = nRT, where P is the pressure, V is the volume, n is
the number of moles, R is the gas constant, and T is the temperature.
Final Answer: Using the Principles of Ideal gas law, we can solve the
problem as following:
Step 1: Apply Ideal gas law to find out the pressure of an ideal gas. The
ideal gas law states that PV = nRT, where P is the pressure, V is the
volume, n is the number of moles, R is the gas constant, and T is the
temperature.
Step 2: Apply Ideal gas law to find out the pressure of an ideal gas when
the temperature is increased by a factor of 2 and the volume is increased
by a factor of 8. If the temperature is increased by a factor of 2, then T be-
comes 2T. If the volume is increased by a factor of 8, then V becomes 8V .
Substituting these values into the ideal gas law, we get: P(8V) = nR(2T)
Dividing both sides by 8V , we get: P = nR(2T) / 8V
We can see that the pressure has decreased by a factor of 4.
Therefore, the correct answer is 1) P decreases by a factor of 4.
Table 28: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
32



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 30):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question Physics students are checking the constant acceleration equations of kinematics
by measuring the velocity of a tennis ball that is dropped and falls 6 meters
and then passes through a photogate. The predicted velocity is 20% above the
velocity measured by the photogate. Which of the following best describes the
cause of the large percent difference?
0) The ball changes its shape while falling.
1) The acceleration of gravity varies as the ball is falling.
2) Air resistance increases the acceleration of the ball.
3) The acceleration of the balls varies with the velocity.
Principles Vertical motion: y = 0.5 * g * t2, where y is the vertical displacement, g is the
acceleration due to gravity, t is the time.
Free fall: is a special case of motion with constant acceleration, in which the
acceleration is due to gravity.
Acceleration to Velocity Formula: v = u + a * t, where v is the final speed, u is
the initial velocity, a is the acceleration, and t is the time.
Answer Using the Principles of Vertical motion and Horizontal motion, we can solve
the problem as following:
Step 1: Apply the Vertical motion equation to find the time it takes for the ball
to fall 6 meters.
y = 0.5 * g * t2
6 m = 0.5 * 9.8 m/s2 * t2
t = 1.12 s
Step 2: Apply the Horizontal motion equation to find the velocity of the ball
when it passes through the photogate.
x = v * t
v = x / t
v = 6 m / 1.12 s
v = 5.36 m/s
Step 3: Compare the predicted velocity with the velocity measured by the
photogate. The predicted velocity is 20% above the velocity measured by the
photogate, so the predicted velocity is 1.2 * 5.36 m/s = 6.43 m/s.
Step 4: Find the cause of the large percent difference. The predicted velocity is
6.43 m/s, but the velocity measured by the photogate is 5.36 m/s. The difference
is 1.07 m/s. This difference is caused by air resistance. Air resistance is a force
that opposes the motion of an object through the air. It is caused by the friction
between the object and the air molecules. The greater the surface area of the
object, the greater the air resistance. The tennis ball has a large surface area, so
it experiences a lot of air resistance. This air resistance slows down the tennis
ball and causes the difference between the predicted velocity and the velocity
measured by the photogate.
Therefore, the correct answer is 0) The ball changes its shape while falling.
Table 27: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Reasoning Error. In this case, the model was able to correctly identify air resistance is the cause of
the velocity difference, but made the error at the final reasoning step of deriving the right answer.
31



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 28):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question Two spheres of net charge +5e and -6e briefly come into contact. Afterward,
which of the following is a possible combination of net charges for the two
spheres?
0) +3e and -4e
1) +1.5e and -2.5e
2) Both of the above
3) None of the above
Principles Law of conservation of charge: the total charge in a closed system remains
constant.
Coulomb’s Law: the mutual repulsion of like charges demands that the charges
be as far apart as possible.
Answer Using the Principles of Conservation of charge, we can solve the problem as
following:
Step 1: Apply Conservation of charge to find out the possible combination of
net charges for the two spheres.
The total charge of the two spheres before they come into contact is +5e - 6e =
-1e. After they come into contact, the total charge of the two spheres must still
be -1e.
Step 2: Find out the possible combination of net charges for the two spheres.
The possible combination of net charges for the two spheres are:
(1) +3e and -4e
(2) +1.5e and -2.5e
Therefore, the correct answer is 2) Both of the above.
Table 25: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Principle Error. In this case, Coulomb’s Law is not needed for solving this problem. Instead, the
Charge Quantization Principle is missing from the retrieved principles, and is in fact needed to rule
out option (2) of fractional charges.
29



Source: data\tc16_2312.10997v5\referenced_papers\[23]_2212.14024.pdf (Page 1):

DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
QHow many storeys are in...Q In which city did Akeem Ellis play in 2017?A Ellesmere PortQ When was the discoverer of Palomar 4 born?A 1889TrainDemonstratedefdemonstrate(x:Example) -> Example:x.demos = annotate(x.train, attempt)returnxdefattempt(d:Example):d= search(d)d= predict(d)if d.pred ==d.answer: returnd1QHow many storeys are in the castle...Q When was the discoverer of Palomar 4 born?A 1889Hop1Who discovered Palomar 4?Psg1Edwin Hubble discovered Palomar 4...Hop2When was Edwin Powell born?Psg2Edwin Powell Hubble (1889–1953) was...Pred1889x : ExampleQ In which city did Akeem Ellis play...A Ellesmere Port... ...PredWaterloo❌Demos“How many storeys are in the castle David Gregory inherited?”QHow many storeys are in the...Demos. . .Hop1Which castle did David Gregory inherit?Psg1David Gregory inherited Kinnairdy Castle...Hop2How many storeys are in Kinnairdy Castle?Psg2Kinnairdy Castle […] having five storeys...QHow many storeys does the.... . .. . .PredFive storeysSearchdefsearch(x:Example) -> Example:x.hop1 =generate(hop_template)(x).predx.psg1 =retrieve(x.hop1, k=1)[0]x.hop2 =generate(hop_template)(x).predx.psg2 =retrieve(x.hop2, k=1)[0]returnx2 Predictdefpredict(x:Example) -> Example:x.context = [x.psg1, x.psg2]x.pred=generate(qa_template)(x).predreturnx3“Five storeys”
Figure 2.A toy example of a DSP program for multi-hop question answering. Given an input question and a 2-shot training set, the
DEMONSTRATE stage programmatically annotates intermediate transformations on the training examples using a form of weak supervision.
Learning from a resulting demonstration, the SEARCH stage decomposes the complex input question and retrieves supporting information
over two retrieval hops. Finally, the PREDICT stage uses the demonstration and retrieved passages to answer the question.
retrieve-then-read strategy fails because the RM cannot ﬁnd
passages that directly answer the question.
We introduce the DEMONSTRATE –SEARCH –PREDICT
(DSP ) framework for in-context learning, which relies en-
tirely on passing natural language text (and scores) be-
tween a frozen RM and LM. DSP introduces a num-
ber of composable functions that bootstrap training exam-
ples (DEMONSTRATE ), gather information from a knowl-
edge corpus ( SEARCH ), and generate grounded outputs
(PREDICT ), using them to systematically unify techniques
from the retrieval-augmented NLP and the in-context learn-
ing literatures (Lee et al., 2019; Khattab et al., 2021a; Anan-
tha et al., 2020; Gao et al., 2022; Izacard et al., 2022; Dohan
et al., 2022; Zelikman et al., 2022; Zhang et al., 2022).
We use DSP to suggest powerful strategies for knowledge-
intensive tasks with compositions of these techniques. This
reveals new conceptual possibilities for in-context learning
in general (§2), and it allows us to present rich programs
that set new state-of-the-art results (§3).
Figure 1 shows the path that a DSP program might take to
arrive at an answer, and Figure 2 illustrates how a deliberate
program achieves this. Instead of asking the LM to answer
this complex question, the program’sSEARCH stage uses the
LM to generate a query “Which castle did David Gregory
inherit?” The RM retrieves a passage saying Gregory inher-
ited the Kinnairdy Castle. After a second search “hop” ﬁnds
the castle’s number of storeys, thePREDICT stage queries
the LM with these passages to answer the original question.
Although this program implements behaviors such as query
generation, it requires no hand-labeled examples of these
intermediate transformations (i.e., of the queries and pas-
sages of both retrieval hops). Instead, the DEMONSTRATE
stage uses labeled question–answer pairs to implement a
form of weak supervision that programmatically annotates
the transformations invoked within SEARCH and PREDICT .
We evaluate severalDSP programs on answering questions
in open-domain, multi-hop, and conversational settings. In
them, we implement novel and reusable transformations
such as bootstrapping annotations for all of our pipelines
with weak supervision (§2.3), reliably rewriting questions to
resolve conversational dependencies and iteratively decom-
pose complex queries with summarization of intermediate
hops (§2.4), and generating grounded responses from mul-
tiple passages with self-consistency (§2.5). We report pre-
liminary results on Open-SQuAD, HotPotQA, and QReCC
using the frozen LM GPT-3.5 and RM ColBERTv2 (Khat-
tab & Zaharia, 2020; Santhanam et al., 2022b) with no
ﬁne-tuning. Our DSP programs deliver 37–120%, 8–39%,
and 80–290% relative gains against corresponding vanilla
LMs, a standard retrieve-then-read pipeline, and a contem-
poraneous self-ask pipeline (Press et al., 2022), respectively.
Future versions of this report will include additional test
tasks and LM choices.
In summary, this work makes the following contributions.
First, we argue that simple task-agnostic pipelines for in-
context learning should give way to deliberate, task-aware
strategies. Second, we show that this shift need not be a
burden: with DSP , such strategies can be easily expressed
as short programs using composable operators. Third, this
composability spawns powerful capacities, like automati-
cally annotating demonstrations for complex pipelines from
end-task labels. Fourth, for three knowledge-intensive tasks,
we implement rich programs that establish state-of-the-art
results for in-context learning.



### Claim 136/179

#### Claim Text
Ferritic/martensitic steels are considered promising candidates structure materials of the breeding blanket, reactor pressure vessel (RPV), as well as the fuel cladding in the future fusion and fission reactors .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[173]_2403.10131.pdf (Page 0):

Preprint, Under Review
RAFT: Adapting Language Model to Domain Specific RAG
Tianjun Zhang ∗
Department of Computer Science
UC Berkeley
Berkeley, CA 94720, USA
{tianjunz}@berkeley.edu
Shishir G. Patil, Naman Jain, Sheng Shen
Department of Computer Science
UC Berkeley
Berkeley, CA 94720, USA
{shishirpatil,naman_jain,sheng.s}@berkeley.edu
Matei Zaharia, Ion Stoica, Joseph E. Gonzalez
Department of Computer Science
UC Berkeley
Berkeley, CA 94720, USA
{matei,istoica,jegonzal}@berkeley.edu
Abstract
Pretraining Large Language Models (LLMs) on large corpora of textual
data is now a standard paradigm. When using these LLMs for many
downstream applications, it is common to additionally incorporate new in-
formation into the pretrained model either through RAG-based-prompting,
or finetuning. However, the best methodology to incorporate information
remains an open question. In this paper, we present Retrieval Augmented
Fine Tuning (RAFT), a training recipe which improves the model’s ability
to answer questions in "open-book" in-domain settings. In training RAFT,
given a question, and a set of retrieved documents, we train the model to
ignore those documents that don’t help in answering the question, which
we call, distractor documents. RAFT accomplishes this by citing verbatim
the right sequence from the relevant document to help answer the question.
This coupled with RAFT’s chain-of-thought-style response helps improve
the model’s ability to reason. In domain specific RAG, RAFT consistently
improves the model’s performance across PubMed, HotpotQA, and Gorilla
datasets, presenting a post-training recipe to improve pre-trained LLMs to
in-domain RAG.
1 Introduction
Trained on vast quantities of public data, Large Language Models LLMs have achieved
significant advances in a wide range of general knowledge reasoning tasks Brown et al.
(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized
domains to support tasks ranging from code completion for specific software frameworks
to question answering on specific document collections (e.g., legal or medical documents).
In these settings, general knowledge reasoning is less critical and instead the primary goal
is to maximize accuracy based on a given set of documents. Indeed, adapting LLMs to the
specialized domains (e.g., recent news, enterprise private documents, or program resources
constructed after the training cutoff) is essential to many emerging applications (Vu et al.,
2023; Lazaridou et al., 2022) and is the focus of this work.
This paper studies the following question – How do we adapt pre-trained LLMs for Retrieval
Augmented Generation (RAG) in specialized domains?
When it comes to adapting LLMs to specialized domains, we consider the following two
candidates: in-context learning through Retrieval-Augmented Generation (RAG) and super-
vised fine-tuning. RAG based methods allow the LLM to reference the documents when
∗Corresponding author, personal website: tianjunz.github.io
1
arXiv:2403.10131v2  [cs.CL]  5 Jun 2024



Source: data\tc16_2312.10997v5\referenced_papers\[43]_2308.07922.pdf (Page 18):

Published as a conference paper at COLM 2024
that ATLAS is pretrained solely with masked language modeling, where each masked span
usually contains only a handful of tokens. Besides, while RAVEN’s answers are not always
entirely accurate, they generally exhibit higher quality compared to ATLAS . Furthermore,
the use of Fusion-in-Context Learning in RAVEN appears to contribute to a more coherent
and informative generation.
19



Source: data\tc16_2312.10997v5\referenced_papers\[173]_2403.10131.pdf (Page 3):

Preprint, Under Review
be used at test-time with Retrieval Augmented Generation (RAG) setting, where additional
documents can be introduced in the prompt to help the model answer the question. This
can be represented as follows:
{Train: Q → A}, {0-shot Inference: Q → A}, {RAG Inference: Q + D → A}
RAFT: Retrieval Augmented Fine-Tuning (RAFT), presents a novel recipe to prepare fine-
tuning data to tailor the models for domain-specific open-book setting, equivalent to in-
domain RAG In RAFT, we prepare the training data such that each data point contains a
question (Q), a set of documents (Dk), and a corresponding Chain-of-though style answer
(A∗) generated from one of the document ( D∗). We differentiate between two types of
documents: ‘golden’ documents ( D∗) i.e. the documents from which the answer to the
question can be deduced, and ‘distractor’ documents ( Di) that do not contain answer-
relevant information. As an implementation detail, the ‘golden’ document doesn’t need to
be a single document, but can be more than one document, as is the case in HotpotQA Yang
et al. (2018). Then, for P fraction of the questions (qi) in the dataset, we retain the golden
document (d∗
i ) along with distractor documents (dk−1). For (1 − P) fraction of the questions
(qi) in the dataset, we include no golden document and only include distractor documents
(dk). We then fine-tune the language model using standard supervised training (SFT)
technique, training it to generate answers from the provided documents and question. Fig. 2
illustrates the high-level design principal for RAFT .
We demonstrate that our RAG approach trains the model to perform better RAG on the set
of documents it is trained on i.e., in-domain. By removing the golden documents in some
instances, we are compelling the model to memorize answers instead of deriving them from
the context. The training data for RAFT is as follows, and an example training data can be
seen in Fig. 3:
P % of data: Q + D∗ + D1 + D2 + . . . + Dk → A∗
(1 − P) % of data: Q + D1 + D2 + . . . + Dk → A∗
Subsequently, for the test scenario, the model is provided with the Q and top-k documents
retrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.
A key factor in enhancing training quality is the generation of a reasoning process, such
as Chain-of-Thought, to explain the provided answers. RAFT approach is similar: we
demonstrate that creating a full reasoning chain and in-addition, clearly citing sources
enhances the model’s accuracy in answering questions. In Fig. 3, we illustrate this set-
up. Generating the training data in this fashion, involves presenting the model with a
question, context, and verified answers, and then requesting it to form a reasoning chain
that appropriately references the original context.
For all the datasets in our experiments, we generate the answers using the technique
described above. Note that the Gorilla APIBench dataset, already includes reasoning
in the answers. We provide an example of the generation step in Fig. 3, the detailed
reasoning answer includes a citation from the original context inside ##begin_quote## and
##end_quote## as well as the detailed explanation on how to reach the conclusion based on
the citations. We demonstrate that adding detailed reasoning paragraphs can help boost the
model’s performance in our experiment section.
4 Evaluation
We design our experiments to study how well RAFT performs compared to various base-
lines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading
and extracting information from in-domain documents, than domain-specific finetuned
model, and general-purpose model with RAG. As an ablation, we also demonstrate how
important it is for the model to learn with Chain-of-Thought responses. In this section,
we will first introduce all the datasets we used in the experiments, then all the baseline
model/fine-tuning techniques that we benchmark against.
4



Source: data\tc16_2312.10997v5\referenced_papers\[173]_2403.10131.pdf (Page 2):

Preprint, Under Review
Figure 2: Overview of our RAFT method. The top-left figure depicts our approach of
adapting LLMs to reading solution from a set of positive and distractor documents in
contrast to standard RAG setup where models are trained based on the retriever outputs,
which is a mixture of both memorization and reading. At test time, all methods follow the
standard RAG setting, provided with a top-k retrieved documents in the context.
the exam. For LLMs, this is equivalent to the scenario, for example, in which the LLM is
used as a chatbot. In this scenario the LLM draws from the knowledge baked in during
pre-training and supervised-finetuning to respond to the users’ prompt.
Open Book Exam In contrast, we liken the open-book exam setting to the scenario in
which the LLM can refer to external sources of information (e.g., a website or a book chapter).
In such scenarios, typically, the LLM is paired with retriever which retrieves ‘k’ documents
(or specific segments of the document) which are appended to the users’ prompt. It is
only through these documents retrieved that the LLM gains access to “domain-specific
information”. As a result, we argue that the LLM’s performance in these settings, where it
is trained as a general-purpose LLM is largely dependent on the quality of the retriever and
how accurately the retriever can identify the most relevant piece of information.
Domain-Specific Open-Book Exam In this paper, we focus on the narrower but increas-
ingly popular domain than the general open book exam, which we call the domain-specific
open-book exam. Here, we know apriori the domain in which the LLM will be tested. The
LLM can respond to the users’ prompt using use any and all information from this specific
domain, which it has been fine-tuned on. Examples of domain specific examples include
enterprise documents, code repositories belonging to an organization, etc. In all these
scenarios, the LLM will be used to respond to the questions, whose answers can be found
within a collection of documents. The retrieval technique itself has little to no-impact on the
mechanism (though it may impact the accuracy). This paper studies the domain-specific
open-book setting and how to adapt a pretrained LLM to this specific domain, including
how to make it more robust to a varying number of retrieved documents and distractors.
3 RAFT
In this section, we present RAFT, a novel way of training LLMs for domain-specific open-
book exams. We first introduce the classical technique of supervised fine-tuning, followed
with the key takeaways from our experiments. Then, we introduce RAFT , a modified
version of general instruction tuning. Lastly, we provide an overview of the experiments to
expect in the later sections.
Supervised Finetuning
Consider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The
formulation consists of the Dataset (D) from which a set of Question (Q) and corresponding
answer (A) pairs are derived or already available. In the classical SFT setting, the model is
trained to improve it’s ability to answer the questions based on it’s knowledge - obtained
either during pre-training, or during the SFT training phase. The model so trained can also
3



Source: data\tc16_2312.10997v5\referenced_papers\[173]_2403.10131.pdf (Page 9):

Preprint, Under Review
et al., 2023; Liu et al., 2024). These works focus on constructing a combination of finetuning
dataset for RAG and train a model to perform well on these tasks. In particular, in their
settings, at test time, the domain or documents can be different than the training time;
whereas our paper studies a slightly opposite scenario where we only care about testing the
LLM on the same set of documents.
7 Conclusion
RAFT is a training strategy designed to enhance the model’s performance in answering
questions within a specific domain, in "open-book" settings. We highlight several crucial
design decisions, such as training the model alongside distractor documents, organizing the
dataset so a portion lacks golden documents in their context, and formulating answers in a
chain-of-thought manner with direct quotations from the relevant text. Our evaluations on
PubMed, HotpotQA, and Gorilla API Bench underline RAFT’s significant potential.
References
Anthropic. Prompt engineering for claude’s long context window. 2023.
Asai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H. Self-rag: Learning to retrieve, generate,
and critique through self-reflection. arXiv preprint arXiv:2310.11511, 2023.
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driess-
che, G. B., Lespiau, J.-B., Damoc, B., Clark, A., et al. Improving language models by
retrieving from trillions of tokens. In International conference on machine learning, pp.
2206–2240. PMLR, 2022.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P ., Neelakantan, A.,
Shyam, P ., Sastry, G., Askell, A., et al. Language models are few-shot learners.Advances
in neural information processing systems, 33:1877–1901, 2020.
Carlini, N., Liu, C., Erlingsson, Ú., Kos, J., and Song, D. The secret sharer: Evaluating and
testing unintended memorization in neural networks. In 28th USENIX Security Symposium
(USENIX Security 19), pp. 267–284, 2019.
Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A.,
Brown, T., Song, D., Erlingsson, U., et al. Extracting training data from large language
models. In 30th USENIX Security Symposium (USENIX Security 21), pp. 2633–2650, 2021.
Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C. Quantifying
memorization across neural language models. In The Eleventh International Conference on
Learning Representations, 2022.
Dernoncourt, F. and Lee, J. Y. Pubmed 200k rct: a dataset for sequential sentence classification
in medical abstracts. arXiv preprint arXiv:1710.06071, 2017.
Feldman, V . Does learning require memorization? a short tale about a long tail. InProceedings
of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pp. 954–959, 2020.
Guu, K., Lee, K., Tung, Z., Pasupat, P ., and Chang, M. Retrieval augmented language model
pre-training. In International conference on machine learning, pp. 3929–3938. PMLR, 2020.
Izacard, G., Lewis, P ., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J.,
Joulin, A., Riedel, S., and Grave, E. Atlas: Few-shot learning with retrieval augmented
language models. Journal of Machine Learning Research, 24(251):1–43, 2023. URL http:
//jmlr.org/papers/v24/23-0037.html.
Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W., and Lu, X. Pubmedqa: A dataset for biomedical
research question answering. arXiv preprint arXiv:1909.06146, 2019.
10



### Claim 137/179

#### Claim Text
In practice, the standard weakly-coupled approach is limited to particles with an excess density (Uhlmann reports stable integration forρp/ρf >1.2, while Zhou & Balachandar have shown how this limit can be somewhat lowered by adapting the value of the volume associated with the Lagrangian points).

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 17):

One might also conjecture that this intersection point has a deeper meaning. If we cannot increase the model
size beyond N∗ without qualitatively different data requirements, perhaps this means that once we reach
C∗
min and N∗, we have extracted all of the reliable information available in natural language data. In this
interpretation, L∗ would provide a rough estimate for the entropy-per-token 7 of natural language. In this
scenario, we would expect the loss trend to level off at or before L∗.
We can guess at the functional form of L(Cmin) as it levels off by considering a version of our training
dataset with added noise. For example, we could append a random string of tokens to each context shown
to the model to artiﬁcially boost the loss by a constant additive factor. Then, the distance from the noise
ﬂoor L−Lnoise would be a more meaningful performance metric, with even a small decrease in this distance
potentially representing a signiﬁcant boost in qualitative performance. Since the artiﬁcial noise would affect
all of our trends equally, the critical point of 6.8 would not change (aside from the absolute value ofL∗), and
may be meaningful even if it occurs after the leveling off.
7 Related Work
Power laws can arise from a wide variety of sources [THK18]. Power-law scalings with model and dataset
size in density estimation [Was06] and in random forest models [Bia12] may be connected with our results.
These models suggest that power-law exponents may have a very rough interpretation as the inverse of the
number of relevant features in the data.
Some early [BB01, Goo01] work found power-law scalings between performance and dataset size. More
recent work [HNA+17, HAD19] also investigated scaling between model size and data size; their work is
perhaps the closest to ours in the literature 8. Note, however, that [HNA +17] found super-linear scaling of
dataset size with model size, whereas we ﬁnd a sub-linear scaling. There are some parallels between our
ﬁndings on optimal allocation of compute and [Kom19], including power-law learning curves. EfﬁcientNets
[TL19] also appear to obey an approximate power-law relation between accuracy and model size. Very recent
work [RRBS19b] studies scaling with both dataset size and model size for a variety of datasets, and ﬁts an
ansatz similar to ours.
EfﬁcientNet [TL19] advocates scaling depth and width exponentially (with different coefﬁcients) for optimal
performance of image models, resulting in a power-law scaling of width as a function of depth. We ﬁnd that
for language models this power should be roughly one when scaling up (as width/depth should remain ﬁxed).
But more importantly, we ﬁnd that the precise architectural hyperparameters are unimportant compared to the
overall scale of the language model. In [VWB16] it was argued that deep models can function as ensembles
of shallower models, which could potentially explain this ﬁnding. Earlier work [ZK16] has compared width
and depth, and found that wide ResNets can outperform deep ResNets on image classiﬁcation. Some studies
ﬁx computation per data example, which tends to scale in proportion to the number of model parameters,
whereas we investigate scaling with both model size and the quantity of training computation.
Various works [AS17, BHMM18] have investigated generalization in highly overparameterized models, ﬁnd-
ing a “jamming transition” [GJS+19] when the model size reaches the dataset size (this may require training
many orders of magnitude beyond typical practice, and in particular does not use early stopping). We do
not observe such a transition, and ﬁnd that the necessary training data scales sublinearly in the model size.
Expansions in the model size, particularly at large width [JGH18, LXS+19], may provide a useful framework
for thinking about some of our scaling relations. Our results on optimization, such as the shape of learning
curves, can likely be explained using a noisy quadratic model, which can provide quite accurate predictions
[ZLN+19] in realistic settings. Making this connection quantitative will require a characterization of the
Hessian spectrum [Pap18, GKX19, GARD18].
8 Discussion
We have observed consistent scalings of language model log-likelihood loss with non-embedding parameter
count N, dataset size D, and optimized training computation Cmin, as encapsulated in Equations (1.5) and
(1.6). Conversely, we ﬁnd very weak dependence on many architectural and optimization hyperparameters.
Since scalings with N,D,C min are power-laws, there are diminishing returns with increasing scale.
7Deﬁning words using the wc utility, the WebText2 dataset has1.4 tokens per word and 4.3 characters per token.
8After this work was completed, [RRBS19a] also appeared, which makes similar predictions for the dependence of
loss on both model and dataset size.
18



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 14):

Models between 0.6x and 2.2x the 
optimal size can be trained with a 
20% larger compute budget
Smaller models require 
more steps to train, while 
larger models require fewer
Our framework does not 
capture early training dynamics
Figure 12 Left: Given a ﬁxed compute budget, a particular model size is optimal, though somewhat larger
or smaller models can be trained with minimal additional compute. Right: Models larger than the compute-
efﬁcient size require fewer steps to train, allowing for potentially faster training if sufﬁcient additional paral-
lelism is possible. Note that this equation should not be trusted for very large models, as it is only valid in the
power-law region of the learning curve, after initial transient effects.
10 8
 10 6
 10 4
 10 2
 100
Compute (PF-days), non-embedding
2
3
4
5
6
7Test Loss
L = (Cmin/2.3 108) 0.050
L = (C/2.0 107) 0.057
Figure 13 When adjusting performance to simulate training far below the critical batch size, we ﬁnd a
somewhat altered power law for L(Cmin) when compared with the fully empirical results. The conspicuous
lump at 10−5 PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks
in the power-law ﬁts. It is the L(Cmin) trend that we expect to provide a reliable extrapolation for larger
compute.
that in fact we could train more efﬁciently 6 by training at the batch size Bcrit discussed in Section 5.1.
Large and small values of the loss could have been achieved with fewer samples or fewer steps, respectively,
and correcting for this inefﬁciency by standardizing to the critical batch size results in cleaner and more
predictable trends.
In this section we will adjust for this oversight. More importantly, we will use the results of Section 5
to determine the optimal allocation of compute between model size N and the quantity of data processed
during training, namely 2BcritSmin. We will determine this allocation both empirically and theoretically, by
using the equation for L(N,Smin), and we will demonstrate that these methods agree.
6.1 Optimal Performance and Allocations
Let us ﬁrst study the loss as a function of the optimally allocated compute from Equation (5.5). The result is
plotted in Figure 13, along with a power-law ﬁt. We see that as compared to the compute plot of Figure 1, the
new ﬁt with Cmin is somewhat improved.
Given L(Cmin), it is natural to ask for the optimal model sizeN(Cmin) that provides the minimal loss with a
given quantity of training compute. The optimal model size is shown in Figure 14. We observe thatN(Cmin)
6One might ask why we did not simply train at Bcrit in the ﬁrst place. The reason is that it depends not only on the
model but also on the target value of the loss we wish to achieve, and so is a moving target.
15



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 21):

B.3 Comparison to Inefﬁcient
Typically, researchers train models until they appear to be close to convergence. In this section, we compare
the efﬁcient training procedure described above to this more typical setup. We deﬁne a the convergence factor
f as the percent deviation from the converged loss:
L(N,C) = (1 +f) L(N,∞) . (B.11)
For compute-efﬁcient training we have f = αN/αS ≈ 10% from the previous section, but researchers
typically use a much smaller value. Here, we choose f′= 2%as an estimate. For a ﬁxed value of the loss,
we predict:
Nf
Nf′
=
(1 +f
1 +f′
)1/αN
≈2.7 (B.12)
Sf
Sf′
=
(
1 +1
f
1 + 1
f′
)1/αS
≈0.13 (B.13)
Cf
Cf′
= Nf
Nf′
Sf
Sf′
≈0.35 (B.14)
So that compute-efﬁcient training uses 7.7x fewer parameter updates, 2.7x more parameters, and 65% less
compute to reach the same loss.
B.4 Suboptimal Model Sizes
We can solve A.1 to ﬁnd an expression for the amount of compute needed to reach a given value of the loss
Lwith a model of size N:
C(N,L) =
(
6B∗Sc
N
L1/αB
)(
L−
(Nc
N
)αN)−1/αS
. (B.15)
Using A.6 and A.9, we can eliminate Lin favor of Neﬀ (L), the model size which reaches Lmost efﬁciently.
From there, we ﬁnd an expression for the excess compute needed as a consequence of using a suboptimal
model size:
C(N,Neﬀ)
C(Neﬀ,Neﬀ) = N
Neﬀ
[
1 +αS
αN
(
1 −
(Neﬀ
N
)αN)]−1/αS
. (B.16)
The result is shown in Figure X. Models between 0.6x and 2.2x the optimal size can be used with only a
20% increase in compute budget. Using a smaller model is useful when accounting for the cost inference. A
larger model can be trained the the same level of performance in fewer steps, allowing for more parallelism
and faster training if sufﬁcient harware is available (see Figure Y):
S(N,Neﬀ)
S(Neﬀ,Neﬀ) =
[
1 +αS
αN
(
1 −
(Neﬀ
N
)αN)]−1/αS
. (B.17)
A 2.2x larger model requires 45% fewer steps at a cost of 20% more training compute. Note that this equation
should not be trusted for very large models, as it is only valid in the power-law region of the learning curve
after initial transient effects.
C Caveats
In this section we list some potential caveats to our analysis.
• At present we do not have a solid theoretical understanding for any of our proposed scaling laws.
The scaling relations with model size and compute are especially mysterious. It may be possible to
understand scaling at very large Dholding model size ﬁxed [AS17], and also the shape of learning
curves late in training, by modeling the loss with a noisy quadratic. But the scaling with Dat very
large model size still remains mysterious. Without a theory or a systematic understanding of the
corrections to our scaling laws, it’s difﬁcult to determine in what circumstances they can be trusted.
22



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 7):

Feed-Forward Ratio (dff / dmodel) 
50M Parameters Aspect Ratio (dmodel / nlayer) Attention Head Dimension (dmodel / nhead) 
25M Parameters
10%
8%
6%
4%
2%
0% Loss Increase
A wide range of architectures 
achieve similar performance
22% additional compute
compensates for 1% loss increase
Figure 5 Performance depends very mildly on model shape when the total number of non-embedding
parameters N is held ﬁxed. The loss varies only a few percent over a wide range of shapes. Small differences
in parameter counts are compensated for by using the ﬁt to L(N) as a baseline. Aspect ratio in particular can
vary by a factor of 40 while only slightly impacting performance; an (nlayer,dmodel) = (6,4288) reaches a
loss within 3% of the (48,1600) model used in [RWC+19].
106 107 108 109
Parameters (with embedding)
2
3
4
5
6
7Test Loss
0 Layer
1 Layer
2 Layers
3 Layers
6 Layers
> 6 Layers
103 104 105 106 107 108 109
Parameters (non-embedding)
2
3
4
5
6
7Test Loss
1 Layer
2 Layers
3 Layers
6 Layers
> 6 Layers
Figure 6 Left: When we include embedding parameters, performance appears to depend strongly on the
number of layers in addition to the number of parameters. Right: When we exclude embedding parameters,
the performance of models with different depths converge to a single trend. Only models with fewer than 2
layers or with extreme depth-to-width ratios deviate signiﬁcantly from the trend.
In this section we will display data along with empirically-motivated ﬁts, deferring theoretical analysis to
later sections.
3.1 Approximate Transformer Shape and Hyperparameter Independence
Transformer performance depends very weakly on the shape parametersnlayer,nheads, and dﬀ when we hold
the total non-embedding parameter count N ﬁxed. To establish these results we trained models with ﬁxed
size while varying a single hyperparameter. This was simplest for the case of nheads. When varying nlayer,
we simultaneously varied dmodel while keeping N ≈12nlayerd2
model ﬁxed. Similarly, to vary dﬀ at ﬁxed
model size we also simultaneously varied the dmodel parameter, as required by the parameter counts in Table
1. Independence of nlayers would follow if deeper Transformers effectively behave as ensembles of shallower
models, as has been suggested for ResNets [VWB16]. The results are shown in Figure 5.
3.2 Performance with Non-Embedding Parameter Count N
In Figure 6 we display the performance of a wide variety of models, ranging from small models with shape
(nlayer,dmodel) = (2,128) through billion-parameter models, ranging in shape from (6,4288) through
(207,768). Here we have trained to near convergence on the full WebText2 dataset and observe no over-
ﬁtting (except possibly for the very largest models).
As shown in Figure 1, we ﬁnd a steady trend with non-embedding parameter countN, which can be ﬁt to the
ﬁrst term of Equation (1.5), so that
L(N) ≈
(Nc
N
)αN
(3.1)
8



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 15):

10 7
 10 5
 10 3
 10 1
Compute (PF-days), non-embedding
103
105
107
Parameters (non-embedding)
N = (1.3 109) C0.73
min
N = (1.6 109) C0.88
10 7
 10 5
 10 3
 10 1
Compute (PF-days), excluding embeddings
0
5000
10000
15000Steps
Smin (adjusted)
Smin = (5.4 103) C0.03
min
S (fixed-batch)
Figure 14 Left: Each value of the compute budget Cmin has an associated optimal model size N. Optimal
model size grows very rapidly with Cmin, increasing by 5x for each 10x increase in compute. The number
of data examples processed makes up the remainder of the increase, growing relatively modestly by only 2x.
Right: The batch-adjusted number of optimization steps also grows very slowly, if at all, meaning that most
of the growth in data examples processed can be used for increased batch sizes.
can be ﬁt very well with a power-law
N(Cmin) ∝(Cmin)0.73. (6.1)
In Figure 12, we show the effect of training models of sub-optimal sizes (see Appendix B.4).
By deﬁnition Cmin ≡6NBcritS, and so we can use N(Cmin) to extract further results. In particular, since
prior ﬁts show B ∝L−4.8 and L∝C−0.05
min , we can conclude that Bcrit ∝C0.24
min . This leads us to conclude
that the optimal number of steps will only grow very slowly with compute, as
Smin ∝(Cmin)0.03, (6.2)
matching the empirical results in Figure 14. In fact the measured exponent is sufﬁciently small that our results
may even be consistent with an exponent of zero.
Thus we conclude that as we scale up language modeling with an optimal allocation of computation, we
should predominantly increase the model size N, while simultaneously scaling up the batch size via B ∝
Bcrit with negligible increase in the number of serial steps. Since compute-efﬁcient training uses relatively
few optimization steps, additional work on speeding up early training dynamics may be warranted.
6.2 Predictions from L(N,Smin)
The results for L(Cmin) and the allocations can be predicted from the L(N,Smin) equation obtained in
Section 5. Given our equation for L(N,Smin), we can substitute Smin = Cmin
6NB and then ﬁnd the minimum
of the loss as a function of N, while ﬁxing the training compute. We carry out this procedure in detail in
Appendix B, where we also provide some additional predictions.
For the loss as a function of training compute, we predict that
L(Cmin) =
(Cmin
c
Cmin
)αmin
C
(6.3)
where
αmin
C ≡ 1
1/αS + 1/αB + 1/αN
≈0.054 (6.4)
in excellent agreement with the exponent of Figure 13. We also predict that
N(Cmin) ∝(Cmin)αmin
C /αN ≈(Cmin)0.71 (6.5)
which also matches the scaling of Figure 14 to within a few percent. Our scaling laws provide a predictive
framework for the performance of language modeling.
16



### Claim 138/179

#### Claim Text
In this context, the work we present here results from the Anomalous Diffusion (AnDi) challenge , and in particular from our participation to its second edition .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[35]_2402.13482.pdf (Page 4):

Construction of LLM Input Text
Context: One of the most effective measures for 
preventing the spread of JN.1, a variant of COVID-19 …
In-Context
Target-Context
Context: COVID-19 symptoms can range from …
Input: Common symptoms of COVID-19?
Output: Fever, cough, and shortness of breath
Context: COVID-19 prevention strategies have …
Input: What are strategies to prevent COVID-19?
Output: Wearing masks, maintaining distance …
…
COVID-QA
External Data Store
Input: JN.1 moved swiftly to 
become the most widely 
circulating variant of COVID-19.
Seed Data
Retriever
Figure 2: RADA Framework Overview. We first retrieve the
external instances (relevant to the seed data) from the external
data store, and construct in-context and target-context of LLM
prompts with the retrieved samples along with the seed data.
proaches that primarily focus on sourcing relevant
documents that are likely to contain the answers
to the given query, in the context of our retrieval-
augmented data augmentation scenario, we aim at
fetching the relevant instances from other datasets,
which are used as a source for generating the data
along with the original samples. Therefore, these
retrieved instances should ideally facilitate the gen-
eration of new and enriched samples. In addition,
the instances to be retrieved can vary, which can
be either complete input-output pairs or simply the
inputs or outputs alone, depending on the specific
requirements of data augmentation processes. We
explain how we design retrieval in Section 3.2.2.
3.2.2 Retrieval for Data Augmentation
The input to LLMs can be viewed from two differ-
ent perspectives: in-context learning which refers
to their ability to learn from the input demonstra-
tions; and task-solving where the model executes
specific tasks requested by users (e.g., data augmen-
tation). According to them, we propose two distinct
instantiations of retrieval for LLM-powered data
augmentation below (illustrated in Figure 2).
Retrieval for In-Context Learning In-context
learning plays a crucial role in enabling LLMs to
align their outputs with the contextual cues pro-
vided in the input examples. Similarly, in the con-
text of data augmentation, it may enable LLMs to
learn from examples (e.g., input-output pairs) in the
seed data, to generate new input-output pairs. How-
ever, in low-resource settings that we consider, the
combination of data samples to provide as the ex-
amples in the input prompt is largely limited. This
limitation highlights the advantage of our retrieval-
augmented data augmentation framework, which
can fill the input demonstrations with samples from
external datasets. Yet, as not all the samples are rel-
evant, we retrieve only the relevant samples based
on the similarity between the sample in seed dataD
and the external sample in data store C, as follows:
{ci}k
i=1 = Retriever(q, C) where q ∈ D1. Math-
ematically, the combination of demonstrations to
use as the LLM input is expanded toO((k ×|D|)3)
from O(|D|3), where |D| is typically small in the
low-resource setting and we assume using 3 demon-
strations with top-k sample retrievals.
Retrieval for Target Sample Generation Un-
like in-context examples providing background in-
formation for data augmentation, the context to
be retrieved and used here has a different goal,
which should serve as a source for generating a
complete input-output pair or one among them
when given the other, depending on the specific
use cases. Specifically, for question answering, a
certain document can be used as a context to derive
a query-answer pair along with their in-context ex-
amples. Another example is to provide a question
as a context and then generate its answers, or vice
versa to augment queries. It is worth noting that,
while the usage of instances from the store C is dif-
ferent, their retrieval mechanism is the same as how
we retrieve instances for in-context examples. For-
mally, {ci}k
i=1 = Retriever(q, C) where q can
be, for question answering, either the document or
the question from D. Also, the augmented sam-
ples generated directly from the retrieved instances
are similar in nature to the original samples, as we
consider only the relevant top-k instances for data
augmentation, which can ensure a high degree of
contextual coherence with seed samples, while be-
ing more diverse against the generation with seed.
4 Experimental Setups
In this section, we outline the experimental setups,
including the datasets, models, and implementation
details. We provide more details in Appendix A.
4.1 Tasks and Datasets
We validate our RADA on training data augmenta-
tion and test-time data augmentation scenarios.
Training Data Augmentation The goal of train-
ing data augmentation is to expand the given sam-
ples, which is useful when new events occur that
the model needs to adapt to, while having only lim-
ited data available for training. To test RADA with
this scenario, we use three low-resource domain-
specific datasets: Covid QA (Möller et al., 2020)
1The similarity calculation mechanism can vary, and, in
this work, we consider the similarity between input queries.



Source: data\tc16_2312.10997v5\referenced_papers\[27]_2310.01352.pdf (Page 11):

Published as a conference paper at ICLR 2024
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.
DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In
Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp.
2368–2378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:
10.18653/v1/N19-1246. URL https://aclanthology.org/N19-1246.
Hady Elsahar, Pavlos V ougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique
Laforest, and Elena Simperl. T-REx: A large scale alignment of natural language with knowledge
base triples. In Proceedings of the Eleventh International Conference on Language Resources and
Evaluation (LREC 2018), Miyazaki, Japan, May 2018. European Language Resources Associa-
tion (ELRA). URL https://aclanthology.org/L18-1544.
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:
Long form question answering. In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics, pp. 3558–3567, Florence, Italy, July 2019. Association for Com-
putational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/
P19-1346.
Thibault Formal, Benjamin Piwowarski, and St ´ephane Clinchant. Splade: Sparse lexical and ex-
pansion model for first stage ranking. In Proceedings of the 44th International ACM SIGIR
Conference on Research and Development in Information Retrieval . Association for Comput-
ing Machinery, 2021. ISBN 9781450380379. doi: 10.1145/3404835.3463098. URL https:
//doi.org/10.1145/3404835.3463098.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle
use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions
of the Association for Computational Linguistics , 9:346–361, 2021. doi: 10.1162/tacl a 00370.
URL https://aclanthology.org/2021.tacl-1.21.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval aug-
mented language model pre-training. In Proceedings of the 37th International Conference on
Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of Proceedings of
Machine Learning Research, pp. 3929–3938. PMLR, 2020. URL http://proceedings.
mlr.press/v119/guu20a.html.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob
Steinhardt. Measuring massive multitask language understanding. Proceedings of the Interna-
tional Conference on Learning Representations (ICLR), 2021a.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,
Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with
the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings
of the Neural Information Processing Systems Track on Datasets and Benchmarks
1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual , 2021b. URL
https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/
hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html.
Karl Moritz Hermann, Tom ´as Kocisk ´y, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa
Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Corinna
Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (eds.),
Advances in Neural Information Processing Systems 28: Annual Conference on Neural In-
formation Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada , pp.
1693–1701, 2015. URL https://proceedings.neurips.cc/paper/2015/hash/
afdec7005cc9f14302cd0474fd0f3c96-Abstract.html.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F ¨urstenau, Manfred Pinkal, Marc
Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. Robust disambiguation of named
entities in text. In Proceedings of the 2011 Conference on Empirical Methods in Natural Lan-
guage Processing, pp. 782–792, Edinburgh, Scotland, UK., July 2011. Association for Computa-
tional Linguistics. URL https://aclanthology.org/D11-1072.
12



Source: data\tc16_2312.10997v5\referenced_papers\[35]_2402.13482.pdf (Page 11):

Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Nat-
ural Language Processing, EMNLP-IJCNLP 2019,
Hong Kong, China, November 3-7, 2019, pages 3980–
3990. Association for Computational Linguistics.
Stephen E. Robertson, Steve Walker, Susan Jones,
Micheline Hancock-Beaulieu, and Mike Gatford.
1994. Okapi at TREC-3. In Proceedings of The Third
Text REtrieval Conference, TREC 1994, Gaithers-
burg, Maryland, USA, November 2-4, 1994, volume
500-225 of NIST Special Publication , pages 109–
126. National Institute of Standards and Technology
(NIST).
Jon Saad-Falcon, Omar Khattab, Keshav Santhanam,
Radu Florian, Martin Franz, Salim Roukos, Avirup
Sil, Md. Arafat Sultan, and Christopher Potts. 2023.
UDAPDR: unsupervised domain adaptation via LLM
prompting and distillation of rerankers. In Proceed-
ings of the 2023 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2023, Sin-
gapore, December 6-10, 2023, pages 11265–11279.
Association for Computational Linguistics.
Gözde Gül Sahin and Mark Steedman. 2018. Data
augmentation via dependency tree morphing for low-
resource languages. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, Brussels, Belgium, October 31 - Novem-
ber 4, 2018, pages 5004–5009. Association for Com-
putational Linguistics.
Vinay Samuel, Houda Aynaou, Arijit Ghosh Chowd-
hury, Karthik Venkat Ramanan, and Aman Chadha.
2023. Can llms augment low-resource reading com-
prehension datasets? opportunities and challenges.
arXiv preprint arXiv:2309.12426, abs/2309.12426.
Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-
hishek Srivastava, and Iryna Gurevych. 2021. BEIR:
A heterogenous benchmark for zero-shot evalua-
tion of information retrieval models. arXiv preprint
arXiv:2104.08663, abs/2104.08663.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurélien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models. arXiv preprint arXiv:2307.09288.
Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-SNE. Journal of Machine
Learning Research, 9:2579–2605.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa
Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh
Hajishirzi. 2023a. Self-instruct: Aligning language
models with self-generated instructions. In Proceed-
ings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), ACL 2023, Toronto, Canada, July 9-14, 2023,
pages 13484–13508. Association for Computational
Linguistics.
Yue Wang, Xinrui Wang, Juntao Li, Jinxiong Chang,
Qishen Zhang, Zhongyi Liu, Guannan Zhang, and
Min Zhang. 2023b. Harnessing the power of david
against goliath: Exploring instruction data generation
without using closed-source models. arXiv preprint
arXiv:2308.12711, abs/2308.12711.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,
and Denny Zhou. 2022. Chain-of-thought prompting
elicits reasoning in large language models. In Ad-
vances in Neural Information Processing Systems 35:
Annual Conference on Neural Information Process-
ing Systems 2022, NeurIPS 2022, New Orleans, LA,
USA, November 28 - December 9, 2022.
Jason W. Wei and Kai Zou. 2019. EDA: easy data
augmentation techniques for boosting performance
on text classification tasks. In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing,
EMNLP-IJCNLP 2019, Hong Kong, China, Novem-
ber 3-7, 2019 , pages 6381–6387. Association for
Computational Linguistics.
Chenxi Whitehouse, Monojit Choudhury, and Al-
ham Fikri Aji. 2023. Llm-powered data augmen-
tation for enhanced crosslingual performance. arXiv
preprint arXiv:2305.14288.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
Joe Davison, Sam Shleifer, Patrick von Platen, Clara
Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le
Scao, Sylvain Gugger, Mariama Drame, Quentin
Lhoest, and Alexander M. Rush. 2020. Transformers:
State-of-the-art natural language processing. In Pro-
ceedings of the 2020 Conference on Empirical Meth-
ods in Natural Language Processing: System Demon-
strations, EMNLP 2020 - Demos, Online, November
16-20, 2020, pages 38–45. Association for Computa-
tional Linguistics.



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 29):

[SLA+18] Christopher J. Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and
George E. Dahl. Measuring the effects of data parallelism on neural network training, 2018,
arXiv:1811.03600. 12
[SS18] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory
cost. CoRR, abs/1804.04235, 2018, 1804.04235. URLhttp://arxiv.org/abs/1804.04235.
7
[THK18] Stefan Thurner, Rudolf Hanel, and Peter Klimek. Introduction to the theory of complex systems.
Oxford University Press, 2018. 18
[TL19] Mingxing Tan and Quoc V . Le. Efﬁcientnet: Rethinking model scaling for convolutional neural
networks. CoRR, abs/1905.11946, 2019, 1905.11946. URL http://arxiv.org/abs/1905.
11946. 18
[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural
Information Processing Systems 30 , pages 5998–6008. Curran Associates, Inc., 2017. URL
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf . 2, 6
[VWB16] Andreas Veit, Michael Wilber, and Serge Belongie. Residual networks behave like ensembles
of relatively shallow networks, 2016, arXiv:1605.06431. 8, 18
[Was06] Larry Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006.
18
[WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose
language understanding systems, 2019, 1905.00537. 2
[WRH17] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Growing a brain: Fine-tuning by in-
creasing model capacity. 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), Jul 2017. doi:10.1109/cvpr.2017.323. 19
[WYL19] Wei Wen, Feng Yan, and Hai Li. Autogrow: Automatic layer growing in deep convolutional
networks, 2019, 1906.02909. 19
[YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V .
Le. Xlnet: Generalized autoregressive pretraining for language understanding, 2019,
arXiv:1906.08237. 2
[ZK16] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. Procedings of the British
Machine Vision Conference 2016, 2016. doi:10.5244/c.30.87. 18
[ZKZ+15] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-
ralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books. 2015 IEEE International Conference on Computer Vision
(ICCV), Dec 2015. doi:10.1109/iccv.2015.11. 7
[ZLN+19] Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl,
Christopher J. Shallue, and Roger B. Grosse. Which algorithmic choices matter at which batch
sizes? insights from a noisy quadratic model. CoRR, abs/1907.04164, 2019, 1907.04164. URL
http://arxiv.org/abs/1907.04164. 12, 18
30



Source: data\tc16_2312.10997v5\referenced_papers\[35]_2402.13482.pdf (Page 7):

1x 3x 5x 10x 30x 100x55
60
65
70F1
Covid QA
Self-Instruct
Augment w/ Seed Data
RADA (Ours)
1x 3x 5x 10x 30x 100x
Augmentation Size Relative to Seed Data
10
15
20
25
30
Policy QA
1x 3x 5x 10x 30x 100x10
20
30
40
Tech QA
Figure 6: Results of varying the augmentation size on domain-specific QA, where
we increase the size by factors of 1, 3, 5, 10, 30, and 100 relative to the seed data size.
Methods Tech QA
RADA (Ours) 44.37
w/o In-context Retriever 41.24
w/o Target-context Retriever 34.42
w/o All Retrievers 30.38
Table 4: Ablation study of the proposed
RADA on the Tech QA dataset.
models in Figure 4 and report their lexical overlaps
in Figure 5. Specifically, for the visualization, we
first embed the generated instances with Sentence-
BERT (Reimers and Gurevych, 2019a) into the
latent space and project them with t-SNE (van der
Maaten and Hinton, 2008). From this, we observe
that, unlike Augment w/ Seed Data whose gener-
ated samples are close to the seed data, the samples
generated from RADA are broadly dispersed across
the space. Further, we measure the max ROUGE-L
scores between the seed instances and the gener-
ated instances where lower scores indicate higher
diversity. As shown in Figure 5, RADA generates
distinct samples to the seed data thanks to retriev-
ing and utilizing the external contexts beyond the
seed data, unlike baselines that rely solely on it.
Analysis of Augmented Data Size To see how
the performance changes as a function of the size
of augmented data samples, we vary the augmenta-
tion size relative to the seed data size by a factor of
1, 3, 5, and up to 100 times and report the results in
Figure 63. Firstly, when the amount of augmented
data is very small, baseline performances are com-
parable with RADA since the data samples that can
be generated from the seed data alone can have a
certain diversity level as we augment only a small
amount. However, as the size of augmentation
expands, RADA consistently outperforms base-
lines, showcasing its ability to generate broader
and richer samples through retrieval augmentation,
while the performance starts to converge after a
100-time increase in data augmentation.
Ablation Study To see how each component of
RADA affects the overall performance, we conduct
an ablation study where we replace our in-context
and target-context retrieval modules with random
retrievals. As shown in Table 4, we observe that,
without retrieving relevant instances, the perfor-
mances drop substantially since irrelevant samples
(to the target tasks/datasets) are used to construct
the in-context examples and target context, leading
3Due to the cost of running Self-Instruct, we are not able
to generate its samples for the 100 times augmentation-level.
Table 5: Results of another LLM (ChatGPT) for data aug-
mentation on domain-specific QA with seed examples of 10.
Covid Policy Tech Average
Self-Instruct 57.86 26.20 33.42 39.16
CQA Generation 65.64 27.20 34.16 42.33
RADA (Ours) 67.19 28.59 36.17 43.98
to generating the samples not useful for them. Fur-
thermore, the target-context retriever is particularly
important for data augmentation, since this context
is used to directly derive the instances for training.
Analysis of Using Different LLMs Finally, we
conduct an auxiliary analysis to see whether the
superiority of RADA is consistent across different
LLMs, compared to existing baselines. In partic-
ular, we use ChatGPT 3.5 (released on June 13,
2023) as the basis model for data augmentation,
and report the results in Table 5. From this, we
observe that RADA significantly outperforms base-
lines with another LLM, demonstrating its robust-
ness across different LLMs for data augmentation.
6 Conclusion
In this work, we pointed out the limitation of exist-
ing data augmentation approaches that use the seed
data alone for low-resource domain tasks, leading
to generating suboptimal and less diverse instances,
despite the existence of plenty of external samples
available. Inspired by this, we proposed the LLM-
powered Retrieval-Augmented Data Augmentation
(RADA) framework, which augments the seed data
by leveraging the samples retrieved from the exter-
nal data store based on their relevance with the seed
data, during data augmentation. Specifically, the in-
put to LLMs for data augmentation can be viewed
from two different angles of in-context examples
and task-solving context, and we constructed them
through samples from within and across the seed
data and the retrieved data. Through extensive eval-
uation results on multiple datasets with training and
test-time data augmentation scenarios, we showed
that RADA outperforms strong LLM-powered data
augmentation baselines substantially. In addition,
our findings reveal that the data samples generated
from our approach are much more diverse against



### Claim 139/179

#### Claim Text
In the context of DLM /FD methods, the implementation in is gradient discontinuity capturing and can be viewed as an implicit ghost node method with additional volume forcing.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[23]_2212.14024.pdf (Page 3):

DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
invoke and compose DSP primitives (i.e., built-in functions)
to build the DEMONSTRATE , SEARCH , and PREDICT trans-
formations that deﬁne the program.
Transformations A transformation is a function that
takes an Example as input and returns an Example, pop-
ulating new ﬁelds (or modifying existing ﬁelds) in it. This
program invokes three developer-deﬁned transformations,
namely, multihop_demonstrate, multihop_search, and
multihop_predict. Transformations may themselves in-
voke other transformations, and they act analogously to
layers in standard deep neural network (DNN) program-
ming frameworks such as PyTorch, except that they pass
text data instead of tensors between each other and do not
involve backpropagation.
We categorize transformations according to their behavior
(or purpose) under one of the DEMONSTRATE , SEARCH ,
and PREDICT stages. That said, DSP does not impose this
categorization and allows us to deﬁne functions that may
blend these stages. We will discuss each of the three stages
next.
2.3. DEMONSTRATE
It is known that including examples of the desired behavior
from the LM in its prompt typically leads to better perfor-
mance (Brown et al., 2020). In DSP , a demonstration is a
training example that has been prepared to illustrate speciﬁc
desired behaviors from the LM. A DEMONSTRATE transfor-
mation takes as input x of type Example and prepares a list
of demonstrations in x.demos, typically by selecting a sub-
set of the training examples in x.train and bootstrapping
new ﬁelds in them.
Bootstrapping Demonstrations Examples in the train-
ing set typically consist of the input text and the target
output of the task. The DEMONSTRATE stage can aug-
ment a training example by programmatically bootstrapping
annotations for intermediate transformations. In our run-
ning “multi-hop” example, the demonstrations illustrate
three LM-based transformations: (i) how to break down the
input question in order to gather information for answer-
ing it (i.e., ﬁrst-hop retrieval), (ii) how to use information
gathered in an earlier “hop” to ask follow-up questions, and
(iii) how to use the information gathered to answer complex
questions.
1 Examples = list [ Example ]
2 Transformation = Callable [[ Example ],
3 Optional [ Example ]]
4
5 annotate ( train : Examples , fn: Transformation )
6 -> Examples
Akin to a specialized map, the annotate primitive accepts
a user-deﬁned transformation fn and applies it over a list
of training examples. Whenever fn returns an example
(rather than None), annotate caches the intermediate pre-
dictions (i.e., the generated queries and retrieved passages).
These predictions serve as successful demonstrations for the
pipeline transformations. In simple uses, fn may attempt
to answer the example “zero-shot” one or more times. This
is typically done by invoking the SEARCH and PREDICT
stages of the program. When an answer is produced, if
fn assesses it as correct, it returns a populated example in
which the intermediate predictions are present.
Case Study The snippet below deﬁnes the func-
tion multihop_demonstrate, called in Line 3 of
multihop_program, and illustrates the usage of annotate.
1 from dsp import sample , annotate
2
3 def attempt_example (d: Example ):
4 d = d. copy ( demos =[])
5 d = multihop_search (d)
6 d = multihop_predict (d)
7 return d if d. pred == d. answer else None
8
9 def multihop_demonstrate (x: Example ):
10 demos = annotate (x. train , attempt_example )
11 return Example (x, demos = demos )
In Line 10, multihop_demonstrate invokes annotate,
which bootstraps missing ﬁelds in training examples by
caching annotations from attempt_example. The transfor-
mation attempt_example takes a training example d and
attempts to answer it in a zero-shot fashion: it creates a copy
of d with no demonstrations (Line 4; i.e., zero-shot) and
invokes the multi-hop search and predict pipeline (Lines 5
and 6). Each transformation returns an updated version of
d with additional ﬁelds populated. If the pipeline answers
correctly (Line 7), the updated d is returned.
Figure 2 illustrates this behavior. DEMONSTRATE trans-
forms a training question–answer pair to a fully-populated
demonstration, including ﬁelds such as hop1 and hop2 (i.e.,
queries for multi-hop search) as well as psg1 and psg2.
When the LM is later invoked to conduct a transformation,
say, generating a “second-hop” query duringSEARCH , the
psg1 ﬁeld serves as context and the hop2 ﬁeld serves as a
label for this particular training example.
Discussion This simple case study illustrates the power of
composition in the DSP abstraction. Because the pipeline
is a well-deﬁned program in which transformations com-
municate by passing text attached to Examples, a simple
map-and-ﬁlter strategy can leverage the LM and RM to
bootstrap annotations for a full pipeline from end-task la-
bels. This is an extensible strategy, but even in its simplest
form it generalizes the approaches explored recently by Ze-
likman et al. (2022), Wei et al. (2022), Zhang et al. (2022),
and Huang et al. (2022) in which an LM self-generates
chain-of-thought rationales for an individual prompt.



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 2):

Retrieve Anything To Augment Large Language Models Conference’17, July 2017, Washington, DC, USA
the model’s information seeking capability in the conversational
context. 3) Tool Learning. The ToolLLM dataset [ 62] is used to
learn the selection of appropriate tools in the tool-using context.
4) Instruction Tuning: To retrieve useful demonstration examples
for in-context learning, we re-purpose FLAN [86] and UPRISE [18],
which are originally designed for instruction tuning. 5) Genera-
tion. The model is trained to extract valuable historical information
(i.e. memory) based on a long conversation dataset: Multi-Session
Chat [93], as well as long-range language modeling datasets: in-
cluding Books3 [25], ArXiv [25], CodeParrot [79]. These datasets
can be grouped into two types based on the availability of labels.
•Labeled data. The datasets on the first three types of tasks are
composed of pairwise texts, where hard-coded labels are presented.
For question answering datasets (MSMARCO, NQ), each data in-
stance consists of a query and the source passage of answer, denoted
as <query, passage>. For conversational search dataset (QReCC),
each data instance is made up of a conversational query and the
source passage of answer, denoted as <conversation, passage>. For
tool learning dataset (ToolLLM), each data instance includes an
instruction and the description of the needed tool, denoted as <in-
struction, tool desc>.
•Non-labeled data. In contrast, the last two types of datasets
do not have explicit labels. For instruction tuning datasets (FLAN,
UPRISE), each instance consists of human’s instruction and the ex-
pected output: <instruction, output>. For generation datasets, each
instance is a long text sequence partitioned into chunks: [chunk_0,
..., chunk_L]. Books3, ArXiv, and CodeParrot are made up of plain
texts, which are chunked into spans of equal length (128 tokens per
chunk). Multi-Session Chat is composed of conversations, where
each chunk corresponds to a pair of consecutive utterances.
2.2 Training Methodology
2.2.1 Formulation of Training Reward. In our work, we explore
two types of supervision signals for training the LLM-Embedder.
Firstly, we can directly utilize the hard labels provided by the labeled
datasets. Secondly, we aim to optimize the LLM’s final performance
with retrieval augmentation. To achieve this goal, we leverage the
reward produced by LLM for both labeled and unlabeled datasets.
Particularly, given the expected output of the LLM, denoted as𝑂,
and a retrieval candidate, denoted as𝐶, the reward for the candidate,
represented as 𝑟𝐶|𝑂, is derived by the following equation:
𝑟𝐶|𝑂 =
Ö|𝑂|
𝑖=1 LLM(𝑜𝑖|𝐶,𝑂:𝑖−1). (1)
Here, 𝑜𝑖 represents the 𝑖-th token of the expected output, and
LLM(𝑥|𝑦)stands for the LLM’s generation likelihood of producing
𝑥 given the context 𝑦. In other words, a higher reward is assigned
to a retrieval candidate if it results in a higher generation likelihood
for the expected output.
The LLM based reward is applied in the following ways for
each of the tasks in consideration. 1) For Question Answering:
the reward is computed as the generation likelihood of answers
given one single candidate passage. 2) For Instruction Tuning: The
reward is computed as the generation likelihood of the instructed
output given one candidate example. 3) For Generation: the reward
is computed as the generation likelihood of a new content given
one candidate historical chunk. Note that the LLM reward is not
applied to conversational search and tool learning datasets, as there
is no clear expectation of the LLM’s output in these cases.
Given the two sources of supervision signals of LLM-Embedder,
i.e. the native hard labels and the soft reward derived from LLM,
the training is conducted with a composite recipe. The contrastive
learning is applied to capture the semantic relationship reflected
by the hard labels; meanwhile, the knowledge distillation is used
to learn from the soft rewards derived from LLM.
2.2.2 Contrastive Learning. For each pair of hard-labeled texts: 𝑞
and 𝑝 (e.g., query and passage), the loss function of contrastive
learning is formulated in the following way:
min .
∑︁
(𝑞,𝑝)
−log exp(⟨𝒆𝑞,𝒆𝑝⟩/𝜏)Í
𝑝′∈Pexp(⟨𝒆𝑞,𝒆𝑝′⟩/𝜏), (2)
where 𝒆∗stands for the embedding, ⟨·⟩indicates the inner product
operator, Pare the union of positive and negative samples,𝜏 refers
to the temperature. To improve the discriminative power of embed-
dings across diverse application scenarios, we employ a couple of
key designs in our contrastive learning framework.
The first featured design is theInstruction-based Fine-Tuning.
In this approach, each task is assigned with a unique task instruction
denoted as 𝐼𝑡. While generating the query-side embedding, the task
instruction and query content are concatenated and jointly encoded,
resulting in the update of query embedding: 𝒆𝑞 ←encode([𝐼𝑡,𝑞]).
This task-specific instructions plays a pivotal role in initializing the
embedding model with distinct activations, thereby facilitating the
discrimination between different tasks.
The second notable feature is theHomogeneous In-Batch Neg-
ative Sampling. It calls for a considerable amount of negative sam-
ples to guarantee the embedding’s discriminativeness [30, 63, 82].
In our work, this is realized by the joint usage of in-batch negatives
and hard negatives. We also apply cross-device sharing [ 63, 91],
which further expands the scale of negative samples. Consequently,
our method results in 𝐵 ×𝐾 ×𝑁 −1 negative samples in total,
where 𝐵is the batch size, 𝐾 is the number of GPU devices, 𝑁 is the
total number of positive and hard negative samples. However, the
vanilla practice of in-batch negative sampling presents one draw-
back in our multi-task settings. Particularly, the embeddings shared
between different datasets (namely heterogenous negative samples)
are mostly irrelevant, which are less effective for discriminating the
semantic relationships within a specific task scenario. To address
this limitation, we introduce a regularization strategy for the organi-
zation of training data, where the data instances from the same task
are grouped into consecutive mini-batches. The strategy makes the
majority of in-batch negative samples to originate from the same
dataset (i.e. homogeneous negative samples), thus enhancing the
discriminative power of embeddings for each specific task.
2.2.3 Knowledge Distillation. In our training framework, knowl-
edge distillation plays a crucial role in learning from the LLM’s
reward. we employ the KL-divergence to minimize the gap between
the distributions of candidates computed using LLM’s rewards and
those predicted by the embedding model. In particular, for each
query 𝑞and its candidate list P: [𝑝1, ..., 𝑝𝑁], we derive the LLM’s
rewards towards the candidates, denoted as 𝑅: [𝑟1, ..., 𝑟𝑁], using



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 3):

Conference’17, July 2017, Washington, DC, USA Zhang and Xiao, et al.
Eq 1. To make the LLM’s rewards suitable for distillation, we trans-
form each reward into a normalized weight:𝑤𝑖 ←softmax𝑅(𝑟𝑖/𝛼),
where 𝛼 represents the temperature. On top of these elements, the
KL divergence is computed by the following equation:
min .
∑︁
P
−𝑤𝑖 ∗log exp(⟨𝒆𝑞,𝒆𝑝⟩/𝜏)Í
𝑝′∈Pexp(⟨𝒆𝑞,𝒆𝑝′⟩/𝜏). (3)
While the above formulation has been successfully employed in
mono-task settings [29, 46, 81], applying it directly to our multi-task
scenario poses unique challenges. Notably, the magnitude of LLM’s
rewards can exhibit high fluctuations due to the diverse training
samples from various tasks. In many cases, the LLM’s rewards
closely distribute, making it challenging to distinguish the quality
of candidates. In contrast, in many other cases, the rewards become
polarized, with candidates receiving either a positive reward or
nearly zero rewards. Both of these scenarios contribute little to the
distillation process and can severely impair the training effect.
•Stabilized Distillation . To address the challenge of fluctu-
ated rewards in our multi-task scenario, we introduce a modified
formulation of the loss function. This adaptation effectively alle-
viates the negative impact resulted from the rewards’ fluctuations.
Particularly, instead of using LLM rewards solely as “soft weights”,
we also leverage them as hard ranking labels. Given LLMs’ rewards
𝑅: [𝑟1, ..., 𝑟𝑁], we re-rank the candidates in a top-down order. This
operation results in a new order for the candidates, denoted as
P: [𝑝1, ..., 𝑝𝑁], where 𝑟𝑖 ≥𝑟𝑖+1. The loss function for knowledge
distillation is accordingly transformed as follows:
min .
∑︁
𝑃 −𝑤𝑖 ∗log exp(⟨e𝑞,e𝑝𝑖 ⟩/𝜏)Í
𝑝′∈P exp(⟨e𝑞,e𝑝′⟩/𝜏).
Here, P comprises two sources: the lower-ranked candidates of 𝑝𝑖:
[𝑝𝑖+1, ..., 𝑝𝑁]; and the the in-batch negative samples.
Our adapted formulation serves to stabilize fluctuated rewards
in two fundamental ways. On one hand, the model is consistently
trained to promote 𝑝𝑖 compared to its lower-ranked counterparts
[𝑝𝑖+1, ...,𝑝𝑁]. This means that the model is always able to learn from
the LLMs’ preferences, regardless of the absolute value of rewards.
This mechanism is particularly effective in handling cases where
LLMs’ rewards are too closely distributed. On the other hand, when
the top-ranked candidate receives a significantly higher reward
compared to the other candidates, the weights will become one-hot.
In this scenario, the distillation process will be reduced to the form
of contrastive learning, with the top-ranked candidate treated as
a positive sample. This mechanism help to address the situations
where polarized rewards are generated by LLMs.
2.3 Retrieval Augmentation of LLMs
The multi-tasking capacity of LLM-Embedder makes it as a versatile
solution. By connecting to the vector DB where any needed external
elements are stored, it may support a wide variety of retrieval
augmentation tasks. In this place, we discuss the typical scenarios
empowered by LLM-Embedder (Figure 2), with focusing on three
key issues: 1) what to store in the vector DB, 2) what is used to
query the vector DB, 3) how to leverage the retrieved data.
•Knowledge Enhancement. When handling knowledge in-
tensive tasks [37, 59], the entire docs from the knowledge corpus
can be encoded and stored in vector DB. In many cases, questions
Long Memory
LLM-Embedder
DocsChunksExamplesTools
Vector DB
Knowledge enhanced In-Context LearningTool Augmented
LLM
Figure 2: Retrieval augmentation with LLM-Embedder.
are explicitly presented, which can be used to query the vector DB.
In other cases, the working context during the generation process
can be used as query [ 27, 34]. The retrieved docs can be directly
applied or refined for more informative segments [44]. Finally, the
query and retrieved docs are concatenated to generate knowledge-
grounded answer, e.g., [knowledge, query] →answer.
•Long-Context Modeling. When dealing with a long context,
the entire history can be chunked, encoded, and off-loaded to the
vector database. The working context during the generation process
can be used to query the vector DB for relevant chunks. In many
cases, both the relevant chunk, e.g., chunk_𝑖, and its subsequent
chunk_𝑖+1 are used for memory augmentation [15], because the
subsequent chunk can be more critical to the future generation. The
retrieved chunks are used to back-fill the current context, where
new content can be generated with remote but important memory,
e.g., [retrieved chunks, current context] →new generation.
•In-context Learning. The demonstration examples, organized
in the form of “(task instruction, expected output)”, can be encoded
and pre-stocked in vector DB. When a new task is given, the task’s
instruction is used to query the vector DB [18, 83]. The retrieved
examples are concatenated with the task’s instruction, based on
which the in-context learning can be conducted, e.g., [retrieved
examples, instruction] →task completion.
•Tool Learning. The tool’s functionality can be verbalized as
a description, and paired with its API: “(description, API)”. In this
way, a massive toolkit can be managed by vector DB based on the
encoded description [62]. Given a user request that involves the
use of tools, the user request can be encoded and used to query
the vector DB. The retrieved tool is executed via its API, where the
execution result is returned for LLM to complete the remaining
generation: [user request, tool’s execution result] →generation.
3 EXPERIMENT
The experimental study is to clarify three basic research questions.
RQ 1. can LLM-Embedder comprehensively support the diverse
scenarios of LLM’s retrieval augmentation. RQ 2. what is LLM-
Embedder’s impact to each specific scenario.RQ 3. what are the key
factors influencing the empirical performance of LLM-Embedder.



Source: data\tc16_2312.10997v5\referenced_papers\[172]_2309.17453.pdf (Page 3):

Published as a conference paper at ICLR 2024
Dense Attention Window Attention Sliding Window 
w/ Re-computation StreamingLLM
Dense Attention Window Attention StreamingLLM
Figure 3: Language modeling perplexity on texts with 20K tokens across various LLM. Observations reveal
consistent trends: (1) Dense attention fails once the input length surpasses the pre-training attention window
size. (2) Window attention collapses once the input length exceeds the cache size, i.e., the initial tokens are
evicted. (3) StreamingLLM demonstrates stable performance, with its perplexity nearly matching that of the
sliding window with re-computation baseline.
Improving LLMs’ Utilization of Long Textoptimizes LLMs to better capture and employ the
content within the context rather than merely taking them as inputs. As highlighted by Liu et al.
and Li et al., success in the previously mentioned two directions does not necessarily translate to
competent utilization of lengthy contexts. Addressing this effective usage of prolonged contexts
within LLMs is still a challenge. Our work concentrates on stably harnessing the most recent tokens,
enabling the seamless streaming application of LLMs.
3 S TREAMING LLM
3.1 T HE FAILURE OF WINDOW ATTENTION AND ATTENTION SINKS
While the window attention technique offers efficiency during inference, it results in an exceedingly
high language modeling perplexity. Consequently, the model’s performance is unsuitable for deploy-
ment in streaming applications. In this section, we use the concept of attention sink to explain the
failure of window attention, serving as the inspiration behind StreamingLLM.
Identifying the Point of Perplexity Surge.Figure 3 shows the perplexity of language modeling on
a 20K token text. It is evident that perplexity spikes when the text length surpasses the cache size, led
by the exclusion of initial tokens. This suggests that the initial tokens, regardless of their distance
from the predicted tokens, are crucial for maintaining the stability of LLMs.
Why do LLMs break when removinginitial tokens’ KV? We visualize attention maps from
all layers and heads of the Llama-2-7B and models in Figure 2. We find that, beyond the bottom
two layers, the model consistently focuses on the initial tokens across all layers and heads. The
implication is clear: removing these initial tokens’ KV will remove a considerable portion of the
denominator in the SoftMax function (Equation 1) in attention computation. This alteration leads to a
significant shift in the distribution of attention scores away from what would be expected in normal
inference settings.
SoftMax(x)i = exi
ex1 + PN
j=2 exj
, x 1 ≫ xj, j∈ 2, . . . , N (1)
There are two possible explanations for the importance of the initial tokens in language modeling:
(1) Either their semantics are crucial, or (2) the model learns a bias towards their absolute position.
To distinguish between these possibilities, we conduct experiments (Table 1), wherein the first four
tokens are substituted with the linebreak token “\n". The observations indicate that the model still
significantly emphasizes these initial linebreak tokens. Furthermore, reintroducing them restores
the language modeling perplexity to levels comparable to having the original initial tokens. This
suggests that the absolute position of the starting tokens, rather than their semantic value, holds
greater significance.
LLMs attend to Initial Tokens as Attention Sinks.To explain why the model disproportionately
focuses on initial tokens—regardless of their semantic relevance to language modeling, we introduce
the concept of “attention sink". The nature of the SoftMax function (Equation 1) prevents all attended
tokens from having zero values. This requires aggregating some information from other tokens across
all heads in all layers, even if the current embedding has sufficient self-contained information for its
prediction. Consequently, the model tends to dump unnecessary attention values to specific tokens. A
similar observation has been made in the realm of quantization outliers (Xiao et al., 2023; Bondarenko
et al., 2023), leading to the proposal of SoftMax-Off-by-One (Miller, 2023) as a potential remedy.
4



### Claim 140/179

#### Claim Text
The start values of the parameters were taken from GEISA for 15N2O and from HITRAN2020 for the remaining isotopocules.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[33]_2305.17653.pdf (Page 12):

Runs SST-2 SST-5 CoLA TREC CR MR MPQA Subj Average
T5-base (220M)
Run 1 94.4 57.0 57.7 81.0 92.5 90.8 90.3 97.0 82.6
Run 2 93.5 56.5 57.6 80.8 90.6 91.3 90.1 97.0 82.2
Run 3 93.9 57.1 57.5 80.6 91.9 91.2 90.4 97.0 82.5
Average 93.9 56.9 57.6 80.8 91.7 91.1 90.3 97.0 82.4
Std 0.45 0.32 0.10 0.20 0.97 0.26 0.15 0.0 0.21
T5-large (770M)
Run 1 96.0 59.4 64.0 81.2 92.8 93.0 90.8 97.6 84.4
Run 2 95.4 60.1 60.1 81.0 92.0 92.3 90.5 97.3 83.6
Run 3 95.8 60.0 59.1 80.6 92.9 91.9 90.5 97.6 83.6
Average 95.7 59.8 61.1 80.9 92.6 92.4 90.6 97.5 83.8
Std 0.31 0.38 2.59 0.31 0.49 0.56 0.17 0.17 0.45
Table 8: Multiple run results of PGRA.
G Datasets and Metrics
We use the Wiki1M from SimCSE (Gao et al.,
2021b) as our external datastore. This dataset is a
subset of Wikipedia and used in (Gao et al., 2021b).
We report information on tasks in Table 12. We
use the same configuration as (Gao et al., 2021a),
including dataset splits.
H Training Details
As stated in Section 3.1, we search hyper-
parameters of learning rate of {1e-5, 2e-5, 5e-5,
8e-5, 1e-4} and batch sizes of {4, 8}. We train our
models for 5000 steps on the training set. The best
hyperparamters found are shown in Table 7.
I Prompts
We include all prompts used in all 8 tasks in Ta-
ble 13.



Source: data\tc16_2312.10997v5\referenced_papers\[3]_2310.20158.pdf (Page 17):

In our experiments, we notice that the number of output tokens is negligible compared to the number
of input tokens. Specifically, when making calls to GPT -3.5-Turbo, approximately 1000 tokens
are utilized for input, while calls to GPT -4use around 2000 tokens for input. This results in an
approximate cost of $1.02 per API query, with specific cost details provided in Table A1.
Table A1: The cost of API calls per query.
Model Number of API calls Cost ($)
GPT -3.5-Turbo 320 0.48
GPT -4 9 0.54
C H YPERPARAMETER TUNING
In our final re-ranking step with GPT -4, we conduct hyperparameter tuning for the window size
(w) and step size ( s), exploring two specific configurations: w,s = 20,10 and w,s = 10,5. Our
results indicate that the w,s = 10,5 configuration outperforms the w,s = 20,10 setup, with a higher
nDCG@10 score of 0.7390 compared to 0.7376.
D A DDITIONAL RESULTS
Extended results on SPARTA(Zhao et al., 2021), docT5query (Cheriton, 2019), GenQ (Thakur
et al., 2021), ColBERT (Khattab & Zaharia, 2020), BM25+CE (Wang et al., 2020), and TART -
Rerank (FLAN-T5-XL)(Asai et al., 2022) can be found in Table A2 and Table A3.
Table A2: Retrieval performance (nDCG@10) on BEIR datasets.
Method TREC-COVID NFCorpus Signal-1M (RT) TREC-NEWS Robust04 Touch´e-2020 DBPedia SciFact
BM25 59.5 30.8 33.0 39.5 40.7 44.2 31.8 67.9SPARTA 53.8 30.1 25.2 25.8 27.6 17.5 31.4 58.2docT5query 71.3 32.8 30.7 42.0 43.7 34.7 33.1 67.7DPR 33.2 18.9 15.5 16.1 25.2 13.1 26.3 31.8ANCE 65.4 23.7 24.9 38.2 39.2 24.0 28.1 50.7TAS-B 48.1 31.9 28.9 37.7 42.7 16.2 38.4 64.3GenQ 61.9 31.9 28.1 39.6 36.2 18.2 32.8 64.4ColBERT 67.7 30.5 27.4 39.3 39.1 20.2 39.2 67.1BM25+CE 75.7 35.0 33.8 43.1 27.1 40.9 68.8monoBERT(340M) 70.0 36.9 31.4 44.6 49.3 31.8 41.9 71.4monoT5(220M) 78.34 37.4 31.7 46.8 51.7 30.8 42.4 73.4monoT5(3B) 80.7 39.0 32.6 48.5 56.7 32.4 44.4 76.6TART -Rerank (FLAN-T5-XL)75.1 36.0 25.8 40.0 50.8 27.5 42.5 74.8UPR 68.1 35.0 31.9 43.1 42.4 19.7 30.9 72.7Promptagator++(zero-shot) 76.0 36.0 - - - 27.8 41.3 73.6Promptagator++(few-shot) 76.2 37.0 - - - 38.1 43.4 73.1RankGPT 85.5 38.5 34.4 52.9 57.6 38.6 47.1 75.0RRR(this work) 86.4 39.9 29.8 53.6 67.4 29.8 51.0 77.2
Table A3: Retrieval performance ( Recall@100) on BEIR datasets. For TREC-COVID, capped
Recall@100 is used.
Method TREC-COVID NFCorpus Signal-1M (RT) TREC-NEWS Robust04 Touch´e-2020 DBPedia SciFact
BM25 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5SPARTA 40.9 24.3 27.0 26.2 21.5 38.1 41.1 86.3docT5query 54.1 25.3 35.1 43.9 35.7 55.7 36.5 91.4DPR 21.2 20.8 16.2 21.5 21.1 30.1 34.9 72.7ANCE 45.7 23.2 23.9 39.8 27.4 45.8 31.9 81.6TAS-B 38.7 28.0 30.4 41.8 33.1 43.1 49.9 89.1GenQ 45.6 28.0 28.1 41.2 29.8 45.1 43.1 89.3ColBERT 46.4 25.4 28.3 36.7 31.0 43.9 46.1 87.8BM25+CE 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5monoBERT(340M) 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5monoT5(220M) 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5monoT5(3B) 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5RankGPT 49.8 24.6 37.0 44.7 37.5 58.2 46.8 92.5RRR(this work) 54.8 32.4 32.4 51.6 45.4 52.2 55.0 94.3
18



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 15):

0 50 100 150
1.6
1.8
2
2.2
2.4
2.6
2.8
3 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(a) ELI5 - p = 10%
0 50 100 150
1.4
1.45
1.5
1.55
1.6
1.65 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (b) ELI5 - p = 30%
0 50 100 150
1.04
1.06
1.08
1.1
1.12
1.14 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (c) ELI5 - p = 50%
0 10 20 30 40 50
1.6
1.8
2
2.2
2.4
2.6
2.8
3
3.2
3.4 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(d) MS MARCO - p = 10%
0 10 20 30 40 50
1.15
1.2
1.25
1.3
1.35
1.4 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
0.91 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (f) MS MARCO - p = 50%
0 20 40 60 80 100
1.5
2
2.5
3
3.5
1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(g) NQ - p = 10%
0 20 40 60 80 100
1.5
1.6
1.7
1.8
1.9
2 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
Loa ding [ M a thJ a x ] / ex tensions/ M a thM enu .j s (h) NQ - p = 30%
0 20 40 60 80 100
1.22
1.24
1.26
1.28
1.3
1.32
1.34
1.36 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (i) NQ - p = 50%
Figure 7: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-12), for FiD-Base models.
Each row represents a different dataset, and every column represents a different filtering percentage (10,30,50).



Source: data\tc16_2312.10997v5\referenced_papers\[17]_2305.02437.pdf (Page 18):

Table 11: Confidence region for SOTA model in XSum and BigPatent.
System ROUGE-1/2/L 95%-conf.int
XSum
BRIOjoint
50.3 0.49986 - 0.50602
26.7 0.26300 - 0.26989
41.6 0.41231 - 0.41900
BigPatent
BARTjoint
62.9 0.62664 - 0.63080
48.1 0.47783 - 0.48333
59.6 0.59401 - 0.59847
Table 12: Generation Latency analysis.
NMT XSum BigPatent DailyDialog
Average Input Length 87 512 1024 71
Average Output Length 44 75 127 16
CPU
Retrieval-augmented Baseline 0.97 1.79 3.16 0.32
Selfmem
Candidate Generation 3.20 7.50 15.00 1.02
Memory Selection 0.50 0.52 0.95 0.14
Hypothesis Generation 0.97 1.79 3.00 0.32
×4.80 ×5.47 ×6.04 ×4.63
CUDA
Retrieval-augmented Baseline 0.29 0.44 0.75 0.10
Selfmem
Candidate Generation 0.51 1.00 1.72 0.18
Memory Selection 0.01 0.01 0.01 0.01
Hypothesis Generation 0.29 0.44 0.75 0.10
×2.76 ×2.99 ×3.35 ×2.91
19



Source: data\tc16_2312.10997v5\referenced_papers\[42]_2208.03299.pdf (Page 32):

Number of parameters 220M 770M 3B 11B
NaturalQuestions 64-shot 27.0 35.4 41.3 45.1
NaturalQuestions full 54.1 60.8 63.4 64.0
TriviaQA 64-shot 55.3 65.0 70.2 71.4
TriviaQA full 71.8 74.9 77.5 78.0
Table 19: Impact of model size on question answering datasets.We report exact match performance
on the test sets of NaturalQuestions and TriviaQA ﬁltered depending on the number of parameters in the
reader module. For these experiments the index contains the December 2018 Wikipedia dump.
Model AIDA FEV T-REx zsRE NQ HoPo TQA WoW
acc acc acc acc em em em f1
Atlas 64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4
Atlas full dataset 92.7 94.4 84.8 80.9 63.4 51.4 84.4 21.0
Table 20:Downstream results on KILT dev sets.
question: {query text} answer: [MASK_0]
and train the model to generate the mask token followed by the expected output:
[MASK_0] {output}.
We retrieve 20 passages and generate answer using greedy decoding. In KILT, FEVER is a two-way
classiﬁcationtaskofclaims. We lexicalize the“SUPPORTS”(resp. ‘REFUTES”)labelinto“true” (respectively
“false”).
For few-shot ﬁne-tuning we trainAtlas for 30 steps using 64 random samples from the train sets. The
retriever is trained using query-side ﬁne-tuning. We evaluate models every 5 steps and select the best one on
the development set based on the reported metric. We use AdamW with a batch size of 32 and a learning
rate of4 ×10−5 with linear decay and 5 iterations of warmup for both the language model and the retriever.
For the ﬁne-tuning on the full datasets, the model is trained for 5k gradient steps. We evaluate models every
500 steps and select the best one on the development set based on the reported metric. The index is refreshed
every 500 step for the ﬁrst 1000 iterations, and every 2k steps afterwards. We use AdamW with a batch size
of 64 and a learning rate of4 ×10−5 with linear decay and 500 iterations of warmup for both the language
model and the retriever.
We report results on the development sets in Table 20.
33



### Claim 141/179

#### Claim Text
Lipid rafts and membrane heterogeneity: Interactions of particles with lipid rafts in cell membranes have been shown to lead to complex diffusive behaviors ;.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[105]_2310.01558.pdf (Page 2):

Published as a conference paper at ICLR 2024
0
20
40
60
80
NQ
(Single-hop)
Bamboogle
(Explicit Multi-hop)
2WikiMQA
(Explicit Multi-hop)
StrategyQA
(Implicit Multi-hop)
Fermi
(Implicit Multi-hop)
No Retrieval RALM (Top-1 Retrieval) RALM (Random Retrieval)
Figure 2: Accuracy for Llama-2-13B few-shot prompted on five QA tasks, in three settings: (a)
without retrieval, (b) with top-1 retrieval from a strong search engine, and (c) with a randomly-
retrieved passage. Retrieval augmentation can boost performance, but even strong retrieval hurts
performance on StrategyQA and Fermi, and random contexts reduce performance dramatically.
2 M AKING RALM S ROBUST TO IRRELEVANT CONTEXTS
We now present our methods for building RALMs that are robust to irrelevant contexts. We begin
by describing the common approach for incorporating evidence into RALMs. Next, we explore
a natural baseline for using an NLI model to identify irrelevant contexts. Last, we describe our
procedure for finetuning models to be robust to irrelevant context.
In-context RALMs Language models define a probability distribution over sequences of to-
kens, with auto-regressive models assigning a probability via next-token prediction: pLM =
Πn
i=1pθ(xi|x<i), where x<i is the sequence of tokens preceding xi at each step and θ denotes
the parameters of the LM. For RALMs, we follow the definition of in-context RALMs from Ram
et al. (2023), where context sentences are retrieved from a corpus C, and generation is con-
ditioned on the retrieved context. Given the retrieval operation RC, this can be formalized as
pRALM = Π n
i=1pθ(xi|RC(x<i); x<i), where [RC(x<i); x<i] denotes the concatenation of the re-
trieved evidence with the generated sequence. Generation in LMs and RALMs can also be condi-
tioned on additional input, which we omit for brevity.
In our setting, we focus on RALMs for ODQA. We follow recent approaches such as Self-Ask and
IR-CoT (Press et al., 2023; Trivedi et al., 2023; Yoran et al., 2023), for interleaving retrieval with
multi-hop question answering (see Fig. 3). Retrieval is performed for every intermediate question
and each context is prepended to the question. In the single-hop setting, the model has to generate
the answer given a question and retrieved context. In the multi-hop setting, the model has to generate
intermediate questions and answers until arriving at the final answer and the retriever is called for
the original question and after each intermediate question. Formally, x in this case is the generated
decomposition until an intermediate step andRC(x) are the retrieved contexts for all questions inx.
2.1 I DENTIFYING IRRELEVANT CONTEXTS WITH NLI MODELS .
NLI models (Dagan et al., 2006; Bowman et al., 2015) classify whether a textual hypothesis is
entailed, neutral, or contradicted given a textualpremise. Recent work successfully used NLI models
to automatically identify hallucinations (Honovich et al., 2022) and statement attribution (Bohnet
et al., 2023) when presented with a context and generated text. Similarly, a natural baseline is to
frame irrelevant context identification as an NLI problem, by using the retrieved context only when
the hypothesis (i.e., final answer and intermediate question-answer pairs; Fig. 3) are classified as
entailed by the premise (i.e., the retrieved context). We use a simple back-off strategy where we
generate twice, once with pLM and once with pRALM , and only use the RALM if the NLI model
classified all generated answers (and intermediate questions) as entailed by the retrieved evidence.
3



Source: data\tc16_2312.10997v5\referenced_papers\[48]_2310.01558.pdf (Page 2):

Published as a conference paper at ICLR 2024
0
20
40
60
80
NQ
(Single-hop)
Bamboogle
(Explicit Multi-hop)
2WikiMQA
(Explicit Multi-hop)
StrategyQA
(Implicit Multi-hop)
Fermi
(Implicit Multi-hop)
No Retrieval RALM (Top-1 Retrieval) RALM (Random Retrieval)
Figure 2: Accuracy for Llama-2-13B few-shot prompted on five QA tasks, in three settings: (a)
without retrieval, (b) with top-1 retrieval from a strong search engine, and (c) with a randomly-
retrieved passage. Retrieval augmentation can boost performance, but even strong retrieval hurts
performance on StrategyQA and Fermi, and random contexts reduce performance dramatically.
2 M AKING RALM S ROBUST TO IRRELEVANT CONTEXTS
We now present our methods for building RALMs that are robust to irrelevant contexts. We begin
by describing the common approach for incorporating evidence into RALMs. Next, we explore
a natural baseline for using an NLI model to identify irrelevant contexts. Last, we describe our
procedure for finetuning models to be robust to irrelevant context.
In-context RALMs Language models define a probability distribution over sequences of to-
kens, with auto-regressive models assigning a probability via next-token prediction: pLM =
Πn
i=1pθ(xi|x<i), where x<i is the sequence of tokens preceding xi at each step and θ denotes
the parameters of the LM. For RALMs, we follow the definition of in-context RALMs from Ram
et al. (2023), where context sentences are retrieved from a corpus C, and generation is con-
ditioned on the retrieved context. Given the retrieval operation RC, this can be formalized as
pRALM = Π n
i=1pθ(xi|RC(x<i); x<i), where [RC(x<i); x<i] denotes the concatenation of the re-
trieved evidence with the generated sequence. Generation in LMs and RALMs can also be condi-
tioned on additional input, which we omit for brevity.
In our setting, we focus on RALMs for ODQA. We follow recent approaches such as Self-Ask and
IR-CoT (Press et al., 2023; Trivedi et al., 2023; Yoran et al., 2023), for interleaving retrieval with
multi-hop question answering (see Fig. 3). Retrieval is performed for every intermediate question
and each context is prepended to the question. In the single-hop setting, the model has to generate
the answer given a question and retrieved context. In the multi-hop setting, the model has to generate
intermediate questions and answers until arriving at the final answer and the retriever is called for
the original question and after each intermediate question. Formally, x in this case is the generated
decomposition until an intermediate step andRC(x) are the retrieved contexts for all questions inx.
2.1 I DENTIFYING IRRELEVANT CONTEXTS WITH NLI MODELS .
NLI models (Dagan et al., 2006; Bowman et al., 2015) classify whether a textual hypothesis is
entailed, neutral, or contradicted given a textualpremise. Recent work successfully used NLI models
to automatically identify hallucinations (Honovich et al., 2022) and statement attribution (Bohnet
et al., 2023) when presented with a context and generated text. Similarly, a natural baseline is to
frame irrelevant context identification as an NLI problem, by using the retrieved context only when
the hypothesis (i.e., final answer and intermediate question-answer pairs; Fig. 3) are classified as
entailed by the premise (i.e., the retrieved context). We use a simple back-off strategy where we
generate twice, once with pLM and once with pRALM , and only use the RALM if the NLI model
classified all generated answers (and intermediate questions) as entailed by the retrieved evidence.
3



Source: data\tc16_2312.10997v5\referenced_papers\[64]_2302.00083.pdf (Page 6):

WikiText-103
Perplexity
10.0
20.0
30.0
40.0
OPT-125M OPT-350M OPT-1.3B OPT-2.7B OPT-6.7B OPT-13B OPT-30B OPT-66B
No Retrieval In-Context RALM (BM25)
RealNews
Perplexity
3.0
8.0
13.0
18.0
OPT-125M OPT-350M OPT-1.3B OPT-2.7B OPT-6.7B OPT-13B OPT-30B OPT-66B
No Retrieval In-Context RALM (BM25)
Figure 4: Results of OPT models (Zhang et al., 2022) on the test set of WikiText-103 (word-level
perplexity) and the development set of RealNews (token-level perplexity). In-Context RALM models use
a BM25 retriever with s = 4(i.e., the retriever is called every four tokens) and ℓ = 32(i.e., the retriever
query is comprised of the last 32 tokens of the prefix). In-Context RALM with an off-the-shelf retriever
improved the performance of a 6.7B parameter OPT model to match that of a 66B parameter OPT model.
appealing since applying a BM25 retriever is sig-
nificantly cheaper than the neural alternatives.
5.2 Frequent Retrieval Improves Language
Modeling
We investigated the effect of varying the retrieval
stride s (i.e., the number of tokens between consec-
utive retrieval operations). Figure 5 shows that LM
performance improved as the retrieval operation
became more frequent. This supports the intuition
that retrieved documents become more relevant the
closer the retrieval query becomes to the gener-
ated tokens. Of course, each retrieval operation
imposes a runtime cost. To balance performance
and runtime, we used s = 4 in our experiments.
For comparison, RETRO employed a retrieval fre-
quency of s = 64(Borgeaud et al., 2022), which
leads to large degradation in perplexity. Intuitively,
retrieving with high frequency (low retrieval stride)
allows to ground the LM in higher resolution.
5.3 A Contextualization vs. Recency Tradeoff
in Query Length
We also investigated the effect of varying ℓ, the
length of the retrieval query for BM25. Figure 6
reveals an interesting tradeoff and a sweet spot
around a query length of 32 tokens. Similar ex-
periments for dense retrievers are given in App. A.
We conjecture that when the retriever query is too
short, it does not include enough of the input con-
text, decreasing the retrieved document’s relevance.
Conversely, excessively growing the retriever query
deemphasizes the tokens at the very end of the pre-
fix, diluting the query’s relevance to the LM task.
6 Improving In-Context RALM with
LM-Oriented Reranking
Since In-Context RALM uses a fixed document
reading component by definition, it is natural to
ask whether performance can be improved by spe-
cializing its document retrieval mechanism to the
LM task. Indeed, there is considerable scope for
improvement: the previous section considered con-
ditioning the model only on the first document re-



Source: data\tc16_2312.10997v5\referenced_papers\[101]_2310.06839.pdf (Page 3):

1 5 10 15 20
Number of Retained Documents
0
20
40
60
80
100Recall(%)
LLMLingua
BM25
OpenAI
Voyageai
BGE-large-en v1.5
SBERT
Gzip
Cohere-Rerank
BGE-llmembeder
Jina
LongLLMLingua rk 
w/o restrict
BGE-Ranker-large
LongLLMLingua rk
(a) Recall Distribution
1 5 10 15 20
Document Position in the Prompt
0.0
0.2
0.4
0.6
0.8
1.0Document Avg. Perplexity
Perplexity
Contrastive Perplexity (b) Perplexity Distribution (5th)
Figure 3: (a) Comparison of recall on NaturalQuestions Multi-documemnt QA dataset, which increases from top to
bottom in terms of Recall@1. Different colors represent different types of methods. Among them, yellow represents
traditional relevance methods, green signifies embedding-based methods, and red denotes rerank-based methods.
(b) Comparison between perplexities and contrastive perplexities of tokens in the prompt from Multi-documemnt
QA dataset. The document containing the ground-truth information is located in the 5th position. More results on
position can be found in the Appendix C.1.
iterative compression mechanism following LLM-
Lingua and directly calculate token perplexities
to compress xins and xque. In this section, we in-
vestigate how to make the fine-grained token-level
compression over {xdoc
k }K′
k=1 aware of the question
xque, so that the compressed results could contain
more question-relevant key information.
A straightforward solution for the awareness of
xque is to simply concatenate it at the beginning
of the whole context. However, this will result in
low perplexities of relevant tokens in the context
following the condition of question xque, further
reducing their differentiation from other tokens.
In this paper, we propose contrastive perplexity,
i.e., the distribution shift caused by the condition of
the question, to represent the association between
the token and the question. The contrastive perplex-
ity based importance metric si for each token xi in
{xdoc
k }K′
k=1 can be formulated as:
si = perplexity(xi|x<i)−perplexity(xi|xque, x<i).
(3)
Additionally, we provide the derivation of its
mathematical significance in the Appendix A, con-
cluding that it is equivalent to conditional pointwise
mutual information (Church and Hanks, 1989).
Figure 3b illustrates the difference between per-
plexities and contrastive perplexities. The distri-
bution of perplexities appears random, making it
challenging to extract information related to the
question. However, tokens with high contrastive
perplexities tend to cluster near the ground-truth
document, which contains information relevant to
the question. This suggests that the proposed con-
trastive perplexity can better distinguish tokens
relevant to the question, thus improving the key
information density in the compressed results.
4.2 How to reduce information loss in the
middle?
As demonstrated in Figure 1b, LLM achieves the
highest performance when relevant information oc-
curs at the beginning and significantly degrades if
relevant information is located in the middle of long
contexts. After the coarse-grained compression, we
have obtained a set of documents {xdoc
k }K′
k=1 with
their corresponding importance scores {rk}K′
k=1 in-
dicating their association with the question xque.
Therefore, we reorder documents using their impor-
tance scores to better leverage LLMs’ information
perception difference in positions:
(xins, xdoc
1 , ··· , xdoc
K′ ,xque)
rk
−→
(xins,xdoc
r1 , ··· , xdoc
rK′ , xque)
(4)
4.3 How to achieve adaptive granular control
during compression?
In fine-grained compression, LLMLingua applies
the same compression ratio over all documents ob-
tained from budget controller. However, the key
information density of different documents is differ-
ent. The more relevant to the question a document



Source: data\tc16_2312.10997v5\referenced_papers\[173]_2403.10131.pdf (Page 6):

Preprint, Under Review
Table 2: Ablation on Chain-of-Thought : The numbers of RAFT and RAFT without
CoT. Results on various datasets show that adding CoT can significantly improve the
performance of the finetuned model. With a gains of 9.66% and 14.93% in the Hotpot QA
and HuggingFace datasets respectively.
PubMed HotpotQA HuggingFace Torch Hub TensorFlow
RAFT w.o CoT 68.30 25.62 59.07 86.56 83.21
RAFT 73.30 35.28 74.00 84.95 86.86
demonstrates a scenario where the DSF model becomes confused by a question asking for
the identity of a screenwriter. Instead of providing the correct name, it mistakenly cites one
of the films written by the screenwriter. In contrast, the RAFT model accurately answers the
question. This discrepancy suggests that training a model solely with question-answer pairs
may impair its ability to derive relevant context from provided documents. The comparison
underscores the importance of incorporating both standard instructional tuning and context
comprehension into the training dataset to preserve and enhance the model’s ability to
process text effectively.
HotPot QA
Question: What screenwriter with credits for “Evolution”[0/1879]e a film starring
Nicolas Cage and Téa Leoni?
Documents: . . . David Weissman is a screenwriter and director. His film credits
include “The Family Man” (2000), “Evolution” (2001), and “When in Rome” (2010).
. . .
The Family Man is a 2000 American romantic comedy-drama film directed by Brett
Ratner, written by David Diamond and David Weissman, and starringNicolas Cage
and Téa Leoni. . . .
—————————————————————————————————————
—————————–
RAFT: ##Reason: The screenwriter with credits for the film “Evolution,” starring
Nicolas Cage and Téa Leoni, is David Weissman. This information is provided in the
reference documents where it mentions David Weissman as a screenwriter with film
credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome”
(2010). Therefore, the screenwriter for “Evolution” is David Weissman. ##Answer:
David Weissman ✓
DSF: The Family Man ✗
Figure 4: Comparison of RAFT and DSF : On the HotPot QA dataset, we can see that DSF
model extracts the wrong information from the context when the question is asking who is
the screen writer and it answers a film name. RAFT manages to get the accurate results .
4.4 Should we train the LLM always with the golden context for RAG?
In our exploration of whether large language models (LLMs) should always be trained with
the golden context for Retrieval-Augmented Generation (RAG), we address a key question:
what proportion (p%) of the training data should include golden documents? Intuitively,
one might assume that for effective training in reading and extracting information from
context (e.g., RAG tasks), the golden document should always be included during training
(P = 100%). However, our findings challenge this assumption: incorporating a portion of
the training data without the golden document in the context (P = 80%) appears to enhance
the model’s performance on RAG tasks.
7



### Claim 142/179

#### Claim Text
Hou et al. using object kinetic Monte Carlo simulations, demonstrated that cluster size distributions in cascade debris and the spatial extent of vacancy and SIA clusters in displacement cascades play major role in the evolution of cluster size distributions after long enough time (at 0.1 dpa).

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 19):

Appendices
A Summary of Power Laws
For easier reference, we provide a summary below of the key trends described throughout the paper.
Parameters Data Compute Batch Size Equation
N ∞ ∞ Fixed L(N) = (Nc/N)αN
∞ D Early Stop Fixed L(D) = (Dc/D)αD
Optimal ∞ C Fixed L(C) = (Cc/C)αC
(naive)
Nopt Dopt Cmin B ≪Bcrit L(Cmin) =
(
Cmin
c /Cmin
)αmin
C
N D Early Stop Fixed L(N,D) =
[(Nc
N
)αN
αD + Dc
D
]αD
N ∞ Ssteps B L(N,S) =
(Nc
N
)αN
+
(
Sc
Smin(S,B)
)αS
Table 4
The empirical ﬁtted values for these trends are:
Power Law Scale (tokenization-dependent)
αN = 0.076 Nc = 8.8 ×1013 params (non-embed)
αD = 0.095 Dc = 5.4 ×1013 tokens
αC = 0.057 Cc = 1.6 ×107 PF-days
αmin
C = 0.050 Cmin
c = 3.1 ×108 PF-days
αB = 0.21 B∗= 2.1 ×108 tokens
αS = 0.76 Sc = 2.1 ×103 steps
Table 5
The optimal parameters for compute efﬁcient training are given by:
Compute-Efﬁcient Value Power Law Scale
Nopt = Ne ·CpN
min pN = 0.73 Ne = 1.3 ·109 params
B ≪Bcrit = B∗
L1/αB = BeCpB
min pB = 0.24 Be = 2.0 ·106 tokens
Smin = Se ·CpS
min (lower bound) pS = 0.03 Se = 5.4 ·103 steps
Dopt = De ·CpD
min (1 epoch) pD = 0.27 De = 2·1010 tokens
Table 6
B Empirical Model of Compute-Efﬁcient Frontier
Throughout this appendix all values of C,S, and αC are adjusted for training at the critical batch size Bcrit.
We have left off the ‘adj’ label to avoid cluttering the notation.
B.1 Deﬁning Equations
The power-law ﬁt to the learning curves implies a simple prescription for compute-efﬁcient training. In this
appendix, we will derive the optimal performance, model size, and number of training steps as a function of
20



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 14):

Models between 0.6x and 2.2x the 
optimal size can be trained with a 
20% larger compute budget
Smaller models require 
more steps to train, while 
larger models require fewer
Our framework does not 
capture early training dynamics
Figure 12 Left: Given a ﬁxed compute budget, a particular model size is optimal, though somewhat larger
or smaller models can be trained with minimal additional compute. Right: Models larger than the compute-
efﬁcient size require fewer steps to train, allowing for potentially faster training if sufﬁcient additional paral-
lelism is possible. Note that this equation should not be trusted for very large models, as it is only valid in the
power-law region of the learning curve, after initial transient effects.
10 8
 10 6
 10 4
 10 2
 100
Compute (PF-days), non-embedding
2
3
4
5
6
7Test Loss
L = (Cmin/2.3 108) 0.050
L = (C/2.0 107) 0.057
Figure 13 When adjusting performance to simulate training far below the critical batch size, we ﬁnd a
somewhat altered power law for L(Cmin) when compared with the fully empirical results. The conspicuous
lump at 10−5 PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks
in the power-law ﬁts. It is the L(Cmin) trend that we expect to provide a reliable extrapolation for larger
compute.
that in fact we could train more efﬁciently 6 by training at the batch size Bcrit discussed in Section 5.1.
Large and small values of the loss could have been achieved with fewer samples or fewer steps, respectively,
and correcting for this inefﬁciency by standardizing to the critical batch size results in cleaner and more
predictable trends.
In this section we will adjust for this oversight. More importantly, we will use the results of Section 5
to determine the optimal allocation of compute between model size N and the quantity of data processed
during training, namely 2BcritSmin. We will determine this allocation both empirically and theoretically, by
using the equation for L(N,Smin), and we will demonstrate that these methods agree.
6.1 Optimal Performance and Allocations
Let us ﬁrst study the loss as a function of the optimally allocated compute from Equation (5.5). The result is
plotted in Figure 13, along with a power-law ﬁt. We see that as compared to the compute plot of Figure 1, the
new ﬁt with Cmin is somewhat improved.
Given L(Cmin), it is natural to ask for the optimal model sizeN(Cmin) that provides the minimal loss with a
given quantity of training compute. The optimal model size is shown in Figure 14. We observe thatN(Cmin)
6One might ask why we did not simply train at Bcrit in the ﬁrst place. The reason is that it depends not only on the
model but also on the target value of the loss we wish to achieve, and so is a moving target.
15



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 4):

107 108 109 1010
Tokens in Dataset
2.5
3.0
3.5
4.0
4.5Loss
Loss vs Model and Dataset Size
Params
708M
302M
85M
3M
25M
393.2K
104 105
Estimated Smin
2.4
2.8
3.2
3.6
4.0
4.4Loss
Loss vs Model Size and Training Steps
106
107
108
Parameters (non-embed)
Figure 4 Left : The early-stopped test loss L(N,D) varies predictably with the dataset size Dand model
size N according to Equation (1.5). Right: After an initial transient period, learning curves for all model
sizes N can be ﬁt with Equation (1.6), which is parameterized in terms of Smin, the number of steps when
training at large batch size (details in Section 5.1).
These relations hold across eight orders of magnitude in Cmin, six orders of magnitude in N, and over two
orders of magnitude in D. They depend very weakly on model shape and other Transformer hyperparameters
(depth, width, number of self-attention heads), with speciﬁc numerical values associated with the Webtext2
training set [RWC +19]. The power laws αN,αD,αmin
C specify the degree of performance improvement
expected as we scale up N, D, or Cmin; for example, doubling the number of parameters yields a loss that
is smaller by a factor 2−αN = 0.95. The precise numerical values of Nc,Cmin
c , and Dc depend on the
vocabulary size and tokenization and hence do not have a fundamental meaning.
The critical batch size, which determines the speed/efﬁciency tradeoff for data parallelism ([MKAT18]), also
roughly obeys a power law in L:
Bcrit (L) = B∗
L1/αB
, B ∗∼2 ·108 tokens, αB ∼0.21 (1.4)
Equation (1.1) and (1.2) together suggest that as we increase the model size, we should increase the dataset
size sublinearly according to D ∝N
αN
αD ∼N0.74. In fact, we ﬁnd that there is a single equation combining
(1.1) and (1.2) that governs the simultaneous dependence on N and Dand governs the degree of overﬁtting:
L(N,D) =
[(Nc
N
)αN
αD
+ Dc
D
]αD
(1.5)
with ﬁts pictured on the left in ﬁgure 4. We conjecture that this functional form may also parameterize the
trained log-likelihood for other generative modeling tasks.
When training a given model for a ﬁnite number of parameter update steps S in the inﬁnite data limit, after
an initial transient period, the learning curves can be accurately ﬁt by (see the right of ﬁgure 4)
L(N,S) =
(Nc
N
)αN
+
( Sc
Smin(S)
)αS
(1.6)
where Sc ≈2.1 ×103 and αS ≈0.76, and Smin(S) is the minimum possible number of optimization steps
(parameter updates) estimated using Equation (5.4).
When training within a ﬁxed compute budget C, but with no other constraints, Equation (1.6) leads to the
prediction that the optimal model size N, optimal batch size B, optimal number of steps S, and dataset size
Dshould grow as
N ∝Cαmin
C /αN, B ∝Cαmin
C /αB, S ∝Cαmin
C /αS, D = B·S (1.7)
with
αmin
C = 1/(1/αS + 1/αB + 1/αN) (1.8)
which closely matches the empirically optimal results N ∝C0.73
min , B ∝C0.24
min , and S ∝C0.03
min . As the
computational budget C increases, it should be spent primarily on larger models, without dramatic increases
in training time or dataset size (see Figure 3). This also implies that as models grow larger, they become
increasingly sample efﬁcient. In practice, researchers typically train smaller models for longer than would
5



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 15):

10 7
 10 5
 10 3
 10 1
Compute (PF-days), non-embedding
103
105
107
Parameters (non-embedding)
N = (1.3 109) C0.73
min
N = (1.6 109) C0.88
10 7
 10 5
 10 3
 10 1
Compute (PF-days), excluding embeddings
0
5000
10000
15000Steps
Smin (adjusted)
Smin = (5.4 103) C0.03
min
S (fixed-batch)
Figure 14 Left: Each value of the compute budget Cmin has an associated optimal model size N. Optimal
model size grows very rapidly with Cmin, increasing by 5x for each 10x increase in compute. The number
of data examples processed makes up the remainder of the increase, growing relatively modestly by only 2x.
Right: The batch-adjusted number of optimization steps also grows very slowly, if at all, meaning that most
of the growth in data examples processed can be used for increased batch sizes.
can be ﬁt very well with a power-law
N(Cmin) ∝(Cmin)0.73. (6.1)
In Figure 12, we show the effect of training models of sub-optimal sizes (see Appendix B.4).
By deﬁnition Cmin ≡6NBcritS, and so we can use N(Cmin) to extract further results. In particular, since
prior ﬁts show B ∝L−4.8 and L∝C−0.05
min , we can conclude that Bcrit ∝C0.24
min . This leads us to conclude
that the optimal number of steps will only grow very slowly with compute, as
Smin ∝(Cmin)0.03, (6.2)
matching the empirical results in Figure 14. In fact the measured exponent is sufﬁciently small that our results
may even be consistent with an exponent of zero.
Thus we conclude that as we scale up language modeling with an optimal allocation of computation, we
should predominantly increase the model size N, while simultaneously scaling up the batch size via B ∝
Bcrit with negligible increase in the number of serial steps. Since compute-efﬁcient training uses relatively
few optimization steps, additional work on speeding up early training dynamics may be warranted.
6.2 Predictions from L(N,Smin)
The results for L(Cmin) and the allocations can be predicted from the L(N,Smin) equation obtained in
Section 5. Given our equation for L(N,Smin), we can substitute Smin = Cmin
6NB and then ﬁnd the minimum
of the loss as a function of N, while ﬁxing the training compute. We carry out this procedure in detail in
Appendix B, where we also provide some additional predictions.
For the loss as a function of training compute, we predict that
L(Cmin) =
(Cmin
c
Cmin
)αmin
C
(6.3)
where
αmin
C ≡ 1
1/αS + 1/αB + 1/αN
≈0.054 (6.4)
in excellent agreement with the exponent of Figure 13. We also predict that
N(Cmin) ∝(Cmin)αmin
C /αN ≈(Cmin)0.71 (6.5)
which also matches the scaling of Figure 14 to within a few percent. Our scaling laws provide a predictive
framework for the performance of language modeling.
16



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 21):

B.3 Comparison to Inefﬁcient
Typically, researchers train models until they appear to be close to convergence. In this section, we compare
the efﬁcient training procedure described above to this more typical setup. We deﬁne a the convergence factor
f as the percent deviation from the converged loss:
L(N,C) = (1 +f) L(N,∞) . (B.11)
For compute-efﬁcient training we have f = αN/αS ≈ 10% from the previous section, but researchers
typically use a much smaller value. Here, we choose f′= 2%as an estimate. For a ﬁxed value of the loss,
we predict:
Nf
Nf′
=
(1 +f
1 +f′
)1/αN
≈2.7 (B.12)
Sf
Sf′
=
(
1 +1
f
1 + 1
f′
)1/αS
≈0.13 (B.13)
Cf
Cf′
= Nf
Nf′
Sf
Sf′
≈0.35 (B.14)
So that compute-efﬁcient training uses 7.7x fewer parameter updates, 2.7x more parameters, and 65% less
compute to reach the same loss.
B.4 Suboptimal Model Sizes
We can solve A.1 to ﬁnd an expression for the amount of compute needed to reach a given value of the loss
Lwith a model of size N:
C(N,L) =
(
6B∗Sc
N
L1/αB
)(
L−
(Nc
N
)αN)−1/αS
. (B.15)
Using A.6 and A.9, we can eliminate Lin favor of Neﬀ (L), the model size which reaches Lmost efﬁciently.
From there, we ﬁnd an expression for the excess compute needed as a consequence of using a suboptimal
model size:
C(N,Neﬀ)
C(Neﬀ,Neﬀ) = N
Neﬀ
[
1 +αS
αN
(
1 −
(Neﬀ
N
)αN)]−1/αS
. (B.16)
The result is shown in Figure X. Models between 0.6x and 2.2x the optimal size can be used with only a
20% increase in compute budget. Using a smaller model is useful when accounting for the cost inference. A
larger model can be trained the the same level of performance in fewer steps, allowing for more parallelism
and faster training if sufﬁcient harware is available (see Figure Y):
S(N,Neﬀ)
S(Neﬀ,Neﬀ) =
[
1 +αS
αN
(
1 −
(Neﬀ
N
)αN)]−1/αS
. (B.17)
A 2.2x larger model requires 45% fewer steps at a cost of 20% more training compute. Note that this equation
should not be trusted for very large models, as it is only valid in the power-law region of the learning curve
after initial transient effects.
C Caveats
In this section we list some potential caveats to our analysis.
• At present we do not have a solid theoretical understanding for any of our proposed scaling laws.
The scaling relations with model size and compute are especially mysterious. It may be possible to
understand scaling at very large Dholding model size ﬁxed [AS17], and also the shape of learning
curves late in training, by modeling the loss with a noisy quadratic. But the scaling with Dat very
large model size still remains mysterious. Without a theory or a systematic understanding of the
corrections to our scaling laws, it’s difﬁcult to determine in what circumstances they can be trusted.
22



### Claim 143/179

#### Claim Text
Several authors have proposed alternatives in the context of LBM methods .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[64]_2302.00083.pdf (Page 0):

In-Context Retrieval-Augmented Language Models
Ori Ram∗ Yoav Levine∗ Itay Dalmedigos Dor Muhlgay
Amnon Shashua Kevin Leyton-Brown Yoav Shoham
AI21 Labs
{orir,yoavl,itayd,dorm,amnons,kevinlb,yoavs}@ai21.com
Abstract
Retrieval-Augmented Language Modeling
(RALM) methods, which condition a lan-
guage model (LM) on relevant documents
from a grounding corpus during generation,
were shown to significantly improve lan-
guage modeling performance. In addition,
they can mitigate the problem of factually
inaccurate text generation and provide natu-
ral source attribution mechanism. Existing
RALM approaches focus on modifying the
LM architecture in order to facilitate the in-
corporation of external information, signifi-
cantly complicating deployment. This paper
considers a simple alternative, which we dub
In-Context RALM: leaving the LM architec-
ture unchanged and prepending grounding
documents to the input, without any further
training of the LM. We show that In-Context
RALM that builds on off-the-shelf general
purpose retrievers provides surprisingly large
LM gains across model sizes and diverse cor-
pora. We also demonstrate that the document
retrieval and ranking mechanism can be spe-
cialized to the RALM setting to further boost
performance. We conclude that In-Context
RALM has considerable potential to increase
the prevalence of LM grounding, particularly
in settings where a pretrained LM must be
used without modification or even via API
access.1
1 Introduction
Recent advances in language modeling (LM) have
dramatically increased the usefulness of machine-
generated text across a wide range of use-cases
and domains (Brown et al., 2020). However, the
mainstream paradigm of generating text with LMs
bears inherent limitations in access to external
knowledge. First, LMs are not coupled with any
∗Equal contribution.
1Our code is available at https://github.com/
AI21Labs/in-context-ralm
Perplexity
10.0
15.0
20.0
25.0
30.0
GPT-2 345M (M) GPT-2 1.5B (XL)
No Retrieval In-Context RALM (BM25)
In-Context RALM (Predictive Reranking)
Figure 1: Our framework, dubbed In-Context
RALM, provides large language modeling gains on
the test set of WikiText-103, without modifying the
LM. Adapting the use of a BM25 retriever (Robert-
son and Zaragoza, 2009) to the LM task (§5) yields
significant gains, and choosing the grounding doc-
uments via our new class of Predictive Rerankers
(§6) provides a further boost. See Table 1 for the
full results on five diverse corpora.
source attribution, and must be trained in order
to incorporate up-to-date information that was not
seen during training. More importantly, they tend
to produce factual inaccuracies and errors (Lin
et al., 2022; Maynez et al., 2020; Huang et al.,
2020). This problem is present in any LM gen-
eration scenario, and is exacerbated when gener-
ation is made in uncommon domains or private
data. A promising approach for addressing the
above is Retrieval-Augmented Language Modeling
(RALM), grounding the LM during generation by
conditioning on relevant documents retrieved from
an external knowledge source. RALM systems in-
clude two high level components: (i) document se-
lection, selecting the set of documents upon which
to condition; and (ii) document reading, determin-
ing how to incorporate the selected documents into
the LM generation process.
Leading RALM systems introduced recently
arXiv:2302.00083v3  [cs.CL]  1 Aug 2023



Source: data\tc16_2312.10997v5\referenced_papers\[64]_2302.00083.pdf (Page 2):

methods that are specialized for the LM task. These
in turn can be used to improve both In-Context
RALM and other more elaborate RALM methods
that currently leverage general purpose retrievers.
Second, due to its compatibility with off-the-shelf
LMs, In-Context RALM can help drive wider de-
ployment of RALM systems.
2 Related Work
RALM approaches can be roughly divided into two
families of models: (i) nearest-neighbor language
models (also called kNN-LM), and (ii) retrieve
and read models. Our work belongs to the second
family, but is distinct in that it involves no further
training of the LM.
Nearest Neighbor Language ModelsThe kNN-
LM approach was first introduced in Khandel-
wal et al. (2020). The authors suggest a simple
inference-time model that interpolates between two
next-token distributions: one induced by the LM
itself, and one induced by the k neighbors from the
retrieval corpus that are closest to the query token in
the LM embedding space. Zhong et al. (2022) sug-
gest a framework for training these models. While
they showed significant gains from kNN-LM, the
approach requires storing the representations for
each token in the corpus, an expensive requirement
even for a small corpus like Wikipedia. Although
numerous approaches have been suggested for al-
leviating this issue (He et al., 2021; Alon et al.,
2022), scaling any of them to large corpora remains
an open challenge.
Retrieve and Read Models This family of
RALMs creates a clear division between document
selection and document reading components. All
prior work involves training the LM. We begin by
describing works that use this approach for tack-
ling downstream tasks, and then mention works ori-
ented towards RALM. Lewis et al. (2020) and Izac-
ard and Grave (2021) fine tuned encoder–decoder
architectures for downstream knowledge-intensive
tasks. Izacard et al. (2022b) explored different
ways of pretraining such models, while Levine
et al. (2022c) pretrained an autoregressive LM on
clusters of nearest neighbors in sentence embed-
ding space. Levine et al. (2022a) showed competi-
tive open domain question-answering performance
by prompt-tuning a frozen LM as a reader. Guu
et al. (2020) pretrained REALM, a retrieval aug-
mented bidirectional, masked LM, later fine-tuned
for open-domain question answering. The work
closest to this paper—with a focus on the language
modeling task—is RETRO (Borgeaud et al., 2022),
which modifies an autoregressive LM to attend to
relevant documents via chunked cross-attention,
thus introducing new parameters to the model. Our
In-Context RALM differs from prior work in this
family of models in two key aspects:
• We use off-the-shelf LMs for document read-
ing without any further training of the LM.
• We focus on how to choose documents for
improved LM performance.
3 Our Framework
3.1 In-Context RALM
Language models define probability distributions
over sequences of tokens. Given such a sequence
x1, ..., xn, the standard way to model its probabil-
ity is via next-token prediction: p(x1, ..., xn) =Qn
i=1 p(xi|x<i), where x<i := x1, ..., xi−1 is the
sequence of tokens preceding xi, also referred to
as its prefix. This autoregressive model is usu-
ally implemented via a learned transformer net-
work (Vaswani et al., 2017) parameterized by the
set of parameters θ:
p(x1, ..., xn) =
nY
i=1
pθ(xi|x<i), (1)
where the conditional probabilities are modeled
by employing a causal self-attention mask (Rad-
ford et al., 2018). Notably, leading LMs such
as GPT-2 (Radford et al., 2019), GPT-3 (Brown
et al., 2020), OPT (Zhang et al., 2022) or Jurassic-
1 (Lieber et al., 2021) follow this simple parame-
terization.
Retrieval augmented language models (RALMs)
add an operation that retrieves one or more docu-
ments from an external corpus C, and condition the
above LM predictions on these documents. Specifi-
cally, for predicting xi, the retrieval operation from
C depends on its prefix: RC(x<i), so the most
general RALM decomposition is: p(x1, ..., xn) =Qn
i=1 p(xi|x<i, RC(x<i)). In order to condition
the LM generation on the retrieved document, pre-
vious RALM approaches used specialized architec-
tures or algorithms (see §2). Inspired by the suc-
cess of In-Context Learning (Brown et al., 2020;
Dong et al., 2023), In-Context RALM refers to the
following specific, simple method of concatenating



Source: data\tc16_2312.10997v5\referenced_papers\[103]_2303.08559.pdf (Page 0):

Large Language Model Is Not a Good Few-shot InformationExtractor,
but a GoodReranker for Hard Samples!
Yubo Ma1, Yixin Cao2, YongChing Hong1, Aixin Sun1
1 S-Lab, Nanyang Technological University
2 Singapore Management University
yubo001@e.ntu.edu.sg
Abstract
Large Language Models (LLMs) have made
remarkable strides in various tasks. Whether
LLMs are competitive few-shot solvers for in-
formation extraction (IE) tasks, however, re-
mains an open problem. In this work, we
aim to provide a thorough answer to this ques-
tion. Through extensive experiments on nine
datasets across four IE tasks, we demonstrate
that current advanced LLMs consistently ex-
hibit inferior performance, higher latency, and
increased budget requirements compared to
fine-tuned SLMs under most settings. There-
fore, we conclude that LLMs are not effec-
tive few-shot information extractors in gen-
eral 1. Nonetheless, we illustrate that with
appropriate prompting strategies, LLMs can
effectively complement SLMs and tackle chal-
lenging samples that SLMs struggle with. And
moreover, we propose an adaptive filter-then-
rerank paradigm to combine the strengths of
LLMs and SLMs. In this paradigm, SLMs
serve as filters and LLMs serve as rerankers.
By prompting LLMs to rerank a small portion
of difficult samples identified by SLMs, our pre-
liminary system consistently achieves promis-
ing improvements (2.4% F1-gain on average)
on various IE tasks, with an acceptable time
and cost investment. Our code is available at
https://github.com/mayubo2333/LLM-IE.
1 Introduction
Large Language Models (LLMs, Brown et al. 2020;
Chowdhery et al. 2022; Touvron et al. 2023) have
shown remarkable abilities on various NLP applica-
tions such as factual question answering (Yu et al.,
2023; Sun et al., 2023), arithmetic reasoning (Chen
et al., 2022a; Qian et al., 2023) and logical rea-
soning (Jung et al., 2022; Pan et al., 2023). Given
the reasoning, memorization, instruction-following
and few-shot adaption capabilities emerging from
1A more precise assertion is that current LLMs, with
vanilla prompting setting and without IE-specific fine-tuning,
are not good few-shot information extractors in general.
LLMs, it prompts a compelling question: Can
LLMs be used to boost performance in few-shot
information extraction (IE) tasks?
To answer this question, we conduct an exten-
sive empirical study to compare the performance
between LLMs using in-context learning 2 (ICL)
and fine-tuned Small Language Models (SLMs).
We fairly evaluate SLMs-based and LLMs-based
methods across nine datasets spanning four com-
mon IE tasks: (1) Named Entity Recognition, (2)
Relation Extraction, (3) Event Detection and (4)
Event Argument Extraction. For each dataset, we
explored four to six settings to encompass typi-
cal low-resource extents, from 1-shot to 20-shot or
even more. Given the potential sensitivity of LLMs’
performance to the prompt context, we meticu-
lously considered variations in instruction, demon-
stration number and selection strategy, prompt for-
mat, etc. Our study reveals that LLMs excel over
SLMs only when annotations are extremely lim-
ited, i.e., both label types 3 and the samples 4 per
label are extremely scarce. With more ( e.g., hun-
dreds of) samples, SLMs significantly outperform
LLMs. Furthermore, LLMs incur greater inference
latency and costs than fine-tuned SLMs. Hence, we
claim that current LLMs are not good few-shot
information extractors in general.
We further investigate whether LLMs and SLMs
exhibit different abilities to handle various types of
samples. We categorize samples according to their
difficulty measured by SLMs’ confidence scores,
and compare LLMs’ and SLMs’ results within each
group. We find that LLMs are good at hard sam-
ples, though bad at easy samples. We posit that
the knowledge and reasoning abilities in LLMs en-
able them to handle hard samples (which are sim-
2All LLMs discussed in this paper are not fine-tuned, and
results for LLMs are based on in-context learning.
3Label types denote entity/relation/event/role types in dif-
ferent tasks. We use them interchangeably there-in-after.
4Samples refer to (i) demonstrations in ICL of LLMs, or
(ii) training samples for SLMs’ fine-tuning.
arXiv:2303.08559v2  [cs.CL]  21 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[36]_2303.08559.pdf (Page 0):

Large Language Model Is Not a Good Few-shot InformationExtractor,
but a GoodReranker for Hard Samples!
Yubo Ma1, Yixin Cao2, YongChing Hong1, Aixin Sun1
1 S-Lab, Nanyang Technological University
2 Singapore Management University
yubo001@e.ntu.edu.sg
Abstract
Large Language Models (LLMs) have made
remarkable strides in various tasks. Whether
LLMs are competitive few-shot solvers for in-
formation extraction (IE) tasks, however, re-
mains an open problem. In this work, we
aim to provide a thorough answer to this ques-
tion. Through extensive experiments on nine
datasets across four IE tasks, we demonstrate
that current advanced LLMs consistently ex-
hibit inferior performance, higher latency, and
increased budget requirements compared to
fine-tuned SLMs under most settings. There-
fore, we conclude that LLMs are not effec-
tive few-shot information extractors in gen-
eral 1. Nonetheless, we illustrate that with
appropriate prompting strategies, LLMs can
effectively complement SLMs and tackle chal-
lenging samples that SLMs struggle with. And
moreover, we propose an adaptive filter-then-
rerank paradigm to combine the strengths of
LLMs and SLMs. In this paradigm, SLMs
serve as filters and LLMs serve as rerankers.
By prompting LLMs to rerank a small portion
of difficult samples identified by SLMs, our pre-
liminary system consistently achieves promis-
ing improvements (2.4% F1-gain on average)
on various IE tasks, with an acceptable time
and cost investment. Our code is available at
https://github.com/mayubo2333/LLM-IE.
1 Introduction
Large Language Models (LLMs, Brown et al. 2020;
Chowdhery et al. 2022; Touvron et al. 2023) have
shown remarkable abilities on various NLP applica-
tions such as factual question answering (Yu et al.,
2023; Sun et al., 2023), arithmetic reasoning (Chen
et al., 2022a; Qian et al., 2023) and logical rea-
soning (Jung et al., 2022; Pan et al., 2023). Given
the reasoning, memorization, instruction-following
and few-shot adaption capabilities emerging from
1A more precise assertion is that current LLMs, with
vanilla prompting setting and without IE-specific fine-tuning,
are not good few-shot information extractors in general.
LLMs, it prompts a compelling question: Can
LLMs be used to boost performance in few-shot
information extraction (IE) tasks?
To answer this question, we conduct an exten-
sive empirical study to compare the performance
between LLMs using in-context learning 2 (ICL)
and fine-tuned Small Language Models (SLMs).
We fairly evaluate SLMs-based and LLMs-based
methods across nine datasets spanning four com-
mon IE tasks: (1) Named Entity Recognition, (2)
Relation Extraction, (3) Event Detection and (4)
Event Argument Extraction. For each dataset, we
explored four to six settings to encompass typi-
cal low-resource extents, from 1-shot to 20-shot or
even more. Given the potential sensitivity of LLMs’
performance to the prompt context, we meticu-
lously considered variations in instruction, demon-
stration number and selection strategy, prompt for-
mat, etc. Our study reveals that LLMs excel over
SLMs only when annotations are extremely lim-
ited, i.e., both label types 3 and the samples 4 per
label are extremely scarce. With more ( e.g., hun-
dreds of) samples, SLMs significantly outperform
LLMs. Furthermore, LLMs incur greater inference
latency and costs than fine-tuned SLMs. Hence, we
claim that current LLMs are not good few-shot
information extractors in general.
We further investigate whether LLMs and SLMs
exhibit different abilities to handle various types of
samples. We categorize samples according to their
difficulty measured by SLMs’ confidence scores,
and compare LLMs’ and SLMs’ results within each
group. We find that LLMs are good at hard sam-
ples, though bad at easy samples. We posit that
the knowledge and reasoning abilities in LLMs en-
able them to handle hard samples (which are sim-
2All LLMs discussed in this paper are not fine-tuned, and
results for LLMs are based on in-context learning.
3Label types denote entity/relation/event/role types in dif-
ferent tasks. We use them interchangeably there-in-after.
4Samples refer to (i) demonstrations in ICL of LLMs, or
(ii) training samples for SLMs’ fine-tuning.
arXiv:2303.08559v2  [cs.CL]  21 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[64]_2302.00083.pdf (Page 6):

WikiText-103
Perplexity
10.0
20.0
30.0
40.0
OPT-125M OPT-350M OPT-1.3B OPT-2.7B OPT-6.7B OPT-13B OPT-30B OPT-66B
No Retrieval In-Context RALM (BM25)
RealNews
Perplexity
3.0
8.0
13.0
18.0
OPT-125M OPT-350M OPT-1.3B OPT-2.7B OPT-6.7B OPT-13B OPT-30B OPT-66B
No Retrieval In-Context RALM (BM25)
Figure 4: Results of OPT models (Zhang et al., 2022) on the test set of WikiText-103 (word-level
perplexity) and the development set of RealNews (token-level perplexity). In-Context RALM models use
a BM25 retriever with s = 4(i.e., the retriever is called every four tokens) and ℓ = 32(i.e., the retriever
query is comprised of the last 32 tokens of the prefix). In-Context RALM with an off-the-shelf retriever
improved the performance of a 6.7B parameter OPT model to match that of a 66B parameter OPT model.
appealing since applying a BM25 retriever is sig-
nificantly cheaper than the neural alternatives.
5.2 Frequent Retrieval Improves Language
Modeling
We investigated the effect of varying the retrieval
stride s (i.e., the number of tokens between consec-
utive retrieval operations). Figure 5 shows that LM
performance improved as the retrieval operation
became more frequent. This supports the intuition
that retrieved documents become more relevant the
closer the retrieval query becomes to the gener-
ated tokens. Of course, each retrieval operation
imposes a runtime cost. To balance performance
and runtime, we used s = 4 in our experiments.
For comparison, RETRO employed a retrieval fre-
quency of s = 64(Borgeaud et al., 2022), which
leads to large degradation in perplexity. Intuitively,
retrieving with high frequency (low retrieval stride)
allows to ground the LM in higher resolution.
5.3 A Contextualization vs. Recency Tradeoff
in Query Length
We also investigated the effect of varying ℓ, the
length of the retrieval query for BM25. Figure 6
reveals an interesting tradeoff and a sweet spot
around a query length of 32 tokens. Similar ex-
periments for dense retrievers are given in App. A.
We conjecture that when the retriever query is too
short, it does not include enough of the input con-
text, decreasing the retrieved document’s relevance.
Conversely, excessively growing the retriever query
deemphasizes the tokens at the very end of the pre-
fix, diluting the query’s relevance to the LM task.
6 Improving In-Context RALM with
LM-Oriented Reranking
Since In-Context RALM uses a fixed document
reading component by definition, it is natural to
ask whether performance can be improved by spe-
cializing its document retrieval mechanism to the
LM task. Indeed, there is considerable scope for
improvement: the previous section considered con-
ditioning the model only on the first document re-



### Claim 144/179

#### Claim Text
To characterize a complex contagion process on simplicial complexes, a simplicial contagion model was recently developed .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[93]_2309.11495.pdf (Page 11):

Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation
reduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021.
Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. Head-to-tail: How knowledge-
able are large language models (llm)? aka will llms replace knowledge graphs? arXiv preprint
arXiv:2308.10168, 2023a.
Weiwei Sun, Zhengliang Shi, Shen Gao, Pengjie Ren, Maarten de Rijke, and Zhaochun Ren. Con-
trastive learning reduces hallucination in conversations. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 37, pp. 13618–13626, 2023b.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee
Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-
tian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,
Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,
2023b.
Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. A stitch in time saves
nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation.
arXiv preprint arXiv:2307.03987, 2023.
Chaojun Wang and Rico Sennrich. On exposure bias, hallucination and domain shift in neural
machine translation. arXiv preprint arXiv:2005.03642, 2020.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-
ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.
arXiv preprint arXiv:2203.11171, 2022.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in
Neural Information Processing Systems, 35:24824–24837, 2022.
Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. Large language models are reasoners
with self-verification. arXiv preprint arXiv:2212.09561, 2022.
Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith,
Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for
language model training. arXiv preprint arXiv:2306.01693, 2023.
Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. Improving language
models via plug-and-play retrieval feedback. arXiv preprint arXiv:2305.14002, 2023.
Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. How language model
hallucinations can snowball. arXiv preprint arXiv:2305.13534, 2023.
Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. Verify-and-edit: A
knowledge-enhanced chain-of-thought framework. arXiv preprint arXiv:2305.03268, 2023.
12



Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 3):

Language Models as Semantic Indexers
… Codebook𝑬… Codebook𝑬
…
Semantic IDsWordtoken
He onlya rival
Deep TransEncoderDeep TransDecoder
Semantic Indexer
<s>
ShallowTransformer
Masked token 
…He only
is beenrival
ReconstructorSemantic Encoder
is been
Semantic ID representation
Document
 hints
Figure 1.The LMI NDEXER self-supervised ID learning framework overview. The proposed semantic indexer includes a semantic ID
encoder and several codebooks. During self-supervised learning, there is a reconstructor to reconstruct the input document from semantic
ID representations.
query channel input embeddings, and dh (token embeddings
correspond to dh) are fed as key and value channel input
embeddings in the multi-head self-attention. We adopt a
shallow reconstructor which has limited reconstruction capa-
bility based only on the hints in order to force the semantic
indexer to provide high-quality representations. The recon-
struction is conducted as follows:
zw “Reconϕpcd, dhq“
ÿ
t
Transpq “ct
d, k“dh, v“dhq
Preconpw|cd, dhq“ softmaxpW zwq
(5)
where W is the token embedding matrix. However, directly
adopting the reconstruction objective with cd as input to the
reconstructor will not optimize the semantic encoder. Since
the codebook look-up in Eq.(2) is a hard/discrete operation,
the reconstruction objective backpropagation gradients will
flow to the embeddings in the codebook rather than to the
parameters in the semantic encoder. To this end, we propose
to approximate the argmax operation similar to (Jang et al.,
2016) as follows:
ˆct
d “
$
&
%
arg maxet
jPEt ht
d ¨et
j forward pass.
ř
et
jPEt
exppht
d¨et
jqř
et
jPEt exppht
d¨et
jq et
j backward pass. (6)
In the forward pass, we still adopt the argmaxp¨qhard op-
eration; while in the backward pass, the selected semantic
embedding becomes a weighted average of the codebook
embeddings, to enable gradients to flow to ht
d and finally
to the parameters in the semantic encoder. In our imple-
mentation, we achieve this by adopting the stop gradient
operator (Van Den Oord et al., 2017). The reconstruction is
then conducted by
zw “Reconϕpˆct
d, dhq“
ÿ
t
Transpq “ˆct
d, k“dh, v“dhq
(7)
3.2. Training Self-Supervised Semantic Indexer
Progressive Training. To optimize the semantic indexer
and obtain semantic IDs in an auto-regressive way, we adopt
the progressive training scheme similar to (Sun et al., 2023).
The entire learning process consists ofT learning steps, each
corresponding to a specific semantic IDct
d being learned and
optimized at position t within the range of [T]. Additionally,
at each step t, both the ID ct
d and the model parameters
associated with generating ct
d are updated, while previously
generated IDs căt
d remain unchanged. The reconstruction
objective in t-step is shown as:
Lt
recon “´
ÿ
d
ÿ
wPdzdt
h
logPreconpw|cďt
d , dt
hq. (8)
Here dt
h is the hints provided for learning ID on position
t. We will gradually reduce the amounts of hints dt
h as t
increases to inject new knowledge into the new IDs, and
finally contribute to a hierarchical, coarse-to-fine-grained
semantic ID learning.
Contrastive Loss. The reconstruction objective in Eq.(8)
can force the semantic IDs to capture document-level se-
mantics. However, only optimizing the objective can lead to
the case where similar documents sharing căt
d also have the
same ct
d. To alleviate this issue, we propose a contrastive
objective to promote distinction between documents that
previously shared the same prefix, enabling the model to
discern finer-grained hierarchical relationships between doc-
uments:
Lt
contrastive “´
ÿ
d
log exppht
d ¨ht
dq
exppht
d ¨ht
dq` ř
căt
d1 “căt
d
exppht
d ¨ht
d1q.
(9)
The contrastive objective can help push ht
d of documents
sharing the same căt
d away in the t-th latent space and force
them to obtain diverse ct
d, finally contributing to higher
codebook utilization.
Commitment Loss. In addition, when learning the docu-
ment semantic IDs for position t, it is important that the
semantic indexer should remember the IDs that are already
learned before position t. To this end, we add a commitment
loss as:
Lt
commitment “´
ÿ
d
ÿ
jăt
log Pspcj
d|d, căj
d q. (10)
We optimize our model at step t based on a combination of
4



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 17):

One might also conjecture that this intersection point has a deeper meaning. If we cannot increase the model
size beyond N∗ without qualitatively different data requirements, perhaps this means that once we reach
C∗
min and N∗, we have extracted all of the reliable information available in natural language data. In this
interpretation, L∗ would provide a rough estimate for the entropy-per-token 7 of natural language. In this
scenario, we would expect the loss trend to level off at or before L∗.
We can guess at the functional form of L(Cmin) as it levels off by considering a version of our training
dataset with added noise. For example, we could append a random string of tokens to each context shown
to the model to artiﬁcially boost the loss by a constant additive factor. Then, the distance from the noise
ﬂoor L−Lnoise would be a more meaningful performance metric, with even a small decrease in this distance
potentially representing a signiﬁcant boost in qualitative performance. Since the artiﬁcial noise would affect
all of our trends equally, the critical point of 6.8 would not change (aside from the absolute value ofL∗), and
may be meaningful even if it occurs after the leveling off.
7 Related Work
Power laws can arise from a wide variety of sources [THK18]. Power-law scalings with model and dataset
size in density estimation [Was06] and in random forest models [Bia12] may be connected with our results.
These models suggest that power-law exponents may have a very rough interpretation as the inverse of the
number of relevant features in the data.
Some early [BB01, Goo01] work found power-law scalings between performance and dataset size. More
recent work [HNA+17, HAD19] also investigated scaling between model size and data size; their work is
perhaps the closest to ours in the literature 8. Note, however, that [HNA +17] found super-linear scaling of
dataset size with model size, whereas we ﬁnd a sub-linear scaling. There are some parallels between our
ﬁndings on optimal allocation of compute and [Kom19], including power-law learning curves. EfﬁcientNets
[TL19] also appear to obey an approximate power-law relation between accuracy and model size. Very recent
work [RRBS19b] studies scaling with both dataset size and model size for a variety of datasets, and ﬁts an
ansatz similar to ours.
EfﬁcientNet [TL19] advocates scaling depth and width exponentially (with different coefﬁcients) for optimal
performance of image models, resulting in a power-law scaling of width as a function of depth. We ﬁnd that
for language models this power should be roughly one when scaling up (as width/depth should remain ﬁxed).
But more importantly, we ﬁnd that the precise architectural hyperparameters are unimportant compared to the
overall scale of the language model. In [VWB16] it was argued that deep models can function as ensembles
of shallower models, which could potentially explain this ﬁnding. Earlier work [ZK16] has compared width
and depth, and found that wide ResNets can outperform deep ResNets on image classiﬁcation. Some studies
ﬁx computation per data example, which tends to scale in proportion to the number of model parameters,
whereas we investigate scaling with both model size and the quantity of training computation.
Various works [AS17, BHMM18] have investigated generalization in highly overparameterized models, ﬁnd-
ing a “jamming transition” [GJS+19] when the model size reaches the dataset size (this may require training
many orders of magnitude beyond typical practice, and in particular does not use early stopping). We do
not observe such a transition, and ﬁnd that the necessary training data scales sublinearly in the model size.
Expansions in the model size, particularly at large width [JGH18, LXS+19], may provide a useful framework
for thinking about some of our scaling relations. Our results on optimization, such as the shape of learning
curves, can likely be explained using a noisy quadratic model, which can provide quite accurate predictions
[ZLN+19] in realistic settings. Making this connection quantitative will require a characterization of the
Hessian spectrum [Pap18, GKX19, GARD18].
8 Discussion
We have observed consistent scalings of language model log-likelihood loss with non-embedding parameter
count N, dataset size D, and optimized training computation Cmin, as encapsulated in Equations (1.5) and
(1.6). Conversely, we ﬁnd very weak dependence on many architectural and optimization hyperparameters.
Since scalings with N,D,C min are power-laws, there are diminishing returns with increasing scale.
7Deﬁning words using the wc utility, the WebText2 dataset has1.4 tokens per word and 4.3 characters per token.
8After this work was completed, [RRBS19a] also appeared, which makes similar predictions for the dependence of
loss on both model and dataset size.
18



Source: data\tc16_2312.10997v5\referenced_papers\[93]_2309.11495.pdf (Page 0):

CHAIN -OF-VERIFICATION REDUCES HALLUCINATION
IN LARGE LANGUAGE MODELS
Shehzaad Dhuliawala
Meta AI & ETH Z¨urich
Mojtaba Komeili
Meta AI
Jing Xu
Meta AI
Roberta Raileanu
Meta AI
Xian Li
Meta AI
Asli Celikyilmaz
Meta AI
Jason Weston
Meta AI
ABSTRACT
Generation of plausible yet incorrect factual information, termed hallucination,
is an unsolved issue in large language models. We study the ability of language
models to deliberate on the responses they give in order to correct their mistakes.
We develop the Chain-of-Verification (COVE) method whereby the model first (i)
drafts an initial response; then (ii) plans verification questions to fact-check its
draft; (iii) answers those questions independently so the answers are not biased
by other responses; and (iv) generates its final verified response. In experiments,
we show COVE decreases hallucinations across a variety of tasks, from list-based
questions from Wikidata, closed book MultiSpanQA and longform text generation.
1 I NTRODUCTION
Large Language Models (LLMs) are trained on huge corpora of text documents with billions of
tokens of text. It has been shown that as the number of model parameters is increased, performance
at tasks such as closed book QA improve in accuracy, and larger models can generate more correct
factual statements (Radford et al., 2019; Petroni et al., 2019). However, even the largest models can
still fail, particularly on lesser known torso and tail distribution facts (Sun et al., 2023a), i.e. those
that occur relatively rarely in the training corpora. In those cases where the model is incorrect, they
instead generate an alternative response which is typically plausible looking (e.g., a similar entity, but
an incorrect one). These factually incorrect generations are referred to as hallucinations (Maynez
et al., 2020). Further, in longform tasks consisting of generating multiple sentences or paragraphs, the
hallucination problem can be exacerbated due to the issue of exposure bias (Wang & Sennrich, 2020).
The current wave of language modeling research goes beyond next word prediction, and has focused
on their ability to reason. Improved performance in reasoning tasks can be gained by encouraging
language models to first generate internal thoughts or reasoning chains before responding (Wei et al.,
2022; Adolphs et al., 2021; Wang et al., 2022; Lanchantin et al., 2023), as well as updating their
initial response through self-critique (Press et al., 2022; Madaan et al., 2023). In this work we
follow this line of research to study how and when language-model-based reasoning can be used to
reduce hallucinations. We develop an approach, called Chain-of-Verification (CoVe) which, given
an initial draft response, first plans verification questions to check its work, and then systematically
answers those questions in order to finally produce an improved revised response. We find that
independent verification questions tend to provide more accurate facts than those in the original
longform answer, and hence improve the correctness of the overall response. We study variations on
this recipe across a range of tasks: from list-based questions, closed booked QA and longform text
generation. We first propose a joint approach for generating the entire verification chain left-to-right,
which improves performance and decreases hallucinations compared to the baseline language model.
However, models that attend to existing hallucinations in the context from their own generations tend
to repeat the hallucinations. Hence we also introduce further improvements with factored variants
which separate out the verification chain steps, in terms of which context is attended to. We show
how these factored variants give further performance gains across all three tasks considered.
1
arXiv:2309.11495v2  [cs.CL]  25 Sep 2023



Source: data\tc16_2312.10997v5\referenced_papers\[168]_2311.08147.pdf (Page 10):

Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-
Dodds, Jack Clark, Samuel R. Bowman, Amanda
Askell, Roger Grosse, Danny Hernandez, Deep Gan-
guli, Evan Hubinger, Nicholas Schiefer, and Jared
Kaplan. 2023. Discovering language model behav-
iors with model-written evaluations. In Findings of
the Association for Computational Linguistics: ACL
2023, pages 13387–13434, Toronto, Canada. Associ-
ation for Computational Linguistics.
Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen,
Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,
Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su,
Huadong Wang, Cheng Qian, Runchu Tian, Kunlun
Zhu, Shi Liang, Xingyu Shen, Bokai Xu, Zhen Zhang,
Yining Ye, Bo Li, Ziwei Tang, Jing Yi, Yu Zhu, Zhen-
ning Dai, Lan Yan, Xin Cong, Ya-Ting Lu, Weilin
Zhao, Yuxiang Huang, Jun-Han Yan, Xu Han, Xian
Sun, Dahai Li, Jason Phang, Cheng Yang, Tong-
shuang Wu, Heng Ji, Zhiyuan Liu, and Maosong
Sun. 2023a. Tool learning with foundation models.
ArXiv, abs/2304.08354.
Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan,
Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru Tang,
Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie,
Jie Zhou, Marc H. Gerstein, Dahai Li, Zhiyuan Liu,
and Maosong Sun. 2023b. Toolllm: Facilitating large
language models to master 16000+ real-world apis.
ArXiv, abs/2307.16789.
Vipula Rawte, Amit Sheth, and Amitava Das. 2023. A
survey of hallucination in large foundation models.
arXiv preprint arXiv:2309.05922.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. 2023. Toolformer:
Language models can teach themselves to use tools.
ArXiv, abs/2302.04761.
Mrinank Sharma, Meg Tong, Tomasz Korbak, David
Duvenaud, Amanda Askell, Samuel R. Bowman,
Newton Cheng, Esin Durmus, Zac Hatfield-Dodds,
Scott R. Johnston, Shauna Kravec, Timothy Maxwell,
Sam McCandlish, Kamal Ndousse, Oliver Rausch,
Nicholas Schiefer, Da Yan, Miranda Zhang, and
Ethan Perez. 2023. Towards understanding syco-
phancy in language models.
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,
and Jason Weston. 2021. Retrieval augmentation
reduces hallucination in conversation. In Findings
of the Association for Computational Linguistics:
EMNLP 2021, pages 3784–3803, Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models.
Alex Turner, Lisa Thiergart, David Udell, Gavin Leech,
Ulisse Mini, and Monte MacDiarmid. 2023. Acti-
vation addition: Steering language models without
optimization. arXiv preprint arXiv:2308.10248.
Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023.
Self-knowledge guided retrieval augmentation for
large language models.
Jerry W. Wei, Da Huang, Yifeng Lu, Denny Zhou,
and Quoc V . Le. 2023. Simple synthetic data re-
duces sycophancy in large language models. ArXiv,
abs/2308.03958.
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang,
Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang,
Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng
Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao,
Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Ji-
aming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su,
Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang
Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Pei-
dong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li,
Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong
Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin
Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li,
Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan
Zhou, and Zhiying Wu. 2023. Baichuan 2: Open
large-scale language models.
Xiaoyu Yang, Stephen Obadinma, Huasha Zhao, Qiong
Zhang, Stan Matwin, and Xiaodan Zhu. 2020.
Semeval-2020 task 5: Counterfactual recognition.
ArXiv, abs/2008.00563.
W. Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang,
and Ashish Sabharwal. 2023. Improving language
models via plug-and-play retrieval feedback. ArXiv,
abs/2305.14002.
Biao Zhang, Barry Haddow, and Alexandra Birch.
2023a. Prompting large language model for machine
translation: A case study. ArXiv, abs/2301.07069.
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,
Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,
Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei
Bi, Freda Shi, and Shuming Shi. 2023b. Siren’s song
in the ai ocean: A survey on hallucination in large
language models. ArXiv, abs/2309.01219.



### Claim 145/179

#### Claim Text
The deposition of materials was performed by electron beam evaporator using selfaligned shadow evaporation technique .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 11):

Language Models as Semantic Indexers
Table 7.Dataset Statistics
Dataset # Items # Users # Rec history (train/dev/test) # Search query (train/dev/test) # Search labels (train/dev/test)
Amazon-Beauty 12,101 22,363 111,815 / 22,363 / 22,363 1,049 / 150 / 338 1,907 / 268 / 582
Amazon-Sports 18,357 35,598 177,990 / 35,598 / 35,598 1,299 / 186 / 443 2,209 / 311 / 764
Amazon-Toys 11,924 19,412 97,060 / 19,412 / 19,412 1,010 / 145 / 351 1,653 / 250 / 594
Table 8.Dataset Statistics
Dataset # Documents # Query (train/test) # Search labels (train/test)
NQ320k 109,739 307,373 / 7,830 307,373 / 7,830
MACRO 1M 1,000,000 502,939 / 6,980 532,751 / 7437
TREC-DL 1M 1,000,000 502,939 / 93 532,751 / 1,069
Algorithm 1 Self-supervised ID Learning Procedure of
LMI NDEXER
1: Input: The document corpus tdu.
2: Output: The semantic IDs tcduof the documents tdu.
A semantic indexer SemIndexerp¨qwhich contains a
semantic encoder SemEncθp¨qand codebooks tEtut.
A reconstruction model Reconϕp¨q.
3: Begin
4: // initialize semantic encoder
5: SemEncθp¨qÐ T5-base;
6: // reconstruction warm up
7: minϕ L0
recon “´ ř
d
ř
wPdzd0
h
log Preconpw|d0
h q;
8: for t “1, . . . , Tdo
9: // semantic encoder & codebook warm up
10: ht ÐSemEncθpd, cdq;
11: zw ÐReconϕpq “tct
d, ht
du, k“dt
hq;
12: minϕ,ϕc Lt “Lt
recon `Lt
contrastive `Lt
commitment;
13: ht
d ÐSemEncθpd, ct
dq;
14: Et ÐKMeanspht
dq;
15: // whole framework training
16: zw ÐReconϕpq “tct
d, ct
du, k“dh, v“dhq;
17: minϕ,ϕc Lt “Lt
recon `Lt
contrastive `Lt
commitment;
18: ct
d Ðargmaxjpspct
d “j|cd, dq;
19: end for
20: Return tcdu, SemIndexerp¨q;
21: End
rate in {1e-3, 2e-3, 5e-3}. The training epochs are set to be
30, 10, and 5 for Amazon datasets, NQ, and MS MACRO
respectively. The hyper-parameter configuration for self-
supervised semantic indexer training can be found in Table
9.
In the downstream recommendation task, for generative
recommendation methods with semantic IDs (rq-V AE in-
dexer, hierarchical clustering indexer, and LMI NDEXER ),
we concatenate the textual information (title & description)
of the user’s previously interacted items, serve it as the input
text into the generative language model and ask the model
to generate the ID for next item. The baselines are using
the same T5-base checkpoint. We train all the compared
generative recommendation methods for 10,000 steps with
the learning rate searched in {1e-2, 1e-3, 1e-4}. The batch
size is set to be 32, the maximum input text length is set
to be 1024 and all experiments are run on an 8 A100 40G
machine. The number of beams for beam search is set to
20. The hyper-parameter configuration for generative rec-
ommendation training can be found in Table 10.
In the downstream product search task, for generative re-
trieval methods with semantic IDs (rq-V AE indexer, hierar-
chical clustering indexer, and LMI NDEXER ), we serve the
query as the input text into the generative language model
and ask the model to generate the ID for the relevant items.
All baselines initially load the same T5-base checkpoint.
We train all the compared generative retrieval methods for
10,000 steps with the learning rate searched in {1e-2, 1e-
3, 1e-4}. The batch size is set to 32, the maximum input
text length is set to be 1024 and all experiments are run on
an 8 A100 40G machine. The number of beams for beam
search is set to 20. The hyper-parameter configuration for
generative product search training can be found in Table 11.
In the downstream document retrieval task, for generative
retrieval methods with semantic IDs (rq-V AE indexer, hier-
archical clustering indexer, and LMI NDEXER ), we serve the
query as the input text into the semantic indexer and ask the
model to generate the ID for the relevant documents. Fol-
lowing (Wang et al., 2022), we use docT5query (Nogueira
12



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 7):

Conference’17, July 2017, Washington, DC, USA Zhang and Xiao, et al.
Table 4: Ablation study for the three influential factors about LLM-Embedder’s training: using soft reward from LLM, stabilized
distillation, instruction based fine-tuning, in-batch negative sampling from the same scenario.
Knowledge ICL Long Tool Conv Search
Method MMLU PopQA Misc. MSC ArXiv ToolLLM QReCC
w.o. LLM Reward 0.4872 0.4794 0.6217 13.9176 3.2495 0.8927 0.4945
w.o. Instruction FT 0.4776 0.5025 0.6211 13.9125 3.2383 0.8192 0.5029
w.o. homo NS 0.4791 0.4520 0.6200 14.0441 3.2558 0.8364 0.4563
w.o. Stablized Distill 0.4815 0.5027 0.6105 13.6090 3.2441 0.7905 0.4865
AAR 0.4826 0.4792 0.5938 14.6999 3.3260 0.4200 0.2877
API-Retriever 0.4625 0.2488 0.5942 14.7834 3.3858 0.8017 0.1137
LLM-R 0.4625 0.2506 0.6262 14.4746 3.3635 0.1321 0.0234
LLM-Embedder 0.4903 0.5052 0.6268 13.4832 3.2322 0.8645 0.5053
the embeddings, because a great portion of the negative samples
will come from different tasks, which are irrelevant with each other.
As we can observe, LLM-Embedder’s performance is decreased due
to such a change, especially for PopQA and Conv Search, where a
massive candidate pool is presented (Wikipedia corpus).
For “w.o. stabilized distill ”, we replace our stabilized distilla-
tion with the conventional KL-divergence based method. As intro-
duced, this operation handles the fluctuated reward from LLM such
that distillation can become more stabilized. We can observe that
LLM-Embedder’s performance is reduced once this step is removed,
especially for ICL where LLM’s reward is the major training signal.
4 RELATED WORKS
The related works are reviewed from two perspectives: retrieval
augmented large language models, and dense retrieval.
•Retrieval Augmented LLMs . Large language models (LLMs)
are praised for their unprecedented capability on language under-
standing and generation. Compared with the conventional methods,
LLMs exhibit overwhelming generality and notable advantages on
typical NLP tasks [17, 19, 78]. Despite such superiority, LLMs still
face a series of severe challenges, such as hallucination, human
alignment, and long-term memory. Many of the existing problems
are caused by the inherent boundaries, which cannot be addressed
by LLMs alone, but to rely on support from the external world. The
retrieval-augmented LLMs are regarded as a go-to option to bridge
LLMs with the external assistance [4, 51]. For the past few years,
they have been widely applied to several critical scenarios. One com-
mon case is the knowledge enhancement. The internal knowledge
of LLMs can be incomplete, static, and limited by the popularity
bias. When dealing with knowledge intensive tasks, the retrieval
augmented LLMs will look for necessary information from an ex-
ternal database, where the generated content can be grounded on
proper knowledge [15, 31, 32, 41]. Besides, the retrieval augmented
LLMs are also used to retrieve historical context to establish long-
term memory [71, 85], retrieve examples to improve the instruction
following capability [18, 83], and retrieve tools to engage with the
physical world [62].
The retrieval augmented LLMs consist of two basic parts: gener-
ator and retriever. According to previous studies [32, 41, 83, 96], the
retrieval augmentation effect is highly influenced by the retrieved
content. In practice, there are two common types of retrievers. One
is to leverage the general purpose retrievers, such as sparse models
like BM25 [69], and dense models, like DPR [37], contriever [30],
E5 [81], BGE [89], OpenAI text embedding [56]. The other option
is develop task-specific retriever, e.g., AAR for knowledge enhance-
ment [96], LLM-R [85] for in-context learning. The general pur-
pose methods are praised for their generality and simplicity for
usage, but may suffer from an inferior retrieval quality. In contrast,
the task-specific ones can better fit one scenario, but fall short in
transferability. Compared with the existing works, LLM-Embedder
unifies the generality and speciality: it comprehensive supports all
major retrieval augmentation needs of LLMs, meanwhile achieving
the leading performance in every application scenario.
•Dense retrieval. Dense retrieval leverages latent representa-
tion of texts, i.e. embeddings, to search for relevant information
from a vector DB. In recent years, it has grown into a major para-
digm of information retrieval. The success of dense retrieval can
attribute to several reasons. The first and foremost driving force is
the development of pre-trained language models[22, 45, 65], where
the textual data can be represented in a highly expressive man-
ner. The general pre-trained models are further improved by the
retrieval-oriented ones [46, 81], which better establish the sentence-
level representation capability during the pre-training stage. The
second factor is the advancement of contrastive learning. On one
hand, there has been a major upgrade of negative sampling, where
massive [30, 37] and sufficiently hard samples [92] are utilized to
help with the embedding’s discriminativeness. On the other hand,
the training objective is improved as well. Instead of simply learning
from hard labels, the embedding models are made to distill knowl-
edge from a more precise ranking model [29, 63, 90]. This notably
facilitates the embedding model to encode fine-grained semantic
relationships. Thirdly, the generality becomes increasingly empha-
sized in these days, where embeddings need to handle a wide variety
of application scenarios. For this purpose, people come up with
many different strategies, e.g., data augmentation [42, 80], domain
adaptation [36, 95], instruction-based fine-tuning [ 5, 76], which
help the model to better handle diverse tasks. These factors are
incorporated and optimized while developing our training recipe,
which results in the empirical competitiveness of LLM-Embedder.



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 14):

Retrieve Anything To Augment Large Language Models Conference’17, July 2017, Washington, DC, USA
Table 9: Dataset details for training.
Task Dataset #Train Sample Repetition Stablized Distillation Reward Temperature
Question Answering MSMARCO 400870 1 ✓ 1
NQ 58622 1 1
In-Context Learning – 591359 1 ✓ 1
Long Conversation MSC 48925 1 ✓ 0.1
Long-Range Language Modeling
Books3 10000 1
✓
0.1
Arxiv 10000 1 0.1
CodeParrot 10000 1 0.1
Tool Learning ToolBench 87322 2 ✗ n.a.
Conversational Search QReCC 29596 1 ✗ n.a.
Total n.a. 1333911 n.a. n.a. n.a.
Table 10: Instructions for each task.
Task Input Instruction
Question Answering Query Represent this query for retrieving relevant documents:
Key Represent this document for retrieval:
In-Context Learning Query Convert this example into vector to look for useful examples:
Key Convert this example into vector for retrieval:
Long Conversation Query Embed this dialogue to find useful historical dialogues:
Key Embed this historical dialogue for retrieval:
Long-Range Language Modeling Query Embed this text chunk for finding useful historical chunks:
Key Embed this historical text chunk for retrieval:
Tool Learning Query Transform this user request for fetching helpful tool descriptions:
Key Transform this tool description for retrieval:
Conversational Search Query Encode this query and context for searching relevant passages:
Key Encode this passage for retrieval:
Table 11: Hyper parameter settings for training.
#GPU 8×A100 (40G)
#Hard Negative 7
Batch Size Per GPU 100
Optimizer AdamW
Learning Rate 5e-6
Weight Decay 0.01
Scheduler Linear with Warm Up of 0.2
Max Steps 10000
Gradient Checkpointing ✓
B IMPLEMENTATION DETAILS
B.1 Instructions
We use diversified instructions to discriminate different tasks for
the retriever. The instructions used for each task are shown in
Table 10.
B.2 Training Settings
The hyper parameter settings for training LLM-Embedder are re-
ported in Table 11. For evaluation, we use the Flat index from
Faiss [35] when retrieving from an external corpus is required. We
will release our code upon the acceptance of the paper.
C IMPACT OF LLM-EMBEDDER ON
DIFFERENT LLMS
We evaluate the impact of LLM-Embedder different LLMs to val-
idate its generalization ability. Specifically, we utilize Aquila-7B-
Chat [1], Qwen-7B-Chat [6], Baichuan2-7B-Chat [9], and Llama-
2-13B-Chat [78]. The results are shown in Table 12. Specifically,
we compare two baselines: None, where LLM is used individually
without retrieval augmentation; BGE, where LLM is augmented
with retrieved knowledge, examples, and memory (introduced in
Appendix A). We report the average accuracy for MMLU, accuracy
for PopQA, average score for in-context learning, and perplexity for
both Multi-Session Chat and Arxiv. Note that we do not replicate



Source: data\tc16_2312.10997v5\referenced_papers\[93]_2309.11495.pdf (Page 2):

procedure, mitigated, and then the generation is continued. An alternative to using the confidence
scores is to leverage inconsistencies in the LLMs output to detect hallucination. Agrawal et al. (2023)
use both multiple samples and consistency detection by asking direct and indirect queries to check for
hallucinated references. Cohen et al. (2023) introduce a method called LM vs LM which simulates
an interactive setup between two LLMs where one LLM acts as an examiner and tests if the output
is consistent via repeated cross-examination. Cohen et al. (2023) shows that using inconsistencies
for QA tasks can outperform using confidence scores for hallucination detection. C OVE also uses a
related self-consistency approach, but without the multi-agent (multi-LLM) debate concept.
A third approach is to use external tools to help mitigate hallucinations, rather than relying solely on
the abilities of the language model itself. For example, retrieval-augmented generation can decrease
hallucinations by using factual documents for grounding (Shuster et al., 2021; Jiang et al., 2023b;
Yu et al., 2023) or chain-of-thought verification (Zhao et al., 2023). Other approaches include using
tools for fact-checking (Chern et al., 2023a; Galitsky, 2023; Peng et al., 2023), or linking to external
documents with attribution (Menick et al., 2022; Rashkin et al., 2023; Gao et al., 2023).
There are also a number of related works in improving reasoning for logical and mathematical tasks,
even if they do not address reducing hallucination explicitly. Several approaches have been shown to
improve results with extended reasoning steps by the system, such as chain-of-thought (Wei et al.,
2022), deductive verification (Ling et al., 2023), and self-verification (Miao et al., 2023; Jiang et al.,
2023a; Weng et al., 2022). The latter tries to predict the (masked) question given the answer for math
problems, and use that as evidence that this is the correct solution.
3 C HAIN -OF-VERIFICATION
Our approach assumes access to a base LLM that – despite potentially being prone to hallucination –
is capable of being prompted with general instructions in either a few-shot or zero-shot fashion. Akey
assumption of our method is that this language model, when suitably prompted, can both generate
and execute a plan of how to verify itself in order to check its own work, and finally incorporate this
analysis into an improved response.
Our overall process, which we call Chain-of-Verification (CoVe), thus performs four core steps:
1. Generate Baseline Response: Given a query, generate the response using the LLM.
2. Plan Verifications: Given both query and baseline response, generate a list of verification
questions that could help to self-analyze if there are any mistakes in the original response.
3. Execute Verifications: Answer each verification question in turn, and hence check the answer
against the original response to check for inconsistencies or mistakes.
4. Generate Final Verified Response: Given the discovered inconsistencies (if any), generate a
revised response incorporating the verification results.
Each of these steps is performed by prompting the same LLM in different ways to obtain the desired
response. While steps (1), (2) and (4) all can be invoked with a single prompt, we investigate
variations of step (3) including joint, 2-step and factored versions. These variants either involve a
single prompt, two prompts or else independent prompts per question, where more sophisticated
decomposition can yield improved results.
We describe these steps in more detail below. An overview of the approach is illustrated in Figure 1,
and in the Appendix in Figure 3.
3.1 B ASELINE RESPONSE
Given a query, we generate left-to-right as usual using the LLM, with no special tricks. While this is
the first step in the CoVe pipeline, it also serves as the baseline we wish to improve in our experiments
(i.e., we will directly compare this baseline response with the final verified response from our overall
method).
Given such baseline generations are typically prone to hallucination, CoVe attempts to identify these
hallucinations, and correct them, in the following steps.
3



Source: data\tc16_2312.10997v5\referenced_papers\[70]_2310.13243.pdf (Page 9):

Table 6: Overall effectiveness of the models and statistical significance analysis. The best results are highlighted in
boldface. Superscripts denote significant differences (t-test, p ≤ 0.05). x -> y denotes the x retriever re-ranked by y
re-ranker.
# Model TRECC DBpedia FiQA Robust04
a BM25 59.5b 31.8b 23.6b 40.7e
b QLM-Dirichlet 50.8 29.5 20.5 40.7e
c HyDE 58.2 37.1abe 26.6ab 41.8e
d BM25+HyDE 69.9abc 41.6abcefghiklmop 30.9abc 49.7abcef
e BM25 -> T5-11B 67.9abc 33.7ab 32.0abc 27.4f BM25 -> Alpaca-7B 67.0abc 35.0ab 33.7abcd 44.6abe
g BM25 -> StableLM-7B 74.0abcefi 37.2abef 34.1abcde 48.3abcef
h BM25 -> StableVicuna-13B71.8abcfi 39.4abefgp 39.1abcdefgi 51.3abcefg
i BM25 -> Falcon-7B-instruct66.8abc 38.2abef 33.4abcd 50.7abcefg
j BM25 -> Falcon-40B-instruct70.2abc 40.5abcefgiklp 40.8abcdefghi 51.3abcefg
k BM25 -> T0-3B 71.6abc 38.8abefg 41.4abcdefghi 50.1abcefg
l BM25 -> T0-11B 73.9abcefijop 38.7abefg 43.8abcdefghijkmopq 49.7abcef
m BM25 -> FlanT5-3B 71.1abc 39.7abcefgip 41.2abcdefghi 50.0abcefg
n BM25 -> FlanT5-11B 74.9abcdefijkmop 41.7abcefghiklmop 43.3abcdefghijkmopq 52.4abcdefgiklm
o BM25 -> LLaMA-7B 69.4abc 39.9abcefgip 41.5abcdefghi 53.6abcdefghijklm
p BM25 -> LLaMA-13B 69.8abc 37.6abef 41.8abcdefghi 54.2abcdefghijklmn
q BM25 -> Falcon-7B 73.3abcefiop 41.4abcefghiklmop 41.2abcdefghi 52.5abcdefgiklm
r BM25 -> Falcon-40B 75.2abcdefijkmop 41.0abcefghiklop 43.1abcdefghijkmopq 53.1abcdefghijklm
s BM25 -> monoT5-3B 79.8abcdefghijklmopqv 44.8abcdefghijklmnopqr46.0abcdefghijklmnopqr56.2abcdefghijklmnopqr
t BM25 -> monoT5-3B-InPars-v283.7abcdefghijklmnopqrsuvwxyz46.5abcdefghijklmnopqrs46.1abcdefghijklmnopqr58.5abcdefghijklmnopqrsw
u BM25+HyDE -> FlanT5-11B75.8abcdefijkmop 46.2abcdefghijklmnopqrx49.5abcdefghijklmnopqrstvw56.6abcdefghijklmnoqr
v BM25+HyDE -> LLaMA-7B72.4abc 45.4abcdefghijklmnopqr46.8abcdefghijklmnopqr57.4abcdefghijklmnopqrw
w BM25+HyDE -> Falcon-7B76.6abcdefijkmopv 46.1abcdefghijklmnopqr45.8abcdefghijklmnopqr55.1abcdefghijklmq
x BM25+HyDE -> FlanT5-11B-fewshot77.2abcdefhijkmopuv 45.1abcdefghijklmnopqr49.7abcdefghijklmnopqrstvw58.3abcdefghijklmnopqruw
y BM25+HyDE -> LLaMA-7B-fewshot77.8abcdefhijkmopv 47.7abcdefghijklmnopqrsuvwx50.4abcdefghijklmnopqrstvwz59.5abcdefghijklmnopqrsuvw
z BM25+HyDE -> Falcon-7B-fewshot78.6abcdefghijkmopqv 48.0abcdefghijklmnopqrsuvwx48.6abcdefghijklmnopqrstvw59.0abcdefghijklmnopqrsuvw
that is commonly used as the zero-shot first-
stage retrieval method. We use the Pyserini
“two-click reproductions” (Ma et al., 2022) to
produce the BM25 results on BEIR datasets.
• QLM-Dirichlet (Zhai and Lafferty, 2001):
The traditional QLM method that exploits
term statistics and Dirichlet smoothing tech-
nique to estimate query likelihood, we also
use Pyserini implementation for this baseline.
• Contriever (Izacard et al., 2022): A zero-shot
dense retriever that pre-trained on text para-
graphs with unsupervised contrastive learning.
• HyDE (Gao et al., 2022): A two-step zero-
shot first-stage retriever that leverages gen-
erative LLMs and Contriever. In the first
step, a prompt is provided to a LLM to gener-
ate multiple documents relevant to the given
query. Subsequently, in the second step,
the generated documents are encoded into
vectors using the Contriever query encoder
and then aggregated to form a new query
vector for the search process. We utilized
the open-sourced implementation provided
by the original authors for our experiments:
https://github.com/texttron/hyde.
• Contriver-msmarco. A Contriever check-
point further pre-trained on MS MARCO
training data. We use the Pyserini provided
pre-build dense vector index and model check-
point for this baseline.
• SPLADE-distill (Formal et al., 2022): A
first-stage sparse retrieval model that exploits
BERT PLM to learn query/document sparse
term expansion and weights. We use the Py-
serini provided pre-build index and SPLADE
checkpoint to produce the results.
• DRAGON+ (Lin et al., 2023): A dense re-
triever model that fine-tuned on augmented
MS MARCO corpus and uses multiple retriev-
ers to conduct automatical relevance labeling.
It stands as the current state-of-the-art dense
retriever in the transfer learning setting. We
use the scores reported on the BEIR learder-
board 6 for this baseline.
• T5-QLM-large (Zhuang et al., 2021): A
T5-based QLM method that fine-tuned on
MS MARCO QG training data. We use the
implement this method with open-sourced
docTquery-T5 (Nogueira and Lin, 2019)
checkpoint 7.
• monoT5-3B (Nogueira et al., 2020). A T5-
based cross-encoder re-ranker that fine-tuned
on MS MARCO training data. We use the
6https://eval.ai/web/challenges/
challenge-page/1897/leaderboard/4475
7castorini/doc2query-t5-large-msmarco



### Claim 146/179

#### Claim Text
Detailed information about the fabrication technology of the investigated device can be found in .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[93]_2309.11495.pdf (Page 15):

8.5 F ACTOR +REVISE : I DENTIFY WHICH FACTS ARE CONSISTENT
Context: <Original Fact>.
From another source,
<output of execute verification step: Q + A>
Response: CONSISTENT. <Consistent fact>
Context: <Original Fact>.
From another source,
<output of execute verification step: Q + A>
Response: INCONSISTENT.
Context: <Original Fact>.
From another source,
<output of execute verification step: Q + A>
Response: PARTIALLY CONSISTENT. <Consistent part>
Table 9: In the CoVe (Factor + Revise) variant, as part of step (3) after subsection 8.3, the model is
made to explicitly identify which facts are consistent between the two sources. The consistent facts
can then be spliced together.
16



Source: data\tc16_2312.10997v5\referenced_papers\[75]_2305.04757.pdf (Page 5):

Table 1: Evaluating on three different tasks, requiring factual (FM2), tabular (NQ-Table), and medical
(MedMC-QA) knowledge. ⋄: we ﬁne-tune the dense retrieval models with the task data. †: we
use InstructGPT3.5 to generate the chain-of-thoughts as the background knowledge. ‡: we use
InstructGPT3.5 to generate the background documents.
Models FM2 NQ-Table MedMC-QA
Direct generation without guiding.
InstructGPT3.5 [Ouyang et al., 2022] 59.4 16.9 44.4
Generation with retrieval guiding.
BM25 + InstructGPT3.5 [Karpukhin et al., 2020] 65.2 17.1 -
⋄REPLUG + InstructGPT3.5 [Shi et al., 2023] 65.9 24.3 -
Generation with self-guiding.
†CoT + InstructGPT3.5 [Kojima et al., 2022] 60.4 21.4 41.5
‡GenRead + InstructGPT3.5 [Yu et al., 2023] 65.5 23.5 44.4
PKG + InstructGPT3.5 (Ours) 67.3 28.8 47.4
Table 2: Evaluating on the ScienceQA, requiring multimodal science knowledge. †: results from Lu
et al. [2023]. gpt-3.5-turbo is much more capable than text-davinic-002.
Models NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg
Base on gpt-3.5-turbo.
†ChatGPT 78.82 70.98 83.18 77.37 67.92 86.13 80.72 74.03 78.31
†Chameleon 81.62 70.64 84.00 79.77 70.80 86.62 81.86 76.53 79.93
Base on text-davinic-002.
InstructGPT3.5 72.96 62.88 76.09 70.77 62.77 77.84 75.04 65.59 71.66
+CoT 71.94 61.19 74.00 69.50 61.18 75.75 72.61 65.92 70.22
+GenRead 72.91 64.68 76.36 72.14 63.31 76.66 74.96 66.91 72.08
+PKG (Ours) 79.35 82.90 81.91 79.86 74.32 83.41 80.80 80.69 80.76
step-by-step" to generate the chain-of-thought as the background knowledge. The second method,
GenRead Yu et al. [2023], directly requires the InstructGPTs to provide task-speciﬁc knowledge with
the prompt "Please provide the background document from [domain] to [task]."
4.2 Factual Knowledge
Datasets and Implementation Details.We evaluate our approach on the FM2 dataset [Eisenschlos
et al., 2021], which is a benchmark for fact-checking. In this task, given a factual claim, our models
are required to determine whether it is true or false. We use the claim in the training set and the
corresponding evidence as factual knowledge. Additionally, we sample 100k passages from English
Wikipedia, each consisting of up to 256 tokens. We treat the ﬁrst sentence as the input and the
remaining sentences as background knowledge. Accuracy is adopted as the evaluation metric. More
details can be found in Appendix A and B.
Results. As shown in Table 1, our PKG outperforms all the baseline systems for fact-checking. In
comparison to direct generation, the results reveal that it is necessary to provide extra background
knowledge for InstructGPTs with retrieval-based or generation-based methods. Speciﬁcally, our PKG
outperforms InstructGPT3.5 by 7.9% (67.9% vs. 59.4%), and outperforms REPLUG, a retrieval-
based method, by 1.4% (67.3% vs. 65.9%). It is noteworthy that our generation-based method does
not necessitate an additional knowledge database as the retrieval-based methods. Additionally, our
PKG performs better than the self-guiding method GenRead by 1.8% (67.3% vs. 65.5%), indicating
that our PKG can provide more useful information than the InstructGPTs themselves.
6



Source: data\tc16_2312.10997v5\referenced_papers\[173]_2403.10131.pdf (Page 5):

Preprint, Under Review
like to highlight that (NQ, Trivia QA, and HotpotQA) are relatively general domain whereas
the latter two domains are on domain-specific documents.
Baselines We consider the following baselines for our experiments:
• LlaMA2-7B-chat model with 0-shot prompting: this is the commonly used
instruction-finetuned model for QA tasks, where we provide clearly written instruc-
tions, but no reference documentation.
• LlaMA2-7B-chat model with RAG (Llama2 + RAG): similar to the previous setting,
except here we include reference documents. This is a popular technique when
dealing with domain-specific QA tasks.
• Domain-Specific Finetuning with 0-shot prompting (DSF): Standard supervised-
finetuning, without documents in context. We find that its mostly useful to align
the answering style of the model as well as get familiar with the domain context.
• Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific
finetuned-model with external knowledge using RAG. So, for the “knowledge” the
model does not know, it can still refer to the context.
4.1 Results
Using the above datasets and baselines, we evaluate our model RAFT and demonstrate
the effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly
outperforms the baselines. Compared with the base Llama-2 instruction-tuned model,
RAFT with RAG does much better in terms of extracting information as well as being
robust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on
Torch Hub evaluation. Compared with DSF on the specific dataset, our model does better at
relying on the provided context to solve the problem. RAFT does much better on the tasks
like Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).
Note that for PubMed QA, since it is a binary yes/no question, we don’t observe significant
gains when we compare our model with DSF + RAG. Even compared with a much larger
and better model GPT-3.5, RAFT demonstrates significant advantages.
Overall, the LLaMA-7B model, both with and without the RAG, performs poorly due to its
answering style not aligning with the ground truth. By applying domain-specific tuning,
we significantly enhance its performance. This process enables the model to learn and adopt
the appropriate style of answering. However, introducing RAG to a domain-specifically
fine-tuned (DSF) model doesn’t invariably lead to better outcomes. This might indicate that
the model lacks training in context processing and extracting useful information from it. By
incorporating our method, RAFT , we train the model not only to match its answering style
with that required but also to improve its document processing capabilities. Consequently,
our approach outperforms all others.
4.2 Effect of CoT
We also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach
in enhancing the model’s performance. As indicated in Table 2, simply providing the answer
to a question may not always be adequate. This approach can lead to a rapid decrease
in loss, resulting in the model beginning to overfit. Incorporating a reasoning chain that
not only guides the model to the answer but also enriches the model’s understanding can
improve the overall accuracy and prevent overfitting to concise answers. In our experiments,
integrating the Chain-of-Thought significantly enhances training robustness. We employ
GPT-4-1106 to generate our Chain-of-Thought prompts and include an example of the
prompt we used in Figure 3.
4.3 Qualitative Analysis
To illustrate the potential advantages of RAFT over the domain-specifically fine-tuned
(DSF) approach, we present a comparative example in Figure 4. This example qualitatively
6



Source: data\tc16_2312.10997v5\referenced_papers\[50]_2311.09210.pdf (Page 7):

34.3
54.361.464.667.269.2
41.8
56.663.465.970.073.3
36.3
55.863.665.270.173.1
304050607080
100%80%60%40%20%0%
EMScore
NQ
Standard RALMRALM with CoNRALM with CoN (hybrid)
Figure 3: Using a hybrid training strategy demonstrates
slightly lower robustness across various noise ratios but
consistently better performance than standard RALMs.
Models↓ Inference Time(s)
Retrieve-Read 0.6104
+ CHAIN-OF-NOTE 12.0192
+ CHAIN-OF-NOTE (hybrid) 0.6074
Table 5: The inference time comparison shows the aver-
age decoding time per example on 8×A100 GPUs.
questions / total questions. This highlights our
model’s enhanced capability to discern and disre-
gard information that is unfamiliar or not learned
during its initial training phase.
3.5 Evaluation on Hybrid Training Strategy
As illustrated in Figure 3 and Table 5, our proposed
RALM equipped with a hybrid strategy demon-
strates slightly lower robustness across various
noise ratios while but keeping similar effcient de-
coding time consumption to the standard RALM.
This indicates that our CHAIN -OF-NOTE frame-
work, when implemented with a hybrid training
strategy, is highly applicable to a wide range of
real-world business scenarios. This enhancement
in robustness without significant time overhead
highlights the practical value and efficiency of our
approach, making it a viable solution for environ-
ments where QA accuracy can vary but inference
time is crucial.
4 Related Work
Retrieval-Augmented Language Models (RALMs)
represent a significant advancement in natural lan-
guage processing, combining the power of large
language models with the specificity and detail pro-
vided by external knowledge sources (Guu et al.,
2020; Lewis et al., 2020; Izacard et al., 2022). Re-
cent studies highlight the impact of context rele-
vance on language model performance (Creswell
et al., 2022; Shi et al., 2023a; Yoran et al., 2023).
Notably, Creswell et al. (2022) demonstrated that
incorporating random or irrelevant contexts could
adversely affect QA performance. In contrast, Shi
et al. (2023a) discovered that adding irrelevant con-
text to exemplars or task-specific instructions can
sometimes enhance model performance, implying
that models might intrinsically possess capabilities,
developed during pre-training, to manage such sce-
narios. Most pertinent to our research is the study
by Yoran et al. (2023), which focused on training
RALMs to disregard irrelevant contexts. This ap-
proach, while distinct from our proposed solution,
underscores the importance of context relevance in
enhancing the effectiveness of RALMs.
Besides, we present more related Chain-of- Xs
approaches (e.g., Chain-of-Thought (CoT) (Wei
et al., 2022)) in the Appendix A.1.1 and A.2.
5 Conclusion
In this paper, we introduce the CHAIN -OF-NOTE
(CON) framework, a novel methodology designed
to enhance the robustness of RALMs. The central
concept of CON revolves around the generation of
sequential reading notes for each retrieved docu-
ment. This process allows for an in-depth assess-
ment of document relevance to the posed question
and aids in synthesizing this information to craft
the final answer. Our experiments show that GPT-4,
when equipped with CON, outperforms the CHAIN -
OF-THOUGHT approach. Besides, we utilized GPT-
4 to create 10K CON data, subsequently trained on
a LLaMa-2 7B model. Our experiments across four
open-domain QA benchmarks show that RALMs
equipped with CON significantly outperform stan-
dard fine-tuned RALMs.
6 Limitations
One major limitation of the CHAIN -OF-NOTE
(CON) approach is its increased inference cost
due to the sequential generation of notes. While
CON is beneficial for assessing the relevance and
integrating external knowledge, it results in longer
response times, which is problematic for time-
sensitive applications. Moreover, the system’s effi-
ciency depends on the conciseness and relevance
of the generated notes, which can fluctuate based
on the complexity of the retrieved documents.



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 12):

User Input Introduce the film “The Only Thing” (Four Flaming Days) to me.
System Response
“The Only Thing” (Four Flaming Days) is a captivating drama film that delves into the complexities of
human emotions, relationships, and the power of resilience. Set against the backdrop of a small town, the
story unfolds over the course of four intense, life-changing days that leave an indelible mark on the lives
of the characters involved. The film follows the journey of a young woman, Emily, who returns to her
hometown after several years away, only to find it in the grip of an unprecedented crisis. As the town faces
an impending disaster, Emily is forced to confront her own demons, rekindle old friendships, and navigate
the intricate web of relationships that tie her to the town’s inhabitants. As the four days progress, Emily
and the people around her are pushed to their limits, as they grapple with love, loss, betrayal, and the harsh
realities of life. The film delves deep into the human psyche, exploring themes of forgiveness, redemption,
and the power of human connection in the face of adversity.
User Input What genre of movie is “The Only Thing” (Four Flaming Days)?
System ResponseI’m sorry. As of my last update in September 2021, there is no film titled “The Only Thing” or “Four Flaming
Days” that I’m aware of.
Table 9: A real example of the over-conservative phenomenon of ChatGPT (July 2023 Version). As demonstrated
in this example, ChatGPT refuses to provide a fairly clear answer it already knows, specifically, the genre of "The
Only Thing" being a drama film (highlighted in red within the first response).
and LLM knowledge boundaries. In light of this,
Schulman (2023) propose to solve this problem
during RLHF. They design a special reward func-
tion just for mitigating hallucinations, as shown
in Table 8. “Unhedged/Hedged Correct/Wrong”
here means the LLM provides correct or wrong
answers with a positive or hesitant tone. “Unin-
formative” denote the safe answers like “I don’t
know”. The core idea is to encourage LLMs to
challenge the premise, express uncertainty, and
commit incapability by learning from specially de-
signed rewards. This method, which we refer to
as honesty-oriented RL, offers several advantages
over honesty-oriented SFT. The primary benefit is
that it allows LLMs to freely explore their knowl-
edge boundaries, thereby enhancing their general-
ization capabilities to OOD cases. Additionally, it
reduces the need for extensive human annotation
and eliminates the requirement for annotators to
guess the knowledge boundaries of LLMs.
Summary & Discussion. Reinforcement learn-
ing can guide LLMs in exploring their knowl-
edge boundaries, enabling them to decline to an-
swer questions beyond their capacity rather than
fabricating untruthful responses. However, we
note this approach also poses unique challenges.
For instance, RL-tuned LLMs may exhibit over-
conservatism due to an imbalanced trade-off be-
tween helpfulness and honesty (Ouyang et al.,
2022). An example of this is illustrated in Ta-
ble 9. As observed in this case, ChatGPT tends
to be overly hedged and refrains from providing a
clear answer that it already knows, as evidenced
in another dialogue turn. This could be attributed
to the unreasonable design of the reward function
or the poor quality of the training data for the re-
ward model. We hope future work can take such
problems into consideration.
5.4 Mitigation during Inference
Compared with the aforementioned training-time
mitigation approaches, mitigating hallucinations
in the inference time could be more cost-effective
and controllable. Therefore, most existing studies
focus on this direction, which we will introduce in
detail in the following sections.
5.4.1 Designing Decoding Strategies
Decoding strategies, such as greedy decoding and
beam search decoding, determine how we choose
output tokens from the probability distribution
generated by models (Zarrieß et al., 2021).
Lee et al. (2022) carry out a factuality assess-
ment of content generated by LLMs using differ-
ent decoding strategies. They find that nucleus
sampling (a.k.a top-p sampling) (Holtzman et al.,
2019) falls short of greedy decoding in terms of
factuality. They argue that this underperformance
could be attributed to the randomness introduced
by top-p sampling to boost diversity, which may
inadvertently lead to hallucinations since LLMs
tend to fabricate information to generate diverse
responses. In view of this, they introduce a decod-
ing algorithm termed factual-nucleus sampling ,
which aims to strike a more effective balance be-
tween diversity and factuality by leveraging the
strengths of both top-p and greedy decoding.
Dhuliawala et al. (2023) develop a decoding
framework known as the Chain-of-Verification
(COVE). This framework is based on the obser-
vation that independent verification questions typ-
13



### Claim 147/179

#### Claim Text
Additional forces acting on the particles, beyond those which arise due to direct solid-solid contact or due to a failure to resolve lubrication (cf. chapter 11), can be integrated into the PR-DNS framework in a relatively straightforward manner, e.g. short-range cohesive forces and electrical charges .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 10):

106 107 108 109
Params (non-embed)
2.5
3.0
3.5
4.0
4.5Test Loss
Data Size Bottleneck
Data Size
21M
43M
86M
172M
344M
688M
1.4B
22.0B
10 4
 10 3
 10 2
 10 1
N N/ D/D
0.0
0.1
0.2
0.3
0.4
0.5L/L(D = ) 1
Overfitting
Data Size
21M
43M
86M
172M
344M
688M
1.4B
22.0B
Figure 9 The early-stopped test loss L(N,D) depends predictably on the dataset size Dand model size N
according to Equation (1.5). Left: For large D, performance is a straight power law inN. For a smaller ﬁxed
D, performance stops improving as N increases and the model begins to overﬁt. (The reverse is also true,
see Figure 4.) Right: The extent of overﬁtting depends predominantly on the ratio N
αN
αD /D, as predicted in
equation (4.3). The line is our ﬁt to that equation.
Since we stop training early when the test loss ceases to improve and optimize all models in the same way, we
expect that larger models should always perform better than smaller models. But with ﬁxed ﬁnite D, we also
do not expect any model to be capable of approaching the best possible loss (ie the entropy of text). Similarly,
a model with ﬁxed size will be capacity-limited. These considerations motivate our second principle. Note
that knowledge of L(N) at inﬁnite Dand L(D) at inﬁnite N fully determines all the parameters in L(N,D).
The third principle is more speculative. There is a simple and general reason one might expect overﬁtting
to scale ∝1/D at very large D. Overﬁtting should be related to the variance or the signal-to-noise ratio
of the dataset [AS17], and this scales as 1/D. This expectation should hold for any smooth loss function,
since we expect to be able to expand the loss about the D →∞ limit. However, this argument assumes that
1/Dcorrections dominate over other sources of variance, such as the ﬁnite batch size and other limits on the
efﬁcacy of optimization. Without empirical conﬁrmation, we would not be very conﬁdent of its applicability.
Our third principle explains the asymmetry between the roles of N and D in Equation (1.5). Very similar
symmetric expressions4 are possible, but they would not have a 1/D expansion with integer powers, and
would require the introduction of an additional parameter.
In any case, we will see that our equation for L(N,D) ﬁts the data well, which is the most important justiﬁ-
cation for our L(N,D) ansatz.
4.2 Results
We regularize all our models with 10% dropout, and by tracking test loss and stopping once it is no longer
decreasing. The results are displayed in Figure 9, including a ﬁt to the four parameters αN,αD,Nc,Dc in
Equation (1.5):
Parameter αN αD Nc Dc
Value 0.076 0.103 6.4 ×1013 1.8 ×1013
Table 2 Fits to L(N,D)
We obtain an excellent ﬁt, with the exception of the runs where the dataset has been reduced by a factor of
1024, to about 2 ×107 tokens. With such a small dataset, an epoch consists of only 40 parameter updates.
Perhaps such a tiny dataset represents a different regime for language modeling, as overﬁtting happens very
early in training (see Figure 16). Also note that the parameters differ very slightly from those obtained in
Section 3, as here we are ﬁtting the full L(N,D) rather than just L(N,∞) or L(∞,D).
To chart the borderlands of the inﬁnite data limit, we can directly study the extent of overﬁtting. For all but
the largest models, we see no sign of overﬁtting when training with the full 22B token WebText2 dataset,
so we can take it as representative of D = ∞. Thus we can compare ﬁnite D to the inﬁnite data limit by
4For example, one might have used L(N, D) =
[( Nc
N
)αN
+
( Dc
D
)αD]β
, but this does not have a 1/D expansion.
11



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 19):

Appendices
A Summary of Power Laws
For easier reference, we provide a summary below of the key trends described throughout the paper.
Parameters Data Compute Batch Size Equation
N ∞ ∞ Fixed L(N) = (Nc/N)αN
∞ D Early Stop Fixed L(D) = (Dc/D)αD
Optimal ∞ C Fixed L(C) = (Cc/C)αC
(naive)
Nopt Dopt Cmin B ≪Bcrit L(Cmin) =
(
Cmin
c /Cmin
)αmin
C
N D Early Stop Fixed L(N,D) =
[(Nc
N
)αN
αD + Dc
D
]αD
N ∞ Ssteps B L(N,S) =
(Nc
N
)αN
+
(
Sc
Smin(S,B)
)αS
Table 4
The empirical ﬁtted values for these trends are:
Power Law Scale (tokenization-dependent)
αN = 0.076 Nc = 8.8 ×1013 params (non-embed)
αD = 0.095 Dc = 5.4 ×1013 tokens
αC = 0.057 Cc = 1.6 ×107 PF-days
αmin
C = 0.050 Cmin
c = 3.1 ×108 PF-days
αB = 0.21 B∗= 2.1 ×108 tokens
αS = 0.76 Sc = 2.1 ×103 steps
Table 5
The optimal parameters for compute efﬁcient training are given by:
Compute-Efﬁcient Value Power Law Scale
Nopt = Ne ·CpN
min pN = 0.73 Ne = 1.3 ·109 params
B ≪Bcrit = B∗
L1/αB = BeCpB
min pB = 0.24 Be = 2.0 ·106 tokens
Smin = Se ·CpS
min (lower bound) pS = 0.03 Se = 5.4 ·103 steps
Dopt = De ·CpD
min (1 epoch) pD = 0.27 De = 2·1010 tokens
Table 6
B Empirical Model of Compute-Efﬁcient Frontier
Throughout this appendix all values of C,S, and αC are adjusted for training at the critical batch size Bcrit.
We have left off the ‘adj’ label to avoid cluttering the notation.
B.1 Deﬁning Equations
The power-law ﬁt to the learning curves implies a simple prescription for compute-efﬁcient training. In this
appendix, we will derive the optimal performance, model size, and number of training steps as a function of
20



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 9):

104 105 106 107 108 109
Parameters (non-embedding)
3
4
5
6
7Test Loss
WebText2 (Test)
Internet Books
Books
Wikipedia
Common Crawl
2.53.03.54.04.55.0
Test Loss on Training Distribution
2.5
3.0
3.5
4.0
4.5
5.0Loss on Other Distribution
Books during training
Wikipedia during training
Books at convergence
Wikipedia at convergence
Figure 8 Left: Generalization performance to other data distributions improves smoothly with model size,
with only a small and very slowly growing offset from the WebText2 training distribution. Right: Gener-
alization performance depends only on training distribution performance, and not on the phase of training.
We compare generalization of converged models (points) to that of a single large model (dashed curves) as it
trains.
with the best performance on step S = C
6BS. Note that in these results the batch size B remains ﬁxed for
all models, which means that these empirical results are not truly optimal. We will account for this in later
sections using an adjusted Cmin to produce cleaner trends.
The result appears as the heavy black line on the left-hand plot in Figure 1. It can be ﬁt with
L(C) ≈
(Cc
C
)αC
(3.3)
The ﬁgure also includes images of individual learning curves to clarify when individual models are optimal.
We will study the optimal allocation of compute more closely later on. The data strongly suggests that sample
efﬁciency improves with model size, and we also illustrate this directly in Figure 19 in the appendix.
4 Charting the Inﬁnite Data Limit and Overﬁtting
In Section 3 we found a number of basic scaling laws for language modeling performance. Here we will
study the performance of a model of size N trained on a dataset with D tokens while varying N and D
simultaneously. We will empirically demonstrate that the optimally trained test loss accords with the scaling
law of Equation (1.5). This provides guidance on how much data we would need to train models of increasing
size while keeping overﬁtting under control.
4.1 Proposed L(N,D) Equation
We have chosen the parameterization (1.5) (repeated here for convenience):
L(N,D) =
[(Nc
N
)αN
αD
+ Dc
D
]αD
(4.1)
using three principles:
1. Changes in vocabulary size or tokenization are expected to rescale the loss by an overall factor. The
parameterization of L(N,D) (and all models of the loss) must naturally allow for such a rescaling.
2. Fixing Dand sending N →∞, the overall loss should approach L(D). Conversely, ﬁxing N and
sending D→∞ the loss must approach L(N).
3. L(N,D) should be analytic at D= ∞, so that it has a series expansion in1/Dwith integer powers.
Theoretical support for this principle is signiﬁcantly weaker than for the ﬁrst two.
Our choice of L(N,D) satisﬁes the ﬁrst requirement because we can rescale Nc,Dc with changes in the
vocabulary. This also implies that the values of Nc,Dc have no fundamental meaning.
10



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 21):

B.3 Comparison to Inefﬁcient
Typically, researchers train models until they appear to be close to convergence. In this section, we compare
the efﬁcient training procedure described above to this more typical setup. We deﬁne a the convergence factor
f as the percent deviation from the converged loss:
L(N,C) = (1 +f) L(N,∞) . (B.11)
For compute-efﬁcient training we have f = αN/αS ≈ 10% from the previous section, but researchers
typically use a much smaller value. Here, we choose f′= 2%as an estimate. For a ﬁxed value of the loss,
we predict:
Nf
Nf′
=
(1 +f
1 +f′
)1/αN
≈2.7 (B.12)
Sf
Sf′
=
(
1 +1
f
1 + 1
f′
)1/αS
≈0.13 (B.13)
Cf
Cf′
= Nf
Nf′
Sf
Sf′
≈0.35 (B.14)
So that compute-efﬁcient training uses 7.7x fewer parameter updates, 2.7x more parameters, and 65% less
compute to reach the same loss.
B.4 Suboptimal Model Sizes
We can solve A.1 to ﬁnd an expression for the amount of compute needed to reach a given value of the loss
Lwith a model of size N:
C(N,L) =
(
6B∗Sc
N
L1/αB
)(
L−
(Nc
N
)αN)−1/αS
. (B.15)
Using A.6 and A.9, we can eliminate Lin favor of Neﬀ (L), the model size which reaches Lmost efﬁciently.
From there, we ﬁnd an expression for the excess compute needed as a consequence of using a suboptimal
model size:
C(N,Neﬀ)
C(Neﬀ,Neﬀ) = N
Neﬀ
[
1 +αS
αN
(
1 −
(Neﬀ
N
)αN)]−1/αS
. (B.16)
The result is shown in Figure X. Models between 0.6x and 2.2x the optimal size can be used with only a
20% increase in compute budget. Using a smaller model is useful when accounting for the cost inference. A
larger model can be trained the the same level of performance in fewer steps, allowing for more parallelism
and faster training if sufﬁcient harware is available (see Figure Y):
S(N,Neﬀ)
S(Neﬀ,Neﬀ) =
[
1 +αS
αN
(
1 −
(Neﬀ
N
)αN)]−1/αS
. (B.17)
A 2.2x larger model requires 45% fewer steps at a cost of 20% more training compute. Note that this equation
should not be trusted for very large models, as it is only valid in the power-law region of the learning curve
after initial transient effects.
C Caveats
In this section we list some potential caveats to our analysis.
• At present we do not have a solid theoretical understanding for any of our proposed scaling laws.
The scaling relations with model size and compute are especially mysterious. It may be possible to
understand scaling at very large Dholding model size ﬁxed [AS17], and also the shape of learning
curves late in training, by modeling the loss with a noisy quadratic. But the scaling with Dat very
large model size still remains mysterious. Without a theory or a systematic understanding of the
corrections to our scaling laws, it’s difﬁcult to determine in what circumstances they can be trusted.
22



Source: data\tc16_2312.10997v5\referenced_papers\[102]_2004.04906.pdf (Page 5):

Sample efﬁciency We explore how many train-
ing examples are needed to achieve good passage
retrieval performance. Figure 1 illustrates the top-k
retrieval accuracy with respect to different num-
bers of training examples, measured on the devel-
opment set of Natural Questions. As is shown, a
dense passage retriever trained using only 1,000 ex-
amples already outperforms BM25. This suggests
that with a general pretrained language model, it is
possible to train a high-quality dense retriever with
a small number of question–passage pairs. Adding
more training examples (from 1k to 59k) further
improves the retrieval accuracy consistently.
In-batch negative training We test different
training schemes on the development set of Natural
Questions and summarize the results in Table 3.
The top block is the standard 1-of-N training set-
ting, where each question in the batch is paired
with a positive passage and its own set of nneg-
ative passages (Eq. (2)). We ﬁnd that the choice
of negatives — random, BM25 or gold passages
(positive passages from other questions) — does
not impact the top-kaccuracy much in this setting
when k≥20.
The middle bock is the in-batch negative training
(Section 3.2) setting. We ﬁnd that using a similar
conﬁguration (7 gold negative passages), in-batch
negative training improves the results substantially.
The key difference between the two is whether the
gold negative passages come from the same batch
or from the whole training set. Effectively, in-batch
negative training is an easy and memory-efﬁcient
way to reuse the negative examples already in the
batch rather than creating new ones. It produces
more pairs and thus increases the number of train-
ing examples, which might contribute to the good
model performance. As a result, accuracy consis-
tently improves as the batch size grows.
Finally, we explore in-batch negative training
with additional “hard” negative passages that have
high BM25 scores given the question, but do not
contain the answer string (the bottom block). These
additional passages are used as negative passages
for all questions in the same batch. We ﬁnd that
adding a single BM25 negative passage improves
the result substantially while adding two does not
help further.
Impact of gold passages We use passages that
match the gold contexts in the original datasets
(when available) as positive examples (Section 4.2).
Type #N IB Top-5 Top-20 Top-100
Random 7  47.0 64.3 77.8
BM25 7  50.0 63.3 74.8
Gold 7  42.6 63.1 78.3
Gold 7  51.1 69.1 80.8
Gold 31  52.1 70.8 82.1
Gold 127  55.8 73.0 83.1
G.+BM25(1) 31+32  65.0 77.3 84.4
G.+BM25(2) 31+64  64.5 76.4 84.0
G.+BM25(1) 127+128  65.8 78.0 84.9
Table 3: Comparison of different training schemes,
measured as top-kretrieval accuracy on Natural Ques-
tions (development set). #N: number of negative
examples, IB: in-batch training. G.+BM25 (1) and
G.+BM25(2) denote in-batch training with 1 or 2 ad-
ditional BM25 negatives, which serve as negative pas-
sages for all questions in the batch.
Our experiments on Natural Questions show that
switching to distantly-supervised passages (using
the highest-ranked BM25 passage that contains the
answer), has only a small impact: 1 point lower
top-kaccuracy for retrieval. Appendix A contains
more details.
Similarity and loss Besides dot product, cosine
and Euclidean L2 distance are also commonly used
as decomposable similarity functions. We test these
alternatives and ﬁnd that L2 performs compara-
ble to dot product, and both of them are superior
to cosine. Similarly, in addition to negative log-
likelihood, a popular option for ranking is triplet
loss, which compares a positive passage and a nega-
tive one directly with respect to a question (Burges
et al., 2005). Our experiments show that using
triplet loss does not affect the results much. More
details can be found in Appendix B.
Cross-dataset generalization One interesting
question regarding DPR’s discriminative training
is how much performance degradation it may suf-
fer from a non-iid setting. In other words, can
it still generalize well when directly applied to
a different dataset without additional ﬁne-tuning?
To test the cross-dataset generalization, we train
DPR on Natural Questions only and test it directly
on the smaller WebQuestions and CuratedTREC
datasets. We ﬁnd that DPR generalizes well, with
3-5 points loss from the best performing ﬁne-tuned
model in top-20 retrieval accuracy (69.9/86.3 vs.
75.0/89.1 for WebQuestions and TREC, respec-
tively), while still greatly outperforming the BM25
baseline (55.0/70.9).



### Claim 148/179

#### Claim Text
Nevertheless, Peng et al. show that, when a sufficient grid resolution is used, the two methods are able to provide accurate results for most of turbulent statistics in both the carrier and dispersed phases in turbulent particle-laden flow simulations.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 13):

0 1000 2000 3000 4000
24.5
25
25.5
26
26.5
27
FLOPS (MACs)
Rouge-L
2%
(a) ELI5
0 1000 2000 3000 4000
20
21
22
23
24
25
FLOPS (MACs)
2% (b) MS MARCO
0 1000 2000 3000 4000
24
25
26
27
28
29
30
31
32
FLOPS (MACs)
2% (c) NQ
0 20 40 60 80 100
24.5
25
25.5
26
26.5
27
Input P sgs
Rouge-L
2%
(d) ELI5
0 20 40 60 80 100
20
21
22
23
24
25
Input P sgs
2% (e) MS MARCO
0 20 40 60 80 100
24
25
26
27
28
29
30
31
32
FiD
CALM
T ok en Filtering
Combined
Input P sgs
2% (f) NQ
Figure 6: The ROUGE-L performance on FiD-Base vs. the FLOPS (MACs) (First row), and vs. the input passage
amount (Input Psg., second row). The results are on the test set for each dataset, for the various methods utilized.
We observe that the trends of the FLOPS (MACs) and the Input Psg. are very similar, since the passages effect the
encoder the most, and the encoder has the most impact on FLOPS (MACs) (as stated by de Jong et al. (2022)).



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 5):

passages. For all the evaluated datasets, we retrieve
100 passages for each question from the index, us-
ing a combination of dense and sparse passage
rankers. We refer the reader to Appendix C for
more details regarding the retrieval process.
Hardware We used 824GB NVIDIA RTX3090for
training base-sized models, and 840GB A100GPUs
for training large-sized models. For inference and
latency measurements we used a single accelerator.
Inference setup Throughout our latency mea-
surements, we used a batch size of 1 and averaged
the latency over all queries. Decoding is done us-
ing beam-search with 4 beams, and similarly as (Su
et al., 2022) we limit the generated answer length
to 300 tokens. We also control the minimal answer
length per dataset, which we specify in Table 4 in
the Appendix.
5.4 Performance vs. Efficiency Evaluation
Process
We use KILT’s implementation of ROUGE-L and
F1 for performance measurements6. We measure
efficiency as end-to-end latency in seconds for
generating an answer to a question. To evaluate
each method, on each dataset, we focus on the per-
formance vs. efficiency trade-off, particularly on
ROUGE-L vs. latency. For the evaluation of a
given method (for example Token Filtering), we
perform a hyperparameter search over multiple
combinations on the development set, and end up
with a collection of 2-dimensional points, where
the x-axis is the latency, and the y-axis is the per-
formance (ROUGE-L). Each latency-performance
measurement is averaged across all questions in the
development set. Then, we take the maximum and
minimum over the observed values of the x-axis,
and divide the resulting range into equally-sized
intervals. In our experiments, we use 30 distinct in-
tervals. For each interval, we find it’s representative
point, by taking the point with the maximum y-axis
value from all the combinations in interval. Once
all such points are determined per interval, they
form a curve, which we name as the Max Curve of
the method. We visualize the process in Figure 4,
where the line in blue is the Max Curve.
Thus, for a method to be better than another, the
Max Curve for it should be above and to the left
of the curve of the other, meaning that it reaches
equivalent results for less resources. Using the
6https://github.com/facebookresearch/KILT
y 1 
y 2 y 3 y 4 
y 5 
x 1 x 2 x 3 x 4 x 5 x 6 
Performance
Smoothing
Latency
Figure 4: For Performance (y-axis) vs. Latency (x-axis),
we divide the x-axis into 5 intervals over all hyperpa-
rameter combination results on the development set, rep-
resented as yellow dots. For each interval, we choose
the combination with the best performance (i.e. y-axis
value), thus forming the Max Curve in blue, with its
smoothed version in green.
curve, we find the best hyperparameters for each
method per interval, by selecting the hyperparame-
ters of the representative point in the current inter-
val. Finally, we take the best setting per interval,
and run each one on the test set. For each of our
methods, we produce a smoothed version, as the
results are not necessarily monotonically increas-
ing, which is shown in Figure 4 as the green line.
These smoothed curves are the ones showcased in
the final results in Figure 5.
When performing the search over the hyperpa-
rameter space, we used grid search, with the hyper-
parameters and their value ranges being specified
in Table 7 in the Appendix. Other methods (such
as random search, Bayesian Optimization (Snoek
et al., 2012), etc.) may be attempted just as well in
future works.
6 Experimental Results
6.1 Main Trade-off Comparison
In Figure 5, we showcase the performance vs. effi-
ciency trade-off in terms of ROUGE-L and latency
on the test set of each dataset. These results are
the ones obtained after performing the hyperparam-
eter optimization procedure stated described Sec-
tion 5.4. The methods shown are the standard FiD
model, CALM, Token Filtering, and Combined.
For the base-sized models (Figures 5a, 5b, 5c),
we can observe all methods improve upon the base-
line model, each one in a different aspect. For
CALM, the method is able to reach lower latency



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 6):

2 2.5 3 3.5 4 4.5 5 5.5
23.5
24
24.5
25
25.5
26
26.5
27
Latency (seconds)
Rouge-L
2%
(a) ELI5 - Base
0.6 0.8 1 1.2 1.4 1.6 1.8 2 2.2
20
21
22
23
24
25
Latency (seconds)
2% (b) MS MARCO- Base
1.5 2 2.5 3 3.5 4 4.5
24
25
26
27
28
29
30
31
32
Latency (seconds)
2% (c) NQ - Base
4 6 8 10
24
24.5
25
25.5
26
26.5
Latency (seconds)
Rouge-L
2%
(d) ELI5 - Large
2 3 4 5
21
21.5
22
22.5
23
23.5
24
24.5
25
Latency (seconds)
2% (e) MS MARCO- Large
4 6 8 10 12
26
27
28
29
30
31
32
F iD
CALM
T ok en Filtering
Combined
Latency (seconds)
2% (f) NQ - Large
Figure 5: ROUGE-L Performance results of the different methods on the test sets, plotted as smoothed Max Curves,
as a function of latency (seconds), for Base (top) and Large (bottom) models. Overall, our combined approach is
able to reach a better trade-off than the regular FiD model, for most cases.
values, due to skipping the redundant layer compu-
tations. In the case of Token Filtering, it is also able
to preserve and at times improve the performance
of the model overall, while the latency improve-
ment remains limited, since it is still computing the
remaining tokens across all decoder layers. The
performance improvement is presumably due to the
redundant tokens being removed early on during
the generation process, hence allowing the model
to better attend to the salient information in the
input.
When combining both methods, the performance
enhancement of the Token Filtering and the latency
reduction of CALM produce a better curve than
either method alone. In addition, we showcase
the drop in 2% performance per dataset, show-
ing that our method is able to reduce the latency
significantly more than the regular FiD, with the
best reduction reached on the MS MARCO dataset
for FiD-Base, saving 62.2% of the latency. In the
NQ dataset however, for both the base-sized and
large-sized models, while the CALM method does
achieve proper latency reduction, the Token Filter-
ing does not effect the results significantly. Since
we focus on real-world scenarios, we showcase
the trade-off with the actual latency, instead of
measurements such as FLOPS (MACs), as done by
previous works (de Jong et al., 2022). For those, we
refer to Figure 6 in the Appendix for the trade-off
and FLOPS (MACs) analysis. For the large-sized
models (Figures 5d, 5e, and 5f), we observe similar
patterns to those in the base-sized variations, with
the latency values being significantly larger. We
note that the overall performance values for these
models are not substantially different than those
produced by the smaller versions, hence we do not
focus as much on them.
6.2 Performance Comparison
To asses the performance of our Combined method
further, we choose the best performing hyperpa-
rameter setting for the FiD-Base model, and report
the test set results for each dataset, with Table 1
showing the results, compared to approaches sug-
gested in Su et al. (2022). In particular, we com-
pare to their implementation of FiD (named RBG
FID) and their suggested system (named RBG),
with the results of both taken from their published
work. We note that both our models and the RBG
models are of the same size. In our experiments,
we denote the original FiD model we trained as
FiD (ours), the FiD model with the Token Filter-
ing method as FiD TF, and the Combined method
as FiD Comb. On both datasets, FiD (ours), FiD
TF and FiD Comb achieve state-of-the-art results,
with Combined reaching the best overall perfor-
mance in terms of ROUGE-L and F1 (aside from
MS MARCO F1, where our approach is second).



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 18):

0 50 100 150
0.8
1
1.2
1.4
1.6
1.8
2
2.2 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens
(a) ELI5 - p = 10%
0 50 100 150
0.9
1
1.1
1.2
1.3
1.4
1.5
1.6 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (b) ELI5 - p = 30%
0 50 100 150
0.9
0.95
1
1.05
1.1
1.15 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (c) ELI5 - p = 50%
0 10 20 30 40 50
0.5
1
1.5
2
2.5
3 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens
(d) MS MARCO - p = 10%
0 10 20 30 40 50
0.8
1
1.2
1.4
1.6 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.8
0.85
0.9
0.95
1
1.05
1.1
1.15
1.2
1.25 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (f) MS MARCO - p = 50%
0 20 40 60 80 100 120
0.5
1
1.5
2
2.5
3
3.5
4
4.5 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens
(g) NQ - p = 10%
0 20 40 60 80 100 120
0.6
0.8
1
1.2
1.4
1.6
1.8
2 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens (h) NQ - p = 30%
0 20 40 60 80 100 120
0.8
0.9
1
1.1
1.2
1.3
1.4 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens (i) NQ - p = 50%
Figure 10: The percentage of tokens that were chosen from each passage, for FiD-Large models. The gold passage
(labeled as 1) is colored red. Each row represents a different dataset, and every column represents a different filtering
percentage (10,30,50).



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 16):

0 50 100 150
1
1.5
2
2.5
3 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens
(a) ELI5 - p = 10%
0 50 100 150
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (b) ELI5 - p = 30%
0 50 100 150
0.85
0.9
0.95
1
1.05
1.1
1.15
1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (c) ELI5 - p = 50%
0 10 20 30 40 50
0.5
1
1.5
2
2.5
3
3.5 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens
(d) MS MARCO - p = 10%
0 10 20 30 40 50
0.6
0.8
1
1.2
1.4
1.6
1.8 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.8
0.9
1
1.1
1.2
1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (f) MS MARCO - p = 50%
0 20 40 60 80 100
0.5
1
1.5
2
2.5
3
3.5 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens
(g) NQ - p = 10%
0 20 40 60 80 100
0.8
1
1.2
1.4
1.6
1.8
2 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens (h) NQ - p = 30%
0 20 40 60 80 100
0.9
1
1.1
1.2
1.3
1.4 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens (i) NQ - p = 50%
Figure 8: The percentage of tokens that were chosen from each passage, for FiD-Base models. The gold passage
(labeled as 1) is colored red. Each row represents a different dataset, and every column represents a different filtering
percentage (10,30,50).



### Claim 149/179

#### Claim Text
This is consistent with the analysis of Zhou & Balachandar which we have already discussed above.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[33]_2305.17653.pdf (Page 4):

Method Retrieval SST-2 SST-5 CoLA TREC CR MR MPQA Subj Average
ICL (OPT-13b) ✗ 93.0 46.0 1.8 26.8 73.2 61.7 71.6 51.1 53.2
T5-base (220M)
k-NN ✓ 59.2 22.8 1.0 28.0 51.8 55.1 52.6 72.1 42.9
LM-BFF ✗ 86.0 45.5 5.5 76.2 90.0 83.1 82.3 90.2 69.9
T5-base ✗ 91.3 56.7 30.4 80.4 89.8 89.4 89.2 96.0 77.9
RAG ✓ 93.0 57.5 58.5 80.4 87.2 90.2 89.5 96.5 81.6
FiD ✓ 92.2 56.6 56.9 80.8 91.3 90.1 89.8 96.6 81.8
PGRA (Ours) ✓ 93.9 56.9 57.0 80.8 91.7 91.1 90.3 97.0 82.3
T5-large (770M)
k-NN ✓ 64.5 23.6 2.1 28.8 56.8 58.2 53.7 72.4 45.0
LM-BFF ✗ 90.8 49.0 6.9 70.6 91.1 83.5 89.5 88.4 71.2
T5-large ✗ 95.2 59.2 60.7 80.8 92.1 91.5 90.7 97.3 83.4
RAG ✓ 95.2 57.2 60.1 80.2 91.2 92.1 90.6 96.4 82.9
FiD ✓ 94.8 59.5 60.2 80.8 92.4 92.5 90.6 97.5 83.5
PGRA (Ours) ✓ 95.7 59.8 61.1 80.9 92.6 92.4 90.6 97.5 83.8
Table 2: The results of baselines and our PGRA . For models with T5-base backbone, we use d = 16. For models
with T5-large backbone, we use d = 8 in the second stage due to GPU memory limitation. The best results are
bolded, and the second-best ones are underlined.
line of in-context learning, we use the same tem-
plates as ours in the second stage.
3.2 Results
We compare our proposed PGRA with the afore-
mentioned baseline methods, where the results are
shown in Table 2. We include results on both T5-
base and T5-large models for generality reasons.
We run our experiments three times and report de-
tails of each run in Appendix A. We report average
results here and first-run results in the analysis sec-
tion below.
Firstly, the PGRA can significantly outperform
the simple k-Nearest Neighbour and few-shot meth-
ods, including in-context learning with OPT-13b
and LM-BFF. As for the k-Nearest Neighbour, it
is simply based on the distances of embeddings
encoded by T5. As for the few-shot methods, in-
context learning uses prompts to elicit PLMs to
generate answers without updating parameters. It
is worth noting that we use in-context learning with
OPT-13b as our prompt-guided reranker in the sec-
ond stage. The performance of in-context learning
is ordinary, so it is surprising that it can assist on
PGRA . We will further discuss the reason behind
this in Section 4.1. Meanwhile, LM-BFF is further
fine-tuned on the prompts to give answers. Thus,
its performance is obviously higher than k-Nearest
Neighbour and in-context learning with OPT-13b
but remains a large gap to PGRA.
Secondly, compared to supervised learning (i.e.,
T5-base and T5-large) and retrieval-augmented
baselines, PGRA still outperforms them across
most tasks. Specifically, the line of retrieval meth-
ods with a T5-base reader outperforms supervised
learning with the T5-base model, while retrieval-
augmented methods with a T5-large reader are
worse or comparable to supervised learning with
the T5-large model. Furthermore, our method
PGRA can obviously surpass these baselines, in
both T5-base and T5-large setups. In conclusion,
extensive experimental results have shown that our
PGRA is effective on diverse NKI tasks.
4 Analysis
4.1 Effects of Label Consistency
In this section, we probe the influence of retrieved
evidence on the model performance of our PGRA
from the aspect of label consistency. Note that
our external text is without any task-specific labels.
Therefore, we use a T5-base model fine-tuned on
the specific task, which is the closest to our PGRA
reader but without retrieval, to generate pseudo-
label for all text in the external resource. In detail,
if the pseudo-label of evidence is the same as the
ground-truth label of the input, we say the evi-
dence is consistent with the input. We can then
directly detect the relation between the number of
consistent evidence and model performance at the
instance level. Specifically, out of 16 pieces of
total retrieved evidence, the number of consistent
evidence with the same (pseudo) labels as the input
varies from 0 and 16.
Taking the SST-2 task as an example, we count



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 10):

neural retrievers for open-domain question answering.
In Annual Meeting of the Association for Computa-
tional Linguistics.
Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani,
Dara Bahri, Vinh Quang Tran, Yi Tay, and Donald
Metzler. 2022. Confident adaptive language model-
ing. ArXiv, abs/2207.07061.
Roy Schwartz, Gabriel Stanovsky, Swabha
Swayamdipta, Jesse Dodge, and Noah A. Smith.
2020. The right tool for the job: Matching model
and instance complexities. In Annual Meeting of the
Association for Computational Linguistics.
Jasper Snoek, H. Larochelle, and Ryan P. Adams. 2012.
Practical bayesian optimization of machine learning
algorithms. ArXiv, abs/1206.2944.
Dan Su, Xiaoguang Li, Jindi Zhang, Lifeng Shang, Xin
Jiang, Qun Liu, and Pascale Fung. 2022. Read before
generate! faithful long form question answering with
machine reading. ArXiv, abs/2203.00343.
Surat Teerapittayanon, Bradley McDanel, and H. T.
Kung. 2016. Branchynet: Fast inference via early
exiting from deep neural networks. 2016 23rd Inter-
national Conference on Pattern Recognition (ICPR),
pages 2464–2469.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, volume 30. Curran Associates, Inc.
Ellen M. V oorhees. 1999. The trec-8 question answer-
ing track report. In Text Retrieval Conference.
David Wingate, Mohammad Shoeybi, and Taylor
Sorensen. 2022. Prompt compression and contrastive
conditioning for controllability and toxicity reduc-
tion in language models. In Conference on Empirical
Methods in Natural Language Processing.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
and Jamie Brew. 2019. Huggingface’s transformers:
State-of-the-art natural language processing. ArXiv,
abs/1910.03771.
Donghan Yu, Chenguang Zhu, Yuwei Fang, W. Yu,
Shuohang Wang, Yichong Xu, Xiang Ren, Yim-
ing Yang, and Michael Zeng. 2021. Kg-fid: Infus-
ing knowledge graph in fusion-in-decoder for open-
domain question answering. ArXiv, abs/2110.04330.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2019. Bertscore:
Evaluating text generation with bert. ArXiv,
abs/1904.09675.
A Cross Attention Pattern Analysis
In this section, we continue our discussion from
section 3.2, regarding the analysis of the cross-
attention scores.
In Figure 7, we present multiple versions of
the plot in Figure 3a, with the rows indicating the
dataset (ELI5, MS MARCO, NQ), and the columns
representing a different percent of chosen tokens
(10, 30, 50). For MS MARCO and NQ in 10%,
the percentage of the gold passage tokens remains
high for the lower layers, starting from token 10.
The other layers do not reach the same percentage
and degrade during the generation process. When
increasing the percentage to 30, and later 50, the
percentage of the gold passage is getting reduced
substantially.
In Figure 8, we showcase an extended version
of Figure 3b, for the various datasets and chosen
token percentages as in Figure 7, with the rows
and columns being similarly organized. For 10%,
the gold passage gets the most tokens out of all
the rest, for all datasets, with the lower passages
getting less than 1%. However, for 30%, the gold
passage is no longer the highest ranking for some
of the datasets (MS MARCO, NQ), with the upper
passages reaching higher, and the lower ones still
being at the 1% mark. At 50%, the gold passage is
no longer the most prominent, with it being nearly
as insignificant as the lower passages in the case
of MSMARCO. This suggests that increasing the
percentage of tokens taken introduces unnecessary
noise to the selected tokens, thus forcing the model
to receive input from lower ranked passages. For
MS MARCO and NQ in 10%, the percentage of
the gold passage tokens remains high for the lower
layers, starting from token 10. While the results
above were done using an FiD-Base model, similar
patterns are present for FiD-Large models through
all previously discussed aspects.
B Attention Score Computation
Extensions
In addition to the methods introduced in 4, the
computation of the cross-attention scores can be
further altered in a few key areas, which we tackle
as well.
Value Normalization. As mentioned in Izacard
et al. (2022), the scores can benefit from scaling by
the l2 normalized values tensor V . Thus, we can
instead transform Ai
t,l into:



Source: data\tc16_2312.10997v5\referenced_papers\[63]_2401.18059.pdf (Page 15):

Published as a conference paper at ICLR 2024
datasets examined: QuALITY , QASPER, and NarrativeQA. Figure 5 illustrates a clear linear corre-
lation between the initial document length and the total token expenditure, emphasizing that RAP-
TOR maintains a linear token scaling regardless of document complexity or length.
Figure 6: Build time as a function of document length for documents of up to 80,000 tokens. RAP-
TOR tree construction time scales linearly with document length for each of the datasets.
Build Time We also empirically observed a consistent linear trend between the document length
and the build time, as shown in Figure 6. This suggests that RAPTOR scales linearly in terms of
time, making it a viable solution for efficiently processing large corpora of varying lengths.
Conclusion Overall, our empirical results indicate that RAPTOR scales both in terms of tokens
expended and build time. Even as the complexity and volume of the input text grow, the cost of
constructing the tree scales predictably and linearly. This demonstrates that RAPTOR is computa-
tionally efficient and well-suited for processing large and diverse corpora.
B A BLATION STUDY ON CLUSTERING MECHANISM IN RAPTOR
To assess the effectiveness of the clustering mechanism in our RAPTOR approach, we conducted
an ablation study on the QuALITY dataset. This study compares RAPTOR’s performance with a
balanced tree-style encoding and summarization of contiguous chunks, in contrast to our standard
clustering method.
B.1 M ETHODOLOGY
Both configurations in this ablation study utilized SBERT embeddings and UnifiedQA to maintain
consistency in retrieval. For RAPTOR, we employed our typical clustering and summarization
process. In contrast, the alternative setup involved creating a balanced tree by recursively encoding
and summarizing contiguous text chunks. We determined the window size for this setup based on
the average cluster size observed in RAPTOR, which is approximately 6.7 nodes. Hence, we chose
a window size of 7 nodes. The collapsed tree approach was applied for retrieval in both models.
B.2 R ESULTS & DISCUSSION
The results of the ablation study are presented in table 9. The results from this ablation study clearly
indicate an improvement in accuracy when employing RAPTOR’s clustering mechanism over the
recency-based tree approach. This finding substantiates our hypothesis that the clustering strategy in
RAPTOR is more effective in capturing homogeneous content for summarization, thereby enhancing
the overall retrieval performance.
16



Source: data\tc16_2312.10997v5\referenced_papers\[50]_2311.09210.pdf (Page 5):

Models NQ TriviaQA WebQ Average
EM F1 EM F1 EM F1 EM F1
Backbone language model: LLaMa-2 7B
QA fine-tune w/o IR 28.80 37.53 63.19 68.61 28.30 42.77 35.98 44.27
SAIL (Luo et al., 2023)* 36.20 44.23 73.20 80.92 27.92 40.65 45.77 55.27
Retrieve-Read (Shi et al., 2023c)47.39 55.81 74.92 81.53 29.58 43.51 48.49 56.97
+ CHAIN-OF-NOTE (ours) 48.92 57.53 76.27 82.25 32.33 46.68 50.46 58.78
(+1.53) (+1.72) (+1.35) (+0.72) (+2.75) (+3.17) (+1.97) (+1.81)
Backbone language model:GPT-4-1106†
QA prompt w/o IR 54.0 74.2 56.2 61.5
Retrieve-Read (Shi et al., 2023c) 61.8 70.6 56.8 63.1
+ CHAIN-OF-THOUGHT 63.6 71.2 58.4 64.4
+ CHAIN-OF-NOTE (OURS) 63.8 74.6 58.8 65.7
(+2.0) (+4.0) (+2.0) (+2.6)
Table 2: The RALM, when equipped with CHAIN -OF-NOTE (CON), demonstrates a marginal improvement over
the standard RALM in full test set evaluations. Significantly, it outperforms the standard RALM system in scenarios
with noisy documents, suggesting that CON can substantially enhance the model’s noise robustness.
* SAIL was designed for retrieval-augmented instruction tuning, and as such, may not be ideally factual QA.
† Evaluating GPT-4 outputs with EM score is challenging; we opt for Accuracy, with reasons outlined in§3.1.3.
Karpukhin et al. (2020); Zhu et al. (2021). For
EM score, an answer is deemed correct if its nor-
malized form – obtained through the normalization
procedure delineated by (Karpukhin et al., 2020)
– corresponds to any acceptable answer in the pro-
vided list. Similar to EM score, F1 score treats the
prediction and ground truth as bags of tokens, and
compute the average overlap between the predic-
tion and ground truth answer (Chen et al., 2017).
Besides, we use reject rate (RR) to evaluate the
unknown robustness when given questions beyond
a language model’s knowledge scope.
Finally, since GPT-4 is not directly trained on
open-domain QA benchmarks, employing EM /
F1 for evaluation is challenging. Therefore, we
adopt the approach outlined in Mallen et al. (2023);
Kandpal et al. (2023), utilizing accuracy as the
evaluation metric. Accuracy considers a prediction
correct if any substring of the prediction exactly
matches any of the provided correct answers.
3.2 Evaluation on Overall QA Performance
Table 2 demonstrates that the RALM consistently
outperforms the directly fine-tuned LLaMa-2 with
QA pairs, without retrieval. This improvement
is closely tied to the effectiveness of the retrieval
process. As indicated in Table 1, DPR demon-
strates markedly superior retrieval performance on
the NQ and TriviaQA datasets compared to WebQ.
Consequently, the benefits of retrieval are more
pronounced on NQ and TriviaQA. Furthermore,
when comparing our enhanced RALM, which inte-
grates CON, with the standard RALM, our method
persistently shows better performance. There is
an average improvement of +1.97 in EM scores
across all three datasets when using LLaMa-2 as
backbone language model. Delving deeper, we
find that this improvement varies depending on
whether DPR successfully retrieves relevant doc-
uments. Specifically, the average improvement is
+1.2 when DPR retrieves relevant documents and
+2.3 when it does not on the NQ dataset. This dis-
parity suggests that our CON improve RALM’s in
scenarios where more noisy documents are fetched
in the first retrieval stage. This observation aligns
with our findings on noise robustness, which are
elaborated in the subsequent sections detailing our
experimental results.
Furthermore, the dynamics observed with larger
language models differ from those noted in experi-
ments with smaller-sized models due to their supe-
rior factual knowledge. The impact of utilizing re-
trieval is observed to be less pronounced with larger
models and can even be detrimental in certain cases,
such as with TriviaQA, where questions are mostly
straightforward. Concerning the comparison be-
tween CON and the baseline, the performance trend
remains consistent with that observed in smaller-
sized models, suggesting that CON maintains its
significance across different model sizes.



Source: data\tc16_2312.10997v5\referenced_papers\[20]_2303.08518.pdf (Page 14):

D Analysis on Performance Decline
We conduct analysis on why UPRISE shows neg-
ative performance when testing on Coreference
Resolution and Commonsense Reasoning tasks.
Coreference Resolution hardly benefits from
demonstrations. For Coreference Resolution task
type, we observe that even vanilla few-shot prompt-
ing underperforms zero-shot prompting, as high-
lighted in Table 65. This trend is consistent with
GPT-3 (Brown et al., 2020), GLaM (Du et al.,
2022), and LaMDA-PT (Thoppilan et al., 2022), as
reported by FLAN (Wei et al., 2022a). These mod-
els also exhibit limited performance gain from few-
shot prompting compared to zero-shot for Corefer-
ence Resolution. We deduce that the task’s inherent
nature might make it less responsive to demonstra-
tions, regardless of their alignment with the task.
Method 0-SHOT FEW-SHOT
Coreference. 59.3 50.6
Table 6: Average scores of vanilla zero-shot and few-
shot prompting of Coreference Resolution tasks.
Commonsense Reasoning is harmed by differ-
ent demonstration format. By analyzing the re-
trieved training task types (as shown in Figure 10),
we find that Closed-book QA is the most-frequently
retrieved type when testing Commonsense Reason-
ing. However, the two types differ significantly on
the input-output format: Closed-book QA follows
a question-answering format, but Commonsense
Reasoning follows the language modeling format,
which may lead to the decrease in performance.
E Extended Related Work
Prompt Design. In-context Learning (Brown et al.,
2020) is a method that helps LLMs transfer to new
tasks via inference alone by conditioning a concate-
nation of training demonstrations and testing input,
without any gradient updates.
With standard in-context learning, LLMs strug-
gle to tackle complex arithmetic, commonsense,
and symbolic reasoning tasks. Chain-of-Thoughts
(CoT) (Wei et al., 2022b) proposes providing LLMs
with a series of intermediate reasoning steps as
demonstrations to induce LLMs to produce another
5WSC273 dataset of Coreference Resolution has no train-
ing set, thus it’s excluded from the average task scores calcu-
lation.
series of intermediate reasoning steps that lead to
the final answer.
Prompt Tuning. Traditional natural language
prompts require significant human engineering and
can lead to suboptimal performance. Prompt tuning
proposes to learn a prompt represented by contin-
uous parameters rather than discrete natural lan-
guage tokens (Liu et al., 2021). Prompt tuning
takes the source text embedded by the LM input
embeddings and prepends learnable embeddings
to obtain a new embedded sequence. A variant of
prompt tuning is prefix tuning (Li and Liang, 2021;
Lester et al., 2021), where the learnable vectors are
added not only to the input but to all transformer
layers.
F Analysis on Retrieved Training
Clusters
To further interpret the impact of the retrieved
prompts on the testing task performance, we ana-
lyze which training task clusters are retrieved when
testing on the held-out cluster.
As shown in the visualisation plot in Figure 10,
clusters including diverse question types like Read-
ing Comprehension correspond to high retrieved ra-
tios (e.g., 80.7% for Close-QA and 36.1% for NLI),
while the less diverse Sentiment Analysis cluster
does not reach the top ranks. This finding fur-
ther supports that including tasks of diverse ques-
tion/answer types in the training data contributes
to good generalizability of the retriever.
Struct.Summ.Core.Comm.Senti.Para.NLIClosedRead.
0.2 12.5 1.8 3.3 7.8 3.7 33.1 37.5 0.0 Read.
0.0 0.1 0.2 1.1 0.0 0.9 16.9 0.0 80.7 Closed
0.1 0.2 2.5 2.1 0.6 47.0 0.0 11.5 36.1 NLI
0.0 0.4 74.4 0.1 0.2 0.0 13.9 6.2 4.8 Para.
6.2 34.9 1.9 9.4 0.0 1.3 23.4 5.1 17.7 Senti.
0.3 1.7 23.5 0.0 12.4 0.1 2.1 40.7 19.3 Comm.
0.1 9.4 0.0 51.5 0.1 33.1 1.1 3.3 1.6 Core.
RETRIEVED PROMPT CLUSTER (%)
TESTING CLUSTER
Figure 10: Percentages of retrieved prompts in each
training task cluster when testing on the held-out cluster.



### Claim 150/179

#### Claim Text
In order to reduce the Lagrangian velocity error, Gsell & Favier propose to multiply the Lagrangian force relation (5.5) by a constant correction factor which only depends on the choice of the kernel function.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 30):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question Physics students are checking the constant acceleration equations of kinematics
by measuring the velocity of a tennis ball that is dropped and falls 6 meters
and then passes through a photogate. The predicted velocity is 20% above the
velocity measured by the photogate. Which of the following best describes the
cause of the large percent difference?
0) The ball changes its shape while falling.
1) The acceleration of gravity varies as the ball is falling.
2) Air resistance increases the acceleration of the ball.
3) The acceleration of the balls varies with the velocity.
Principles Vertical motion: y = 0.5 * g * t2, where y is the vertical displacement, g is the
acceleration due to gravity, t is the time.
Free fall: is a special case of motion with constant acceleration, in which the
acceleration is due to gravity.
Acceleration to Velocity Formula: v = u + a * t, where v is the final speed, u is
the initial velocity, a is the acceleration, and t is the time.
Answer Using the Principles of Vertical motion and Horizontal motion, we can solve
the problem as following:
Step 1: Apply the Vertical motion equation to find the time it takes for the ball
to fall 6 meters.
y = 0.5 * g * t2
6 m = 0.5 * 9.8 m/s2 * t2
t = 1.12 s
Step 2: Apply the Horizontal motion equation to find the velocity of the ball
when it passes through the photogate.
x = v * t
v = x / t
v = 6 m / 1.12 s
v = 5.36 m/s
Step 3: Compare the predicted velocity with the velocity measured by the
photogate. The predicted velocity is 20% above the velocity measured by the
photogate, so the predicted velocity is 1.2 * 5.36 m/s = 6.43 m/s.
Step 4: Find the cause of the large percent difference. The predicted velocity is
6.43 m/s, but the velocity measured by the photogate is 5.36 m/s. The difference
is 1.07 m/s. This difference is caused by air resistance. Air resistance is a force
that opposes the motion of an object through the air. It is caused by the friction
between the object and the air molecules. The greater the surface area of the
object, the greater the air resistance. The tennis ball has a large surface area, so
it experiences a lot of air resistance. This air resistance slows down the tennis
ball and causes the difference between the predicted velocity and the velocity
measured by the photogate.
Therefore, the correct answer is 0) The ball changes its shape while falling.
Table 27: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Reasoning Error. In this case, the model was able to correctly identify air resistance is the cause of
the velocity difference, but made the error at the final reasoning step of deriving the right answer.
31



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 20):

the compute budget. We start with the Equation (1.6), repeated here for convenience:
L(N,S) =
(Nc
N
)αN
+
(Sc
S
)αS
. (B.1)
Here, S represents the number of parameter updates when training at the critical batch size [MKAT18],
which was deﬁned in Equation (5.2)9:
B(L) = B∗
L1/αB
. (B.2)
We would like to determine optimal training parameters for a ﬁxed compute budget, so we replace S =
C/(6NB (L)), where Cis the number of FLOPs used in the training run:
L(N,C) =
(Nc
N
)αN
+
(
6B∗Sc
N
L1/αBC
)αS
. (B.3)
Now, we set ∂NL
⏐⏐
C = 0to ﬁnd the condition for optimality:
0 = ∂L
∂N
⏐⏐
C
= −αN
N
(Nc
N
)αN
+ αS
N
(
6B∗Sc
N
L1/αBC
)αS (
1 −5N
L∂L
∂N
⏐⏐
C
)
=⇒ αN
αS
(Nc
N
)αN
=
(
6B∗Sc
N
L1/αBC
)αS
(B.4)
Equation (B.3) and (B.4) together determine the compute-efﬁcient frontier.
B.2 Efﬁcient Training
Now we assemble the implications of (B.3) and (B.4). First, note that inserting (B.4) into (B.3) yields
L(Neﬀ (C) ,C) =
(
1 +αN
αS
)
L(Neﬀ,∞) , (B.5)
which implies that for compute-efﬁcient training, we should train to a ﬁxed percentage αN
αS
≈10% above
the converged loss. Next, let’s determine how the optimal loss depends on the compute budget. Eliminating
N yields a power-law dependence of performance on compute:
L(C) =
(Cc
C
)αC
(B.6)
where we deﬁned
αC = 1/(1/αS + 1/αB + 1/αN) ≈0.052 (B.7)
Cc = 6NcB∗Sc
(
1 +αN
αS
)1/αS+1/αN (αS
αN
)1/αS
. (B.8)
Similarly, we can eliminate Lto ﬁnd N(C):
N(C)
Nc
=
(C
Cc
)αC/αN (
1 +αN
αS
)1/αN
(B.9)
and
S(C) = Cc
6NcB∗
(
1 +αN
αS
)−1/αN (C
Cc
)αC/αS
(B.10)
9There is a slight ambiguity here: we can imagine training either at a constant batch size B (Ltarget), or we could
instead train at a variable batch size ˜B (L), where ˜B is the instantaneous critical batch size (as opposed to B, which is
the averaged version). These two prescriptions result in the same number of steps, so we can ignore this subtlety (see
[MKAT18]).
21



Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 21):

B.3 Comparison to Inefﬁcient
Typically, researchers train models until they appear to be close to convergence. In this section, we compare
the efﬁcient training procedure described above to this more typical setup. We deﬁne a the convergence factor
f as the percent deviation from the converged loss:
L(N,C) = (1 +f) L(N,∞) . (B.11)
For compute-efﬁcient training we have f = αN/αS ≈ 10% from the previous section, but researchers
typically use a much smaller value. Here, we choose f′= 2%as an estimate. For a ﬁxed value of the loss,
we predict:
Nf
Nf′
=
(1 +f
1 +f′
)1/αN
≈2.7 (B.12)
Sf
Sf′
=
(
1 +1
f
1 + 1
f′
)1/αS
≈0.13 (B.13)
Cf
Cf′
= Nf
Nf′
Sf
Sf′
≈0.35 (B.14)
So that compute-efﬁcient training uses 7.7x fewer parameter updates, 2.7x more parameters, and 65% less
compute to reach the same loss.
B.4 Suboptimal Model Sizes
We can solve A.1 to ﬁnd an expression for the amount of compute needed to reach a given value of the loss
Lwith a model of size N:
C(N,L) =
(
6B∗Sc
N
L1/αB
)(
L−
(Nc
N
)αN)−1/αS
. (B.15)
Using A.6 and A.9, we can eliminate Lin favor of Neﬀ (L), the model size which reaches Lmost efﬁciently.
From there, we ﬁnd an expression for the excess compute needed as a consequence of using a suboptimal
model size:
C(N,Neﬀ)
C(Neﬀ,Neﬀ) = N
Neﬀ
[
1 +αS
αN
(
1 −
(Neﬀ
N
)αN)]−1/αS
. (B.16)
The result is shown in Figure X. Models between 0.6x and 2.2x the optimal size can be used with only a
20% increase in compute budget. Using a smaller model is useful when accounting for the cost inference. A
larger model can be trained the the same level of performance in fewer steps, allowing for more parallelism
and faster training if sufﬁcient harware is available (see Figure Y):
S(N,Neﬀ)
S(Neﬀ,Neﬀ) =
[
1 +αS
αN
(
1 −
(Neﬀ
N
)αN)]−1/αS
. (B.17)
A 2.2x larger model requires 45% fewer steps at a cost of 20% more training compute. Note that this equation
should not be trusted for very large models, as it is only valid in the power-law region of the learning curve
after initial transient effects.
C Caveats
In this section we list some potential caveats to our analysis.
• At present we do not have a solid theoretical understanding for any of our proposed scaling laws.
The scaling relations with model size and compute are especially mysterious. It may be possible to
understand scaling at very large Dholding model size ﬁxed [AS17], and also the shape of learning
curves late in training, by modeling the loss with a noisy quadratic. But the scaling with Dat very
large model size still remains mysterious. Without a theory or a systematic understanding of the
corrections to our scaling laws, it’s difﬁcult to determine in what circumstances they can be trusted.
22



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 14):

Models between 0.6x and 2.2x the 
optimal size can be trained with a 
20% larger compute budget
Smaller models require 
more steps to train, while 
larger models require fewer
Our framework does not 
capture early training dynamics
Figure 12 Left: Given a ﬁxed compute budget, a particular model size is optimal, though somewhat larger
or smaller models can be trained with minimal additional compute. Right: Models larger than the compute-
efﬁcient size require fewer steps to train, allowing for potentially faster training if sufﬁcient additional paral-
lelism is possible. Note that this equation should not be trusted for very large models, as it is only valid in the
power-law region of the learning curve, after initial transient effects.
10 8
 10 6
 10 4
 10 2
 100
Compute (PF-days), non-embedding
2
3
4
5
6
7Test Loss
L = (Cmin/2.3 108) 0.050
L = (C/2.0 107) 0.057
Figure 13 When adjusting performance to simulate training far below the critical batch size, we ﬁnd a
somewhat altered power law for L(Cmin) when compared with the fully empirical results. The conspicuous
lump at 10−5 PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks
in the power-law ﬁts. It is the L(Cmin) trend that we expect to provide a reliable extrapolation for larger
compute.
that in fact we could train more efﬁciently 6 by training at the batch size Bcrit discussed in Section 5.1.
Large and small values of the loss could have been achieved with fewer samples or fewer steps, respectively,
and correcting for this inefﬁciency by standardizing to the critical batch size results in cleaner and more
predictable trends.
In this section we will adjust for this oversight. More importantly, we will use the results of Section 5
to determine the optimal allocation of compute between model size N and the quantity of data processed
during training, namely 2BcritSmin. We will determine this allocation both empirically and theoretically, by
using the equation for L(N,Smin), and we will demonstrate that these methods agree.
6.1 Optimal Performance and Allocations
Let us ﬁrst study the loss as a function of the optimally allocated compute from Equation (5.5). The result is
plotted in Figure 13, along with a power-law ﬁt. We see that as compared to the compute plot of Figure 1, the
new ﬁt with Cmin is somewhat improved.
Given L(Cmin), it is natural to ask for the optimal model sizeN(Cmin) that provides the minimal loss with a
given quantity of training compute. The optimal model size is shown in Figure 14. We observe thatN(Cmin)
6One might ask why we did not simply train at Bcrit in the ﬁrst place. The reason is that it depends not only on the
model but also on the target value of the loss we wish to achieve, and so is a moving target.
15



### Claim 151/179

#### Claim Text
In order to ensure the second-order spatial accuracy in a bounce-back process, a number of interpolated bounce-back (IBB) schemes have been introduced over the years, with the conditional IBB scheme proposed by Bouzidi et al. and the unified IBB scheme by Yu et al. being the most representative.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 19):

111:20 Lyu, et al.
Table 5. The experimental results for evaluating different retrievers in our benchmark.
task name retriever name bleu rouge-L bertScore RAGQuestEval length
precision recall
text continuation
BM25 3.51 17 .56 83 .83 27.25 23.70 370.5
Dense 3.66 17.78 83.99 26.96 24.68 367.6
Hybrid 3.69 17.69 83 .97 27 .24 24 .01 362.4
Hybrid+Rerank 3.55 17 .55 83 .90 26 .69 24 .02 370.3
summarization
BM25 25.19 33.77 87 .82 70.78 44.30 190.4
Dense 23.69 33 .53 88.49 68.06 46 .18 205.9
Hybrid 24.21 33 .81 88 .24 68 .70 45 .63 199.8
Hybrid+Rerank 24.33 33.90 88.48 68 .34 46.41 200.2
question answering
BM25 39.91 57 .33 83 .36 51 .90 69 .17 69.6
1-document
Dense 39.76 57 .24 83 .81 52 .67 70 .82 73.3
Hybrid 39.67 57 .38 84 .06 52 .71 70 .83 70.8
Hybrid+Rerank 40.63 58.26 84.68 54.60 73.92 72.8
question answering
BM25 24.61 38.31 86 .86 42 .26 54 .56 138.4
2-document
Dense 22.75 37 .25 87 .16 42 .93 56 .73 149.8
Hybrid 24.03 38 .43 87 .30 45 .67 58 .01 144.6
Hybrid+Rerank 24.53 38.91 87.89 47.18 58.12 151.7
question answering
BM25 20.98 34 .33 87 .02 37 .04 48 .53 147.6
3-document
Dense 21.05 35 .04 87 .81 40 .32 51 .37 156.6
Hybrid 21.35 35 .34 87 .66 41 .07 51 .09 150.8
Hybrid+Rerank 21.74 35.88 88.21 41.59 52.84 157.1
hallucination
BM25 33.09 54.21 80.86 64.80 79 .90 59.0
modification
Dense 32.35 53 .04 80 .49 65 .07 80 .85 64.8
Hybrid 32.22 52 .92 80 .57 66.30 81.03 63.4
Hybrid+Rerank 32.62 53 .01 80 .62 65 .57 80 .82 64.9
are two main types of retrievers: Keyword-based search-sparse retrieval algorithms , which
use keywords and their frequencies to compute the relevance between documents and queries.
Common sparse retrieval algorithms include TF-IDF and BM25. BM25 is an enhanced TF-IDF
method, which accounts for factors such as the length and position of words in the document.
Dense retrieval algorithms, which use deep learning models to encode documents and queries into
low-dimensional vectors, and then measure the cosine similarity between them. This method can
capture the semantic and contextual information of words, and improve the retrieval performance.
In order to combine the advantages of both types of retrievers, we can fuse their retrieval results
and randomly sample k from them as contexts for LLMs(Hybrid). Alternatively, we can also use a
re-ranking model to re-rank the fused retrieval results, and then select the top-k ones as the context
of LLMs(Hybrid+Rerank). In our experiments, we employ the bge-rank as the rerank model.
Text Continuation: As Table 5 displays, the performance of the dense retriever is roughly
equivalent to that of BM25, except for the key information recall rate. Compared to the keyword-
based algorithm, the modern vector search can capture the semantic and contextual information of
words, so that more content that does not match keywords but is obviously semantically related can
be retrieved. However, the RAG system using BM25 also performs well. In terms of the precision of
key information, BM25 even exceeds the dense retriever. This suggests that in the continuation task,
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



Source: data\tc16_2312.10997v5\referenced_papers\[59]_2310.05149.pdf (Page 3):

Table 1: Exact match performance on single-hop question answering. All ITRG results are from the last iteration (T = 5).
Method Natural Questions TriviaQA
0-shot 1-shot 5-shot 0-shot 1-shot 5-shot
GPT 3.5 Text-davinci-002 12.0 24.6 33.0 46.0 74.2 76.0
Text-davinci-003 29.4 33.0 33.8 75.8 78.6 77.8
LLaMA 33B
Vanilla LM 27.0 29.4 32.4 74.8 70.8 75.8
Retrieve-then-Read 27.8 30.6 29.8 74.6 76.0 76.0
Generate-then-Read 28.0 31.4 31.0 73.6 77.2 77.6
ITRG (refine) 34.4 34.6 34.8 79.0 79.4 80.6
ITRG (refresh) 37.6 38.4 38.0 77.0 78.6 79.4
Table 2: Exact match performance on multi-hop question answering. All ITRG results are from the last iteration (T = 5).
Method 2WikiMultiHopQA HotpotQA
0-shot 1-shot 5-shot 0-shot 1-shot 5-shot
GPT 3.5 Text-davinci-002 16.4 27.6 30.8 12.2 20.2 22.2
Text-davinci-003 27.2 27.0 29.8 25.0 25.8 26.6
LLaMA 33B
Vanilla LM 24.4 27.6 31.8 22.6 25.0 27.0
COT - - 32.2 - - 28.6
Retrieve-then-Read 27.4 29.2 32.0 28.4 29.8 30.4
Generate-then-Read 30.0 30.4 31.6 25.0 27.0 27.0
ITRG (refine) 33.0 33.6 37.0 28.8 29.6 30.6
ITRG (refresh) 32.2 36.2 38.6 31.0 32.6 33.4
Table 3: Exact match performance of ITRG (refresh) at dif-
ferent iterations in 5-shot setting.
Iteration 1 2 3 4 5
Natural Questions 34.0 35.2 37.0 37.2 38.0
TriviaQA 79.8 79.2 79.8 79.8 79.4
2WikiMultiHopQA 34.8 37.4 37.2 38.6 38.6
HotpotQA 32.6 32.8 34.0 33.4 33.4
and improves by 6.8 points in absolute gains. Compared to
vanilla LM, ITRG (refresh) can improve the EM score by 9.4,
7.6, and 6.4 points respectively in 0-shot, 1-shot, and 5-shot
settings on the Hotpotqa dataset.
4.2. Performance at Different Iterations
In this section, we analyze the performance of our model and
the quality of the generated documents during the iteration
process. Specifically, we present the results of ITRG (refresh)
at different iterations in 5-shot setting in Table 3. We measure
the answer recall of generated documents at different itera-
tion steps and present results in Table 4. Table 3 shows that
the performance of the model gradually improves with iter-
ation. And Table 4 shows that the quality of the generated
documents also gradually improves with iteration. These re-
sults verify that our iterative retrieval-generation collaborative
Table 4: Answer recall of generated documents at different
iterations with ITRG (refresh).
Iteration 1 2 3 4 5
Natural Questions 44.0 46.4 48.4 48.8 48.0
TriviaQA 18.8 19.0 20.2 19.2 19.2
2WikiMultiHopQA 34.2 36.6 35.0 40.0 37.0
HotpotQA 34.2 34.8 35.6 33.8 33.6
framework is effective and can further enhance the reasoning
capabilities of large language models.
5. CONCLUSION
In this paper, we present ITRG, which is an iterative retrieval-
generation synergy framework, containing two important
steps: generation-augmented retrieval and retrieval-augmented
generation. They form a closed loop, and can improve
each other via multiple iterations. We propose a simple
and effective generation-augmented retrieval strategy and
two retrieval-augmented generation strategies. Empirical re-
sults show our approach significantly exceeds several strong
baselines, including GPT 3.5, on four open domain ques-
tion answering datasets, which indicates that our method can
significantly improve the reasoning ability of large language
models.



Source: data\tc16_2312.10997v5\referenced_papers\[26]_2401.06954.pdf (Page 2):

Figure 2: Differing from previous RAGs that update
LLMs, retrievers, or both, BGM connects a frozen LLM
and a frozen retriever through a lightweight bridge
model which adapts the retrieved information to the
LLM’s preference. This makes BGM applicable to
“large” LMs and off-the-shelf retrievers.
from various knowledge sources is proven effective
across numerous NLP tasks, including language
modeling (Borgeaud et al., 2022; Khandelwal et al.,
2020; Shi et al., 2023b), question answering (Lewis
et al., 2020; Izacard et al., 2022; de Jong et al.,
2023; De Jong et al., 2023; Shi et al., 2023b; Guu
et al., 2020; Izacard and Grave, 2020; Xu et al.,
2023), fact versification (Lewis et al., 2020) and
text generation (Lewis et al., 2020). Specifically,
RAG utilizes input as a query and comprises two
main components: (1) a retriever retrieves a set
of items from a side corpus. Particular items may
vary across different tasks, including documents,
passages, or even tokens. In this study, we focus on
retrieving passages; and (2) a LLM incorporates
the retrieved items, as additional information in the
input context, and makes final predictions.
A fundamental question in this process arises
regarding the disparate preferences between LLMs
and retrievers, as LLMs performing optimally only
when their preferences are satisfied. Bridging the
preference gap is crucial. Depending on which
components are subject to updates, this challenge
can be categorized into three families.
Finetuning retrievers and LLMs jointly. This
is the most widely used setting of RAG (Izacard
et al., 2022; Khandelwal et al., 2020; Wu et al.,
2022; Guu et al., 2020; Lewis et al., 2020). How-
ever, most prior work is based on relative small
LMs (< 1B). For example, Altas (Izacard et al.,
2022) finetunes LLM (T5 (Raffel et al., 2020a))
and retriever (Contriever (Izacard et al., 2021))
jointly by leveraging the LLM to provide super-
visory signal to train the retriever. RAG (Lewis
et al., 2020) uses a tunnable query encoder and
DPR (Karpukhin et al., 2020) as retriever, BART
as LLM, and design an end-to-end schema to train
the query encoder and the LLM.
Finetuning LLMs only. Updating retrievers is
not always desirable as it is costly and requires
the document index to be periodically updated. To
bridge the preference gap, it is also possible to only
update the LLMs. FiD (Izacard and Grave, 2020)
takes the retrieved documents and query as input,
finetunes the LLM to adapt to the external infor-
mation. Similarly, Lummen (De Jong et al., 2023)
and Glimmer (de Jong et al., 2023) improve FiD
via adding reranker and pre-encoding memeory.
Finetuning retrievers only. Although above
systems have shown improves results, they are not
always applicable in practice. Many LLMs (espe-
cially larger ones) are only available as black-box
APIs. Given this constraint, a natural approach
is to only update the retrievers to ensure they can
retrieve passages that are more compatible with
LLMs. REPLUG (Shi et al., 2023b) adapts a sim-
ilar idea as Atlas but fix the LM. RECOMP (Xu
et al., 2023) trains a compressors to summarize
the retrieved document from retriever. However,
this family of models is incapable of performing
any sample-level selection and can only choose top
passages by setting a fixed threshold.
Unlike the existing approaches, BGM does not
fine-tune the LLM or the retriever and instead
employs a bridge model in between (Fig. 2).
RL for information retrieval. Before the LLM
era, RL has been used in information retrieval (IR)
(Xia et al., 2017; Wei et al., 2017; Zeng et al., 2018;
Xu et al., 2020). The core approach was to frame
the IR problem as a Markov Decision Process and
apply an RL algorithm to solve it. Typically, an
IR task would be structured to determine which
document to select for a ranking position, using
ranking metrics such as DCG as the reward. None
of these existing studies explore the application of
RL in the context of RAG.
In the LLM era, RL has been used in query
rewriting for retrieval (Wu et al., 2021; Nogueira
and Cho, 2017; Adolphs et al., 2021), where a
black-box retriever is assumed. This is a different
problem from RAG. Bacciu et al. (2023) suggest us-
ing RL to fine-tune retriever in the RAG context in
their opinion paper, not supported by experiments.
Their work does not recognize the importance of
bridging the gap between retrievers and LLMs, nor



Source: data\tc16_2312.10997v5\referenced_papers\[101]_2310.06839.pdf (Page 3):

1 5 10 15 20
Number of Retained Documents
0
20
40
60
80
100Recall(%)
LLMLingua
BM25
OpenAI
Voyageai
BGE-large-en v1.5
SBERT
Gzip
Cohere-Rerank
BGE-llmembeder
Jina
LongLLMLingua rk 
w/o restrict
BGE-Ranker-large
LongLLMLingua rk
(a) Recall Distribution
1 5 10 15 20
Document Position in the Prompt
0.0
0.2
0.4
0.6
0.8
1.0Document Avg. Perplexity
Perplexity
Contrastive Perplexity (b) Perplexity Distribution (5th)
Figure 3: (a) Comparison of recall on NaturalQuestions Multi-documemnt QA dataset, which increases from top to
bottom in terms of Recall@1. Different colors represent different types of methods. Among them, yellow represents
traditional relevance methods, green signifies embedding-based methods, and red denotes rerank-based methods.
(b) Comparison between perplexities and contrastive perplexities of tokens in the prompt from Multi-documemnt
QA dataset. The document containing the ground-truth information is located in the 5th position. More results on
position can be found in the Appendix C.1.
iterative compression mechanism following LLM-
Lingua and directly calculate token perplexities
to compress xins and xque. In this section, we in-
vestigate how to make the fine-grained token-level
compression over {xdoc
k }K′
k=1 aware of the question
xque, so that the compressed results could contain
more question-relevant key information.
A straightforward solution for the awareness of
xque is to simply concatenate it at the beginning
of the whole context. However, this will result in
low perplexities of relevant tokens in the context
following the condition of question xque, further
reducing their differentiation from other tokens.
In this paper, we propose contrastive perplexity,
i.e., the distribution shift caused by the condition of
the question, to represent the association between
the token and the question. The contrastive perplex-
ity based importance metric si for each token xi in
{xdoc
k }K′
k=1 can be formulated as:
si = perplexity(xi|x<i)−perplexity(xi|xque, x<i).
(3)
Additionally, we provide the derivation of its
mathematical significance in the Appendix A, con-
cluding that it is equivalent to conditional pointwise
mutual information (Church and Hanks, 1989).
Figure 3b illustrates the difference between per-
plexities and contrastive perplexities. The distri-
bution of perplexities appears random, making it
challenging to extract information related to the
question. However, tokens with high contrastive
perplexities tend to cluster near the ground-truth
document, which contains information relevant to
the question. This suggests that the proposed con-
trastive perplexity can better distinguish tokens
relevant to the question, thus improving the key
information density in the compressed results.
4.2 How to reduce information loss in the
middle?
As demonstrated in Figure 1b, LLM achieves the
highest performance when relevant information oc-
curs at the beginning and significantly degrades if
relevant information is located in the middle of long
contexts. After the coarse-grained compression, we
have obtained a set of documents {xdoc
k }K′
k=1 with
their corresponding importance scores {rk}K′
k=1 in-
dicating their association with the question xque.
Therefore, we reorder documents using their impor-
tance scores to better leverage LLMs’ information
perception difference in positions:
(xins, xdoc
1 , ··· , xdoc
K′ ,xque)
rk
−→
(xins,xdoc
r1 , ··· , xdoc
rK′ , xque)
(4)
4.3 How to achieve adaptive granular control
during compression?
In fine-grained compression, LLMLingua applies
the same compression ratio over all documents ob-
tained from budget controller. However, the key
information density of different documents is differ-
ent. The more relevant to the question a document



### Claim 152/179

#### Claim Text
Network motifs are the most common higherorder structures, defined as specific patterns of edges between vertices that appear statistically significant in the network .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 7):

Jiawei Zhang
we can represent the graph loading API call as
<API>𝐺𝐿(file-path,node-subset,link-subset)→ 𝐺</API>, (10)
where “𝐺𝐿()” denotes abbreviation of the “Graph Loading” func-
tion name, and the function parameters “file-path”, “node-subset”
and “link-subset” specify the local graph data file path (or the on-
line repository URL if the data is stored on the web), subsets of
specific nodes and links, respectively. The notation “→𝐺” explic-
itly represents the loaded graph data with the reference variable
𝐺 = (V,E), which is optional actually depending on the applica-
tion task and settings. What’s more, if the local file directory or
the online repository root URL has been pre-provided to the “𝐺𝐿()”
function already, then we can just simplify the “file-path” with the
specific “graph-name” instead when calling this API function.
Furthermore, when the parameters “ node-subset” and “ link-
subset” are either omitted or assigned with the strings “all nodes”
and “all links”, respectively, then the API function call will just load
the whole graph. For some cases, we can only specify the subset
of nodes to be loaded (e.g., {𝑣𝑖,𝑣𝑗,··· ,𝑣𝑘}⊂V in the graph) but
cannot enumerate all the related links, we can just assign the “node-
subset” and “link-subset” parameters with values “{𝑣𝑖,𝑣𝑗,··· ,𝑣𝑘}”
and “all related links” (or the “link-subset” parameter is just omitted).
It will provide us with more flexibility in loading sub-graphs based
on the provided node set and their internal links. Similarly, we can
also only specify the subset of links, by assigning the “node-subset”
with “all related nodes” or just omitted it, it will automatically load
the nodes composing those provided links in the graph data, like
the second graph data loading prompt example shown in Table 1.
Besides that example, as shown at the top part of Table 1, we also
provide a few other prompt examples of the graph data loading API
calls, which can retrieve and load the requested graph data from
the (local) files according to the input textual statements.
4.3.2 Graph Property Reasoning. Graph structured data may have
various properties, such as diameter, density, center and shortest
path, which can capture different characteristics of the graph data
and have extensive applications in real-world graph structured data.
For reasoning such graph properties, it usually requires the model
to not only know the property definitions but also has very strong
logic reasoning and mathematical calculation abilities to compute
such properties. For the existing language models, either masked
language models or autoregressive language models , it will be very
hard (almost impossible) for them to conduct the reasoning process
for such complex properties based on the input graphs.
In this paper, to empower LLMs with the graph property reason-
ing ability, we introduce a group of external APIs, which can be
called by the language models for reasoning about those properties.
To illustrate howGraph-ToolFormer handles such graph property
reasoning tasks, we will use the small-sized lollipop graph shown
in Figure 1 (the top-left graph in green color) as an example in this
part, which can be loaded via the following API calls as introduced
before:
<API>𝐺𝐿(“𝑙𝑜𝑙𝑙𝑖𝑝𝑜𝑝 ”)→ 𝐺𝑙</API>, (11)
where the loaded the graph can also be referred to by notation 𝐺𝑙.
For simplicity, in the following part, we will also use the above
loaded lollipop graph 𝐺𝑙 as an example to introduce the graph
property reasoning APIs for readers.
Order and Size : Formally, given a graph, like the loaded lollipop
graph 𝐺𝑙 = (V,E), its order denotes the number of nodes in the
graph, i.e., |V|, and its size is the number of links in the graph, i.e.,
|E|. We can represent the API calls for reasoning the order and size
properties of the lollipop graph as
<API>𝐺𝑅(𝐺𝐿(“𝑙𝑜𝑙𝑙𝑖𝑝𝑜𝑝 ”),“𝑡𝑜𝑜𝑙𝑥:𝑜𝑟𝑑𝑒𝑟”)→ 𝑟</API>, (12)
<API>𝐺𝑅(𝐺𝐿(“𝑙𝑜𝑙𝑙𝑖𝑝𝑜𝑝 ”),“𝑡𝑜𝑜𝑙𝑥:𝑠𝑖𝑧𝑒”)→ 𝑟</API>. (13)
If the lollipop graph has been pre-loaded via other API calls al-
ready and can be referred to as 𝐺𝑙, the above API calls can also be
simplified as follows:
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑜𝑟𝑑𝑒𝑟”)→ 𝑟</API>, (14)
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑠𝑖𝑧𝑒”)→ 𝑟</API>, (15)
where the notation 𝐺𝑅()denotes the abbreviated “Graph Rea-
soning” function name and the parameters “order” and “size”
represent the graph properties to be reasoned. The notation
“toolx:desired_property” denotes the desired graph property reason-
ing with the toolx toolkit. The toolx is a graph property calculation
toolkit created in this paper for Graph-ToolFormer based on the
networkx, and we will introduce more information about the graph
reasoning models and toolkits used in this paper in the next ex-
periment section instead. The notation “→𝑟” specifies the output
result 𝑟by the graph property reasoning API call to be included into
the output statements. As introduced before, the returning output
result tag “→𝑟” of the API calls is actually optional, inclusion of
which depends on both the reasoning context and application task.
Density: Graphdensity denotes the ratio of existing links in a graph
compared with the maximal number of potential links among nodes
in a graph. If the input lollipop graph 𝐺𝑙 = (V,E)is directed, its
density can be represented as |E|
|V|(|V|− 1); while if 𝐺𝑙 is undirected,
its density can be represented as 2|E|
|V|(|V|− 1). Formally, the API
calls that can be used for computing the density of graph can be
represented as follows:
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑑𝑒𝑛𝑠𝑖𝑡𝑦”,𝑖𝑠-𝑑𝑖𝑟𝑒𝑐𝑡𝑒𝑑 )→ 𝑟</API>, (16)
where the boolean “is-directed” parameter differentiates directed
graph from undirected ones in the density calculation.
Shortest Path: The shortest path between two nodes in a graph is
a path of shortest possible length connecting them via the nodes
and links in the graph. The API call for reasoning the length of the
shortest path from 𝑛𝑜𝑑𝑒1 to 𝑛𝑜𝑑𝑒2 in a graph can be represented as
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑠ℎ𝑜𝑟𝑡𝑒𝑠𝑡 -𝑝𝑎𝑡ℎ”,𝑛𝑜𝑑𝑒1,𝑛𝑜𝑑𝑒2)→ 𝑟</API>.
(17)
Meanwhile, the average length of shortest path for all nodes in the
graph can be obtained via the following API call instead
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑎𝑣𝑔-𝑠ℎ𝑜𝑟𝑡𝑒𝑠𝑡 -𝑝𝑎𝑡ℎ”)→ 𝑟</API>. (18)
Besides the average shortest path length , we can also reason for the
largest shortest path length and the smallest shortest path length of
a graph as follows:
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑚𝑎𝑥-𝑠ℎ𝑜𝑟𝑡𝑒𝑠𝑡 -𝑝𝑎𝑡ℎ”)→ 𝑟</API>, (19)
<API>𝐺𝑅(𝐺𝑙,“𝑡𝑜𝑜𝑙𝑥:𝑚𝑖𝑛-𝑠ℎ𝑜𝑟𝑡𝑒𝑠𝑡 -𝑝𝑎𝑡ℎ”)→ 𝑟</API>. (20)



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 24):

Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT
Table 5: Case analysis of graph reasoning API call statement generation, query parsing, and query execution and final output
generation in Graph-ToolFormer.
Tasks Graph Reasoning Examples withGraph-ToolFormer
Input Statements Generated Statements with API callsQuery parsing Reasoning Final Statement Output
Graph
Property
Reasoning
What is the order of the dia-
mond graph?
The order of the diamond graph is [GR(GL("gpr",
{"diamond_graph"}), "toolx:order")–>r].
((’GR’, [(’GL’, [’gpr’, ’{"dia-
mond_graph"}’]), ’toolx:order’]),
[True])
4 The order of the diamond graph
is 4.
The path graph has a center at
nodes [TBR].
The path graph has a center at nodes
[GR(GL("gpr", {"path_graph"}), "toolx:center")–
>r].
((’GR’, [(’GL’, [’gpr’,
’{"path_graph"}’]), ’toolx:center’]),
[True])
[5, 6] The path graph has a center at
nodes [5, 6].
The nodes in the wheel graph
have eccentricity values of
[TBR].
The nodes in the wheel graph have eccentric-
ity values of [GR(GL("gpr", {"wheel_graph"}),
"toolx:eccentricity")–>r].
((’GR’, [(’GL’, [’gpr’,
’{"wheel_graph"}’]),
’toolx:eccentricity’]), [True])
{0: 1, 1: 2, 2: 2, 3: 2,
4: 2, 5: 2}
The nodes in the wheel graph
have eccentricity values of {0: 1,
1: 2, 2: 2, 3: 2, 4: 2, 5: 2}.
Bibliographic
Network
Reasoning
What is the topic of paper
#83826 in the cora biblio-
graphic network?
The topic of paper #83826 in the cora
bibliographic network is [GR(GL("cora"),
"graph_bert:topic", paper#83826)–>r].
((’GR’, [(’GL’, [’cora’]),
’graph_bert:topic’, ’paper#83826’]),
[True])
Neural_NetworksThe topic of paper #83826 in
the cora bibliographic network
is Neural_Networks.
Paper #2998, situated in the
citeseer bibliographic network,
is centered around the [TBR]
topic.
Paper #2998, situated in the citeseer bib-
liographic network, is centered around
the [GR(GL("citeseer"), "graph_bert:topic",
paper#2998)–>r] topic.
((’GR’, [(’GL’, [’citeseer’]),
’graph_bert:topic’, ’paper#2998’]),
[True])
AI Paper #2998, situated in the cite-
seer bibliographic network, is
centered around the AI topic.
Within the pubmed biblio-
graphic network, paper #16466
focuses on the subject of [TBR].
Within the pubmed bibliographic net-
work, paper #16466 focuses on the subject
of [GR(GL("pubmed"), "graph_bert:topic",
paper#16466)–>r].
((’GR’, [(’GL’, [’pubmed’]),
’graph_bert:topic’, ’paper#16466’]),
[True])
1 Within the pubmed biblio-
graphic network, paper #16466
focuses on the subject of 1.
Molecular
Graph
Reasoning
What is the function for the
protein molecular graph #138
in proteins?
The function for the protein molecular
graph #138 in proteins is [GR(GL("proteins"),
"seg_bert:molecule_function", instance#138)–>r].
((’GR’, [(’GL’, [’proteins’]),
’seg_bert:molecule_function’,
’instance#138’]), [True])
0 The function for the protein
molecular graph #138 in proteins
is 0.
In mutag, instance #30 of
the chemical molecular graph
demonstrates a function of
[TBR].
In mutag, instance #30 of the chemical
molecular graph demonstrates a function of
[GR(GL("mutag"), "seg_bert:molecule_function",
instance#30)–>r].
((’GR’, [(’GL’, [’mutag’]),
’seg_bert:molecule_function’,
’instance#30’]), [True])
2 In mutag, instance #30 of the
chemical molecular graph
demonstrates a function of 2.
For chemical molecular graph
instance #652 in nci1, its func-
tion is [TBR].
For chemical molecular graph instance
#652 in nci1, its function is [GR(GL("nci1"),
"seg_bert:molecule_function", instance#652)–>r].
((’GR’, [(’GL’, [’nci1’]),
’seg_bert:molecule_function’,
’instance#652’]), [True])
0 For chemical molecular graph in-
stance #652 in nci1, its function
is 0.
The chemical molecular graph
numbered 239 in ptc is charac-
terized by a function of [TBR].
The chemical molecular graph numbered 239 in
ptc is characterized by a function of [GR(GL("ptc"),
"seg_bert:molecule_function", instance#239)–>r].
((’GR’, [(’GL’, [’ptc’]),
’seg_bert:molecule_function’,
’instance#239’]), [True])
0 The chemical molecular graph
numbered 239 in ptc is charac-
terized by a function of 0.
Recommender
System
Reasoning
How likely user
#A1HOLE9R6WPT85
will be interested in item
#B00005MOTF in Amazon?
The likelihood that user #A1HOLE9R6WPT85
will be interested in item #B00005MOTF in Ama-
zon is [GR(GL("amazon"), "bpr:recommendation",
user#A1HOLE9R6WPT85, item#B00005MOTF)–
>r].
((’GR’, [(’GL’, [’amazon’]),
’bpr:recommendation’,
’user#A1HOLE9R6WPT85’,
’item#B00005MOTF’]), [True])
0.008 The likelihood that user
#A1HOLE9R6WPT85 will be in-
terested in item #B00005MOTF
in Amazon is 0.008.
The likelihood that user #u1527
will be interested in music
from artisit #i5422 in Last-fm
is [TBR].
The likelihood that user #u1527 will be in-
terested in music from artisit #i5422 in Last-
fm is [GR(GL("last-fm"), "bpr:recommendation",
user#u1527, artisit#i5422)–>r].
((’GR’, [(’GL’, [’last-fm’]),
’bpr:recommendation’,
’user#u1527’, ’artisit#i5422’]),
[True])
0.248 The likelihood that user #u1527
will be interested in music from
artisit #i5422 in Last-fm is 0.248.
In Movielens, the top 10 movies
that user #u272 likes include
[TBR].
In Movielens, the top 10 movies that user
#u272 likes include [GR(GL("movielens"),
"bpr:topk_recommendation", user#u272, 10)–>r].
((’GR’, [(’GL’, [’movielens’]),
’bpr:topk_recommendation’,
’user#u272’, ’10’]), [True])
[’i286’, ’i288’,
’i258’, ’i294’, ’i300’,
’i50’, ’i313’, ’i100’,
’i181’, ’i269’]
In Movielens, the top 10 movies
that user #u272 likes include
[’i286’, ’i288’, ’i258’, ’i294’, ’i300’,
’i50’, ’i313’, ’i100’, ’i181’, ’i269’].
Social
Network
Reasoning
In the online social network
foursquare, which community
is user user/1265481 involved
in?
In the online social network foursquare,
user user/1265481 is involved in the
[GR(GL("foursquare"), "kmeans:community",
user#user/1265481)–>r] communities formed by
users.
(’GR’, [(’GL’, [’foursquare’]),
’kmeans:community’,
’user#user/1265481’])
#2 In the online social network
foursquare, user user/1265481 is
involved in the #2 communities
formed by users.
In the online social network
twitter, are user #deeprogress
and user #alejandro1254 be-
long to the same community?
In the online social network twitter,
user #deeprogress and user #alejan-
dro1254 belong to [GR(GL("twitter"),
"kmeans:common_community_check",
user#deeprogress, user#alejandro1254)–>r]
community.
(’GR’, [(’GL’, [’twitter’]),
’kmeans:common_community_check’,
’user#deeprogress’,
’user#alejandro1254’])
The Same In the online social network twit-
ter, user #deeprogress and user
#alejandro1254 belong to the
same community.
Knowledge
Graph
Reasoning
According to the Freebase
knowledge graph, what
is the relation between
entity#/m/053yx and en-
tity#/m/015_1q?
According to the Freebase knowl-
edge graph, the relation between en-
tity#/m/053yx and entity#/m/015_1q is
[GR(GL("freebase"), "transe:relation", en-
tity#/m/053yx, entity#/m/015_1q)–>r].
(’GR’, [(’GL’, [’freebase’]),
’transe:relation’, ’entity#/m/053yx’,
’entity#/m/015_1q’])
/music/artist/labelAccording to the Freebase
knowledge graph, the relation
between entity#/m/053yx
and entity#/m/015_1q is
/music/artist/label.
According to the WordNet
knowledge graph, via relation
#_hypernym, what entity can
we obtain from entity #imagi-
nation.n.02?
According to the WordNet knowledge graph, via
relation #_hypernym, we can obtain entity #imag-
ination.n.02 from entity [GR(GL("wordnet"),
"transe:head_entity", relation#_hypernym,
entity#imagination.n.02)–>r].
(’GR’, [(’GL’, [’wordnet’]),
’transe:head_entity’, ’re-
lation#_hypernym’, ’en-
tity#imagination.n.02’])
chimera.n.02 According to the WordNet
knowledge graph, via relation
#_hypernym, we can obtain
entity #imagination.n.02 from
entity chimera.n.02.



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 15):

Jiawei Zhang
function for fine-tuning the LLM as
ℓ(D)= 1
|D|
∑︁
(w𝑖,¯w𝑖 )∈D
ℓ(ˆw𝑖, ¯w𝑖) (51)
= 1
|D|
∑︁
(w𝑖,¯w𝑖 )∈D
∑︁
𝑗
cross-entropy(ˆw𝑖(𝑗), ¯w𝑖(𝑗)). (52)
4.6 Graph Reasoning Q&A Prompts
What’s more, to provide Graph-ToolFormer with basic Q&A abil-
ity for graph reasoning, besides the above statements based graph
reasoning prompts, we will also design some Q&A based prompts
for fine-tuning Graph-ToolFormer as well. The graph reasoning
Q&A based prompts are created in a very similar way as above, but
we will replace the input statements with other reasoning ques-
tions instead, and the output will still be a statement with graph
reasoning API calls corresponding to the input question.
Formally, we also list of the graph reasoning Q&A prompt exam-
ples used in this paper as follows, which will be merged into the
previous prompts for fine-tuning Graph-ToolFormer. Different
from the previous input-output statement prompts (where the out-
put is almost a duplicated copy of the input but with API calls), the
inputs and outputs in question-answer prompts are not duplicated
copies of each other anymore. However, with the above autoregres-
sive generation of the desired output statement introduced before,
the Graph-ToolFormer using causal language models as the back-
bone is still capable to generate the desired output statements for
the input question queries.
1. Graph Property Reasoning Q&A Prompt Examples:
• Input: What is the order of the barbell
graph?
• Output: The order of the barbell
graph is [GR(GL("gpr", "barbell_graph"),
"toolx:order")–>r].
• Input: What is the size of the star graph?
• Output: The size of the star graph is
[GR(GL("gpr", "star_graph"), "toolx:size")–
>r].
• Input: What is the density of the dodecahe-
dral graph?
• Output: The density of dodecahedral graph
is [GR(GL("gpr", "dodecahedral_graph"),
"toolx:density")–>r].
• Input: What is the eccentricity of node
#25 in the balanced tree?
• Output: The eccentricity of node #25
in the balanced tree is [GR(GL("gpr",
"balanced_tree"), "toolx:eccentricity",
"node#25")–>r].
• Input: What is the radius of the lollipop
graph?
• Output: The radius of the lollipop
graph is [GR(GL("gpr", "lollipop_graph"),
"toolx:radius")–>r].
• Input: What is the center of the star
graph?
• Output: The center of the star graph in-
cludes node(s) [GR(GL("gpr", "star_graph"),
"toolx:center")–>r].
• Input: What is the length of shortest path
between node #5 and node #0 in the octahe-
dral graph?
• Output: In the octahedral graph, the
length of shortest path between node #5 and
node #0 is [GR(GL("gpr", "octahedral_graph"),
"toolx:shortest_path", "node#5", "node#0")–
>r].
• Input: What is the diameter of the bino-
mial tree?
• Output: The diameter of the binomial
tree is [GR(GL("gpr", "binomial_tree"),
"toolx:diameter")–>r].
• Input: What is the periphery of the house
x graph?
• Output: The periphery of the house
x graph includes node(s) [GR(GL("gpr",
"house_x_graph"), "toolx:periphery")–>r].
2. Bibliographic Network Reasoning Q&A Prompt Exam-
ples:
• Input: What is the topic of paper #83826
in the cora bibliographic network?
• Output: The topic of paper #83826 in the
cora bibliographic network is [GR(GL("cora"),
"graph_bert:topic", paper#83826)–>r].
• Input: What is the topic of paper #5832 in
the pubmed bibliographic network?
• Output: The topic of paper #5832 in
the pubmed bibliographic network is
[GR(GL("pubmed"), "graph_bert:topic",
paper#5832)–>r].
• Input: What is the topic of paper #3230 in
the citeseer bibliographic network?



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 16):

Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT
• Output: The topic of paper #3230 in
the citeseer bibliographic network is
[GR(GL("citeseer"), "graph_bert:topic",
paper#3230)–>r].
3. Molecular Graph Reasoning Q&A Prompt Examples:
• Input: What is the function for the pro-
tein molecular graph #138 in proteins?
• Output: The function for the
protein molecular graph #138 in
proteins is [GR(GL("proteins"),
"seg_bert:molecule_function", instance#138)–
>r].
• Input: What is the function for the chemi-
cal molecular graph #129 in mutag?
• Output: The function for the
chemical molecular graph #129
in mutag is [GR(GL("mutag"),
"seg_bert:molecule_function", instance#129)–
>r].
• Input: What is the function for the chemi-
cal molecular graph #322 in nci1?
• Output: The function for the chem-
ical molecular graph #322 in nci1 is
[GR(GL("nci1"), "seg_bert:molecule_function",
instance#322)–>r].
• Input: What is the function for the chemi-
cal molecular graph #44 in ptc?
• Output: The function for the chemical
molecular graph #44 in ptc is [GR(GL("ptc"),
"seg_bert:molecule_function", instance#44)–
>r].
4. Social Network Reasoning Q&A Prompt Examples:
• Input: In foursquare, what is the id of
user sparkey215’s community?
• Output: In foursquare, the id
of user sparkey215’s community is
[GR(GL("foursquare"), "kmeans:community",
user#sparkey215)–>r].
• Input: In the online social network
foursquare, are user #user/9674821 and user
#ljaniszewski8 belong to the same community?
• Output: In the online social net-
work foursquare, user #user/9674821
and user #ljaniszewski8 be-
long to [GR(GL("foursquare"),
"kmeans:common_community_check",
user#user/9674821, user#ljaniszewski8)–>r]
community.
5. Recommender System Reasoning Q&A Prompt Examples:
• Input: How likely user #A23E9QQHJLNGUI
will be interested in item #B004PIPG2A in
Amazon?
• Output: The likelihood that user
#A23E9QQHJLNGUI will be interested in item
#B004PIPG2A in Amazon is [GR(GL("amazon"),
"bpr:recommendation", user#A23E9QQHJLNGUI,
item#B004PIPG2A)–>r].
• Input: How likely user #u329 will be inter-
ested in music of artist #i8323 in Last-fm?
• Output: The likelihood that user
#u329 will be interested in music from
artist #i8323 in Last-fm is [GR(GL("last-
fm"), "bpr:recommendation", user#u329,
artist#i8323)–>r].
• Input: How likely user #u650 will be inter-
ested in movie #i671 in Movielens?
• Output: The likelihood that user
#u650 will be interested in movie #i671
in Movielens is [GR(GL("movielens"),
"bpr:recommendation", user#u650,
movie#i671)–>r].
6. Knowledge Graph Reasoning Q&A Prompt Examples:
• Input: According to the Freebase knowl-
edge graph, what is the relation between
entity#/m/053yx and entity#/m/015_1q?
• Output: According to the Freebase knowl-
edge graph, the relation between en-
tity#/m/053yx and entity#/m/015_1q is
[GR(GL("freebase"), "transe:relation", en-
tity#/m/053yx, entity#/m/015_1q)–>r].
• Input: According to the WordNet knowledge
graph, via relation #_hypernym, we derive
entity #imagination.n.02 from what entity?
• Output: According to the WordNet knowledge
graph, via relation #_hypernym, we can ob-
tain entity #imagination.n.02 from entity
[GR(GL("wordnet"), "transe:head_entity", re-
lation#_hypernym, entity#imagination.n.02)–
>r].



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 29):

Jiawei Zhang
Table 8: Analysis of generated API query cases different from ground-truth queries in the testing set.
Input
Sources
Language Generation Examples
Input Payload Generated Outputs Before Fine-Tuning Generated Outputs After Fine-Tuning
Pile
Data
Roman Catholic Diocese of Tambacounda\n\nThe Ro-
man Catholic Diocese of Tambacounda () is a diocese
located in the city of Tambacounda in the Ecclesiastical
province of Dakar in Senegal.
\n\nHistory\n\nThe Diocese of Tambacounda
was erected by Pope Pius XI on December 21,
1933, on territory split off from the Diocese of
Saint-Louis du Sénégal. \n\nBishops\n\nDiocese
of Tambacounda \nErected: December 21,
1933\nMetropolitan: Archdiocese of Dakar\n
center>The Roman Catholic Diocese of Tambacounda
() is a diocese located in the city of Tambacounda in
the <center>Ecclesiastical province of Dakar in Sene-
gal.</center> center>Output: </center> center>The
<center>Roman</center> <center>Catholic
Mud Hens pitcher Evan Reed charged with sexual
assault\n\nMud Hens pitcher Evan Reed was charged
July 30 with sexual assault related to a March incident
in Detroit when he was a member of the Detroit Tigers.
\n\nMud Hens pitcher Evan Reed was charged July 30
with sexual assault related to a March incident in Detroit
when he was a member of the Detroit Tigers.\n\nMud
Hens pitcher Evan Reed was charged July 30 with sexual
assault related to a March incident in Detroit when he
was a member of the Detroit Tigers.\n\nMud Hens
pitcher Evan Reed was charged July 30 with sexual
assault related to
center>###<centercentercentercentercentercentercenter
centercentercentercentercentercentercentercenter
centercentercentercentercentercentercentercenter
centercentercentercentercentercentercentercenter
centercentercentercentercenter
Random
Sources
Thursday’s verdict only added to the intrigue surround-
ing the gravest legal and political unknown from the
2021 Capitol insurrection that hangs over the 2024 cam-
paign: will Donald Trump, the president who inspired
the uprising, face his own legal and political price?
I don’t think there’s any question that he’s going to be
impeached, ” Rep. Al Green (D-TX) told The Daily Beast.
“I think he’s going to be impeached, and I think he’s
going to be removed from office. ”\n\nGreen’s
will be the verdict only added to the intrigue surround-
ing the gravest legal and political unknown from the
2021 Capitol insurrection that hangs over the 2024 cam-
paign: will Donald Trump, the president who inspired
the uprising, face his own legal and political price?br>
A source of debate for centuries, the village of Ponte
Buriano, a suburb of Arezzo in the Tuscany region of
Italy, is so convinced the bridge behind Mona Lisa is
the Ponte Buriano that they’ve made it a key feature of
their local tourism campaign, even laying claim to the
bridge on the village’s welcome sign.
\n\nThe Ponte Buriano is one of the oldest bridges in
Tuscany, dating back to the 12th century. It’s also one of
the most famous, thanks to Leonardo da Vinci’s Mona
Lisa, the most famous painting in the world.\n\nThe
The village of [GR(GL("mona_lisa"), "bpr:search",
GR(GL("ponte_buriano"), GR(GL("search"),
GR(GL("mona_lisa"), GR(GL("search
•Efficiency: With LoRA and quantized models/optimizers,
we can reduce the model fine-tuning memory capacity re-
quirement to less than 11GB and the memory capacity re-
quirement even lower for the model inference stage. Mean-
while, integrated with the large-sized graph data, pre-trained
graph models, and necessary pre-processed data, the effi-
ciency of Graph-ToolFormer for various graph reasoning
task can still be a problem. In this paper, we introduce a
tentative approach to make the problem less severe with
the working memory. However, if we plan to deployGraph-
ToolFormer on devices with very small memories, like cell-
phones or embedded equipments, new techniques will still
be needed to improve the model learning and inference effi-
ciency.
•Diverse Applications : Due to the limited space, we can
only study a few number of the graph reasoning tasks with
Graph-ToolFormer in this paper. Meanwhile, in the real-
world, we have lots of graph structured data that may require
the LLMs to handle them to reason for the desired outputs.
Therefore, a very promising future work direction is to apply
Graph-ToolFormer to study diverse real-world graph/net-
work data oriented reasoning tasks with LLMs. We list a
few of them here just for the readers’ information, and the
readers may explore more diverse reasoning tasks according
to your own backgrounds and expertises.
– Urban Computing and Smart City : In the offline world,
we have extensively connected traffic networks that bridge
different local communities, cities and countries by lo-
cal roads, national highways, international fights and
ocean freight corridors. Applying LLMs for knowledge
extraction and reasoning based on such traffic networks
is critical for the current urban computing and smart city
projects.
– IoT and Smart Home : Assisted with the 5G, the IoT net-
work effectively bridges the cyber world with the physical
devices and equipments together via extremely fast com-
munication channels. The LLMs provide the opportunity
for us to utilize language models as the general interface
for controlling the devices within the IoT networks, which
is also the main objective for building the smart home
system.
– Healthcare : During the past years, the world has suf-
fered a lot from the covid-19 pandemic. Similar to the
protein molecules studied in this paper, both the virus
and the vaccines can also be represented as the molecular
graphs. LLMs with the molecular graph reasoning ability
have the potential to improve our current healthcare sys-
tem in many perspectives, like early identification of virus ,
analysis of the virus pathogenicity and creation of vaccines .
What’s more, the LLMs with the social network reasoning
ability will also help infer the potential virus propagation
among people , early prediction of highly infectious commu-
nities and identify rumors and misinformation about the
pandemic (at the online social networks).



### Claim 153/179

#### Claim Text
These reactors operate under extremely harsh conditions, characterized by high temperatures and intense radiation fields, necessitating structural materials with greater radiation tolerance than those used in current fission reactors to resist radiation-induced degradation .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[3]_2310.20158.pdf (Page 6):

Table 4: RRR configuration (Temperature=0 for LLMs).
Component Model Details
Rewriter g GPT -4 token limit 20
Retriever f BM25 Lin et al. (2021)
Relevance σ GPT -3.5-Turbo score ∈ [1, 2, . . . ,5]
Re-ranker h GPT -3.5-Turbo+ GPT -4 Sun et al. (2023)
(A) Datasets and metrics: We present quantitative evaluations ofRRR on two standard IR bench-
marks: BEIR (Thakur et al. (2021)) and TREC-DL (Craswell et al. (2020a;b)). Due to resource
constraints (LLM calls), we perform evaluations on subsets of these benchmarks as listed below
(each input query costs approximately 1USD to complete; cost breakdown in Appendix B).
BEIR is a benchmark for comprehensive zero-shot evaluation of models across a diverse spectrum
of IR tasks. We take 8 datasets with relatively small test sets (listed in Table 1) out the 18 total
available.
TREC-DL is a dedicated deep-learning track within the Text Retrieval Conference (TREC) encom-
passing document retrieval and passage retrieval tasks. We report results on the passage retrieval
dataset TREC-DL20, with 54 queries and 8.8M documents. For hyperparameter tuning, we use the
TREC-DL19 dataset with 43 queries; see (D) below.
We report nDCG@10 and Recall@100 metrics.
(B) Compared techniques: We compare with several zero-shot IR methods: dense passage re-
trieval DPR (Karpukhin et al., 2020), ANCE (Xiong et al., 2020), TAS-B (Hofst¨atter et al., 2021),
sparse retrieval BM25 (Robertson & Zaragoza, 2009)), document re-ranking monoT5 (Nogueira
et al., 2020), RankGPT (Sun et al., 2023), and data generation/augmentation Promptagator (Dai
et al., 2022). For all the baselines, we quote results from Thakur et al. (2021) and Sun et al. (2023)
computed in identical setting.
(C) Implementation details: We run all our experiments on a single machine with an A-100 GPU,
24 CPU cores and 220 GB of memory. We useGPT -3.5-Turbo(OpenAI (2022)), with a token limit
of 4097, and GPT -4(OpenAI (2023)), with a token limit of 8192.
The configurations for different components of the RRR algorithm are listed in Table 4. For the
retriever f, we use the BM25 model from the Pyserini (Lin et al., 2021) package. The re-ranker
h follows the two-step process described in the RankGPT paper (Sun et al., 2023): first, re-rank N
documents in S (after Step 15 of Algorithm 1) using GPT -3.5-Turbo; then, further re-rank the top
30 documents using GPT -4.
(D) Hyperparameter selection: We perform hyperparameter tuning on the TREC-DL19 dataset.
First, we tune the number of rewritesNrw and the relevance thresholdτ by optimizing Recall@100.
Secondly, to optimize nDCG@10, we tune the sliding window constants w (window size) and s
(step size) in the re-ranker h. The best values for these parameters are Nrw = 5, τ = 1 (i.e., prune the
least scoring ones), w = 10 and s = 5. See Appendix C for details.
5 R ESULTS
We focus on the following questions in our evaluations:
(1) End-to-end performance: How effective isRRR compared to baseline and SOTA zero-shot IR
techniques on the benchmarks introduced in Section 4?
(2) Effectiveness of feedback loop: How does RRR perform with more rewrites?
(3) Ablative study: How do the design choices in the proposed method impact performance?
5.1 E ND-TO-END PERFORMANCE
We show the performance metrics for the compared methods on theBEIR datasets in Tables 1 and 2.
Our method outperforms all the baselines and SOTA zero-shot IR techniques on 6 out of 8 datasets,
7



Source: data\tc16_2312.10997v5\referenced_papers\[61]_2212.10509.pdf (Page 13):

Model HpQA Br HpQA 2WikiMQA MQ 2H MQ
InterAug (Lazaridou et al., 2022) − | − 30.3 | − − | − − | − − | −
RECITE (Sun et al., 2022) − | − 37.1 | 48.4 − | − − | − − | −
ReAct (Yao et al., 2022) − | − 35.1 | − − | − − | − − | −
SelfAsk (Press et al., 2022) − | − − | − 40.1 | − 15.2 | − − | −
DecomP (Khot et al., 2022) − | 50.0 − | − − | 59.3 − | − − | −
DecomP (Khot et al., 2023) * − | − − | 53.5 − | 70.8 − | − − | 30.9
DSP (Khattab et al., 2023) * − | − 51.4 | 62.9 − | − − | − − | −
IRCoT QA (ours) 45.8 | 58.5 49.3 | 60.7 57.7 | 68.0 34.2 | 43.8 26.5 | 36.5
Table 3: Extended comparison with published LLM-based ODQA systems (as of May 25, 2023) on EM and
F1 scores (with new numbers marked with *). ‘ −’: score is unavailable. HpQA Br: Bridge questions subset of
HotpotQA. MQ2H: MuSiQue 2-hop questions. IRCoT remains SOTA for MuSiQue and is close to SOTA for
HotpotQA and 2WikiMultihopQA. Note the comparisons here are not head-to-head as discussed in the text.
Flan-T5-XXL GPT3
Model HotpotQA 2WikiMQA MuSiQue IIRC HotpotQA 2WikiMQA MuSiQue IIRC
ZeroR QA Direct 25.3± 0.3 32.7± 0.3 13.7± 0.3 28.9± 0.3 41.0± 1.1 38.5± 1.1 19.0± 1.2 40.9± 0.7
CoT 22.9 ± 0.1 31.7± 1.5 10.3± 0.5 24.4± 0.1 47.5± 0.4 41.2± 1.0 25.2± 1.2 52.1± 0.1
OneR QA Direct 49.7± 0.5 51.2± 0.3 25.8± 0.6 40.0± 1.3 50.7± 0.1 46.4± 2.9 20.4± 0.3 40.1± 0.9
CoT 43.1 ± 0.7 47.8± 0.9 17.6± 0.2 34.5± 1.5 53.6± 0.7 54.8± 2.1 29.4± 0.8 49.8± 2.3
IRCoT QA Direct 59.1± 0.9 66.5± 1.4 30.8± 0.2 42.5± 2.1 60.6± 1.0 63.5± 2.7 36.0± 0.5 47.9± 2.3
CoT 52.0 ± 0.6 55.1± 1.0 24.9± 1.0 36.5± 1.3 60.7± 1.1 68.0± 1.5 36.5± 1.2 49.9± 1.1
Table 4: Answer F1 for different ODQA models made from NoR, One and IRCoT retrievals, and Direct and
CoT prompting readers. For Flan-T5-XXL, Direct prompting is a better choice for the reader, and for GPT3, CoT
prompting is a better choice for the reader. Hence, we make different reader choices for Flan-T5 and GPT3 for the
experiments in the main paper. Note that IRCoT QA > OneR QA > ZeroR QA holds up regardless of this choice.
DSP (Khattab et al., 2023) provides a way to pro-
grammatically define interactions between LLM
and retrieval for ODQA (e.g., via question decom-
position), bootstrap demonstrations for such a pro-
gram, and use them to make the answer prediction.
It uses GPT3.5 LLM with ColBERT-based retrieval.
Since most of these methods use different knowl-
edge sources or APIs and are built using different
LLMs and retrieval models, it’s difficult to make a
fair scientific comparison across these systems. Ad-
ditionally, the evaluations in the respective papers
are on different random subsets (from the same
distribution) of test instances.
Despite these differences, it is still informative to
explore, in a leaderboard-style fashion, howIRCoT
performs relative to the best numbers published
for these recent systems. Table 3 shows results
from different systems, including contemporane-
ous and newer numbers. The two new systems in
this table (relative to Table 1) are DecomP (newer
version) and DSP. While IRCoT remains SOTA on
MuSiQue, DSP outperforms it on HotpotQA by 2.0
points and the newer version of Decomp outper-
forms IRCoT on 2WikiMultihopQA by 2.8 points.
We speculate DecomP performs well on 2WikiMul-
tihopQA because it has only a few easy-to-predict
decomposition patterns, which DecomP’s question
decomposition can leverage. The lack of such pat-
terns in HotpotQA and MuSiQue causes it to un-
derperform compared to IRCoT. Lastly, it will be
useful to assess whether DSP, which is hardcoded
for 2-hop questions like that of HotpotQA, will
work well for a dataset with a varied number of
hops like that of MuSiQue. We leave this further
investigation to future work.
D Additional CoT Generation Examples
Table 5 provides illustrations, in addition to the
ones provided in Table 2, for how the CoT gen-
erations for NoR QA, OneR QA, and IRCoT QA
methods vary. This gives an insight into how IR-
CoT improves QA performance. Since NoR re-
lies completely on parametric knowledge, it often
makes a factual error in the first sentence, which de-
rails the full reasoning chain. Some of this factual
information can be fixed by OneR, especially infor-
mation closest to the question (i.e., can be retrieved
using the question). This is insufficient for fixing



Source: data\tc16_2312.10997v5\referenced_papers\[98]_2307.03172.pdf (Page 0):

Lost in the Middle: How Language Models Use Long Contexts
Nelson F. Liu1∗ Kevin Lin2 John Hewitt1 Ashwin Paranjape3
Michele Bevilacqua3 Fabio Petroni3 Percy Liang1
1Stanford University 2University of California, Berkeley 3Samaya AI
nfliu@cs.stanford.edu
Abstract
While recent language models have the abil-
ity to take long contexts as input, relatively
little is known about how well they use
longer context. We analyze the performance
of language models on two tasks that require
identifying relevant information in their in-
put contexts: multi-document question an-
swering and key-value retrieval. We find that
performance can degrade significantly when
changing the position of relevant informa-
tion, indicating that current language models
do not robustly make use of information in
long input contexts. In particular, we observe
that performance is often highest when rele-
vant information occurs at the beginning or
end of the input context, and significantly
degrades when models must access relevant
information in the middle of long contexts,
even for explicitly long-context models. Our
analysis provides a better understanding of
how language models use their input context
and provides new evaluation protocols for
future long-context language models.
1 Introduction
Language models have become an important and
flexible building block in a variety of user-facing
language technologies, including conversational
interfaces, search and summarization, and collabo-
rative writing (Shuster et al., 2022; Thoppilan et al.,
2022; Lee et al., 2022, inter alia). These models
perform downstream tasks primarily via prompting:
all relevant task specification and data to process is
formatted as a textual input context, and the model
returns a generated text completion. These input
contexts can contain thousands of tokens, espe-
cially when language models are used to process
long documents (e.g., legal or scientific documents,
conversation histories, etc.) or when language mod-
els are augmented with external information (e.g.,
*Work partially completed as an intern at Samaya AI.
1st 5th 10th 15th 20th
Position of Document with the Answer
55
60
65
70
75Accuracy
20 T otal Retrieved Documents (~4K tokens)
gpt-3.5-turbo-0613
gpt-3.5-turbo-0613 (closed-book)
Figure 1: Changing the location of relevant information
(in this case, the position of the passage that answers an
input question) within the language model’s input con-
text results in a U-shaped performance curve—models
are better at using relevant information that occurs at the
very beginning (primacy bias) or end of its input context
(recency bias), and performance degrades significantly
when models must access and use information located
in the middle of its input context.
relevant documents from a search engine, database
query results, etc; Petroni et al., 2020; Ram et al.,
2023; Shi et al., 2023; Mallen et al., 2023; Schick
et al., 2023, inter alia).
Handling these use-cases requires language mod-
els to successfully operate over long sequences. Ex-
isting language models are generally implemented
with Transformers (Vaswani et al., 2017), which re-
quire memory and compute that increases quadrat-
ically in sequence length. As a result, Trans-
former language models were often trained with
relatively small context windows (between 512-
2048 tokens). Recent improvements in hardware
(e.g., faster GPUs with more memory) and algo-
rithms (Dai et al., 2019; Dao et al., 2022; Poli et al.,
arXiv:2307.03172v3  [cs.CL]  20 Nov 2023



Source: data\tc16_2312.10997v5\referenced_papers\[61]_2212.10509.pdf (Page 6):

Figure 5: Retrieval recall for OneR and IRCoT using Flan-T5-XXL (Left) and GPT3 (Right) in out-of-distribution
(OOD) setting. HQ (HotpotQA), 2W (2WikiMultihopQA), MQ (MuSiQue). The result X →Y indicates prompt
demonstrations are from dataset X and evaluation is on dataset Y .IRCoT outperforms OneR in such an OOD setting.
Figure 6: Answer F1 for NoR QA, OneR QA and IRCoT QA using Flan-T5-XXL (Left) and GPT3 (Right) in
out-of-distribution (OOD) setting. HQ (HotpotQA), 2W (2WikiMultihopQA), MQ (MuSiQue). The result X→Y
indicates prompt demonstrations are from dataset X and evaluation is on dataset Y . IRCoTQA outperforms OneR
QA and NoR QA in such OOD setting.
Figure 7: Number of questions, out of 40, where CoT
generated by GPT3 using different methods has at least
1 factual error. Factual errors: IRCoT < OneR < NoR.
of the facts11 is not true.12 As Fig. 7 shows, NoR
makes the most factual errors, OneR makes fewer,
11all sentences before the final “answer is:” sentence.
12Note that factual error doesn’t necessarily mean the pre-
dicted answer is incorrect and vice-versa. This is because the
model can generate a wrong answer despite all correct facts,
and vice-versa. We also account for the possibility of answer
annotation errors in the original datasets.
and IRCoT the least. In particular, IRCoT reduces
the factual errors over OneR by 50% on HotpotQA
and 40% on 2WikiMultihopQA.
Table 2 illustrates how the CoT predictions for
different methods vary qualitatively. Since NoR
relies completely on parametric knowledge, it often
makes a factual error in the first sentence, which
derails the full CoT. OneR can retrieve relevant
information closest to the question and is less likely
to make such errors early on, but it still makes
errors later in the CoT. IRCoT, on the other hand,
is often able to prevent such errors in each step.
IRCoT is also effective for smaller models.To
see how effective IRCoT is at different LM sizes,
we show the scaling plots in Fig. 8. 13 We com-
pare the recall for OneR andIRCoT using Flan-T5
{base (0.2B), large (0.7B), XL (3B), XXL (11B)},
and GPT3 code-davinci-002 (175B). IRCoT
with even the smallest model (0.2B) is better than
13We skip IIRC here as the smaller models are not good at
identifying Wikipedia titles from a paragraph and a question
which is necessary for IIRC (see App. B).



Source: data\tc16_2312.10997v5\referenced_papers\[61]_2212.10509.pdf (Page 7):

Figure 8: Retrieval recall for OneR (bottom) and IRCoT (top) for LMs of increasing sizes: Flan-T5 {base (0.2B),
large (0.7B), XL (3B), XXL (11B)} and GPT3 (175B) on HotpotQA, 2WikiMultihopQA, MuSiQue. IRCoT
outperforms OneR for all model sizes, including the 0.3B model, and the difference roughly grows with model size.
Note: OneR doesn’t use LM in its retrieval and so has a fixed score.
Figure 9: Answer F1 for ODQA models made using OneR (bottom) and IRCoT (top) for LMs of increasing sizes:
Flan-T5 {base (0.2B), large (0.7B), XL (3B), XXL (11B)} and GPT3 (175B) on HotpotQA, 2WikiMultihopQA and
MuSiQue. IRCoT QA outperforms OneR QA for all model sizes except for the smallest, 0.3B. IRCoT with 3B
model even outperforms OneR with 58X larger GPT3 model showing the value of improved retrieval.
OneR, and the performance roughly improves with
the model size. This shows the CoT generation
capabilities of even small models can be leveraged
for improving retrieval. Furthermore, we show the
effect of model size on the QA score in Fig. 9. For
all sizes except the smallest (0.2B), we see IRCoT
QA is better than OneR QA. Moreover, IRCoT
with a 3B model even outperforms OneR and NoR
with a 58X larger 175B GPT3 model in all datasets.
IRCoT is SOTA for few-shot multistep ODQA.14
We compare IRCoT QA with five recent ap-
proaches to using LLMs for ODQA: Internet-
Augmented QA (Lazaridou et al., 2022), RE-
CITE (Sun et al., 2022) ReAct (Yao et al., 2022),
SelfAsk (Press et al., 2022), and DecomP (Khot
et al., 2022). Although these are not head-to-head
comparisons as different methods use different
APIs, knowledge sources, and even LLMs (see
App. C for details), it is still informative to ex-
plore, in a leaderboard-style fashion, how IRCoT
performs relative to the best numbers published for
these recent systems.
14App. §C reports updated SOTA numbers, including con-
temporaneous and newer works.
Model HpQA Br HpQA 2WikiMQA MQ 2H
InterAug − | − 30.3 | − − | − − | −
RECITE − | − 37.1 | 48.4 − | − − | −
ReAct − | − 35.1 | − − | − − | −
SelfAsk − | − − | − 40.1 | − 15.2 | −
DecomP − | 50.0 − | − − | 59.3 − | −
IRCoT QA 45.8 | 58.5 49.3 | 60.7 57.7 | 68.0 34.2 | 43.8
Table 1: Comparison with other LLM-based ODQA
systems on EM and F1 scores. ‘ −’: score is unavail-
able. HpQABr: Bridge questions subset of HotpotQA.
MQ2H: MuSiQue 2-hop questions. IRCoT QA with
GPT3 (ours) outperforms other systems by a large mar-
gin. Note: Comparisons aren’t head-to-head as dis-
cussed in the text. App. §C reports updated SOTA num-
bers, including contemporaneous and newer works.
As shown in Table 1, IRCoT QA significantly
outperforms all of these recent systems by a large
margin, setting a new state of the art in terms of
what’s achievable via retrieval-augmented LLMs
(without supervised training).
6 Conclusions
Chain-of-thought prompting has significantly im-
proved LLMs’ ability to perform multi-step reason-



### Claim 154/179

#### Claim Text
I NTRODUCTION The global outbreak of COVID-19 has disrupted the order in all countries and communities, posing a significant threat to human life and property –.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[37]_2211.07067.pdf (Page 14):

Conﬂict.Demonstrate.DemonstrateWithViolence Demonstrator who is demonstrating agent?
Regulator who is the regulator?
VisualDisplay what is the visual display?
Topic what is the topic for the demonstration?
Target who is the target of the demonstration?
Place where the demonstration takes place?
Conﬂict.Demonstrate.Unspeciﬁed Demonstrator who is demonstrating agent?
Regulator who is the regulator?
VisualDisplay what is the visual display?
Topic what is the topic for the demonstration?
Target who is the target of the demonstration?
Place where the demonstration takes place?
Conﬂict:Attack Attacker Who is the attacking agent?
Instrument What is the instrument used in the attack?
Place Where the attack takes place?
Target Who is the target of the attack?
Victim Who is the target of the attack?
Conﬂict:Demonstrate Entity Who is demonstrating agent?
Place Where the demonstration takes place?
Contact.Contact.Broadcast Communicator who is communicating agents?
Recipient who is the recipient?
Instrument What is the instrument used in the communication?
Topic what is the communicating topic?
Place Where it takes place?
Contact.Contact.Correspondence Participant who is communicating agents?
Instrument What is the instrument used in the communication?
Topic what is the communicating topic?
Place Where it takes place?
Contact.Contact.Meet Participant Who are meeting?
Topic what is the topic of the meeting
Place Where the meeting takes place?
Contact.Contact.Unspeciﬁed Participant who is communicating agents?
Topic what is the communicating topic?
Place Where it takes place?
Contact.Prevarication.Unspeciﬁed Communicator who is communicating agents?
Recipient who is communicating agents?
Topic what is the communicating topic?
Place Where it takes place?
Contact.RequestCommand.Unspeciﬁed Communicator who is communicating agents?
Recipient who is communicating agents?
Topic what is the communicating topic?
Place Where it takes place?
Contact.ThreatenCoerce.Unspeciﬁed Communicator who is communicating agents?
Recipient who is communicating agents?
Topic what is the communicating topic?
Place Where it takes place?
Contact:Meet Entity Who are meeting?
Place Where the meeting takes place?
Contact:Phone-Write Entity Who is communicating agents?
Place Where it takes place?
Control.ImpedeInterfereWith.Unspeciﬁed Impeder who is the impeder agent?
ImpededEvent what is the impede event?
Place where the impede takes place?
Disaster.Crash.Unspeciﬁed DriverPassenger Who is responsible for the transport event?
Vehicle What is the vehicle used to transport the person or artifact?
CrashObject what is being crashed into?
Place where the transport takes place?
Disaster.DiseaseOutbreak.Unspeciﬁed Disease what broke out?
Victim Who is the harmed person?
Place Where the disease takes place?
Disaster.FireExplosion.Unspeciﬁed FireExplosionObject what caught ﬁre?
Instrument What is the instrument used in the explosion?
Place where the explosion takes place?
GenericCrime.GenericCrime.GenericCrime Perpetrator who committed a crime?
Victim Who is the target of the crime?
Place Where the crime takes place?



Source: data\tc16_2312.10997v5\referenced_papers\[37]_2211.07067.pdf (Page 16):

Justice:Trial-Hearing Adjudicator Who is the judge or court?
Defendant Who is on trial?
Place Where the trial takes place?
Prosecutor Who is the prosecuting agent?
Life.Consume.Unspeciﬁed ConsumingEntity what is the consuming agent?
ConsumedThing what is consumed?
Place where the consuming takes place?
Life.Die.Unspeciﬁed Victim Who died?
Place Where the death takes place?
Killer Who is the attacking agent?
MedicalIssue what is the medical issue
Life.Illness.Unspeciﬁed Victim who is victim?
DeliberateInjurer who is the deliberate injurer
Disease what is the disease or sickness?
Place where the event takes place?
Life.Infect.Unspeciﬁed Victim who is victim?
InfectingAgent who infected?
Source what is the infect from?
Place where the event takes place?
Life.Injure.Unspeciﬁed Victim Who is the harmed person?
Injurer Who is the attacking agent?
Instrument What is the device used to inﬂict the harm?
BodyPart what is the body part being harmed?
MedicalCondition what is the medical issue?
Place Where the injuring takes place?
Life:Be-Born Person Who is born?
Place Where the birth takes place?
Life:Die Agent Who is the attacking agent?
Instrument What is the device used to kill?
Place Where the death takes place?
Victim Who died?
Life:Divorce Person Who are divorced?
Place Where the divorce takes place?
Life:Injure Agent Who is the attacking agent?
Instrument What is the device used to inﬂict the harm?
Place Where the injuring takes place?
Victim Who is the harmed person?
Life:Marry Person Who are married?
Place Where the marriage takes place?
Medical.Diagnosis.Unspeciﬁed Treater who diagnosed the patient?
Patient who is diagnosed?
SymptomSign what is the symptom?
MedicalCondition what is the medical condition?
Place where the event takes place?
Medical.Intervention.Unspeciﬁed Treater what treated the patient?
Patient who is treated?
MedicalIssue what is the medical issue?
Instrument What is the instrument used in the treatment?
Place Where the treatment takes place?
Medical.Vaccinate.Unspeciﬁed Treater what treated the patient?
Patient who is treated?
VaccineTarget who is the target of the vaccination?
VaccineMethod what is the method of the vaccination?
Place Where the vaccination takes place?
Movement.Transportation.Evacuation Transporter Who is responsible for the transport event?
PassengerArtifact Who is being transported?
Vehicle What is the vehicle used to transport the person or artifact?
Origin Where the transporting originated?
Destination Where the transporting is directed?
Movement.Transportation.IllegalTransportation Transporter Who is responsible for the transport event?
PassengerArtifact Who is being transported?
Vehicle What is the vehicle used to transport the person or artifact?
Origin Where the transporting originated?
Destination Where the transporting is directed?
Movement.Transportation.PreventPassage Transporter Who is responsible for the transport event?
PassengerArtifact Who is being transported?
Vehicle What is the vehicle used to transport the person or artifact?
Preventer who is preventing the transport?
Origin Where the transporting originated?
Destination Where the transporting is directed?
Movement.Transportation.Unspeciﬁed Transporter Who is responsible for the transport event?
PassengerArtifact Who is being transported?
Vehicle What is the vehicle used to transport the person or artifact?
Origin Where the transporting originated?
Destination Where the transporting is directed?



Source: data\tc16_2312.10997v5\referenced_papers\[36]_2303.08559.pdf (Page 28):

Table 21: Templates for ACE05 dataset, where {evt} is the placeholder for event type.
Event Template
no-event The word {evt} does not trigger any known event.
Movement.Transport The word {evt} triggers a TRANSPORT event: an ARTIFACT (WEAPON or
VEHICLE) or a PERSON is moved from one PLACE (GEOPOLITICAL ENTITY ,
FACILITY , LOCATION) to another.
Personnel.Elect The word {evt} triggers an ELECT event which implies an election.
Personnel.Start-Position The word {evt} triggers a START-POSITION event: a PERSON elected or appointed
begins working for (or changes offices within) an ORGANIZATION or GOVERN-
MENT.
Personnel.Nominate The word {evt} triggers a NOMINATE event: a PERSON is proposed for a position
through official channels.
Conflict.Attack The word {evt} triggers an ATTACK event: a violent physical act causing harm or
damage.
Personnel.End-Position The word {evt} triggers an END-POSITION event: a PERSON stops working for
(or changes offices within) an ORGANIZATION or GOVERNMENT.
Contact.Meet The word {evt} triggers a MEET event: two or more entities come together at a
single location and interact with one another face-to-face.
Life.Marry The word {evt} triggers a MARRY event: two people are married under the legal
definition.
Contact.Phone-Write The word {evt} triggers a PHONE-WRITE event: two or more people directly
engage in discussion which does not take place ’face-to-face’.
Transaction.Transfer-Money The word {evt} triggers a TRANSFER-MONEY event: giving, receiving, borrowing,
or lending money when it is NOT in the context of purchasing something.
Justice.Sue The word {evt} triggers a SUE event: a court proceeding has been initiated for the
purposes of determining the liability of a PERSON, ORGANIZATION or GEOPO-
LITICAL ENTITY accused of committing a crime or neglecting a commitment
Conflict.Demonstrate The word {evt} triggers a DEMONSTRATE event: a large number of people come
together in a public area to protest or demand some sort of official action. For eample:
protests, sit-ins, strikes and riots.
Business.End-Org The word {evt} triggers an END-ORG event: an ORGANIZATION ceases to exist
(in other words, goes out of business).
Life.Injure The word {evt} triggers an INJURE event: a PERSON gets/got injured whether it
occurs accidentally, intentionally or even self-inflicted.
Life.Die The word {evt} triggers a DIE event: a PERSON dies/died whether it occurs acci-
dentally, intentionally or even self-inflicted.
Justice.Arrest-Jail The word {evt} triggers a ARREST-JAIL event: a PERSON is sent to prison.
Transaction.Transfer-
Ownership
The word {evt} triggers a TRANSFER-OWNERSHIP event: The buying, selling,
loaning, borrowing, giving, or receiving of artifacts or organizations by an individual
or organization.
Justice.Execute The word {evt} triggers an EXECUTE event: a PERSON is/was executed
Justice.Trial-Hearing The word {evt} triggers a TRIAL-HEARING event: a court proceeding has been
initiated for the purposes of determining the guilt or innocence of a PERSON,
ORGANIZATION or GEOPOLITICAL ENTITY accused of committing a crime.
Justice.Sentence The word {evt} triggers a SENTENCE event: the punishment for the DEFENDANT
is issued
Life.Be-Born The word {evt} triggers a BE-BORN event: a PERSON is given birth to.
Justice.Charge-Indict The word {evt} triggers a CHARGE-INDICT event: a PERSON, ORGANIZATION
or GEOPOLITICAL ENTITY is accused of a crime
Business.Start-Org The word {evt} triggers a START-ORG event: a new ORGANIZATION is created.
Justice.Convict The word {evt} trigges a CONVICT event: a PERSON, ORGANIZATION or
GEOPOLITICAL ENTITY is convicted whenever it has been found guilty of a
CRIME.
Business.Declare-Bankruptcy The word {evt} triggers a DECLARE-BANKRUPTCY event: an Entity officially
requests legal protection from debt collection due to an extremely negative balance
sheet.
Justice.Release-Parole The word {evt} triggers a RELEASE-PAROLE event.



Source: data\tc16_2312.10997v5\referenced_papers\[103]_2303.08559.pdf (Page 28):

Table 21: Templates for ACE05 dataset, where {evt} is the placeholder for event type.
Event Template
no-event The word {evt} does not trigger any known event.
Movement.Transport The word {evt} triggers a TRANSPORT event: an ARTIFACT (WEAPON or
VEHICLE) or a PERSON is moved from one PLACE (GEOPOLITICAL ENTITY ,
FACILITY , LOCATION) to another.
Personnel.Elect The word {evt} triggers an ELECT event which implies an election.
Personnel.Start-Position The word {evt} triggers a START-POSITION event: a PERSON elected or appointed
begins working for (or changes offices within) an ORGANIZATION or GOVERN-
MENT.
Personnel.Nominate The word {evt} triggers a NOMINATE event: a PERSON is proposed for a position
through official channels.
Conflict.Attack The word {evt} triggers an ATTACK event: a violent physical act causing harm or
damage.
Personnel.End-Position The word {evt} triggers an END-POSITION event: a PERSON stops working for
(or changes offices within) an ORGANIZATION or GOVERNMENT.
Contact.Meet The word {evt} triggers a MEET event: two or more entities come together at a
single location and interact with one another face-to-face.
Life.Marry The word {evt} triggers a MARRY event: two people are married under the legal
definition.
Contact.Phone-Write The word {evt} triggers a PHONE-WRITE event: two or more people directly
engage in discussion which does not take place ’face-to-face’.
Transaction.Transfer-Money The word {evt} triggers a TRANSFER-MONEY event: giving, receiving, borrowing,
or lending money when it is NOT in the context of purchasing something.
Justice.Sue The word {evt} triggers a SUE event: a court proceeding has been initiated for the
purposes of determining the liability of a PERSON, ORGANIZATION or GEOPO-
LITICAL ENTITY accused of committing a crime or neglecting a commitment
Conflict.Demonstrate The word {evt} triggers a DEMONSTRATE event: a large number of people come
together in a public area to protest or demand some sort of official action. For eample:
protests, sit-ins, strikes and riots.
Business.End-Org The word {evt} triggers an END-ORG event: an ORGANIZATION ceases to exist
(in other words, goes out of business).
Life.Injure The word {evt} triggers an INJURE event: a PERSON gets/got injured whether it
occurs accidentally, intentionally or even self-inflicted.
Life.Die The word {evt} triggers a DIE event: a PERSON dies/died whether it occurs acci-
dentally, intentionally or even self-inflicted.
Justice.Arrest-Jail The word {evt} triggers a ARREST-JAIL event: a PERSON is sent to prison.
Transaction.Transfer-
Ownership
The word {evt} triggers a TRANSFER-OWNERSHIP event: The buying, selling,
loaning, borrowing, giving, or receiving of artifacts or organizations by an individual
or organization.
Justice.Execute The word {evt} triggers an EXECUTE event: a PERSON is/was executed
Justice.Trial-Hearing The word {evt} triggers a TRIAL-HEARING event: a court proceeding has been
initiated for the purposes of determining the guilt or innocence of a PERSON,
ORGANIZATION or GEOPOLITICAL ENTITY accused of committing a crime.
Justice.Sentence The word {evt} triggers a SENTENCE event: the punishment for the DEFENDANT
is issued
Life.Be-Born The word {evt} triggers a BE-BORN event: a PERSON is given birth to.
Justice.Charge-Indict The word {evt} triggers a CHARGE-INDICT event: a PERSON, ORGANIZATION
or GEOPOLITICAL ENTITY is accused of a crime
Business.Start-Org The word {evt} triggers a START-ORG event: a new ORGANIZATION is created.
Justice.Convict The word {evt} trigges a CONVICT event: a PERSON, ORGANIZATION or
GEOPOLITICAL ENTITY is convicted whenever it has been found guilty of a
CRIME.
Business.Declare-Bankruptcy The word {evt} triggers a DECLARE-BANKRUPTCY event: an Entity officially
requests legal protection from debt collection due to an extremely negative balance
sheet.
Justice.Release-Parole The word {evt} triggers a RELEASE-PAROLE event.



Source: data\tc16_2312.10997v5\referenced_papers\[35]_2402.13482.pdf (Page 16):

Table 9: The example question-answer pairs generated from different models on Covid QA.
Types Samples
Augment
w/ Seed Data
Context: polymerase chain reaction testing, the time lag between hospitalization and
reporting was longer for early cases compared with that of more recent cases. Among the
seven locations reporting importation, the total volume of inbound passengers from China
was m = 63.1 million per year in 2017 [9] , of which 100q = 2.1% were from Wuhan [10] ,
a home of n = 19.0 million people as the catchment population of Wuhan airport.
Generated Question: What was the total volume of inbound passengers from China to
the seven locations reporting importation in 2017?
Generated Answer: 63.1 million per year
Self-Instruct
Context: polymerase chain reaction testing, the time lag between hospitalization and
reporting was longer for early cases compared with that of more recent cases. Among the
seven locations reporting importation, the total volume of inbound passengers from China
was m = 63.1 million per year in 2017 [9] , of which 100q = 2.1% were from Wuhan [10] ,
a home of n = 19.0 million people as the catchment population of Wuhan airport.
Generated Question: What is the total volume of inbound passengers from China?
Generated Answer: m = 63.1 million per year in 2017
RADA (Ours)
Context: MYTH: The flu is just a bad cold. Influenza may cause bad cold symptoms,
like sore throat, runny nose, sneezing, hoarseness, and cough. But in the United States
alone, 36,000 people die and more than 200,000 are hospitalized each year because of the
flu. During the 2017/18 flu season, flu activity has significantly increased throughout the
majority of the country with the A(H3N2) viruses predominating so far, according to the
Centers for Disease Control (CDC).
Generated Question: What are the consequences of getting the flu?
Generated Answer: 36,000 people die and more than 200,000 are hospitalized each year
because of the flu.
Table 10: The example question-answer pairs generated from different models on Policy QA.
Types Samples
Augment
w/ Seed Data
Context: IP Addresses. We use your IP address to help diagnose problems with our
server, and to administer our Web site. Your IP address is used to help identify you and,
in the case of the Coffee Review Marketplace, your shopping cart and to gather broad
demographic information in the interest of improving our site and service.
Generated Question: How does the company use IP addresses?
Generated Answer: to help diagnose problems with our server, and to administer our
Web site.
Self-Instruct
Context: IP Addresses. We use your IP address to help diagnose problems with our
server, and to administer our Web site. Your IP address is used to help identify you and,
in the case of the Coffee Review Marketplace, your shopping cart and to gather broad
demographic information in the interest of improving our site and service.
Generated Question: What information does the company use IP addresses for?
Generated Answer: to help diagnose problems with our server, and to administer our
Web site. Your IP address is used to help identify you and, in the case of the Coffee
Review Marketplace, your shopping cart and to gather broad demographic information in
the interest of improving our site and service.
RADA (Ours)
Context: The 1998 Data Protection Act sets the rules for how UK businesses can gather,
store and use people confidential data. The Act is organised around eight principles of
data protection. They include the idea that personal data should be obtained and processed
fairly and lawfully, that only the right data should be used, that data should be accurate
and so on. Principle Seven covers data security. It states:
Generated Question: What is the main purpose of the 1998 Data Protection Act?
Generated Answer: The 1998 Data Protection Act sets the rules for how UK businesses
can gather, store and use people confidential data.



### Claim 155/179

#### Claim Text
Additionally, these shift measurements are very prone to errors in the beam parameters and the detection geometry .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 5):

Preprint
log (512)
Figure 3: Log token length distribution
of the GitHub Issue Similarity Dataset.
Table 1: Statistics of the proposed GitHub Issues Similar-
ity Dataset. #Positive denotes the count of positive pairs,
and #Negative represents the number of negative pairs.
Split Train Validation Test
#Positive 9457 774 807
#Negative 9108 773 741
Total 18565 1547 1548
4.2 I MPLEMENTATION DETAILS
In this paper, we use the pre-trained uncased BERT base model (110M parameters) as the backbone
model. For a fair comparison, all BERT-based baselines also adopt this setting. We set the value
of τ for the cosine objective and the in-batch negative objective to 0.05, based on prior research.
Additionally, we determined the value of τ for the angle objective to be 1.0 through grid search.
4.3 M AIN RESULTS
In this section, we will first introduce the baselines, then the results of the transfer STS tasks, then
the results of the non-transfer STS tasks, and finally a summary.
Baselines We compare our proposed model with widely used baselines, encompassing both un-
supervised and supervised models. The unsupervised models are average GloVe (Pennington et al.,
2014), BERT-flow (Li et al., 2020), BERT-whitening (Su et al., 2021), LLaMA2 (Touvron et al.,
2023b), and contrastive learning models including IS-BERT (Zhang et al., 2020), CT-BERT (Carls-
son et al., 2020), SimCSE (Gao et al., 2021), ConSERT (Yan et al., 2021), and DiffCSE (Chuang
et al., 2022). On the other hand, the chosen supervised models are InferSent (Conneau et al., 2017),
USE (Cer et al., 2018), SBERT (Reimers & Gurevych, 2019), CoSENT (Su, 2022), as well as su-
pervised versions of SimCSE and ConSERT.
Transfer STS TasksFor a fair comparison, we train AnglE with the NLI datasets MNLI (Williams
et al., 2018) and SNLI (Bowman et al., 2015) and then transfer it to evaluate seven STS benchmark
datasets. The evaluation results are presented in Table 2. It is evident that AnglE-BERT and AnglE-
LLaMA consistently outperform the baselines with a gain of 0.80% and 0.72% in average score,
respectively, over the previous SOTA SimCSE-BERT and SimCSE-LLaMA. Note that supervised
SBERT and CoSENT show lower results than other unsupervised contrastive learning models like
SimCSE and DiffCSE. This difference might arise from the difference in data distributions between
the training and test data in the transfer STS tasks. They struggle to effectively generalize to STS
tasks when trained solely with NLI datasets. In contrast, contrastive learning models exhibit better
generalization capabilities due to their alignment and uniformity features. Because AnglE optimizes
both the supervised cosine objective and the in-batch negative objective. This can allow AnglE to
generalize well in transfer STS tasks. Additionally, the angle optimization in AnglE mitigates the
negative impact of the saturation zone in the cosine function to produce better performance than
other baselines.
Non-transfer STS Tasks To provide a comprehensive analysis, we also evaluate the performance
of the baselines in the non-transfer setting. We train the baselines on the train set and evaluate them
on the test or validation set. Two typical models, SimCSE and SBERT, representing contrastive and
supervised learning, are compared with our model. The results of the non-transfer STS tasks are
listed in Table 3, where we evaluate the baselines on four short-text datasets (MRPC, STS-B, QQP,
and QNLI) and one long-text dataset (GitHub Issues Similarity Dataset). SimCSE notably performs
poorly compared to SBERT and AnglE in the non-transfer setting. This is due to the limitation of
the small-scale training set, as there are not enough samples for SimCSE to effectively learn repre-
sentations. Furthermore, the datasets only provide pair-supervised data, namely (x, x+) or (x, x−),
which prevents SimCSE from utilizing its hard negative objective that relies on triple-supervised
6



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 13):

0 1000 2000 3000 4000
24.5
25
25.5
26
26.5
27
FLOPS (MACs)
Rouge-L
2%
(a) ELI5
0 1000 2000 3000 4000
20
21
22
23
24
25
FLOPS (MACs)
2% (b) MS MARCO
0 1000 2000 3000 4000
24
25
26
27
28
29
30
31
32
FLOPS (MACs)
2% (c) NQ
0 20 40 60 80 100
24.5
25
25.5
26
26.5
27
Input P sgs
Rouge-L
2%
(d) ELI5
0 20 40 60 80 100
20
21
22
23
24
25
Input P sgs
2% (e) MS MARCO
0 20 40 60 80 100
24
25
26
27
28
29
30
31
32
FiD
CALM
T ok en Filtering
Combined
Input P sgs
2% (f) NQ
Figure 6: The ROUGE-L performance on FiD-Base vs. the FLOPS (MACs) (First row), and vs. the input passage
amount (Input Psg., second row). The results are on the test set for each dataset, for the various methods utilized.
We observe that the trends of the FLOPS (MACs) and the Input Psg. are very similar, since the passages effect the
encoder the most, and the encoder has the most impact on FLOPS (MACs) (as stated by de Jong et al. (2022)).



Source: data\tc16_2312.10997v5\referenced_papers\[54]_2401.14887.pdf (Page 6):

The Power of Noise: Redefining Retrieval for RAG Systems SIGIR ’24, July 14–18, 2024, Washington, DC, USA
Table 3: Accuracy of Llama2-7b in configurations involving random Wikipedia documents and retrieved documents [I, , , Q].
Rows denote the number of random documents added, and columns show the quantity of retrieved documents . The left
section reports results using Contriever, and the right section using BM25. Scenarios where the prompt exceeded the model’s
input limit, leading to potential data truncation, are not included ( - ). Each value not marked with an asterisk * represents a
statistically significant change from the base case of retrieved documents only [I, , Q] (first row), as determined by a Wilcoxon
test (p-value < 0.01).
Contriever BM25
#
# 1 2 3 4 5 8 10 1 2 3 4 5 8 10
0 0.1620 0.1866 0.1876 0.1866 0.1921 0.2198 0.2108 0.2008 0.2208 0.2084 0.2028 0.2243 0.2492 0.2447
1 0.1308 0.1616 0.1717 0.1893* 0.1987* 0.2153* 0.2146* 0.1568 0.1963 0.1921 0.2115 0.2295* 0.2475* 0.2506*
2 0.1315 0.1644 0.1859* 0.2008 0.2174 0.2156* 0.2368 0.1644 0.1973 0.2080* 0.2281 0.2558 0.2495* 0.2596
3 0.1301 0.1727 0.2008 0.2316 0.2201 0.2198 0.2409 0.1568 0.2063 0.2160 0.2520 0.2579 0.2644 0.2707
5 0.1464 0.2056 0.2233 0.2240 0.2150 0.2451 0.2482 0.1772 0.2402 0.2437 0.2520 0.2554 0.2804 0.2866
8 0.1734 0.2066 0.2336 0.2375 0.2454 0.2416 0.2364 0.1994 0.2451 0.2579 0.2769 0.2817 0.2859 0.2777
10 0.1796 0.2174 0.2450 0.2502 0.2499 0.2420 - 0.2108 0.2589 0.2734 0.2835 0.2935 0.2853 -
15 0.2018 0.2354 0.2551 0.2530 - - - 0.2243 0.2686 0.2790 0.2928 - - -
16 0.2032 0.2471 0.2558 - - - - 0.2323 0.2662 0.2838 - - - -
17 0.2039 0.2426 - - - - - 0.2326 0.2693 - - - - -
18 0.2073 - - - - - - 0.2309 - - - - - -
Figure 3: This heatmap depicts the attention distribution
across the context documents from the example shown in
Figure 2, relative to the answer generated by Llama2-7b in
a prompt structured as [I, /u♀k, ⋆, Q]. Cell (i, j) denotes the
mean attention that tokens in the generated answer allocate
to the tokens of the i-th document within the j-th attention
layer. This mean attention for each document is calculated
by averaging the attention scores across all its constituent
tokens.
in the presence of noise, as can be seen in Table 2. Instead, we ob-
serve an improvement in performance under the best-performing
setting (near [I, , ⋆, Q]), with an improvement of 0.08 (+36%) in
LLM Input - Random and Gold ⋆
Task instruction...
Documents:
Document [140](Title: Richard Yates (novelist)) For much
of his life, Yates’s work met almost universal critical ac-
claim, yet not one of his books sold over 12,000 copies in...
Document [242] (Title: Android version history) Code
name Version number Initial release date API level Security
patches (No codename ) 1.0 September 23...
Document [3](Title: Millennium Falcon) Han Solo won
the Millennium Falcon from Lando Calrissian in the card
game sabacc...
Question: who owned the millennium falcon be-
fore han solo
Answer: Lando Calrissian
Figure 4: Example LLM input with a correct output, high-
lighted in green. The context of the prompt is composed of
random documents and the gold near the query. The task
instruction is as in Figure 1.
the case of MPT. Furthermore, we observe that different models
exhibit distinct behaviors. Both Llama2 and Phi-2 showed improve-
ments in this setting when the noise is introduced furthest from
the query. However, when the noise is positioned in the far[I, ⋆,
, Q] and mid [I, , ⋆, , Q] settings, these models exhibit a
decline in performance. Notably, this performance degradation is
much less accentuated when compared to the earlier setting with
distracting documents. This suggests that while Llama2 and Phi-2
can effectively handle noise far from the query, their ability to sift



Source: data\tc16_2312.10997v5\referenced_papers\[54]_2401.14887.pdf (Page 5):

SIGIR ’24, July 14–18, 2024, Washington, DC, USA Cuconasu and Trappolini, et al.
Table 1: Accuracy results of the LLMs when evaluated with prompts composed of the gold document ⋆and a varying number
of distracting /u♀kdocuments. The table illustrates how the inclusion of an increasing number of distracting documents affects
LLM’s performance. Scenarios where the prompt exceeded the model’s input limit, leading to potential data truncation, are not
included ( - ). All values not marked with an asterisk * denote statistically significant changes from the gold-only document
scenario [I, ⋆, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01). Additionally, the closed-book accuracy scores
for the models are as follows: Llama2 (0.1123), MPT (0.1205), Phi-2 (0.0488), Falcon (0.1083).
Far - [I, ⋆, /u♀k, Q] Mid - [I, /u♀k, ⋆, /u♀k, Q] Near - [I, /u♀k, ⋆, Q]
# /u♀kLlama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon
0 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330
1 0.4586 0.1976 0.3585 0.3469 no-mid no-mid no-mid no-mid 0.4283 0.1791 0.4227 0.3602
2 0.3455 0.1913 0.3430 0.3246 0.3322 0.1802 0.3375 0.2823 0.3974 0.2002 0.3975 0.3111
4 0.2745 0.2209* 0.3019 0.2670 0.2857 0.1775 0.2885 0.2378 0.3795 0.2059* 0.3701 0.2736
6 0.2898 0.2171* 0.2943 0.2392 0.2698 0.1424 0.2625 0.2103 0.3880 0.1892 0.3623 0.2656
8 0.2643 0.2077* 0.2513 0.1878 0.2268 0.1002 0.2360 0.1745 0.3748 0.1944 0.3423 0.2424
10 0.2537 - - - 0.2180 - - - 0.3716 - - -
12 0.2688 - - - 0.2382 - - - 0.3991 - - -
14 0.2583 - - - 0.2280 - - - 0.4118 - - -
16 0.2413 - - - 0.2024 - - - 0.3889 - - -
18 0.2348 - - - 0.1795 - - - 0.3781 - - -
Table 2: Accuracy results of the LLMs when evaluated with prompts composed of the gold document ⋆and a varying number
of random documents. Surprisingly, increasing the number of random documents in the Near setting improves LLM’s
performance. Scenarios where the prompt exceeded the model’s input limit, leading to potential data truncation, are not
included ( - ). All values not marked with an asterisk * denote statistically significant changes from the gold-only document
scenario [I, ⋆, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01). Additionally, the closed-book accuracy scores
for the models are as follows: Llama2 (0.1123), MPT (0.1205), Phi-2 (0.0488), Falcon (0.1083).
Far - [I, ⋆, , Q] Mid - [I, , ⋆, , Q] Near - [I, , ⋆, Q]
# Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon
0 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330
1 0.4733 0.2447 0.4329 0.4035 no-mid no-mid no-mid no-mid 0.4862 0.2125* 0.4587 0.4091
2 0.3776 0.2639 0.4249 0.3805 0.3928 0.2584 0.4293 0.3612 0.5032 0.2660 0.4614 0.3912
4 0.3109 0.2933 0.4091 0.3468 0.3998 0.2577 0.3985 0.3462 0.5221 0.2930 0.4311 0.3949
6 0.3547 0.3036 0.4130 0.3250 0.4138 0.2265 0.3891 0.3196 0.5681* 0.2890 0.4388 0.3908
8 0.3106 0.3039 0.3812 0.2543 0.3734 0.1566 0.3596 0.2767 0.5609* 0.2911 0.4258 0.3704
10 0.3390 - - - 0.3675 - - - 0.5579* - - -
12 0.3736 - - - 0.3641 - - - 0.5836 - - -
14 0.3527 - - - 0.3372 - - - 0.5859 - - -
16 0.3401 - - - 0.3159 - - - 0.5722 - - -
18 0.3466 - - - 0.2982 - - - 0.5588* - - -
impact on the model’s effectiveness. We define the positions of the
gold document as follows:
•Near: placed adjacent to the query in the prompt [I, /u♀k, ⋆,
Q] (as in Figure 2)
•Mid: inserted in the middle of the context [I, /u♀k, ⋆, /u♀k, Q]
•Far: positioned as far as possible from the query in the con-
text [I, ⋆, /u♀k, Q]
Results in these settings partially corroborate evidence from [30].
The accuracy is higher when the gold document is near the query,
lower when the gold document is furthest from it, and lowest when
the gold document is placed in the middle of the context. For in-
stance, Llama2, with 18 distracting documents, reaches an accuracy
of 0.37, 0.23, and 0.17, respectively. These results are consistent
across all models tested in the setting with distracting documents.
5.3 Impact of Noise
We devise an additional experimental setting aimed at evaluating
the robustness of the RAG system against noise. To this effect, we
take the gold document and add to it a certain number of docu-
ments picked at random from the corpus; see an example in Figure
4. Against our expectations, the performance does not deteriorate



### Claim 156/179

#### Claim Text
In the preceeding sections, we elaborated on IBM, DLM and Boltzmann-based approaches, but we did not discuss many other methods such as ghost node , cut-cell and overset grid methods for the sake of conciseness.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[84]_2402.07630.pdf (Page 14):

• Zero-CoT. Zero-shot Chain-of-thought (Zero-CoT) prompting [18] is a follow-up to CoT
prompting [45], which introduces an incredibly simple zero shot prompt by appending the
words "Let’s think step by step." to the end of a question.
• CoT-BAG. Build-a-Graph Prompting (BAG) [44] is a prompting technique that adds "Let’s
construct a graph with the nodes and edges first." after the textual description of the graph is
explicitly given.
• KAPING. KAPING [1] is a zero-shot knowledge-augmented prompting method for knowl-
edge graph question answering. It first retrieves triples related to the question from the
graph, then prepends them to the input question in the form of a prompt, which is then
forwarded to LLMs to generate the answer.
2) Frozen LLM w/ prompt tuning (PT): Keeping the parameters of the LLM frozen and adapting only
the prompt. This includes soft prompt tuning (see Figure 5a), GraphToken [ 31], which is a graph
prompt tuning method, and our G-Retriever method (see Figure5b).
LLM
(Self Attention Layers)
LLM
(Text Embedder)
prompt
bieberjaxon
frozen
trainable
node_id, node_attr
0,p!nk
1,gender
2,1club.fm: power
...
src, edge_attr, dst
0,is_reviewed,1
2,artist,0
3,contributions,4
...
Question: What is the name
of justin bieber brother?
Answer:
(a) Prompt Tuning
Graph Encoder
LLM
(Self Attention Layers)
LLM
(Text Embedder)
bieberjaxon
Projection
node_id, node_attr
15, justin bieber
294, jaxon bieber
356, jeremy bieber
551, m.0gxnnwp
src, edge_attr, dst
294, parents, 356
356, children, 15
551, sibling, 294
551, sibling, 15
Question: What is the name
of justin bieber brother?
Answer:
frozen
trainable (b) G-Retriever
Figure 5: Model configuration 2) Frozen LLM w/ prompt tuning.
3) Tuned LLM: Fine-tuning the LLM with LoRA. This includes standard fine-tuning of an LLM for
downstream tasks using LoRA (see Figure 6a) and G-Retriever with LoRA (see Figure 6b).
LLM
(Self Attention Layers)
LLM
(Text Embedder)
bieberjaxon
trainable
node_id, node_attr
0,p!nk
1,gender
2,1club.fm: power
...
src, edge_attr, dst
0,is_reviewed,1
2,artist,0
3,contributions,4
...
Question: What is the name
of justin bieber brother?
Answer:
(a) LoRA
Graph Encoder
LLM
(Self Attention Layers)
LLM
(Text Embedder)
bieberjaxon
Projection
node_id, node_attr
15, justin bieber
294, jaxon bieber
356, jeremy bieber
551, m.0gxnnwp
src, edge_attr, dst
294, parents, 356
356, children, 15
551, sibling, 294
551, sibling, 15
Question: What is the name
of justin bieber brother?
Answer:
trainable
 (b) G-Retriever w/ LoRA
Figure 6: Model configuration 3) Tuned LLM.
B.3 Details of Ablation Study
This section illustrates the modifications made to the original architecture in the ablation study, as
presented in Figure 7.
Without Graph Encoder (w/o GraphEncoder): In this setting, we replaced the graph encoder with
trainable soft tokens, setting the number of these virtual tokens to 10.
Without Projection Layer (w/o Projection Layer):Here, we removed the projection layer following
the graph encoder. We configured the output dimension of the graph encoder to be 4,096, matching the
hidden dimension of Llama2-7b. This allows the output graph token (the yellow token in Figure 7b)
to be concatenated directly with the LLM tokens (blue tokens).
Without Textualized Graph (w/o Textualized Graph): In this configuration, we modified the
textual input to the LLM. Rather than using a combination of the question and the textualized graph,
we solely used the question.
15



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 22):

103 104 105
Sc × [L(N, D) L(N, )] 1/ S
103
104
105
Sstop
Early Stopping Step
Data Size
21M
43M
86M
172M
344M
688M
1.4B
103 104 105
Step
2
3
4
5
6Loss
Test Loss
Train Loss
108
109
1010
Dataset Size (Tokens)
Figure 16 Left: We characterize the step on which early stopping occurs, as a function of the extent of
overﬁtting. The red line indicates a lower bound for early stopping that is derived in Section 5.3. Right:
We display train and test loss for a series of 300M parameter models trained on different sized dataset sub-
samples. The test loss typically follows that of a run done with unrestricted data until diverging. Note that the
degree of overﬁtting (as compared to the inﬁnite data limit) is signiﬁcantly overestimated by Ltest −Ltrain
(denoted by a black bar for each run).
• We are not especially conﬁdent in the prediction of Bcrit(L) for values of the loss far outside the
range we have explored. Changes in Bcrit could have a signiﬁcant impact on trade-offs between
data parallelism and the number of serial training steps required, which would have a major impact
on training time.
• We did not thoroughly investigate the small data regime, and our ﬁts for L(N,D) were poor for
the smallest values of D (where an epoch corresponded to only 40 steps). Furthermore, we did
not experiment with regularization and data augmentation. Improvements in these could alter our
results, quantitatively or qualitatively.
• We used the estimated training compute C ≈6NBS, which did not include contributions propor-
tional to nctx (see Section 2.1). So our scalings with compute may be confounded in practice in the
regime of very large nctx, speciﬁcally where nctx ≳ 12dmodel.
• We tuned learning rates, and we experimented with learning rate schedules. But we may have
neglected to tune some hyperparameter (e.g. intialization scale or momentum) that have an important
effect on scaling.
• The optimal choice of learning rate is sensitive to the target loss. When training close to convergence,
it may be necessary to use a smaller learning rate to avoid divergences. But when conducting a short
training run (eg due to compute limitations), it may be possible to use a larger learning rate. We did
not experiment with higher learning rates for training runs that did not proceed to convergence.
D Supplemental Figures
D.1 Early Stopping and Test vs Train
In section 5.3 we described the result shown in Figure 16, which provides a prediction for a lower bound on
the early stopping step. We also show the train and test loss for a given model size when training on different
sized datasets.
D.2 Universal Transformers
We compare the performance of standard Transformers to recurrent Transformers [DGV +18] in Figure 17.
These models re-use parameters, and so perform slightly better as a function of N, but slightly worse as a
function of compute C. We include several different different possibilities for parameter re-use.
D.3 Batch Size
We measure the critical batch size using the data displayed in ﬁgure 18. This made it possible to estimate
Bcrit(L) in ﬁgure 10.
23



Source: data\tc16_2312.10997v5\referenced_papers\[84]_2402.07630.pdf (Page 15):

LLM
(Self Attention Layers)
LLM
(Text Embedder)
prompt
bieberjaxon
node_id, node_attr
15, justin bieber
294, jaxon bieber
356, jeremy bieber
551, m.0gxnnwp
src, edge_attr, dst
294, parents, 356
356, children, 15
551, sibling, 294
551, sibling, 15
Question: What is the name
of justin bieber brother?
Answer:
frozen
trainable
(a) w/o GraphEncoder
Graph Encoder
LLM
(Self Attention Layers)
LLM
(Text Embedder)
bieberjaxon
node_id, node_attr
15, justin bieber
294, jaxon bieber
356, jeremy bieber
551, m.0gxnnwp
src, edge_attr, dst
294, parents, 356
356, children, 15
551, sibling, 294
551, sibling, 15
Question: What is the name
of justin bieber brother?
Answer:
frozen
trainable (b) w/o Projection Layer
Graph Encoder
LLM
(Self Attention Layers)
LLM
(Text Embedder)
bieberjaxon
Projection
frozen
trainable
Question: What is the name
of justin bieber brother?
Answer: (c) w/o Textualized Graph
Figure 7: Ablation study configurations.
B.4 The Choice of Graph Encoder
In addition to the Graph Transformer [ 37], we explore other GNNs as the graph encoder, such
as GCN [ 17] and the GAT [ 43]. The comparative results of these models on the WebQSP and
ExplaGraphs datasets are presented in Table 7.
Table 7: Performance of different graph encoders on the WebQSP and ExplaGraphs datasets.
Graph Encoder WebQSP ExplaGraphs
GCN [17] 70.70 0.8394
GAT [43] 70.27 0.8430
Graph Transformer [37] 70.49 0.8516
The results demonstrate that our proposed method exhibits consistent robustness across different graph
encoders. Notably, all three encoders – GCN, GAT, and GraphTransformer – demonstrate competitive
and closely aligned performance on the WebQSP dataset, with Hit@1 scores of 70.70, 70.27, and
70.49, respectively. However, the performance differentiation becomes more pronounced on the
ExplaGraphs dataset, where GraphTransformer exhibits a superior Hit@1 score of 0.8516, followed
by GAT and GCN with scores of 0.8430 and 0.8394, respectively. This variation in performance
across the datasets highlights the importance of encoder selection based on the specific characteristics
and requirements of the dataset.
B.5 The Choice of LLM
As for the choice of LLM, we considered both Llama2-7b and Llama2-13b. Our experiments
demonstrate that stronger LLMs enhance the effectiveness of our method, as shown in Table 8,
indicating that it benefits from the increased scale of the LLMs.
Table 8: Performance of different LLMs on the WebQSP dataset.
LLM Llama2-7b Llama2-13b
Hit@1 70.49 75.58
C GraphQA Benchmark
In this section, we detail how our GraphQA benchmark differs from the original datasets, including
the specific processing steps we employed. For concrete examples that illustrate the differences
between the raw text in the original dataset and in our GraphQA benchmark, please refer to Table 9.
ExplaGraphs. The original dataset2 [35] represents relationships using triplets. We have standard-
ized this format by converting the triplets into a graph representation. Specifically, each head and tail
in a triplet is transformed into a node, and the relation is transformed into an edge. Since the test
dataset labels are not available, we have utilized only the training and validation (val) datasets from
the original collection. We further divided these into training, val, and test subsets, using a 6:2:2 ratio.
2https://explagraphs.github.io/
16



Source: data\tc16_2312.10997v5\referenced_papers\[84]_2402.07630.pdf (Page 13):

A Impact Statements
As LLMs are applied to increasingly diverse tasks, their ability to process complex structured data
will be increasingly vital. Our work aims to enhance LLMs’ ability to interact with graph-structured
data, while resisting hallucination, thus improving model reliability. We also enhance explainability,
both by returning the retrieved subgraph, and through the use of conversational interfaces for ‘chatting
with a graph’, which allows for better human-AI interaction and for models to behave in a way that is
more well-aligned with human expectations.
B Experiment
B.1 Implementation Settings
Experiments are conducted using 2 NVIDIA A100-80G GPUs. Each experiment is replicated four
times, utilizing different seeds for each run to ensure robustness and reproducibility.
Graph Encoder. We use Graph Transformer [37] as the GNN backbone. Our configuration employs
4 layers, each with 4 attention heads, and a hidden dimension size of 1024.
LLM. We use the open-sourced Llama2-7b [42] as the LLM backbone. In fine-tuning the LLM with
LoRA [10], the lora_r parameter (dimension for LoRA update matrices) is set to 8, and lora_alpha
(scaling factor) is set to 16. The dropout rate is set to 0.05. In prompt tuning, the LLM is configured
with 10 virtual tokens. The number of max text length is 512, the number of max new tokens, i.e., the
maximum numbers of tokens to generate, is 32.
PCST. For retrieval over graphs via PCST, for theSceneGraphs dataset, we select the top k nodes
and edges, setting k to 3. Here, the cost of edges, denoted as Ce, is set to 1. Regarding the WebQSP
dataset, we set k = 3 for nodes and k = 5 for edges, with the edge cost, Ce, adjusted to 0.5.
For the ExplaGraphs dataset, which is characterized by a small graph size averaging 5.17 nodes
and 4.25 edges (as detailed in Table 2), the entire graph can fit in the LLM’s context window size.
Consequently, we aim to retrieve the whole graph by setting k to 0, effectively returning the original
graph unaltered.
Optimization. We use the AdamW [27] optimizer. We set the initial learning rate at 1e-5, with a
weight decay of 0.05. The learning rate decays with a half-cycle cosine decay after the warm-up
period. The batch size is 4, and the number of epochs is 10. To prevent overfitting and ensure training
efficiency, an early stopping mechanism is implemented with a patience setting of 2 epochs.
B.2 Details of Model Configurations
In our experiments, we consider three model configurations:
1) Inference-only: Using a frozen LLM for direct question answering with textual graph and question,
see Figure 4.
LLM
(Self Attention Layers)
LLM
(Text Embedder)
node_id, node_attr
0,p!nk
1,gender
2,1club.fm: power
...
src, edge_attr, dst
0,is_reviewed,1
2,artist,0
3,contributions,4
...
Question: What is the name
of justin bieber brother?
Answer:
frozen
Figure 4: Model configuration 1) Inference-only.
• Zero-shot. In this approach, the model is given a textual graph description and a task
description, and is immediately asked to produce the desired output. No additional examples
or demonstrations are provided.
14



Source: data\tc16_2312.10997v5\referenced_papers\[172]_2309.17453.pdf (Page 2):

Published as a conference paper at ICLR 2024
Layer 0 Head 0 Layer 1 Head 0 Layer 2 Head 0
Layer 23 Head 0 Layer 31 Head 0
Layer 16 Head 0Layer 9 Head 0
Figure 2: Visualization of the average attention logits in Llama-2-7B over 256 sentences, each with a length
of 16. Observations include: (1) The attention maps in the first two layers (layers 0 and 1) exhibit the "local"
pattern, with recent tokens receiving more attention. (2) Beyond the bottom two layers, the model heavily attends
to the initial token across all layers and heads.
Furthermore, we confirm our attention sink hypothesis and demonstrate that language models can
be pre-trained to require only a single attention sink token for streaming deployment. Specifically,
we suggest that an extra learnable token at the beginning of all training samples can serve as a
designated attention sink. By pre-training 160-million parameter language models from scratch, we
demonstrate that adding this single sink token preserves the model’s performance in streaming cases.
This stands in contrast to vanilla models, which necessitate the reintroduction of multiple initial
tokens as attention sinks to achieve the same performance level.
Finally, we emphasize that StreamingLLM efficiently generates coherent text from tokens within
the KV cache without extending the LLMs’ context length. It suits continuous operation needs with
minimal memory use and past data reliance. Additionally, StreamingLLM can complement context
extension methods to increase the attendable recent context.
2 R ELATED WORK
Extensive research has been done on applying LLMs to lengthy texts, with three main areas of focus:
Length Extrapolation, Context Window Extension, and Improving LLMs’ Utilization of Long
Text. While seemingly related, it’s worth noting that progress in one direction doesn’t necessarily
lead to progress in the other. For example, extending the context size of LLMs doesn’t improve the
model’s performance beyond the context size, and neither approach ensures effective use of the long
context. Our StreamingLLM framework primarily lies in the first category, where LLMs are applied
to text significantly exceeding the pre-training window size, potentially even of infinite length. We do
not expand the attention window size of LLMs or enhance the model’s memory and usage on long
texts. The last two categories are orthogonal to our focus and could be integrated with our techniques.
Length extrapolationaims to enable language models trained on shorter texts to handle longer
ones during testing. A predominant avenue of research targets the development of relative position
encoding methods for Transformer models, enabling them to function beyond their training window.
One such initiative is Rotary Position Embeddings (RoPE) (Su et al., 2021), which transforms
the queries and keys in every attention layer for relative position integration. Despite its promise,
subsequent research (Press et al., 2022; Chen et al., 2023) indicated its underperformance on text
that exceeds the training window. Another approach, ALiBi (Press et al., 2022), biases the query-key
attention scores based on their distance, thereby introducing relative positional information. While
this exhibited improved extrapolation, our tests on MPT models highlighted a breakdown when the
text length was vastly greater than the training length. Current methodologies, however, have yet to
achieve infinite length extrapolation, causing no existing LLMs to fit for streaming applications.
Context Window Extensioncenters on expanding the LLMs’ context window, enabling the process-
ing of more tokens in one forward pass. A primary line of work addresses the training efficiency
problem. Given the attention to computation’s quadratic complexity during training, developing
a long-context LLM is both a computational and memory challenge. Solutions have ranged from
system-focused optimizations like FlashAttention (Dao et al., 2022; Dao, 2023), which accelerates
attention computation and reduces memory footprint, to approximate attention methods (Zaheer
et al., 2020b; Beltagy et al., 2020; Wang et al., 2020; Kitaev et al., 2020) that trade model quality for
efficiency. Recently, there has been a surge of work on extending pre-trained LLMs with RoPE (Chen
et al., 2023; kaiokendev, 2023; bloc97, 2023; Peng et al., 2023), involving position interpolation and
fine-tuning. However, all the aforementioned techniques only extend LLMs’ context window to a
limited extent, which falls short of our paper’s primary concern of handling limitless inputs.
3



### Claim 157/179

#### Claim Text
In recent years, simplicial complexes have been extensively studied and applied in various research fields, such as collaborative networks, semantic networks, cellular networks, and brain networks , , .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 29):

Jiawei Zhang
Table 8: Analysis of generated API query cases different from ground-truth queries in the testing set.
Input
Sources
Language Generation Examples
Input Payload Generated Outputs Before Fine-Tuning Generated Outputs After Fine-Tuning
Pile
Data
Roman Catholic Diocese of Tambacounda\n\nThe Ro-
man Catholic Diocese of Tambacounda () is a diocese
located in the city of Tambacounda in the Ecclesiastical
province of Dakar in Senegal.
\n\nHistory\n\nThe Diocese of Tambacounda
was erected by Pope Pius XI on December 21,
1933, on territory split off from the Diocese of
Saint-Louis du Sénégal. \n\nBishops\n\nDiocese
of Tambacounda \nErected: December 21,
1933\nMetropolitan: Archdiocese of Dakar\n
center>The Roman Catholic Diocese of Tambacounda
() is a diocese located in the city of Tambacounda in
the <center>Ecclesiastical province of Dakar in Sene-
gal.</center> center>Output: </center> center>The
<center>Roman</center> <center>Catholic
Mud Hens pitcher Evan Reed charged with sexual
assault\n\nMud Hens pitcher Evan Reed was charged
July 30 with sexual assault related to a March incident
in Detroit when he was a member of the Detroit Tigers.
\n\nMud Hens pitcher Evan Reed was charged July 30
with sexual assault related to a March incident in Detroit
when he was a member of the Detroit Tigers.\n\nMud
Hens pitcher Evan Reed was charged July 30 with sexual
assault related to a March incident in Detroit when he
was a member of the Detroit Tigers.\n\nMud Hens
pitcher Evan Reed was charged July 30 with sexual
assault related to
center>###<centercentercentercentercentercentercenter
centercentercentercentercentercentercentercenter
centercentercentercentercentercentercentercenter
centercentercentercentercentercentercentercenter
centercentercentercentercenter
Random
Sources
Thursday’s verdict only added to the intrigue surround-
ing the gravest legal and political unknown from the
2021 Capitol insurrection that hangs over the 2024 cam-
paign: will Donald Trump, the president who inspired
the uprising, face his own legal and political price?
I don’t think there’s any question that he’s going to be
impeached, ” Rep. Al Green (D-TX) told The Daily Beast.
“I think he’s going to be impeached, and I think he’s
going to be removed from office. ”\n\nGreen’s
will be the verdict only added to the intrigue surround-
ing the gravest legal and political unknown from the
2021 Capitol insurrection that hangs over the 2024 cam-
paign: will Donald Trump, the president who inspired
the uprising, face his own legal and political price?br>
A source of debate for centuries, the village of Ponte
Buriano, a suburb of Arezzo in the Tuscany region of
Italy, is so convinced the bridge behind Mona Lisa is
the Ponte Buriano that they’ve made it a key feature of
their local tourism campaign, even laying claim to the
bridge on the village’s welcome sign.
\n\nThe Ponte Buriano is one of the oldest bridges in
Tuscany, dating back to the 12th century. It’s also one of
the most famous, thanks to Leonardo da Vinci’s Mona
Lisa, the most famous painting in the world.\n\nThe
The village of [GR(GL("mona_lisa"), "bpr:search",
GR(GL("ponte_buriano"), GR(GL("search"),
GR(GL("mona_lisa"), GR(GL("search
•Efficiency: With LoRA and quantized models/optimizers,
we can reduce the model fine-tuning memory capacity re-
quirement to less than 11GB and the memory capacity re-
quirement even lower for the model inference stage. Mean-
while, integrated with the large-sized graph data, pre-trained
graph models, and necessary pre-processed data, the effi-
ciency of Graph-ToolFormer for various graph reasoning
task can still be a problem. In this paper, we introduce a
tentative approach to make the problem less severe with
the working memory. However, if we plan to deployGraph-
ToolFormer on devices with very small memories, like cell-
phones or embedded equipments, new techniques will still
be needed to improve the model learning and inference effi-
ciency.
•Diverse Applications : Due to the limited space, we can
only study a few number of the graph reasoning tasks with
Graph-ToolFormer in this paper. Meanwhile, in the real-
world, we have lots of graph structured data that may require
the LLMs to handle them to reason for the desired outputs.
Therefore, a very promising future work direction is to apply
Graph-ToolFormer to study diverse real-world graph/net-
work data oriented reasoning tasks with LLMs. We list a
few of them here just for the readers’ information, and the
readers may explore more diverse reasoning tasks according
to your own backgrounds and expertises.
– Urban Computing and Smart City : In the offline world,
we have extensively connected traffic networks that bridge
different local communities, cities and countries by lo-
cal roads, national highways, international fights and
ocean freight corridors. Applying LLMs for knowledge
extraction and reasoning based on such traffic networks
is critical for the current urban computing and smart city
projects.
– IoT and Smart Home : Assisted with the 5G, the IoT net-
work effectively bridges the cyber world with the physical
devices and equipments together via extremely fast com-
munication channels. The LLMs provide the opportunity
for us to utilize language models as the general interface
for controlling the devices within the IoT networks, which
is also the main objective for building the smart home
system.
– Healthcare : During the past years, the world has suf-
fered a lot from the covid-19 pandemic. Similar to the
protein molecules studied in this paper, both the virus
and the vaccines can also be represented as the molecular
graphs. LLMs with the molecular graph reasoning ability
have the potential to improve our current healthcare sys-
tem in many perspectives, like early identification of virus ,
analysis of the virus pathogenicity and creation of vaccines .
What’s more, the LLMs with the social network reasoning
ability will also help infer the potential virus propagation
among people , early prediction of highly infectious commu-
nities and identify rumors and misinformation about the
pandemic (at the online social networks).



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 10):

Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT
more different graph models that can be used for accomplishing
their own graph reasoning tasks.
Protein Molecule Function Reasoning : Protein and chemical
molecule function inference [52] has been a classic problem studied
in bio-chemical research for decades, which has fundamental appli-
cations in the real-world, such as helping design some new drugs
for curing some existing rare diseases. Protein function inference is
not an easy task, because homologous proteins often have several
different functions at the same time. Also such a prediction needs
to be fine-tuned with respect to some mutations but robust with
respect to others. Researchers have been exploring on this problem
with machine learning models, and have also developed a relatively
large protein function database [48] already. However, compared
with the number of protein existing in the real world, the specific
proteins with known functions included in the database is still
very limited. In graph learning, inferring the function of protein
molecules based on its structure has also be extensively studied as
well. Therefore, in this part, we also include it as a graph reasoning
task into Graph-ToolFormer as well.
Different from the bibliographic network, the protein molecular
graphs have much smaller sizes and there will also exist multiple
such graph instances in the dataset. What’s more, the features and
labels of protein molecular graphs are both about the whole molecu-
lar graph, not about the individual nodes anymore. As introduced
in Section 3.2, we can represent the set of studied protein molec-
ular graphs as G= {𝑔1,𝑔2,··· ,𝑔𝑙}, which can be loaded with the
following graph loading API call:
<API>𝐺𝐿(“𝑝𝑟𝑜𝑡𝑒𝑖𝑛-𝑔𝑟𝑎𝑝ℎ-𝑠𝑒𝑡”)→G </API>. (29)
For each molecular graph instance 𝑔𝑖 = (V𝑔𝑖 ,E𝑔𝑖 )in the dataset G,
there will also be raw features and labels related to each protein
molecular graph instance. For instance, for the graph instance 𝑔𝑖 ∈
G, we can represent its raw feature asx𝑔𝑖 and its label as y𝑔𝑖 , where
the label vector will indicate its corresponding functions. Based
on the protein graph structure and its raw features, we can define
the following API call for protein molecule function reasoning as
follows:
<API>𝐺𝑅(G,“𝑠𝑒𝑔-𝑏𝑒𝑟𝑡:𝑚𝑜𝑙𝑒𝑐𝑢𝑙𝑒-𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛 ”,𝑔𝑖)→ 𝑟</API>, (30)
which will call the pre-trained graph neural network SEG-Bert
proposed in [58]. The SEG-Bert with full name “Segmented Graph-
Bert” [58] extends the Graph-Bert model for molecular graph in-
stance representation learning. Besides the SEG-Bert model used
in Graph-ToolFormer, the readers can also customize the Graph-
ToolFormer framework to include other graph models for address-
ing the molecular graph reasoning tasks as well.
Sequential Recommender System Reasoning : In the era of big
data, as more and more data are generated both online and offline,
manual search of information from such big data sources has be-
come infeasible nowadays and we may need recommender systems
[28] to automatically recommend desired information for us instead.
Based on the historical records, sequential recommender system
aims to infer the next item(s) that users may be interested in, which
may lead to either the future purchase action or the review rating
scores of those items. When studying the sequential recommender
systems, it is a common way to model recommender systems as
the bipartite graphs, where the user-item interaction record also
has an attached timestamp. With considerations about the times-
tamps, sequential recommender systems aim to infer the potential
existence (or the weight) of links between user and their interested
items for the next future timestamp. In other words, we can define
the sequential recommendation problem in recommender systems
as a link prediction task with considerations about the temporal
factor.
Formally, according to the above description, we can repre-
sent the sequential recommender system as a bipartite graph
𝐺 = (V,E), where the node set V= U∪I covers both users and
items and the links in set E⊂M×I only exist between users and
item instead. For each user-item pair (𝑢𝑗,𝑖𝑙)∈E in the link set, we
can also obtain its timestamp. The sequential recommender system
data can be loaded with the following API call:
<API>𝐺𝐿(“𝑟𝑒𝑐𝑜𝑚𝑚𝑒𝑛𝑑𝑒𝑟-𝑠𝑦𝑠𝑡𝑒𝑚”)→ 𝐺</API>. (31)
For each user 𝑢𝑗 and item 𝑖𝑙 in the recommender system 𝐺, based
on the historical interaction records (before the current timestamp),
we can learn the embedding representations of them, which will be
used to infer the label between them in the future. Depending on
the modeling approach, the label vector can indicate either whether
the user will purchase the item or not ( i.e., binary classification
task) or the rating score of the user for the item (i.e., the regression
task). Regardless of the specific modeling settings, we can represent
the recommender system reasoning API call in LLMs as follows:
<API>𝐺𝑅(𝐺,“𝑏𝑝𝑟:𝑟𝑒𝑐𝑜𝑚𝑚𝑒𝑛𝑑𝑎𝑡𝑖𝑜𝑛”,𝑢𝑗,𝑖𝑙)→ 𝑟</API>, (32)
which will return either the probability scores that the user 𝑢𝑗 will
be interested in the item 𝑖𝑙 or the specific rating scores that 𝑢𝑗
will give to 𝑖𝑙. We use BPR (Bayesian Personalized Ranking) [42]
as the default recommendation model in Graph-ToolFormer in
this paper, but other recommendation models can also be used for
defining the above recommendation API calls as well. Besides the
recommendation API calls to infer the scores between user and
item, for one specific user 𝑢𝑗, we can also return the list of top-𝑘
recommended items with the following API call:
<API>𝐺𝑅(𝐺,“𝑏𝑝𝑟:𝑡𝑜𝑝𝑘-𝑟𝑒𝑐𝑜𝑚𝑚𝑒𝑛𝑑𝑎𝑡𝑖𝑜𝑛”,𝑢𝑗,𝑘)→ 𝑟</API>,
(33)
where the notation 𝑘 denotes a hyper-parameter to be extracted
from the input statements for the recommendation reasoning.
Online Social Network Community Reasoning : Online social
networks [31], like Facebook, Twitter and Tiktok, provide different
online services for their users to facilitate their online socialization
with friends, family members and colleagues. Users in online social
networks tend to interact more frequently with their online friends,
and they will naturally form their online social communities based
on their online social behaviors. Reasoning for the social commu-
nities of users in online social networks is a complicated problem.
In this part, we will introduce the API calls to empower LLMs to
detect social communities from online social networks.
Formally, we can represent the online social network studied
in this paper as 𝐺 = (V,E), where Vdenotes the set of user
nodes and Edenotes the social interactions among the users in the



Source: data\tc16_2312.10997v5\referenced_papers\[77]_2305.18846.pdf (Page 12):

[34] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object
hallucination in image captioning. In Ellen Riloff, David Chiang, Julia Hockenmaier, and
Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural
Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 4035–4045.
Association for Computational Linguistics, 2018. URL https://doi.org/10.18653/v1/
d18-1437.
[35] Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu,
Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston. Recipes for building an open-
domain chatbot. In Paola Merlo, Jörg Tiedemann, and Reut Tsarfaty, editors, Proceedings of
the 16th Conference of the European Chapter of the Association for Computational Linguistics:
Main Volume, EACL 2021, Online, April 19 - 23, 2021 , pages 300–325. Association for
Computational Linguistics, 2021.
[36] Md. Rashad Al Hasan Rony, Ricardo Usbeck, and Jens Lehmann. Dialokg: Knowledge-
structure aware task-oriented dialogue generation. In Marine Carpuat, Marie-Catherine de Marn-
effe, and Iván Vladimir Meza Ruíz, editors, Findings of the Association for Computational
Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022 , pages 2557–2571.
Association for Computational Linguistics, 2022. URL https://doi.org/10.18653/v1/
2022.findings-naacl.195.
[37] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare
words with subword units. In Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long
Papers. The Association for Computer Linguistics, 2016.
[38] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval aug-
mentation reduces hallucination in conversation. In Marie-Francine Moens, Xuanjing Huang,
Lucia Specia, and Scott Wen-tau Yih, editors,Findings of the Association for Computational
Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November,
2021, pages 3784–3803. Association for Computational Linguistics, 2021.
[39] Yi-Lin Tuan, Yun-Nung Chen, and Hung-yi Lee. Dykgchat: Benchmarking dialogue generation
grounding on dynamic knowledge graphs. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages
1855–1865. Association for Computational Linguistics, 2019.
[40] Yi-Lin Tuan, Sajjad Beygi, Maryam Fazel-Zarandi, Qiaozi Gao, Alessandra Cervone, and
William Yang Wang. Towards large-scale interpretable knowledge graph reasoning for dialogue
systems. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Findings of
the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022,
pages 383–395. Association for Computational Linguistics, 2022. URL https://doi.org/
10.18653/v1/2022.findings-acl.33.
[41] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of
Machine Learning Research, 9(86):2579–2605, 2008. URL http://jmlr.org/papers/v9/
vandermaaten08a.html.
[42] Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha P. Talukdar. Composition-based
multi-relational graph convolutional networks. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman
Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA,
pages 5998–6008, 2017.
[44] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
Bengio. Graph attention networks. In 6th International Conference on Learning Representations,
13



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 28):

[HCC+18] Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V . Le,
and Zhifeng Chen. Gpipe: Efﬁcient training of giant neural networks using pipeline parallelism.
CoRR, abs/1811.06965, 2018, 1811.06965. URL http://arxiv.org/abs/1811.06965. 19
[HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kia-
ninejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is pre-
dictable, empirically, 2017, 1712.00409. 18
[JGH18] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pages
8571–8580, 2018. 18
[KB14] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014,
1412.6980. 7
[Kom19] Aran Komatsuzaki. One epoch is all you need, 2019, arXiv:1906.06669. 18
[KSH12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Proceedings of the 25th International Conference on Neural
Information Processing Systems - Volume 1 , NIPS’12, pages 1097–1105, USA, 2012. Curran
Associates Inc. URL http://dl.acm.org/citation.cfm?id=2999134.2999257. 19
[LCG+19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
Soricut. Albert: A lite bert for self-supervised learning of language representations, 2019,
1909.11942. 9
[LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretrain-
ing approach. CoRR, abs/1907.11692, 2019, 1907.11692. URL http://arxiv.org/abs/
1907.11692. 2
[LSP+18] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and
Noam Shazeer. Generating wikipedia by summarizing long sequences. arXiv:1801.10198 [cs],
2018, 1801.10198. URL http://arxiv.org/abs/1801.10198. 2, 6
[LT16] Henry W Lin and Max Tegmark. Criticality in formal languages and statistical physics. arXiv
preprint arXiv:1606.06737, 2016. 25
[LXS+19] Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent, 2019, arXiv:1902.06720. 18
[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model
of large-batch training, 2018, arXiv:1812.06162. 3, 5, 6, 12, 13, 21
[Pap18] Vardan Papyan. The full spectrum of deep net hessians at scale: Dynamics with sample size.
CoRR, abs/1811.07062, 2018, 1811.07062. URL http://arxiv.org/abs/1811.07062. 18
[RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-
assets/research-covers/languageunsupervised/language understanding paper. pdf, 2018. 2, 6
[RRBS19a] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive
prediction of the generalization error across scales, 2019, 1909.12673. 18
[RRBS19b] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive
prediction of the generalization error across scales, 2019, arXiv:1909.12673. 18
[RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed
text-to-text transformer, 2019, arXiv:1910.10683. 2
[RWC+19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. openai.com, 2019. 2, 5, 6, 7, 8
[SCP+18] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanan-
takool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and
Blake Hechtman. Mesh-tensorﬂow: Deep learning for supercomputers, 2018, 1811.02084. 19
[SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words
with subword units. CoRR, 2015, 1508.07909. 6
29



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 29):

[SLA+18] Christopher J. Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and
George E. Dahl. Measuring the effects of data parallelism on neural network training, 2018,
arXiv:1811.03600. 12
[SS18] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory
cost. CoRR, abs/1804.04235, 2018, 1804.04235. URLhttp://arxiv.org/abs/1804.04235.
7
[THK18] Stefan Thurner, Rudolf Hanel, and Peter Klimek. Introduction to the theory of complex systems.
Oxford University Press, 2018. 18
[TL19] Mingxing Tan and Quoc V . Le. Efﬁcientnet: Rethinking model scaling for convolutional neural
networks. CoRR, abs/1905.11946, 2019, 1905.11946. URL http://arxiv.org/abs/1905.
11946. 18
[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural
Information Processing Systems 30 , pages 5998–6008. Curran Associates, Inc., 2017. URL
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf . 2, 6
[VWB16] Andreas Veit, Michael Wilber, and Serge Belongie. Residual networks behave like ensembles
of relatively shallow networks, 2016, arXiv:1605.06431. 8, 18
[Was06] Larry Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006.
18
[WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose
language understanding systems, 2019, 1905.00537. 2
[WRH17] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Growing a brain: Fine-tuning by in-
creasing model capacity. 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), Jul 2017. doi:10.1109/cvpr.2017.323. 19
[WYL19] Wei Wen, Feng Yan, and Hai Li. Autogrow: Automatic layer growing in deep convolutional
networks, 2019, 1906.02909. 19
[YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V .
Le. Xlnet: Generalized autoregressive pretraining for language understanding, 2019,
arXiv:1906.08237. 2
[ZK16] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. Procedings of the British
Machine Vision Conference 2016, 2016. doi:10.5244/c.30.87. 18
[ZKZ+15] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-
ralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books. 2015 IEEE International Conference on Computer Vision
(ICCV), Dec 2015. doi:10.1109/iccv.2015.11. 7
[ZLN+19] Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl,
Christopher J. Shallue, and Roger B. Grosse. Which algorithmic choices matter at which batch
sizes? insights from a noisy quadratic model. CoRR, abs/1907.04164, 2019, 1907.04164. URL
http://arxiv.org/abs/1907.04164. 12, 18
30



### Claim 158/179

#### Claim Text
A typical small Machin number flagellum is the Chlamydomonas (L ≈ 10 µm), where experimental data in reveal a critical frequency of θc = 2π · 40 s−1 and therefore an estimated Machin number Ma ≈ 2, which the authors considered sufficiently small, in comparison with the bull sperm case (L ≈ 50 µm), where the ratio is instead Ma ≈.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[47]_2305.17331.pdf (Page 13):

1 2 3 4 5 6 7 8 9 10
Number of documents
36
38
40
42
44MMLU Accuracy
Standalone LM
Lt=Flan-T5Base
(a) Flan-T5Base w/ AARANCE.
1 2 3 4 5 6 7 8 9 10
Number of documents
44
46
48
50
52MMLU Accuracy
Standalone LM
Lt=Flan-T5Larg e (b) Flan-T5Large w/ AARANCE.
1 2 3 4 5 6 7 8 9 10
Number of documents
50
52
54
56
58MMLU Accuracy
Standalone LM
Lt=Flan-T5X L (c) Flan-T5XL w/ AARANCE.
Figure 9: Relationship between LM’s performance and the number of augmentation documents.
Category Number
Ts
MSMARCO QA Open Domain QA 148122
KILT-FEVER Fact Checking 10444
KILT-WNED Entity Linking 3396
KILT-T-REx Slot Filling 5000
KILT-TriviaQA Open Domain QA 5359
KILT-Wizard of Wikipedia Dialogue 3054
Tt
MMLU Multi-task Language Understanding 1531
PopQA Open Domain QA 14267
Table 6: Configurations of our source tasks and target tasks.
Methods MMLU
All Hum. Soc. Sci. STEM Other
Flan-T5Base 36.1 40.4 39.8 27.0 40.6
Flan-T5Base Fine-tuning 36.1 38.9 41.2 27.9 39.9
Flan-T5Base w/ Contriever 43.7 44.4 45.0 36.4 51.1
Flan-T5Base w/ ANCE 43.0 44.2 44.3 34.5 51.9
Flan-T5Base w/ AARContriever (Ours) 44.4 44.7 47.7 35.8 52.2
Flan-T5Base w/ AARANCE (Ours) 44.8 42.2 46.4 39.0 53.2
Flan-T5Large 45.1 47.7 53.5 34.4 49.2
Flan-T5Large Fine-tuning 45.3 47.6 54.1 35.2 48.7
Flan-T5Large w/ Contriever 50.7 50.5 56.4 38.9 61.1
Flan-T5Large w/ ANCE 49.2 49.3 56.7 38.1 57.2
Flan-T5Large w/ AARContriever (Ours) 51.8 50.8 59.7 39.4 61.8
Flan-T5Large w/ AARANCE (Ours) 50.4 48.0 58.1 39.3 60.2
Flan-T5XL 51.2 55.5 57.4 38.1 58.7
Flan-T5XL w/ Contriever 56.4 57.3 66.1 43.9 63.2
Flan-T5XL w/ ANCE 55.3 55.9 64.0 41.5 64.9
Flan-T5XL w/ AARContriever (Ours) 56.7 57.7 65.4 43.6 65.1
Flan-T5XL w/ AARANCE (Ours) 56.2 59.4 64.8 41.5 64.9
InstructGPT 60.2 65.7 68.0 46.1 66.5
InstructGPT w/ Contriever 60.5 62.0 71.8 44.3 70.1
InstructGPT w/ ANCE 61.6 62.4 73.4 47.6 68.6
InstructGPT w/ AARContriever (Ours) 61.5 64.5 73.1 45.0 69.9
InstructGPT w/ AARANCE (Ours) 62.2 62.0 72.0 49.2 70.7
Table 7: Fine-tuning results on MMLU. We use the official auxiliary training data of MMLU to fine-tune the LM.



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 13):

0 1000 2000 3000 4000
24.5
25
25.5
26
26.5
27
FLOPS (MACs)
Rouge-L
2%
(a) ELI5
0 1000 2000 3000 4000
20
21
22
23
24
25
FLOPS (MACs)
2% (b) MS MARCO
0 1000 2000 3000 4000
24
25
26
27
28
29
30
31
32
FLOPS (MACs)
2% (c) NQ
0 20 40 60 80 100
24.5
25
25.5
26
26.5
27
Input P sgs
Rouge-L
2%
(d) ELI5
0 20 40 60 80 100
20
21
22
23
24
25
Input P sgs
2% (e) MS MARCO
0 20 40 60 80 100
24
25
26
27
28
29
30
31
32
FiD
CALM
T ok en Filtering
Combined
Input P sgs
2% (f) NQ
Figure 6: The ROUGE-L performance on FiD-Base vs. the FLOPS (MACs) (First row), and vs. the input passage
amount (Input Psg., second row). The results are on the test set for each dataset, for the various methods utilized.
We observe that the trends of the FLOPS (MACs) and the Input Psg. are very similar, since the passages effect the
encoder the most, and the encoder has the most impact on FLOPS (MACs) (as stated by de Jong et al. (2022)).



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 15):

10 7
 10 5
 10 3
 10 1
Compute (PF-days), non-embedding
103
105
107
Parameters (non-embedding)
N = (1.3 109) C0.73
min
N = (1.6 109) C0.88
10 7
 10 5
 10 3
 10 1
Compute (PF-days), excluding embeddings
0
5000
10000
15000Steps
Smin (adjusted)
Smin = (5.4 103) C0.03
min
S (fixed-batch)
Figure 14 Left: Each value of the compute budget Cmin has an associated optimal model size N. Optimal
model size grows very rapidly with Cmin, increasing by 5x for each 10x increase in compute. The number
of data examples processed makes up the remainder of the increase, growing relatively modestly by only 2x.
Right: The batch-adjusted number of optimization steps also grows very slowly, if at all, meaning that most
of the growth in data examples processed can be used for increased batch sizes.
can be ﬁt very well with a power-law
N(Cmin) ∝(Cmin)0.73. (6.1)
In Figure 12, we show the effect of training models of sub-optimal sizes (see Appendix B.4).
By deﬁnition Cmin ≡6NBcritS, and so we can use N(Cmin) to extract further results. In particular, since
prior ﬁts show B ∝L−4.8 and L∝C−0.05
min , we can conclude that Bcrit ∝C0.24
min . This leads us to conclude
that the optimal number of steps will only grow very slowly with compute, as
Smin ∝(Cmin)0.03, (6.2)
matching the empirical results in Figure 14. In fact the measured exponent is sufﬁciently small that our results
may even be consistent with an exponent of zero.
Thus we conclude that as we scale up language modeling with an optimal allocation of computation, we
should predominantly increase the model size N, while simultaneously scaling up the batch size via B ∝
Bcrit with negligible increase in the number of serial steps. Since compute-efﬁcient training uses relatively
few optimization steps, additional work on speeding up early training dynamics may be warranted.
6.2 Predictions from L(N,Smin)
The results for L(Cmin) and the allocations can be predicted from the L(N,Smin) equation obtained in
Section 5. Given our equation for L(N,Smin), we can substitute Smin = Cmin
6NB and then ﬁnd the minimum
of the loss as a function of N, while ﬁxing the training compute. We carry out this procedure in detail in
Appendix B, where we also provide some additional predictions.
For the loss as a function of training compute, we predict that
L(Cmin) =
(Cmin
c
Cmin
)αmin
C
(6.3)
where
αmin
C ≡ 1
1/αS + 1/αB + 1/αN
≈0.054 (6.4)
in excellent agreement with the exponent of Figure 13. We also predict that
N(Cmin) ∝(Cmin)αmin
C /αN ≈(Cmin)0.71 (6.5)
which also matches the scaling of Figure 14 to within a few percent. Our scaling laws provide a predictive
framework for the performance of language modeling.
16



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 17):

0 50 100 150
1.4
1.6
1.8
2
2.2
2.4
2.6 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(a) ELI5 - p = 10%
0 50 100 150
1.35
1.4
1.45
1.5
1.55
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (b) ELI5 - p = 30%
0 50 100 150
1.04
1.06
1.08
1.1
1.12
1.14 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (c) ELI5 - p = 50%
0 10 20 30 40 50
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
3.2 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(d) MS MARCO - p = 10%
0 10 20 30 40 50
1.1
1.15
1.2
1.25
1.3
1.35 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.84
0.86
0.88
0.9
0.92 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (f) MS MARCO - p = 50%
0 20 40 60 80 100 120
1.5
2
2.5
3
3.5
4
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(g) NQ - p = 10%
0 20 40 60 80 100 120
1.5
1.6
1.7
1.8
1.9
2
2.1 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
Loa ding [ M a thJ a x ] / ex tensions/ M a thM enu .j s (h) NQ - p = 30%
0 20 40 60 80 100 120
1.25
1.3
1.35
1.4
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (i) NQ - p = 50%
Figure 9: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-24), for FiD-Large models.
Each row represents a different dataset, and every column represents a different filtering percentage (10,30,50).



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 15):

0 50 100 150
1.6
1.8
2
2.2
2.4
2.6
2.8
3 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(a) ELI5 - p = 10%
0 50 100 150
1.4
1.45
1.5
1.55
1.6
1.65 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (b) ELI5 - p = 30%
0 50 100 150
1.04
1.06
1.08
1.1
1.12
1.14 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (c) ELI5 - p = 50%
0 10 20 30 40 50
1.6
1.8
2
2.2
2.4
2.6
2.8
3
3.2
3.4 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(d) MS MARCO - p = 10%
0 10 20 30 40 50
1.15
1.2
1.25
1.3
1.35
1.4 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
0.91 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (f) MS MARCO - p = 50%
0 20 40 60 80 100
1.5
2
2.5
3
3.5
1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(g) NQ - p = 10%
0 20 40 60 80 100
1.5
1.6
1.7
1.8
1.9
2 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
Loa ding [ M a thJ a x ] / ex tensions/ M a thM enu .j s (h) NQ - p = 30%
0 20 40 60 80 100
1.22
1.24
1.26
1.28
1.3
1.32
1.34
1.36 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (i) NQ - p = 50%
Figure 7: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-12), for FiD-Base models.
Each row represents a different dataset, and every column represents a different filtering percentage (10,30,50).



### Claim 159/179

#### Claim Text
Epilepsy significantly affects the quality and expectancy of patient’s life .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 15):

to rectify hallucinations during the post-
processing stage (Cao et al., 2020; Zhu
et al., 2021; Fabbri et al., 2022). The fixer
can be either another LLM (Peng et al.,
2023a; Zhang et al., 2023d; Chern et al.,
2023; Gou et al., 2023) or a specific small
model (Chen et al., 2023a). Such fix-
ers first interact with external knowledge
sources to gather sufficient evidence, and
then correct hallucinations. For example,
RARR (Gao et al., 2023a) directly prompts
an LLM to ask questions about the content
that needs to be corrected from multiple per-
spectives. Then it uses search engines to re-
trieve relevant knowledge. The LLM-based
fixer finally makes corrections based on re-
trieved evidence. The Verify-then-Edit ap-
proach (Zhao et al., 2023a) aims to enhance
the factuality of predictions by post-editing
reasoning chains based on external knowl-
edge sourced from Wikipedia. To achieve
better performance, LLM-Augmenter (Peng
et al., 2023a) prompts LLMs to summarize
retrieved knowledge before feeding it into the
fixer. Moreover, FacTool (Chern et al., 2023)
and CRITIC (Gou et al., 2023) propose to uti-
lize various external tools to obtain evidence
for the fixer.
Summary & Discussion. Resorting to external
knowledge to mitigate hallucinations in LLMs of-
fers several advantages. Firstly, this method cir-
cumvents the need for modifying LLMs, making
it a plug-and-play and efficient solution. Secondly,
it facilitates the easy transfer of proprietary knowl-
edge (e.g., a company’s internal data) and real-
time updated information to LLMs. Lastly, this
approach enhances the interpretability of infor-
mation generated by LLMs by allowing the trac-
ing of generation results back to the source evi-
dence (Gao et al., 2023b; Yue et al., 2023). How-
ever, this direction also presents some remaining
challenges. We discuss some of them below.
(1) Knowledge verification. In the era of LLMs,
the external knowledge source could extend
beyond a single document corpus or a spe-
cific website to encompass the entire Internet.
However, the information from the Internet is
in the wild, which means they may also be
fabricated, or even generated by LLMs them-
selves (Alemohammad et al., 2023). How to
User Query: What is the height of Mount Kilimanjaro?
User Query: What is the height of Mount Kilimanjaro?
Answer:Theheightis5932meters.
Please provide your confidence level (0-100). 
Answer: The height is 5932 meters. I am 90% confident. 
User Query: What is the height of Mount Kilimanjaro?
 Answer: The height is 5895 meters. 
Answer: The height is 5932 meters. 
Answer: The height is 5921 meters. 
(a) logit-based method
(b) verbalize-based method
(c) consistency-based method
Figure 5: The illustrations of three typical methods for
estimating LLM uncertainty. In the example of the
logit-based method, we use the red/green background
to distinct tokens with low/high generation probabili-
ties. In the example of the consistency-based method,
the responses are acquired from multiple sampling.
verify the authenticity of retrieved knowledge
from the Internet is an open and challenging
problem to be solved.
(2) Performance/efficiency of retriever/fixer.
The performance of the retriever/fixer plays
a vital role in ensuring the effects of hallu-
cination mitigation. Future work may con-
sider jointly optimising the whole working
flow (retriever→LLM→fixer) via reinforce-
ment learning (Qiao et al., 2023) or other
techniques. Besides, the efficiency of the
retriever/fixer is another important factor to
be considered, as the generation speed of
existing LLMs is already a significant bur-
den (Ning et al., 2023).
(3) Knowledge conflict. As introduced be-
fore, the retrieved knowledge may conflict
with the parametric knowledge stored by
LLMs (Qian et al., 2023). Shi et al. (2023b)
reveal that LLMs may fail to sufficiently ex-
ploit retrieved knowledge when knowledge
conflict happens. Xie et al. (2023) take
a more cautious look at this phenomenon.
How to fully utilize context knowledge is
an under-explored question. For example,
Liu et al. (2023d) find the performance of
retrieval-augmented LLMs significantly de-
grades when they must access evidence in the
middle of long contexts.
5.4.3 Exploiting Uncertainty
Uncertainty serves as a valuable indicator for de-
tecting and mitigating hallucinations during the
16



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 1):

Parametric MemorizationPre-training
SFT
RLHF
Inference
Overinflated Self-confidence
Misleading Alignment
Generation-time Risk
Curating Training Data
Honesty-oriented RL
Decoding StrategyKnowledge Retrieve
Honesty-oriented SFT
Exploiting UncertaintySources (Sec. 4) Mitigation (Sec. 5)
Definition (Sec. 2)Input-Conflicting Hallucination
TimeLine
Input-Conflicting Benchmark:BEGIN, QMSum, FENMT,FEQA…
Benchmark (Sec. 3) Context-Conflicting HallucinationFact-Conflicting Hallucination
Context-Conflicting Benchmark:HADES…Fact-Conflicting Benchmark:TruthfulQA,,FActScore,HaluEval,FACTOR…
Figure 2: The overview structure of this paper: We initially categorize LLM hallucinations into three distinct types
and then introduce corresponding evaluation benchmarks. Subsequently, we explore the source of hallucinations
and discuss mitigation strategies throughout the life cycle of LLMs (pre-training→SFT→RLHF→inference).
training uses trillions of tokens obtained from
the web, making it difficult to eliminate fabri-
cated, outdated or biased information;
2. Versatility of LLMs : general-purpose LLMs
are expected to excel in cross-task, cross-
lingual, and cross-domain settings, posing
challenges for comprehensive evaluation and
mitigation of hallucination.
3. Imperceptibility of errors: as a byproduct of
their strong abilities, LLMs may generate false
information that initially seems highly plausi-
ble, making it challenging for models or even
humans to detect hallucination.
In addition, the RLHF process (Ouyang et al.,
2022), the vague knowledge boundary (Ren et al.,
2023) and the black-box property of LLMs (Sun
et al., 2022) also complicate the detection, expla-
nation, and mitigation of hallucination in LLMs.
There has been a notable upsurge in cutting-edge
research dedicated to addressing the aforemen-
tioned challenges, which strongly motivates us to
compile this survey.
We organize this paper as follows, as also
depicted in Figure 2. We first introduce the
background of LLMs and offer our definition of
hallucination in LLMs (§2). Next, we introduce
relevant benchmarks and metrics (§3). Subse-
quently, we discuss potential sources of LLM hal-
lucinations (§4), and provide an in-depth review of
recent work towards addressing the problem (§5).
Finally, we present forward-looking perspectives
(§6). We will consistently update the related
open-source materials, which can be accessed at
https://github.com/HillZhang1999/
llm-hallucination-survey.
2 Hallucination in the Era of LLM
We begin this section by overviewing the history
of LLMs (§2.1). Next, we present our defini-
tion of LLM hallucination, by breaking it down
2



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 9):

able questions. Their empirical study reveals that
even the most advanced LLM, GPT4 (OpenAI,
2023b), shows a significant performance gap when
compared to humans. Ren et al. (2023) note a
correlation between accuracy and confidence, but
such confidence often surpasses the actual capa-
bilities of LLMs, namely over-confidence. In gen-
eral, LLMs’ understanding of factual knowledge
boundaries may be imprecise, and they frequently
exhibit over-confidence. Such over-confidence
misleads LLMs to fabricate answers with unwar-
ranted certainty.
Problematic alignment process could mislead
LLMs into hallucination. LLMs typically un-
dergo an alignment process following pre-training,
where they receive further training on curated
instruction-following examples to align their re-
sponses with human preferences. However, when
trained on instructions for which LLMs have not
acquired prerequisite knowledge from the pre-
training phase, this is actually a misalignment pro-
cess that encourages LLMs to hallucinate (Gold-
berg, 2023; Schulman, 2023). Another potential
issue is sycophancy, where LLMs may generate
responses that favor the user’s perspective rather
than providing correct or truthful answers, which
can result in hallucination (Perez et al., 2022; Rad-
hakrishnan et al., 2023; Wei et al., 2023b).
The generation strategy employed by LLMs
has potential risks. Today’s most advanced
LLMs generate responses sequentially, outputting
one token at a time. Zhang et al. (2023a) discover
that LLMs sometimes over-commit to their early
mistakes, even when they recognize they are in-
correct. In other words, LLMs may prefer snow-
balling hallucination for self-consistency rather
than recovering from errors. This phenomenon
is known as hallucination snowballing . Azaria
and Mitchell (2023) also contend that local opti-
mization (token prediction) does not necessarily
ensure global optimization (sequence prediction),
and early local predictions may lead LLMs into
situations where it becomes challenging to formu-
late a correct response. Lee et al. (2022) highlight
that the randomness introduced by sampling-based
generation strategies, such as top-p and top-k, can
also be a potential source of hallucination.
LLM Pre-train Data Size
GLM (Zeng et al., 2022) 400B tokens
BLOOM (Scao et al., 2022) 366B tokens
GPT-3 (Brown et al., 2020) 300B tokens
LLaMA (Touvron et al., 2023a) 1.4T tokens
Llama 2 (Touvron et al., 2023b) 2T tokens
Table 6: The pre-training data size of popular LLMs.
5 Mitigation of LLM Hallucination
In this section, we provide an extensive review of
recent studies focused on mitigating LLM halluci-
nations. To make the structure clear, we categorize
existing mitigation works based on the timing of
their application within the LLM life cycle.
5.1 Mitigation during Pre-training
Existing work (Zhou et al., 2023a) argues that the
knowledge of LLMs is mostly acquired during the
pre-training phase. The presence of noisy data
such as misinformation in the pre-training corpus
could corrupt the parametric knowledge of LLMs,
which is a significant factor contributing to hallu-
cinations, as previously discussed in § 4. Akyürek
et al. (2022) also demonstrate that it is possible to
trace the factual knowledge acquired by language
models back to their training data. Consequently,
an intuitive approach to mitigating hallucinations
could involve manually or automatically curating
the pre-training corpus to minimize unverifiable or
unreliable data as much as possible.
Before the LLM era, there existed a series of
efforts dedicated to manually eliminating noisy
training data to mitigate hallucinations. For in-
stance, Gardent et al. (2017) focus on the data-to-
text task and enlist human annotators to manually
compose clean and accurate responses based on
given knowledge bases. It has been shown to ef-
fectively reduce hallucinations with such curated
training data. Similarly, Wang (2019) manually
refine the text in existing table-to-text datasets and
observe that this process also substantially alle-
viates fact hallucinations. Besides, Parikh et al.
(2020) instruct annotators to revise verified sen-
tences from Wikipedia rather than directly creat-
ing new sentences when constructing table-to-text
training data. This approach has also been proven
to result in improved factuality of results.
With the advent of the LLM era, curating train-
ing data during pre-training has become increas-
ingly challenging due to the vast scale of pre-
training corpora (as exemplified in Table 6). For
10



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 16):

inference process (Manakul et al., 2023). Typi-
cally, it refers to the confidence level of model out-
puts (Jiang et al., 2021; Huang et al., 2023a; Duan
et al., 2023). Uncertainty can assist users in de-
termining when to trust LLMs. Provided that the
uncertainty of LLM responses can be accurately
characterized, users can filter out or rectify LLMs’
claims with high uncertainty since such claims are
more prone to be fabricated ones (Lin et al., 2023).
Generally speaking, methods for estimating the
uncertainty of LLMs can be categorized into three
types (Xiong et al., 2023), as listed below. To fa-
cilitate understanding, we also present illustrative
examples for these methods in Figure 5.
(1) Logit-based estimation. The first method is
the logit-based method, which requires ac-
cess to the model logits and typically mea-
sures uncertainty by calculating token-level
probability or entropy. This method has been
widely used in the machine learning commu-
nity (Guo et al., 2017).
(2) Verbalize-based estimation. The second is
the verbalize-based method, which involves
directly requesting LLMs to express their un-
certainty, such as using the following prompt:
“Please answer and provide your confidence
score (from 0 to 100). ” This method is
effective due to the impressive verbal and
instruction-following capabilities of LLMs.
Notably, Xiong et al. (2023) further suggest
using chain-of-thoughts prompts (Wei et al.,
2022) to enhance this method.
(3) Consistency-based estimation. The third is
the consistency-based method (Wang et al.,
2022; Shi et al., 2022; Zhao et al., 2023a).
This method operates on the assumption that
LLMs are likely to provide logically incon-
sistent responses for the same question when
they are indecisive and hallucinating facts.
Several recent studies have leveraged uncer-
tainty estimation for detecting and mitigating hal-
lucinations in LLMs. S ELF CHECK GPT (Man-
akul et al., 2023) is the first framework to detect
LLM hallucinations based on uncertainty mea-
surement in a zero-resource and black-box set-
ting. They employ a consistency-based approach
for uncertainty estimation. A non-trivial chal-
lenge in S ELF CHECK GPT is determining how to
measure the consistency of different responses.
Manakul et al. (2023) perform experiments with
BERTScore (Zhang et al., 2019), QA-based met-
rics (Wu and Xiong, 2023) and n-gram metrics.
They finally find that a combination of these ap-
proaches yields the best results. Mündler et al.
(2023) directly utilize an additional LLM to as-
sess whether two LLM responses are logically
contradictory given the same context (Luo et al.,
2023b), which means at least one of them is hal-
lucinated. Consequently, they employ another
LLM to revise such self-contradictory hallucina-
tions from two responses. Agrawal et al. (2023)
further adopt the verbalize-based method to eval-
uate the hallucination rate of LLMs for fabricat-
ing references. Varshney et al. (2023), on the
other hand, use the logit-based method to detect
false concepts in LLMs’ responses with high un-
certainty. They then fix such content with auxil-
iary retrieval-augmented LLMs.
Besides, Zhao et al. (2023b) present a Pareto
optimal self-supervision framework. This frame-
work utilizes available programmatic supervision
to assign a risk score to LLM responses, which can
serve as an indicator of hallucinations. Luo et al.
(2023a) introduce a pre-detection self-evaluation
technique, which aims to evaluate the familiarity
of LLMs with the concepts in user prompts and
prevent the generation of content about those un-
familiar concepts.
Summary & Discussion. Exploiting uncer-
tainty to identify and mitigate LLM hallucinations
is a promising research direction today. Three pri-
mary approaches exist for estimating the uncer-
tainty of LLMs, each presenting its unique chal-
lenges. Firstly, the logit-based method is becom-
ing less applicable for modern commercial LLMs
as they are usually closed-source and black-box,
rendering their output logits inaccessible. Sec-
ondly, regarding the verbalize-based method, re-
searchers have observed that LLMs tend to display
a high degree of overconfidence when expressing
their confidence (Xiong et al., 2023). Thirdly, the
effective measurement of the consistency of differ-
ent responses remains an unresolved issue in the
consistency-based method (Manakul et al., 2023).
We believe that leveraging uncertainty is crucial in
developing trustworthy LLMs and encourage fu-
ture research to address the aforementioned chal-
lenges in this field.
17



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 10):

SFT Dataset Data Size
Alpaca (Taori et al., 2023) 52k samples
GPT4-Alpaca (Peng et al., 2023b) 52k samples
Baize (Xu et al., 2023) 210k samples
Dolly (Conover et al., 2023) 15k samples
Open-assistant (Köpf et al., 2023) 34k samples
LIMA (Zhou et al., 2023a) 1k samples
Table 7: The size of popular SFT datasets.
instance, Llama 2 (Touvron et al., 2023b) conducts
pre-training on about two trillion tokens. There-
fore, compared to manual curation, a more practi-
cal approach today could be automatically select-
ing reliable data or filtering out noisy data. For
example, the pre-training data of GPT-3 (Brown
et al., 2020) is cleaned by using similarity to a
range of high-quality reference corpora. The de-
velopers of Falcon (Penedo et al., 2023) carefully
extract high-quality data from the web via heuris-
tic rules and prove that properly curated pertaining
corpora lead to powerful LLMs. Li et al. (2023f)
propose phi-1.5, a 1.3 billion parameter LLMs
pre-trained on filtered “textbook-like” synthetic
data, which exhibits many traits of much larger
LLMs. In order to mitigate hallucinations, current
LLMs tend to collect pre-training data from credi-
ble text sources. The developers of Llama 2 (Tou-
vron et al., 2023b) strategically up-sample data
from highly factual sources, such as Wikipedia,
when constructing the pre-training corpus. Lee
et al. (2022) propose to prepend the topic pre-
fix to sentences in the factual documents to make
each sentence serve as a standalone fact during
pre-training. Concretely, they treat the document
name as the topic prefix and observe this method
improves LMs’ performance on TruthfulQA.
Summary & Discussion. The mitigation of hal-
lucinations during pre-training is primarily cen-
tred around the curation of pre-training corpora .
Given the vast scale of existing pre-training cor-
pora, current studies predominantly employ sim-
ple heuristic rules for data selection and filtering.
A potential avenue for exploration could be devis-
ing more effective selection or filtering strategies.
5.2 Mitigation during SFT
As a common practice, current LLMs collec-
tively undergo the process known as supervised
fine-tuning (SFT) to elicit their knowledge ac-
quired from pre-training and learn how to inter-
act with users (Wang et al., 2023c; Zhang et al.,
Parametric KnowledgeOf LLMs
 SFT Data
Teach LLMs to hallucinate
Figure 3: The SFT data usually contains samples that
exceed LLMs’ parametric knowledge, which may re-
sult in hallucinations.
2023b). SFT generally involves first annotating
or collecting massive-task instruction-following
data (Chung et al., 2022; Taori et al., 2023),
followed by fine-tuning pre-trained foundational
LLMs on this data using maximum likelihood es-
timation (MLE) (Wei et al., 2021). By employing
well-designed SFT strategies, many recent stud-
ies claim to have built LLMs that achieve perfor-
mance on par with ChatGPT (Wang et al., 2023b).
Similar to pre-training, one potential approach
to reduce hallucination during the SFT stage could
be curating the training data. Given the rela-
tively small volume of SFT data (refer to Table 7),
both manual and automatic curation are viable
options here. Zhou et al. (2023a) have meticu-
lously constructed an instruction-tuning dataset,
comprising 1,000 samples annotated by human ex-
perts. Some other studies (Chen et al., 2023b;
Cao et al., 2023; Lee et al., 2023) have employed
an automatic selection of high-quality instruction-
tuning data, by leveraging LLMs as evaluators or
designing specific rules. Experimental results on
hallucination-related benchmarks, such as Truth-
fulQA (Lin et al., 2021), suggest that LLMs fine-
tuned on such curated instruction data demonstrate
higher levels of truthfulness and factuality com-
pared to LLMs fine-tuned on uncurated data. Fur-
thermore, Mohamed et al. (2023) propose the inte-
gration of domain-specific knowledge sets into the
SFT data, which aims to reduce hallucinations that
arise from a lack of relevant knowledge.
It is worth noting that Schulman (2023) under-
scored a potential risk of the SFT process that
it could induce hallucination from LLMs due to
behavior cloning. Behavior cloning is a concept
in reinforcement learning (Torabi et al., 2018),
which means the model learns directly from im-
itating the expert’s actions. The problem here is
11



### Claim 160/179

#### Claim Text
The Cg value for modeling specific diseases can be estimated through surveys on health attitudes, behaviors, and outcomes, as indicated in Ref. .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[81]_2312.15883.pdf (Page 6):

HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable Medical LLMs Responses Conference’17, July 2017, Washington, DC, USA
Table 2: Performance comparison (in percent ±standard deviation) on CMB-Exam and MMCU-Medical for medical Q&A answer.
Red shading indicates the best-performing model, while blue signifies the second-best in the ablation study, and green signifies
the second-best in baselines.
LLM Turbo LLM GPT 3.5 Baichuan 13B-Chat
Method Dataset MMCU-Medical CMB-Exam MMCU-Medical CMB-Exam
Metric EM PCR EM PCR EM PCR EM PCR
Baselines
Base 43.52 ±1.92 50.55 ±1.88 38.40 ±2.03 46.76 ±1.93 42.20 ±2.87 46.09 ±2.65 36.91 ±2.94 40.95 ±2.70
KGRAG 38.74 ±1.66 43.38 ±1.68 38.00 ±1.90 42.26 ±1.88 34.37 ±2.36 38.51 ±2.10 39.92 ±2.37 45.84 ±2.29
QE 40.28 ±1.15 46.79 ±1.41 36.35 ±0.88 41.84 ±1.10 38.25 ±2.23 44.23 ±1.94 34.27 ±2.88 38.79 ±2.65
CoN 45.74 ±1.42 51.15 ±1.94 42.45 ±1.06 45.65 ±1.65 44.98 ±2.65 50.65 ±1.94 41.37 ±2.45 47.58 ±2.73
CoK 45.15 ±1.59 52.35 ±1.77 42.32 ±1.35 45.98 ±1.80 45.15 ±1.86 51.19 ±1.69 41.87 ±2.18 47.95 ±1.79
KALMV 39.24 ±1.41 43.77 ±1.23 38.24 ±0.84 43.37 ±1.89 36.17 ±2.33 40.85 ±2.11 38.61 ±2.44 43.92 ±1.97
KG-GPT 45.08 ±1.96 52.16 ±1.54 41.49 ±1.04 45.72 ±1.48 44.25 ±2.38 50.97 ±2.65 39.92 ±2.38 45.20 ±1.49
SuRe 44.81 ±1.38 51.49 ±1.97 41.37 ±1.26 44.27 ±1.47 44.77 ±1.80 50.24 ±2.09 39.49 ±1.57 46.22 ±1.70
Ours HyKGE 49.65 ±1.39 57.82 ±1.54 45.94 ±1.20 50.63 ±1.33 49.33 ±1.72 58.12 ±1.79 45.44 ±1.97 51.25 ±1.84
*Performance Gain ↑ 8.55 ∼28.16 10.45 ∼33.29 8.38 ∼26.38 8.28 ∼21.01 9.26 ∼43.53 13.54 ∼50.92 8.53 ∼32.59 6.88 ∼32.12
Ablation
HyKGE (w/o HO) 41.08 ±1.45 49.74 ±1.84 34.40 ±1.13 40.14 ±1.25 39.55 ±1.98 45.28 ±2.14 33.33 ±2.22 35.42 ±2.70
HyKGE (w/o Chains) 48.15 ±1.75 54.53 ±1.68 44.60 ±0.94 48.27 ±1.04 48.65 ±1.91 55.45 ±1.81 43.40 ±1.80 48.81 ±2.75
HyKGE (w/o Description) 48.30 ±1.45 54.01 ±1.86 44.80 ±1.33 48.56 ±1.41 48.22 ±2.12 55.23 ±1.86 43.77 ±2.37 49.86 ±1.47
HyKGE (w/o Fragment) 47.87 ±1.66 54.34 ±1.49 42.33 ±1.02 47.54 ±0.84 47.95 ±1.90 53.45 ±2.33 44.72 ±2.66 49.29 ±2.56
HyKGE (w/o Reranker) 46.38 ±1.65 52.48 ±1.88 41.44 ±0.88 48.84 ±1.09 43.59 ±2.34 46.88 ±2.56 40.65 ±2.27 46.25 ±2.11
Table 3: RAG relevance and answer performance comparison (in mean ±standard deviation) on CMB-Exam, MMCU-Medical
and CMB-Clin for medical Q&A answer with GPT 3.5 Turbo.
Method Dataset MMCU-Medical CMB-Exam CMB-Clin
Metric ACJ PPL ROUGE-R ACJ PPL ROUGE-R BLEU-1 BLEU-4 PPL ROUGE-R
Baselines
Base / 47.42 ±1.24 / / 62.54 ±0.94 / 4.83 ±1.21 6.51 ±1.55 10.38 ±1.47 23.99 ±1.06
KGRAG 13.38 ±4.27 151.22 ±2.87 5.31 ±0.97 18.40 ±5.58 218.67 ±3.68 11.25 ±1.93 5.34 ±1.51 8.77 ±1.90 61.81 ±2.51 22.15 ±1.27
QE 25.53 ±3.68 28.75 ±1.58 14.05 ±1.22 31.91 ±6.82 29.57 ±1.60 16.64 ±2.11 8.85 ±1.97 18.67 ±1.44 28.32 ±2.48 26.24 ±2.20
CoN 19.14 ±5.18 29.01 ±1.61 16.46 ±1.19 14.89 ±5.53 27.35 ±1.93 17.31 ±1.48 12.48 ±1.65 25.81 ±1.04 17.65 ±3.47 31.37±1.87
CoK 18.45 ±4.71 24.38±1.93 18.23±2.02 16.77 ±6.71 28.69 ±2.26 19.94±1.46 12.35 ±1.46 24.79 ±1.18 21.57 ±2.62 30.86 ±2.24
KALMV 14.42 ±3.88 147.22 ±3.12 7.21 ±1.08 18.77 ±5.91 233.49 ±4.19 12.84 ±1.34 5.72 ±1.16 8.27 ±1.20 80.46 ±2.51 23.16 ±2.23
KG-GPT 32.03±4.82 25.76 ±2.45 15.90 ±1.31 38.70±5.44 24.01±3.96 17.72 ±1.80 13.03±0.76 26.14±1.09 15.54±1.38 28.42 ±1.91
SuRe 20.16 ±3.93 26.49 ±2.88 16.91 ±1.84 22.27 ±4.02 30.81 ±2.59 16.18 ±1.70 10.54 ±0.92 24.82 ±1.31 16.84 ±1.46 29.18 ±1.62
Ours HyKGE 59.57±4.37 12.55±1.29 26.89±1.67 71.28±3.88 10.14±1.68 32.11±1.28 18.28±0.48 30.21±1.05 8.56±1.24 33.66±1.54
*Performance Gain 133.33∼345.22 48.52∼91.70 45.75∼406.40 84.19∼378.71 57.77∼95.36 61.03∼185.42 40.29∼278.47 15.57∼364.06 46.85∼89.25 7.30∼51.96
Ablation HyKGE (w/o HO) 41.49±5.36 15.57±2.31 22.30 ±2.37 51.48±4.92 11.23±1.96 29.01±1.96 7.15 ±2.35 11.55 ±1.89 8.96±1.01 30.48 ±2.58
HyKGE (w/o Fragment) 38.30±4.85 18.95 ±2.04 23.63±1.47 41.91 ±4.44 11.26 ±1.45 26.89 ±2.65 11.28±1.76 23.09±1.44 8.99 ±1.72 31.40±0.82
Table 4: Performance and computation time comparison (in
mean ±standard deviation) on MMCU-Medical for medical
Q&A answer with GPT 3.5 Turbo.
Method / Metric EM PCR Avg. Time (s)
HyKGE 49.65 ±1.39 57.82 ±1.54 19.76
HyKGE( +LLM for NER) 48.17 ±1.13 56.77 ±1.02 26.61
HyKGE( +LLM for Reranker) 42.72 ±2.06 48.24 ±1.17 32.51
HyKGE( +LLM for Summary) 43.02 ±3.11 46.54 ±2.08 28.51
three basic medical sciences, pharmacology, nursing, pathology,
clinical medicine, infectious diseases, surgery, anatomy, etc., with a
total of 2,819 questions. The CMB-Exam dataset utilizes qualifying
exams as a data source in the four clinical medicine specialties of
physicians, nurses, medical technicians, and pharmacists, with a to-
tal of 269,359 questions. Given the extensive size of the CMB-Exam
dataset, we randomly sample 4,000 questions for testing. The CMB-
Clin dataset contains 74 high-quality, complex, and real patient
cases with 208 medical questions.
5.1.2 Knowledge Graph. CMeKG (Clinical Medicine Knowledge
Graph)6 [12], CPubMed-KG (Large-scale Chinese Open Medical
6https://cmekg.pcl.ac.cn/, https://github.com/king-yyf/CMeKG_tools
Knowledge Graph) 7 and Disease-KG (Chinese disease Knowledge
Graph)8 are open-source medical KGs, which integrates extensive
medical text data, including diseases, medications, symptoms and
diagnostic treatment technologies. The fused KG has 1,288,721 en-
tities and 3,569,427 relations. However, due to the lack of medical
entity descriptions in its entities, we collect relevant entity knowl-
edge from Wikipedia9, Baidu Baike10, and Medical Baike 11, and
store them as entity descriptions.
5.1.3 LLM Turbo. To fairly verify whether HyKGE can effectively
enhance LLMs, we selected the following two types of general-
domain large models as the base model and explored the gains
brought by HyKGE: GPT 3.5 and Baichuan13B-chat [87].
5.1.4 Compared Methods. In order to explore the advantages of
the HyKGE, we compare the HyKGE results against eight other
models: (1) Base Model (Base) servers as the model without any
external knowledge, used to check the improvement effect of differ-
ent RAG methods. We use GPT 3.5 and Baichuan13B-chat as base
models. (2) Knowledge Graph Retrieval-Augmented Genera-
tion (KGRAG) [ 63–65] uses user query as a reference to retrieve
7https://cpubmed.openi.org.cn/graph/wiki
8https://github.com/nuolade/disease-kb
9https://www.wikipedia.org/
10https://baike.baidu.com/
11https://www.yixue.com/



Source: data\tc16_2312.10997v5\referenced_papers\[81]_2312.15883.pdf (Page 7):

Conference’17, July 2017, Washington, DC, USA Jiang, Zhang and Xu et al.
in the KGs, which is the base model of RAG on KG and has been
widely applied in [63–65]. (3) Query Expansion (QE) [ 5] refor-
mulate the user’s initial query by adding additional terms with a
similar meaning with the help of LLMs. (4) CHAIN-OF-NOTE
(CoN) [93] generates sequential reading notes for retrieved knowl-
edge, enabling a thorough evaluation of their relevance to the given
question and integrating these notes to formulate the final an-
swer. (5) Chain-of-Knowledge (CoK) [ 44] utilize the power of
LLMs and consists of reasoning preparation, dynamic knowledge
adapting, and answer consolidation. (6) Knowledge-Augmented
Language Model Verification (KALMV) [ 6] verifies the output
and the knowledge of the knowledge-augmented LLMs with a sep-
arate verifier. (7) Knowledge Graph Generative Pre-Training
(KG-GPT) [ 34] comprises three steps: Sentence Segmentation,
Graph Retrieval, and Inference, each aimed at partitioning sen-
tences, retrieving relevant graph components, and deriving logical
conclusions. (8) Summarizing Retrievals (SuRe) [ 33] constructs
summaries of the retrieved passages for each of the multiple answer
candidates and confirms the most plausible answer from the can-
didate set by evaluating the validity and ranking of the generated
summaries. Note that we follow the prompts of the baselines as
stated strictly. The baselines and running time are summarized in
Table 1. In RAG Options, CoN requires fine-tuning the retriever,
implying a higher training overhead and the prerequisite of prepar-
ing a dataset. In addition, it is also difficult to migrate to other
domain-specific KGs. In terms of LLMs interactions, QE, CoN, CoK,
KALMV, KG-GPT, SuRe and HyKGE all necessitate engagement
with LLMs. However, CoN, CoK, KALMV, KALMV, KG-GPT and
SuRe entail multiple interactions (more than twice), significantly
escalating the time expenditure.
5.1.5 Evaluation Metrics. As for the evaluation of multi-task medi-
cal choice question performance, we guide LLMs to only answer
the correct answer and employ established metric Exact Match
(EM) as suggested by prior work [ 32, 99]. For the EM score, an
answer is deemed acceptable if its form corresponds to all correct
answers in the provided list. For multiple-choice questions, we also
calculate a Partial Correct Rate (PCR) . In comparison to EM, if
there is a missing answer without any incorrect ones, PCR clas-
sifies it as correct. In addition, to verify the effectiveness of the
retrieved knowledge, we also let LLMs output a complete analysis
process. Then, we measure Artificial Correlation Judgement
(ACJ)by inviting 20 medical experts to rate the retrieved knowledge
according to the criteria of (correlation=1, relevant but useless=0,
irrelevant=-1), and calculate the relevant scores for each question
by sampling 100 questions from the two datasets. Moreover, we
also objectively evaluated the Perplexity (PPL) of LLMs output.
The smaller the PPL, the greater the role of retrieved knowledge
in reducing LLMs’ hallucinations. Moreover, we also complement
our analysis with ROUGE-Recall (ROUGE-R) [86]. ROUGE-R
measures the extent to which the LLMs’ responses cover the re-
trieved knowledge, which is crucial for ensuring comprehensive
information coverage. For open-domain medical Q&A tasks, we
utilize ROUGE-R and Bilingual Evaluation Understudy (BLEU-
1 for answer precision, BLEU-4 for answer fluency) [86] to gauge
the similarity of LLMs responses to the ground-truth doctor anal-
ysis. Additionally, we employ PPL to assess the quality of LLMs
responses.
5.1.6 Experimental Implementation. In HyKGE, 𝑘 = 3,𝑡𝑜𝑝𝐾 =
10,𝛿 = 0.7,𝑙𝑐 = 10,𝑜𝑐 = 4. The prompts for LLMs can refer to
Table 3. Moreover, for all the baselines and HyKGE, we set the
maximum number of returned tokens for LLMs to 500 and the
temperature to 0.6. In all baselines and HyKGE, we first use the
Jieba library in Python to perform word segmentation, and then
use filtered text to filter out tone words and invalid characters
following “chinese_word_cut.txt”12 to avoid errors in knowledge
extraction. For a fair comparison, we apply the same W2NER, GTE
and FlagEmbedding models for all baselines. Moreover, the param-
eters of W2NER are optimized with Adam optimizer [35] with 𝐿2
regularization and dropout on high-quality medical dataset [22, 96],
the learning rate is set to 1e-3, the hidden unit is set to 1024 and
weight decay is 1e-4. Similar to previous work [65], because of the
randomness of LLMs’ outputs, we repeat experiments with differ-
ent random seeds five times and report the average and standard
deviation results. Experimental results are statistically significant
with 𝑝 < 0.05. Implementations are done using the PyTorch 1.9.0
framework [58] in Python 3.9, on an Ubuntu server equipped with
8 A100 GPU and an Intel(R) Xeon(R) CPU.
5.2 Performance Comparison (RQ 1)
To answer RQ1, we conduct experiments and report results of the ac-
curacy on the MMCU-Medical, CMB-Exam and CMB-Clin datasets
with two LLM turbos GPT 3.5 and Baichuan 13B-Chat, as illustrated
in Table 2 and Table 3. From the reported accuracy, we can find the
following observations:
Comparison of RAG methods and Base LLMs. Through com-
parison, we observe that most RAG approaches do not consistently
yield effective outcomes when integrated with KGs, especially in
contrast with the Base model. For instance, the KGRAG method ex-
tracts triples from KG without engaging in essential post-processing
steps like reranking and filtering, thereby infusing an overabun-
dance of noise and compromising the interpretative performance
of LLMs. As for QE tasks, while traditional QE methods typically
show efficacy, LLMs demonstrate a notable difficulty in compre-
hending instructions that necessitate the task-specific rewriting of
multiple-choice questions, which, in turn, detrimentally impacts
LLMs performance in such scenarios. Moreover, this effect is partic-
ularly pronounced in weaker models, such as Baichuan, where the
repercussions of these deficiencies are significantly magnified. How-
ever, the improvement in CoN, CoK, KG-GPT, SuRe and HyKGE is
more remarkable, because leveraging LLMs to explore or organize
knowledge can assist in finding more relational knowledge and the
reranking or filtering methods can highly likely remove irrelevant
noise knowledge chains, and contribute to accuracy improvement.
Comparison of HyKGE and other RAG methods. Firstly, it
is evident that our model, HyKGE, outperforms the baseline models
across all metrics. For instance, the EM and PCR scores see an im-
provement of approximately 8.55%-28.15% and 10.45%-33.29% for
the MMCU-Medical dataset with GPT 3.5 turbo, and the BLEU-1
and ROUGE-R scores see an improvement of approximately40.29%-
278.47% and 7.30%-51.69% for the CMB-Clin dataset with GPT
12https://github.com/Robust-Jay/NLP_Chinese_WordCut/blob/master/stopwords.txt



Source: data\tc16_2312.10997v5\referenced_papers\[81]_2312.15883.pdf (Page 0):

HyKGE: A Hypothesis Knowledge Graph Enhanced Framework
for Accurate and Reliable Medical LLMs Responses
Xinke Jiang*, Ruizhe Zhang*, Yongxin Xu∗
Rihong Qiu, Yue Fang, Zhiyuan Wang, Jinyi Tang, Hongxin Ding
Xu Chu†, Junfeng Zhao†‡, Yasha Wang†‡
Key Laboratory of High Confidence Software Technologies (Peking University)
Ministry of Education; School of Computer Science, Peking University
Beijing, China
ABSTRACT
In this paper, we investigate the retrieval-augmented generation
(RAG) based on Knowledge Graphs (KGs) to improve the accuracy
and reliability of Large Language Models (LLMs). Recent approaches
suffer from insufficient and repetitive knowledge retrieval, tedious
and time-consuming query parsing, and monotonous knowledge
utilization. To this end, we develop aHypothesis Knowledge Graph
Enhanced (HyKGE) framework, which leverages LLMs’ powerful
reasoning capacity to compensate for the incompleteness of user
queries, optimizes the interaction process with LLMs, and provides
diverse retrieved knowledge. Specifically, HyKGE explores the zero-
shot capability and the rich knowledge of LLMs with Hypothesis
Outputs to extend feasible exploration directions in the KGs, as
well as the carefully curated prompt to enhance the density and
efficiency of LLMs’ responses. Furthermore, we introduce the HO
Fragment Granularity-aware Rerank Module to filter out noise
while ensuring the balance between diversity and relevance in
retrieved knowledge. Experiments on two Chinese medical multiple-
choice question datasets and one Chinese open-domain medical
Q&A dataset with two LLM turbos demonstrate the superiority of
HyKGE in terms of accuracy and explainability.
CCS CONCEPTS
• Information systems →Information retrieval query pro-
cessing; Novelty in information retrieval .
KEYWORDS
Natural Language Processing, Large Language Models, Retrieval-
Augmented Generation, Knowledge Graph, Medical Question An-
swering
1 INTRODUCTION
Large Language Models (LLMs) , such as ChatGPT [53] and GPT-
4 [54], have achieved remarkable progress in pivotal areas. By un-
dergoing pre-training on massive text corpora and aligning fine-
tuning to follow human instructions [78, 101], they have recently
demonstrated exceptional performance in a range of downstream
tasks [31]. These achievements underscore the vast potential of
∗Xinke Jiang, Ruizhe Zhang, and Yongxin Xu contributed equally to this research.
{xinkejiang, ruizezhang, xuyx} @stu.pku.edu.cn
†Corresponding authors.
‡Junfeng Zhao is also at the Big Data Technology Research Center, Nanhu Laboratory,
314002, Jiaxing.
LLMs in understanding and generating natural language [73], es-
pecially in the medical domain [ 7, 36, 56, 74, 76, 83, 89, 95, 100].
Despite the advancements of fine-tuning, they still encounter sig-
nificant challenges, including the difficulty in avoiding factual inac-
curacies (i.e., hallucinations and limited explainability) [13, 26, 27],
data constraints (i.e. token resource limit, high training costs, and
privacy concerns)1, catastrophic forgetting [20], outdated knowl-
edge [21], and a lack of expertise in handling specific domains or
highly specialized queries [29]. This undermines their reliability
in areas where accountability and trustworthiness are crucial and
infallible in the medical area [26, 37, 66].
Retrieval-Augmented Generation (RAG) , enhances content
generation by retrieving external information, reduces factual er-
rors in knowledge-intensive tasks with the help of external knowl-
edge and is seen as a promising solution to address incorrect an-
swers, hallucinations, and insufficient interpretability [ 2, 3, 24].
Among the numerous external information sources [88], knowl-
edge graphs (KGs), as a structured data source refined and ex-
tracted through advanced information extraction algorithms, can
provide higher quality context. Compared to documents, KGs em-
body structured knowledge [25, 98], providing succinct content and
facilitating the analysis of intricate relationships among entities,
leading to advanced inference capabilities and enabling extrapola-
tion for efficient knowledge retrieval. They are considered by many
research works to improve the accuracy and reliability of answers
provided by LLMs [57, 81]. However, the gap between unstructured
user queries of inconsistent quality and structured, high-quality
KGs [65] poses significant challenges on how to properly parse
user intent for improving the robustness of retrieved knowledge
(pre-retrieval phase) and how to handle the abundant retrieved
knowledge (post-retrieval phase), which are detailed as follows:
Challenge I: At the pre-retrieval phase, previous works
suffer from how to parse user intent and retrieve reason-
able knowledge based on varying-quality user query. Some
works are based on the Retrieve-Read framework, which initially
obtains knowledge through dense vector retrieval according to user
queries [51, 60, 91]. However, they are stricken with issues such as
unclear expressions and lack of semantic information in the user’s
original query. This misalignment between the semantic spaces of
user queries and high-quality structured knowledge leads to the
retrieval of knowledge that is of insufficient quality and may con-
tain redundant information and noise [8]. In addition, the excessive
redundant knowledge can lead to a waste of token resources, and
1https://www.youtube.com/watch?v=ahnGLM-RC1Y
arXiv:2312.15883v2  [cs.CL]  19 Apr 2024



Source: data\tc16_2312.10997v5\referenced_papers\[81]_2312.15883.pdf (Page 9):

Conference’17, July 2017, Washington, DC, USA Jiang, Zhang and Xu et al.
Figure 4: Case study. We demonstrate the User Query Q, Hypothesis Output HO, Retrieved Reasoning Chains RCand Pruned
Reasoning chains RCprune of HyKGE using GPT-3.5 Turbo to verify the interpretability and effectiveness of HyKGE. Red shed
signifies that the knowledge or answer is derived from evidence in Q, blue shed indicates that the evidence originates from
HO, and green shed represents a corrected answer with the help of KG, despite initially being false in HO.
### User Query Q:
I feel stomach reflux after eating. What medicine should I take?
### Hypothesis Output HO:
Stomach reflux can be a symptom caused by gastroesophageal reflux . 1. Do you have symptoms related to indigestion , excess stomach acid , gastric ulcers ,
bile reflux , or stomach pain ? ... When experiencing mild gastroesophageal reflux, the following treatment plans can be considered:- Avoid overeating - Avoid spicy,
stimulating foods, greasy foods, and alcohol - Maintain a light diet, eat more vegetables. Under appropriate evidence, consider taking H2 receptor antagonists or
proton pump inhibitors . It is not recommended to take calcium carbonate or aluminum hydroxide medications, as calcium carbonate can lead to kidney
stones, and the aluminum element in aluminum hydroxide is toxic and may cause allergic reactions or discomfort.
### Retrieved Reasoning Chains RC:
Kidney stones →Laboratory tests →Serum calcium ←Laboratory tests ←Gastric ulcer. Kidney stones →Clinical symptoms and signs →Fever ←Complications ←
Esophageal cancer. Excess gastric acid →Affected area →Abdomen ←Affected area ←Stomach pain. Niflumic acid →Adverse reaction →Discomfort ←
Adverse reaction ←Calcium carbonate. Gastrointestinal bleeding →Related diseases →Gastric ulcer. Gastric ulcer →Treatment →Aluminum hydroxide.
Calcium carbonate →Indication →Excess gastric acid. Aluminum hydroxide →Indication →Gastric reflux.
Niflumic acid →Third level classification →Others ←Affected area ←Postprandial food reflux.
Duodenogastric reflux and bile reflux gastritis →Treatment →Calcium carbonate. More else ... [125 reasoning chains in total.]
### Pruned Reasoning chains RCprune:
Gastric ulcer →Treatment →Aluminum hydroxide. Calcium carbonate →Indication →Excess gastric acid.
Aluminum hydroxide →Indication →Gastric reflux. Niflumic acid →Third level classification →Others ←Affected area ←Postprandial food reflux.
Duodenogastric reflux and bile reflux gastritis →Treatment →Calcium carbonate. Stomach reflux →Related disease →Excess gastric acid.
Caved-S →Indication →Excess gastric acid. Weile tablets →Indication →Excess gastric acid.
### Answer:
Stomach reflux can be a symptom caused by gastroesophageal reflux disease (GERD) , and this condition could potentially lead to reflux esophagitis. Consider the
following medical recommendations: Acid-suppressing medications (Proton Pump Inhibitors) : You may consider using omeprazole or esomeprazole to effectively
reduce gastric acid secretion, aiding in relieving symptoms of gastric reflux and mitigating the development of reflux esophagitis. Alternatively, you can use
acid-neutralizing medications (antacids) such as magnesium aluminum carbonate (Rennie), calcium carbonate, and aluminum hydroxide. Another option is the use
of H2 receptor antagonists such as ranitidine or famotidine. Additionally, medications like Caved-S or Weile tablets are also suitable for mild to moderate
symptoms of gastric reflux. Gastric protective medications: For short-term relief of stomach pain and discomfort,
you may also consider using gastric mucosal protectants such as sucralfate. It helps form a protective mucous layer, reducing irritation to the stomach mucosa. Diet
and lifestyle adjustments: First, ... it is advisable to consult a healthcare professional for evaluation and diagnosis as soon as possible.
A case study of HyKGE pipeline
1 2 3 4 5
k
0
10
20
30
40
50
60EM Score (%)
Base(MMCU)
Base(CMB)
w/ HO (CMB)
w/o HO (CMB)
w/ HO (MMCU)
w/o HO (MMCU)
101
102
103
104
105
106
107
Retrieval Chains Count
5 7 10 15 30 50
topK
25
30
35
40
45
50EM Score (%)
Base(MMCU)
Base(CMB)
w/ Fragment (CMB)
w/o Fragment (CMB)
w/ Fragment (MMCU)
w/o Fragment (MMCU)
Figure 5: (Left.) Hyper-parameter study with the KG hop 𝑘on
MMCU-Medical and CMB-Exam with GPT 3.5 turbo, from 1
to 5. (Right.) Hyper-parameter study with the reranker 𝑡𝑜𝑝𝐾
on MMCU-Medical and CMB-Exam with GPT 3.5 turbo, from
5 to 50.
Figure 5 (Right.) depicts EM with different reranking thresholds.
Similar to Figure 5 (Left.), as 𝑡𝑜𝑝𝐾 increases, the trends demon-
strate that overwhelming reasoning chains will hamper LLMs’ abil-
ity for comprehension. Meanwhile, it is obvious that HyKGE w/o
Fragment always underperforms on EM as analyzed in Section 5.3.
5.6 Case Study (RQ2 and RQ3)
This case study presents a representative sample that illustrates the
effectiveness of our HyKGE model using GPT-3.5 Turbo as shown
in Table 4. The color coding within the table is key to understand-
ing the source and validity of the information and we have these
observations: i) Compared to a brief user query, semantic spaces of
HOare more abundant and have a clear direction for answering,
helping us better understand user intention and extract more ef-
fective entity information. Ultimately, HyKGE extracted 23 entities
from HOcompared to only 1 from Q. ii) Comparing the RCwith



Source: data\tc16_2312.10997v5\referenced_papers\[169]_2401.17043.pdf (Page 25):

111:26 Lyu, et al.
Table 8. The experimental results for evaluating different large language models in our benchmark.
task name model name bleu rouge-L bertScore RAGQuestEvallength
precision recall
text continuation
ChatGLM2-6B2.06 13 .35 68 .51 20 .68 15 .44 363.3
Qwen-7B 7.10 15.31 77 .94 28 .06 18 .44 159.6
Baichuan2-13B3.97 14 .21 71 .75 28 .62 22 .95 358.4
Qwen-14B 5.70 18 .48 82 .97 27 .89 21 .68 240.1
GPT-3.5-turbo3.66 17 .78 83 .99 26 .96 24 .68 367.6
GPT-4-0613 5.58 19.47 84.91 30.34 28.02 369.8
Qwen2-7B 2.94 16.76 83.82 26.90 23.68 350.0
GPT-4o 4.48 18.85 84.45 30.89 26.11 356.7
summarization
ChatGLM2-6B17.09 28 .16 83 .00 58 .94 40 .35 228.1
Qwen-7B 28.30 30 .21 84 .26 67 .62 40 .03 240.5
Baichuan2-13B24.49 32 .49 85 .64 65 .96 42 .53 179.5
Qwen-14B 32.51 33.33 85 .62 68 .94 40 .57 139.1
GPT-3.5-turbo23.69 33 .53 88 .49 68 .06 46 .18 205.9
GPT-4-0613 24.54 35.91 89.39 71.24 50.53 194.6
Qwen2-7B 14.82 30.00 88.60 62.04 45.93 283.2
GPT-4o 23.24 35.40 89.65 68.28 50.93 217.7
question answering
ChatGLM2-6B29.11 47 .57 79 .59 50 .06 69 .35 90.8
1-document
Qwen-7B 39.63 56 .71 82 .64 51 .77 72 .02 68.8
Baichuan2-13B35.40 53 .85 83 .59 54 .35 76.92 91.3
Qwen-14B 37.95 55 .13 83 .25 53 .03 73 .92 73.8
GPT-3.5-turbo39.76 57.24 83.81 52.67 70 .82 73.3
GPT-4-0613 33.87 51 .42 80 .92 53 .14 62 .39 95.9
Qwen2-7B 23.06 41.25 82.10 60.07 72.17 123.3
GPT-4o 33.32 51.78 83.35 65.33 66.59 74.7
question answering
ChatGLM2-6B15.15 29 .12 82 .30 37 .61 51 .51 193.4
2-document
Qwen-7B 22.61 36 .07 85 .84 42 .32 56 .26 157.6
Baichuan2-13B20.32 35 .56 87 .49 45 .01 61 .47 208.8
Qwen-14B 21.11 34 .97 85 .87 42 .23 56 .59 151.1
GPT-3.5-turbo22.75 37.25 87.16 42 .93 56 .73 149.8
GPT-4-0613 20.38 36 .08 88 .10 49.56 62.56 223.0
Qwen2-7B 15.26 41.25 82.10 48.89 61.41 209.1
GPT-4o 22.84 36.61 88.38 44.04 67.44 124.3
question answering
ChatGLM2-6B14.01 27 .71 83 .42 35 .60 45 .28 204.1
3-document
Qwen-7B 21.63 33 .42 86 .31 39 .14 50 .55 160.6
Baichuan2-13B18.30 33 .34 88 .08 41 .35 55 .75 227.5
Qwen-14B 19.83 33 .33 86 .93 42 .01 51 .70 161.2
GPT-3.5-turbo21.05 35 .04 87 .81 40 .32 51 .37 156.6
GPT-4-0613 19.11 34 .58 88 .88 48.24 56.48 235.1
Qwen2-7B 16.23 32.18 87.69 45.72 55.29 207.2
GPT-4o 22.84 35.98 89.21 43.56 63.90 139.9
hallucination
ChatGLM2-6B13.51 28 .70 71 .26 59 .63 73 .02 176.0
modification
Qwen-7B 22.87 38 .10 73 .52 60 .00 73 .72 172.5
Baichuan2-13B10.56 27 .28 68 .90 54 .42 67 .47 124.8
Qwen-14B 33.78 51 .90 79 .49 67 .05 84.08 89.7
GPT-3.5-turbo32.35 53 .04 80 .49 65 .07 80 .85 64.8
GPT-4-0613 36.69 55.70 81.27 69.18 82.06 63.5
Qwen2-7B 31.07 52.91 80.25 65.48 79.16 49.3
GPT-4o 36.73 54.79 80.90 63.61 73.75 51.9
The top-k value is a crucial parameter for the RAG system, as it determines how many documents
are retrieved for each query. Depending on the scenario, the optimal top-k value may vary. For
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.



### Claim 161/179

#### Claim Text
Methods We used the open source software, MMonCa for the object kinetic Monte Carlo simulation.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[103]_2303.08559.pdf (Page 14):

Table 6: Statistics of nine datasets used. Note that the #mentions for event detection tasks refers to the number of
trigger words, while the #mentions for event argument extraction tasks refers to the number of arguments.
Named Entity Recognition Relation Extraction Event Detection Event Arg Extraction
Dataset CONLL OntoNotes FewNERDTACREV TACREDACE05 MA VEN EREACE05 RAMS ERE
#Label Type 4 18 66 41 41 33 168 38 33 139 38
#Sents Train 14,041 49,706 131,965 68,124 68,124 14,024 32,360 14,736 14,024 7329 14,736
Test 3,453 10,348 37,648 15,509 15,509 728 8,035 1,163 728 871 1,163
#Mentions Train 23,499 128,738 340,247 13,012 13,012 5,349 77,993 6,208 4859 17026 8924
Test 5,648 12,586 96,902 3,123 3,123 424 18,904 551 576 2023 822
Table 7: The statistics of few-shot training sets. We set
different random seeds and generate 5 training sets for
each setting. We report their average statistics.
Dataset Settings # Labels # Sent # Sample # Avg shot
CONLL’03
1-shot
4
4.8 5.8 1.4
5-shot 16.2 21.8 5.5
10-shot 29.2 42.6 10.7
20-shot 65.6 82.0 20.5
OntoNotes
1-shot
18
20.0 33.4 1.9
5-shot 84.8 148.0 8.2
10-shot 158.6 281.0 15.6
20-shot 332.8 547.2 30.4
FewNERD
1-shot
66
89.8 147.0 2.2
5-shot 286.2 494.8 7.5
10-shot 538.0 962.0 14.6
20-shot 1027.2 1851.4 28.1
TACREV
1-shot
41
81.6 41.0 1.0
5-shot 387.6 205.0 5.0
10-shot 741.2 406.0 9.9
20-shot 1367.2 806.0 19.7
50-shot 2872.0 1944.0 47.4
100-shot 4561.0 3520.0 85.9
TACRED
1-shot
41
81.6 41.0 1.0
5-shot 387.6 205.0 5.0
10-shot 741.2 406.0 9.9
20-shot 1367.2 806.0 19.7
50-shot 2871.2 1944.0 47.4
100-shot 4575.2 3520.0 85.9
ACE05
1-shot
33
47.4 41.0 1.2
5-shot 192.8 165.0 5.0
10-shot 334.6 319.4 9.7
20-shot 579.4 598.2 18.1
MA VEN
1-shot
168
157.6 298.0 1.8
5-shot 540.4 1262.2 7.5
10-shot 891.2 2413.8 14.4
20-shot 1286.4 4611.4 27.4
ERE
1-shot
38
48.4 54.6 1.4
5-shot 175.0 219.2 5.8
10-shot 304.8 432.4 11.4
20-shot 521.6 806.6 21.2
ACE05
1-shot
33
23.4 40.2 1.2
5-shot 79.8 178.2 5.4
10-shot 130.8 337.4 10.2
20-shot 213.4 630.2 19.1
RAMS
1-shot
139
130.2 332.6 2.4
5-shot 514.0 1599.6 11.5
10-shot 795.2 3193.2 23.0
20-shot 1070.4 6095.4 43.9
ERE
1-shot
38
21.6 102.8 2.7
5-shot 74.2 403.4 10.6
10-shot 127.2 775.6 20.4
20-shot 190.2 1397.2 36.8
ods by ourselves. We use RoBERTa-large (Liu
et al., 2019) as the backbones. We adopt Auto-
matic Mixed Precision (AMP) training strategy9 to
save memory. We run each experiment on a single
NVIDIA V100 GPU. We train each model with
the AdamW (Loshchilov and Hutter, 2019) opti-
mizer with linear scheduler and 0.1 warm-up steps.
We set the weight-decay coefficient as 1e-5 and
maximum gradient norms as 1.0. We set the batch
size as 64, the maximum input length as 192, the
training step as 500 and the learning rate as 5e-5.
KnowPrompt We implement this method based
on original source code10, and use RoBERTa-large
as our backbones. We set 10 maximum epochs for
50- and 100-shot datasets, and as 50 epochs for
other datasets. We keep all other hyperparameters
as default, and run each experiment on a single
NVIDIA V100 GPU.
PAIE We implement this method on original
source code11, and use BART-large (Lewis et al.,
2020) as backbones. We keep all hyperparameters
as default for ACE and RAMS dataset. For ERE
dataset, we set the training step as 1000, the batch
size as 16 and the learning rate as 2e-5. We run
each experiment on a single NVIDIA V100 GPU.
UIE We implement this method based on original
source code 12, and use T5-large (Raffel et al.,
2020) as the backbones. We run each experiment
on a single NVIDIA Quadro RTX8000 GPU. We
set the batch size as 4 with 4000 training steps.
We set the maximum input length as 800 and the
learning rate as 1e-4.
C LLMs Implementations
Regarding our empirical study, we explore the ICL
abilities of LLMs on few-shot IE tasks. We mainly
use five LLMs from two sources. (1) OpenAI
9https://pytorch.org/docs/stable/amp.html
10https://github.com/zjunlp/KnowPrompt
11https://github.com/mayubo2333/PAIE
12https://github.com/universal-ie/UIE



Source: data\tc16_2312.10997v5\referenced_papers\[36]_2303.08559.pdf (Page 14):

Table 6: Statistics of nine datasets used. Note that the #mentions for event detection tasks refers to the number of
trigger words, while the #mentions for event argument extraction tasks refers to the number of arguments.
Named Entity Recognition Relation Extraction Event Detection Event Arg Extraction
Dataset CONLL OntoNotes FewNERDTACREV TACREDACE05 MA VEN EREACE05 RAMS ERE
#Label Type 4 18 66 41 41 33 168 38 33 139 38
#Sents Train 14,041 49,706 131,965 68,124 68,124 14,024 32,360 14,736 14,024 7329 14,736
Test 3,453 10,348 37,648 15,509 15,509 728 8,035 1,163 728 871 1,163
#Mentions Train 23,499 128,738 340,247 13,012 13,012 5,349 77,993 6,208 4859 17026 8924
Test 5,648 12,586 96,902 3,123 3,123 424 18,904 551 576 2023 822
Table 7: The statistics of few-shot training sets. We set
different random seeds and generate 5 training sets for
each setting. We report their average statistics.
Dataset Settings # Labels # Sent # Sample # Avg shot
CONLL’03
1-shot
4
4.8 5.8 1.4
5-shot 16.2 21.8 5.5
10-shot 29.2 42.6 10.7
20-shot 65.6 82.0 20.5
OntoNotes
1-shot
18
20.0 33.4 1.9
5-shot 84.8 148.0 8.2
10-shot 158.6 281.0 15.6
20-shot 332.8 547.2 30.4
FewNERD
1-shot
66
89.8 147.0 2.2
5-shot 286.2 494.8 7.5
10-shot 538.0 962.0 14.6
20-shot 1027.2 1851.4 28.1
TACREV
1-shot
41
81.6 41.0 1.0
5-shot 387.6 205.0 5.0
10-shot 741.2 406.0 9.9
20-shot 1367.2 806.0 19.7
50-shot 2872.0 1944.0 47.4
100-shot 4561.0 3520.0 85.9
TACRED
1-shot
41
81.6 41.0 1.0
5-shot 387.6 205.0 5.0
10-shot 741.2 406.0 9.9
20-shot 1367.2 806.0 19.7
50-shot 2871.2 1944.0 47.4
100-shot 4575.2 3520.0 85.9
ACE05
1-shot
33
47.4 41.0 1.2
5-shot 192.8 165.0 5.0
10-shot 334.6 319.4 9.7
20-shot 579.4 598.2 18.1
MA VEN
1-shot
168
157.6 298.0 1.8
5-shot 540.4 1262.2 7.5
10-shot 891.2 2413.8 14.4
20-shot 1286.4 4611.4 27.4
ERE
1-shot
38
48.4 54.6 1.4
5-shot 175.0 219.2 5.8
10-shot 304.8 432.4 11.4
20-shot 521.6 806.6 21.2
ACE05
1-shot
33
23.4 40.2 1.2
5-shot 79.8 178.2 5.4
10-shot 130.8 337.4 10.2
20-shot 213.4 630.2 19.1
RAMS
1-shot
139
130.2 332.6 2.4
5-shot 514.0 1599.6 11.5
10-shot 795.2 3193.2 23.0
20-shot 1070.4 6095.4 43.9
ERE
1-shot
38
21.6 102.8 2.7
5-shot 74.2 403.4 10.6
10-shot 127.2 775.6 20.4
20-shot 190.2 1397.2 36.8
ods by ourselves. We use RoBERTa-large (Liu
et al., 2019) as the backbones. We adopt Auto-
matic Mixed Precision (AMP) training strategy9 to
save memory. We run each experiment on a single
NVIDIA V100 GPU. We train each model with
the AdamW (Loshchilov and Hutter, 2019) opti-
mizer with linear scheduler and 0.1 warm-up steps.
We set the weight-decay coefficient as 1e-5 and
maximum gradient norms as 1.0. We set the batch
size as 64, the maximum input length as 192, the
training step as 500 and the learning rate as 5e-5.
KnowPrompt We implement this method based
on original source code10, and use RoBERTa-large
as our backbones. We set 10 maximum epochs for
50- and 100-shot datasets, and as 50 epochs for
other datasets. We keep all other hyperparameters
as default, and run each experiment on a single
NVIDIA V100 GPU.
PAIE We implement this method on original
source code11, and use BART-large (Lewis et al.,
2020) as backbones. We keep all hyperparameters
as default for ACE and RAMS dataset. For ERE
dataset, we set the training step as 1000, the batch
size as 16 and the learning rate as 2e-5. We run
each experiment on a single NVIDIA V100 GPU.
UIE We implement this method based on original
source code 12, and use T5-large (Raffel et al.,
2020) as the backbones. We run each experiment
on a single NVIDIA Quadro RTX8000 GPU. We
set the batch size as 4 with 4000 training steps.
We set the maximum input length as 800 and the
learning rate as 1e-4.
C LLMs Implementations
Regarding our empirical study, we explore the ICL
abilities of LLMs on few-shot IE tasks. We mainly
use five LLMs from two sources. (1) OpenAI
9https://pytorch.org/docs/stable/amp.html
10https://github.com/zjunlp/KnowPrompt
11https://github.com/mayubo2333/PAIE
12https://github.com/universal-ie/UIE



Source: data\tc16_2312.10997v5\referenced_papers\[75]_2305.04757.pdf (Page 15):

Table 7: All experiments results of Figure 3 for different sizes of LMs.
Methods FM2 NQ-Table MedMC-QA ScienceQA
PKG-Davinci 67.3 28.8 47.4 80.76
PKG-Curie 52.0 0.1 23.8 32.87
PKG-Babbage 45.4 0.1 20.0 35.77
PKG-Ada 38.0 0.0 20.6 29.76
Direct-Davinci 59.4 16.9 44.4 71.66
Direct-Curie 50.8 0.0 20.3 36.76
Direct-Babbage 39.5 0.0 20.6 41.08
Direct-Ada 42.6 0.0 20.3 26.79
methods. We can ﬁnd that the retrieval methods cannot provide relevant background documents to
answer the question.
E Errors
Table 13 presents a hallucination error of our PKGs.
16



Source: data\tc16_2312.10997v5\referenced_papers\[82]_2310.12836.pdf (Page 5):

Open-Domain Question Answering The goal
of open-domain question answering (ODQA) task
is to generate answers in response to factual ques-
tions usually with the relevant knowledge retrieved
from the external knowledge source. As the knowl-
edge source, we use Wikipedia which is an open
encyclopedia consisting of millions of documents.
For datasets, we use Natural Questions2 (Lee et al.,
2019) that is modified from Kwiatkowski et al.
(2019) for ODQA and HotpotQA 3 (Yang et al.,
2018), both of which are designed with Wikipedia.
Knowledge Graph Question Answering In ad-
dition to ODQA, we evaluate our KALMV method
on knowledge graph question answering (KGQA),
whose goal is to answer the questions that are an-
swerable by the facts over knowledge graphs. For
datasets, we use WebQSP (Yih et al., 2016) that
is modified from Berant et al. (2013) to filter out
unanswerable questions, and Mintaka (Sen et al.,
2022). Further, for the knowledge source, we use
Wikidata which includes billions of facts that are
represented as the triplet: (subject, relation, object),
and we follow the standard preprocessing setup for
KGQA (Saffari et al., 2021; Baek et al., 2023).
4.2 Baselines and Our Model
We compare our KALMV against relevant base-
lines that augment LMs with external knowledge
and have strategies to reduce hallucinations. Note
that models including verification can refrain from
providing answers if the verifier identifies errors.
Naive Language Models This baseline uses only
the LMs without incorporating external knowledge.
Knowledge-Augmented LMs This baseline aug-
ments LMs with the knowledge retrieved from the
external knowledge base (Wikipedia or Wikidata).
Adaptive Retrieval This baseline (Mallen et al.,
2023) adaptively augments the LMs by retrieving
the knowledge only when the external knowledge is
necessary. In particular, if the entity that appeared
in the question is less frequent, they retrieve the
knowledge and provide it to the LMs. This model,
namely Adaptive Retrieval with Entity, is appli-
cable to questions that have pre-annotated entities
(i.e., KGQA); therefore, we also include its variant,
namely Adaptive Retrieval with Confidence, that
augments LMs with retrieval only when the answer
generation probability of naive LMs is low.
2https://huggingface.co/datasets/nq_open
3https://huggingface.co/datasets/hotpot_qa
LLM-Augmenter This baseline (Peng et al.,
2023) first augments LMs with knowledge retrieval,
and then verifies whether the retrieved knowledge
is reflected in the generated answer with Knowl-
edge F1 (Shuster et al., 2021) that measures over-
lapping terms between the knowledge and the an-
swer. Yet, unlike our KALMV , it cannot identify re-
trieval errors but also uses a heuristic metric for ver-
ification. In addition to the aforementioned LLM-
Augmenter w/ Knowledge F1, we also include the
LLM-Augmenter w/ Confidence that verifies the
answer based on its generation probability.
KALMV This is our Knowledge-Augmented
Language Model Verification (KALMV) method,
which not only verifies both the retrieval and gener-
ation errors with the instruction-finetuned tailored
verifier, but also iteratively rectifies errors.
4.3 Evaluation Metrics
Following the standard evaluation protocol of gen-
erative QA (Mallen et al., 2023; Baek et al., 2023),
we use F1 which measures the number of overlap-
ping words between the generated answer and the
labeled answer with precision/recall, EM which
measures whether the generated answer is exactly
the same as the labeled answer, and accuracy which
measures whether the generated answer includes
the labeled answer. For KGQA, following Baek
et al. (2023), we further consider a set of alternative
names of the labeled answers available in Wikidata.
4.4 Implementation Details
We use the same retriever across different models
for fair comparisons. In particular, for ODQA, we
use BM25 (Robertson et al., 1994) that considers
the term-based matching, following Mallen et al.
(2023). Also, for KGQA, we use MPNet (Song
et al., 2020) that is based on the dense retrieval,
following Baek et al. (2023). For the input prompt
to LMs for all baselines and our model, we fol-
low the existing works (Mallen et al., 2023; Baek
et al., 2023) which use the simple prompt, such as
"Context: {Context}. Question: {Question}. An-
swer: ". Regarding the LMs to generate answers,
we use FLAN (Chung et al., 2022) with three dif-
ferent sizes: Base, Large, and XL having 250M,
780M, and 3B parameters, respectively. In our
KALMV , we use the FLAN Base as the verifica-
tion LM, and we instruction-finetune it with the
batch size of 8 and the learning rate of 5e-5 with
AdamW (Loshchilov and Hutter, 2019) as the op-
timizer. In addition, we set the maximum number



Source: data\tc16_2312.10997v5\referenced_papers\[160]_2308.10633.pdf (Page 8):

Asli Celikyilmaz, Edouard Grave, Yann LeCun, and
Thomas Scialom. 2023. Augmented language mod-
els: a survey. arXiv preprint arXiv:2302.07842.
Niklas Muennighoff, Nouamane Tazi, Loic Magne, and
Nils Reimers. 2023. MTEB: Massive text embedding
benchmark. In Proceedings of the 17th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 2014–2037, Dubrovnik,
Croatia. Association for Computational Linguistics.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
Long Ouyang, Christina Kim, Christopher Hesse,
Shantanu Jain, Vineet Kosaraju, William Saunders,
et al. 2021. Webgpt: Browser-assisted question-
answering with human feedback. arXiv preprint
arXiv:2112.09332.
Youyang Ng, Daisuke Miyashita, Yasuto Hoshi, Ya-
suhiro Morioka, Osamu Torii, Tomoya Kodama, and
Jun Deguchi. 2023. SimplyRetrieve: A private and
lightweight retrieval-centric generative ai tool. arXiv
preprint arXiv:2308.03983.
OpenAI. 2023. GPT-4 technical report. arXiv preprint
arXiv:2303.08774, abs/2303.08774.
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
Lewis, Majid Yazdani, Nicola De Cao, James Thorne,
Yacine Jernite, Vladimir Karpukhin, Jean Maillard,
Vassilis Plachouras, Tim Rocktäschel, and Sebastian
Riedel. 2021. KILT: a benchmark for knowledge
intensive language tasks. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 2523–2544, Online.
Association for Computational Linguistics.
Ori Ram, Liat Bezalel, Adi Zicher, Yonatan Be-
linkov, Jonathan Berant, and Amir Globerson. 2022.
What are you token about? dense retrieval as
distributions over the vocabulary. arXiv preprint
arXiv:2212.10380.
Stephen Robertson and Hugo Zaragoza. 2009. The
probabilistic relevance framework: BM25 and be-
yond. Found. Trends Inf. Retr., 3(4):333–389.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Min-
joon Seo, Rich James, Mike Lewis, Luke Zettle-
moyer, and Wen-tau Yih. 2023. RePlug: Retrieval-
augmented black-box language models. arXiv
preprint arXiv:2301.12652.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F Christiano. 2020. Learn-
ing to summarize with human feedback. In Ad-
vances in Neural Information Processing Systems,
volume 33, pages 3008–3021. Curran Associates,
Inc.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. LLaMA:
Open and efficient foundation language models.
arXiv preprint arXiv:2302.13971.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023b. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing
Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,
and Furu Wei. 2022. Text embeddings by weakly-
supervised contrastive pre-training. arXiv preprint
arXiv:2212.03533.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language processing.
In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations, pages 38–45, Online. Association
for Computational Linguistics.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin
Jiang. 2023. WizardLM: Empowering large lan-
guage models to follow complex instructions. arXiv
preprint arXiv:2304.12244.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik R Narasimhan, and Yuan Cao. 2023.
ReAct: Synergizing reasoning and acting in language
models. In The Eleventh International Conference
on Learning Representations.
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,
Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy
Ba. 2023. Large language models are human-level
prompt engineers. In The Eleventh International
Conference on Learning Representations.
A Appendix
A.1 Computational Resources
The evaluation experiments are conducted on an
Ubuntu 20.04.6 server equipped with Intel(R)
Xeon(R) Gold 6326 CPU at 2.90 GHz CPU cores,
and one node with 4×NVIDIA A100 Tensor Core
GPU with 40 GB memory, and a RAID-5 array
with a Dell(R) PERC H745 Front controller and
KIOXIA(R) PM6-R SAS SSDs for storage. The
CUDA version is 12.2, the Python version is 3.9.16,
the PyTorch version is 2.0.1, and the Transformers
version is 4.29.2.



### Claim 162/179

#### Claim Text
This technique, however, provides discrete measurements for individual species and its measurement accuracy is limited by the isotope ‘scrambling’ fragmentation, i.e. the NO+ fragment ions containing the terminal N atom, rather than the central N attached to the O atom as in the original molecule .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[61]_2212.10509.pdf (Page 6):

Figure 5: Retrieval recall for OneR and IRCoT using Flan-T5-XXL (Left) and GPT3 (Right) in out-of-distribution
(OOD) setting. HQ (HotpotQA), 2W (2WikiMultihopQA), MQ (MuSiQue). The result X →Y indicates prompt
demonstrations are from dataset X and evaluation is on dataset Y .IRCoT outperforms OneR in such an OOD setting.
Figure 6: Answer F1 for NoR QA, OneR QA and IRCoT QA using Flan-T5-XXL (Left) and GPT3 (Right) in
out-of-distribution (OOD) setting. HQ (HotpotQA), 2W (2WikiMultihopQA), MQ (MuSiQue). The result X→Y
indicates prompt demonstrations are from dataset X and evaluation is on dataset Y . IRCoTQA outperforms OneR
QA and NoR QA in such OOD setting.
Figure 7: Number of questions, out of 40, where CoT
generated by GPT3 using different methods has at least
1 factual error. Factual errors: IRCoT < OneR < NoR.
of the facts11 is not true.12 As Fig. 7 shows, NoR
makes the most factual errors, OneR makes fewer,
11all sentences before the final “answer is:” sentence.
12Note that factual error doesn’t necessarily mean the pre-
dicted answer is incorrect and vice-versa. This is because the
model can generate a wrong answer despite all correct facts,
and vice-versa. We also account for the possibility of answer
annotation errors in the original datasets.
and IRCoT the least. In particular, IRCoT reduces
the factual errors over OneR by 50% on HotpotQA
and 40% on 2WikiMultihopQA.
Table 2 illustrates how the CoT predictions for
different methods vary qualitatively. Since NoR
relies completely on parametric knowledge, it often
makes a factual error in the first sentence, which
derails the full CoT. OneR can retrieve relevant
information closest to the question and is less likely
to make such errors early on, but it still makes
errors later in the CoT. IRCoT, on the other hand,
is often able to prevent such errors in each step.
IRCoT is also effective for smaller models.To
see how effective IRCoT is at different LM sizes,
we show the scaling plots in Fig. 8. 13 We com-
pare the recall for OneR andIRCoT using Flan-T5
{base (0.2B), large (0.7B), XL (3B), XXL (11B)},
and GPT3 code-davinci-002 (175B). IRCoT
with even the smallest model (0.2B) is better than
13We skip IIRC here as the smaller models are not good at
identifying Wikipedia titles from a paragraph and a question
which is necessary for IIRC (see App. B).



Source: data\tc16_2312.10997v5\referenced_papers\[61]_2212.10509.pdf (Page 13):

Model HpQA Br HpQA 2WikiMQA MQ 2H MQ
InterAug (Lazaridou et al., 2022) − | − 30.3 | − − | − − | − − | −
RECITE (Sun et al., 2022) − | − 37.1 | 48.4 − | − − | − − | −
ReAct (Yao et al., 2022) − | − 35.1 | − − | − − | − − | −
SelfAsk (Press et al., 2022) − | − − | − 40.1 | − 15.2 | − − | −
DecomP (Khot et al., 2022) − | 50.0 − | − − | 59.3 − | − − | −
DecomP (Khot et al., 2023) * − | − − | 53.5 − | 70.8 − | − − | 30.9
DSP (Khattab et al., 2023) * − | − 51.4 | 62.9 − | − − | − − | −
IRCoT QA (ours) 45.8 | 58.5 49.3 | 60.7 57.7 | 68.0 34.2 | 43.8 26.5 | 36.5
Table 3: Extended comparison with published LLM-based ODQA systems (as of May 25, 2023) on EM and
F1 scores (with new numbers marked with *). ‘ −’: score is unavailable. HpQA Br: Bridge questions subset of
HotpotQA. MQ2H: MuSiQue 2-hop questions. IRCoT remains SOTA for MuSiQue and is close to SOTA for
HotpotQA and 2WikiMultihopQA. Note the comparisons here are not head-to-head as discussed in the text.
Flan-T5-XXL GPT3
Model HotpotQA 2WikiMQA MuSiQue IIRC HotpotQA 2WikiMQA MuSiQue IIRC
ZeroR QA Direct 25.3± 0.3 32.7± 0.3 13.7± 0.3 28.9± 0.3 41.0± 1.1 38.5± 1.1 19.0± 1.2 40.9± 0.7
CoT 22.9 ± 0.1 31.7± 1.5 10.3± 0.5 24.4± 0.1 47.5± 0.4 41.2± 1.0 25.2± 1.2 52.1± 0.1
OneR QA Direct 49.7± 0.5 51.2± 0.3 25.8± 0.6 40.0± 1.3 50.7± 0.1 46.4± 2.9 20.4± 0.3 40.1± 0.9
CoT 43.1 ± 0.7 47.8± 0.9 17.6± 0.2 34.5± 1.5 53.6± 0.7 54.8± 2.1 29.4± 0.8 49.8± 2.3
IRCoT QA Direct 59.1± 0.9 66.5± 1.4 30.8± 0.2 42.5± 2.1 60.6± 1.0 63.5± 2.7 36.0± 0.5 47.9± 2.3
CoT 52.0 ± 0.6 55.1± 1.0 24.9± 1.0 36.5± 1.3 60.7± 1.1 68.0± 1.5 36.5± 1.2 49.9± 1.1
Table 4: Answer F1 for different ODQA models made from NoR, One and IRCoT retrievals, and Direct and
CoT prompting readers. For Flan-T5-XXL, Direct prompting is a better choice for the reader, and for GPT3, CoT
prompting is a better choice for the reader. Hence, we make different reader choices for Flan-T5 and GPT3 for the
experiments in the main paper. Note that IRCoT QA > OneR QA > ZeroR QA holds up regardless of this choice.
DSP (Khattab et al., 2023) provides a way to pro-
grammatically define interactions between LLM
and retrieval for ODQA (e.g., via question decom-
position), bootstrap demonstrations for such a pro-
gram, and use them to make the answer prediction.
It uses GPT3.5 LLM with ColBERT-based retrieval.
Since most of these methods use different knowl-
edge sources or APIs and are built using different
LLMs and retrieval models, it’s difficult to make a
fair scientific comparison across these systems. Ad-
ditionally, the evaluations in the respective papers
are on different random subsets (from the same
distribution) of test instances.
Despite these differences, it is still informative to
explore, in a leaderboard-style fashion, howIRCoT
performs relative to the best numbers published
for these recent systems. Table 3 shows results
from different systems, including contemporane-
ous and newer numbers. The two new systems in
this table (relative to Table 1) are DecomP (newer
version) and DSP. While IRCoT remains SOTA on
MuSiQue, DSP outperforms it on HotpotQA by 2.0
points and the newer version of Decomp outper-
forms IRCoT on 2WikiMultihopQA by 2.8 points.
We speculate DecomP performs well on 2WikiMul-
tihopQA because it has only a few easy-to-predict
decomposition patterns, which DecomP’s question
decomposition can leverage. The lack of such pat-
terns in HotpotQA and MuSiQue causes it to un-
derperform compared to IRCoT. Lastly, it will be
useful to assess whether DSP, which is hardcoded
for 2-hop questions like that of HotpotQA, will
work well for a dataset with a varied number of
hops like that of MuSiQue. We leave this further
investigation to future work.
D Additional CoT Generation Examples
Table 5 provides illustrations, in addition to the
ones provided in Table 2, for how the CoT gen-
erations for NoR QA, OneR QA, and IRCoT QA
methods vary. This gives an insight into how IR-
CoT improves QA performance. Since NoR re-
lies completely on parametric knowledge, it often
makes a factual error in the first sentence, which de-
rails the full reasoning chain. Some of this factual
information can be fixed by OneR, especially infor-
mation closest to the question (i.e., can be retrieved
using the question). This is insufficient for fixing



Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 5):

Method HotPotQA 2WikiMultiHopQA MuSiQue Bamboogle Feverous StrategyQA
EM F1 Acc † EM F1 Acc † EM F1 Acc † EM F1 Acc † Acc Acc† Acc Acc†
Without Retrieval
Direct 21.9 36.8 44.8 21.3 29.2 33.9 7.0 18.7 15.8 11.2 24.4 28.0 60.1 60.1 66.5 66.7
CoT 30.0 44.1 50.0 30.0 39.6 44.0 19.4 30.9 28.6 43.2 51.1 60.0 59.8 59.8 71.0 71.0
With Retrieval
Direct 31.6 44.7 53.3 27.3 35.4 43.6 13.9 28.2 26.5 17.6 31.8 43.2 69.8 69.8 65.6 65.6
ReAct 24.9 44.7 61.1 28.0 38.5 45.9 23.4 37.0 37.9 21.8 31.0 40.3 66.4 66.4 66.9 66.9
Self-Ask 36.8 55.2 64.8 37.3 48.8 55.9 27.6 41.5 42.9 31.5 41.2 54.8 70.7 70.7 70.2 70.2
DSP 43.8 55.0 60.8 - - - - - - - - - - - - -
ITER-RETGEN 1 39.2 53.9 65.5 33.7 45.2 55.4 24.2 38.6 38.1 36.8 47.7 57.6 67.0 67.0 72.0 72.0
ITER-RETGEN 2 44.1 58.6 71.2 34.9 47.0 58.1 26.4 41.1 41.0 38.4 48.7 59.2 68.8 68.8 73.0 73.0
ITER-RETGEN 3 45.2 59.9 71.4 34.8 47.8 58.3 25.7 41.4 40.8 37.6 47.0 59.2 69.0 69.0 72.3 72.3
ITER-RETGEN 4 45.8 61.1 73.4 36.0 47.4 58.5 26.7 41.8 40.8 38.4 49.6 60.0 71.5 71.5 73.8 73.8
ITER-RETGEN 5 45.2 60.3 72.8 35.5 47.5 58.8 25.7 40.7 39.6 39.2 49.7 60.8 70.3 70.3 73.2 73.2
ITER-RETGEN 6 45.9 61.0 73.3 35.5 48.1 59.4 25.9 40.5 39.8 40.0 50.0 59.2 70.9 70.9 72.4 72.4
ITER-RETGEN 7 45.1 60.4 72.9 35.5 47.4 58.4 26.1 42.0 41.0 40.0 50.7 60.8 70.5 70.5 74.1 74.1
Table 2: Evaluation results on multi-hop question answering, fact verification, and commonsense reasoning datasets.
Acc† is the accuracy of model outputs evaluated with text-davinci-003. For ITER -RETGEN, we evaluated LLM
outputs in different iterations (up to 7 iterations). Underlined metric values are higher than those of Self-Ask.
Method HotPotQA 2WikiMultiHopQA MuSiQue Bamboogle Feverous StrategyQA
# API # Doc # API # Doc # API # Doc # API # Doc # API # Doc # API # Doc
ReAct 2.9 14.3 3.0 15.0 2.9 14.4 2.8 14.1 2.1 10.6 2.8 14.2
Self-Ask 3.2 16.0 3.2 15.9 3.0 14.8 3.0 14.9 2.3 11.3 3.0 15.1
Table 3: Average numbers of API calls to text-davinci-003 and retrieved paragraphs for ReAct and Self-Ask.
Note that ITER -RETGEN (T = 2) achieves significantly higher or competitive Acc† with fewer API calls (i.e., 2)
and fewer retrieved paragraphs (5 per iteration, 10 in total).
It is worth noting that, as shown by Table 3, ITER -
RETGEN (T = 2) is superior to or competitive with
ReAct and Self-Ask using fewer API calls to the
LLM (i.e., 2) and fewer retrieved paragraphs (i.e.,
5 per iteration, 10 in total). ITER -RETGEN is also
conceptually simple, which is to iterate retrieval-
augmented CoT, without complex processing.
We also compared I TER -RETGEN with DSP
which also generates the answer using CoT based
on retrieved knowledge but differs in information
collection and processing. In each iteration, ITER -
RETGEN retrieves knowledge based on (1) the ques-
tion and (2) the previous model output which shows
what may be needed to answer the question. With
the number of iterations increasing, we tend to
obtain a more comprehensive and relevant set of
knowledge. Besides, unlike DSP, we do not summa-
rize the retrieved documents for answer generation,
and thus will not introduce summarization errors.
As shown in Table 2, ITER -RETGEN outperforms
DSP significantly. We manually investigate 10 ran-
dom questions where DSP fails but ITER -RETGEN
provides correct answers. On 40% of them, DSP
fails to retrieve documents that cover the correct
answers, while on 50% of them, the summarized
knowledge is misleading, e.g., for the question
“What occupation do Chris Menges and Aram
Avakian share?”, DSP generates a wrong sum-
mary “Chris Menges and Aram Avakian are
both members of the American and British
Societies of Cinematographers.”, while the
retrieved documents mention that Aram Avakian is
a film editor and director, and only Chris Menges
is with the American and British Societies of Cine-
matographers.
Acc† is a Reliable Metric To investigate how re-
liable Acc† is, we focused on model outputs where
EM and Acc† disagree, and manually checked
which metric gives more correct labels. On each
of the four multi-hop question answering datasets,



Source: data\tc16_2312.10997v5\referenced_papers\[61]_2212.10509.pdf (Page 5):

Figure 3: Retrieval recall for one-step retriever (OneR) and IRCoT instantiated from Flan-T5-XXL (left) and GPT3
(right) models. IRCoT outperforms OneR for both models and all datasets.
Figure 4: Answer F1 for ODQA model made using (i) no retriever (NoR QA) (ii) one-step retriever (OneR QA) and
(iii) IRCoT QA instantiated from Flan-T5-XXL (left) and GPT3 (right) models. IRCoT QA outperforms OneR QA
and NoR QA for both models on all datasets, except for GPT3 on IIRC.
Flan-T5-XXL and GPT3 LMs. For both models,
IRCoT significantly outperforms one-step retrieval
across all datasets. For Flan-T5-XXL, IRCoT im-
proves our recall metric relative to one-step re-
trieval, on HotpotQA by 7.9, on 2WikiMultihopQA
by 14.3, on MuSiQue by 3.5, and on IIRC by 10.2
points. For GPT3, this improvement is by 11.3, 22.6,
12.5, and 21.2 points, respectively.
IRCoT QA outperforms NoR and OneR QA.
Fig. 4 compares ODQA performance using
NoR, OneR and IRCoT retriever made from
Flan-T5-XXL and GPT3 LMs. For Flan-T5-XXL,
IRCoT QA outperforms OneR QA on HotpotQA
by 9.4, on 2WikiMultihopQA by 15.3, on MuSiQue
by 5.0 and IIRC by 2.5 F1 points. For GPT3, the
corresponding numbers (except for IIRC) are 7.1,
13.2, and 7.1 F1 points. For GPT3, IRCoT doesn’t
improve the QA score on IIRC, despite signifi-
cantly improved retrieval (21 points as shown in
Fig. 3). This is likely because IIRC relevant knowl-
edge may already be present in GPT3, as also ev-
idenced by its NoR QA score being similar. For
other datasets and model combinations, NoR QA is
much worse than IRCoT QA, indicating the limits
of the models’ parametric knowledge.
IRCoT is effective in OOD setting.Since CoT
may not always be easy to write for new datasets,
we evaluate NoR, OneR, and IRCoT on generaliza-
tion to new datasets, i.e. OOD setting. To do so,
we use prompt demonstrations from one dataset to
evaluate on another dataset.9 For all pairs of the
datasets10 and for both Flan-T5-XXL and GPT3, we
find the same trend as in the IID setting: IRCoT re-
trieval outperforms OneR (Fig. 5), and IRCoT QA
outperforms both OneR QA and NoR QA (Fig. 6).
IRCoT generates CoT with fewer factual errors.
To assess whether our approach also improves the
factuality of generated CoTs, we manually anno-
tated CoTs generated by NoR QA, OneR QA, and
IRCoT QA using GPT3 for 40 randomly sampled
questions from each of the four datasets. We con-
sidered CoT to have a factual error if at least one
9We use the evaluation dataset’s corpus for retrieval.
10We skip IIRC in this exploration as the task is structured
a bit differently and requires special handling (see App. B).



Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 6):

Dataset HotPotQA Feverous
Retriever Original Distilled w/oy1 Distilled w/y1 Original Distilled w/oy1 Distilled w/y1
ITER-RETGEN1 65.5 67.1 67.7 67.0 67.3 70.7
ITER-RETGEN2 71.2 75.2 75.7 68.8 68.1 69.5
Table 4: Effect of using LLM generation y1 on optimizing a dense retriever. We evaluated I TER -RETGEN on
HotPotQA and Feverous in terms of Acc†.
Subset CoT ✓ CoT% w/ Answer Retrieved w/o Answer Retrieved
Method Self-Ask I TER-RETGEN2 Self-Ask ITER-RETGEN2 Self-Ask ITER-RETGEN2 Self-Ask ITER-RETGEN2
HotPotQA 77.5 88.0 52.0 54.4 78.1 86.9 29.9 40.8
2WikiMultiHopQA 68.8 78.2 46.2 42.0 73.1 77.2 30.1 42.3
MuSiQue 68.5 66.9 32.6 30.7 72.9 78.9 12.2 22.9
Bamboogle 73.0 77.3 28.0 32.0 76.2 82.2 32.8 46.2
Table 5: Comparisons between Self-Ask and ITER -RETGEN (T = 2) on different subsets, in terms of Acc†. CoT ✓
is the subset of questions which CoT answers correctly without retrieval; CoT % is the complement. w/ Answer
Retrieved is the subset of questions for which a method (Self-Ask or I TER -RETGEN) successfully retrieves
paragraphs that mention the answers; w/o Answer Retrievedis the complement. ITER -RETGEN tends to be much
better at preserving the LLM’s performance on questions that can be solved using CoT without retrieval, and is
consistently more accurate regardless of whether retrieved knowledge mentions the answers or not.
we randomly sampled 20 model outputs from the
second iteration of ITER -RETGEN, resulting in 80
samples in total. For 98.75% of samples, EM is
0 and Acc† is 1, while Acc† gives the correct la-
bels 97.5% of the time, indicating that EM severely
underestimates model performance. We also car-
ried out the same evaluation for Self-Ask, andAcc†
gives the correct labels 98.75% of the time when it
is inconsistent with EM.
Acc† offers the advantage of identifying model
outputs that are semantically correct, even if
their surface forms differ from the annotated an-
swers. As an illustration, for the question “Which
country Jan Baptist Van Rensselaer’s
father is from?”, the annotated answer isDutch,
while the model prediction is Netherlands, which
is correct in terms of Acc† but is penalized by EM.
Notably, ITER -RETGEN (T ≥ 2) consistently
demonstrate lower EM but higher Acc† than Self-
Ask on 2WikiMultiHopQA, suggesting that en-
hancements in EM do not necessarily reflect im-
provements in the quality of generated answers.
Iteration 1 2 3 4 5 6 7
HotPotQA 49.5 66.1 65.7 66.5 66.7 66.7 67.1
2WikiMultiHopQA 29.0 45.2 46.2 46.7 45.8 45.8 46.5
MuSiQue 18.6 32.3 32.3 33.7 32.7 33.5 32.9
Bamboogle 20.8 36.0 36.8 36.0 35.2 36.0 36.0
Table 6: Answer recall of retrieved paragraphs in differ-
ent iterations for ITER -RETGEN.
Generation Benefits Retrieval Adaptation To
investigate how LLM outputs can be leveraged
for retrieval adaptation, we experimented on Hot-
PotQA and Feverous. Specifically, on each dataset,
we sampled 9,000 random questions from the train
set for training, and 1,000 for validation. We ap-
plied ITER -RETGEN for one iteration, and used the
model outputs y1 for retrieval adaptation as in Sec-
tion 3.4. We used TART (Asai et al., 2022) as the
re-ranker, and distilled knowledge from TART to
the dense retriever for no more than 1,000 steps.
Batch size was 32 and learning rate was 1e-5. We
used the retriever checkpoint with the lowest distil-
lation loss.
As shown by Table 4, retrieval adaptation en-
ables ITER -RETGEN to achieve significantly higher
Acc† with fewer iterations. We also demonstrated
the benefits of using y1 for adaptation by showing
its improvements over a variant which only dif-
fers in that the re-ranker has no access to y1; the
training objective of this variant can be obtained by
removing all y1 notations in Eq. 3.
4.6 Ablation Study
4.6.1 Generation Augments Retrieval
Table 6 shows the answer recall of retrieval in dif-
ferent iterations. The first iteration uses only the
questions for retrieval and suffers from low an-
swer recall. In the second iteration, retrieval, aug-
mented with the LLM output from the first iteration,



### Claim 163/179

#### Claim Text
Respecting the symmetric structure of the axoneme, this model aims to elucidate the tug-of-war scenario of antagonistic motors between filaments at the microscopic scale .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 2):

Preprint
• We present extensive experiments on STS and demonstrate that AnglE can substantially improve
the text embedding quality in various scenarios.
2 R ELATED WORK
This section is organized as follows: we first introduce the unsupervised approaches, then the super-
vised approaches, and finally give a summary.
Unsupervised Approaches Early studies (Hill et al., 2016; Pagliardini et al., 2018) have demon-
strated the efficacy of augmenting word2vec (Mikolov et al., 2013) with n-gram embeddings, yield-
ing strong results in text embeddings. Recently, BERT-flow (Li et al., 2020) has introduced a flow-
based approach that maps BERT embeddings to a standard Gaussian latent space. On the other hand,
BERT-whitening (Su et al., 2021) applies the whitening operation to BERT embeddings to enhance
text embeddings. Furthermore, very recent research (Carlsson et al., 2020; Zhang et al., 2020; Giorgi
et al., 2021; Gao et al., 2021; Yan et al., 2021; Chuang et al., 2022; Jiang et al., 2022b; Zhuo et al.,
2023) has focused on leveraging contrastive objectives to improve the quality of text embeddings.
Supervised Approaches Supervised text embeddings usually perform better than their unsuper-
vised counterparts (Gao et al., 2021). Various studies have effectively utilized supervised datasets to
enhance the learning of text embeddings. In particular, Conneau et al. (2017) introduced a method
that leverages supervised Natural Language Inference (NLI) tasks for this purpose. Building on
a transformer backbone, USE (Cer et al., 2018) incorporates the SNLI dataset to augment unsu-
pervised training, resulting in improved performance. Furthermore, SBERT (Reimers & Gurevych,
2019) enhances text embedding by combining BERT with a siamese architecture. Jiang et al. (2022a;
2023) proposed the use of prompt engineering to improve text embeddings.
However, most existing models optimize the cosine similarity but neglect the negative effect of the
saturation zone of the cosine function. To address this issue, this paper proposes a novel angle-
optimized text embedding model to improve the quality of text embedding.
3 M ETHODOLOGY
This section will introduce the components of the proposed angle-optimized text embedding model,
including the input layer, cosine objective, in-batch negative objective, and angle objective.
3.1 I NPUT LAYER
For the input sentences, we first apply padding to ensure a consistent length l. Next, we map each
word to a continuous d-dimensional space to produce word embeddings ei ∈ Rd. These word
embeddings are then concatenated to form the model input: E = [e1, e2, . . . ,el] ∈ Rl×d. Subse-
quently, the model input is passed through an encoder such as BERT (Devlin et al., 2019), RoBERTa
(Liu et al., 2019), and LLaMA (Touvron et al., 2023a;b) to obtain the contextual representation X.
3.2 C OSINE OBJECTIVE
Following the prior study (Su, 2022), we employ the cosine objective function for end-to-end opti-
mization of cosine similarity between representations, as follows:
Lcos = log

1 +
X
s(Xi,Xj)>s(Xm,Xn)
e
cos(Xm,Xn)−cos(Xi,Xj)
τ

 (1)
where τ is a temperature hyperparameter, cos(·) is the cosine similarity function, and s(u, v) is the
similarity between u and v. By optimizing the Lcos, we expect the cosine similarity of the high
similarity pair to be greater than that of the low similarity pair.
3



Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 0):

Preprint
This is a preprint version. It is recommended to refer to the conference version (ACL24), titled
AoE: Angle-optimized Embeddings for Semantic Textual Similarity . https://aclanthology.
org/2024.acl-long.101/.
ANGLE -OPTIMIZED TEXT EMBEDDINGS
Xianming Li, Jing Li∗
Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR
xianming.li@connect.polyu.hk, jing-amelia.li@polyu.edu.hk
ABSTRACT
High-quality text embedding is pivotal in improving semantic textual similarity
(STS) tasks, which are crucial components in Large Language Model (LLM) ap-
plications. However, a common challenge existing text embedding models face is
the problem of vanishing gradients, primarily due to their reliance on the cosine
function in the optimization objective, which has saturation zones. To address this
issue, this paper proposes a novel angle-optimized text embedding model called
AnglE. The core idea of AnglE is to introduce angle optimization in a complex
space. This novel approach effectively mitigates the adverse effects of the satura-
tion zone in the cosine function, which can impede gradient and hinder optimiza-
tion processes. To set up a comprehensive STS evaluation, we experimented on
existing short-text STS datasets and a newly collected long-text STS dataset from
GitHub Issues. Furthermore, we examine domain-specific STS scenarios with
limited labeled data and explore how AnglE works with LLM-annotated data. Ex-
tensive experiments were conducted on various tasks including short-text STS,
long-text STS, and domain-specific STS tasks. The results show that AnglE out-
performs the state-of-the-art (SOTA) STS models that ignore the cosine saturation
zone. These findings demonstrate the ability of AnglE to generate high-quality
text embeddings and the usefulness of angle optimization in STS. The code is
available at: https://github.com/SeanLee97/AnglE.
1 I NTRODUCTION
The development of text embeddings (Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017;
Cer et al., 2018; Reimers & Gurevych, 2019; Gao et al., 2021) is an essential research challenge in
the NLP community. Text embeddings effectively feature key semantic and syntactic information
in language, which broadly affects the performance of downstream tasks, such as text classification
(Li et al., 2021), sentiment analysis (Suresh & Ong, 2021; Zhang et al., 2022), semantic matching
(Grill et al., 2020; Lu et al., 2020), clustering (Reimers & Gurevych, 2019; Xu et al., 2023), and
question-answering (QA) system (Yue et al., 2021). In particular, text embedding models play a
crucial role in LLMs such as ChatGPT (OpenAI, 2022; 2023), LLaMA (Touvron et al., 2023a;b),
and ChatGLM (Du et al., 2022)-based applications. These LLM-based applications heavily rely on
high-quality text embeddings for tasks such as vector search, where related documents are retrieved
for LLM QA (Asai et al., 2023).
Recent studies (Gao et al., 2021; Jiang et al., 2022b; Chuang et al., 2022; Chanchani & Huang, 2023;
Zhuo et al., 2023) have utilized pre-trained language models such as BERT (Devlin et al., 2019) and
RoBERTa (Liu et al., 2019) in combination with contrastive learning to enhance the quality of text
embeddings. These approaches involve pulling semantically similar samples together and pushing
apart those not (Gao et al., 2021). In these contrastive models, positive samples that are semanti-
cally similar can be generated by data augmentation, while negative samples that are dissimilar are
selected from different texts within the same mini-batch (in-batch negatives). However, supervised
negatives are underutilized, and the correctness of in-batch negatives is difficult to guarantee without
∗Corresponding author
1
arXiv:2309.12871v9  [cs.CL]  31 Dec 2024



Source: data\tc16_2312.10997v5\referenced_papers\[81]_2312.15883.pdf (Page 3):

Conference’17, July 2017, Washington, DC, USA Jiang, Zhang and Xu et al.
Hypothesis Output
LLM
Query and Hypothesis Output
Hypothesis Output Module
W2NER Model
NER Module
Recognized Entities
 Knowledge Graph
Embedding Alignment
Reasoning Chains in Knowledge Graph
Knowledge Graph Retrieval Module
TopK Chains
Reranker
HO Fragment Granularity-aware 
Rerank Module
GTE Encoder
Retrieved 
Knowledge
User Query
Context:
Reasoning Chains
Question:
    User query
Answer
Entities
Data 
Chunker
LLM
Align
User Query
Reader
Entity 
 Relation
Figure 2: The overall framework of HyKGE. HyKGE first feeds the user query ( Q) through the LLMs and obtains Hypothesis
Output (HO). Then through the NER Module, a W2NER model is applied to recognize entities and isolate relations. Through
GTE Encoder, these recognized entities are then linked with entities in KGs. After that, HyKGE extracts three types of relevant
reasoning chains from KGs. Then, because of the sparseness of Q, in the HO Fragment Granularity-aware Rerank Module,
HyKGE chunks Qand HOand align with reasoning chains via a TopK Chains Reranker, to eliminate irrelevant knowledge.
Finally, we organize retrieved knowledge with the user query and obtain answers through LLM Reader.
•Post-Retrieval Phase utilizes the HO Fragment Granularity-
aware rerank approach. First, the hypothesis output and the user
query are segmented into discrete fragments, and subsequently,
we rerank the retrieved reasoning chains based on the fragments.
•LLM Readeris fed with the user query and the pruned retrieved
reasoning chains, organized with carefully designed prompts.
Next, we will delineate each phase in detail in the following sub-
sections and state the overall process in Section 4.5.
4.1 Pre-Retrieval Phase
Firstly, we let LLMs generate hypothesis outputs (HO) in response
to user query Q, and then use the NER model to extract entities
from both HOand Q. During this process, LLMs utilize inherent
medical knowledge to explore potential answers. Although HO
may contain factual errors or hallucinations between entities, the
NER Module focuses solely on the extraction of entities while dis-
regarding the relations, thus significantly isolating the correlation
among medical entities. The subsequent graph retrieval phase (c.f.
Section 4.2) searches the correct reasoning chains to discern and
reintegrate the relationships between medical entities, avoiding
LLMs’ shortages. The combination of HOM and NM provides us with
a direction for exploration and identifies corresponding anchors in
the KGs to guide subsequent graph retrieval, ensuring consistency
and effectiveness in information processing.
4.1.1 Hypothesis Output Module. To enhance the quality of HO,
due to LLMs’ robust reasoning abilities and potential as knowledge
bases, we meticulously design instructions to guide LLMs in a step-
by-step exploration and thoughtful consideration of problems, as
illustrated in Figure 3 (Up.). Here, the prompt (a textual instruction)
is denoted as PHO, and Qto HOas:
HO= LLM(Q|P HO). (1)
Thus, in light of the powerful reasoning abilities as well as the
knowledgeable medical cognition, galore medical knowledge rele-
vant to Qis discovered.
4.1.2 NER Module. Although there still remains a possibility of an
inaccurate comprehension of relationships within HO(i.e. halluci-
nations or misunderstanding between medical entities), training
a discriminative model or using other general-domain LLMs for
authenticity HOis extremely labor-intensive and will lead to error
accumulation. To tackle this issue, we extract entities instead of
relationships, and utilize the completely unmistakable triplets in
KGs for authenticity instead of the relations analyzed in HO. As a
consequence, we have trained a medical Named Entity Recognition
(NER) model using the CMEEE dataset3 [22, 96]. Our NER Module
3https://tianchi.aliyun.com/dataset/144495



Source: data\tc16_2312.10997v5\referenced_papers\[105]_2310.01558.pdf (Page 14):

Published as a conference paper at ICLR 2024
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha
Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language
models. In The Eleventh International Conference on Learning Representations , 2023. URL
https://openreview.net/forum?id=1PL1NIMMrw.
Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing datasets for multi-hop read-
ing comprehension across documents.Transactions of the Association for Computational Linguis-
tics, 6:287–302, 2018. doi: 10.1162/tacl a 00021. URL https://aclanthology.org/
Q18-1021.
Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-
tence understanding through inference. InProceedings of the 2018 Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, 2018. Association for Com-
putational Linguistics. doi: 10.18653/v1/N18-1101. URL https://aclanthology.org/
N18-1101.
Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and
Jonathan Berant. Break it down: A question understanding benchmark. Transactions of the
Association for Computational Linguistics , 8:183–198, 2020. doi: 10.1162/tacl a 00309. URL
https://aclanthology.org/2020.tacl-1.13.
Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. An-
swering questions by meta-reasoning over multiple chains of thought. In Houda Bouamor,
Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Meth-
ods in Natural Language Processing , pp. 5942–5966, Singapore, December 2023. Associa-
tion for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.364. URL https:
//aclanthology.org/2023.emnlp-main.364.
Zexuan Zhong, Zhengxuan Wu, Christopher Manning, Christopher Potts, and Danqi Chen.
MQuAKE: Assessing knowledge editing in language models via multi-hop questions. In Houda
Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing , pp. 15686–15702, Singapore, December 2023.
Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.971. URL
https://aclanthology.org/2023.emnlp-main.971.
Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. Context-faithful prompting for
large language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),Findings of the As-
sociation for Computational Linguistics: EMNLP 2023, pp. 14544–14556, Singapore, December
2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.968.
URL https://aclanthology.org/2023.findings-emnlp.968.
A A PPENDIX
A.1 M ODELS
Llama-2 In all cases, we use the vanilla variant of the Llama-2 models from https://
huggingface.co/meta-llama, with half precision.
Decomposition Generation Questions in our multi-hop datasets require between 2-4 decomposi-
tion steps. Hence we limit the number of generation steps to 5. In Tab. 8 we show that the number
of cases in which the model does not arrive at an answer in 5 steps, termed as failures, is very
small when generating with top-1 results from G OOGLE SEARCH , at 0.4% for 2W IKI MQA and
1.2% for STRATEGY QA. Failures are much higher when retrieving random contexts, at 37.0% for
2WIKI MQA and 34.4% for S TRATEGY QA. These are usually cases the model enters an infinite
loop. Following recent work, (Wang et al., 2023; Yoran et al., 2023) we use greedy decoding when
generating decompositions.
15



### Claim 164/179

#### Claim Text
The first step is to determine a preliminary velocity field ˜uf , solution of the momentum equation (5.1) in the absence of any immersed-boundary forcing, viz. ˜uf −un f ∆t = RHSn+1/2 . (5.3) 3 Modelling approaches and computational methods for particle-laden turbulent flows (a) (b) (c) δ(1) h ∆x -2 -1 0 1 2 0 0.2 0.4 0.6 0.8 r/∆x Figure 5.1: (a) Discrete delta function kernel δ(1) h (r) used in the interpolation (5.4) and spreading (5.6) of variables between Eulerian and Lagrangian locations: 3-point function of ; 4point function of .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 20):

the compute budget. We start with the Equation (1.6), repeated here for convenience:
L(N,S) =
(Nc
N
)αN
+
(Sc
S
)αS
. (B.1)
Here, S represents the number of parameter updates when training at the critical batch size [MKAT18],
which was deﬁned in Equation (5.2)9:
B(L) = B∗
L1/αB
. (B.2)
We would like to determine optimal training parameters for a ﬁxed compute budget, so we replace S =
C/(6NB (L)), where Cis the number of FLOPs used in the training run:
L(N,C) =
(Nc
N
)αN
+
(
6B∗Sc
N
L1/αBC
)αS
. (B.3)
Now, we set ∂NL
⏐⏐
C = 0to ﬁnd the condition for optimality:
0 = ∂L
∂N
⏐⏐
C
= −αN
N
(Nc
N
)αN
+ αS
N
(
6B∗Sc
N
L1/αBC
)αS (
1 −5N
L∂L
∂N
⏐⏐
C
)
=⇒ αN
αS
(Nc
N
)αN
=
(
6B∗Sc
N
L1/αBC
)αS
(B.4)
Equation (B.3) and (B.4) together determine the compute-efﬁcient frontier.
B.2 Efﬁcient Training
Now we assemble the implications of (B.3) and (B.4). First, note that inserting (B.4) into (B.3) yields
L(Neﬀ (C) ,C) =
(
1 +αN
αS
)
L(Neﬀ,∞) , (B.5)
which implies that for compute-efﬁcient training, we should train to a ﬁxed percentage αN
αS
≈10% above
the converged loss. Next, let’s determine how the optimal loss depends on the compute budget. Eliminating
N yields a power-law dependence of performance on compute:
L(C) =
(Cc
C
)αC
(B.6)
where we deﬁned
αC = 1/(1/αS + 1/αB + 1/αN) ≈0.052 (B.7)
Cc = 6NcB∗Sc
(
1 +αN
αS
)1/αS+1/αN (αS
αN
)1/αS
. (B.8)
Similarly, we can eliminate Lto ﬁnd N(C):
N(C)
Nc
=
(C
Cc
)αC/αN (
1 +αN
αS
)1/αN
(B.9)
and
S(C) = Cc
6NcB∗
(
1 +αN
αS
)−1/αN (C
Cc
)αC/αS
(B.10)
9There is a slight ambiguity here: we can imagine training either at a constant batch size B (Ltarget), or we could
instead train at a variable batch size ˜B (L), where ˜B is the instantaneous critical batch size (as opposed to B, which is
the averaged version). These two prescriptions result in the same number of steps, so we can ignore this subtlety (see
[MKAT18]).
21



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 19):

Appendices
A Summary of Power Laws
For easier reference, we provide a summary below of the key trends described throughout the paper.
Parameters Data Compute Batch Size Equation
N ∞ ∞ Fixed L(N) = (Nc/N)αN
∞ D Early Stop Fixed L(D) = (Dc/D)αD
Optimal ∞ C Fixed L(C) = (Cc/C)αC
(naive)
Nopt Dopt Cmin B ≪Bcrit L(Cmin) =
(
Cmin
c /Cmin
)αmin
C
N D Early Stop Fixed L(N,D) =
[(Nc
N
)αN
αD + Dc
D
]αD
N ∞ Ssteps B L(N,S) =
(Nc
N
)αN
+
(
Sc
Smin(S,B)
)αS
Table 4
The empirical ﬁtted values for these trends are:
Power Law Scale (tokenization-dependent)
αN = 0.076 Nc = 8.8 ×1013 params (non-embed)
αD = 0.095 Dc = 5.4 ×1013 tokens
αC = 0.057 Cc = 1.6 ×107 PF-days
αmin
C = 0.050 Cmin
c = 3.1 ×108 PF-days
αB = 0.21 B∗= 2.1 ×108 tokens
αS = 0.76 Sc = 2.1 ×103 steps
Table 5
The optimal parameters for compute efﬁcient training are given by:
Compute-Efﬁcient Value Power Law Scale
Nopt = Ne ·CpN
min pN = 0.73 Ne = 1.3 ·109 params
B ≪Bcrit = B∗
L1/αB = BeCpB
min pB = 0.24 Be = 2.0 ·106 tokens
Smin = Se ·CpS
min (lower bound) pS = 0.03 Se = 5.4 ·103 steps
Dopt = De ·CpD
min (1 epoch) pD = 0.27 De = 2·1010 tokens
Table 6
B Empirical Model of Compute-Efﬁcient Frontier
Throughout this appendix all values of C,S, and αC are adjusted for training at the critical batch size Bcrit.
We have left off the ‘adj’ label to avoid cluttering the notation.
B.1 Deﬁning Equations
The power-law ﬁt to the learning curves implies a simple prescription for compute-efﬁcient training. In this
appendix, we will derive the optimal performance, model size, and number of training steps as a function of
20



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 13):

104 106 108
Parameters (non-embedding)
2
3
4
5
6
7
8Test Loss
Performance vs Compute Budget
10 5
10 4
10 3
10 2
10 1
100
PF-dayss
106 107 108 109
Parameters (non-embedding)
2.4
3.0
3.6
4.2
4.8
5.4Test Loss
Performance vs Steps
104
105
Steps
Figure 11 When we hold either total compute or number of training steps ﬁxed, performance follows
L(N,S) from Equation (5.6). Each value of compute budget has an associated optimal model size that
maximizes performance. Mediocre ﬁts at small Sare unsurprising, as the power-law equation for the learning
curves breaks down very early in training.
Parameter αN αS Nc Sc
Value 0.077 0.76 6.5 ×1013 2.1 ×103
Table 3 Fits to L(N,S)
With these parameters, we obtain the learning curve ﬁts in Figure 4. Though the ﬁts are imperfect, we believe
they are quite compelling given the simplicity of Equation (5.6).
The data and ﬁts can be visualized in a different and more interesting way, as shown in Figure 11. There we
study the test loss as a function of model size while ﬁxing either the total non-embedding compute C used
in training, or the number of steps S. For the ﬁts we use Equation (5.5) and (5.4) along with the parameters
above and Equation (5.6).
The power-law dependence of the loss on Smin reﬂects the interplay of optimizer dynamics and the loss
landscape. Since the ﬁts are best late in training, when the loss may be approximately quadratic, the power-
law should provide information about the spectrum of the Hessian of the loss. Its universality suggests that
the Hessian eigenvalue density is roughly independent of model size.
5.3 Lower Bound on Early Stopping Step
The results for L(N,Smin) can be used to derive a lower-bound (and rough estimate) of the step at which
early stopping should occur when training is data limited. It is motivated by the idea that ﬁnite and inﬁniteD
learning curves for a given model will be very similar until we reach Smin ≈Sstop. Thus overﬁtting should
be proportional to the correction from simply ending training atSstop. This will underestimate Sstop, because
in reality the test loss will decrease more slowly when we have a ﬁniteD, and therefore we will require more
training steps to reach the optimal test loss at ﬁnite D. This line of reasoning leads to the inequality
Sstop(N,D) ≳ Sc
[L(N,D) −L(N,∞)]1/αS
(5.7)
where L(N,∞) is the converged loss, evaluated with inﬁnite available data. This inequality and its com-
parison to the empirical data is displayed in Figure 16 in the appendix. In that ﬁgure, the values of Sstop
and L(N,D) are empirical (though Sstop is adjusted to mimic training at B ≫Bcrit), while L(N,∞) is
computed from the ﬁt to L(N,D) evaluated at D= ∞.
6 Optimal Allocation of the Compute Budget
We displayed the empirical trend of performance as a function of the computation used during training in
the top-right of Figure 1. However, this result involved training at a ﬁxed batch size B, whereas we know
14



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 21):

B.3 Comparison to Inefﬁcient
Typically, researchers train models until they appear to be close to convergence. In this section, we compare
the efﬁcient training procedure described above to this more typical setup. We deﬁne a the convergence factor
f as the percent deviation from the converged loss:
L(N,C) = (1 +f) L(N,∞) . (B.11)
For compute-efﬁcient training we have f = αN/αS ≈ 10% from the previous section, but researchers
typically use a much smaller value. Here, we choose f′= 2%as an estimate. For a ﬁxed value of the loss,
we predict:
Nf
Nf′
=
(1 +f
1 +f′
)1/αN
≈2.7 (B.12)
Sf
Sf′
=
(
1 +1
f
1 + 1
f′
)1/αS
≈0.13 (B.13)
Cf
Cf′
= Nf
Nf′
Sf
Sf′
≈0.35 (B.14)
So that compute-efﬁcient training uses 7.7x fewer parameter updates, 2.7x more parameters, and 65% less
compute to reach the same loss.
B.4 Suboptimal Model Sizes
We can solve A.1 to ﬁnd an expression for the amount of compute needed to reach a given value of the loss
Lwith a model of size N:
C(N,L) =
(
6B∗Sc
N
L1/αB
)(
L−
(Nc
N
)αN)−1/αS
. (B.15)
Using A.6 and A.9, we can eliminate Lin favor of Neﬀ (L), the model size which reaches Lmost efﬁciently.
From there, we ﬁnd an expression for the excess compute needed as a consequence of using a suboptimal
model size:
C(N,Neﬀ)
C(Neﬀ,Neﬀ) = N
Neﬀ
[
1 +αS
αN
(
1 −
(Neﬀ
N
)αN)]−1/αS
. (B.16)
The result is shown in Figure X. Models between 0.6x and 2.2x the optimal size can be used with only a
20% increase in compute budget. Using a smaller model is useful when accounting for the cost inference. A
larger model can be trained the the same level of performance in fewer steps, allowing for more parallelism
and faster training if sufﬁcient harware is available (see Figure Y):
S(N,Neﬀ)
S(Neﬀ,Neﬀ) =
[
1 +αS
αN
(
1 −
(Neﬀ
N
)αN)]−1/αS
. (B.17)
A 2.2x larger model requires 45% fewer steps at a cost of 20% more training compute. Note that this equation
should not be trusted for very large models, as it is only valid in the power-law region of the learning curve
after initial transient effects.
C Caveats
In this section we list some potential caveats to our analysis.
• At present we do not have a solid theoretical understanding for any of our proposed scaling laws.
The scaling relations with model size and compute are especially mysterious. It may be possible to
understand scaling at very large Dholding model size ﬁxed [AS17], and also the shape of learning
curves late in training, by modeling the loss with a noisy quadratic. But the scaling with Dat very
large model size still remains mysterious. Without a theory or a systematic understanding of the
corrections to our scaling laws, it’s difﬁcult to determine in what circumstances they can be trusted.
22



Source: data\tc16_2312.10997v5\referenced_papers\[174]_2001.08361.pdf (Page 14):

Models between 0.6x and 2.2x the 
optimal size can be trained with a 
20% larger compute budget
Smaller models require 
more steps to train, while 
larger models require fewer
Our framework does not 
capture early training dynamics
Figure 12 Left: Given a ﬁxed compute budget, a particular model size is optimal, though somewhat larger
or smaller models can be trained with minimal additional compute. Right: Models larger than the compute-
efﬁcient size require fewer steps to train, allowing for potentially faster training if sufﬁcient additional paral-
lelism is possible. Note that this equation should not be trusted for very large models, as it is only valid in the
power-law region of the learning curve, after initial transient effects.
10 8
 10 6
 10 4
 10 2
 100
Compute (PF-days), non-embedding
2
3
4
5
6
7Test Loss
L = (Cmin/2.3 108) 0.050
L = (C/2.0 107) 0.057
Figure 13 When adjusting performance to simulate training far below the critical batch size, we ﬁnd a
somewhat altered power law for L(Cmin) when compared with the fully empirical results. The conspicuous
lump at 10−5 PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks
in the power-law ﬁts. It is the L(Cmin) trend that we expect to provide a reliable extrapolation for larger
compute.
that in fact we could train more efﬁciently 6 by training at the batch size Bcrit discussed in Section 5.1.
Large and small values of the loss could have been achieved with fewer samples or fewer steps, respectively,
and correcting for this inefﬁciency by standardizing to the critical batch size results in cleaner and more
predictable trends.
In this section we will adjust for this oversight. More importantly, we will use the results of Section 5
to determine the optimal allocation of compute between model size N and the quantity of data processed
during training, namely 2BcritSmin. We will determine this allocation both empirically and theoretically, by
using the equation for L(N,Smin), and we will demonstrate that these methods agree.
6.1 Optimal Performance and Allocations
Let us ﬁrst study the loss as a function of the optimally allocated compute from Equation (5.5). The result is
plotted in Figure 13, along with a power-law ﬁt. We see that as compared to the compute plot of Figure 1, the
new ﬁt with Cmin is somewhat improved.
Given L(Cmin), it is natural to ask for the optimal model sizeN(Cmin) that provides the minimal loss with a
given quantity of training compute. The optimal model size is shown in Figure 14. We observe thatN(Cmin)
6One might ask why we did not simply train at Bcrit in the ﬁrst place. The reason is that it depends not only on the
model but also on the target value of the loss we wish to achieve, and so is a moving target.
15



### Claim 165/179

#### Claim Text
The δ-doped and (100)-implanted samples exhibit comparable fluorescence in the bulk, whereas the (111)-implanted one has fluorescence about four times smaller, which could be related to a less efficient NV yield by the implantation at this crystallographic orientation .By the δ-doped sample the saturation curves were measured before and after each annealing step.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 15):

0 50 100 150
1.6
1.8
2
2.2
2.4
2.6
2.8
3 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(a) ELI5 - p = 10%
0 50 100 150
1.4
1.45
1.5
1.55
1.6
1.65 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (b) ELI5 - p = 30%
0 50 100 150
1.04
1.06
1.08
1.1
1.12
1.14 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (c) ELI5 - p = 50%
0 10 20 30 40 50
1.6
1.8
2
2.2
2.4
2.6
2.8
3
3.2
3.4 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(d) MS MARCO - p = 10%
0 10 20 30 40 50
1.15
1.2
1.25
1.3
1.35
1.4 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
0.91 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (f) MS MARCO - p = 50%
0 20 40 60 80 100
1.5
2
2.5
3
3.5
1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(g) NQ - p = 10%
0 20 40 60 80 100
1.5
1.6
1.7
1.8
1.9
2 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
Loa ding [ M a thJ a x ] / ex tensions/ M a thM enu .j s (h) NQ - p = 30%
0 20 40 60 80 100
1.22
1.24
1.26
1.28
1.3
1.32
1.34
1.36 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (i) NQ - p = 50%
Figure 7: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-12), for FiD-Base models.
Each row represents a different dataset, and every column represents a different filtering percentage (10,30,50).



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 17):

0 50 100 150
1.4
1.6
1.8
2
2.2
2.4
2.6 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(a) ELI5 - p = 10%
0 50 100 150
1.35
1.4
1.45
1.5
1.55
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (b) ELI5 - p = 30%
0 50 100 150
1.04
1.06
1.08
1.1
1.12
1.14 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (c) ELI5 - p = 50%
0 10 20 30 40 50
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
3.2 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(d) MS MARCO - p = 10%
0 10 20 30 40 50
1.1
1.15
1.2
1.25
1.3
1.35 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.84
0.86
0.88
0.9
0.92 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (f) MS MARCO - p = 50%
0 20 40 60 80 100 120
1.5
2
2.5
3
3.5
4
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(g) NQ - p = 10%
0 20 40 60 80 100 120
1.5
1.6
1.7
1.8
1.9
2
2.1 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
Loa ding [ M a thJ a x ] / ex tensions/ M a thM enu .j s (h) NQ - p = 30%
0 20 40 60 80 100 120
1.25
1.3
1.35
1.4
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (i) NQ - p = 50%
Figure 9: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-24), for FiD-Large models.
Each row represents a different dataset, and every column represents a different filtering percentage (10,30,50).



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 13):

0 1000 2000 3000 4000
24.5
25
25.5
26
26.5
27
FLOPS (MACs)
Rouge-L
2%
(a) ELI5
0 1000 2000 3000 4000
20
21
22
23
24
25
FLOPS (MACs)
2% (b) MS MARCO
0 1000 2000 3000 4000
24
25
26
27
28
29
30
31
32
FLOPS (MACs)
2% (c) NQ
0 20 40 60 80 100
24.5
25
25.5
26
26.5
27
Input P sgs
Rouge-L
2%
(d) ELI5
0 20 40 60 80 100
20
21
22
23
24
25
Input P sgs
2% (e) MS MARCO
0 20 40 60 80 100
24
25
26
27
28
29
30
31
32
FiD
CALM
T ok en Filtering
Combined
Input P sgs
2% (f) NQ
Figure 6: The ROUGE-L performance on FiD-Base vs. the FLOPS (MACs) (First row), and vs. the input passage
amount (Input Psg., second row). The results are on the test set for each dataset, for the various methods utilized.
We observe that the trends of the FLOPS (MACs) and the Input Psg. are very similar, since the passages effect the
encoder the most, and the encoder has the most impact on FLOPS (MACs) (as stated by de Jong et al. (2022)).



Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 14):

Language Models as Semantic Indexers
0 10 20 30 40 50
#Doc in each ID
0.0
0.1
0.2
0.3
0.4Density
(a) Beauty
0 10 20 30 40 50 60 70 80
#Doc in each ID
0.00
0.05
0.10
0.15
0.20
0.25
0.30Density (b) Sports
0 5 10 15 20 25 30 35
#Doc in each ID
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7Density (c) Toys
Figure 6.Duplication study of LMI NDEXER ’s semantic IDs.
00.0050.010.0150.020.0250.030.0350.040.045
1 2 3
Recall@5
ID length
(a) beauty
00.0050.010.0150.020.025
1 2 3
Recall@5
ID length (b) sports
00.0050.010.0150.020.0250.030.0350.040.045
1 2 3
Recall@5
ID length (c) toys
Figure 7.Semantic ID length study on recommendation.
to unseen documents, demonstrating its strong semantic
capturing capability.
Table 15.Zero-shot study.
Model Recall@50 Recall@100
rq-V AE indexer 0.0000 0.0105
HC indexer 0.0000 0.0070
LMINDEXER 0.0455 0.0524
15



Source: data\tc16_2312.10997v5\referenced_papers\[30]_2312.06648.pdf (Page 5):

101 102 103
Frequency
20
40
60Recall@5
SimCSE
101 102 103
Frequency
20
40
60Recall@5
Contriever
101 102 103
Frequency
40
50
60Recall@5
DPR
101 102 103
Frequency
50
60
70
80Recall@5
GTR
Passage Sentence Proposition
Figure 3: Document retrieval recall vs. the frequency of the target entity in each question from the Entity Questions
dataset. The frequency of each entity (i.e. smaller value ⇒ less common entities, and vice versa) is estimated by
the frequency of the entity in its top-1000 passage retrieved by BM25. On queries with less common entities, we
observe that retrieving by proposition shows a larger advantage over retrieval by proposition.
Retriever Granularity NQ TQA WebQ SQuAD EQ Avg.
top-5 top-20 top-5 top-20 top-5 top-20 top-5 top-20 top-5 top-20 top-5 top-20
Unsupervised Dense Retrievers
SimCSE Passage 16.6 23.6 32.3 40.8 15.5 19.1 14.6 20.7 16.1 20.3 19.0 24.9
Sentence 20.7 28.1 36.0 44.5 18.5 21.9 19.6 25.8 19.9 25.1 23.0 29.1
Proposition24.5 33.1 37.5 46.2 19.7 23.0 21.4 27.6 26.8 32.0 26.0 32.4
Contriever Passage 23.2 35.1 40.8 50.8 16.3 22.1 23.9 32.7 20.2 27.9 24.9 33.7
Sentence 26.0 36.8 43.4 52.9 18.4 23.9 26.7 34.7 23.7 30.3 27.6 35.7
Proposition28.9 39.2 47.2 55.6 19.5 25.2 30.8 37.6 28.8 35.8 31.1 38.7
Supervised Dense Retrievers
DPR Passage 41.1 45.6 50.6 57.0 23.7 25.5 18.8 25.4 25.3 29.7 31.9 36.6
Sentence 40.3 45.6 51.7 57.6 24.0 26.9 21.1 27.4 28.6 32.9 33.1 38.1
Proposition39.7 45.2 51.0 56.8 24.3 27.5 22.2 28.3 32.0 36.0 33.9 38.8
GTR Passage 39.8 46.1 49.7 55.9 23.0 25.9 29.9 35.1 37.8 39.6 36.0 40.5
Sentence 39.4 45.9 51.7 58.0 23.2 26.1 35.7 39.1 38.0 39.9 37.6 41.8
Proposition40.0 46.9 52.5 58.4 24.2 26.5 37.8 40.4 39.2 41.0 38.7 42.6
Table 4: Open-domain QA performance (Exact Match) using Fusion-in-Decoder model (Izacard and Grave, 2021)
to extract answer from top-5 and top-20 passages retrieved on the index of passages, sentences, and propositions.
with 16% relative improvements.
5.2 Retrieval on Finer-grained Index ⇒
Better Cross-Task Generalization
Our results show the advantage of retrieval on
proposition-level index in cross-task generalization
settings. We observe that on SQuAD and Entity
Questions, retrieval on the proposition-level index
brings more performance gain over the passage-
level index and sentence-level index.
To better understand where the improvements
can be attributed, we conduct an additional analysis
on Entity Questions. As Entity Questions features
questions targeting the properties of longer-tail enti-
ties, we study how the retrieval performance under
three different granularities is affected by the occu-
rance of the target entity in question, i.e. whether
the entity appears frequently in Wikipedia or not.
We estimate the frequency of each entity with the
following method. Given the surface form of an en-
tity, we use BM25 to retrieve the top 1000 relevant
passages from Wikipedia. We use the number of
occurrences of the entity in its relevant passages as
an estimate of its frequency. With the 20,000 test
queries, around 25% of the target entities have an
frequency value of less or equal to 3.
Figure 3 shows the passage retrieval perfor-
mance vs. the frequency of the target entity in
each question. Across all four dense retrievers,
we observe that retrieving by proposition shows a
much larger advantage over retrieving by passages
with questions targeting less common entities. As
the frequency of entities increases, the performance
gap decreases. Our findings indicate that the per-
formance gain from retrieval by proposition can
mostly be attributed to queries for long-tailed infor-
mation. This echoes our observation that retrieval
on proposition-level index improves the cross-task
generalization performance of dense retrievers.
5.3 Higher Passage Recall ⇒ Higher
Downstream QA Accuracy
To further understand whether the passage retrieval
on a finer-grained index achieves higher down-
6



### Claim 166/179

#### Claim Text
Interestingly, the equation first derived there has recently been rederived in using a molecular mechanics approach.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 34):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-
Chemistry
Original Question: Which of the following indicates that a reaction is
spontaneous?
0) at equilibrium there are more products than reactants
1) the value of ∆G0 is greater than zero
2) the value of ∆S0 is greater than zero
3) the value of Keq is less than one
Original Answer [Incorrect]: 2) the value of ∆S- is greater than zero.
A reaction is spontaneous if the value of ∆S- is greater than zero. This is
because a positive ∆S- indicates that the reaction results in an increase in
entropy, which is a measure of disorder. A spontaneous reaction is one that
occurs without the input of energy, and an increase in entropy is a natural
tendency of the universe.
Stepback Question: What are the chemistry principles behind this question?
Principles:
Spontaneous reactions: A spontaneous reaction is a reaction that occurs without
the input of energy.
Final Answer: A reaction is spontaneous if the change in Gibbs free energy is
negative (∆G <0). The value of ∆G0 is greater than zero indicates that the
reaction is not spontaneous.
The correct answer is 0) at equilibrium there are more products than reactants.
MMLU-
Chemistry
Original Question: Potassium-40 is a minor isotope found in naturally oc-
curring potassium. It is radioactive and can be detected on simple radiation
counters. How many protons, neutrons, and electrons does potassium-40 have
when it is part of K2SO4?
0) 21 neutrons, 19 protons, 18 electrons
1) 20 neutrons, 19 protons, 19 electrons
2) 21 neutrons, 19 protons, 19 electrons
3) 19 neutrons, 19 protons, 19 electrons
Original Answer [Incorrect]: The correct answer is 2).
Potassium-40 has 19 protons and 21 neutrons. When it is part of K2SO4, it has
19 electrons.
Stepback Question: What are the chemistry principles behind this question?
Principles:
Atomic number: The atomic number of an element is the number of protons in
the nucleus of an atom of that element.
Final Answer: The atomic number of potassium is 19, so all potassium atoms
have 19 protons. The mass number of potassium-40 is 40, so it has 40 - 19 =
21 neutrons. When potassium is part of a compound, it loses one electron to
become a potassium ion. Therefore, potassium-40 has 19 - 1 = 18 electrons
when it is part of K2SO4.
The correct answer is 0) 21 neutrons, 19 protons, 18 electrons.
Table 31: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Chemistry dataset.
35



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 22):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset Question Answer
MMLU
Physics
A point charge, Q1 = +1 mC, is
fixed at the origin. How much
work is required to move another
charge, Q2 = +8 µC, from the
point (0, 4 meters) to the point
(3 meters, 0)?
0) 3.5 J
1) 6.0 J
2) 22.5 J
3) 40 J
Sure, let’s think step by step.
1. Calculate the electric potential energy of Q2
at the initial position. The electric potential
energy (U) of a point charge Q2 at a distance
r from another point charge Q1 is given by the
following equation: U = k * Q1 * Q2 / r, where k
is Coulomb’s constant.
Therefore, the electric potential energy of Q2 at
the initial position is:
Uinitial = (9 ×109Nm2/C2) ∗(1 ×10−3C) ∗(8 ×
10−6C)/4m = 18 J
2. Calculate the electric potential energy of Q2 at
the final position. The electric potential energy of
Q2 at the final position is:
Ufinal = (9 × 109Nm2/C2) ∗ (1 × 10−3C) ∗ (8 ×
10−6C)/3m = 24 J
3. Calculate the work required to move Q2 from
the initial position to the final position. The work
(W) required to move a charge from one position
to another is equal to the change in electric
potential energy (∆U) between the two positions.
W = ∆U = Ufinal - Uinitial = 24 J - 18 J = 6 J
Therefore, the answer is 1) 6.0 J.
Table 18: Demonstration exemplar for CoT prompting for MMLU Physics.
23



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 32):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: A liquid flows at a constant flow rate through a pipe
with circular cross-sections of varying diameters. At one point in the
pipe, the diameter is 2 cm and the flow speed is 18 m/s. What is the flow
speed at another point in this pipe, where the diameter is 3 cm?
0) 4 m/s
1) 6 m/s
2) 8 m/s
3) 12 m/s
Original Answer [Incorrect]: The answer is 3) 12 m/s A step-by-step
solution is below: The flow rate is constant, so the product of the speed
and the cross-sectional area is constant. The cross-sectional area is
proportional to the square of the diameter, so the speed is inversely
proportional to the square of the diameter. The ratio of the speeds is the
inverse of the ratio of the squares of the diameters, so the speed at the
second point is 18m/s ∗ (2cm/3cm)2 = 12 m/s.
Stepback Question: What are the physics principles behind this
question?
Principles:
Conservation of Mass: the amount of mass flowing through the pipe at
two different points must be equal, or A1 * v1 = A2 * v2, where A1 is
the area of cross-section one, v1 is the velocity at point 1, A2 is the area
of cross-section two, v2 is the velocity at point 2.
Area of Circle: A = π ∗ r2, where r is the radius of the circle.
Final Answer: Using the Principles of Continuity equation, we can
solve the problem as following:
Step 1: Apply the Continuity equation to find out the flow speed at
another point in this pipe. The continuity equation states that the product
of the area and the velocity is constant at all points in a pipe. In this
problem, we are given the following information:
A1 = π ∗ (2cm)2 = 4πcm2
v1 = 18 m/s
A2 = π ∗ (3cm)2 = 9πcm2
We can use the continuity equation to find v2:
A1 * v1 = A2 * v2
4πcm2 ∗ 18m/s = 9πcm2 ∗ v2
v2 = 8 m/s
Therefore, the flow speed at another point in this pipe is 8 m/s.
The correct answer is 2) 8 m/s.
Table 29: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
33



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 31):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: What happens to the pressure, P, of an ideal gas if
the temperature is increased by a factor of 2 and the volume is increased
by a factor of 8 ?
0) P decreases by a factor of 16.
1) P decreases by a factor of 4.
2) P decreases by a factor of 2.
3) P increases by a factor of 4.
Original Answer [Incorrect]: The correct answer is 0. The ideal gas
law is PV = nRT. If the temperature is increased by a factor of 2 and the
volume is increased by a factor of 8, then the pressure will decrease by a
factor of 16.
Stepback Question: What are the physics principles behind this
question?
Principles:
Ideal gas law: PV = nRT, where P is the pressure, V is the volume, n is
the number of moles, R is the gas constant, and T is the temperature.
Final Answer: Using the Principles of Ideal gas law, we can solve the
problem as following:
Step 1: Apply Ideal gas law to find out the pressure of an ideal gas. The
ideal gas law states that PV = nRT, where P is the pressure, V is the
volume, n is the number of moles, R is the gas constant, and T is the
temperature.
Step 2: Apply Ideal gas law to find out the pressure of an ideal gas when
the temperature is increased by a factor of 2 and the volume is increased
by a factor of 8. If the temperature is increased by a factor of 2, then T be-
comes 2T. If the volume is increased by a factor of 8, then V becomes 8V .
Substituting these values into the ideal gas law, we get: P(8V) = nR(2T)
Dividing both sides by 8V , we get: P = nR(2T) / 8V
We can see that the pressure has decreased by a factor of 4.
Therefore, the correct answer is 1) P decreases by a factor of 4.
Table 28: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
32



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 14):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Are the following two answers to the given question equivalent? Do not
consider whether the answers are right or wrong, but only whether they
are equivalent. Directly state ”Yes” or ”No”.
Question: Which title was conferred to Anna Muzychuk in 2007?
Answer 1: Anna Muzychuk was conferred the title of International
Master (IM) in 2007. She earned the title by scoring three norms in rapid
chess tournaments.
Answer 2: International Master
Answer 1 (short): International Master
Answer 2 (short): International Master
Are the two answers equivalent?Yes
Question: What state is Seattle located in?
Answer 1: Seattle is in Washington State.
Answer 2: The answer is George Washington.
Answer 1 (short): Washington State
Answer 2 (short): George Washington
Are the two answers equivalent?No
Question: <Question>
Answer 1: <Model Output>
Answer 2: <Target Label>
Table 6: Illustration of few shot evaluation with the PaLM-2L model.
MMLU Physics/Chemistry First-Principle Prompt
You are an expert at Physics/Chemistry. You are given
a Physics/Chemistry problem. Your task is to extract the
Physics/Chemistry concepts and principles involved in solving
the problem. Here are a few examples:
Question: <Question Example1>
Principles Involved: <Principles Example1>
...
Question: <Question Example5>
Principles Involved: <Principles Example5>
Question: <Question>
Principles Involved:
Table 7: Prompt of extracting the underlying principles involved in MMLU physics and chemistry
questions.
C.2 H YPER -PARAMETERS FOR EVALUATION WITH PALM-2L
We use PaLM-2L as the scoring model for evaluation. We experiment with different sampling
temperatures, and find that T = 1 gives us a highly-accurate evaluation. For example, we sampled
100 test examples and the model predictions, and manually rated the correctness of the model scoring.
We found that out of 4 trials, the model scoring agrees with human ratings 97%, 98%, 99% and 99%
of the time.
D P ROMPTS AND FEW SHOT EXAMPLES
D.1 STEM
For MMLU high-school Physics and Chemistry, we first prompt the model to generate the first
principles behind the question. Using the generated first principles, we further prompt the model to
generate the final answer through few-shot demonstrations The prompt generating first principles is
shown in Table 7 for MMLU high-school Physics and Chemistry.
15



### Claim 167/179

#### Claim Text
A seminal study in this field was conducted by Machin , who demonstrated the importance of internal activation within a flagellum as a mechanism to sustain oscillations.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 17):

Which musical currently holds the record as Broadway's fourth-longest running show?
User Input
The musical “Chicago” holds the record as Broadway's fourth-longest running show.
I see your point, but …
Most of your claims are right, but …
Multi-Agent Interaction
Final ResponseAs of September 2021, the musical “Wicked” holds the record as Broadway's fourth-longest running show.
Figure 6: An example of the process of multi-agent in-
teraction for mitigating LLM hallucinations.
5.5 Other Methods
In addition to the above approaches, other tech-
niques demonstrating the potential for reducing
hallucinations are shown below.
Multi-agent interaction. Some recent research
has sought to address the hallucination problem
in LLMs from a multi-agent perspective, wherein
multiple LLMs (also known as agents) indepen-
dently propose and collaboratively debate their re-
sponses to reach a single consensus, as exempli-
fied in Figure 6. Du et al. (2023) is a pioneer-
ing work in this line. They initially developed a
benchmark for assessing the factual accuracy of
prominent computer scientist biographies gener-
ated by LMs. Their findings reveal that an indi-
vidual LLM can easily generate hallucinated in-
formation within this benchmark; however, such
hallucinations can be mitigated by engaging mul-
tiple LLMs in a debate to achieve consensus. Be-
sides, Cohen et al. (2023) ask one LLM to gen-
erate claims (acting as E XAMINEE ) and another
to raise questions about these claims and check
the truthfulness of them (acting as E XAMINER ).
Wang et al. (2023d) instead propose prompting a
single LLM to identify, simulate, and iteratively
self-collaborate with multiple personas, such as
Harry Potter Fan and Jay Chou Fan. By leverag-
ing an LLM as a cognitive synergist, it effectively
reduces hallucinations with relatively low costs.
Prompt engineering. Existing research high-
lights that the behavior of LLMs can significantly
vary based on the prompts given by users (Si et al.,
2022; Zhu et al., 2023). In terms of hallucina-
tion, users may encounter an LLM that initially
responds accurately but begins to hallucinate in-
formation when using different prompts. In light
of this observation, Zhang et al. (2023a) endeav-
our to engineer more effective prompts to mitigate
hallucination. Concretely, they employ the chain-
of-thought prompt (Wei et al., 2022) to compel
LLMs to generate reasoning steps before provid-
ing the final answers. However, chain-of-thought
may introduce some new challenges. The po-
tential of hallucinated reasoning steps is one of
them. Furthermore, a popular practice nowadays
involves explicitly instructing LLMs not to dis-
seminate false or unverifiable information when
designing the “system prompt”, i.e., the special
messages used to steer the behavior of LLMs.
The following system prompt used for Llama 2-
Chat (Touvron et al., 2023b) exemplifies this ap-
proach: If you don’t know the answer to a ques-
tion, please don’t share false information.
Analyzing LLMs’ internal states. Azaria and
Mitchell (2023) contend that LLMs may be aware
of their own falsehoods, implying that their in-
ternal states could be utilized to detect halluci-
nations. They propose Statement Accuracy Pre-
diction based on Language Model Activations
(SAPLMA), which adds a classifier on top of each
hidden layer of the LLM to determine truthful-
ness. Experimental results indicate that LLMs
might “know” when the statements they gener-
ate are false, and SAPLMA can effectively ex-
tract such information. The Inference-Time In-
tervention (ITI) method (Li et al., 2023b) is also
grounded in a similar hypothesis. They further
shift model activations alongside factuality-related
heads during inference and discover that this can
mitigate hallucinations. These studies suggest that
“the hallucination within LLMs may be more a re-
sult of generation techniques than the underlying
representation” (Agrawal et al., 2023).
Human-in-the-loop. Zhang et al. (2023c) posit
that a potential cause of hallucination in LLMs
could be the misalignment between knowledge
and user questions, a phenomenon that is par-
ticularly prevalent in the context of retrieval-
augmented generation (RAG). To address this is-
18



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 8):

ators. In particular, FactScore (Min et al., 2023)
begins by employing a passage retriever, such
as Generalizable T5-based Retrievers (Ni et al.,
2022), to gather pertinent information. Subse-
quently, an evaluation model, such as LLaMA-
65B (Touvron et al., 2023a), uses the retrieved
knowledge to determine the truthfulness of a state-
ment. They further adopt micro F1 scores and er-
ror rates to assess the reliability of the automatic
metrics in comparison with human evaluation.
Mündler et al. (2023) design dedicated prompts to
query an evaluator LLM (e.g., ChatGPT (OpenAI,
2023a)) whether the subjective LLM contradicts
itself under the same context, and report classifi-
cation metrics, including precision, recall, and F1
score.
Rule-based automatic evaluation. For discrim-
ination benchmarks (Li et al., 2023a; Muhlgay
et al., 2023), common rule-based classification
metrics such as accuracy can be directly applied
to evaluating the ability of LLMs to discriminate
factual statements from non-factual ones. Bang
et al. (2023) also compute accuracy to reflect the
model’s ability to identify misinformation on sci-
entific and social claims related to COVID-19. In
contrast, another line of research (Lee et al., 2022;
Yu et al., 2023a) focuses on devising heuristic
methods specifically designed for assessing hal-
lucination. FactualityPrompt (Lee et al., 2022)
combines named-entity-based metric and textual
entailment-based metric to capture different as-
pects of factuality. To evaluate knowledge cre-
ation, Yu et al. (2023a) devise a self-contrast met-
ric to quantify model consistency in generating
factual statements. They accomplish this by com-
paring model-generated texts with and without in-
cluding golden knowledge as part of the prompts
based on Rouge-L (F1) (Lin, 2004).
4 Sources of LLM Hallucination
In this section, we aim to explore the various fac-
tors that can induce hallucinations within LLMs.
We identify four primary sources that span differ-
ent stages of the LLM life cycle.
LLMs lack relevant knowledge or internalize
false knowledge. During the pre-training phase,
LLMs amass a vast amount of knowledge from an
enormous volume of training data, which is then
stored within their model parameters. When asked
to answer questions or complete tasks, LLMs of-
ten exhibit hallucinations if they lack pertinent
knowledge or have internalized false knowledge
from the training corpora.
Li et al. (2022c) discover that LLMs sometimes
misinterpret spurious correlations, such as posi-
tionally close or highly co-occurring associations,
as factual knowledge. Specifically, McKenna
et al. (2023) investigate the hallucination prob-
lem within the context of the natural language
inference (NLI) task and find a strong correla-
tion between LLM hallucination and the distri-
bution of the training data. For example, they
observe that LLMs are biased toward affirm-
ing test samples where the hypotheses are at-
tested in the training data. Besides, Dziri et al.
(2022) argue that hallucination is also present in
human-generated corpora (can be reflected as out-
dated (Liska et al., 2022; Luu et al., 2022), bi-
ased (Chang et al., 2019; Garrido-Muñoz et al.,
2021), or fabricated (Penedo et al., 2023) expres-
sion). As a result, LLMs are prone to replicate or
even amplify this hallucination behavior. Wu et al.
(2023b) reveal that the memorizing and reason-
ing performance of PLMs for ontological knowl-
edge is less than perfect. Sun et al. (2023a) put
forward a benchmark named Head-to-Tail to eval-
uate the factual knowledge of LLMs for entities
with different levels of popularity. Experimental
results suggest that LLMs still perform unsatisfac-
torily on torso and tail facts. Furthermore, Zheng
et al. (2023c) identified two additional abilities as-
sociated with knowledge memorization that en-
able LLMs to provide truthful answers:knowledge
recall and knowledge reasoning. Deficiencies in
either of these abilities can lead to hallucinations.
LLMs sometimes overestimate their capacities.
Some studies have been conducted with the aim
of understanding whether language models can
assess the accuracy of their responses and rec-
ognize their knowledge boundaries. Kadavath
et al. (2022) conduct experiments that demon-
strate LLMs’ ability to evaluate the correctness
of their own responses (self-evaluation) and de-
termine whether they know the answer to a given
question. However, for very large LLMs, the
distribution entropy of correct and incorrect an-
swers could be similar, suggesting that LLMs are
equally confident when generating incorrect an-
swers as they are generating correct ones. Yin
et al. (2023) also evaluate the capacity of pop-
ular LLMs to identify unanswerable or unknow-
9



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 19):

Is there a person under the tree?
Yes, there is a person under the tree.
Figure 7: An example of object hallucination in
LVLMs. We highlight the hallucination in red, as there
is no person under the tree in this picture.
lucinations compared to smaller models. For in-
stance, Li et al. (2023e) discuss the object halluci-
nation of LVLMs, wherein LVLMs generate con-
tent containing objects that are inconsistent with
or absent from the input image, such as the ex-
ample in Figure 7. To effectively measure ob-
ject hallucinations generated by LVLMs, Liu et al.
(2023a) propose a GPT4-Assisted Visual Instruc-
tion Evaluation (GA VIE) benchmark. Gunjal et al.
(2023) introduce a multi-modal hallucination de-
tection dataset named M-HalDetect, further study
the unfaithful descriptions and inaccurate rela-
tionships beyond object hallucinations in LVLMs.
Furthermore, in addition to images, some stud-
ies have extended LLMs to other modalities such
as audio (Wu et al., 2023a; Su et al., 2023) and
video (Maaz et al., 2023), making it interesting to
investigate hallucination in these new scenarios.
Model editing. As elaborated in § 4, hallucina-
tions in LLMs may primarily stem from the mem-
orization of false information or the absence of
correct factual knowledge. To mitigate these is-
sues in LLMs with minimal computational over-
head, the concept of model editing has been in-
troduced (Sinitsin et al., 2020; De Cao et al.,
2021). This approach involves modifying the be-
havior of models in a manner that is both data-
and computation-efficient. At present, there are
two mainstream paradigms for model editing. The
first involves the incorporation of an auxiliary
sub-network (Mitchell et al., 2022; Huang et al.,
2023b), while the second entails direct modifi-
cation of the original model parameters (Meng
et al., 2022a,b). This technique may be instrumen-
tal in eliminating LLMs’ hallucinations by editing
their stored factual knowledge in purpose (Lan-
ham et al., 2023; Onoe et al., 2023). How-
ever, this emerging field still faces numerous chal-
lenges. These could include editing black-box
LLMs (Murty et al., 2022), in-context model edit-
ing (Zheng et al., 2023a), and multi-hop model
editing (Zhong et al., 2023), etc.
Attack/defense for inducing hallucination. As
previously discussed, significant efforts have been
undertaken by both researchers and companies to
guarantee that LLMs produce truthful responses,
ultimately improving the overall user experi-
ence. Cutting-edge commercial LLMs, such as
GPT4 (OpenAI, 2023b), appear to have acquired
a decent ability to generate proper responses to
factuality-related queries. However, they are not
invincible. Several studies show that LLMs can
be manipulated using techniques like meticulously
crafted jailbreak prompts to elicit arbitrary desired
responses (Wei et al., 2023a; Zou et al., 2023), in-
cluding hallucinations. Consequently, the attack-
ing and defending strategies for inducing halluci-
nations could also be a promising research direc-
tion. This is particularly important as the gener-
ation of fabricated information could potentially
breach relevant laws, leading to the forced shut-
down of LLM applications. This direction is also
intimately tied to the robustness of existing hallu-
cination mitigation methods.
Others. Given that the current research on hal-
lucinations in LLMs is still in its early stages,
there are also many other intriguing and promis-
ing avenues for further investigation. For in-
stance, researchers have begun to treat LLMs as
agents for open-world planning in the pursuit of
AGI (Park et al., 2023; Wang et al., 2023a). Ad-
dressing the hallucination problem within the con-
text of LLMs-as-agents presents brand-new chal-
lenges and holds considerable practical value. Be-
sides, analyzing and tracing LLM hallucinations
from the linguistic aspect is another interesting re-
search topic. Rawte et al. (2023) show that the oc-
currence of LLM hallucination is closely related
to linguistic nuances of the user prompts, such
as readability, formality, and concreteness. We
believe all these directions merit thorough explo-
20



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 1):

Parametric MemorizationPre-training
SFT
RLHF
Inference
Overinflated Self-confidence
Misleading Alignment
Generation-time Risk
Curating Training Data
Honesty-oriented RL
Decoding StrategyKnowledge Retrieve
Honesty-oriented SFT
Exploiting UncertaintySources (Sec. 4) Mitigation (Sec. 5)
Definition (Sec. 2)Input-Conflicting Hallucination
TimeLine
Input-Conflicting Benchmark:BEGIN, QMSum, FENMT,FEQA…
Benchmark (Sec. 3) Context-Conflicting HallucinationFact-Conflicting Hallucination
Context-Conflicting Benchmark:HADES…Fact-Conflicting Benchmark:TruthfulQA,,FActScore,HaluEval,FACTOR…
Figure 2: The overview structure of this paper: We initially categorize LLM hallucinations into three distinct types
and then introduce corresponding evaluation benchmarks. Subsequently, we explore the source of hallucinations
and discuss mitigation strategies throughout the life cycle of LLMs (pre-training→SFT→RLHF→inference).
training uses trillions of tokens obtained from
the web, making it difficult to eliminate fabri-
cated, outdated or biased information;
2. Versatility of LLMs : general-purpose LLMs
are expected to excel in cross-task, cross-
lingual, and cross-domain settings, posing
challenges for comprehensive evaluation and
mitigation of hallucination.
3. Imperceptibility of errors: as a byproduct of
their strong abilities, LLMs may generate false
information that initially seems highly plausi-
ble, making it challenging for models or even
humans to detect hallucination.
In addition, the RLHF process (Ouyang et al.,
2022), the vague knowledge boundary (Ren et al.,
2023) and the black-box property of LLMs (Sun
et al., 2022) also complicate the detection, expla-
nation, and mitigation of hallucination in LLMs.
There has been a notable upsurge in cutting-edge
research dedicated to addressing the aforemen-
tioned challenges, which strongly motivates us to
compile this survey.
We organize this paper as follows, as also
depicted in Figure 2. We first introduce the
background of LLMs and offer our definition of
hallucination in LLMs (§2). Next, we introduce
relevant benchmarks and metrics (§3). Subse-
quently, we discuss potential sources of LLM hal-
lucinations (§4), and provide an in-depth review of
recent work towards addressing the problem (§5).
Finally, we present forward-looking perspectives
(§6). We will consistently update the related
open-source materials, which can be accessed at
https://github.com/HillZhang1999/
llm-hallucination-survey.
2 Hallucination in the Era of LLM
We begin this section by overviewing the history
of LLMs (§2.1). Next, we present our defini-
tion of LLM hallucination, by breaking it down
2



Source: data\tc16_2312.10997v5\referenced_papers\[167]_2309.01431.pdf (Page 0):

Benchmarking Large Language Models in Retrieval-Augmented Generation
Jiawei Chen1,3, Hongyu Lin1,*, Xianpei Han1,2,*, Le Sun1,2
1Chinese Information Processing Laboratory 2State Key Laboratory of Computer Science
Institute of Software, Chinese Academy of Sciences, Beijing, China
3University of Chinese Academy of Sciences, Beijing, China
{jiawei2020,hongyu,xianpei,sunle}@iscas.ac.cn
Abstract
Retrieval-Augmented Generation (RAG) is a promising ap-
proach for mitigating the hallucination of large language
models (LLMs). However, existing research lacks rigorous
evaluation of the impact of retrieval-augmented generation
on different large language models, which make it challeng-
ing to identify the potential bottlenecks in the capabilities
of RAG for different LLMs. In this paper, we systemati-
cally investigate the impact of Retrieval-Augmented Gener-
ation on large language models. We analyze the performance
of different large language models in 4 fundamental abili-
ties required for RAG, including noise robustness, negative
rejection, information integration, and counterfactual robust-
ness. To this end, we establish Retrieval-Augmented Genera-
tion Benchmark (RGB), a new corpus for RAG evaluation in
both English and Chinese. RGB divides the instances within
the benchmark into 4 separate testbeds based on the afore-
mentioned fundamental abilities required to resolve the case.
Then we evaluate 6 representative LLMs on RGB to diag-
nose the challenges of current LLMs when applying RAG.
Evaluation reveals that while LLMs exhibit a certain degree
of noise robustness, they still struggle significantly in terms of
negative rejection, information integration, and dealing with
false information. The aforementioned assessment outcomes
indicate that there is still a considerable journey ahead to ef-
fectively apply RAG to LLMs.
Introduction
Recently, there have been impressive advancements in large
language models (LLMs) like ChatGPT (OpenAI 2022) and
ChatGLM (THUDM 2023a). Although these models have
shown remarkable general abilities (Bang et al. 2023; Guo
et al. 2023), they still suffer severely from challenges includ-
ing factual hallucination (Cao et al. 2020; Raunak, Menezes,
and Junczys-Dowmunt 2021; Ji et al. 2023), knowledge out-
dating (He, Zhang, and Roth 2022), and the lack of domain-
specific expertise (Li et al. 2023c; Shen et al. 2023).
Incorporating external knowledge via information re-
trieval, i.e., Retrieval-Augmented Generation (RAG), has
been regarded as a promising way to resolve the above chal-
lenges. (Guu et al. 2020; Lewis et al. 2020; Borgeaud et al.
* Corresponding authors.
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
Noise Robustness Negative Rejection
Who was awarded the 2022 Nobel prize in 
literature?
The Nobel Prize in Literature for 2022 is 
awarded to the French author Annie Ernaux, 
“for the courage and clinical acuity …
The Nobel Prize in Literature for 2021 is 
awarded to the novelist Abdulrazak Gurnah, 
born in Zanzibar and active in …
Annie Ernaux
Question
External documents contain noises
Retrieval Augmented 
Generation
Who was awarded the 2022 Nobel prize in 
literature?
The Nobel Prize in Literature for 2021 is 
awarded to the novelist Abdulrazak Gurnah, 
born in Zanzibar and active in …
The 2020 Nobel Laureate in Literature, 
poet Louise Glück, has written both poetry 
and essays about poetry. Since her…
I can not answer the question because of the 
insufficient information in documents
Question
External documents are all noises
Information Integration
When were the ChatGPT app for iOS and 
ChatGPT api launched?
On May 18th, 2023, OpenAI introduced its 
own ChatGPT app for iOS…
That changed on March 1, when OpenAI 
announced the release of API access to 
ChatGPT and Whisper,…
May 18 and March 1.
Question
External documents contain all answers
Retrieval Augmented 
Generation
Counterfactual Robustness
Which city hosted the Olympic games in 
2004?
The 2004 Olympic Games returned home to 
New York, birthplace of the … 
After leading all voting rounds, New York
easily defeated Rome in the fifth and 
final vote …
There are factual errors in the provided 
documents. The answer should be Athens. 
Question
Counterfactual external documents
Retrieval Augmented 
Generation
Retrieval Augmented 
Generation
Figure 1: Illustration of 4 kinds of abilities required for
retrieval-augmented generation of LLMs.
2022; Izacard et al. 2022). With the help of external knowl-
edge, LLMs can generate more accurate and reliable re-
sponses. The most common method is to use a search engine
as a retriever such as New Bing. Due to the vast amount of
information available on the Internet, using a search engine
can provide more real-time information.
However, Retrieval-Augmented Generation brings not
only positive effects to LLMs (Liu, Zhang, and Liang 2023;
Maynez et al. 2020). On one hand, there is a significant
amount of noise information even fake news in the content
available on the Internet, which poses challenges for search
engines in accurately retrieving desirable knowledge. On the
other hand, LLMs suffer from unreliable generation chal-
lenge. LLMs can be misled by incorrect information con-
tained in the context (Bian et al. 2023) and also suffer from
hallucination during the generation (Adlakha et al. 2023),
resulting in generating content that goes beyond external in-
arXiv:2309.01431v2  [cs.CL]  20 Dec 2023



### Claim 168/179

#### Claim Text
This assumption is based on the fact that, under the effect of homophily , individuals tend to imitate the behavior of their peers.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 12):

User Input Introduce the film “The Only Thing” (Four Flaming Days) to me.
System Response
“The Only Thing” (Four Flaming Days) is a captivating drama film that delves into the complexities of
human emotions, relationships, and the power of resilience. Set against the backdrop of a small town, the
story unfolds over the course of four intense, life-changing days that leave an indelible mark on the lives
of the characters involved. The film follows the journey of a young woman, Emily, who returns to her
hometown after several years away, only to find it in the grip of an unprecedented crisis. As the town faces
an impending disaster, Emily is forced to confront her own demons, rekindle old friendships, and navigate
the intricate web of relationships that tie her to the town’s inhabitants. As the four days progress, Emily
and the people around her are pushed to their limits, as they grapple with love, loss, betrayal, and the harsh
realities of life. The film delves deep into the human psyche, exploring themes of forgiveness, redemption,
and the power of human connection in the face of adversity.
User Input What genre of movie is “The Only Thing” (Four Flaming Days)?
System ResponseI’m sorry. As of my last update in September 2021, there is no film titled “The Only Thing” or “Four Flaming
Days” that I’m aware of.
Table 9: A real example of the over-conservative phenomenon of ChatGPT (July 2023 Version). As demonstrated
in this example, ChatGPT refuses to provide a fairly clear answer it already knows, specifically, the genre of "The
Only Thing" being a drama film (highlighted in red within the first response).
and LLM knowledge boundaries. In light of this,
Schulman (2023) propose to solve this problem
during RLHF. They design a special reward func-
tion just for mitigating hallucinations, as shown
in Table 8. “Unhedged/Hedged Correct/Wrong”
here means the LLM provides correct or wrong
answers with a positive or hesitant tone. “Unin-
formative” denote the safe answers like “I don’t
know”. The core idea is to encourage LLMs to
challenge the premise, express uncertainty, and
commit incapability by learning from specially de-
signed rewards. This method, which we refer to
as honesty-oriented RL, offers several advantages
over honesty-oriented SFT. The primary benefit is
that it allows LLMs to freely explore their knowl-
edge boundaries, thereby enhancing their general-
ization capabilities to OOD cases. Additionally, it
reduces the need for extensive human annotation
and eliminates the requirement for annotators to
guess the knowledge boundaries of LLMs.
Summary & Discussion. Reinforcement learn-
ing can guide LLMs in exploring their knowl-
edge boundaries, enabling them to decline to an-
swer questions beyond their capacity rather than
fabricating untruthful responses. However, we
note this approach also poses unique challenges.
For instance, RL-tuned LLMs may exhibit over-
conservatism due to an imbalanced trade-off be-
tween helpfulness and honesty (Ouyang et al.,
2022). An example of this is illustrated in Ta-
ble 9. As observed in this case, ChatGPT tends
to be overly hedged and refrains from providing a
clear answer that it already knows, as evidenced
in another dialogue turn. This could be attributed
to the unreasonable design of the reward function
or the poor quality of the training data for the re-
ward model. We hope future work can take such
problems into consideration.
5.4 Mitigation during Inference
Compared with the aforementioned training-time
mitigation approaches, mitigating hallucinations
in the inference time could be more cost-effective
and controllable. Therefore, most existing studies
focus on this direction, which we will introduce in
detail in the following sections.
5.4.1 Designing Decoding Strategies
Decoding strategies, such as greedy decoding and
beam search decoding, determine how we choose
output tokens from the probability distribution
generated by models (Zarrieß et al., 2021).
Lee et al. (2022) carry out a factuality assess-
ment of content generated by LLMs using differ-
ent decoding strategies. They find that nucleus
sampling (a.k.a top-p sampling) (Holtzman et al.,
2019) falls short of greedy decoding in terms of
factuality. They argue that this underperformance
could be attributed to the randomness introduced
by top-p sampling to boost diversity, which may
inadvertently lead to hallucinations since LLMs
tend to fabricate information to generate diverse
responses. In view of this, they introduce a decod-
ing algorithm termed factual-nucleus sampling ,
which aims to strike a more effective balance be-
tween diversity and factuality by leveraging the
strengths of both top-p and greedy decoding.
Dhuliawala et al. (2023) develop a decoding
framework known as the Chain-of-Verification
(COVE). This framework is based on the obser-
vation that independent verification questions typ-
13



Source: data\tc16_2312.10997v5\referenced_papers\[2]_2309.01219.pdf (Page 11):

that this method simply mimics behavior without
learning a strategy to achieve the final goal. The
SFT process of LLMs can be viewed as a spe-
cial case of behavior cloning, where LLMs learn
the format and style of interaction by mimicking
humans. As for LLMs, despite having encoded
a substantial amount of knowledge into their pa-
rameters, there remains knowledge that surpasses
their capacity (Yin et al., 2023; Ren et al., 2023).
By cloning human behaviors during SFT, LLMs
learn to respond to all questions with a predom-
inantly positive tone, without assessing whether
these questions exceed their knowledge bound-
aries (see Figure 3). As a result, during inference,
if prompted to answer questions related to un-
learned knowledge, they are likely to confidently
produce hallucinations. One way to remit this
problem can be the honesty-oriented SFT, which
means introducing some honest samples into the
SFT data. The honest samples refer to responses
that admit incompetence, such as “Sorry, I don’t
know”. The Moss project (Sun et al., 2023b) open-
sourced their SFT data, which includes such hon-
est samples. We observed that models tuned with
them could learn to refuse to answer specific ques-
tions, therefore helping reduce hallucinations.
Summary & Discussion. Curating the training
data is one approach for mitigating hallucinations
during the SFT phase. Thanks to the acceptable
volume of SFT data, they can be manually curated
by human experts. Recently, we have performed
a preliminary human inspection and observed that
some widely-used synthetic SFT data, such as Al-
paca (Taori et al., 2023), contains a considerable
amount of hallucinated answers due to the lack of
human inspection. This calls for careful attention
when researchers try to build SFT datasets based
on self-instruct (Wang et al., 2023c).
Previous work also pointed out that the SFT
process may inadvertently introduce hallucina-
tions, by forcing LLMs to answer questions that
surpass their knowledge boundaries. Some re-
searchers have suggested honesty-oriented SFT as
a solution. However, we argue this method has two
main problems. Firstly, it exhibits limited gen-
eralization capabilities towards out-of-distribution
(OOD) cases. Secondly, the annotated honest
samples just reflect the incompetence and uncer-
tainty of annotators rather than those of LLMs, as
annotators are unaware of LLMs’ real knowledge
boundaries. Such challenges make solving this is-
Situation Reward Value
Unhedged Correct +1
Hedged Correct +0.5
Uninformative 0
Hedged Wrong -2
Unhedged Wrong -4
Table 8: An example of reward design for mitigating
LLM hallucinations through RL (Schulman, 2023).
sue during SFT sub-optimal.
5.3 Mitigation during RLHF
Nowadays, many researchers attempt to fur-
ther improve the supervised fine-tuned LLMs
via reinforcement learning from human feedback
(RLHF) (Fernandes et al., 2023). This process
consists of two steps: 1) train a reward model
(RW) as the proxy for human preference, which
aims to assign an appropriate reward value to each
LLM response; 2) optimize the SFT model with
the reward model’s feedback, by using RL algo-
rithms such as PPO (Schulman et al., 2017).
Leveraging human feedback not only closes the
gap between machine-generated content and hu-
man preference but also helps LLMs align with
desired criteria or goals. One commonly used
criterion today is “3H”, which denotes helpful,
honest, and harmless (Ouyang et al., 2022; Bai
et al., 2022; Zheng et al., 2023b). The hon-
est aspect here just refers to the minimization of
hallucinations in LLM responses. Current ad-
vanced LLMs, such as InstructGPT (Ouyang et al.,
2022), ChatGPT (OpenAI, 2023a), GPT4 (Ope-
nAI, 2023b), and Llama2-Chat (Touvron et al.,
2023b), have collectively considered this aspect
during RLHF. For example, GPT4 uses synthetic
hallucination data to train the reward model and
perform RL, which increases accuracy on Truth-
fulQA (Lin et al., 2021) from about 30% to 60%.
Moreover, Lightman et al. (2023) use the process
supervision to detect and mitigate hallucinations
for reasoning tasks, which provides feedback for
each intermediate reasoning step.
As discussed in the previous section, the phe-
nomenon of behavior cloning during the SFT stage
can potentially lead to hallucinations. Some re-
searchers have attempted to address this issue by
integrating honest samples into the original SFT
data. However, this approach has certain limita-
tions, such as unsatisfactory OOD generalization
capabilities and a misalignment between human
12



Source: data\tc16_2312.10997v5\referenced_papers\[42]_2208.03299.pdf (Page 26):

Table 13: MMLU scores with de-biasing:
Setting Model All Hum. Soc. Sci. STEM Other
zero-shot
Standard 36.8 37.5 39.0 30.2 39.7
All permutations 48.5 45.7 55.2 39.4 54.4
Cyclic Permutations 47.1 43.6 54.1 38.0 54.9
5-shot
Standard 43.4 41.8 49.3 33.9 48.8
All permutations 49.0 46.0 56.1 40.5 54.6
Cyclic Permutations 47.9 46.1 54.6 38.8 52.8
A Training details and additional results
A.1 MMLU
A.1.1 Training Details
Featurization MMLU consists of multiple choice questions with four possible lexicalized answer options.
We represent the input using the following template:
question: {question text}
options: (A) {answer 1 text} (B) {answer 2 text} (C) {answer 3 text} (D) {answer 4 text}
answer: [MASK_0]
and train the model to generate the mask token followed by the letter of the correct answer:
[MASK_0] {correct answer option letter}
This format closely matches the format of MLM pre-training objective, aiding few-shot learning. When
training, we permute the order of the answer options, i.e. shuﬄing which answer option appears as letter A
etc. This helps reduce overﬁtting, and encourages a uniform prior on the letters.
Standard inference Once trained we obtain predictions from the model by selecting the pre-softmax
logits for the tokens A, B, C and D, and performing a softmax over them to obtain a distribution over the 4
answer options. For standard inference, we then simply return the answer corresponding to the argmax of
this distribution.
De-biased Inference As mentioned in the main text, even though our model is ﬁnetuned with data
that encourages a uniform prior over answer letters (by permuting which answer option letter is used with
which lexical answer option text in training data), this may not be enough to ensure the model has no
residual bias towards speciﬁc letters. Consider answersa, questionsq and a nuisance variablez∈Z, which
represents the ordering of the answer options or, equivalently, which answer letter gets assigned to which
answer option text. There are 4 answer options in MMLU, and thus|Z|= 24 unique ways they can be
ordered, or assigned to given letters. Running our model with our standard inference for a questionq,
corresponds to calculatingp(a|q= q,z = z) for the answer orderingz that happens to appear in the dataset.
We can control forz by running the model with all possible answer orderings in the input, and marginalizing:
p(a|q = q) = ∑
z′∈Zp(a|q = q,z = z′)p(z = z′|q = q), and assumingp(z = z′|q = q) is uniform (no answer
ordering is more likely than another), this reduces to simplyp(a|q = q) ∝∑
z′∈Zp(a|q = q,z = z′). This
procedure requires 24 forward passes, one for each answer ordering, so is 24×slower than standard inference.
Table 13 shows the result of applying the full permutation de-biasing, which leads to an 12% improvement
zero-shot and 6% in 5-shot performance overall. Empirically, using only the cyclic permutations of the answer
order provided in the original dataset (of which there are 4) works nearly as well, which is what we report in
the main paper, and only increases inference compute by a factor of 4, rather than 24. Cyclic permutation
de-biasing improves over standard inference by 10% in zero-shot and 5% in 5-shot. Empirically, de-biased
inference is largely unnecessary when training in the 5-shot multitask or full dataset setting, as there is
enough data for the model to learn a more uniform prior over the letters.
27



Source: data\tc16_2312.10997v5\referenced_papers\[107]_2306.02224.pdf (Page 4):

Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions
making autonomous decisions. However, by introducing
opinions from various weak learners, GPT4 can enhance its
performance. Considering these diverse viewpoints may al-
low GPT4 to mitigate its own biases and overcome inherent
limitations.
Interestingly, the inclusion of a single IL choice as an ad-
ditional opinion in the context resulted in improved perfor-
mance for all LLMs. This performance boost is particularly
noteworthy for GPT4, because GPT4 by itself outperforms
the additional opinions provided by IL models, while still
benefiting from this method.
To explore the impact of single vs. multiple additional
opinions, we also tested sampled top five additional opin-
ions vs. one (see Table 1), and observed GPT4 with top 5
additional opinions reached the best Success Rate, Rewards
and Precision across all groups (Table 1). From an intelligent
agent perspective, awareness of an additional opinion, or
even multiple distinct opinions, can be beneficial, as argued
in [23]. This suggests that providing LLMs with one or a
few additional opinions of reasonable quality can serve as a
reference, resulting in a more informed decision.
Out of curiosity, we also conducted one ablation study
by providing random one additional opinion to GPT3.5. We
observed the worst performance - the lowest reward (22.333)
and an equivalently low success rate (0.060) as Rule based
model.
In Figure 2, we observe that Language Learning Models
(LLMs) predominantly take in the additional opinion sug-
gested by expert models, with GPT4 exhibiting the highest
standard and the greatest proportion of disagreements. We
consider any match among the top 5 additional opinions
as being taken into account by the LLMs. Intriguingly, for
GPT4, the ratio of considered opinions escalates from 0.549
to 0.602 as the number of opinions increases from 1 to 5.
This trend could partially elucidate the disparity in the final
outcomes of success rates.
3.4.2 ALFWorld. Taking inspiration from our previous Web-
shop experiment, we employed a similar paradigm to com-
bine Large Language Models (LLMs) with Expert models
in the context of the ALFWorld dataset. The fundamental
premise remained analogous, but due to resource constraints
and the extensive size of ALFWorld (134 unseen games), we
confined our initial trial to a top 1 Imitation Learning (IL)
opinion.
Our hypothesis received empirical confirmation from the
data observed for GPT4 and Claude, as detailed in Table
2. The success rate showed slight improvements, although
the original DAgger agent provided by ALFWorld was less
effective for general tasks compared to specific ones, as
demonstrated by the task-specific performance rates listed
in Table 4 from the ALFWorld paper [19], i.e in a disparate
performance range, the DAgger agent excelled in the ’Cool
and Place’ task, achieving a remarkable 1.00 success rate.
However, for the ’Pick Two and Place’ task, its proficiency
was considerably lower, with a success rate of only 0.24.
Our investigation further revealed a substantial variance
in efficiency concerning the number of steps taken by the
ALFWorld IL model to complete tasks. This observation
highlights an additional layer of complexity, suggesting that
Figure 2: For Webshop, the ratios of LLMs considering
or disagreeing with additional opinion provided by
expert models. For the top 5 scenario, we consider it
as an agreement if any additional opinion matches.
the IL’s performance is not only variable in terms of task
success rates, but also in the practical efficiency of task ex-
ecution. This large variability in step numbers indicates
that while the IL model may excel in some tasks, it can be
markedly inefficient in others, further reinforcing the in-
herent limitations and variability of IL models in complex
task simulations like ALFWorld. In terms of task completion,
the IL’s outcomes are governed by real-time rewards. As
such, all completed tasks have to be executed correctly, as
evidenced by 1.00 precision for the both IL with and without
Beam Search. Conversely, the tasks left incomplete were
typically ones where the agent had exhausted the available
steps, with most of these marked by repeated and meaning-
less actions as presented in Appendix 2.
One of the standout observations from our study was
GPT4’s ability to integrate past memories, task descriptions,
and environmental data to discern the pertinence of sugges-
tions from the IL model. Even amidst noise, GPT4 demon-
strated a robust capacity to differentiate beneficial from
irrelevant advice, often confidently disregarding sugges-
tions that were not beneficial, as illustrated in Appendix 3.
Moreover, GPT4 was even able to extract values from the
initial part of a repetitive action pattern suggested by the IL,
underscoring its exceptional ability to distill useful informa-
tion. Contrastingly, GPT3.5 was easily misled by irrelevant
suggestions and frequently became entangled in the repeti-
tive advice offered by the IL. Indeed, such confusion even
compromised GPT3.5’s capability to perform tasks that it
could otherwise successfully accomplish independently, as
detailed in Appendix 4. This highlights a stark divergence
from the patterns observed with GPT4 under ALFWorld
Context.
In a compelling revelation, this study demonstrated a
marked difference between the contexts of the Webshop



Source: data\tc16_2312.10997v5\referenced_papers\[14]_2305.15294.pdf (Page 7):

HotPotQA
Question: The arena where the Lewiston Maineiacs played their home games can seat how many people?
Answer: 3,677 seated
Iteration 1
Retrieved Knowledge
(1) Title: Lewiston Maineiacs Context: The Lewiston Maineiacs were a junior ice hockey team ... The team played its home
games at the Androscoggin Bank Colisée ...
(2) Title: Bangor Auditorium Context: ... a 5,948 seat multi-purpose arena located in Bangor, Maine ...
Generation
The Lewiston Maineiacs were a junior ice hockey team of the Quebec Major Junior Hockey League based in Lewiston, Maine. The
team played its home games at the Androscoggin Bank Colisée.The Androscoggin Bank Colisée can seat up to 5,948 people.
So the answer is 5,948
Iteration 2
Retrieved Knowledge
(1) Title: Lewiston Maineiacs Context: The Lewiston Maineiacs were a junior ice hockey team ... The team played its home
games at the Androscoggin Bank Colisée ...
(2) Title: Androscoggin Bank Colisée Context: The Androscoggin Bank Colisée (formerly Central Maine Civic Center and
Lewiston Colisee) is a 4,000 capacity (3,677 seated) multi-purpose arena ...
Generation
The Lewiston Maineiacs were a junior ice hockey team of the Quebec Major Junior Hockey League based in Lewiston, Maine.
They played their home games at the Androscoggin Bank Colisée. The Androscoggin Bank Colisée has a capacity of 4,000
(3,677 seated). So the answer is 3,677
StrategyQA
Question: Can you get Raclette in YMCA headquarters city?
Answer: Yes
Iteration 1
Retrieved Knowledge
(1) Title: YMCA Building (Racine, Wisconsin) Context: ... Since the 50’s, headquarters of the publishing house and the
bookstore are located in 11 rue de la Montagne Sainte-Geneviève 75005 Paris ...
Generation
The YMCA headquarters is located in Paris, France.Raclette is a traditional Swiss dish, made with melted cheeseserved with
potatoes, pickles, and other accompaniments. It is not a traditional French dish, so it is unlikely to be found in Paris. So the
answer is No
Iteration 2
Retrieved Knowledge
(1) Title: Raclette Context: Raclette is a Swiss dish, also popular in France, based on heating cheese and scraping off the melted
part ...
(2) Title: YMCA Building (Racine, Wisconsin) Context: ... Since the 50’s, headquarters of the publishing house and the
bookstore are located in 11 rue de la Montagne Sainte-Geneviève 75005 Paris ...
Generation
YMCA headquarters is located in Paris, France. Raclette is a dish native to parts of Switzerland, but it is also popular in France.
So it is likely that Raclette can be found in Paris. So the answer is Yes
Table 7: Two examples demonstrating retrieval-generation synergy. We underline generated phrases that help
retrieve relevant knowledge which successfully corrects factual errors (in red) in the second iteration. Irrelevant
retrieved paragraphs are not shown in the table for brevity.
achieves significantly higher recall, indicating that
LLM generations can help bridge the semantic gaps
between complex questions and their supporting
knowledge. However, performance quickly hits a
plateau afterwards.
4.6.2 I TER -RETGEN Leverages Parametric
and Non-Parametric Knowledge Better
Ideally, an LLM should flexibly utilize non-
parametric knowledge or parametric knowledge
depending on whether in-context non-parametric
knowledge is relevant or not. Table 5 presents per-
formance breakdowns on different subsets of ques-
tions for investigation. We considered the ability
of CoT to answer a question correctly without re-
trieval as a proxy for assessing an LLM’s capability
to answer the question using its parametric knowl-
edge. Compared with Self-Ask, I TER -RETGEN
tends to be significantly better at preserving the
LLM’s performance on questions that the LLM can
solve using CoT without retrieval, while being com-
petitive on the complementary subset. This may
be because the structural constraints from Self-Ask
makes an LLM over-sensitive to the precision and
comprehensiveness of follow-up question genera-
tion and answering, and Self-Ask is also incapable
of processing all retrieved knowledge as a whole,
thus reducing the LLM’s flexibility in solving a
question. Moreover, I TER -RETGEN consistently
outperforms Self-Ask by a large margin, regardless



### Claim 169/179

#### Claim Text
IBM method of , CFL = 0.3; □ IBM method of , CFL = 0.15; △ LBM computation of , using central linear interpolation.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[101]_2310.06839.pdf (Page 18):

Compressed Prompt:
Please complete the code given below.
public class MessageArchiveManagement
private static final long MILLISECONDS_IN_DAY = 24 * 00 *0;
public static final long_CUP = MCON_DAY
/.../
.("",.getStart
add
ifget() >0
Node end("
end.("
endNode.Value("", Util.getTimestamp(query.getEnd
addNode
} if (.withid null && contact null && !isference
Node with(" .with
.Value("valuewith
.(
// queryMessageive(connection, nextQuery
final(connectionProtocol(), query
synchronized (eries)
// queries.add(nextQuery } }
public boolean queryInProgress( contact, OnLoaded
moreMessagesLoadedListener)
ized (eries)
(Query query : queries)
if(query.getWith().equals(contact.getUserId()))
if (query.onMoreMessagesLoaded == null &&MessagesListener
null) query.setOnMoreMessagesLoaded(Listener}
return true;}} return false;}}
private void finalizeQuery(Protocol protocol, Query query) {
synchronized (queries) {
.remove(query); }
Contact contact = null;
if (query.getWith() != null) {
contact = protocol.getItemByUID(query.getWith()); }
if (contact != null) {
Next line of code:
LLMs’ Response:
contact.setLastMessageTransmitted(query.getEnd());\n
Ground Truth:
if (contact.setLastMessageTransmitted(query.getEnd())) {
Zero-shot LLMs’ Response:
contact.removeQuery(query);\n
Figure 8: Cases study on lcc code completion task in LongBench benchmark (Bai et al., 2023) in 2,000 constraint
using GPT-3.5-Turbo.



Source: data\tc16_2312.10997v5\referenced_papers\[103]_2303.08559.pdf (Page 0):

Large Language Model Is Not a Good Few-shot InformationExtractor,
but a GoodReranker for Hard Samples!
Yubo Ma1, Yixin Cao2, YongChing Hong1, Aixin Sun1
1 S-Lab, Nanyang Technological University
2 Singapore Management University
yubo001@e.ntu.edu.sg
Abstract
Large Language Models (LLMs) have made
remarkable strides in various tasks. Whether
LLMs are competitive few-shot solvers for in-
formation extraction (IE) tasks, however, re-
mains an open problem. In this work, we
aim to provide a thorough answer to this ques-
tion. Through extensive experiments on nine
datasets across four IE tasks, we demonstrate
that current advanced LLMs consistently ex-
hibit inferior performance, higher latency, and
increased budget requirements compared to
fine-tuned SLMs under most settings. There-
fore, we conclude that LLMs are not effec-
tive few-shot information extractors in gen-
eral 1. Nonetheless, we illustrate that with
appropriate prompting strategies, LLMs can
effectively complement SLMs and tackle chal-
lenging samples that SLMs struggle with. And
moreover, we propose an adaptive filter-then-
rerank paradigm to combine the strengths of
LLMs and SLMs. In this paradigm, SLMs
serve as filters and LLMs serve as rerankers.
By prompting LLMs to rerank a small portion
of difficult samples identified by SLMs, our pre-
liminary system consistently achieves promis-
ing improvements (2.4% F1-gain on average)
on various IE tasks, with an acceptable time
and cost investment. Our code is available at
https://github.com/mayubo2333/LLM-IE.
1 Introduction
Large Language Models (LLMs, Brown et al. 2020;
Chowdhery et al. 2022; Touvron et al. 2023) have
shown remarkable abilities on various NLP applica-
tions such as factual question answering (Yu et al.,
2023; Sun et al., 2023), arithmetic reasoning (Chen
et al., 2022a; Qian et al., 2023) and logical rea-
soning (Jung et al., 2022; Pan et al., 2023). Given
the reasoning, memorization, instruction-following
and few-shot adaption capabilities emerging from
1A more precise assertion is that current LLMs, with
vanilla prompting setting and without IE-specific fine-tuning,
are not good few-shot information extractors in general.
LLMs, it prompts a compelling question: Can
LLMs be used to boost performance in few-shot
information extraction (IE) tasks?
To answer this question, we conduct an exten-
sive empirical study to compare the performance
between LLMs using in-context learning 2 (ICL)
and fine-tuned Small Language Models (SLMs).
We fairly evaluate SLMs-based and LLMs-based
methods across nine datasets spanning four com-
mon IE tasks: (1) Named Entity Recognition, (2)
Relation Extraction, (3) Event Detection and (4)
Event Argument Extraction. For each dataset, we
explored four to six settings to encompass typi-
cal low-resource extents, from 1-shot to 20-shot or
even more. Given the potential sensitivity of LLMs’
performance to the prompt context, we meticu-
lously considered variations in instruction, demon-
stration number and selection strategy, prompt for-
mat, etc. Our study reveals that LLMs excel over
SLMs only when annotations are extremely lim-
ited, i.e., both label types 3 and the samples 4 per
label are extremely scarce. With more ( e.g., hun-
dreds of) samples, SLMs significantly outperform
LLMs. Furthermore, LLMs incur greater inference
latency and costs than fine-tuned SLMs. Hence, we
claim that current LLMs are not good few-shot
information extractors in general.
We further investigate whether LLMs and SLMs
exhibit different abilities to handle various types of
samples. We categorize samples according to their
difficulty measured by SLMs’ confidence scores,
and compare LLMs’ and SLMs’ results within each
group. We find that LLMs are good at hard sam-
ples, though bad at easy samples. We posit that
the knowledge and reasoning abilities in LLMs en-
able them to handle hard samples (which are sim-
2All LLMs discussed in this paper are not fine-tuned, and
results for LLMs are based on in-context learning.
3Label types denote entity/relation/event/role types in dif-
ferent tasks. We use them interchangeably there-in-after.
4Samples refer to (i) demonstrations in ICL of LLMs, or
(ii) training samples for SLMs’ fine-tuning.
arXiv:2303.08559v2  [cs.CL]  21 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[36]_2303.08559.pdf (Page 0):

Large Language Model Is Not a Good Few-shot InformationExtractor,
but a GoodReranker for Hard Samples!
Yubo Ma1, Yixin Cao2, YongChing Hong1, Aixin Sun1
1 S-Lab, Nanyang Technological University
2 Singapore Management University
yubo001@e.ntu.edu.sg
Abstract
Large Language Models (LLMs) have made
remarkable strides in various tasks. Whether
LLMs are competitive few-shot solvers for in-
formation extraction (IE) tasks, however, re-
mains an open problem. In this work, we
aim to provide a thorough answer to this ques-
tion. Through extensive experiments on nine
datasets across four IE tasks, we demonstrate
that current advanced LLMs consistently ex-
hibit inferior performance, higher latency, and
increased budget requirements compared to
fine-tuned SLMs under most settings. There-
fore, we conclude that LLMs are not effec-
tive few-shot information extractors in gen-
eral 1. Nonetheless, we illustrate that with
appropriate prompting strategies, LLMs can
effectively complement SLMs and tackle chal-
lenging samples that SLMs struggle with. And
moreover, we propose an adaptive filter-then-
rerank paradigm to combine the strengths of
LLMs and SLMs. In this paradigm, SLMs
serve as filters and LLMs serve as rerankers.
By prompting LLMs to rerank a small portion
of difficult samples identified by SLMs, our pre-
liminary system consistently achieves promis-
ing improvements (2.4% F1-gain on average)
on various IE tasks, with an acceptable time
and cost investment. Our code is available at
https://github.com/mayubo2333/LLM-IE.
1 Introduction
Large Language Models (LLMs, Brown et al. 2020;
Chowdhery et al. 2022; Touvron et al. 2023) have
shown remarkable abilities on various NLP applica-
tions such as factual question answering (Yu et al.,
2023; Sun et al., 2023), arithmetic reasoning (Chen
et al., 2022a; Qian et al., 2023) and logical rea-
soning (Jung et al., 2022; Pan et al., 2023). Given
the reasoning, memorization, instruction-following
and few-shot adaption capabilities emerging from
1A more precise assertion is that current LLMs, with
vanilla prompting setting and without IE-specific fine-tuning,
are not good few-shot information extractors in general.
LLMs, it prompts a compelling question: Can
LLMs be used to boost performance in few-shot
information extraction (IE) tasks?
To answer this question, we conduct an exten-
sive empirical study to compare the performance
between LLMs using in-context learning 2 (ICL)
and fine-tuned Small Language Models (SLMs).
We fairly evaluate SLMs-based and LLMs-based
methods across nine datasets spanning four com-
mon IE tasks: (1) Named Entity Recognition, (2)
Relation Extraction, (3) Event Detection and (4)
Event Argument Extraction. For each dataset, we
explored four to six settings to encompass typi-
cal low-resource extents, from 1-shot to 20-shot or
even more. Given the potential sensitivity of LLMs’
performance to the prompt context, we meticu-
lously considered variations in instruction, demon-
stration number and selection strategy, prompt for-
mat, etc. Our study reveals that LLMs excel over
SLMs only when annotations are extremely lim-
ited, i.e., both label types 3 and the samples 4 per
label are extremely scarce. With more ( e.g., hun-
dreds of) samples, SLMs significantly outperform
LLMs. Furthermore, LLMs incur greater inference
latency and costs than fine-tuned SLMs. Hence, we
claim that current LLMs are not good few-shot
information extractors in general.
We further investigate whether LLMs and SLMs
exhibit different abilities to handle various types of
samples. We categorize samples according to their
difficulty measured by SLMs’ confidence scores,
and compare LLMs’ and SLMs’ results within each
group. We find that LLMs are good at hard sam-
ples, though bad at easy samples. We posit that
the knowledge and reasoning abilities in LLMs en-
able them to handle hard samples (which are sim-
2All LLMs discussed in this paper are not fine-tuned, and
results for LLMs are based on in-context learning.
3Label types denote entity/relation/event/role types in dif-
ferent tasks. We use them interchangeably there-in-after.
4Samples refer to (i) demonstrations in ICL of LLMs, or
(ii) training samples for SLMs’ fine-tuning.
arXiv:2303.08559v2  [cs.CL]  21 Oct 2023



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 15):

Conference’17, July 2017, Washington, DC, USA Zhang and Xiao, et al.
Table 12: The impact of LLM-Embedder on different LLMs.
LLM Retriever MMLU PopQA ICL MSC Arxiv
Llama-2-7B-Chat
None 0.4599 0.2061 0.4645 19.3501 3.7647
BGE 0.4896 0.4491 0.5974 14.2943 3.2912
LLM-Embedder 0.4903 0.5052 0.6268 13.4832 3.2322
Aquila-7B-Chat
None 0.4499 0.2028 0.5145 16.0108 3.1204
BGE 0.4832 0.3982 0.5732 14.1843 2.7914
LLM-Embedder 0.4847 0.4405 0.5903 14.1836 2.7351
Qwen-7B-Chat
None 0.5561 0.2393 0.5346 21.0466 2.7888
BGE 0.5787 0.4447 0.6329 16.2064 2.5165
LLM-Embedder 0.5762 0.4782 0.6457 15.4524 2.4824
Baichuan2-7B-Chat
None 0.5226 0.2356 0.4907 18.9711 2.7510
BGE 0.5534 0.4407 0.5960 16.0759 2.4440
LLM-Embedder 0.5511 0.4848 0.6179 15.5890 2.4131
Llama-2-13B-Chat
None 0.5386 0.2886 0.4607 14.7334 3.2357
BGE 0.5603 0.4595 0.6196 11.6875 2.9036
LLM-Embedder 0.5580 0.5026 0.6439 11.5384 2.8540
the evaluation of tool learning and conversational search because
their performances are directly measured by retrieval metrics.
We can observe that our conclusions in Section 3.2.2 still holds.
First of all, retrieval from external world benefits LLM’s perfor-
mance in all four scenarios, since the performance of the plain
LLM (i.e. None) underperforms retrieval-augmented one (BGE and
LLM-Embedder). Besides, our proposed LLM-Embedder is able to
generalize well and maintain its superiority over BGE on most
datasets (PopQA and ICL in particular). An exception is MMLU,
where LLM-Embedder is slightly outperformed by BGE when using
Qwen, Baichuan, and Llama-2-13B. It seems that different LLMs
utilize the same knowledge in different ways, thereby obtaining a
little different results.



Source: data\tc16_2312.10997v5\referenced_papers\[101]_2310.06839.pdf (Page 1):

1 5 10 15 20
Document Number in the Prompt
85
90
95
100Normalized Performance(%)
Multi-Document QA
Code Completion
Summarization
(a) Performance v.s. Document Number
1st 5th 10th 15th 20th
Position of Document with the Answer
55
60
65
70
75Accuracy(%)
 Original
LongLLMLingua 
w/o Reorder (4x)
LongLLMLingua (4x) (b) Performance v.s. Key Information Position
Figure 1: (a) LLMs’ performance in downstream tasks decreases with increased noise in prompts. In this case,
we keep k most relevant documents/paragraphs based on the ground-truth or LongLLMLingua rk. A larger k
implies more noise introduced into the prompt. To improve the key information density in the prompt, we present
question-aware coarse-to-fine compression. (b) LLMs’ ability to capture the relevant information depends on their
positions in the prompt. To reduce information loss in the middle, we introduce a document reordering mechanism.
subsequence recovery strategy to improve the in-
tegrity of the key information (4.4). (5) We evaluate
LongLLMLingua across five benchmarks, i.e., Nat-
uralQuestions (Liu et al., 2024), LongBench (Bai
et al., 2023), ZeroSCROLLS (Shaham et al., 2023),
MuSicQue (Trivedi et al., 2022), and LooGLE (Li
et al., 2023b), covering a variety of long con-
text scenarios. Experimental results reveal that
LongLLMLingua’s compressed prompts outper-
form original prompts in terms of performance,
cost efficiency, and system latency.
2 Problem Formulation
Following LLMLingua (Jiang et al., 2023a), we
use x = (xins, xdoc
1 , ··· , xdoc
K , xque) to represent
a prompt, including the instruction xins, K docu-
ments xdoc
i , and the question xque. However, this
definition can be adjusted for specific scenarios.
The objective of a prompt compression system can
be formulated as:
min
ex
Dϕ (y, ey) +λ∥ex∥0, (1)
where ex represents the compressed prompt, a token-
level subsequence of x. y and ey represent the
LLM-generated results from x and ex, respectively.
Dϕ measures the distance function, such as KL di-
vergence. λ serves as a hyper-parameter balancing
the compression ratio. Additionally, this study ex-
plores a permutation operation space over the K
documents (xdoc
1 , ··· , xdoc
K ) for joint optimization.
3 Preliminary: LLMLingua
LLMLingua (Jiang et al., 2023a) utilizes a small
language model MS to evaluate the perplexity of
each prompt token, removing those with lower per-
plexities. This method is premised on the idea
that tokens with lower perplexities have a negli-
gible effect on the language model’s overall en-
tropy gain, implying their removal slightly impacts
the LLMs’ contextual understanding. This process
is viewed as an application of "LM is Compres-
sion" (Delétang et al., 2023). LLMLingua include
three key components: budget controller, iterative
token-level prompt compression, and distribution
alignment, highlighted by italic text in Figure 2.
The budget controller assigns varying compres-
sion ratios to different parts of the prompt (i.e.,
instruction, demonstrations, question), implement-
ing coarse-level prompt compression. Subsequent
steps involve dividing intermediate results into seg-
ments and applying token-level compression iter-
atively, where each token’s perplexity based on
preceding compressed segments. To aware differ-
ent target LLMs, LLMLingua fine-tunesMS using
data from the target LLM.
4 LongLLMLingua
LongLLMLingua builds on LLMLingua to better
compress prompts in long context scenorias. It tack-
les three main issues in handling lengthy contexts,
as introduced in Sec. 1. This approach focuses on
making LLMs more effective at recognizing key



### Claim 170/179

#### Claim Text
Contrastingly in CVD NV-diamonds, the NV centers have better coherence properties, in the expense of a lower control over their localization and concentration .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[50]_2311.09210.pdf (Page 5):

Models NQ TriviaQA WebQ Average
EM F1 EM F1 EM F1 EM F1
Backbone language model: LLaMa-2 7B
QA fine-tune w/o IR 28.80 37.53 63.19 68.61 28.30 42.77 35.98 44.27
SAIL (Luo et al., 2023)* 36.20 44.23 73.20 80.92 27.92 40.65 45.77 55.27
Retrieve-Read (Shi et al., 2023c)47.39 55.81 74.92 81.53 29.58 43.51 48.49 56.97
+ CHAIN-OF-NOTE (ours) 48.92 57.53 76.27 82.25 32.33 46.68 50.46 58.78
(+1.53) (+1.72) (+1.35) (+0.72) (+2.75) (+3.17) (+1.97) (+1.81)
Backbone language model:GPT-4-1106†
QA prompt w/o IR 54.0 74.2 56.2 61.5
Retrieve-Read (Shi et al., 2023c) 61.8 70.6 56.8 63.1
+ CHAIN-OF-THOUGHT 63.6 71.2 58.4 64.4
+ CHAIN-OF-NOTE (OURS) 63.8 74.6 58.8 65.7
(+2.0) (+4.0) (+2.0) (+2.6)
Table 2: The RALM, when equipped with CHAIN -OF-NOTE (CON), demonstrates a marginal improvement over
the standard RALM in full test set evaluations. Significantly, it outperforms the standard RALM system in scenarios
with noisy documents, suggesting that CON can substantially enhance the model’s noise robustness.
* SAIL was designed for retrieval-augmented instruction tuning, and as such, may not be ideally factual QA.
† Evaluating GPT-4 outputs with EM score is challenging; we opt for Accuracy, with reasons outlined in§3.1.3.
Karpukhin et al. (2020); Zhu et al. (2021). For
EM score, an answer is deemed correct if its nor-
malized form – obtained through the normalization
procedure delineated by (Karpukhin et al., 2020)
– corresponds to any acceptable answer in the pro-
vided list. Similar to EM score, F1 score treats the
prediction and ground truth as bags of tokens, and
compute the average overlap between the predic-
tion and ground truth answer (Chen et al., 2017).
Besides, we use reject rate (RR) to evaluate the
unknown robustness when given questions beyond
a language model’s knowledge scope.
Finally, since GPT-4 is not directly trained on
open-domain QA benchmarks, employing EM /
F1 for evaluation is challenging. Therefore, we
adopt the approach outlined in Mallen et al. (2023);
Kandpal et al. (2023), utilizing accuracy as the
evaluation metric. Accuracy considers a prediction
correct if any substring of the prediction exactly
matches any of the provided correct answers.
3.2 Evaluation on Overall QA Performance
Table 2 demonstrates that the RALM consistently
outperforms the directly fine-tuned LLaMa-2 with
QA pairs, without retrieval. This improvement
is closely tied to the effectiveness of the retrieval
process. As indicated in Table 1, DPR demon-
strates markedly superior retrieval performance on
the NQ and TriviaQA datasets compared to WebQ.
Consequently, the benefits of retrieval are more
pronounced on NQ and TriviaQA. Furthermore,
when comparing our enhanced RALM, which inte-
grates CON, with the standard RALM, our method
persistently shows better performance. There is
an average improvement of +1.97 in EM scores
across all three datasets when using LLaMa-2 as
backbone language model. Delving deeper, we
find that this improvement varies depending on
whether DPR successfully retrieves relevant doc-
uments. Specifically, the average improvement is
+1.2 when DPR retrieves relevant documents and
+2.3 when it does not on the NQ dataset. This dis-
parity suggests that our CON improve RALM’s in
scenarios where more noisy documents are fetched
in the first retrieval stage. This observation aligns
with our findings on noise robustness, which are
elaborated in the subsequent sections detailing our
experimental results.
Furthermore, the dynamics observed with larger
language models differ from those noted in experi-
ments with smaller-sized models due to their supe-
rior factual knowledge. The impact of utilizing re-
trieval is observed to be less pronounced with larger
models and can even be detrimental in certain cases,
such as with TriviaQA, where questions are mostly
straightforward. Concerning the comparison be-
tween CON and the baseline, the performance trend
remains consistent with that observed in smaller-
sized models, suggesting that CON maintains its
significance across different model sizes.



Source: data\tc16_2312.10997v5\referenced_papers\[50]_2311.09210.pdf (Page 6):

Models Noise NQ TriviaQA WebQ Average
Ratio EM F1 EM F1 EM F1 EM F1
Retrieve-Read 100% 34.28 41.74 55.30 61.67 29.58 46.34 39.72 49.92
+ CHAIN-OF-NOTE 41.83 49.58 64.30 70.00 36.85 53.07 47.66 57.55
(+7.55) (+7.84) (+9.00) (+8.33) (+7.27) (+6.73) (+7.94) (+7.63)
Retrieve-Read 80% 54.28 61.03 73.83 80.02 35.46 52.70 54.52 64.58
+ CHAIN-OF-NOTE 56.63 63.23 75.89 81.24 40.60 56.54 57.70 67.00
(+2.35) (+2.20) (+2.06) (+1.22) (+5.14) (+3.84) (+3.18) (+2.42)
Retrieve-Read 60% 61.44 67.94 78.44 83.65 37.01 54.16 58.96 68.58
+ CHAIN-OF-NOTE 63.43 69.33 78.79 84.07 41.26 56.91 61.16 70.10
(+1.99) (+1.39) (+0.35) (+0.42) (+4.25) (+2.75) (+2.20) (+1.52)
Retrieve-Read 40% 64.62 71.12 80.56 86.76 38.40 55.60 61.19 71.16
+ CHAIN-OF-NOTE 65.91 72.22 81.72 87.11 42.16 58.15 63.26 72.49
(+1.29) (+1.10) (+1.16) (+0.35) (+3.76) (+2.55) (+2.07) (+1.33)
Retrieve-Read 20% 67.21 73.69 81.73 87.89 39.95 56.66 62.96 72.75
+ CHAIN-OF-NOTE 70.00 76.08 82.86 88.24 44.36 60.13 65.74 74.82
(+2.79) (+2.39) (+1.13) (+0.35) (+4.41) (+3.47) (+2.78) (+2.07)
Retrieve-Read 0% 69.23 75.57 83.34 89.44 42.24 58.59 64.93 74.53
+ CHAIN-OF-NOTE 73.28 79.86 83.52 88.94 46.16 62.38 67.65 77.06
(+4.05) (+4.29) (+0.18) (-0.50) (+3.92) (+3.79) (+2.72) (+2.53)
Table 3: Evaluation on Noise Robustness. The backbone language model is LLaMa-2 7B. The CHAIN -OF-NOTE
framework shows superior performance compared to the standard RALM system, particularly notable at higher
noise ratios.We explain how we synthesize data with different noise ratios under real-world scenarios in§ 3.1.1.
Models↓ RealTimeQA
EM F1 RR
Retrieve-Read (Shi et al., 2023c)15.6 19.9 6.1
+ CHAIN-OF-NOTE (ours) 15.7 20.3 13.0
Table 4: Evaluation on Unknown Robustness. TheCON
shows better performance than standard RALM system.
3.3 Evaluation on Noise Robustness
As illustrated in Table 2, when faced with entirely
noisy documents, both the standard RALM and
our CHAIN -OF-NOTE enhanced RALM underper-
formed compared to the no-retrieval setting. This
suggests that RALMs can be misled by noisy infor-
mation, leading to more hallucinations.
Notably, equipping the model withCON enables
it to perform nearly as well as the baseline model
directly fine-tuned with QA pairs without retrieval,
showcasing its robustness to noise and its ability
to disregard irrelevant information. The CON ap-
proach is effective not only in fine-tuned, smaller-
sized models but also in large language models,
such as GPT-4, with adjustments made only to the
prompt. Besides, in comparison to the CHAIN -OF-
THOUGHT technique, commonly utilized in rea-
soning scenarios, CON presents a more efficient
strategy for retrieval-augmented settings, particu-
larly in addressing knowledge-intensive tasks.
Table 3 shows that RALM enhanced with CON
consistently outperforms the standard RALM, es-
pecially in scenarios with exclusively noisy doc-
uments. An average improvement of +7.9 in EM
score on fully noisy documents is observed on three
open-domain QA datasets, in average. Experiments
with lower noise ratios also consistently demon-
strate the improvements brought by CON, aligning
the overall QA performance.
3.4 Evaluation on Unknown Robustness
Table 4 illustrates that our RALM equipped with
CON exhibits superior robustness in handling un-
known scenario, particularly evident in the Real-
TimeQA benchmark. This benchmark falls com-
pletely outside the model’s domain and contains
real-time information that was not part of the
LLaMa-2 pre-training data. Despite this, models
are still capable of providing correct answers in
some cases, as the answers remain consistent over
time. In comparison to the standard RALM sys-
tem, our method shows a significant improvement,
exceeding +10.5 in its ability to reject to answer
questions in unknown scenario. The evaluation is
based on reject rate (RR), i.e., number of rejected



Source: data\tc16_2312.10997v5\referenced_papers\[19]_2311.06595.pdf (Page 7):

Figure 2: Model performance over differences between zero-shot and CREA-ICL method with k=1
and k=3 demonstrations for Vio-Lens test set using bloomz-3b (left) and mbert (right).
Figure 3: Example showing the different answer language when answering a Romanian question
information, which can affect their performance. The balance between performance and computa-
tional effort becomes clear when analyzing resource allocation, and shows that while models such as
Bloomz-3b demonstrate superior performance, larger models do not guarantee better results across
all evaluation criteria. This suggests the need for a more selective approach to model selection.
Furthermore, in the question-answering (QA) domain, zero-shot methods have been shown to
outperform the CREA-ICL approach. This underscores the robustness of zero-shot strategies in
classification contexts, but also points to their limitations in generative tasks, where they may not
always be the optimal choice. This finding encourages a more nuanced application of models,
depending on the nature of the task at hand.
Our analysis of cross-lingual tasks shows that classification accuracy often has an advantage over
that of generative tasks, especially when dealing with multilingual data. Classification tasks typically
require the model to produce short, constrained outputs, often limited to a single word or short phrase
from a predefined set of options. This specificity in response reduces the likelihood of encountering
language confusion.
Conversely, generative tasks require the production of longer sequences of text, which increases the
challenge of maintaining language consistency. This is evident in scenarios where a model, while
capturing the correct conceptual meaning, outputs in an unexpected language. Figure 3 illustrates this,
where the generated output is conceptually accurate but linguistically misaligned. Our methodology,
with its dependence on English templates, may inadvertently worsen this problem rather than mitigate
it, as the templates may bias the generative process towards English.
Furthermore, the metrics currently used for generative tasks still struggle with evaluating output
that is linguistically inconsistent. Our generative tasks have been modified to include a language
constraint that requires the output to match the language of the original text. However, as shown in
Table 4, this adjustment resulted in only marginal improvements. The implications are significant:
when evaluating generative tasks in a cross-lingual context, there remains a significant challenge in
accurately capturing the quality of language-specific output. In the future, the development of metrics
that can better account for language consistency will be crucial for evaluating the true effectiveness
of models in generative multilingual scenarios.
8



Source: data\tc16_2312.10997v5\referenced_papers\[97]_2310.07554.pdf (Page 6):

Retrieve Anything To Augment Large Language Models Conference’17, July 2017, Washington, DC, USA
Table 2: Impact on in-context learning. The performances are measured by Misc. metrics (see Appendix).
In-Context Learning
Method CQA Comm Coref Para NLI RC Sent D2T Summ Avg
None 0.2923 0.7212 0.6578 0.5242 0.4478 0.4892 0.7077 0.1982 0.1447 0.4645
BM25 0.3603 0.7019 0.6029 0.5059 0.4583 0.5396 0.7284 0.3019 0.1555 0.4840
Instructor 0.5003 0.7772 0.5735 0.6312 0.5360 0.6219 0.9148 0.4595 0.4572 0.6036
Contriever 0.4912 0.7723 0.5624 0.6358 0.5466 0.6297 0.9141 0.4380 0.4444 0.6009
RetroMAE-BEIR 0.4594 0.7742 0.5840 0.5755 0.5408 0.6029 0.9286 0.4661 0.4465 0.5939
BGE∗ 0.4718 0.7773 0.5550 0.6171 0.5413 0.5988 0.9281 0.4719 0.4521 0.5974
AAR 0.4809 0.7796 0.5848 0.5890 0.5354 0.6039 0.9210 0.4445 0.4410 0.5938
API-Retriever 0.4765 0.7620 0.5465 0.6266 0.5204 0.6096 0.9245 0.4866 0.4424 0.5945
LLM-R† 0.5165 0.7802 0.5830 0.6567 0.6145 0.6223 0.9059 0.4777 0.4878 0.6262
LLM-Embedder 0.5163 0.7842 0.5927 0.6556 0.6041 0.6318 0.9224 0.4731 0.4742 0.6268
Table 3: Impact on long conversation and language modeling (PPL), tool learning (NDCG), conv search (NDCG).
Conversation Language Modeling Tool C-Search
Method MSC Books3 Arxiv CodeParrot PG19 (o.d.) ToolLLM QReCC
None 19.3501 8.8193 3.7647 2.7663 10.2510 – –
Recency 13.9569 8.7391 3.4158 2.5989 10.2216 – –
BM25 14.6512 8.6576 3.3106 2.4591 10.1960 0.5115 0.4341
Instructor 14.8799 8.6619 3.3546 2.4756 10.2011 0.3882 0.2863
Contriever 14.2129 8.6460 3.2709 2.4437 10.1616 0.4904 0.3563
RetroMAE-BEIR 14.3990 8.6376 3.2903 2.4592 10.1731 0.5205 0.4037
BGE∗ 14.2943 8.6311 3.2912 2.4578 10.1541 0.5761 0.3856
AAR 14.6999 8.6381 3.3260 2.4666 10.1808 0.4200 0.2877
API-Retriever† 14.7834 8.6722 3.3858 2.4919 10.1833 0.8017 0.1137
Conv-ANCE† – – – – – – 0.4560
LLM-R 14.4746 8.6619 3.3635 2.4724 10.2024 0.1321 0.0234
LLM-Embedder 13.4832 8.6080 3.2322 2.4303 10.1185 0.8645 0.5053
a simple yet strong baseline called Recency. Rather than using re-
trieved context, Recency directly leverages the most recent context
immediately preceding the current window. For example, in con-
versation, it considers the last pair of utterances before the current
session; and in language modeling, it introduces the content within
the range of 2049-4096 tokens preceding the latest 2048 tokens.
With the introduction of this new baseline, the impact of retrieval
augmentation becomes more nuanced. On one hand, the LLM-
Embedder continues to exhibit superior performance across various
situations. On the other hand, other retrievers no longer guarantee a
consistent enhancement: although alternative retrieval-augmented
methods yield improved generation quality for language modeling,
a majority of them fall short of Recency’s performance while dealing
with conversation. This observation underscores the challenges
regarding effective memory retrieval in practice.
•Tool Learning and Conversation Search . The experiment
results on tool learning and conversational search are shown in
Table 3. In line with our prior observations, the task-specific ap-
proaches, i.e. the API retriever (Tool) and Conv-ANCE (Conv Search),
consistently deliver higher performances then most of the baselines.
Besides, unlike other cases, BM25 overtakes most of the embedding
models in these two scenarios. However, it’s worth noting that
LLM-Embedder continues to maintain the leading position, which
again highlights its capability in unifying diverse retrieval tasks.
3.2.3 Ablation Studies. The ablation studies are presented to ana-
lyze the influential factors about LLM-Embedder’s training process
(see Table 4): reward from LLM, instruction based fine-tuning, ho-
mogeneous in-batch negative sampling, and stabilized distillation.
For “w.o. LLM reward ”, we replace the soft reward from LLM
by using highest rated candidates as positive samples (i.e. hard la-
bels). By doing so, the knowledge distillation is reduced to contrast
learning. The empirical performance in most of the scenarios are
decreased due to such a change. However, the performances in tool
learning and conversational search are little affect; this is compre-
hensible knowing that LLM-Embedder is purely trained with hard
labels in both scenarios.
For “w.o. instruction FT ”, we remove the task-specific instruc-
tions while fine-tuning LLM-Embedder. Without such a component,
it will become harder for the embedding model to discriminate the
retrieval task in different scenarios. This speculation is consistent
with the observed result, as LLM-Embedder’s performance is de-
creased from such a change.
For “w.o. homo NS ”, the homogeneous in-batch negative sam-
pling is disabled. Such a change could reduce the discrimination of



### Claim 171/179

#### Claim Text
Extending it to solid particles moving through fluid is straightforward .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 32):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: A liquid flows at a constant flow rate through a pipe
with circular cross-sections of varying diameters. At one point in the
pipe, the diameter is 2 cm and the flow speed is 18 m/s. What is the flow
speed at another point in this pipe, where the diameter is 3 cm?
0) 4 m/s
1) 6 m/s
2) 8 m/s
3) 12 m/s
Original Answer [Incorrect]: The answer is 3) 12 m/s A step-by-step
solution is below: The flow rate is constant, so the product of the speed
and the cross-sectional area is constant. The cross-sectional area is
proportional to the square of the diameter, so the speed is inversely
proportional to the square of the diameter. The ratio of the speeds is the
inverse of the ratio of the squares of the diameters, so the speed at the
second point is 18m/s ∗ (2cm/3cm)2 = 12 m/s.
Stepback Question: What are the physics principles behind this
question?
Principles:
Conservation of Mass: the amount of mass flowing through the pipe at
two different points must be equal, or A1 * v1 = A2 * v2, where A1 is
the area of cross-section one, v1 is the velocity at point 1, A2 is the area
of cross-section two, v2 is the velocity at point 2.
Area of Circle: A = π ∗ r2, where r is the radius of the circle.
Final Answer: Using the Principles of Continuity equation, we can
solve the problem as following:
Step 1: Apply the Continuity equation to find out the flow speed at
another point in this pipe. The continuity equation states that the product
of the area and the velocity is constant at all points in a pipe. In this
problem, we are given the following information:
A1 = π ∗ (2cm)2 = 4πcm2
v1 = 18 m/s
A2 = π ∗ (3cm)2 = 9πcm2
We can use the continuity equation to find v2:
A1 * v1 = A2 * v2
4πcm2 ∗ 18m/s = 9πcm2 ∗ v2
v2 = 8 m/s
Therefore, the flow speed at another point in this pipe is 8 m/s.
The correct answer is 2) 8 m/s.
Table 29: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
33



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 30):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question Physics students are checking the constant acceleration equations of kinematics
by measuring the velocity of a tennis ball that is dropped and falls 6 meters
and then passes through a photogate. The predicted velocity is 20% above the
velocity measured by the photogate. Which of the following best describes the
cause of the large percent difference?
0) The ball changes its shape while falling.
1) The acceleration of gravity varies as the ball is falling.
2) Air resistance increases the acceleration of the ball.
3) The acceleration of the balls varies with the velocity.
Principles Vertical motion: y = 0.5 * g * t2, where y is the vertical displacement, g is the
acceleration due to gravity, t is the time.
Free fall: is a special case of motion with constant acceleration, in which the
acceleration is due to gravity.
Acceleration to Velocity Formula: v = u + a * t, where v is the final speed, u is
the initial velocity, a is the acceleration, and t is the time.
Answer Using the Principles of Vertical motion and Horizontal motion, we can solve
the problem as following:
Step 1: Apply the Vertical motion equation to find the time it takes for the ball
to fall 6 meters.
y = 0.5 * g * t2
6 m = 0.5 * 9.8 m/s2 * t2
t = 1.12 s
Step 2: Apply the Horizontal motion equation to find the velocity of the ball
when it passes through the photogate.
x = v * t
v = x / t
v = 6 m / 1.12 s
v = 5.36 m/s
Step 3: Compare the predicted velocity with the velocity measured by the
photogate. The predicted velocity is 20% above the velocity measured by the
photogate, so the predicted velocity is 1.2 * 5.36 m/s = 6.43 m/s.
Step 4: Find the cause of the large percent difference. The predicted velocity is
6.43 m/s, but the velocity measured by the photogate is 5.36 m/s. The difference
is 1.07 m/s. This difference is caused by air resistance. Air resistance is a force
that opposes the motion of an object through the air. It is caused by the friction
between the object and the air molecules. The greater the surface area of the
object, the greater the air resistance. The tennis ball has a large surface area, so
it experiences a lot of air resistance. This air resistance slows down the tennis
ball and causes the difference between the predicted velocity and the velocity
measured by the photogate.
Therefore, the correct answer is 0) The ball changes its shape while falling.
Table 27: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Reasoning Error. In this case, the model was able to correctly identify air resistance is the cause of
the velocity difference, but made the error at the final reasoning step of deriving the right answer.
31



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 16):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question A spherical conductor carries a net charge. How is this charge distributed on
the sphere?
0) The charge is evenly distributed on the surface.
1) The charge resides on the surface only; the distribution of charge on the
surface depends on what other charged objects are near the sphere.
2) The charge moves continually within the sphere.
3) The charge is distributed uniformly throughout the sphere.
Principles Coulomb’s Law: the force between two charged particles is proportional to the
product of their charges and inversely proportional to the square of the distance
between them, F = k * q1 * q2 / r, where F is the electric force, k is a constant,
q1 and q2 are the charges the particles carry, and r is the distance between them.
Answer Using the Principles of Coulomb’s Law, we can solve the problem as following:
Step 1: Apply Coulomb’s Law to find out how charges are distributed on the
surface.
In the case of a spherical conductor, the charges on the surface will repel each
other. The further apart the charges are, the less force they will exert on each
other. Therefore, the charges will distribute themselves evenly on the surface
of the sphere, as this is the configuration that minimizes the repulsive force
between them.
Step 2: Apply Coulomb’s Law to find out what happens if there are other
charges present.
The distribution of charge on the surface may also be affected by the presence
of other charged objects near the sphere. For example, if a negatively charged
object is brought near a positively charged sphere, the negative charges on the
sphere will be repelled and will move to the opposite side of the sphere. This
will result in a non-uniform distribution of charge on the surface of the sphere.
Therefore, the correct answer is 1) The charge resides on the surface only; the
distribution of charge on the surface depends on what other charged objects are
near the sphere.
Table 9: Few-shot demonstration exemplar for MMLU high-school Physics.
17



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 29):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question The circumference of a helium-filled balloon is measured for three different
conditions: at room temperature, after being in a warm oven for 30 minutes,
and after being in a freezer for 30 minutes. A student plotting the circumference
cubed C3 as a function of temperature T, should expect to find which of the
following?
0) a cubic relationship between C3 and T
1) an indirect relationship between C3 and T
2) an extrapolated temperature T where C3 reaches zero
3) a maximum C3 as the temperature T increases
Principles Ideal Gas Law: PV = nRT, where P is the pressure, V is the volume, n is
the number of moles of gas, R is the ideal gas constant, T is the absolute
temperature.
V olume of a sphere:V = 4/3 ∗ π ∗ r3, where r is the radius.
Absolute temperature: Absolute temperature is always written as kelvin.
Absolute zero: Absolute zero is the lowest limit of the kelvin scale and is the
lowest possible temperature (- 273.15 °C).
Answer Using the Principles of Charles’s Law and Gay-Lussac’s Law, we can solve the
problem as following:
Step 1: Apply Charles’s Law to find out the relationship between the volume
and temperature of the gas. Charles’s Law states that the volume of a fixed
mass of a gas is directly proportional to the absolute temperature of the gas
when pressure is kept constant. This means that if the temperature of the gas
increases, the volume of the gas will also increase.
Step 2: Apply Gay-Lussac’s Law to find out the relationship between the
pressure and temperature of the gas. Gay-Lussac’s Law states that the pressure
of a fixed mass of a gas is directly proportional to the absolute temperature of
the gas when volume is kept constant. This means that if the temperature of the
gas increases, the pressure of the gas will also increase.
Step 3: Apply Charles’s Law and Gay-Lussac’s Law to find out the relationship
between the circumference and temperature of the gas.The circumference of the
balloon is directly proportional to the volume of the gas. This is because the
circumference of the balloon is the distance around the outside of the balloon,
and the volume of the gas is the amount of space that the gas takes up.
The pressure of the gas is directly proportional to the temperature of the gas.
This is because the pressure of the gas is the force that the gas exerts on the
walls of the balloon, and the temperature of the gas is the measure of the
average kinetic energy of the gas molecules.
Therefore, the circumference of the balloon is directly proportional to the
temperature of the gas. This means that if the temperature of the gas increases,
the circumference of the balloon will also increase.
Step 4: Apply the relationship between the circumference and temperature
of the gas to find out the relationship between the circumference cubed and
temperature of the gas. The circumference cubed is directly proportional to the
temperature of the gas. This is because the circumference cubed is the volume
of the balloon, and the volume of the balloon is directly proportional to the
temperature of the gas.
Therefore, the student should expect to find a cubic relationship between C3
and T. The correct answer is 0) a cubic relationship between C3 and T.
Table 26: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Context Loss. In this case, at the end of the Reasoning chain, the model forgot the original question,
and lose the context to apply the reasoning to the question.
30



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 31):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: What happens to the pressure, P, of an ideal gas if
the temperature is increased by a factor of 2 and the volume is increased
by a factor of 8 ?
0) P decreases by a factor of 16.
1) P decreases by a factor of 4.
2) P decreases by a factor of 2.
3) P increases by a factor of 4.
Original Answer [Incorrect]: The correct answer is 0. The ideal gas
law is PV = nRT. If the temperature is increased by a factor of 2 and the
volume is increased by a factor of 8, then the pressure will decrease by a
factor of 16.
Stepback Question: What are the physics principles behind this
question?
Principles:
Ideal gas law: PV = nRT, where P is the pressure, V is the volume, n is
the number of moles, R is the gas constant, and T is the temperature.
Final Answer: Using the Principles of Ideal gas law, we can solve the
problem as following:
Step 1: Apply Ideal gas law to find out the pressure of an ideal gas. The
ideal gas law states that PV = nRT, where P is the pressure, V is the
volume, n is the number of moles, R is the gas constant, and T is the
temperature.
Step 2: Apply Ideal gas law to find out the pressure of an ideal gas when
the temperature is increased by a factor of 2 and the volume is increased
by a factor of 8. If the temperature is increased by a factor of 2, then T be-
comes 2T. If the volume is increased by a factor of 8, then V becomes 8V .
Substituting these values into the ideal gas law, we get: P(8V) = nR(2T)
Dividing both sides by 8V , we get: P = nR(2T) / 8V
We can see that the pressure has decreased by a factor of 4.
Therefore, the correct answer is 1) P decreases by a factor of 4.
Table 28: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
32



### Claim 172/179

#### Claim Text
We observe a typical known behavior in flagella : by 15 increasing the parameter Sp, the traveling wave velocity increases.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 32):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: A liquid flows at a constant flow rate through a pipe
with circular cross-sections of varying diameters. At one point in the
pipe, the diameter is 2 cm and the flow speed is 18 m/s. What is the flow
speed at another point in this pipe, where the diameter is 3 cm?
0) 4 m/s
1) 6 m/s
2) 8 m/s
3) 12 m/s
Original Answer [Incorrect]: The answer is 3) 12 m/s A step-by-step
solution is below: The flow rate is constant, so the product of the speed
and the cross-sectional area is constant. The cross-sectional area is
proportional to the square of the diameter, so the speed is inversely
proportional to the square of the diameter. The ratio of the speeds is the
inverse of the ratio of the squares of the diameters, so the speed at the
second point is 18m/s ∗ (2cm/3cm)2 = 12 m/s.
Stepback Question: What are the physics principles behind this
question?
Principles:
Conservation of Mass: the amount of mass flowing through the pipe at
two different points must be equal, or A1 * v1 = A2 * v2, where A1 is
the area of cross-section one, v1 is the velocity at point 1, A2 is the area
of cross-section two, v2 is the velocity at point 2.
Area of Circle: A = π ∗ r2, where r is the radius of the circle.
Final Answer: Using the Principles of Continuity equation, we can
solve the problem as following:
Step 1: Apply the Continuity equation to find out the flow speed at
another point in this pipe. The continuity equation states that the product
of the area and the velocity is constant at all points in a pipe. In this
problem, we are given the following information:
A1 = π ∗ (2cm)2 = 4πcm2
v1 = 18 m/s
A2 = π ∗ (3cm)2 = 9πcm2
We can use the continuity equation to find v2:
A1 * v1 = A2 * v2
4πcm2 ∗ 18m/s = 9πcm2 ∗ v2
v2 = 8 m/s
Therefore, the flow speed at another point in this pipe is 8 m/s.
The correct answer is 2) 8 m/s.
Table 29: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
33



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 30):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question Physics students are checking the constant acceleration equations of kinematics
by measuring the velocity of a tennis ball that is dropped and falls 6 meters
and then passes through a photogate. The predicted velocity is 20% above the
velocity measured by the photogate. Which of the following best describes the
cause of the large percent difference?
0) The ball changes its shape while falling.
1) The acceleration of gravity varies as the ball is falling.
2) Air resistance increases the acceleration of the ball.
3) The acceleration of the balls varies with the velocity.
Principles Vertical motion: y = 0.5 * g * t2, where y is the vertical displacement, g is the
acceleration due to gravity, t is the time.
Free fall: is a special case of motion with constant acceleration, in which the
acceleration is due to gravity.
Acceleration to Velocity Formula: v = u + a * t, where v is the final speed, u is
the initial velocity, a is the acceleration, and t is the time.
Answer Using the Principles of Vertical motion and Horizontal motion, we can solve
the problem as following:
Step 1: Apply the Vertical motion equation to find the time it takes for the ball
to fall 6 meters.
y = 0.5 * g * t2
6 m = 0.5 * 9.8 m/s2 * t2
t = 1.12 s
Step 2: Apply the Horizontal motion equation to find the velocity of the ball
when it passes through the photogate.
x = v * t
v = x / t
v = 6 m / 1.12 s
v = 5.36 m/s
Step 3: Compare the predicted velocity with the velocity measured by the
photogate. The predicted velocity is 20% above the velocity measured by the
photogate, so the predicted velocity is 1.2 * 5.36 m/s = 6.43 m/s.
Step 4: Find the cause of the large percent difference. The predicted velocity is
6.43 m/s, but the velocity measured by the photogate is 5.36 m/s. The difference
is 1.07 m/s. This difference is caused by air resistance. Air resistance is a force
that opposes the motion of an object through the air. It is caused by the friction
between the object and the air molecules. The greater the surface area of the
object, the greater the air resistance. The tennis ball has a large surface area, so
it experiences a lot of air resistance. This air resistance slows down the tennis
ball and causes the difference between the predicted velocity and the velocity
measured by the photogate.
Therefore, the correct answer is 0) The ball changes its shape while falling.
Table 27: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Reasoning Error. In this case, the model was able to correctly identify air resistance is the cause of
the velocity difference, but made the error at the final reasoning step of deriving the right answer.
31



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 26):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question An alarm whose frequency is 400 Hz is dropped out of a third-floor window. The
student who drops it measures the frequency with a very sensitive oscilloscope.
The measured frequency
0) appears higher than 400 Hz and the frequency increase as it falls
1) appears higher than 400 Hz and the frequency decreases as it falls
2) appears lower than 400 Hz and the frequency decreases as it falls
3) appears lower than 400 Hz and the frequency increases as it falls
Principles Doppler Effect: the effect produced by the motion of a wave source, observer, or
both that causes a change in wave frequency perceived by an observer relative
to the frequency produced by the source. The Doppler effect is observed with
all types of waves, including electromagnetic and acoustic waves. It was named
after Austrian physicist Christian Doppler.
Answer Using the Principles of Doppler Effect, we can solve the problem as following:
Step 1: Apply Doppler Effect to find out the measured frequency.
The Doppler Effect is the change in frequency of a wave as measured by an
observer who is moving relative to the source of the wave. In this case, the
source of the wave is the alarm, and the observer is the student who drops it.
As the alarm falls, the distance between the alarm and the student decreases.
This means that the frequency of the wave as measured by the student will
increase. Therefore, the measured frequency will appear higher than 400 Hz,
and the frequency will increase as the alarm falls.
Therefore, the correct answer is 0) appears higher than 400 Hz and the frequency
increase as it falls.
Table 23: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Factual Error. In this case, the Factual error leads to the wrong answer despite that the Principle and
Reasoning are all sound.
27



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 16):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question A spherical conductor carries a net charge. How is this charge distributed on
the sphere?
0) The charge is evenly distributed on the surface.
1) The charge resides on the surface only; the distribution of charge on the
surface depends on what other charged objects are near the sphere.
2) The charge moves continually within the sphere.
3) The charge is distributed uniformly throughout the sphere.
Principles Coulomb’s Law: the force between two charged particles is proportional to the
product of their charges and inversely proportional to the square of the distance
between them, F = k * q1 * q2 / r, where F is the electric force, k is a constant,
q1 and q2 are the charges the particles carry, and r is the distance between them.
Answer Using the Principles of Coulomb’s Law, we can solve the problem as following:
Step 1: Apply Coulomb’s Law to find out how charges are distributed on the
surface.
In the case of a spherical conductor, the charges on the surface will repel each
other. The further apart the charges are, the less force they will exert on each
other. Therefore, the charges will distribute themselves evenly on the surface
of the sphere, as this is the configuration that minimizes the repulsive force
between them.
Step 2: Apply Coulomb’s Law to find out what happens if there are other
charges present.
The distribution of charge on the surface may also be affected by the presence
of other charged objects near the sphere. For example, if a negatively charged
object is brought near a positively charged sphere, the negative charges on the
sphere will be repelled and will move to the opposite side of the sphere. This
will result in a non-uniform distribution of charge on the surface of the sphere.
Therefore, the correct answer is 1) The charge resides on the surface only; the
distribution of charge on the surface depends on what other charged objects are
near the sphere.
Table 9: Few-shot demonstration exemplar for MMLU high-school Physics.
17



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 31):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-Physics Original Question: What happens to the pressure, P, of an ideal gas if
the temperature is increased by a factor of 2 and the volume is increased
by a factor of 8 ?
0) P decreases by a factor of 16.
1) P decreases by a factor of 4.
2) P decreases by a factor of 2.
3) P increases by a factor of 4.
Original Answer [Incorrect]: The correct answer is 0. The ideal gas
law is PV = nRT. If the temperature is increased by a factor of 2 and the
volume is increased by a factor of 8, then the pressure will decrease by a
factor of 16.
Stepback Question: What are the physics principles behind this
question?
Principles:
Ideal gas law: PV = nRT, where P is the pressure, V is the volume, n is
the number of moles, R is the gas constant, and T is the temperature.
Final Answer: Using the Principles of Ideal gas law, we can solve the
problem as following:
Step 1: Apply Ideal gas law to find out the pressure of an ideal gas. The
ideal gas law states that PV = nRT, where P is the pressure, V is the
volume, n is the number of moles, R is the gas constant, and T is the
temperature.
Step 2: Apply Ideal gas law to find out the pressure of an ideal gas when
the temperature is increased by a factor of 2 and the volume is increased
by a factor of 8. If the temperature is increased by a factor of 2, then T be-
comes 2T. If the volume is increased by a factor of 8, then V becomes 8V .
Substituting these values into the ideal gas law, we get: P(8V) = nR(2T)
Dividing both sides by 8V , we get: P = nR(2T) / 8V
We can see that the pressure has decreased by a factor of 4.
Therefore, the correct answer is 1) P decreases by a factor of 4.
Table 28: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Physics dataset.
32



### Claim 173/179

#### Claim Text
The HITRAN2020 database contains line parameters f or the first five most abundant isotopocules of nitrous oxide – namely 14N216O, 14N15N16O, 15N14N16O, 14N218O, and 14N217O.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[98]_2307.03172.pdf (Page 16):

F Token Counts
Table 2, Table 3, and Table 4 present the average and maximum number of tokens in each of the input
contexts for all experimental settings. Note that MPT-30B and MPT-30B-Instruct use the same tokenizer,
GPT-3.5-Turbo and GPT-3.5-Turbo (16K) use the same tokenizer, and Claude-1.3 and Claude-1.3 (100K)
use the same tokenizer. Furthermore, the Claude-1.3 tokenizer is the same as the GPT-3.5-Turbo tokenizer,
modulo some additional special tokens that do not appear in our data. As a result, the token counts for
these two model families is the same in our experimental settings.
Closed-Book Oracle
avg ± stdev max avg ± stdev max
LongChat-13B (16K) 55.6 ± 2.7 70 219.7 ± 48.5 588
MPT-30B 43.5 ± 2.2 58 187.9 ± 41.8 482
GPT-3.5-Turbo 15.3 ± 2.2 29 156.0 ± 41.8 449
Claude-1.3 15.3 ± 2.2 29 156.0 ± 41.8 449
Table 2: Token count statistics for each of the evaluated models on the closed-book and oracle multi-document
question answering settings.
10 docs 20 docs 30 docs
avg ± stdev max avg ± stdev max avg ± stdev max
LongChat-13B (16K) 1749.9 ± 112.4 2511 3464.6 ± 202.3 4955 5181.9 ± 294.7 7729
MPT-30B 1499.7 ± 88.5 1907 2962.4 ± 158.4 3730 4426.9 ± 230.5 5475
GPT-3.5-Turbo 1475.6 ± 86.5 1960 2946.2 ± 155.1 3920 4419.2 ± 226.5 6101
Claude-1.3 1475.6 ± 86.5 1960 2946.2 ± 155.1 3920 4419.2 ± 226.5 6101
Table 3: Token count statistics for each of the evaluated models on each of the document question answering
settings.
75 KV pairs 140 KV pairs 300 KV pairs
avg ± stdev max avg ± stdev max avg ± stdev max
LongChat-13B (16K) 5444.5 ± 19.1 5500 10072.4 ± 24.1 10139 21467.3 ± 35.9 21582
MPT-30B 4110.5 ± 23.8 4187 7600.9 ± 31.1 7687 16192.4 ± 46.6 16319
GPT-3.5-Turbo 3768.7 ± 25.6 3844 6992.8 ± 34.1 7088 14929.4 ± 50.7 15048
Claude-1.3 3768.7 ± 25.6 3844 6992.8 ± 34.1 7088 14929.4 ± 50.7 15048
Table 4: Token count statistics for each of the evaluated models on each of the key-value (KV) retrieval settings.



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 15):

0 50 100 150
1.6
1.8
2
2.2
2.4
2.6
2.8
3 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(a) ELI5 - p = 10%
0 50 100 150
1.4
1.45
1.5
1.55
1.6
1.65 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (b) ELI5 - p = 30%
0 50 100 150
1.04
1.06
1.08
1.1
1.12
1.14 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (c) ELI5 - p = 50%
0 10 20 30 40 50
1.6
1.8
2
2.2
2.4
2.6
2.8
3
3.2
3.4 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(d) MS MARCO - p = 10%
0 10 20 30 40 50
1.15
1.2
1.25
1.3
1.35
1.4 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
0.91 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (f) MS MARCO - p = 50%
0 20 40 60 80 100
1.5
2
2.5
3
3.5
1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(g) NQ - p = 10%
0 20 40 60 80 100
1.5
1.6
1.7
1.8
1.9
2 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
Loa ding [ M a thJ a x ] / ex tensions/ M a thM enu .j s (h) NQ - p = 30%
0 20 40 60 80 100
1.22
1.24
1.26
1.28
1.3
1.32
1.34
1.36 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (i) NQ - p = 50%
Figure 7: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-12), for FiD-Base models.
Each row represents a different dataset, and every column represents a different filtering percentage (10,30,50).



Source: data\tc16_2312.10997v5\referenced_papers\[38]_2307.07164.pdf (Page 13):

Input
What happens next in this paragraph? How to survive remedial classes Look at the course as an opportunity.
Many students are discouraged when they are assigned to a remedial class. Some assume this placement
means they aren’t ready for college. OPTIONS:
A) However, people who are not unable to do what they’re given on campus, or those who are cut out
from college academies, are likely to have some little snitches. You want to be prepared for a negative
outcome if possible.
B) In this case, you should consider what you will do if your subject consists of a certain term or number
of subject areas. You could set up a study study program yourself or tutor a student who is struggling to
thoroughly comprehend where they sat for homework.
C) If you take the course, you might find you feel highly motivated after passing the test. Try to develop a
positive attitude towards the course so that you are not discouraged when you take your homework at the
end of the day.
D) However, being assigned a remedial class doesn’t mean that you are behind, just that you have an
opportunity to receive better instruction and improve your skills in a subject that you have struggled
with in the past. There is nothing unusual about being asked to attend a remedial course: two thirds of
community college students take at least one remedial course.
Output D
Table 9: Input-output format for GPT-35-Turbo. This example is from the HellaSwag dataset. We add some line
breaks for better readability.
Task Zero-shot Random Kmeans BM25 E5 base SBERT EPR LLM-R
1 iter 2 iter 3 iter
AESLC 5.8 19.4 19.0 26.8 27.0 25.3 26.0 26.7 27.3 27.1
AGNews 31.5 67.4 71.9 90.6 90.6 90.2 91.8 92.4 93.5 93.5
ARC Chall. 35.6 39.7 40.5 40.3 44.6 42.8 43.0 43.4 43.6 44.0
ARC Easy 51.0 60.0 61.8 59.9 63.0 63.1 63.1 63.6 63.3 63.6
BoolQ 64.7 70.0 69.0 74.7 72.4 73.9 74.8 75.6 75.1 74.1
CommonGen 19.2 36.3 34.4 37.6 37.4 37.6 39.2 38.2 37.7 37.3
COPA 66.0 80.0 85.0 78.0 83.0 82.0 82.0 84.0 84.0 84.0
DART 22.9 52.0 46.6 55.9 54.7 54.4 56.2 57.3 57.2 57.3
E2E NLG 34.6 52.7 46.4 54.5 51.8 50.2 53.6 54.9 54.7 54.9
Gigaword 15.3 30.0 30.7 32.7 32.5 32.6 32.4 33.3 32.5 31.8
HellaSwag 71.5 73.9 74.0 74.9 75.2 75.3 75.2 75.4 75.5 75.4
MNLI (m) 35.8 46.3 44.2 50.1 44.5 50.8 59.9 68.2 70.2 69.8
MNLI (mm) 35.6 48.1 45.4 48.3 44.7 49.3 61.5 69.5 72.0 71.3
MRPC 69.1 49.5 38.0 61.8 41.2 52.7 55.9 62.3 75.3 78.2
MultiRC 57.0 48.5 34.1 54.2 56.0 55.3 50.4 52.9 51.5 52.1
NQ 0.3 21.5 22.6 37.6 39.3 39.4 39.2 39.4 39.1 39.2
OpenBook QA 41.6 49.8 49.0 49.6 51.4 51.4 49.6 50.8 52.2 53.4
PAWS 53.2 57.0 56.6 56.6 55.4 58.2 57.7 57.0 56.6 57.0
PIQA 77.0 79.1 79.4 81.3 81.3 80.7 80.5 80.9 81.6 80.6
QNLI 49.2 56.4 53.4 62.2 61.5 61.9 65.0 74.4 69.6 69.4
QQP 57.7 63.4 63.3 79.8 77.5 81.3 81.7 80.1 82.6 83.3
RTE 59.6 59.9 58.5 65.7 63.9 67.2 66.8 67.2 68.6 70.4
Sentiment140 49.3 88.6 89.4 90.8 93.9 92.2 91.4 90.8 91.1 90.3
SNLI 39.8 43.7 52.5 47.1 53.5 58.4 68.4 80.2 82.0 82.2
SQuAD v1 2.1 64.1 62.3 61.2 60.8 61.6 64.3 60.7 57.3 52.5
SST2 54.4 85.9 89.7 84.4 92.1 87.6 88.7 94.0 93.8 93.1
Winogrande 62.0 66.7 66.5 67.5 66.9 66.5 66.5 67.9 68.1 67.2
WSC 64.4 60.6 56.7 56.7 61.5 63.5 61.5 60.6 63.5 66.4
WSC273 74.0 74.4 74.7 64.5 65.2 62.6 65.2 74.4 79.5 78.8
Yelp 47.9 92.0 93.5 93.5 97.3 95.9 95.1 95.7 95.9 95.5
Average 44.9 57.9 57.0 61.3 61.4 62.1 63.5 65.7 66.5 66.4
Table 10: Detailed results for each dataset.



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 17):

0 50 100 150
1.4
1.6
1.8
2
2.2
2.4
2.6 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(a) ELI5 - p = 10%
0 50 100 150
1.35
1.4
1.45
1.5
1.55
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (b) ELI5 - p = 30%
0 50 100 150
1.04
1.06
1.08
1.1
1.12
1.14 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (c) ELI5 - p = 50%
0 10 20 30 40 50
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
3.2 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(d) MS MARCO - p = 10%
0 10 20 30 40 50
1.1
1.15
1.2
1.25
1.3
1.35 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.84
0.86
0.88
0.9
0.92 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (f) MS MARCO - p = 50%
0 20 40 60 80 100 120
1.5
2
2.5
3
3.5
4
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
(g) NQ - p = 10%
0 20 40 60 80 100 120
1.5
1.6
1.7
1.8
1.9
2
2.1 1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen
Loa ding [ M a thJ a x ] / ex tensions/ M a thM enu .j s (h) NQ - p = 30%
0 20 40 60 80 100 120
1.25
1.3
1.35
1.4
1
2
3
6
12
18
24
Answer Length (in tokens)
% of Gold from Chosen (i) NQ - p = 50%
Figure 9: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-24), for FiD-Large models.
Each row represents a different dataset, and every column represents a different filtering percentage (10,30,50).



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 16):

0 50 100 150
1
1.5
2
2.5
3 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens
(a) ELI5 - p = 10%
0 50 100 150
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (b) ELI5 - p = 30%
0 50 100 150
0.85
0.9
0.95
1
1.05
1.1
1.15
1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (c) ELI5 - p = 50%
0 10 20 30 40 50
0.5
1
1.5
2
2.5
3
3.5 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens
(d) MS MARCO - p = 10%
0 10 20 30 40 50
0.6
0.8
1
1.2
1.4
1.6
1.8 1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.8
0.9
1
1.1
1.2
1
2
10
20
50
80
100
Answer Length (in tokens)
% from Chosen Tokens (f) MS MARCO - p = 50%
0 20 40 60 80 100
0.5
1
1.5
2
2.5
3
3.5 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens
(g) NQ - p = 10%
0 20 40 60 80 100
0.8
1
1.2
1.4
1.6
1.8
2 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens (h) NQ - p = 30%
0 20 40 60 80 100
0.9
1
1.1
1.2
1.3
1.4 1
2
10
20
50
80
Answer Length (in tokens)
% from Chosen Tokens (i) NQ - p = 50%
Figure 8: The percentage of tokens that were chosen from each passage, for FiD-Base models. The gold passage
(labeled as 1) is colored red. Each row represents a different dataset, and every column represents a different filtering
percentage (10,30,50).



### Claim 174/179

#### Claim Text
On the other hand, the δ-doped sample presents a strong Raman emission from the diamond host below 550 nm , which saturates the counts at higher wavelengths, 4 0 7 14 NV− SiV (a) δ-doped 600 650 700 750 800 Wavelength (nm) 0 7 14 NV0 (b) (111)-implanted Intensity (kCounts/s) FIG.

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 14):

Language Models as Semantic Indexers
0 10 20 30 40 50
#Doc in each ID
0.0
0.1
0.2
0.3
0.4Density
(a) Beauty
0 10 20 30 40 50 60 70 80
#Doc in each ID
0.00
0.05
0.10
0.15
0.20
0.25
0.30Density (b) Sports
0 5 10 15 20 25 30 35
#Doc in each ID
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7Density (c) Toys
Figure 6.Duplication study of LMI NDEXER ’s semantic IDs.
00.0050.010.0150.020.0250.030.0350.040.045
1 2 3
Recall@5
ID length
(a) beauty
00.0050.010.0150.020.025
1 2 3
Recall@5
ID length (b) sports
00.0050.010.0150.020.0250.030.0350.040.045
1 2 3
Recall@5
ID length (c) toys
Figure 7.Semantic ID length study on recommendation.
to unseen documents, demonstrating its strong semantic
capturing capability.
Table 15.Zero-shot study.
Model Recall@50 Recall@100
rq-V AE indexer 0.0000 0.0105
HC indexer 0.0000 0.0070
LMINDEXER 0.0455 0.0524
15



Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 5):

Language Models as Semantic Indexers
… Codebook𝑬… Codebook𝑬
…
Deep TransEncoderDeep TransDecoder
Semantic IndexerSemantic Encoder
User history
Document IDs… Codebook𝑬… Codebook𝑬
…
Deep TransEncoderDeep TransDecoder
Semantic IndexerSemantic Encoder
Item IDs
query
(a) Recommendation(b) Retrieval
Figure 2.LMI NDEXER can be fine-tuned on downstream tasks including recommendation (user history as input and item ID as output)
and retrieval (query as input and document ID as output).
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08
Probability
Sets & Kits
Maternity
Eyes
Hair Color
Lips
Face
Body
Styling Products
Cleansers
Conditioners
Scrubs & Body Treatments
Bath
Bags & Cases
Makeup Brushes & Tools
Makeup Sets
Nails
Nail Tools
Hands & Nails
Category
Probabilities by Category and Semantic ID
Semantic ID
(0, *, *)
(1, *, *)
(2, *, *)
(3, *, *)
(a) The ground-truth category distribution for items in the
Amazon-Beauty dataset is colored by the value of first ID c1.
0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040
Probability
Cosmetic Bags
Nail Art
Nail Art Equipment
Nail Brushes
Nail Files & Buffers
Nail Polish
Nail Treatments
Refillable Containers
Sets & Kits
Top & Base Coats
Category
Probabilities by Category and Semantic ID
Semantic ID
(0, 0, *)
(0, 1, *)
(0, 2, *)
(0, 3, *)
(0, 4, *)
(0, 5, *)
(0, 6, *)
(0, 7, *)
(0, 8, *)
(0, 9, *)
(0, 10, *)
(0, 11, *)
(0, 12, *)
(0, 13, *)
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14
Probability
Body Scrubs
Body Washes
Bubble Bath
Cleansers
Creams & Moisturizers
Creams, Gels & Lotions
Exfoliators & Scrubs
Moisturizers
Soaps
Treatments & Masks
Category
Probabilities by Category and Semantic ID
Semantic ID
(1, 0, *)
(1, 1, *)
(1, 2, *)
(1, 3, *)
(1, 4, *)
(1, 5, *)
(1, 6, *)
(1, 7, *)
(1, 8, *)
(1, 9, *)
(1, 10, *)
(1, 11, *)
(1, 12, *)
(1, 13, *)
(1, 14, *)
(1, 15, *)
(1, 16, *)
(1, 17, *)
(1, 18, *)
0.00 0.05 0.10 0.15 0.20 0.25
Probability
Blush
Chemical Hair Dyes
Concealers & Neutralizers
Eye Shadow
Foundation
Liner & Shadow Combinations
Lip Glosses
Lip Stains
Lipstick
Lipstick Primers
Mascara
Nail Polish
Powder
Category
Probabilities by Category and Semantic ID
Semantic ID
(2, 0, *)
(2, 1, *)
(2, 2, *)
(2, 3, *)
(2, 4, *)
(2, 5, *)
(2, 6, *)
(2, 7, *)
(2, 8, *)
(2, 9, *)
(2, 10, *)
(2, 11, *)
(2, 12, *)
(2, 13, *)
(2, 14, *)
(2, 15, *)
(2, 16, *)
(2, 17, *)
(2, 18, *)
(2, 19, *)
(2, 20, *)
0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200
Probability
Combinations
Creams
Creams & Moisturizers
Dark Circle Treatments
Fillers
Moisturizers
Oils & Serums
Puffiness Treatments
Sets & Kits
Treatments & Masks
Category
Probabilities by Category and Semantic ID
Semantic ID
(3, 0, *)
(3, 1, *)
(3, 2, *)
(3, 3, *)
(3, 4, *)
(3, 5, *)
(3, 6, *)
(3, 7, *)
(3, 8, *)
(3, 9, *)
(3, 10, *)
(3, 11, *)
(3, 12, *)
(3, 13, *)
(3, 14, *)
(3, 15, *)
(3, 16, *)
(3, 17, *)
(3, 18, *)
(b) The category distributions for items having the Semantic ID as
(c1, ˚, ˚), where c1 P{0, 1, 2, 3}. The categories are colored based
on the second semantic token c2.
Figure 3.Semantic ID qualitative study on Amazon-Beauty.
Qualitative Results. We conduct a detailed study on the
quality of the learned semantic IDs from LMI NDEXER on
Amazon-Beauty dataset. For each product d in the dataset,
its learned semantic ID is represented as cd “c1
dc2
dc3
d. We
randomly select four c1
d values (i.e., 0, 1, 2, 3) and analyze
the products whose c1
d Pt0, 1, 2, 3u. The results are shown
in Figure 3. In Figure 3(a), we summarize each item’s
category using c1 to visualize c1-specific categories in the
overall category distribution of the dataset. As shown in Fig-
ure 3(a), c1 captures the coarse-grained category of the item.
For example, c1 “1 contains most of the products related
to “Bath”. Similarly, the majority of items with c1 “0 are
“Tool” and “Make-up” products for nails. We also visualize
the hierarchical structure of LMI NDEXER learned Semantic
IDs by fixing c1 and visualizing the category distribution
for all possible values of c2 in Figure 3(b). We again found
that the second ID c2 further categorizes the coarse-grained
semantics captured with c1 into fine-grained categories.
4.3. Training Study
In this section, we study the optimization process (recon-
structor collapse, posterior collapse, and contrastive loss
discussed in Sec 3.2) of our semantic indexer from two
Table 1.ID quantitative study (AMI) on Amazon datasets.
Model Beauty Sports Toys
rq-V AE indexer (BERT) 0.2654 0.2774 0.3154
HC indexer (BERT) 0.2428 0.2387 0.2729
rq-V AE indexer (In-domain SimCSE) 0.3100 0.2695 0.3126
HC indexer (In-domain SimCSE) 0.2771 0.2622 0.2968
LMINDEXER 0.3563 0.4163 0.3536
perspectives: reconstruction quality and semantic ID diver-
sity. We serve token reconstruction Macro-F1 (Opitz &
Burst, 2019) and semantic ID perplexity of all the docu-
ments (Horgan, 1995) as the main evaluation metrics. A
high-quality semantic indexer should contribute to a high
reconstruction quality (high Macro-F1) and a high semantic
ID diversity (high perplexity). We conduct model studies on
Amazon-sports shown in Figure 4 and have the following
findings: 1) Reconstructor collapse: The reconstruction
Macro-F1 is low without reconstructor warm-up, shown in
Figure 4(a). In this case, the reconstructor suffers from low
reconstruction capability and cannot provide meaningful
signals to train the semantic indexer. 2) Posterior collapse:
The semantic ID perplexity is low without semantic encoder
and codebook warm-up, shown in Figure 4(b). This indi-
6



Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 15):

Language Models as Semantic Indexers
00.010.020.030.040.050.06
128 256 512
Recall@5
Codebook Size
(a) Recommendation
0.20.220.240.260.280.30.320.34
128 256 512
NDCG@5
Codebook Size (b) Product Search
Figure 8.Codebook size study on Amazon-Beauty.
16



Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 3):

Language Models as Semantic Indexers
… Codebook𝑬… Codebook𝑬
…
Semantic IDsWordtoken
He onlya rival
Deep TransEncoderDeep TransDecoder
Semantic Indexer
<s>
ShallowTransformer
Masked token 
…He only
is beenrival
ReconstructorSemantic Encoder
is been
Semantic ID representation
Document
 hints
Figure 1.The LMI NDEXER self-supervised ID learning framework overview. The proposed semantic indexer includes a semantic ID
encoder and several codebooks. During self-supervised learning, there is a reconstructor to reconstruct the input document from semantic
ID representations.
query channel input embeddings, and dh (token embeddings
correspond to dh) are fed as key and value channel input
embeddings in the multi-head self-attention. We adopt a
shallow reconstructor which has limited reconstruction capa-
bility based only on the hints in order to force the semantic
indexer to provide high-quality representations. The recon-
struction is conducted as follows:
zw “Reconϕpcd, dhq“
ÿ
t
Transpq “ct
d, k“dh, v“dhq
Preconpw|cd, dhq“ softmaxpW zwq
(5)
where W is the token embedding matrix. However, directly
adopting the reconstruction objective with cd as input to the
reconstructor will not optimize the semantic encoder. Since
the codebook look-up in Eq.(2) is a hard/discrete operation,
the reconstruction objective backpropagation gradients will
flow to the embeddings in the codebook rather than to the
parameters in the semantic encoder. To this end, we propose
to approximate the argmax operation similar to (Jang et al.,
2016) as follows:
ˆct
d “
$
&
%
arg maxet
jPEt ht
d ¨et
j forward pass.
ř
et
jPEt
exppht
d¨et
jqř
et
jPEt exppht
d¨et
jq et
j backward pass. (6)
In the forward pass, we still adopt the argmaxp¨qhard op-
eration; while in the backward pass, the selected semantic
embedding becomes a weighted average of the codebook
embeddings, to enable gradients to flow to ht
d and finally
to the parameters in the semantic encoder. In our imple-
mentation, we achieve this by adopting the stop gradient
operator (Van Den Oord et al., 2017). The reconstruction is
then conducted by
zw “Reconϕpˆct
d, dhq“
ÿ
t
Transpq “ˆct
d, k“dh, v“dhq
(7)
3.2. Training Self-Supervised Semantic Indexer
Progressive Training. To optimize the semantic indexer
and obtain semantic IDs in an auto-regressive way, we adopt
the progressive training scheme similar to (Sun et al., 2023).
The entire learning process consists ofT learning steps, each
corresponding to a specific semantic IDct
d being learned and
optimized at position t within the range of [T]. Additionally,
at each step t, both the ID ct
d and the model parameters
associated with generating ct
d are updated, while previously
generated IDs căt
d remain unchanged. The reconstruction
objective in t-step is shown as:
Lt
recon “´
ÿ
d
ÿ
wPdzdt
h
logPreconpw|cďt
d , dt
hq. (8)
Here dt
h is the hints provided for learning ID on position
t. We will gradually reduce the amounts of hints dt
h as t
increases to inject new knowledge into the new IDs, and
finally contribute to a hierarchical, coarse-to-fine-grained
semantic ID learning.
Contrastive Loss. The reconstruction objective in Eq.(8)
can force the semantic IDs to capture document-level se-
mantics. However, only optimizing the objective can lead to
the case where similar documents sharing căt
d also have the
same ct
d. To alleviate this issue, we propose a contrastive
objective to promote distinction between documents that
previously shared the same prefix, enabling the model to
discern finer-grained hierarchical relationships between doc-
uments:
Lt
contrastive “´
ÿ
d
log exppht
d ¨ht
dq
exppht
d ¨ht
dq` ř
căt
d1 “căt
d
exppht
d ¨ht
d1q.
(9)
The contrastive objective can help push ht
d of documents
sharing the same căt
d away in the t-th latent space and force
them to obtain diverse ct
d, finally contributing to higher
codebook utilization.
Commitment Loss. In addition, when learning the docu-
ment semantic IDs for position t, it is important that the
semantic indexer should remember the IDs that are already
learned before position t. To this end, we add a commitment
loss as:
Lt
commitment “´
ÿ
d
ÿ
jăt
log Pspcj
d|d, căj
d q. (10)
We optimize our model at step t based on a combination of
4



Source: data\tc16_2312.10997v5\referenced_papers\[40]_2310.07815.pdf (Page 12):

Language Models as Semantic Indexers
Table 9.Hyper-parameter configuration for self-supervised semantic ID learning.
Parameter Amazon-Beauty Amazon-Sports Amazon-Toys NQ MACRO-1M
Optimizer Adam Adam Adam Adam Adam
Adamϵ 1e-6 1e-6 1e-6 1e-6 1e-6
Adampβ1, β2q (0.9, 0.999) (0.9, 0.999) (0.9, 0.999) (0.9, 0.999) (0.9, 0.999)
Batch size 128 128 128 128 128
Max epochs 30 30 30 10 5
Max sequence length 512 512 512 512 128
ID length 3 3 3 3 3
Codebook size 512 512 512 5120 51200
Hint ratio 50%, 30% 50%, 30% 50%, 30% 50%, 30% 50%, 30%
Learning rate searched in {1e-3, 2e-3, 5e-3}
Backbone LM T5-base
Table 10.Hyper-parameter configuration for generative recommendation.
Parameter Amazon-Beauty Amazon-Sports Amazon-Toys
Optimizer Adam Adam Adam
Adamϵ 1e-6 1e-6 1e-6
Adampβ1, β2q (0.9, 0.999) (0.9, 0.999) (0.9, 0.999)
Batch size 32 32 32
Max steps 10,000 10,000 10,000
Max sequence length 1024 1024 1024
Bean size 20 20 20
Learning rate searched in {1e-2, 1e-3, 1e-4}
Backbone LM T5-base
et al., 2019) to generate pseudo queries for each document
in NQ and MS MACRO for training augmentation. The
number of pseudo queries for each document is set to be 15
and 20 respectively. We train all the compared generative
retrieval methods for 250,000 and 500,000 steps in NQ and
MS MACRO respectively, with the learning rate searched
in {5e-4, 1e-3, 5e-3 }. The batch size is set to 2048, the
maximum input text length is set to 32 and all experiments
are run on an 8 A100 40G machine. The number of beams
for beam search is set to 20. All baselines initially load the
same T5-base checkpoint. The hyper-parameter configura-
tion for generative document retrieval training can be found
in Table 12.
A.4. Definition of AMI
The Adjusted Mutual Information (AMI) score (Vinh et al.,
2009) is a measure used in statistics and information the-
ory to quantify the agreement between two clusters (in our
experiments, the two clusters refer to ground truth cate-
gory clusters and Semantic ID clusters) while correcting for
chance. It is an adjustment of the Mutual Information (MI)
score that accounts for the fact that MI is generally higher
for clusters with a larger number of clusters, thus providing
a normalized score that is more comparable across different
clusters.
A.5. Duplication Study of Semantic IDs
The duplication issue is very important in learning self-
supervised semantic IDs. To alleviate this issue, we propose
a contrastive objective in Section 3.2 to promote distinction
between documents that previously shared the same prefix
and encourage them to obtain different ID for the next posi-
tion (alleviate duplication). The effectiveness of this design
is shown in Figure 4(d)(e). We can find that during self-
supervised learning, if the contrastive objective is added,
the difference ratio on the next ID position of documents
sharing ID prefix is larger and the diversity (perplexity) of
IDs on the next position is larger, which means that the
duplication issue is alleviated.
We also plot the density curve of the number of documents
assigned to each semantic ID after self-supervised learning.
The results are shown in Figure 6. We can find that the
semantic IDs learned by LMIndexer are quite distinguish-
able since most IDs contain less than 5 documents. While
it is nearly impossible to guarantee zero duplication after
self-supervised training since there can be documents that
have nearly the same semantics, we simply add another final
ID position to distinguish them.
A.6. Human Evaluation of Semantic ID Quality
In this section, we conduct a human evaluation of the learned
semantic IDs from different methods. We adopt a three-step
pipeline to conduct the evaluation: 1) We Randomly select
product pairs in the Amazon-sports dataset that share the
13



### Claim 175/179

#### Claim Text
Interestingly, when considering the isolated axoneme f = 0, the cubic model can be interpreted as a form of van der Pol oscillator for the velocity variable .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[93]_2309.11495.pdf (Page 15):

8.5 F ACTOR +REVISE : I DENTIFY WHICH FACTS ARE CONSISTENT
Context: <Original Fact>.
From another source,
<output of execute verification step: Q + A>
Response: CONSISTENT. <Consistent fact>
Context: <Original Fact>.
From another source,
<output of execute verification step: Q + A>
Response: INCONSISTENT.
Context: <Original Fact>.
From another source,
<output of execute verification step: Q + A>
Response: PARTIALLY CONSISTENT. <Consistent part>
Table 9: In the CoVe (Factor + Revise) variant, as part of step (3) after subsection 8.3, the model is
made to explicitly identify which facts are consistent between the two sources. The consistent facts
can then be spliced together.
16



Source: data\tc16_2312.10997v5\referenced_papers\[93]_2309.11495.pdf (Page 18):

Figure 8: Examples where questions asking for a fact are answered correctly, but verifying via a
yes/no question is incorrect (the model tends to agree with the way the question is stated, even if it
was stated incorrectly).
19



Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 2):

Preprint
• We present extensive experiments on STS and demonstrate that AnglE can substantially improve
the text embedding quality in various scenarios.
2 R ELATED WORK
This section is organized as follows: we first introduce the unsupervised approaches, then the super-
vised approaches, and finally give a summary.
Unsupervised Approaches Early studies (Hill et al., 2016; Pagliardini et al., 2018) have demon-
strated the efficacy of augmenting word2vec (Mikolov et al., 2013) with n-gram embeddings, yield-
ing strong results in text embeddings. Recently, BERT-flow (Li et al., 2020) has introduced a flow-
based approach that maps BERT embeddings to a standard Gaussian latent space. On the other hand,
BERT-whitening (Su et al., 2021) applies the whitening operation to BERT embeddings to enhance
text embeddings. Furthermore, very recent research (Carlsson et al., 2020; Zhang et al., 2020; Giorgi
et al., 2021; Gao et al., 2021; Yan et al., 2021; Chuang et al., 2022; Jiang et al., 2022b; Zhuo et al.,
2023) has focused on leveraging contrastive objectives to improve the quality of text embeddings.
Supervised Approaches Supervised text embeddings usually perform better than their unsuper-
vised counterparts (Gao et al., 2021). Various studies have effectively utilized supervised datasets to
enhance the learning of text embeddings. In particular, Conneau et al. (2017) introduced a method
that leverages supervised Natural Language Inference (NLI) tasks for this purpose. Building on
a transformer backbone, USE (Cer et al., 2018) incorporates the SNLI dataset to augment unsu-
pervised training, resulting in improved performance. Furthermore, SBERT (Reimers & Gurevych,
2019) enhances text embedding by combining BERT with a siamese architecture. Jiang et al. (2022a;
2023) proposed the use of prompt engineering to improve text embeddings.
However, most existing models optimize the cosine similarity but neglect the negative effect of the
saturation zone of the cosine function. To address this issue, this paper proposes a novel angle-
optimized text embedding model to improve the quality of text embedding.
3 M ETHODOLOGY
This section will introduce the components of the proposed angle-optimized text embedding model,
including the input layer, cosine objective, in-batch negative objective, and angle objective.
3.1 I NPUT LAYER
For the input sentences, we first apply padding to ensure a consistent length l. Next, we map each
word to a continuous d-dimensional space to produce word embeddings ei ∈ Rd. These word
embeddings are then concatenated to form the model input: E = [e1, e2, . . . ,el] ∈ Rl×d. Subse-
quently, the model input is passed through an encoder such as BERT (Devlin et al., 2019), RoBERTa
(Liu et al., 2019), and LLaMA (Touvron et al., 2023a;b) to obtain the contextual representation X.
3.2 C OSINE OBJECTIVE
Following the prior study (Su, 2022), we employ the cosine objective function for end-to-end opti-
mization of cosine similarity between representations, as follows:
Lcos = log

1 +
X
s(Xi,Xj)>s(Xm,Xn)
e
cos(Xm,Xn)−cos(Xi,Xj)
τ

 (1)
where τ is a temperature hyperparameter, cos(·) is the cosine similarity function, and s(u, v) is the
similarity between u and v. By optimizing the Lcos, we expect the cosine similarity of the high
similarity pair to be greater than that of the low similarity pair.
3



Source: data\tc16_2312.10997v5\referenced_papers\[93]_2309.11495.pdf (Page 3):

3.2 P LAN VERIFICATIONS
Conditioned on the original query and the baseline response, the model is prompted to generate
a series of verification questions that test the factual claims in the original baseline response. For
example if part of a longform model response contains the statement “The Mexican–American War
was an armed conflict between the United States and Mexico from 1846 to 1848”, then one possible
verification question to check those dates could be “When did the Mexican American war start and
end?”. We note that verification questions are not templated and the language model is free to phrase
these in any form it wants, and they also do not have to closely match the phrasing of the original text.
In our experiments, we perform such verification planning by providing a few-shot prompt of
(response, verification) demonstrations to our LLM. See section 8 for the few-shot prompts we will
use in our experiments. We note it is also possible with a sufficiently performant instruction-following
LLM that this could be performed zero-shot.
3.3 E XECUTE VERIFICATIONS
Given the planned verification questions, the next step is to answer them in order to assess if any
hallucinations exist. While techniques such as retrieval-augmentation could be used in this process,
such as verification via search engine, in this work we do not explore tool-use. Instead, we consider
only using the LLM itself in all steps of CoVe, hence the model is used to check its own work. We
investigate several variants of verification execution, called joint, 2-Step, factored and factor+revise.
Joint In the joint method, the planning and execution (steps 2 and 3) are accomplished by using a
single LLM prompt, whereby the few-shot demonstrations include both verification questions and
their answers immediately after the questions. In this approach separate prompts are not needed.
2-Step A potential disadvantage of the joint method is that because the verification questions must
condition on the baseline response in the LLM context, and the method is joint, the verification
answers have to condition on the initial response as well. This may increase the likelihood of
repetition, another known issue of modern LLMs (Holtzman et al., 2019). This means the verification
questions might hallucinate similarly to the original baseline response, which defeats the purpose.
We hence instead separate the planning and execution into separate steps, both with their own LLM
prompt. The planning prompt conditions on the baseline response in the first step. The verification
questions generated from planning are answered in the second step, where crucially the context given
to the LLM prompt only contains the questions, and not the original baseline response and hence
cannot repeat those answers directly.
Factored Another, more sophisticated approach, is to answer all questions independently as separate
prompts. Again, crucially, those prompts do not contain the original baseline response and are hence
not prone to simply copying or repeating it. The factored approach has the further advantage of
removing any potential interference not only from the baseline response, but also between answer
contexts, and is somewhat related to the recent (concurrent) work of Radhakrishnan et al. (2023)
for subquestion answering by factored decomposition, hence we adopt their naming. It can also
potentially handle more verification questions by virtue of them not all having to fit with the same
single context. While this is potentially more computationally expensive, requiring the execution
of many more LLM prompts, they can be run in parallel, and hence be batched. In order to do
this, we first have to take the set of generated questions from subsection 3.2 and parse them into
separate questions, which is a relatively easy task as the few-shot demonstrations we provide indicate
they should be generated as a comma-separated list. We can then split them out into separate LLM
prompts.
Factor+Revise After answering the verification questions, the overall CoVe pipeline then has to
either implicitly or explicitly cross-check whether those answers indicate an inconsistency with the
original responses. In the factor+revise approach, we execute this as a deliberate step via an extra
LLM prompt, which may make it easier for the final system to reason about this step explicitly.
Differently to answering the verification questions, the cross-checking phase needs to condition
on both the baseline response and the verification question and answer. We thus execute this as
separate LLM prompts, one “cross-check” prompt for each question, with again a set of few-shot
4



### Claim 176/179

#### Claim Text
This technique has subsequently been refined for larger volume fractions .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[34]_2311.08377.pdf (Page 6):

posneg
Full Psg FilCo (ours) Silver
posneg
posneg
ELI5
posneg
FEVER
posneg
NQ HotpotQATQA
WoW
posneg
Figure 5: Improvement on examples retrieved with positive (top) and negative passages (bottom), respectively.
ing with our hypothesis, the generation model pro-
duces more correct outputs when we remove (i)
distracting content in positive passages, and (ii)
negative passages.
4.5 Evaluating Filtered Contexts
We evaluate context filtering outputs from two as-
pects: reduced input length and increased answer
precision.
Full Psg FilCo
Figure 6: Number of input tokens after filtering retrieved
contexts with different strategies.
Shorter Inputs In Figure 6, we measure the av-
erage number of tokens in model inputs after filter-
ing the retrieved contexts using different methods.
More specifically, we do not filter context in the
FULL setting, filter context by passage in the PSG
setting, and filter context in the sentence level with
FILCO. Model inputs contain the original query
and (filtered) context.7 Our method (the FILCO col-
umn) effectively reduces input length by 44 −64%.
Higher Precision To evaluate the amount of po-
tentially redundant information in the context, we
measure the unigram precision of outputs with re-
spect to filtered or unfiltered contexts.
7We tokenize all text sequences with the LlamaTokenizer.
As shown in Table 3, context after filtering
achieves much higher precision for all tasks. Par-
ticularly for abstractive tasks, SILVER filtering in-
creases the precision by +14.5 on HotpotQA and
+60.7 on WoW. Moreover, model-filtered contexts
(FILCO) are largely comparable to SILVER , and
sometimes even better, such as+3.8 points in TQA.
For other tasks, the small gaps between them mini-
mally affect the end generation, as already shown
in Figure 4. We conjecture these lost contents are
not essential for models, particularly if they only
involve common entities (Mallen et al., 2023).
However, filtering with the PSG baseline often
leads to precisions lower than the FULL setting,
despite the fact that it has higher output scores than
FULL . Coarse granularity for context filtering may
be one major reason for its loss in precision.
Method FULL PSG FIL CO SILVER
NQ 2.5 1.3 5.1 7.3
TQA 4.5 3.0 8.4 4.6
HOTPOT QA 2.6 2.6 10.8 17.1
ELI5 92.9 92.5 98.8 98.8
FEVER 1.2 1.2 5.1 4.4
WOW 10.8 35.5 62.9 71.5
Table 3: Precision of canonical outputs with respect to
contexts filtered with different methods.
5 Comparing Context Filtering Strategies
To justify the selection of context filtering strategies
in §4, we compare different measures to create filter
training data, as introduced in §2.
5.1 Results with Different Strategies
We compare using three methods in §2.2 —
STRINC, LEXICAL , and CXMI — to train the con-



Source: data\tc16_2312.10997v5\referenced_papers\[70]_2310.13243.pdf (Page 3):

re-rankers across all datasets, except for T5-QLM-
large, which is based on a smaller T5 model. This
outcome is expected since these methods benefit
from utilizing extensive human-judged QA training
data and the knowledge can be effectively trans-
ferred to the datasets we tested.
On the other hand, zero-shot QLMs and QG
fine-tuned QLMs exhibit competitive, similar ef-
fectiveness. This finding is somewhat surprising,
considering that QG fine-tuned QLMs are explic-
itly trained on QG tasks, making them a form of
transfer learning. This finding suggests that pre-
trained-only models such as LLaMA and Falcon
possess strong zero-shot QLM ranking capabilities.
Another interesting finding is that instruction
tuning can hinder LLMs’ QLM ranking ability if
the QG task is not included in the instruction fine-
tuning data. This is evident in the results of Alpaca-
7B, StableVicuna-13B, Falcon-7B-instruct and
Falcon-40B-instruct, which are instruction-tuned
versions of LLaMA-7B, LLaMA-13B, Falcon-7B
and Falcon-40B, respectively. Our hypothesis to
this unexpected finding is that instruction-tuned
models tend to pay more attention to the task in-
structions and less attention to the input content
itself. Although they are good at following instruc-
tions in the generation task, the most important
information for evaluating query likelihood is in
the document content, thus instruction-tuning hurts
query likelihood estimation for LLMs. On the other
hand, QG instruction-tuned LLMs show large im-
provements in QLM ranking. For example, the T0
and FlanT5 models are QG-tuned versions of T5
models, and they perform better. These results con-
firm that T0 and FlanT5 leverage their fine-tuning
data, thus should be considered within the transfer
learning setting.
In terms of model size, larger LLMs generally
tend to be more effective, although there are ex-
ceptions. For instance, LLaMA-7B outperforms
LLaMA-13B on DBpedia.
4.2 Interpolation with BM25
Table 2 demonstrates the impact of interpolating
with BM25 scores. Notably, we observe a large
decrease in the effectiveness of monoT5 re-rankers,
which are trained on large-scale QA domain data,
when interpolating with BM25. This finding aligns
with a study conducted by Yates et al. (2021). In
contrast, QLM re-rankers consistently exhibited
higher effectiveness across most datasets when us-
Table 2: Interpolation results. Increased/decreased
scores are noted with ↑ / ↓.
Methods TRECC DBpedia FiQA Robust04 Avg
without interpolationmonoT5-3B 79.8 44.8 46.0 56.2 56.7monoT5-3B-InPars-v2 83.8 46.6 46.1 58.5 58.8FlanT5-3B 72.0 37.0 41.7 47.0 49.4FlanT5-11B 75.1 39.9 44.9 50.8 52.7LLaMA-7B 68.0 37.5 41.8 51.6 49.7Falcon-7B 73.1 39.5 41.7 49.2 50.9
with interpolationmonoT5-3B 66.3 ↓ 44.6↓ 41.5↓ 55.1↓ 51.9↓
monoT5-3B-InPars-v2 82.1↓ 45.5↓ 43.5↓ 54.0↓ 56.3↓
FlanT5-3B 71.1 ↓ 39.7↑ 41.2↓ 50.0↑ 50.5↑
FlanT5-11B 74.9 ↓ 41.7↑ 43.3↓ 52.4↑ 53.1↑
LLaMA-7B 69.4 ↑ 39.9↑ 41.5↓ 53.6↑ 51.1↑
Falcon-7B 73.3 ↑ 41.7↑ 41.3↓ 52.5↑ 52.2↑
ing interpolation with BM25. It is worth noting
that this improvement is (almost) cost-free, as it
does not require any additional relevance score es-
timation; it simply involves linearly interpolating
with scores from the first stage.
We note that the results in Table 2 are obtained
by setting α = 0.2 without tuning this parameter
because we are testing our method in zero-shot
setting where this parameter needs to be set with-
out validation data. Nonetheless, we conduct a
post-hoc analysis on TRECC to understand the sen-
sitivity of this parameter. The results are presented
in Figure 1. From the results, we can draw the
following conclusions:
1. The interpolation strategy consistently has a
negative impact on monoT5-3B, while it con-
sistently benefits instruction-tuned and zero-
shot rerankers.
2. Instruction-tuned rerankers consistently un-
derperform their corresponding zero-shot
rerankers, regardless of the set alpha value.
3. Optimal values of α for both instruction-tuned
and zero-shot rerankers fall within the range
of 0.1 to 0.4.
4.3 Effective ranking pipeline
In Table 3 we push the boundary of our two-stage
QLM ranking pipeline in both zero-shot and few-
shot setting to obtain high ranking effectiveness.
For this purpose, we use the same linear interpola-
tion as Equation 2 with α = 0.5 to combine BM25
and HyDE as the zero-shot first-stage retriever. 5
The top 100 documents retrieved by this hybrid
retriever are then re-ranked using QLMs.
5This value was chosen to provide equal weight to the two
components, and no parameter exploration was undertaken.



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 6):

2 2.5 3 3.5 4 4.5 5 5.5
23.5
24
24.5
25
25.5
26
26.5
27
Latency (seconds)
Rouge-L
2%
(a) ELI5 - Base
0.6 0.8 1 1.2 1.4 1.6 1.8 2 2.2
20
21
22
23
24
25
Latency (seconds)
2% (b) MS MARCO- Base
1.5 2 2.5 3 3.5 4 4.5
24
25
26
27
28
29
30
31
32
Latency (seconds)
2% (c) NQ - Base
4 6 8 10
24
24.5
25
25.5
26
26.5
Latency (seconds)
Rouge-L
2%
(d) ELI5 - Large
2 3 4 5
21
21.5
22
22.5
23
23.5
24
24.5
25
Latency (seconds)
2% (e) MS MARCO- Large
4 6 8 10 12
26
27
28
29
30
31
32
F iD
CALM
T ok en Filtering
Combined
Latency (seconds)
2% (f) NQ - Large
Figure 5: ROUGE-L Performance results of the different methods on the test sets, plotted as smoothed Max Curves,
as a function of latency (seconds), for Base (top) and Large (bottom) models. Overall, our combined approach is
able to reach a better trade-off than the regular FiD model, for most cases.
values, due to skipping the redundant layer compu-
tations. In the case of Token Filtering, it is also able
to preserve and at times improve the performance
of the model overall, while the latency improve-
ment remains limited, since it is still computing the
remaining tokens across all decoder layers. The
performance improvement is presumably due to the
redundant tokens being removed early on during
the generation process, hence allowing the model
to better attend to the salient information in the
input.
When combining both methods, the performance
enhancement of the Token Filtering and the latency
reduction of CALM produce a better curve than
either method alone. In addition, we showcase
the drop in 2% performance per dataset, show-
ing that our method is able to reduce the latency
significantly more than the regular FiD, with the
best reduction reached on the MS MARCO dataset
for FiD-Base, saving 62.2% of the latency. In the
NQ dataset however, for both the base-sized and
large-sized models, while the CALM method does
achieve proper latency reduction, the Token Filter-
ing does not effect the results significantly. Since
we focus on real-world scenarios, we showcase
the trade-off with the actual latency, instead of
measurements such as FLOPS (MACs), as done by
previous works (de Jong et al., 2022). For those, we
refer to Figure 6 in the Appendix for the trade-off
and FLOPS (MACs) analysis. For the large-sized
models (Figures 5d, 5e, and 5f), we observe similar
patterns to those in the base-sized variations, with
the latency values being significantly larger. We
note that the overall performance values for these
models are not substantially different than those
produced by the smaller versions, hence we do not
focus as much on them.
6.2 Performance Comparison
To asses the performance of our Combined method
further, we choose the best performing hyperpa-
rameter setting for the FiD-Base model, and report
the test set results for each dataset, with Table 1
showing the results, compared to approaches sug-
gested in Su et al. (2022). In particular, we com-
pare to their implementation of FiD (named RBG
FID) and their suggested system (named RBG),
with the results of both taken from their published
work. We note that both our models and the RBG
models are of the same size. In our experiments,
we denote the original FiD model we trained as
FiD (ours), the FiD model with the Token Filter-
ing method as FiD TF, and the Combined method
as FiD Comb. On both datasets, FiD (ours), FiD
TF and FiD Comb achieve state-of-the-art results,
with Combined reaching the best overall perfor-
mance in terms of ROUGE-L and F1 (aside from
MS MARCO F1, where our approach is second).



Source: data\tc16_2312.10997v5\referenced_papers\[34]_2311.08377.pdf (Page 5):

NQ TQA HotpotQA
ELI5 FEVER WoW
Full Psg FilCo (ours) Silver
Figure 4: Generation performance when passages are filtered with different approaches.
generation in a full-context style, we fine-tune the
FLAN -T5 and LLAMA 2 models to generate out-
puts using the full content of the top-1 passages
under the same experiment setting as in §4.1.
Baseline 2: Passage-Wise Filtering An alter-
native method inspired by Asai et al. (2022) is to
filter context on a passage level. Specifically, for
each passage among the top-1 retrieved ones, the
model decides whether to include the entire piece
of the passage in the input. In comparison, our
method operates in a finer granularity (i.e., on the
sentence level) and could trained with multiple fil-
tering strategies. To show the empirical advantage
of our method, we denote this approach asPSG and
adopt it as another baseline.
Main Approach: Augmenting with Filtered Con-
text As described in §2, we trainMctx to filter the
top-1 retrieved passage P to tsilver, and Mgen to
generate output o with tsilver. To create tsilver, we
use the STRINC measure for NQ and TQA, LEXI -
CAL for FEVER, and CXMI for WoW, HotpotQA,
and ELI5. These measures are shown to be the
optimal settings based on further analysis in §5.
At test time, we provide model-filtered context
tpred to Mgen, and denote the results as FILCO.
To demonstrate the prospective performance upper-
bound, we also evaluate Mgen generation by pro-
viding silver-filtered context tsilver, and denote
these results as SILVER .
4.3 Generation Performance
Results using four methods and two models are
shown in Figure 4. In general, applying context
filtering beforehand significantly improves the re-
sults on all datasets than FULL . Moreover, filtering
in a finer granularity is better than PSG.
Compared to providing Mgen with SILVER fil-
tered contexts, using contents predicted by the filter
model, i.e., FILCO achieves comparable perfor-
mance on all six tasks, indicating effective training
of the context filtering process.
For extractive QA tasks , our method achieves
+4.3 and +8.6 EM increase in NQ with FLAN -T5
and LLAMA 2 models, +1.1 and +0.2 EM increase
in TQA. As exemplified by Figure 1, our context
filter effectively removes distracting alternative an-
swers and irrelevant passages, hence enabling the
generation model to hit the correct answer span
with higher precision and lower effort.
For more complex QA tasks, our method brings
+1.0 and +1.3 F1 increase in HotpotQA with
FLAN -T5 and LLAMA 2 models, and +0.6, +2.6
EM increase in ELI5. The overall improvement is
less significant, compared to extractive QA tasks,
presumably due to the increased task difficulty.
For abstractive generation tasks , our method
brings about even larger improvements: +6.2 and
+4.3 accuracy increase for FEVER with FLAN -
T5 and LLAMA 2, and +3.5, +1.1 F1 increase for
WoW. As could be partially conjectured from the
low precision in Table 2, filtering irrelevant content
helps the model focus on the concerned knowledge.
4.4 Generation With Filtered Positive and
Negative Passages
We decompose datasets into examples with positive
and negative top-1 retrieved passages, to examine
improvements under both scenarios.
As shown in Figure 5, for both positive and nega-
tive passages retrieved, applying FILCO effectively
improves the context quality, hence yields better
end generation results, particularly for abstractive
generation tasks such as FEVER and WoW. Align-



Source: data\tc16_2312.10997v5\referenced_papers\[27]_2310.01352.pdf (Page 3):

Published as a conference paper at ICLR 2024
Table 1: Our intruction tuning datasets. All datasets are downloaded from Hugging Face (Lhoest
et al., 2021), with the exception of those marked with ‡, which are taken from Iyer et al. (2022).
Task HF identifier Dataset name DL DR #Train
Dialogue oasst1 OpenAssistant Conversations Dataset (K ¨opf et al., 2023)✓ ✓ 31,598
Open-Domain
QA
commonsenseqa CommonsenseQA (Talmor et al., 2019) ✓ ✓ 9,741
mathqa MathQA (Amini et al., 2019) ✓ ✓ 29,837
webquestions Web Questions (Berant et al., 2013) ✓ ✓ 3,778
wikiqa Wiki Question Answering (Yang et al., 2015) ✓ ✓ 20,360
yahooanswersqa Yahoo! Answers QA ✓ ✓ 87,362
freebaseqa FreebaseQA (Jiang et al., 2019) ✓ 20,358
msmarco* MS MARCO (Nguyen et al., 2016) ✓ 80,143
Reading Com-
prehension
coqa Conversational Question Answering (Reddy et al., 2019) ✓ 108,647
drop Discrete Reasoning Over Paragraphs (Dua et al., 2019) ✓ 77,400
narrativeqa NarrativeQA (Ko ˇcisk´y et al., 2018) ✓ 32,747
newsqa NewsQA (Trischler et al., 2017) ✓ 74,160
pubmedqa PubMedQA (Jin et al., 2019) ✓ ✓ 1,000
quail QA for Artificial Intelligence (Rogers et al., 2020) ✓ 10,246
quarel QuaRel (Tafjord et al., 2019) ✓ ✓ 1,941
squadv2 SQuAD v2 (Rajpurkar et al., 2018) ✓ 130,319
Summarization cnndailymail CNN / DailyMail (Hermann et al., 2015) ✓ 287,113
Chain-of-
thought
Reasoning
aquarat‡ Algebra QA with Rationales (Ling et al., 2017) ✓ 97,467
ecqa‡ Explanations for CommonsenseQ (Aggarwal et al., 2021)✓ 7,598
gsm8k‡ Grade School Math 8K (Cobbe et al., 2021) ✓ 7,473
compeitionmath‡ MATH (Hendrycks et al., 2021b) ✓ 7,500
strategyqa‡ StrategyQA (Geva et al., 2021) ✓ 2,290
* We only used the question-and-answer pairs in the MS MARCO dataset.
DL, we retrieve the top- ˜k relevant text chunks Ci ⊂ Cbased on xi. Mirroring the inference-time
handling, for each retrieved chunk cij ∈ Ci, we create a separate fine-tuning example by prepending
it to the instructions as a background field, resulting in ˜k independent fine-tuning instances per
original example: {(cij ◦ xi, yi)|j = 1. . .˜k}.4
We fine-tune the language model using the next-token prediction objective and minimize the loss
from tokens in the output segment of each instance (Iyer et al., 2022):
L(DL) =−
X
i
X
j
log pLM (yi|cij ◦ xi). (3)
Integrating in-context retrieval augmentation during fine-tuning gives a twofold benefit. First, it
adapts the LLM to better utilize relevant background knowledge to make a prediction. Secondly,
even state-of-the-art retrievers can falter and return inaccurate results. By training the LLM to make
correct predictions when a wrong retrieved chunk is given, we enable the LLM to ignore misleading
retrieval content and lean into its parametric knowledge in such cases. The efficacy of this fine-
tuning strategy is empirically demonstrated in §5.1.
2.4 R ETRIEVER FINE -TUNING
In addition to fine-tuning the language model with retrieval augmentation, we also fine-tune the
retriever to better align its output with the language model. In particular, we adopt a generalized
version of LSR ( LM-Supervised Retrieval, Shi et al., 2023b) training that leverages the language
model itself to provide supervision for retriever fine-tuning.
For a training sample (x, y) in the retriever fine-tuning dataset DR, we define the LSR score for a
retrieved chunk c as follows:
pLSR(c|x, y) = exp (pLM (y|c ◦ x)/τ)P
c′∈C exp (pLM (y|c′ ◦ x)/τ) ≈ exp (pLM (y|c ◦ x)/τ)P
c′∈C′ exp (pLM (y|c′ ◦ x)/τ), (4)
where τ is a temperature hyperparameter, and C′ ⊂ Cdenotes the top-k retrieved chunks for x. A
higher LSR score indicates that c is more effective at improving the language model’s chance of
4The exceptions are summarization tasks and RC tasks with context dependent questions (e.g. “when was
the writer born?”), where we do not perform retrieval and create the fine-tuning instances using the given
background text instead. For RC tasks with self-contained questions, we use the retrieved chunks in addition to
the given background text to create fine-tuning instances, resulting in ˜k + 1of them per original example.
4



### Claim 177/179

#### Claim Text
The higher-order organizational structures manifested in networks based on specific social, age, or occupational relationships can be effectively captured using the motifs .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 10):

Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT
more different graph models that can be used for accomplishing
their own graph reasoning tasks.
Protein Molecule Function Reasoning : Protein and chemical
molecule function inference [52] has been a classic problem studied
in bio-chemical research for decades, which has fundamental appli-
cations in the real-world, such as helping design some new drugs
for curing some existing rare diseases. Protein function inference is
not an easy task, because homologous proteins often have several
different functions at the same time. Also such a prediction needs
to be fine-tuned with respect to some mutations but robust with
respect to others. Researchers have been exploring on this problem
with machine learning models, and have also developed a relatively
large protein function database [48] already. However, compared
with the number of protein existing in the real world, the specific
proteins with known functions included in the database is still
very limited. In graph learning, inferring the function of protein
molecules based on its structure has also be extensively studied as
well. Therefore, in this part, we also include it as a graph reasoning
task into Graph-ToolFormer as well.
Different from the bibliographic network, the protein molecular
graphs have much smaller sizes and there will also exist multiple
such graph instances in the dataset. What’s more, the features and
labels of protein molecular graphs are both about the whole molecu-
lar graph, not about the individual nodes anymore. As introduced
in Section 3.2, we can represent the set of studied protein molec-
ular graphs as G= {𝑔1,𝑔2,··· ,𝑔𝑙}, which can be loaded with the
following graph loading API call:
<API>𝐺𝐿(“𝑝𝑟𝑜𝑡𝑒𝑖𝑛-𝑔𝑟𝑎𝑝ℎ-𝑠𝑒𝑡”)→G </API>. (29)
For each molecular graph instance 𝑔𝑖 = (V𝑔𝑖 ,E𝑔𝑖 )in the dataset G,
there will also be raw features and labels related to each protein
molecular graph instance. For instance, for the graph instance 𝑔𝑖 ∈
G, we can represent its raw feature asx𝑔𝑖 and its label as y𝑔𝑖 , where
the label vector will indicate its corresponding functions. Based
on the protein graph structure and its raw features, we can define
the following API call for protein molecule function reasoning as
follows:
<API>𝐺𝑅(G,“𝑠𝑒𝑔-𝑏𝑒𝑟𝑡:𝑚𝑜𝑙𝑒𝑐𝑢𝑙𝑒-𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛 ”,𝑔𝑖)→ 𝑟</API>, (30)
which will call the pre-trained graph neural network SEG-Bert
proposed in [58]. The SEG-Bert with full name “Segmented Graph-
Bert” [58] extends the Graph-Bert model for molecular graph in-
stance representation learning. Besides the SEG-Bert model used
in Graph-ToolFormer, the readers can also customize the Graph-
ToolFormer framework to include other graph models for address-
ing the molecular graph reasoning tasks as well.
Sequential Recommender System Reasoning : In the era of big
data, as more and more data are generated both online and offline,
manual search of information from such big data sources has be-
come infeasible nowadays and we may need recommender systems
[28] to automatically recommend desired information for us instead.
Based on the historical records, sequential recommender system
aims to infer the next item(s) that users may be interested in, which
may lead to either the future purchase action or the review rating
scores of those items. When studying the sequential recommender
systems, it is a common way to model recommender systems as
the bipartite graphs, where the user-item interaction record also
has an attached timestamp. With considerations about the times-
tamps, sequential recommender systems aim to infer the potential
existence (or the weight) of links between user and their interested
items for the next future timestamp. In other words, we can define
the sequential recommendation problem in recommender systems
as a link prediction task with considerations about the temporal
factor.
Formally, according to the above description, we can repre-
sent the sequential recommender system as a bipartite graph
𝐺 = (V,E), where the node set V= U∪I covers both users and
items and the links in set E⊂M×I only exist between users and
item instead. For each user-item pair (𝑢𝑗,𝑖𝑙)∈E in the link set, we
can also obtain its timestamp. The sequential recommender system
data can be loaded with the following API call:
<API>𝐺𝐿(“𝑟𝑒𝑐𝑜𝑚𝑚𝑒𝑛𝑑𝑒𝑟-𝑠𝑦𝑠𝑡𝑒𝑚”)→ 𝐺</API>. (31)
For each user 𝑢𝑗 and item 𝑖𝑙 in the recommender system 𝐺, based
on the historical interaction records (before the current timestamp),
we can learn the embedding representations of them, which will be
used to infer the label between them in the future. Depending on
the modeling approach, the label vector can indicate either whether
the user will purchase the item or not ( i.e., binary classification
task) or the rating score of the user for the item (i.e., the regression
task). Regardless of the specific modeling settings, we can represent
the recommender system reasoning API call in LLMs as follows:
<API>𝐺𝑅(𝐺,“𝑏𝑝𝑟:𝑟𝑒𝑐𝑜𝑚𝑚𝑒𝑛𝑑𝑎𝑡𝑖𝑜𝑛”,𝑢𝑗,𝑖𝑙)→ 𝑟</API>, (32)
which will return either the probability scores that the user 𝑢𝑗 will
be interested in the item 𝑖𝑙 or the specific rating scores that 𝑢𝑗
will give to 𝑖𝑙. We use BPR (Bayesian Personalized Ranking) [42]
as the default recommendation model in Graph-ToolFormer in
this paper, but other recommendation models can also be used for
defining the above recommendation API calls as well. Besides the
recommendation API calls to infer the scores between user and
item, for one specific user 𝑢𝑗, we can also return the list of top-𝑘
recommended items with the following API call:
<API>𝐺𝑅(𝐺,“𝑏𝑝𝑟:𝑡𝑜𝑝𝑘-𝑟𝑒𝑐𝑜𝑚𝑚𝑒𝑛𝑑𝑎𝑡𝑖𝑜𝑛”,𝑢𝑗,𝑘)→ 𝑟</API>,
(33)
where the notation 𝑘 denotes a hyper-parameter to be extracted
from the input statements for the recommendation reasoning.
Online Social Network Community Reasoning : Online social
networks [31], like Facebook, Twitter and Tiktok, provide different
online services for their users to facilitate their online socialization
with friends, family members and colleagues. Users in online social
networks tend to interact more frequently with their online friends,
and they will naturally form their online social communities based
on their online social behaviors. Reasoning for the social commu-
nities of users in online social networks is a complicated problem.
In this part, we will introduce the API calls to empower LLMs to
detect social communities from online social networks.
Formally, we can represent the online social network studied
in this paper as 𝐺 = (V,E), where Vdenotes the set of user
nodes and Edenotes the social interactions among the users in the



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 25):

Jiawei Zhang
Table 6: Analysis of generated API query cases different from ground-truth queries in the testing set. In this table, we illustrate
the examples of input statements, generated statements by Graph-ToolFormer and the desired ground-truth statements of
different graph reasoning tasks studied in this paper.
Tasks Failed case examples
Input Statements Generated Statements Ground-Truth Statements
Graph
Property
Reasoning
Nodes have the largest eccen-
tricity [TBR] in the lollipop
graph, which make them part
of its periphery.
root>’s eccentricity is [GR(GL("gpr", "lollipop_graph"),
"toolx:eccentricity", <root>)–>r].root>’s eccentricity is
[GR(GL("gpr", "lollipop_graph"), "toolx:eccentricity", <root>)–
>r1].root>’s eccentricity is [GR(GL("gpr
Nodes [GR(GL("gpr", "lollipop_graph"), "toolx:periphery")–>r]
have the largest eccentricity [GR(GL("gpr", "lollipop_graph"),
"toolx:eccentricity")] in the lollipop graph, which make them
part of its periphery.
The nodes with the smallest ec-
centricity in the house x graph
are [TBR], which are also the
center of the tree.
center>the nodes with the smallest eccentricity in the
house x graph are [GR(GL("gpr", "house_x_graph"),
"toolx:eccentricity")–>r], which are also the [GR(GL("gpr",
"house_x_graph"), "toolx:center")–>r].
The nodes with the smallest eccentricity [GR(GL("gpr",
"house_x_graph"), "toolx:eccentricity")] in the house x graph
are [GR(GL("gpr", "house_x_graph"), "toolx:center")–>r], which
are also the center of the tree.
What is the length of shortest
path between node #3 and node
#2 in the diamond graph?
The length of shortest path between node #3 and node #2
in the diamond graph is [GR(GL("gpr", "diamond_graph"),
"toolx:shortest_path", "node#3", "node#2")–>r].
In the diamond graph, the length of shortest path between
node #3 and node #2 is [GR(GL("gpr", "diamond_graph"),
"toolx:shortest_path", "node#3", "node#2")–>r].
Bibliographic
Network
Reasoning
The cora bibliographic network’
paper #28487 is concerned with
the area of [TBR].
ROOT>’s cora bibliographic network’ paper #28487 is con-
cerned with the area of [GR(GL("cora"), "graph_bert:topic",
paper#28487)–>r].
The cora bibliographic network’ paper #28487 is concerned with
the area of [GR(GL("cora"), "graph_bert:topic", paper#28487)–
>r].
What is the topic of paper #5832
in the pubmed bibliographic
network?
The topic of paper #paper_number> in the pubmed bibli-
ographic network is [GR(GL("pubmed"), "graph_bert:topic",
paper#paper_number>)–>r].
The topic of paper #5832 in the pubmed bibliographic network
is [GR(GL("pubmed"), "graph_bert:topic", paper#5832)–>r].
Molecular
Graph
Reasoning
What is the function for the pro-
tein molecular graph #573 in
proteins?
The function of the protein molecular graph #573 in
proteins is [GR(GL("proteins"), "seg_bert:molecule_function",
instance#573)–>r].
The function for the protein molecular graph #573 in
proteins is [GR(GL("proteins"), "seg_bert:molecule_function",
instance#573)–>r].
Recommender
System
Reasoning
In Amazon, what is the item
that user #A3C08BZRVV500V
will be most likely to purchase
next?
In Amazon, what is the item that user #A3C08BZRVV500V
will be most likely to purchase next is [GR(GL("amazon"),
"bpr:topk_recommendation", user#A3C08BZRVV500V, 1)–>r].
In Amazon, the item that user #A3C08BZRVV500V will
be most likely to purchase next is [GR(GL("amazon"),
"bpr:topk_recommendation", user#A3C08BZRVV500V, 1)–>r].
In Movielens, which movie user
#u273 will be most likely to
watch next?
In Movielens, which movie user #u273 will be most likely to
watch next?
In Movielens, the movie that user #u273 will be
most likely to watch next is [GR(GL("movielens"),
"bpr:topk_recommendation", user#u273, 1)–>r].
In Last-fm, which artisit user
#u1156 will be most likely to lis-
ten to next?
artisit> artisit> artisit> artisit> artisit> artisit> artisit> artisit>
artisit> artisit> artisit> artisit> artisit> artisit>
In Last-fm, the artisit that user #u1156 will be most likely to
listen to next is [GR(GL("last-fm"), "bpr:topk_recommendation",
user#u1156, 1)–>r].
Social
Network
Reasoning
In the online social network
foursquare, are user #victorcar-
bonero and user #user/11979222
belong to the same community?
In the online social network foursquare, user #user/vic-
torcarbonero and user #user/11979222 belong to
[GR(GL("foursquare"), "kmeans:common_community_check",
user#user/victorcarbonero, user#user/11979222)–>r] commu-
nity.
In the online social network foursquare, user #victorcarbonero
and user #user/11979222 belong to [GR(GL("foursquare"),
"kmeans:common_community_check", user#victorcarbonero,
user#user/11979222)–>r] community.
In the online social network
twitter, are user #iancr and user
#ClassyIndeed belong to the
same community?
iancr and user #ClassyIndeed belong to [GR(GL("twitter"),
"kmeans:common_community_check", user#iancr,
user#ClassyIndeed)–>r] community.
In the online social network twitter, user #iancr
and user #ClassyIndeed belong to [GR(GL("twitter"),
"kmeans:common_community_check", user#iancr,
user#ClassyIndeed)–>r] community.
Knowledge
Graph
Reasoning
According to the WordNet
knowledge graph, via relation
#_derivationally_related_form,
we can obtain entity #scare-
monger.n.01 from entity
[TBR].
According to the WordNet knowledge graph, via rela-
tion #_derivationally_related_form, we can obtain en-
tity #scaremonger.n.01 from entity [GR(GL("wordnet"),
"transe:head_entity", relation#_derivationally_related_form,
entity#scaremonger.n.01)–>r].
According to the Freebase
knowledge graph, from en-
tity#/m/03r8tl, via relation
#/award/award_category
/nominees./award/award _nom-
ination/award_nominee, what
entity can we derive?
According to the Freebase knowledge
graph, from entity#/m/03r8tl, via relation
#/award/award_category/nominees./award/award_nomination
/award_nominee, we can derive entity [GR(GL("freebase"), "tr
According to the Freebase knowledge
graph, from entity#/m/03r8tl, via relation
#/award/award_category/nominees./award/award_nomination
/award_nominee, we can derive entity [GR(GL("freebase"),
"transe:tail_entity", entity#/m/03r8tl, rela-
tion#/award/award_category/nominees./award
/award_nomination/award_nominee)–>r].



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 31):

Jiawei Zhang
[43] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,
Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language
models can teach themselves to use tools. ArXiv, abs/2302.04761, 2023.
[44] Timo Schick and Hinrich Schütze. Exploiting cloze-questions for few-shot text
classification and natural language inference. In Conference of the European
Chapter of the Association for Computational Linguistics , 2020.
[45] Chuan Shi, Yitong Li, Jiawei Zhang, Yizhou Sun, and Philip S. Yu. A survey of
heterogeneous information network analysis. IEEE Transactions on Knowledge
and Data Engineering , 29:17–37, 2015.
[46] Ke Sun, Zhouchen Lin, and Zhanxing Zhu. Adagcn: Adaboosting graph convolu-
tional networks into deep models, 2019.
[47] Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S. Yu, and Tianyi Wu. Pathsim: Meta
path-based top-k similarity search in heterogeneous information networks. Proc.
VLDB Endow., 4(11):992–1003, aug 2011.
[48] Damian Szklarczyk, Annika L Gable, Katerina C Nastou, David Lyon, Rebecca
Kirsch, Sampo Pyysalo, Nadezhda T Doncheva, Marc Legeay, Tao Fang, Peer
Bork, Lars J Jensen, and Christian von Mering. The STRING database in 2021:
customizable protein–protein networks, and functional characterization of user-
uploaded gene/measurement sets. Nucleic Acids Research , 49(D1):D605–D612, 11
2020.
[49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lam-
ple. Llama: Open and efficient foundation language models.ArXiv, abs/2302.13971,
2023.
[50] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.
ArXiv, abs/1706.03762, 2017.
[51] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Liò, and Yoshua Bengio. Graph Attention Networks. International Conference on
Learning Representations , 2018.
[52] Saraswathi Vishveshwara, K. V. Brinda, and N. Kannan. Protein structure: In-
sights from graph theory. Journal of Theoretical and Computational Chemistry ,
01(01):187–211, 2002.
[53] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregres-
sive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May
2021.
[54] Albert Webson and Ellie Pavlick. Do prompt-based models really understand the
meaning of their prompts? ArXiv, abs/2109.01247, 2021.
[55] Pinar Yanardag and S.V.N. Vishwanathan. Deep graph kernels. In Proceedings
of the 21th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining , KDD ’15, page 1365–1374, New York, NY, USA, 2015. Association
for Computing Machinery.
[56] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim.
Graph transformer networks. In Neural Information Processing Systems , 2019.
[57] Jiawei Zhang. Graph neural networks for small graph and giant network repre-
sentation learning: An overview. ArXiv, abs/1908.00187, 2019.
[58] Jiawei Zhang. Segmented graph-bert for graph instance modeling. ArXiv,
abs/2002.03283, 2020.
[59] Jiawei Zhang and Lin Meng. Gresnet: Graph residual network for reviving deep
gnns from suspended animation. ArXiv, abs/1909.05729, 2019.
[60] Jiawei Zhang, Haopeng Zhang, Li Sun, and Congying Xia. Graph-bert: Only
attention is needed for learning graph representations. ArXiv, abs/2001.05140,
2020.
A APPENDIX: GRAPH DATA FORMAT
A.1 Graph Data Loading
The datasets are stored in binary format, which can be loaded with
pickle, e.g., the GPR dataset can be loaded as follows:
1 import pickle
2 f = open ( './ gpr', 'rb ')
3 dataset = pickle . load (f)
4 f. close ()
5 print ( dataset . keys ())
A.2 Graph Dataset
For the datasets with 1 single large-scale graph/network (including,
Cora, Pubmed, Citeseer; Twitter, Foursquare; Amazon, Last-FM,
Movielens; WordNet, Freebase), the loaded "dataset" is organized
with a python dictionary with the following format:
1 dataset = {
2 " data_profile ": {
3 'name ': dataset_name ,
4 'order ': node_number ,
5 'size ': link_number ,
6 ' is_directed ': boolean ,
7 ' is_weighted ': boolean ,
8 # besides the above profile information , for
some data , we will also include some other
attributes , like feature vector dimensions , label
space dimension , etc ., in the data profile dict .
9 },
10 " nodes ": {
11 node_id : { 'features ': feature , 'label ': label ,}
12 },
13 " links ": {
14 node_pair : { 'features ': feature , 'label ': label
,}
15 }
16 }
A.3 Graph Instance Dataset
For the datasets with multiple graph instances (GPR; Proteins, Mu-
tag, NCI1, PTC), the loaded "dataset" is organized with a python
dictionary with the following format:
1 dataset = {
2 " data_profile ": {
3 'name ': dataset_name ,
4 ' graph_number ': graph_number ,
5 ' is_directed ': boolean ,
6 ' is_weighted ': boolean ,
7 # besides the above profile information , for
some data , we will also include some other
attributes , like feature vector dimensions , label
space dimension , etc ., in the data profile dict .
8 },
9 'graph_set ': {
10 graph_id : {
11 'nodes ': node_set ,
12 'links ': link_set ,
13 'label ': graph_instance_label ,
14 }
15 }
16 }



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 16):

Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT
• Output: The topic of paper #3230 in
the citeseer bibliographic network is
[GR(GL("citeseer"), "graph_bert:topic",
paper#3230)–>r].
3. Molecular Graph Reasoning Q&A Prompt Examples:
• Input: What is the function for the pro-
tein molecular graph #138 in proteins?
• Output: The function for the
protein molecular graph #138 in
proteins is [GR(GL("proteins"),
"seg_bert:molecule_function", instance#138)–
>r].
• Input: What is the function for the chemi-
cal molecular graph #129 in mutag?
• Output: The function for the
chemical molecular graph #129
in mutag is [GR(GL("mutag"),
"seg_bert:molecule_function", instance#129)–
>r].
• Input: What is the function for the chemi-
cal molecular graph #322 in nci1?
• Output: The function for the chem-
ical molecular graph #322 in nci1 is
[GR(GL("nci1"), "seg_bert:molecule_function",
instance#322)–>r].
• Input: What is the function for the chemi-
cal molecular graph #44 in ptc?
• Output: The function for the chemical
molecular graph #44 in ptc is [GR(GL("ptc"),
"seg_bert:molecule_function", instance#44)–
>r].
4. Social Network Reasoning Q&A Prompt Examples:
• Input: In foursquare, what is the id of
user sparkey215’s community?
• Output: In foursquare, the id
of user sparkey215’s community is
[GR(GL("foursquare"), "kmeans:community",
user#sparkey215)–>r].
• Input: In the online social network
foursquare, are user #user/9674821 and user
#ljaniszewski8 belong to the same community?
• Output: In the online social net-
work foursquare, user #user/9674821
and user #ljaniszewski8 be-
long to [GR(GL("foursquare"),
"kmeans:common_community_check",
user#user/9674821, user#ljaniszewski8)–>r]
community.
5. Recommender System Reasoning Q&A Prompt Examples:
• Input: How likely user #A23E9QQHJLNGUI
will be interested in item #B004PIPG2A in
Amazon?
• Output: The likelihood that user
#A23E9QQHJLNGUI will be interested in item
#B004PIPG2A in Amazon is [GR(GL("amazon"),
"bpr:recommendation", user#A23E9QQHJLNGUI,
item#B004PIPG2A)–>r].
• Input: How likely user #u329 will be inter-
ested in music of artist #i8323 in Last-fm?
• Output: The likelihood that user
#u329 will be interested in music from
artist #i8323 in Last-fm is [GR(GL("last-
fm"), "bpr:recommendation", user#u329,
artist#i8323)–>r].
• Input: How likely user #u650 will be inter-
ested in movie #i671 in Movielens?
• Output: The likelihood that user
#u650 will be interested in movie #i671
in Movielens is [GR(GL("movielens"),
"bpr:recommendation", user#u650,
movie#i671)–>r].
6. Knowledge Graph Reasoning Q&A Prompt Examples:
• Input: According to the Freebase knowl-
edge graph, what is the relation between
entity#/m/053yx and entity#/m/015_1q?
• Output: According to the Freebase knowl-
edge graph, the relation between en-
tity#/m/053yx and entity#/m/015_1q is
[GR(GL("freebase"), "transe:relation", en-
tity#/m/053yx, entity#/m/015_1q)–>r].
• Input: According to the WordNet knowledge
graph, via relation #_hypernym, we derive
entity #imagination.n.02 from what entity?
• Output: According to the WordNet knowledge
graph, via relation #_hypernym, we can ob-
tain entity #imagination.n.02 from entity
[GR(GL("wordnet"), "transe:head_entity", re-
lation#_hypernym, entity#imagination.n.02)–
>r].



Source: data\tc16_2312.10997v5\referenced_papers\[109]_2304.11116.pdf (Page 24):

Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT
Table 5: Case analysis of graph reasoning API call statement generation, query parsing, and query execution and final output
generation in Graph-ToolFormer.
Tasks Graph Reasoning Examples withGraph-ToolFormer
Input Statements Generated Statements with API callsQuery parsing Reasoning Final Statement Output
Graph
Property
Reasoning
What is the order of the dia-
mond graph?
The order of the diamond graph is [GR(GL("gpr",
{"diamond_graph"}), "toolx:order")–>r].
((’GR’, [(’GL’, [’gpr’, ’{"dia-
mond_graph"}’]), ’toolx:order’]),
[True])
4 The order of the diamond graph
is 4.
The path graph has a center at
nodes [TBR].
The path graph has a center at nodes
[GR(GL("gpr", {"path_graph"}), "toolx:center")–
>r].
((’GR’, [(’GL’, [’gpr’,
’{"path_graph"}’]), ’toolx:center’]),
[True])
[5, 6] The path graph has a center at
nodes [5, 6].
The nodes in the wheel graph
have eccentricity values of
[TBR].
The nodes in the wheel graph have eccentric-
ity values of [GR(GL("gpr", {"wheel_graph"}),
"toolx:eccentricity")–>r].
((’GR’, [(’GL’, [’gpr’,
’{"wheel_graph"}’]),
’toolx:eccentricity’]), [True])
{0: 1, 1: 2, 2: 2, 3: 2,
4: 2, 5: 2}
The nodes in the wheel graph
have eccentricity values of {0: 1,
1: 2, 2: 2, 3: 2, 4: 2, 5: 2}.
Bibliographic
Network
Reasoning
What is the topic of paper
#83826 in the cora biblio-
graphic network?
The topic of paper #83826 in the cora
bibliographic network is [GR(GL("cora"),
"graph_bert:topic", paper#83826)–>r].
((’GR’, [(’GL’, [’cora’]),
’graph_bert:topic’, ’paper#83826’]),
[True])
Neural_NetworksThe topic of paper #83826 in
the cora bibliographic network
is Neural_Networks.
Paper #2998, situated in the
citeseer bibliographic network,
is centered around the [TBR]
topic.
Paper #2998, situated in the citeseer bib-
liographic network, is centered around
the [GR(GL("citeseer"), "graph_bert:topic",
paper#2998)–>r] topic.
((’GR’, [(’GL’, [’citeseer’]),
’graph_bert:topic’, ’paper#2998’]),
[True])
AI Paper #2998, situated in the cite-
seer bibliographic network, is
centered around the AI topic.
Within the pubmed biblio-
graphic network, paper #16466
focuses on the subject of [TBR].
Within the pubmed bibliographic net-
work, paper #16466 focuses on the subject
of [GR(GL("pubmed"), "graph_bert:topic",
paper#16466)–>r].
((’GR’, [(’GL’, [’pubmed’]),
’graph_bert:topic’, ’paper#16466’]),
[True])
1 Within the pubmed biblio-
graphic network, paper #16466
focuses on the subject of 1.
Molecular
Graph
Reasoning
What is the function for the
protein molecular graph #138
in proteins?
The function for the protein molecular
graph #138 in proteins is [GR(GL("proteins"),
"seg_bert:molecule_function", instance#138)–>r].
((’GR’, [(’GL’, [’proteins’]),
’seg_bert:molecule_function’,
’instance#138’]), [True])
0 The function for the protein
molecular graph #138 in proteins
is 0.
In mutag, instance #30 of
the chemical molecular graph
demonstrates a function of
[TBR].
In mutag, instance #30 of the chemical
molecular graph demonstrates a function of
[GR(GL("mutag"), "seg_bert:molecule_function",
instance#30)–>r].
((’GR’, [(’GL’, [’mutag’]),
’seg_bert:molecule_function’,
’instance#30’]), [True])
2 In mutag, instance #30 of the
chemical molecular graph
demonstrates a function of 2.
For chemical molecular graph
instance #652 in nci1, its func-
tion is [TBR].
For chemical molecular graph instance
#652 in nci1, its function is [GR(GL("nci1"),
"seg_bert:molecule_function", instance#652)–>r].
((’GR’, [(’GL’, [’nci1’]),
’seg_bert:molecule_function’,
’instance#652’]), [True])
0 For chemical molecular graph in-
stance #652 in nci1, its function
is 0.
The chemical molecular graph
numbered 239 in ptc is charac-
terized by a function of [TBR].
The chemical molecular graph numbered 239 in
ptc is characterized by a function of [GR(GL("ptc"),
"seg_bert:molecule_function", instance#239)–>r].
((’GR’, [(’GL’, [’ptc’]),
’seg_bert:molecule_function’,
’instance#239’]), [True])
0 The chemical molecular graph
numbered 239 in ptc is charac-
terized by a function of 0.
Recommender
System
Reasoning
How likely user
#A1HOLE9R6WPT85
will be interested in item
#B00005MOTF in Amazon?
The likelihood that user #A1HOLE9R6WPT85
will be interested in item #B00005MOTF in Ama-
zon is [GR(GL("amazon"), "bpr:recommendation",
user#A1HOLE9R6WPT85, item#B00005MOTF)–
>r].
((’GR’, [(’GL’, [’amazon’]),
’bpr:recommendation’,
’user#A1HOLE9R6WPT85’,
’item#B00005MOTF’]), [True])
0.008 The likelihood that user
#A1HOLE9R6WPT85 will be in-
terested in item #B00005MOTF
in Amazon is 0.008.
The likelihood that user #u1527
will be interested in music
from artisit #i5422 in Last-fm
is [TBR].
The likelihood that user #u1527 will be in-
terested in music from artisit #i5422 in Last-
fm is [GR(GL("last-fm"), "bpr:recommendation",
user#u1527, artisit#i5422)–>r].
((’GR’, [(’GL’, [’last-fm’]),
’bpr:recommendation’,
’user#u1527’, ’artisit#i5422’]),
[True])
0.248 The likelihood that user #u1527
will be interested in music from
artisit #i5422 in Last-fm is 0.248.
In Movielens, the top 10 movies
that user #u272 likes include
[TBR].
In Movielens, the top 10 movies that user
#u272 likes include [GR(GL("movielens"),
"bpr:topk_recommendation", user#u272, 10)–>r].
((’GR’, [(’GL’, [’movielens’]),
’bpr:topk_recommendation’,
’user#u272’, ’10’]), [True])
[’i286’, ’i288’,
’i258’, ’i294’, ’i300’,
’i50’, ’i313’, ’i100’,
’i181’, ’i269’]
In Movielens, the top 10 movies
that user #u272 likes include
[’i286’, ’i288’, ’i258’, ’i294’, ’i300’,
’i50’, ’i313’, ’i100’, ’i181’, ’i269’].
Social
Network
Reasoning
In the online social network
foursquare, which community
is user user/1265481 involved
in?
In the online social network foursquare,
user user/1265481 is involved in the
[GR(GL("foursquare"), "kmeans:community",
user#user/1265481)–>r] communities formed by
users.
(’GR’, [(’GL’, [’foursquare’]),
’kmeans:community’,
’user#user/1265481’])
#2 In the online social network
foursquare, user user/1265481 is
involved in the #2 communities
formed by users.
In the online social network
twitter, are user #deeprogress
and user #alejandro1254 be-
long to the same community?
In the online social network twitter,
user #deeprogress and user #alejan-
dro1254 belong to [GR(GL("twitter"),
"kmeans:common_community_check",
user#deeprogress, user#alejandro1254)–>r]
community.
(’GR’, [(’GL’, [’twitter’]),
’kmeans:common_community_check’,
’user#deeprogress’,
’user#alejandro1254’])
The Same In the online social network twit-
ter, user #deeprogress and user
#alejandro1254 belong to the
same community.
Knowledge
Graph
Reasoning
According to the Freebase
knowledge graph, what
is the relation between
entity#/m/053yx and en-
tity#/m/015_1q?
According to the Freebase knowl-
edge graph, the relation between en-
tity#/m/053yx and entity#/m/015_1q is
[GR(GL("freebase"), "transe:relation", en-
tity#/m/053yx, entity#/m/015_1q)–>r].
(’GR’, [(’GL’, [’freebase’]),
’transe:relation’, ’entity#/m/053yx’,
’entity#/m/015_1q’])
/music/artist/labelAccording to the Freebase
knowledge graph, the relation
between entity#/m/053yx
and entity#/m/015_1q is
/music/artist/label.
According to the WordNet
knowledge graph, via relation
#_hypernym, what entity can
we obtain from entity #imagi-
nation.n.02?
According to the WordNet knowledge graph, via
relation #_hypernym, we can obtain entity #imag-
ination.n.02 from entity [GR(GL("wordnet"),
"transe:head_entity", relation#_hypernym,
entity#imagination.n.02)–>r].
(’GR’, [(’GL’, [’wordnet’]),
’transe:head_entity’, ’re-
lation#_hypernym’, ’en-
tity#imagination.n.02’])
chimera.n.02 According to the WordNet
knowledge graph, via relation
#_hypernym, we can obtain
entity #imagination.n.02 from
entity chimera.n.02.



### Claim 178/179

#### Claim Text
The method to implement the boundary condition for a fixed solid wall in DUGKS is very similar to that in LBM, except that it is done at the cell interface nodes .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[91]_2308.11730.pdf (Page 2):

Figure 3: Knowledge Graph Construction. We split each document in the document collection into passages. For each passage,
we either directly obtain their embeddings via pre-trained encoders or extract their keywords to build bag-of-word (BOW)
features. Then we connect two passages based on their embedding similarity or whether they share common keywords. Addi-
tionally, we extract tables/pages via Extract-PDF API and add them as structural nodes to the KG. If pages include passages
and tables, we add a directed edge to denote the belonging relations. The table nodes include the markdown formatted content
of that table as Figure 8 in Supplementary has empirically shown that LLMs are able to understand tables in this format.
3 Knowledge Graph Construction
Despite numerous well-established KGs (Hoffart et al. 2013;
Tian et al. 2023b), they treat nodes/edges as entities/rela-
tions, which necessitates sophisticated relational extraction
techniques and thereby limits their applicability in general
domains (Huang et al. 2021). Additionally, their primary fo-
cus on the Wikipedia domain also restricts their usage for
answering non-Wikipedia questions such as ones over legal
or financial documents. To remedy this issue, we propose
generally applicable KG construction methods.
We first analyze two representative questions in Fig-
ure 2(a)-(b) to motivate our KG construction. Answering
these two questions necessitates the deduction of logical as-
sociations among different passages. These associations are
encoded either through 1) lexical similarity: common key-
words shared among different passages, e.g., ‘Alf Clausen’
bridges passage S1 and passage S2 in Figure 2(a), or 2) se-
mantic similarity: syntactic elements that convey semantic
relations, e.g., ‘nationality’ and ‘American director’ in Fig-
ure 2(b). This motivates us to construct the graph by mod-
eling passages as nodes and their lexical/semantic similarity
as edges. More specifically in Figure 3, we split each docu-
ment into individual passages, and for each passage Si, we
add a node vi to the KG with its feature being the text of that
passage Xi. Then we add edges by checking the lexical/se-
mantic similarity between pairs of passage nodes.
TF-IDF KG Construction For adding edges according to
lexical similarity, we first apply TF-IDF keyword extrac-
tion (Ramos et al. 2003) over each document to filter out
meaningless words such as supporting verbs and articles,
which also reduces the dimension of bag-of-word (BOW)
features, sparsifies the constructed graph and increases the
graph traversal efficiency. In addition, we add the document
title into the extracted keyword set since some questions fo-
cus on title entities. We collect the extracted keywords from
all documents to form the keyword space W and then con-
nect two passages if they share any common keyword inW.
KNN-ST/MDR KG Construction For adding edges ac-
cording to semantic similarity, we can readily employ pre-
existing models such as sentence transformers to gener-
ate passage embedding Xi for each node vi and subse-
quently compute pairwise similarity matrix to construct the
K-nearest neighbor (KNN) graph. However, these off-the-
shelf models, typically trained on tasks not so-related to
MD-QA, may not adequately encapsulate necessary logical
associations in their embedding similarity demanded by the
question. To overcome this problem, we follow the train-
ing strategy of MDR (Xiong et al. 2020) and train a sen-
tence encoder by predicting the subsequent supporting facts
based on previously supporting facts, thereby endowing the
encoder with reasoning capability. Consequently, the em-
bedding similarity and the corresponding constructed KNN
graph fundamentally encapsulate the necessary logical asso-
ciations between different passages.
TAGME Moreover, we employ TAGME (Min et al. 2019)
to extract Wikipedia entities from each passage and con-
struct the graph based on whether two passage nodes share
common Wikipedia entities.
In addition to passage nodes, we further add structural
nodes into the graph by extracting document structures via
Extract-PDF 3. In this paper, we only consider adding pages
and tables but the constructed KG can include more differ-
ent types of document structures. The feature of table nodes
is the markdown since LLMs can understand this as demon-
strated in Figure 8 in Supplementary. The feature of page
nodes is the page number and we add directed edges from
it to sentence/table nodes in that page. Note that we do not
aim to propose a one-size-fits-all KG construction method.
Instead, we seek to compare the merits and limitations of
various methods in Table 5, offering guidance on which KGs
are best suited for specific scenarios.
3https://developer.adobe.com/document-services/docs/
overview/pdf-extract-api/



Source: data\tc16_2312.10997v5\referenced_papers\[91]_2308.11730.pdf (Page 3):

Figure 4: LLM-based KG traversal agent for context retrieval. For questions on document structures (left), we employ LLM
to extract structures and retrieve their corresponding contents (the content of pages are passages belonging to that page and
the content of tables is the markdown-formatted text). For questions on document content, we concatenate it with the currently
retrieved context and prompt the LLM to generate the next evidence to answer the question. By comparing the similarity
between the candidate neighboring sentences and the generated passage, we determine the next passage node to traverse.
Correspondingly, the candidate neighbors are updated for the next round of traversal.
Figure 5: Quality of KGs on HotpotQA. For each KG Con-
struction method, as the average number of neighbors in-
creases (KG becomes denser) in the right y-axis, the SF-
EM increases while the precision decreases. KNN-MDR
achieves a better trade-off than TF-IDF and KNN-ST. KGs
constructed by TAGME are denser than others.
To verify the constructed KGs indeed encode the neces-
sary information for MD-QA, we randomly sample ques-
tions from HotpotQA and construct KGs over the set of
documents for each of these questions using our proposed
methods. We vary the hyperparameters to control the spar-
sity of the constructed KG and measure how much percent-
age of the supporting facts are covered by neighbors of the
seeding passages initially searched by TF-IDF. More details
about the four construction methods and their hyperparame-
ters are included in Section 8.5 in Supplementary. As shown
in Figure 5, as the constructed graph becomes denser, the
chance that the neighboring node passages hit the support-
ing facts increases (i.e., SF-EM increases) although the re-
dundant information also increases (i.e., the precision de-
creases). Given the common keywords shared between one
passage to all other passages are typically far less than the
total number of passages across all documents, the density of
the constructed graph by TF-IDF would be upper-bounded,
causing lower SF-EM (evidenced by SF-EM below 0.7 in
Figure 5 for TF-IDF curve). For TAGME, we empirically
find it identifies a larger quantity of entities mentioned in
a single passage, which leads to a denser graph and causes
the starting SF-EM of TAGME to be already around 0.95.
In addition, since KNN-MDR is pre-trained by predicting
the next supporting facts (Xiong et al. 2020) on HotpotQA,
it achieves better trade-off than KNN-ST where the embed-
dings are directly obtained from the sentence transformer
without dataset-specific pre-training.
To summarize, although high SF-EM indicates that the
supporting facts for most questions are fully covered by
the neighbors of seeding passages, low precision signifies
that most of these neighboring passages are irrelevant to the
question. Therefore, if we blindly perform graph traversal
without any question-tailored adaptation, our retrieved con-
texts would include redundant passages and compromise the
capability of LLMs in MD-QA (which is also verified by the
lower performance of KGP-Random in Table 2). To remedy
this issue, in the next section, we introduce an LLM-based
KG traversal agent to adaptively visit neighboring passages
that are most conducive to answering the given question.
4 LLM-based KG Traversal Agent
A natural solution to enable adaptive knowledge graph
traversal is to rank the candidate nodes, i.e., the neighbors
of the already-visited nodes in our case, thereby determining
which ones to visit next. The most straightforward way is to
apply heuristic-based fuzzy matching or embedding-based
similarity ranking, which cannot capture the intrinsic logical
relations between the already traversed paths and the nodes
to visit next. Instead, we fine-tune a large language model
(LLM) to guide the knowledge graph traversal towards the
next most promising passages in approaching the question
based on the visited passages, which we term as the LLM-
based KG traversal agent.



Source: data\tc16_2312.10997v5\referenced_papers\[91]_2308.11730.pdf (Page 1):

Figure 2: Three popular questions that require reasoning and retrieving over passages/pages/tables from multiple documents.
(a) Bridging questionsrely on sequential reasoning while (b) Comparing questionsrely on parallel reasoning over different
passages. (c) Structural questionsrely on fetching contents in the corresponding document structures.
compilation of multi-modality structured data (e.g., pages,
sections, paragraphs, tables, and figures) and some ques-
tions may specifically ask for the content in certain struc-
tures, which necessitates a comprehensive grasp of these
complex document structures. For example, the question in
Figure 2(c) asks about the difference between Page 1 and Ta-
ble 2, which is unanswerable if leveraging heuristic methods
like BM25 or deep-learning ones like DPR (Karpukhin et al.
2020). Building on top of previous challenges, the advent of
LLMs introduces new complexities.
For the challenge of alternatively retrieving and reasoning
knowledge across different documents, although previous
works train a multi-hop retriever (Xiong et al. 2020; Yavuz
et al. 2022) to imitate such process by sequentially fetch-
ing the next passage based on the already-retrieved ones,
none of them explore the potential of engaging LLMs into
this process. More recent works design different prompt-
ing strategies such as Chain/Tree/Graph-of-thought (Trivedi
et al. 2022a; Wei et al. 2022; Yao et al. 2023; Yao, Li, and
Zhao 2023) to guide LLMs approaching answers progres-
sively. However, prompting non-open-sourced LLMs back
and forth incurs forbiddable latency as well as unaffordable
consumption. In addition, how to integrate different docu-
ment structures into the prompt design so that LLMs can
understand them is still an open-ended question.
Given the above challenges, we propose a knowledge
graph prompting (KGP) method for enhancing LLMs in
MD-QA. Specifically, we construct a KG over the given doc-
uments with nodes symbolizing passages or document struc-
tures and edges denoting their lexical/semantic similarity be-
tween passages or intra-document structural relations. Then
for the first challenge of alternative reasoning and retrieving
knowledge across different documents, we design an LLM-
based KG traversal agent, which can alternatively generate
the next evidence to approach the question, i.e., reasoning,
and select the most promising neighbor to visit from the con-
structed KG based on the generated evidence, i.e., retrieval.
Moreover, we apply the instruction fine-tuning strategy to
augment the reasoning capability of the LLM-based KG
traversal agent and hence refrain from repeatedly prompting
non-open-sourced LLMs for evidence generation. For the
multi-modality challenge, we add different types of nodes
to the KG characterizing different document structures and
hence enabling content retrieval within those specific struc-
tures. We highlight our contributions as follows:
• Generally-applicable KG Construction. We propose
three KG construction methods over documents, with pas-
sages or document structures as nodes and their lexical/se-
mantical similarity or structural relations as edges. Then
we empirically evaluate the quality of the constructed
KGs in MD-QA by checking the level of overlap between
the neighborhood and the supporting facts for each ques-
tion (Figure 5). Additionally, we provide a comprehensive
summary of both our proposed and existing KG construc-
tion methods in Table 5 in Supplementary.
• Engaging KG for Prompt Formulation.We design a
Knowledge Graph Prompting (KGP) method, which lever-
ages the LLM-based KG traversal agent to retrieve the
question-relevant contexts by traversing the constructed
KG. Moreover, we fine-tune this agent to adaptively tra-
verse the most promising neighbors for approaching the
question based on the visited nodes (retrieved passages).
• Case Studies Verifying MD-QA Framework.We com-
pare the performance of MD-QA when using different
types of LLM agents in graph traversal (Table 2) on the
KGs constructed over different numbers of documents
(Figure 7(c)). We conduct case studies on visualizing KGP
for MD-QA in Section 8.7 in Supplementary.
2 Notations
Following (Tian et al. 2023a), let G = (V, E) be a knowl-
edge graph constructed from a set of documents D, where
the node set V = {vi}n
i=1 representing document structures
(e.g., passages/pages/tables, etc.) and the edge setE ⊂ V×V
representing the connections among different nodes (e.g.,
semantic/lexical similarity and belonging relations among
document structures, etc.). Let X = {Xi}n
i be node fea-
tures and Xi corresponds to the feature of node vi, the form
of which could be the text for the passage, the markdown for
the table and the page number for the page.



Source: data\tc16_2312.10997v5\referenced_papers\[94]_2309.12871.pdf (Page 3):

Preprint
3.3 I N-BATCH NEGATIVE OBJECTIVE
To further improve performance, we integrate the in-batch negative objective function. Because
in-batch negative samples can serve as a data augmentation technique, which can benefit the gen-
eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that
generate positive samples through data augmentation, we use supervised positive samples. Recog-
nizing that there might be identical sentences within a batch that are not explicitly labeled as positive
samples, causing them to become in-batch negatives, we identify these duplicate sentences and as-
sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch
negative objective function (ibn) is as follows:
Libn = −
X
b
mX
i
log

 ecos(Xbi,X+
bi
)/τ
PN
j e
cos(Xbi,X+
bj
)/τ

, (2)
where τ is a temperature hyperparameter, b stands for the b-th batch, X+
bi
and X+
bj
are the respective
positive samples of Xbi and Xbj , m represents the number of positive pairs in b-th batch, N is the
batch size, and cos(·) is the cosine similarity function.
1
Im 
Re 
divisor
w(1, 1)
Δθ
dividend
z (-1, 1)
quotient
(0, 1)z
w
(a)
x 
Im 
1
ReΔθ
Complex Space
cos(x) 
Δy≈0 
y 
Δx (b)
Figure 2: (a) Division in complex space.∆θ is the angle difference between dividendz and divisorw
in complex space. (b) Angle optimization in cosine saturation zones. Even though∆y ≈ 0 could kill
the gradient, the corresponding angle difference in complex space is still distinct for optimization.
3.4 A NGLE OBJECTIVE
We found that both the cosine and in-batch negative objectives employ the cosine function to mea-
sure similarity. However, it is important to note that the cosine function includes saturation zones,
which can hinder the optimization process. We optimize the angle difference in complex space to
mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de-
picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we
define Xre and Xim are the real part and the imaginary part of X. We follow the implementation
of (Sun et al., 2019) to obtain Xre and Xim by the chunking strategy. Specifically, for the pair
(Xi, Xj), their representations in the complex space are defined as follows:
z = a + bi ∈ C
w = c + di ∈ C, (3)
where a = Xre
i ∈ R, b = Xim
i ∈ R, c = Xre
j ∈ R, and d = Xim
j ∈ R. To compute the angle
difference between z and w, we calculate division in complex space in polar coordinates, as follows:
z
w = γ∆θzw
γ = rz
rw
=
√
a2 + b2
√
c2 + d2
∆θzw = θz − θw,
(4)
4



Source: data\tc16_2312.10997v5\referenced_papers\[91]_2308.11730.pdf (Page 4):

Given a questionq asking about the document content, the
LLM-based graph traversal agent reasons over previously
visited nodes/retrieved passages{sk}j
k=0 and then generates
the next passage sj+1 as follows:
sj+1 = arg max
v∈Nj
ϕ(g(Xv), f(||j
k=0Xk)), (1)
where ||j
k=0Xk concatenates the textual information of pre-
viously retrieved passages/visited nodes. For the choice of
f, one way is to employ encoder-only models like Roberta-
base (Asai et al. 2019; Xiong et al. 2020; Yavuz et al. 2022)
and correspondingly g would be another encoder model with
ϕ(·) being the inner product measuring the embedding sim-
ilarity. Another way is to employ encoder-decoder models
such as T5 (Brown et al. 2020; Touvron et al. 2023) and
correspondingly g would be an identity function with ϕ(·)
measuring the textual similarity. To mitigate the hallucina-
tion issue and enhance the reasoning capability (Wei et al.
2022; Ji et al. 2023) of the LLM traversal agent, we fur-
ther instruction fine-tunef (Chung et al. 2022) by predicting
the next supporting facts based on previous supporting facts,
thereby integrating commonsense knowledge encoded orig-
inally in their pre-trained parameters with the enhanced rea-
soning capability inherited from the instruction fine-tuning.
After visiting the top-scoring nodes selected from the candi-
date neighbor queue by Eq (1), the candidate neighbor queue
is updated by adding neighbors of these newly visited nodes.
We iteratively apply this process until hit the preset budget.
Next, we illustrate the above process with an example in Fig-
ure 4 and present the algorithm thereafter.
Figure 4 presents the content-based question asking ‘In
what year was the creator of the current arrangement of
Simpson’s Theme born?’. We use TF-IDF search to initial-
ize the seeding passage Node 1, which reads: ‘Alf Heiberg
Clausen (born March 28, 1941) is an American film com-
poser’. Subsequently, we prefix the currently retrieved-
context (Node 1) with the question and prompt the LLM to
generate the next evidence required to approach the question
one step closer. Because we augment the reasoning capabil-
ity of the LLM by instruction fine-tuning, it is expected to
recognize the logical association between the question and
the currently retrieved context. Consequently, it can predict
the subsequent passage thatmaintains logical coherence, al-
beit may contain factual mistakes , i.e., ‘Alf Clausen (born
April 16, 1941) is an American composer of film and tele-
vision scores.’ To rectify this potential factual mistake, we
select nodes from the candidate neighbors that match the
most with the LLM-generated passage, in this case, Node
4 ‘Alf Heiberg Clausen (born March 28, 1941) is an Ameri-
can film composer’. Since this passage sources directly from
documents, it inherently ensures the validity of the informa-
tion. Then we prompt LLMs along with the retrieved context
Node 1 and 4 for the answer.
Additionally, for questions asking about document struc-
tures, we extract the document structure names and locate
their corresponding structural nodes in the KG. For the table
node, we retrieve its markdown formatted content while for
the page node, we traverse its one-hop neighbor and obtain
passages belonging to that page.
Algorithm 1: LLM-based KG Traversal Algorithm to Re-
trieve Relevant Context for Content-based Question.
Input: A question q over a set of documents D, the
constructed KG G = {V, E, X} over D, the
fine-tuned LLM-guided graph traversal fGT, the
preset context budget K, the TF-IDF search
function g.
1 Initialize seed passages Vs = g(V, X, q)
2 Initialize the retrieved passage queue P = [{vi}|vi ∈ Vs]
3 Initialize the candidate neighbor queue C = [Ni|vi ∈ Vs]
4 Initialize the retrieved passage counter k = P
Pi∈P |Pi|
5 while queue P and queue C are not empty do
6 Pi ← P.dequeue(), Ci ← C.dequeue()
7 V′
i = Graph Traversal({q} ∪ Pi, Ci, k) by Eq (1)
8 for v ∈ V′
i do
9 P.enqueue(Pi ∪ {v}), C.enqueue(Nv)
10 k ← k + 1
11 if k > Kthen
12 Terminate
13 return Retrieved Passage Queue P
Here we present the algorithm for our proposed KGP
method for MD-QA. Given a question, we first apply LLM
to classify whether the question is asking about the docu-
ment structure or content. If the question focuses on the doc-
ument structure, we extract the structural keywords such as
Page or Table, and retrieve the content in the correspond-
ing structural nodes in KG. If the question focuses on the
document content, we follow the step according to Algo-
rithm 1. Specifically, we first initialize seeding passages Vs
and the reasoning path queue P by TF-IDF search. Then
for each seeding passage vi ∈ Vs, we add its neighbor-
ing passage nodes Ni into the candidate neighbor queue C
(lines 1-4). After that, we iteratively dequeue the earliest en-
queued reasoning path/candidate neighborhood Pi/Ci from
P/C and employ the fine-tuned LLM-based graph traver-
sal agent to rank the dequeued neighbors in Ci by Eq. (1)
(lines 5-7). Last, we select top-k passage nodesV′
i from Ci to
visit next based on their rank and correspondingly update the
candidate neighbor queue and reasoning path queue (lines 8-
13). The above process terminates when either the candidate
neighbor queue becomes empty or the prefixed budgetK for
the retrieved passages is met. The time and space complexity
are thoroughly analyzed in Section 8.3 in Supplementary.
5 Experiment
In this section, we conduct experiments to verify the pro-
posed knowledge graph prompting method (KGP) for MD-
QA. In particular, we answer the following questions:
• Q1 - Section 5.1: How well does KGP perform MD-QA
compared with existing baselines?
• Q2 - Section 5.2-5.3: How do the quality of the con-
structed KG and the LLM-based graph traversal agent
impact the MD-QA performance?
Due to the space limitation, we comprehensively introduce
our experimental setting, including dataset collection, base-
lines, and evaluation criteria, in Supplementary 8.1-8.2.



### Claim 179/179

#### Claim Text
T he lower state constants were fixed to values from rotation spectroscopy in our simulations .

#### Retrieved Documents
Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 34):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
dataset example
MMLU-
Chemistry
Original Question: Which of the following indicates that a reaction is
spontaneous?
0) at equilibrium there are more products than reactants
1) the value of ∆G0 is greater than zero
2) the value of ∆S0 is greater than zero
3) the value of Keq is less than one
Original Answer [Incorrect]: 2) the value of ∆S- is greater than zero.
A reaction is spontaneous if the value of ∆S- is greater than zero. This is
because a positive ∆S- indicates that the reaction results in an increase in
entropy, which is a measure of disorder. A spontaneous reaction is one that
occurs without the input of energy, and an increase in entropy is a natural
tendency of the universe.
Stepback Question: What are the chemistry principles behind this question?
Principles:
Spontaneous reactions: A spontaneous reaction is a reaction that occurs without
the input of energy.
Final Answer: A reaction is spontaneous if the change in Gibbs free energy is
negative (∆G <0). The value of ∆G0 is greater than zero indicates that the
reaction is not spontaneous.
The correct answer is 0) at equilibrium there are more products than reactants.
MMLU-
Chemistry
Original Question: Potassium-40 is a minor isotope found in naturally oc-
curring potassium. It is radioactive and can be detected on simple radiation
counters. How many protons, neutrons, and electrons does potassium-40 have
when it is part of K2SO4?
0) 21 neutrons, 19 protons, 18 electrons
1) 20 neutrons, 19 protons, 19 electrons
2) 21 neutrons, 19 protons, 19 electrons
3) 19 neutrons, 19 protons, 19 electrons
Original Answer [Incorrect]: The correct answer is 2).
Potassium-40 has 19 protons and 21 neutrons. When it is part of K2SO4, it has
19 electrons.
Stepback Question: What are the chemistry principles behind this question?
Principles:
Atomic number: The atomic number of an element is the number of protons in
the nucleus of an atom of that element.
Final Answer: The atomic number of potassium is 19, so all potassium atoms
have 19 protons. The mass number of potassium-40 is 40, so it has 40 - 19 =
21 neutrons. When potassium is part of a compound, it loses one electron to
become a potassium ion. Therefore, potassium-40 has 19 - 1 = 18 electrons
when it is part of K2SO4.
The correct answer is 0) 21 neutrons, 19 protons, 18 electrons.
Table 31: Illustration of wins of STEP -BACK PROMPTING on the MMLU-Chemistry dataset.
35



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 12):

Parameter Ranges
General Parameters
Input Psg. [5, 100]
CALM
Confidence Thresh-
old
[0.2, 0.9]
Threshold Coef. {0.5, 0.7, 0.9}
Threshold Decay {3, 4, 5}
Token Filtering
% of Input {10, 30, 50}
Filtering Token [1,20]
Filtering Layer [1, L]
Table 7: The hyperparameter search space for the
CALM and Token Filtering methods, including param-
eters for all methods (Input Psg.), and the combined
approach. {} indicate a set of possible values, while
[min, max] correspond to a range of values from min
to max. As in the paper, L is the number of layers in
the decoder.



Source: data\tc16_2312.10997v5\referenced_papers\[52]_2310.13682.pdf (Page 15):

0 50 100 150
1.6
1.8
2
2.2
2.4
2.6
2.8
3 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(a) ELI5 - p = 10%
0 50 100 150
1.4
1.45
1.5
1.55
1.6
1.65 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (b) ELI5 - p = 30%
0 50 100 150
1.04
1.06
1.08
1.1
1.12
1.14 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (c) ELI5 - p = 50%
0 10 20 30 40 50
1.6
1.8
2
2.2
2.4
2.6
2.8
3
3.2
3.4 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(d) MS MARCO - p = 10%
0 10 20 30 40 50
1.15
1.2
1.25
1.3
1.35
1.4 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (e) MS MARCO - p = 30%
0 10 20 30 40 50
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
0.91 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (f) MS MARCO - p = 50%
0 20 40 60 80 100
1.5
2
2.5
3
3.5
1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
(g) NQ - p = 10%
0 20 40 60 80 100
1.5
1.6
1.7
1.8
1.9
2 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen
Loa ding [ M a thJ a x ] / ex tensions/ M a thM enu .j s (h) NQ - p = 30%
0 20 40 60 80 100
1.22
1.24
1.26
1.28
1.3
1.32
1.34
1.36 1
2
3
6
12
Answer Length (in tokens)
% of Gold from Chosen (i) NQ - p = 50%
Figure 7: The ratio of tokens that were chosen from the gold passage, per decoder layer (1-12), for FiD-Base models.
Each row represents a different dataset, and every column represents a different filtering percentage (10,30,50).



Source: data\tc16_2312.10997v5\referenced_papers\[10]_2310.06117.pdf (Page 29):

Step-Back Prompting Enables Reasoning Via Abstraction in Large Language Models
Question The circumference of a helium-filled balloon is measured for three different
conditions: at room temperature, after being in a warm oven for 30 minutes,
and after being in a freezer for 30 minutes. A student plotting the circumference
cubed C3 as a function of temperature T, should expect to find which of the
following?
0) a cubic relationship between C3 and T
1) an indirect relationship between C3 and T
2) an extrapolated temperature T where C3 reaches zero
3) a maximum C3 as the temperature T increases
Principles Ideal Gas Law: PV = nRT, where P is the pressure, V is the volume, n is
the number of moles of gas, R is the ideal gas constant, T is the absolute
temperature.
V olume of a sphere:V = 4/3 ∗ π ∗ r3, where r is the radius.
Absolute temperature: Absolute temperature is always written as kelvin.
Absolute zero: Absolute zero is the lowest limit of the kelvin scale and is the
lowest possible temperature (- 273.15 °C).
Answer Using the Principles of Charles’s Law and Gay-Lussac’s Law, we can solve the
problem as following:
Step 1: Apply Charles’s Law to find out the relationship between the volume
and temperature of the gas. Charles’s Law states that the volume of a fixed
mass of a gas is directly proportional to the absolute temperature of the gas
when pressure is kept constant. This means that if the temperature of the gas
increases, the volume of the gas will also increase.
Step 2: Apply Gay-Lussac’s Law to find out the relationship between the
pressure and temperature of the gas. Gay-Lussac’s Law states that the pressure
of a fixed mass of a gas is directly proportional to the absolute temperature of
the gas when volume is kept constant. This means that if the temperature of the
gas increases, the pressure of the gas will also increase.
Step 3: Apply Charles’s Law and Gay-Lussac’s Law to find out the relationship
between the circumference and temperature of the gas.The circumference of the
balloon is directly proportional to the volume of the gas. This is because the
circumference of the balloon is the distance around the outside of the balloon,
and the volume of the gas is the amount of space that the gas takes up.
The pressure of the gas is directly proportional to the temperature of the gas.
This is because the pressure of the gas is the force that the gas exerts on the
walls of the balloon, and the temperature of the gas is the measure of the
average kinetic energy of the gas molecules.
Therefore, the circumference of the balloon is directly proportional to the
temperature of the gas. This means that if the temperature of the gas increases,
the circumference of the balloon will also increase.
Step 4: Apply the relationship between the circumference and temperature
of the gas to find out the relationship between the circumference cubed and
temperature of the gas. The circumference cubed is directly proportional to the
temperature of the gas. This is because the circumference cubed is the volume
of the balloon, and the volume of the balloon is directly proportional to the
temperature of the gas.
Therefore, the student should expect to find a cubic relationship between C3
and T. The correct answer is 0) a cubic relationship between C3 and T.
Table 26: Error example of STEP -BACK PROMPTING on MMLU high-school Physics in the class of
Context Loss. In this case, at the end of the Reasoning chain, the model forgot the original question,
and lose the context to apply the reasoning to the question.
30



Source: data\tc16_2312.10997v5\referenced_papers\[47]_2305.17331.pdf (Page 13):

1 2 3 4 5 6 7 8 9 10
Number of documents
36
38
40
42
44MMLU Accuracy
Standalone LM
Lt=Flan-T5Base
(a) Flan-T5Base w/ AARANCE.
1 2 3 4 5 6 7 8 9 10
Number of documents
44
46
48
50
52MMLU Accuracy
Standalone LM
Lt=Flan-T5Larg e (b) Flan-T5Large w/ AARANCE.
1 2 3 4 5 6 7 8 9 10
Number of documents
50
52
54
56
58MMLU Accuracy
Standalone LM
Lt=Flan-T5X L (c) Flan-T5XL w/ AARANCE.
Figure 9: Relationship between LM’s performance and the number of augmentation documents.
Category Number
Ts
MSMARCO QA Open Domain QA 148122
KILT-FEVER Fact Checking 10444
KILT-WNED Entity Linking 3396
KILT-T-REx Slot Filling 5000
KILT-TriviaQA Open Domain QA 5359
KILT-Wizard of Wikipedia Dialogue 3054
Tt
MMLU Multi-task Language Understanding 1531
PopQA Open Domain QA 14267
Table 6: Configurations of our source tasks and target tasks.
Methods MMLU
All Hum. Soc. Sci. STEM Other
Flan-T5Base 36.1 40.4 39.8 27.0 40.6
Flan-T5Base Fine-tuning 36.1 38.9 41.2 27.9 39.9
Flan-T5Base w/ Contriever 43.7 44.4 45.0 36.4 51.1
Flan-T5Base w/ ANCE 43.0 44.2 44.3 34.5 51.9
Flan-T5Base w/ AARContriever (Ours) 44.4 44.7 47.7 35.8 52.2
Flan-T5Base w/ AARANCE (Ours) 44.8 42.2 46.4 39.0 53.2
Flan-T5Large 45.1 47.7 53.5 34.4 49.2
Flan-T5Large Fine-tuning 45.3 47.6 54.1 35.2 48.7
Flan-T5Large w/ Contriever 50.7 50.5 56.4 38.9 61.1
Flan-T5Large w/ ANCE 49.2 49.3 56.7 38.1 57.2
Flan-T5Large w/ AARContriever (Ours) 51.8 50.8 59.7 39.4 61.8
Flan-T5Large w/ AARANCE (Ours) 50.4 48.0 58.1 39.3 60.2
Flan-T5XL 51.2 55.5 57.4 38.1 58.7
Flan-T5XL w/ Contriever 56.4 57.3 66.1 43.9 63.2
Flan-T5XL w/ ANCE 55.3 55.9 64.0 41.5 64.9
Flan-T5XL w/ AARContriever (Ours) 56.7 57.7 65.4 43.6 65.1
Flan-T5XL w/ AARANCE (Ours) 56.2 59.4 64.8 41.5 64.9
InstructGPT 60.2 65.7 68.0 46.1 66.5
InstructGPT w/ Contriever 60.5 62.0 71.8 44.3 70.1
InstructGPT w/ ANCE 61.6 62.4 73.4 47.6 68.6
InstructGPT w/ AARContriever (Ours) 61.5 64.5 73.1 45.0 69.9
InstructGPT w/ AARANCE (Ours) 62.2 62.0 72.0 49.2 70.7
Table 7: Fine-tuning results on MMLU. We use the official auxiliary training data of MMLU to fine-tune the LM.



## Processing Completed
Finished at: 2025-01-23 18:51:01
