# Claims Processing Log

Processing started at: 2025-01-24 17:48:55

## Table of Contents

[[log_20250124_174855###Claim 1/36|Claim 1/36]]
[[log_20250124_174855###Claim 2/36|Claim 2/36]]
[[log_20250124_174855###Claim 3/36|Claim 3/36]]
[[log_20250124_174855###Claim 4/36|Claim 4/36]]
[[log_20250124_174855###Claim 5/36|Claim 5/36]]
[[log_20250124_174855###Claim 6/36|Claim 6/36]]
[[log_20250124_174855###Claim 7/36|Claim 7/36]]
[[log_20250124_174855###Claim 8/36|Claim 8/36]]
[[log_20250124_174855###Claim 9/36|Claim 9/36]]
[[log_20250124_174855###Claim 10/36|Claim 10/36]]
[[log_20250124_174855###Claim 11/36|Claim 11/36]]
[[log_20250124_174855###Claim 12/36|Claim 12/36]]
[[log_20250124_174855###Claim 13/36|Claim 13/36]]
[[log_20250124_174855###Claim 14/36|Claim 14/36]]
[[log_20250124_174855###Claim 15/36|Claim 15/36]]
[[log_20250124_174855###Claim 16/36|Claim 16/36]]
[[log_20250124_174855###Claim 17/36|Claim 17/36]]
[[log_20250124_174855###Claim 18/36|Claim 18/36]]
[[log_20250124_174855###Claim 19/36|Claim 19/36]]
[[log_20250124_174855###Claim 20/36|Claim 20/36]]
[[log_20250124_174855###Claim 21/36|Claim 21/36]]
[[log_20250124_174855###Claim 22/36|Claim 22/36]]
[[log_20250124_174855###Claim 23/36|Claim 23/36]]
[[log_20250124_174855###Claim 24/36|Claim 24/36]]
[[log_20250124_174855###Claim 25/36|Claim 25/36]]
[[log_20250124_174855###Claim 26/36|Claim 26/36]]
[[log_20250124_174855###Claim 27/36|Claim 27/36]]
[[log_20250124_174855###Claim 28/36|Claim 28/36]]
[[log_20250124_174855###Claim 29/36|Claim 29/36]]
[[log_20250124_174855###Claim 30/36|Claim 30/36]]
[[log_20250124_174855###Claim 31/36|Claim 31/36]]
[[log_20250124_174855###Claim 32/36|Claim 32/36]]
[[log_20250124_174855###Claim 33/36|Claim 33/36]]
[[log_20250124_174855###Claim 34/36|Claim 34/36]]
[[log_20250124_174855###Claim 35/36|Claim 35/36]]
[[log_20250124_174855###Claim 36/36|Claim 36/36]]


## Processing Details


### Claim 1/36

#### Claim Text
LLMs in understanding and generating natural language [73], especially in the medical domain [ 7, 36, 56, 74, 76, 83, 89, 95, 100].

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 2):

3  
Introduction Large language models (LLM) have shown impressive performance in solving complex tasks across various domains that involve language modeling and processing.(Zhao et al. 2023) LLMs are pre-trained on a large corpora of text data in a self-supervised learning framework which can be either Masked Language Modeling (e.g. BERT like models)(Devlin et al. 2018; Liu et al. 2019) or Auto-Regressive framework (GPT like models)(Brown et al. 2020; Luo et al. 2022). This pre-training encodes knowledge about the language in the model parameters. Similar to the transfer learning approach commonly used in deep neural networks, this implicit knowledge can be refined through supervised training to excel in a range of domain-specific tasks.(Wei et al. 2021; Luo et al. 2022) Nevertheless, the “implicit representation” of knowledge in LLM has also been shown to generate non-factual information despite linguistically coherent answers (i.e. “hallucination”) as a response to the input prompt.(Maynez et al. 2020; Raunak, Menezes and Junczys-Dowmunt 2021; Ji et al. 2022) This issue poses a significant challenge for the adoption of LLM models in domains with stringent requirements for accuracy, such as biomedicine.  Various strategies have been introduced to address hallucinations in LLMs. One such solution involves the utilization of domain-specific data for pre-training the LLM, rather than relying on generic text corpora. This approach has led to the creation of models such as PubMedBERT(Yu Gu Microsoft Research, Redmond, WA et al. 2021), BioBERT(Lee et al. 2019), BlueBERT(Lee et al. 2019; Peng, Yan and Lu 2019), SciBERT(Beltagy, Lo and Cohan 2019), ClinicalBERT(Huang, Altosaar and Ranganath 2019), BioGPT(Luo et al. 2022), Med-PaLM(Singhal et al. 2023), and BioMedGPT.(Luo et al. 2023) However, pre-training an LLM from scratch imposes a significant computational cost and time overhead to attain the desired human-like performance. An alternative approach, known as prompt tuning, was recently proposed as a means to enhance LLM performance, for instance through the use of zero-



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 27):

28  
Aging 2024;4:379–95. Touvron H, Martin L, Stone K et al. Llama 2: Open Foundation and Fine-Tuned Chat Models. 2023. Vasilevsky NA, Matentzoglu NA, Toro S et al. Mondo: Unifying diseases for the world, by the world. medRxiv 2022:2022.04.13.22273750. Wang X, Kapanipathi P, Musa R et al. Improving Natural Language Inference Using External Knowledge in the Science Questions Domain. AAAI 2019;33:7208–15. Wei J, Bosma M, Zhao VY et al. Finetuned Language Models Are Zero-Shot Learners. 2021. Wei J, Tay Y, Bommasani R et al. Emergent Abilities of Large Language Models. 2022a. Wei J, Wang X, Schuurmans D et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. 2022b. Xiang Chen Zhejiang University, China, Ningyu Zhang Zhejiang University, China, Xin Xie Zhejiang University, China et al. KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. , DOI: 10.1145/3485447.3511998. Yang A, Wang Q, Liu J et al. Enhancing pre-trained language representations with rich knowledge for machine reading comprehension. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Stroudsburg, PA, USA: Association for Computational Linguistics, 2019. Yasunaga M, Bosselut A, Ren H et al. Deep Bidirectional Language-Knowledge Graph Pretraining. 2022. Yasunaga M, Ren H, Bosselut A et al. QA-GNN: Reasoning with language models and knowledge graphs for question answering. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Stroudsburg, PA, USA: Association for Computational Linguistics, 2021. Yu Gu Microsoft Research, Redmond, WA, Robert Tinn Microsoft Research, Redmond, WA, Hao Cheng Microsoft Research, Redmond, WA et al. Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. ACM Transactions on Computing for Healthcare (HEALTH) 2021, DOI: 10.1145/3458754. Zhao WX, Zhou K, Li J et al. A Survey of Large Language Models. 2023.



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 0):

Published as a conference paper at ICLR 2024
CHAIN -OF-KNOWLEDGE : G ROUNDING LARGE LAN-
GUAGE MODELS VIA DYNAMIC KNOWLEDGE ADAPT-
ING OVER HETEROGENEOUS SOURCES
Xingxuan Li1,2∗† , Ruochen Zhao2∗‡ , Yew Ken Chia1,3∗† , Bosheng Ding1,2†, Shafiq Joty2,4
Soujanya Poria3, Lidong Bing1,5
1DAMO Academy, Alibaba Group, Singapore, 2Nanyang Technological University,
3Singapore University of Technology and Design, 4Salesforce Research,
5Hupan Lab, 310023, Hangzhou, China
{xingxuan.li, yewken.chia, bosheng.ding, l.bing}@alibaba-inc.com
{ruochen002, srjoty}@ntu.edu.sg sporia@sutd.edu.sg
ABSTRACT
We present chain-of-knowledge (CoK) , a novel framework that augments large
language models (LLMs) by dynamically incorporating grounding information
from heterogeneous sources. It results in more factual rationales and reduced
hallucination in generation. Specifically, CoK consists of three stages: reason-
ing preparation, dynamic knowledge adapting, and answer consolidation. Given
a knowledge-intensive question, CoK first prepares several preliminary rationales
and answers while identifying the relevant knowledge domains. If there is no ma-
jority consensus among the answers from samples, CoK corrects the rationales
step by step by adapting knowledge from the identified domains. These corrected
rationales can plausibly serve as a better foundation for the final answer consolida-
tion. Unlike prior studies that primarily use unstructured data, CoK also leverages
structured knowledge sources such as Wikidata and tables that provide more re-
liable factual information. To access both unstructured and structured knowledge
sources in the dynamic knowledge adapting stage, we propose an adaptive query
generator that allows the generation of queries for various types of query lan-
guages, including SPARQL, SQL, and natural sentences. Moreover, to minimize
error propagation between rationales, CoK corrects the rationales progressively
using preceding corrected rationales to generate and correct subsequent rationales.
Extensive experiments show that CoK consistently improves the performance of
LLMs on knowledge-intensive tasks across different domains. Our code is avail-
able at https://github.com/DAMO-NLP-SG/chain-of-knowledge.
1 I NTRODUCTION
In recent years, large language models (LLMs) such as ChatGPT (OpenAI, 2023) have demonstrated
impressive language generation capabilities (Cheng et al., 2023; Ding et al., 2023; Chen et al., 2024).
However, one major challenge of LLMs lies in hallucination, which is their tendency to confidently
generate plausible but factually incorrect texts (Ji et al., 2023; Zhao et al., 2023b). As shown in
Figure 1, given a question, “What year was the Argentine actor who directed El Tio Disparate born?”
which requires factual knowledge to answer, the most advanced LLMs often provide an incorrect
answer. While LLMs have the remarkable capability to recall information from their training data,
effectively updating or controlling the factual knowledge within these models remains challenging
(Luo et al., 2023).
∗ Equal contribution.
† Xingxuan Li, Yew Ken Chia, and Bosheng Ding are under the Joint Ph.D. Program between DAMO
Academy and their corresponding universities.
‡ Ruochen Zhao is under the AISG Ph.D. Fellowship Programme.
1
arXiv:2305.13269v4  [cs.CL]  21 Feb 2024



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 19):

Published as a conference paper at ICLR 2024
Table 12: Performances of ChatGPT and instruction-tuned LlaMA-2-7B on SQL and SPARQL
generation.
Method SQL Eval. Acc. SPARQL Eval. Acc.
ChatGPT 57.1% 8.9%
Finetuned Model 38.6% 41.1%
Figure 4: Human evaluation instructions.
that this is a general limitation of retrieval methods, as retrieval results inevitably contain some noise
due to lack of relevant data or inaccurate queries. To address this challenge, we have designed the
CoK framework to be modular and flexible. Hence, the adaptive query generator models can be
easily swapped for other models that may be more suitable for the given task. Rather than focusing
on using specific query generator models, our focus is that heterogeneous knowledge sources can
be effectively incorporated with LLMs to improve their factual correctness and performance on
knowledge-intensive tasks.
Reasoning Capability of LLMs As CoK relies on the reasoning capability of LLMs, failure cases
may stem from reasoning failures of LLMs. We believe this is a general limitation of generative
models, as LLMs inevitably generate reasoning errors. Case studies of such failures can be found
in Appendix H.2. To address this challenge, CoK is designed to be modular and flexible. And the
black-box LLM can be easily swapped for more advanced models possessing enhanced reasoning
capabilities.
20



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 0):

Baichuan 2: Open Large-scale Language Models
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan
Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai
Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji
Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma
Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun
Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng
Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang
Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, Zhiying Wu
Baichuan Inc.
Abstract
Large language models (LLMs) have
demonstrated remarkable performance on
a variety of natural language tasks based
on just a few examples of natural language
instructions, reducing the need for extensive
feature engineering. However, most powerful
LLMs are closed-source or limited in their
capability for languages other than English. In
this technical report, we present Baichuan 2,
a series of large-scale multilingual language
models containing 7 billion and 13 billion
parameters, trained from scratch, on 2.6 trillion
tokens. Baichuan 2 matches or outperforms
other open-source models of similar size on
public benchmarks like MMLU, CMMLU,
GSM8K, and HumanEval. Furthermore,
Baichuan 2 excels in vertical domains such
as medicine and law. We will release all
pre-training model checkpoints to benefit the
research community in better understanding
the training dynamics of Baichuan 2.
1 Introduction
The field of large language models has witnessed
promising and remarkable progress in recent years.
The size of language models has grown from
millions of parameters, such as ELMo (Peters
et al., 2018), GPT-1 (Radford et al., 2018), to
billions or even trillions of parameters such as GPT-
3 (Brown et al., 2020), PaLM (Chowdhery et al.,
2022; Anil et al., 2023) and Switch Transformers
(Fedus et al., 2022). This increase in scale has
led to significant improvements in the capabilities
of language models, enabling more human-like
fluency and the ability to perform a diverse range
of natural language tasks. With the introduction of
Authors are listed alphabetically, correspondent:
daniel@baichuan-inc.com.
ChatGPT (OpenAI, 2022) from OpenAI, the power
of these models to generate human-like text has
captured widespread public attention. ChatGPT
demonstrates strong language proficiency across
a variety of domains, from conversing casually to
explaining complex concepts. This breakthrough
highlights the potential for large language models
to automate tasks involving natural language
generation and comprehension.
While there have been exciting breakthroughs
and applications of LLMs, most leading LLMs like
GPT-4 (OpenAI, 2023), PaLM-2 (Anil et al., 2023),
and Claude (Claude, 2023) remain closed-sourced.
Developers and researchers have limited access to
the full model parameters, making it difficult for
the community to deeply study or fine-tune these
systems. More openness and transparency around
LLMs could accelerate research and responsible
development within this rapidly advancing field.
LLaMA (Touvron et al., 2023a), a series of large
language models developed by Meta containing up
to 65 billion parameters, has significantly benefited
the LLM research community by being fully open-
sourced. The open nature of LLaMA, along with
other open-source LLMs such as OPT (Zhang
et al., 2022), Bloom (Scao et al., 2022), MPT
(MosaicML, 2023) and Falcon (Penedo et al.,
2023), enables researchers to freely access the
models for examination, experimentation, and
further development. This transparency and access
distinguishes LLaMA from other proprietary
LLMs. By providing full access, the open-source
LLMs have accelerated research and advances in
the field, leading to new models like Alpaca (Taori
et al., 2023), Vicuna (Chiang et al., 2023), and
others (Wang et al., 2022; Zhu et al., 2023; Anand
et al., 2023).
arXiv:2309.10305v2  [cs.CL]  20 Sep 2023



### Claim 2/36

#### Claim Text
However, the gap between unstructured user queries of inconsistent quality and structured, high-quality KGs [65] poses significant challenges on how to properly parse user intent for improving the robustness of retrieved knowledge (pre-retrieval phase) and how to handle the abundant retrieved knowledge (post-retrieval phase), which are detailed as follows: Challenge I: At the pre-retrieval phase, previous works suffer from how to parse user intent and retrieve reasonable knowledge based on varying-quality user query.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 12):

13  
Previous studies have utilized KG in conjunction with LLM for knowledge intensive tasks such as question-answering,(Yasunaga et al. 2021) multi-hop relational reasoning,(Feng et al. 2020) commonsense reasoning,(Lin et al. 2019; Lv et al. 2019) and model pre-training.(Moiseev et al. 2022; Yasunaga et al. 2022) Furthermore, enhancing prompts by incorporating structured knowledge has been described and studied.(Xiang Chen Zhejiang University, China et al.; Lewis et al. 2020; Pan et al. 2023) Naturally, these approaches have bolstered the positive reinforcement between KG and LLM. Nevertheless, it's worth noting that these approaches are only be task-specific and in some cases, the knowledge infusion could grow exponentially by the inclusion of higher order relations.(Bauer, Wang and Bansal 2018; Lin et al. 2019) Such approaches could compromise the limited token space of the LLM. Alternative methods employed knowledge infusion through the direct use of query languages like SPARQL.(Brate et al. 2022) However, this could render the system constraint to the schema of the underlying KG, potentially affecting the flexibility and adaptability of prompts. Moreover, as the KG expands and its schema grows, it could potentially occupy a significant portion of the LLM input token space. This explains why we noticed a greater token usage with the Cypher-RAG method (average usage of 8006 tokens), as it incorporates the entire graph schema into the input prompt for converting natural language into structured Cypher queries. In contrast, KG-RAG requires minimal graph schema, thus eliminating the need to include it in the prompt and resulting in substantial token savings, with a reduction of over 50% in token utilization compared to Cypher-RAG. This finding suggests that when dealing with a graph as large as SPOKE, which contains over 40 million nodes, Cypher-RAG requires LLMs that enable a larger context window. In contrast, KG-RAG is capable of managing this with LLMs that require a relatively smaller window size.



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 15):

16  
parameterized implicit knowledge of an LLM. This knowledge integration results in the generation of domain-specific, reliable and up-to-date meaningful biomedical responses with rich annotations.  Materials and Methods Knowledge Graph based Retrieval Augmented Generation (KG-RAG) Framework The Schema of the proposed KG-RAG framework is shown in Fig 4. The following sections explain each component of this framework. 
 Fig 4. Schema for the Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) Framework. The direction of the arrows indicates the flow of the pipeline in this framework



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 11):

12  
Discussion In this work, we introduce a simple but highly effective framework that combines a biomedical knowledge graph with LLM chat models in a token optimized fashion. This integration resulted in a domain-specific generative system whose responses were firmly grounded in well-established biomedical knowledge. We compared the proposed framework with another RAG approach that utilizes Cypher query and showed that KG-RAG was more robust to prompt perturbation and more efficient in token utilization. In addition, KG-RAG consistently demonstrated superior performance compared to the prompt-based baseline LLM model on all human-curated benchmark datasets. We hypothesize that this performance improvement arises from the fusion of the explicit knowledge from the KG and the implicit knowledge from the LLM. This shows the value of providing domain-specific ground truth at a fine-grained resolution as context at the prompt level.   A heterogeneous knowledge graph with diverse concepts (the biomedical concepts in this case) interconnected at a massive scale has the potential to generate new knowledge as an “emergent property”.(Baranzini et al. 2022; Morris et al. 2023) In fact, as LLMs scale up in various dimensions like model parameters, training data, and training compute, they have been thought to exhibit reasoning or “emerging capabilities”(Wei et al. 2022a) although this observation could also be explained by “in-context learning” or other aspects of the examples.(Brown et al. 2020; Min et al. 2022; Lu et al. 2023) In any case, KG-RAG capitalized this capability and generated biomedical text with rich annotations such as provenance and statistical evidence (if available) thereby resulting in more reliable and knowledge-grounded responses. Additionally, the optimized and fine-grained context retrieval capability of KG-RAG ensured a budget friendly RAG system to apply on proprietary LLMs.



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 14):

15  
more competitive with proprietary models like GPT. The findings underscore the importance of context enrichment techniques for improving the performance of language models on complex tasks in specialized domains.   While the proposed framework has successfully addressed numerous challenges, we recognize there are opportunities for improvement. Currently, this approach is limited to handling biomedical questions centered around diseases due to our focus on embedding disease concepts from the SPOKE KG during recognition of the biomedical entity from the prompt. Future work could expand this scope by including all biomedical concepts (nodes) in SPOKE and other KG. Since SPOKE contains more than 40 million biomedical nodes, this expansion will enable the KG-RAG framework to address a broader range of biomedical questions and thereby enhance its versatility. Currently, we implemented the KG-RAG framework on the SPOKE biomedical knowledge graph. However, we believe that the framework is general enough to extend to other biomedical KG and even other domains as well. Finally, the quality of the retrieved context relies on the information stored in the underlying graph. In our case, SPOKE utilizes meticulously curated knowledge bases to construct its nodes and edges; however, we do not assert that it is entirely error-free or ready for clinical use. Thus, while SPOKE’s reliability has been demonstrated through its successful application in various biomedical contexts.(Himmelstein and Baranzini 2015; Himmelstein et al. 2017; Nelson, Butte and Baranzini 2019; Nelson et al. 2021a, 2021b; Baranzini et al. 2022; Morris et al. 2023; Soman et al. 2023a, 2023b; Tang et al. 2024), it is important to note that this work focused on the creation of a framework rather than on a rigorous and formal evaluation of the KG itself.   In summary, the KG-RAG framework retrieves semantically meaningful context from a knowledge graph using minimal tokens, then combines this explicit knowledge with the



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 7):

8  
 Fig 2. Hyperparameter analysis and RAG comparison (A) Hyperparameter analysis performance curves using prompts with single (top) and two (below) disease entities mentioned in it. The x-axis denotes the ‘Context volume’ (number of associations from KG) and the y-axis denotes the mean performance (Jaccard similarity) across the prompts. The red curve denotes ‘PubMedBert’ and the blue curve denotes ‘MiniLM’ transformer models. (B) shows the comparative analysis between KG-RAG (green color) and Cypher-RAG (red color) in terms of retrieval accuracy (top) and token usage (bottom). Insight shows an example where Cypher-RAG fails to retrieve context from the KG when the input prompt is slightly perturbed, but KG-RAG remains robust in context retrieval. It is evident that KG-RAG has lesser token usage (average of 53.9% reduction in token usage) when compared to Cypher-RAG (bottom). Error bar in the token utilization bar plot (bottom) represents standard error of the mean (sem).     RAG comparative analysis Fig 2B shows the comparative analysis between the proposed KG-RAG and the existing Cypher-RAG approach for context retrieval from a KG (See Materials and Methods). We compared these two frameworks based on their retrieval accuracy, retrieval robustness and token usage. For a test dataset with 100 biomedical questions (S1 Text), Cypher-RAG and KG-RAG showed 75% and 97% retrieval accuracy respectively (Fig 2B top). To test the robustness in context retrieval,



### Claim 3/36

#### Claim Text
This misalignment between the semantic spaces of user queries and high-quality structured knowledge leads to the retrieval of knowledge that is of insufficient quality and may contain redundant information and noise [8].

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 5):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
5 Advanced Experiments
The studies and experiments outlined in section 4 have shown unsatisfactory performance, marked by a degradation
of at least 20% compared to the peak relative performance. Therefore, in this section, we explore various retrieval
approaches for the RAG, recognizing that the quality of the retriever is a crucial factor in enhancing performance for
this type of problem. We conducted an evaluation covering both sparse and dense search, a hybrid method, and even
a multi-stage architecture using a reranker.
In pursuit of code debugging flexibility and easier customization at each stage, we chose not to utilize an RAG frame-
work (like LangChain or Llama-Index). For a comprehensive guide on debugging RAG and more details about retrieval
systems, refer to [20] and [21].
5.1 Retrievers
When deploying retrieval systems, it is essential to achieve a balance between “effectiveness” ( How good are the
results returned?) and “efficiency” (How much time it takes to return the results? or How much resources are used in
terms of disk/RAM/GPU?). This balance ensures that latency, result quality, and computational budget remain within
our application’s required limits. This work will exclusively focus on effectiveness measures to quantify the retrievers
methods quality.
In our retriever experiments, the evaluation strategy centers around assessing how well the retriever performs in re-
trieving relevant information based on each given query qi. To achieve this, we employ the concept of recall. It is
defined as the fraction of the relevant documents for a given query qi that are successfully retrieved in a ranked list
R [21]. This metric is based on binary relevance judgments, assuming that documents are either relevant or not [21].
In this paper, each chunk is considered a document and only the respective chunkdi is considered relevant to the query
qi. While recall is easy to interpret, it does not consider the specific rank positions in which the relevant chunk appears
in R.
To overcome this limitation, we introduce Reciprocal Rank (RR) into our analysis. In this metric, the rank of the
first relevant document to the query in R is used to compute the RR score [21]. Therefore, Reciprocal Rank offers a
more nuanced evaluation by assigning a higher value when the relevant chunk is returned in the early positions of our
retrievers given the respective query.
Recall and Reciprocal Rank were evaluated at a specific cutoff so the measures are presented as R@k and MRR@k.
For each query, its results are evaluated and their mean serves as an aggregated measure of effectiveness of a given
retriever method. The retrievers are introduced below.
In the category of sparse retrievers, we emphasize the BM25, a technique grounded in statistical weighting to assess
relevance between search terms and documents. BM25 employs a scoring function that takes into account term
frequency and document length, offering an efficient approach for retrieving pertinent information and is typically
used as a strong baseline. However, it is exact-match based and can be powerless when query and document are
relevant to each other but has no common words.
On the other hand, when exploring dense retrievers, we often encounter approaches based on the called bi-encoder
design [22]. The bi-encoder independently encodes queries and documents, creating separate vector representations
before calculating similarity. An advantage of this approach is that it can be initialized ‘offline’: document embeddings
can be precomputed, leaving only the query embedding being calculated at search time, reducing latency.
The hybrid search technique aims to leverage the best of both sparse and dense search approaches. Given a question,
both searches are conducted in parallel, generating two lists of candidate documents to answer it. The challenge then
lies in combining the two results in the best possible way, ensuring that the final hybrid list surpasses the individual
searches. Essentially, we can conceptualize it as a voting system, where each searcher casts a vote on the relevance of
a document to a given query, and in the end, the opinions are combined to produce a better result.
The multi-stage search architecture is based on the retrieve-and-rerank pipeline. In the first stage, a retriever with good
recall is typically used to perform an initial filtering of the documents to be returned. From this narrowed-down list,
these candidate documents are then sent to a second stage, which involves higher computational complexity, to rerank
them and enhance the final effectiveness of the system.
Next, we provide more details about each retriever used.



Source: data\tc17_2312.15883v2\referenced_papers\[8]_2401.05856.pdf (Page 0):

Seven Failure Points When Engineering a Retrieval Augmented
Generation System
Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek
{scott.barnett,stefanus.kurniawan,srikanth.thudumu,zach.brannelly,mohamed.abdelrazek}@deakin.edu.au
Applied Artificial Intelligence Institute
Geelong, Australia
ABSTRACT
Software engineers are increasingly adding semantic search capabil-
ities to applications using a strategy known as Retrieval Augmented
Generation (RAG). A RAG system involves finding documents that
semantically match a query and then passing the documents to a
large language model (LLM) such as ChatGPT to extract the right
answer using an LLM. RAG systems aim to: a) reduce the problem
of hallucinated responses from LLMs, b) link sources/references
to generated responses, and c) remove the need for annotating
documents with meta-data. However, RAG systems suffer from lim-
itations inherent to information retrieval systems and from reliance
on LLMs. In this paper, we present an experience report on the
failure points of RAG systems from three case studies from separate
domains: research, education, and biomedical. We share the lessons
learned and present 7 failure points to consider when designing a
RAG system. The two key takeaways arising from our work are: 1)
validation of a RAG system is only feasible during operation, and
2) the robustness of a RAG system evolves rather than designed in
at the start. We conclude with a list of potential research directions
on RAG systems for the software engineering community.
CCS CONCEPTS
• Software and its engineering →Empirical software valida-
tion.
KEYWORDS
Retrieval Augmented Generation, RAG, SE4AI, Case Study
ACM Reference Format:
Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mo-
hamed Abdelrazek . 2024. Seven Failure Points When Engineering a Retrieval
Augmented Generation System. In Proceedings of 3rd International Confer-
ence on AI Engineering — Software Engineering for AI (CAIN 2024). ACM,
New York, NY, USA, 6 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
The new advancements of Large Language Models (LLMs), includ-
ing ChatGPT, have given software engineers new capabilities to
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CAIN 2024, April 2024, Lisbon, Portugal
© 2024 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn
build new HCI solutions, complete complex tasks, summarise docu-
ments, answer questions in a given artefact(s), and generate new
content. However, LLMs suffer from limitations when it comes
to up-to-date knowledge or domain-specific knowledge currently
captured in company’s repositories. Two options to address this
problem are: a) Finetuning LLMs (continue training an LLM using
domain specific artifacts) which requires managing or serving a
fine-tuned LLM; or b) use Retrieval-Augmented Generation (RAG)
Systems that rely on LLMs for generation of answers using existing
(extensible) knowledge artifacts. Both options have pros and cons
related to privacy/security of data, scalability, cost, skills required,
etc. In this paper, we focus on the RAG option.
Retrieval-Augmented Generation (RAG) systems offer a com-
pelling solution to this challenge. By integrating retrieval mecha-
nisms with the generative capabilities of LLMs, RAG systems can
synthesise contextually relevant, accurate, and up-to-date informa-
tion. A Retrieval-Augmented Generation (RAG) system combines
information retrieval capabilities, and generative prowess of LLMs.
The retrieval component focuses on retrieving relevant information
for a user query from a data store. The generation component fo-
cuses on using the retrieved information as a context to generate an
answer for the user query. RAG systems are an important use case
as all unstructured information can now be indexed and available
to query reducing development time no knowledge graph creation
and limited data curation and cleaning.
Software engineers building RAG systems are expected to pre-
process domain knowledge captured as artifacts in different formats,
store processed information in appropriate data store (vector data-
base), implement or integrate the right query-artifact matching
strategy, rank matched artifacts, and call the LLMs API passing in
user queries and context documents. New advances for building
RAG systems are constantly emerging [8, 12] but how they relate
and perform for a specific application context has to be discovered.
In this work we present the lessons learned and 7 failure points
arising from 3 case studies. The purpose of this paper is to provide
1) a reference to practitioners and 2) to present a research road
map for RAG systems. To the best of our knowledge, we present
the first empirical insight into the challenges with creating robust
RAG systems. As advances in LLMs continue to take place, the
software engineering community has a responsibility to provide
knowledge on how to realise robust systems with LLMs. This work
is an important step for robustness in building RAG systems.
Research questions for this work include:
•What are the failure points that occur when engineering a RAG
system? (section 5) We present an empirical experiment using
the BioASQ data set to report on potential failure points. The
experiment involved 15,000 documents and 1000 question
arXiv:2401.05856v1  [cs.SE]  11 Jan 2024



Source: data\tc17_2312.15883v2\referenced_papers\[93]_2311.09210.pdf (Page 0):

Accepted to EMNLP 2024
Chain-of-Note: Enhancing Robustness in Retrieval-Augmented
Language Models
Wenhao Yu, Hongming Zhang, Xiaoman Pan, Peixin Cao,
Kaixin Ma, Jian Li, Hongwei Wang, Dong Yu
Tecent AI Lab
wenhaowyu@global.tencent.com
Abstract
Retrieval-augmented language model (RALM)
represents a significant advancement in miti-
gating factual hallucination by leveraging ex-
ternal knowledge sources. However, the reli-
ability of the retrieved information is not al-
ways guaranteed, and the retrieval of irrele-
vant data can mislead the response generation.
Moreover, standard RALMs frequently neglect
their intrinsic knowledge due to the interference
from retrieved information. In instances where
the retrieved information is irrelevant, RALMs
should ideally utilize their intrinsic knowledge
or, in the absence of both intrinsic and retrieved
knowledge, opt to respond with "unknown" to
avoid hallucination. In this paper, we intro-
duces CHAIN -OF-NOTE (CON), a novel ap-
proach to improve robustness of RALMs in
facing noisy, irrelevant documents and in han-
dling unknown scenarios. The core idea of
CON is to generate sequential reading notes
for each retrieved document, enabling a thor-
ough evaluation of their relevance to the given
question and integrating this information to for-
mulate the final answer. Our experimental re-
sults show that GPT-4, when equipped with
CON, outperforms the CHAIN -OF-THOUGHT
approach. Besides, we utilized GPT-4 to cre-
ate 10K CON data, subsequently trained on
LLaMa-2 7B model. Our experiments across
four open-domain QA benchmarks show that
fine-tuned RALMs equipped with CON signifi-
cantly outperform standard fine-tuned RALMs.
1 Introduction
Retrieval-augmented language models (RALMs)
represent a novel framework that significantly ad-
vances large language models (Touvron et al., 2023;
OpenAI, 2023) by addressing key limitations such
as reducing factual hallucinations (Ji et al., 2023;
Zhang et al., 2023a), injecting up-to-date knowl-
edge in a plug-and-play manner (Dhingra et al.,
2022; Vu et al., 2023), and enhancing domain-
specific expertise (Li et al., 2023; Qin et al., 2023).
Wikipedia: … "It Must Have Been Love" is a song written by Per Gessle and performed by the Swe-dish pop duo Roxette. The power ballad became the duo's third number one hit in United States.
Wikipedia: … "It Must Be Love" is a song written and originally recorded in 1971 by Labi Siffre. It was also recorded by ska/pop band Madness in 1981.
The answer is Labi Siffre. 
The first passage confirms that Roxette is the performer of “It Must Have Been Love.” How-ever, the second passage refers to “It Must Be Love,” which is a different song andirrelevant to the question. Thus, the answer is Roxette. RALMwithCoN
Who performed "It Must Have Been Love”?
RALMw/oCoN
Figure 1: Compared with the current RALMs, the core
idea behind CHAIN -OF-NOTE (CON) is to generate
sequential reading notes for the retrieved documents,
ensuring a systematic assessment of their relevance to
the input question before formulating a final response.
These enhancements primarily stem from integrat-
ing large language models (LLMs) with external
knowledge sources (Guu et al., 2020; Lewis et al.,
2020; Borgeaud et al., 2022; Shi et al., 2023c). In
a typical RALM setup, a query is first processed
by a retriever that searches a vast evidence corpus
for pertinent documents. A reader then examines
these documents, extracting useful information and
formulating the final output answer.
However, there exist several issues with the cur-
rent RALM framework. First, there is no guarantee
that the information retrieval (IR) system will al-
ways yield the most pertinent or trustworthy infor-
mation. The retrieval of irrelevant data can lead to
misguided responses (Shi et al., 2023a; Yoran et al.,
2023), and potentially causing the model to over-
look its inherent knowledge, even when it possesses
adequate information to address the query (Mallen
arXiv:2311.09210v2  [cs.CL]  3 Oct 2024



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 18):

Published as a conference paper at ICLR 2024
Table 10: Details of the evaluation datasets.
Domain Dataset # of Samples
Factual FEVER 1000
Factual HotpotQA 308
Factual FeTaQA 500
Medical MedMCQA 146
Physics MMLU Physics 253
Biology MMLU Biology 454
Table 11: Evaluation of domain selection performance.
Domain Precision Recall F1
Factual 96.0% 96.0% 96.0%
Medical 94.3% 96.1% 95.2%
Physics 89.9% 100.0% 94.6%
Biology 100.0% 92.8% 96.2%
E E VALUATION DATASETS
The evaluation datasets collect datasets from four different domains, including factaul, medical,
physics, and biology. Details of the dataset are in Table 10. We adopt exact match as the evaluation
metrics for HotpotQA, which is a more strict evaluation.
F A NALYSIS
F.1 D OMAIN SELECTION
As CoK relies on selecting relevant knowledge domains, it is important that the domain selection
step is of high quality. Hence, we randomly sample 50 questions from each domain and compare
the predicted domains with our manually annotated domains. As each question may be relevant for
more than one domain, we report the precision, recall, and F1 scores. As shown in Table 11, we find
that while the domain selection is not perfect, the overall F1 scores are more than 94% across all the
domains. Hence, we believe that the current domain selection process is adequate.
F.2 M ODELS OF ADAPTIVE QUERY GENERATOR
Table 12 demonstrates the performances of ChatGPT and instruction-tuned LlaMA-2-7B on SQL
and SPARQL generation. SPARQL is evaluated on 4,779 samples from LC-quad and KQA-pro.
SQL is evaluated on 15,900 samples from WikiSQL and we use the exact-match metric to evaluate
the generated queries with gold queries.
G D ISCUSSION OF LIMITATIONS
Knowledge Sources As CoK relies on external knowledge sources, there are some ethical im-
plications. Notably, LLMs using CoK may still generate inaccurate information if the knowledge
sources contain unreliable information. Hence, this could cause misinformation or manipulation of
public opinion. Another limitation is that there may be conflict between different knowledge sources
in theory. To address the two limitations, we selected authoritative knowledge sources such as Wiki-
data which are unlikely to contain inaccurate or conflicting information. As a result, the risk from
the knowledge sources are reduced.
Knowledge Retrieval On the other hand, CoK may not produce useful outputs if the knowledge
retrieval step is unable to retrieve facts that are relevant to the given question. However, we believe
19



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 3):

Published as a conference paper at ICLR 2024
Table 1: An example of generated query, execution results, and formatted knowledge for rationales
of SPARQL. Knowl. stands for knowledge.
SPARQL
Rationale Souleyman San´e’s son, Leroy San´e, is a professional football player.Generated querySELECT ?answer WHERE{wd:/Souleymane San´e/ wdt:/child/ ?answer .}Execution resultsLeroy San´eFormatted knowl.The fact entity of the sentence “Souleyman San´e’s son, Leroy San´e, is a professional football player” is Leroy San´e.
Reasoning Generation Previous studies have demonstrated the importance of intermediate ratio-
nales for LLMs to answer complex reasoning questions (Wei et al., 2022). In this work, we utilize the
few-shot chain-of-thought (CoT) prompting to generate rationales (Wei et al., 2022). Moreover, we
employ the self-consistency method (Wang et al., 2023) to determine whether external knowledge is
necessary to answer the question. In sampling various reasoning paths and answers, self-consistency
is found to be highly correlated with accuracy. Thus, predictions with high consistency are preserved
without modification. Only questions with “uncertain” answers, i.e., their consistency falls below
a specified threshold, undergo further stages of processing. Such filtering technique is found to be
useful in identifying incorrect predictions by previous works (Yao et al., 2023; Zhao et al., 2023c).
Knowledge Domain Selection To ensure the retrieval of the most pertinent knowledge to the
question, we introduce the knowledge domain selection step. As shown in Figure 2, CoK inte-
grates four distinct knowledge domains: factual, medical, physics, and biology. Moreover, multiple
domains can be identified for answering a single question. To illustrate, when presented with the
question “Who proposed the theory which explains the cause of tides?”, both physics (gravitational
force of the Moon causes tides) and factual (Isaac Newton first proposed the universal gravitation
and explained tidal forces exerted by celestial bodies) domain knowledge are required to answer the
question. The knowledge domain selection is completed through in-context learning.
2.2 D YNAMIC KNOWLEDGE ADAPTING STAGE
Once the preliminary rationales and the identified knowledge domains are obtained, the next stage
is dynamic knowledge adapting, i.e., rectifying rationales based on the retrieved knowledge. To
minimize error propagation, CoK conducts knowledge retrieval and correction of the rationales se-
quentially. The preceding corrected rationales are used to generate the next rationale, which then
undergoes the same knowledge retrieval and correction step.
Knowledge Retrieval Upon identifying relevant domains to the question in the reasoning prepa-
ration stage, all knowledge sources within these domains are utilized for knowledge retrieval. The
knowledge retrieval consists of two steps: query generation and execution.
A) Query Generation Depending on the nature of the knowledge sources, each source is linked
to the most appropriate query language, which could either be structured, such as SPARQL or SQL,
or unstructured, such as natural language sentences. For instance, Wikidata is linked to the SPARQL
query as it consists of knowledge graphs. The flashcard source is linked to the natural sentence query
as it takes the format of natural sentence pairs. An example of generated queries for SPARQL is
shown in Table 1. 1 For instance, given a sentence “Souleyman San ´e’s son, Leroy San ´e, is a pro-
fessional football player”, a SPARQL query, “SELECT ?answer WHERE {wd:/Souleymane
San´e/ wdt:/child/ ?answer.}”, is generated to retrieve relevant knowledge from Wiki-
data. To facilitate the generation of both structured and unstructured queries, an adaptive query gen-
erator (AQG) is used. AQG is a versatile plug-in component, which can be either a tailor-finetuned
model or an off-the-shelf LLM. Details of AQG will be elaborated in Section 3.
B) Query Execution Once the queries are generated, the subsequent step is their execution to
acquire and convert the knowledge into formatted knowledge (see Table 1). A specialized method
is devised to execute queries and format the results for each query language. For SPARQL queries,
entity linking is initially performed to substitute entity spans with IDs, followed by acquiring results
by invoking the API of wikidata.org. Regarding SQL queries, they are executed directly to
1Examples of generated queries for each querying language are in Appendix D.5.
4



### Claim 4/36

#### Claim Text
In addition, the excessive redundant knowledge can lead to a waste of token resources, and 1https://www.youtube.com/watch?v=ahnGLM-RC1Y arXiv:2312.15883v2 [cs.CL] 19 Apr 2024 Conference’17, July 2017, Washington, DC, USA Jiang, Zhang and Xu et al. the response speed of LLMs will drop sharply, which adversely damages the performance in real-world applications [17].

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 9):

10  
using KG-RAG framework (T-test, p-value < 0.0001, t-statistic = -47.7, N = 1000) but not in the prompt-based approach.  
 Fig 3. LLM performance on True/False and MCQ datasets Performance (Accuracy) distributions of LLMs on (A) True/False and (B) MCQ datasets. Blue distribution denotes the performance using prompt-based approach and red distribution denotes the performance using KG-RAG based approach. Black vertical dashed line indicates the mean value of the distribution. The higher the value, the better the performance.



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 2):

3  
Introduction Large language models (LLM) have shown impressive performance in solving complex tasks across various domains that involve language modeling and processing.(Zhao et al. 2023) LLMs are pre-trained on a large corpora of text data in a self-supervised learning framework which can be either Masked Language Modeling (e.g. BERT like models)(Devlin et al. 2018; Liu et al. 2019) or Auto-Regressive framework (GPT like models)(Brown et al. 2020; Luo et al. 2022). This pre-training encodes knowledge about the language in the model parameters. Similar to the transfer learning approach commonly used in deep neural networks, this implicit knowledge can be refined through supervised training to excel in a range of domain-specific tasks.(Wei et al. 2021; Luo et al. 2022) Nevertheless, the “implicit representation” of knowledge in LLM has also been shown to generate non-factual information despite linguistically coherent answers (i.e. “hallucination”) as a response to the input prompt.(Maynez et al. 2020; Raunak, Menezes and Junczys-Dowmunt 2021; Ji et al. 2022) This issue poses a significant challenge for the adoption of LLM models in domains with stringent requirements for accuracy, such as biomedicine.  Various strategies have been introduced to address hallucinations in LLMs. One such solution involves the utilization of domain-specific data for pre-training the LLM, rather than relying on generic text corpora. This approach has led to the creation of models such as PubMedBERT(Yu Gu Microsoft Research, Redmond, WA et al. 2021), BioBERT(Lee et al. 2019), BlueBERT(Lee et al. 2019; Peng, Yan and Lu 2019), SciBERT(Beltagy, Lo and Cohan 2019), ClinicalBERT(Huang, Altosaar and Ranganath 2019), BioGPT(Luo et al. 2022), Med-PaLM(Singhal et al. 2023), and BioMedGPT.(Luo et al. 2023) However, pre-training an LLM from scratch imposes a significant computational cost and time overhead to attain the desired human-like performance. An alternative approach, known as prompt tuning, was recently proposed as a means to enhance LLM performance, for instance through the use of zero-



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 8):

9  
we introduced a slight perturbation to the test dataset by converting the entity names to lowercase (Fig 2B insight). We observed a significant decrease in the retrieval accuracy of Cypher-RAG to 0% (indicating failure to retrieve any context from the graph). This mainly occurs because Cypher-RAG utilizes precise matching of the entity keywords provided in the user prompt to formulate the Cypher query for extracting graph context. On the otherhand, KG-RAG maintained its retrieval accuracy at 97% (i.e. robust to the input perturbation, Fig 2B top). This is because KG-RAG employs a semantic embedding approach to extract graph context, which enhances its ability to effectively handle various representations of entities within user prompts. Next, we analyzed the total token usage of each framework for generating the response for the same test dataset (Fig 2B bottom). We found that Cypher-RAG had an average token usage of 8006 tokens whereas KG-RAG had an average token usage of 3693 tokens (Fig 2B bottom). This represents a 53.9% reduction in the token usage by KG-RAG compared to Cypher-RAG which points to a significant cost-effective retrieval ability of KG-RAG.           Performance on True/False and MCQ datasets Fig 3 shows bootstrap distributions of performance (accuracy) of the three LLMs using prompt-based and KG-RAG framework on True/False (Fig 3A) and MCQ (Fig 3B) datasets. Table 1 summarizes the performance of the three LLMs across these datasets. We observed a consistent performance enhancement for the LLM models under KG-RAG framework on both True/False and MCQ datasets (Table 1). KG-RAG significantly elevated the performance of Llama-2 by approximately 71% from its initial level (0.31±0.03 to 0.53±0.03) on the more challenging MCQ dataset (Table 1). Intriguingly, we also observed a small but significant drop in the performance of GPT-4 model (0.74±0.03) compared to GPT-3.5-Turbo model (0.79±0.02) on MCQ dataset



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 22):

23  
performance of LLMs on True/False and MCQ datasets, 150 questions were randomly sampled with replacement 1000 times (using bootstrapping). The accuracy metric was then calculated for each sampling iteration, resulting in a performance distribution. Cypher-RAG  Cypher-RAG is a technique utilized for retrieving context associated with a node in a Neo4j graph database.(Bratanic 2023) This context can then be leveraged to generate information about the node in natural language using a LLM. The method involves explicitly embedding the schema of the graph into the input prompt, directing the LLM to generate a structured Cypher query based on this schema. The resulting Cypher query is used to make a call to the Neo4j database, and the returned information is utilized as context to respond to the user's prompt. This methodology is integrated into the LangChain python library as GraphCypherQAChain class.(Bratanic 2023) An advantage of this approach is that it allows for the creation of Cypher queries directly from natural language, eliminating the need for users to have the knowledge of Cypher query syntax. However, our analysis revealed certain limitations of this approach. We found that the explicit embedding of the graph schema restricts the input token space and increases token usage for this method. As the complexity of the graph schema increases, users may need to utilize LLMs with longer context window sizes for optimal performance. Additionally, we demonstrated that this method can be sensitive to how the prompt is formulated. Even slight perturbations to the prompt can lead to incorrect Cypher queries and subsequently impact downstream generative processes.



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 13):

14  
To conduct robust benchmarking, we curated datasets that underwent review by domain experts. Given the swift progress in LLM research, we believe that such rigorously vetted datasets could serve as valuable resources not only for evaluating KG-RAG but also for assessing other ongoing LLM endeavors in biomedicine. In our benchmarking analysis, we found an enhancement in LLM performance as a function of the model size in terms of the number of parameters. Intriguingly, with the KG-RAG framework the performance of GPT-4 on the MCQ dataset, despite its model size, dropped significantly compared to that of the GPT-3.5-Turbo on the MCQ dataset. In fact, the performance of GPT-3.5-Turbo under KG-RAG framework was on par with that of the GPT-4 model on True/False datasets. These results suggest that at present, GPT-3.5 may be a better context listener than GPT-4. In fact, a recent study compared the March 2023 version of GPT-4 with the June 2023 version, shedding light on the drift in the LLM performance over time.(Chen, Zaharia and Zou 2023) The study revealed that, as time progressed, GPT-4 exhibited a reduced tendency to adhere to user instructions.(Chen, Zaharia and Zou 2023) In contrast, there was no consistent alteration observed in the instruction-following behavior of GPT-3.5 over time.(Chen, Zaharia and Zou 2023)  When the proprietary GPT models were compared to the open-source Llama-2-13b model, they showed a narrow margin in performance on the biomedical True/False dataset. However, on the more challenging MCQ dataset, Llama-2 initially demonstrated lower performance compared to GPT models. Interestingly, the KG-RAG framework provided a substantial performance boost to Llama-2, improving its performance by ~71% from the baseline. Despite this boost that narrowed the performance gap, the performance of Llama-2 remained lower than that of the GPT models. This suggests that the KG-RAG framework has the potential to capitalize the intrinsic context comprehension capabilities of open-source pretrained models like Llama-2, making them



### Claim 5/36

#### Claim Text
To cope with these challenges, we put forward the Hypothesis Knowledge Graph Enhanced (HyKGE) framework, a novel method based on the hypothesis output module ( HOM) [18] to explore, locate, and prune search directions for accurate and reliable LLMs responses in pre-retrieval phase and greatly preserve the relevance and diversity of search results at in post-retrieval phase. i) Specifically, in the pre-retrieval phase, our key idea is that the zero-shot capability and rich knowledge of LLMs can compensate for the incompleteness of user queries, facilitating alignment with highquality external knowledge.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 7):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
the application of Multiple Negative Ranking Loss, as presented in the article "Efficient Natural Language Response
Suggestion for Smart Reply," Section 4.4 [26]. However, given our current focus on simplicity, we will reserve the
exploration of this technique for future work. At this moment, we choose to utilize the Mean Squared Error (MSE)
Loss.
For the fine-tuning stage, it is necessary to have two types of samples:
• positives: [question i, chunki, label = 1]
• negatives: [question i, chunkj, label = −1], for i ̸= j
Often, as is the case with our dataset, only positive examples are available. However, through a simple and random
shuffling, it is possible to generate negative examples. Demonstrating confidence in transfer learning, we found that
a few examples were sufficient. Our final dataset consisted of approximately 400 examples, maintaining a 1 : 3ratio
between positive and negative examples.
The hyperparameters that exert the most significant impact on performance include the learning rate, batch size, and
the number of dimensions in the projection matrix. The ADA-002 model has 1536 dimensions, and the projection
matrix is of size 1536 × N, where N ∈ 1024, 2048, 4096. In our experiments, we observed that 2048 dimensions
resulted in the best accuracy.
This type of fine-tuning requires low GPU resources, with a training time of approximately 5 minutes using the A100
GPU. The model itself is straightforward, consisting of a matrix with dropout (to mitigate overfitting), followed by the
hyperbolic tangent activation function, which provided additional accuracy gains in the training set.
Figure 6: Cosine similarity of positive and negative classes in the ADA-002 embedding; note the significant overlap
between the classes. Test accuracy (before training): 69.5%
Figure 7: Cosine similarity of positive and negative classes in the customized embedding; the intersection between the
classes is minimal. Test accuracy (after training): 84.3%
When analyzing the cosine similarity between positive and negative classes, we can observe the "shadow" shared by
the histograms. In an ideal scenario, we desire the classes to be disjoint to ensure a clear definition of space. Figure 6
illustrates a significant shadow in the embedding before training, while Figure 7 shows the result after training. Both
graphs are derived from the test set. Test accuracy also improved, leading to a better dense representation.
5.1.4 Hybrid Search
As stated before, hybrid search is applied when is necessary to combine results from two or more retrieval methods.
A widely used algorithm to address this type of problem is known as Reciprocal Rank Fusion (RRF). For a document



Source: data\tc17_2312.15883v2\referenced_papers\[86]_2312.14335.pdf (Page 5):

T able 3: Performance comparison between difference choice s of hyperparameter α ,
recall that α = 0 reduces to vanilla decoding. The reported numbers are aver aged
over eight models.
Datasets α value ROUGE-1 ROUGE-2 ROUGE-L BER TScore-P FactKB
Dbpedia
α =0.0 15.5 4.1 13.8 71.2 32.7
α =0.15 14.9 3.8 13.3 71.1 35.1
α =0.3 14.7 3.6 13.1 70.1 38.1
α =0.5 14.5 3.6 13.0 70.9 38.7
PubMedQA
α =0.0 29.1 9.7 24.0 73.3 89.1
α =0.15 28.7 9.6 23.8 73.2 90.6
α =0.3 28.4 9.4 23.4 73.0 91.7
α =0.5 27.9 9.1 23.1 72.7 93.3
CNN Dailymail
α =0.0 29.1 9.6 25.8 73.9 85.2
α =0.15 30.4 10.9 27.1 74.3 87.2
α =0.3 30.7 11.2 27.4 74.4 88.4
α =0.5 30.7 11.2 27.3 74.4 87.7
XSUM
α =0.0 25.2 7.0 19.7 74.5 50.1
α =0.15 25.4 7.2 19.8 74.8 55.7
α =0.3 25.3 7.2 19.9 74.9 58.0
α =0.5 24.8 6.9 19.4 74.9 58.1
due to the design choices in building the datasets. On the new s summarization datasets, setting α = 0.3
yields an overall better performance.
Reporting Decoding Speed. Previously we have analyzed the additional complexity in te rms of FLOPs
of CAD. Now we take a look at the difference in decoding speed o n a real-world dataset. The experiment
is conducted on a server with a single Nvidia RTX 3090 GPU, CUD A version 12.1, and PyT orch version
2.1.0. W e implement two forward passes, ﬁrst with pθ (yt |c, x, y<t ), and second with pθ (yt |x, y<t ); another
option is packing two computation into one batch which will i ncrease CUDA memory usage.
On CNN Dailymail dataset, vanilla decoding with FP16 and bat ch size=1 leads to a decoding speed of
0.042 second/token, while the decoding speed of CAD with FP1 6 and batch size=1 is 0.077 second/token
with two forward passes. This suggests while CAD can improve generation quality , it also slows down
decoding speed or requires more CUDA memory .
5 Related W ork
Search as an Information-seeking Process.[Mar06] categorize search into three two different informa -
tion seeking processes— Information Lookup and Exploratory Search . Information lookup refers to the user
looking for simple facts or asking factoid questions, while exploratory search/search to learn often in-
volves multiple iterations. Different approaches have bee n explored to reduce users’ cognitive loads in the
information-seeking process. For example, Perplexity AI 3 transfers web search queries into QA-style ques-
tions, and cites sources in the answers to improve veriﬁabil ity . Snippets [BC14] provide short previews for
each webpage to allow users to locate relevant information w ithout examining the webpage. Counterfac-
tual explanations [Xu+23] suggest counterfactual queries as explanations and demonstrate search sessions’
effectiveness can be improved.
Hallucination in Natural Language Generation.Ji et al. [Ji+23] present a survey of hallucination in natu-
ral language generation. There are mainly two types of hallu cinations, namely intrinsic hallucination and
extrinsic hallucination. Intrinsic hallucination refers to the generated output that contradicts the source
content, whereas extrinsic hallucination refers to the gen erated output that cannot be veriﬁed from the
3 https://www .perplexity .ai/
6



Source: data\tc17_2312.15883v2\referenced_papers\[18]_2212.10496.pdf (Page 5):

Swahili Korean Japanese Bengali
w/o relevance judgement
BM25 38.9 28.5 21.2 41.8
mContriever 38.3 22.3 19.5 35.3
HyDE 41.7 30.6 30.7 41.3
w/ relevance judgement
mDPR 7.3 21.9 18.1 25.8
mBERT 37.4 28.1 27.1 35.1
XLM-R 35.1 32.2 24.8 41.7
mContrieverFT 51.2 34.2 32.4 42.3
Table 3: MRR@100 on Mr.Tydi. Best performing w/o
relevance and overall system(s) are marked bold.
under-speciﬁcation of the instruction; more elabo-
rative instructions may help.
4.4 Multilingual Retrieval
Multilingual setup poses several additional chal-
lenges to HyDE. The small-sized contrastive en-
coder gets saturated as the number of languages
scales (Conneau et al., 2020; Izacard et al., 2021).
Meanwhile, our generative LLM faces an opposite
issue: with languages of not as high resource as
English or French, the high capacity LLM can get
under-trained (Hoffmann et al., 2022).
Nevertheless, in Table 3, we still ﬁnd HyDE
able to improve the mContriever model. It can
outperform non-Contriever models ﬁne-tuned on
and transferred from MS-MARCO. On the other
hand, we do observe some margins between HyDE
and ﬁne-tuned mContrieverFT. Since HyDE and
mContrieverFT use similar contrastive encoders,
we hypothesize this is because the non-English lan-
guages we considered are under-trained in both
pre-training and instruction learning stages.
5 Analysis
The generative LLM and contrastive encoder make
up the backbone of HyDE. In this section, we study
the effect of changing their realizations. In partic-
ular, we consider smaller language models (LM)
and ﬁne-tuned encoders. We conduct our studies
on TREC DL19/20.
5.1 Effect of Different Generative Models
In Table 4, we show HyDE using other
instruction-following language models. In
particular, we consider a 52-billion Cohere
model ( command-xlarge-20221108) and a
11-billion FLAN model ( FLAN-T5-xxl; Wei
et al. (2022)). 2 Generally, we observe that all
2Model sizes are from https://crfm.stanford.edu/
helm/v1.0/?models.
Model DL19 DL20
Contriever 44.5 42.1
ContrieverFT 62.1 63.2
HyDE
w/ Contriever
w/ Flan-T5 (11b) 48.9 52.9
w/ Cohere (52b) 53.8 53.8
w/ GPT (175b) 61.3 57.9
w/ ContrieverFT
w/ Flan-T5 (11b) 60.2 62.1
w/ Cohere (52b) 61.4 63.1
w/ GPT (175b) 67.4 63.5
Table 4: NDCG@10 on TREC DL19/20. Effect
of changing different instruction LMs and using ﬁne-
tuned encoder. Best w/o relevance and overall models
are marked bold.
models bring improvement to the unsupervised
Contriever, with larger models bringing larger
improvements. At the time when this paper is
written, the Cohere model is still experimental
without much detail disclosed. We can only
tentatively hypothesize that training techniques
may have also played some role in the performance
difference.
5.2 HyDE with Fine-tuned Encoder
To begin with, HyDE with ﬁne-tuned encoder is
not the intended usage: HyDE is more powerful
and irreplaceable when few relevance labels are
present. Here we are interested to ﬁnd out if
and how HyDE embedding can affect ﬁne-tuned en-
coders. In Table 4, we see that less powerful instruc-
tion LMs can negatively impact the overall perfor-
mance of the ﬁne-tuned retriever. (To remind our
readers, ContrieverFT is in-domain supervisedly
ﬁne-tuned for TREC DL19/20). The performance
degradations remain small. On the other hand, we
also observe the InstructGPT model able to fur-
ther bring up the performance, especially on DL19.
This suggests that there may still exist certain fac-
tors not captured by the ﬁne-tuned encoder but only
by the generative model.
6 Conclusion
At the end of the paper, we encourage the readers
to take a moment and reﬂect on the HyDE model.
Compare it to some of the other recently seen re-
trievers or re-ranker. These other models probably
differ in their architecture, training method, and/or
task, but probably all of them involve modeling
relevance scores between a pair of query and docu-



Source: data\tc17_2312.15883v2\referenced_papers\[19]_2212.10496.pdf (Page 5):

Swahili Korean Japanese Bengali
w/o relevance judgement
BM25 38.9 28.5 21.2 41.8
mContriever 38.3 22.3 19.5 35.3
HyDE 41.7 30.6 30.7 41.3
w/ relevance judgement
mDPR 7.3 21.9 18.1 25.8
mBERT 37.4 28.1 27.1 35.1
XLM-R 35.1 32.2 24.8 41.7
mContrieverFT 51.2 34.2 32.4 42.3
Table 3: MRR@100 on Mr.Tydi. Best performing w/o
relevance and overall system(s) are marked bold.
under-speciﬁcation of the instruction; more elabo-
rative instructions may help.
4.4 Multilingual Retrieval
Multilingual setup poses several additional chal-
lenges to HyDE. The small-sized contrastive en-
coder gets saturated as the number of languages
scales (Conneau et al., 2020; Izacard et al., 2021).
Meanwhile, our generative LLM faces an opposite
issue: with languages of not as high resource as
English or French, the high capacity LLM can get
under-trained (Hoffmann et al., 2022).
Nevertheless, in Table 3, we still ﬁnd HyDE
able to improve the mContriever model. It can
outperform non-Contriever models ﬁne-tuned on
and transferred from MS-MARCO. On the other
hand, we do observe some margins between HyDE
and ﬁne-tuned mContrieverFT. Since HyDE and
mContrieverFT use similar contrastive encoders,
we hypothesize this is because the non-English lan-
guages we considered are under-trained in both
pre-training and instruction learning stages.
5 Analysis
The generative LLM and contrastive encoder make
up the backbone of HyDE. In this section, we study
the effect of changing their realizations. In partic-
ular, we consider smaller language models (LM)
and ﬁne-tuned encoders. We conduct our studies
on TREC DL19/20.
5.1 Effect of Different Generative Models
In Table 4, we show HyDE using other
instruction-following language models. In
particular, we consider a 52-billion Cohere
model ( command-xlarge-20221108) and a
11-billion FLAN model ( FLAN-T5-xxl; Wei
et al. (2022)). 2 Generally, we observe that all
2Model sizes are from https://crfm.stanford.edu/
helm/v1.0/?models.
Model DL19 DL20
Contriever 44.5 42.1
ContrieverFT 62.1 63.2
HyDE
w/ Contriever
w/ Flan-T5 (11b) 48.9 52.9
w/ Cohere (52b) 53.8 53.8
w/ GPT (175b) 61.3 57.9
w/ ContrieverFT
w/ Flan-T5 (11b) 60.2 62.1
w/ Cohere (52b) 61.4 63.1
w/ GPT (175b) 67.4 63.5
Table 4: NDCG@10 on TREC DL19/20. Effect
of changing different instruction LMs and using ﬁne-
tuned encoder. Best w/o relevance and overall models
are marked bold.
models bring improvement to the unsupervised
Contriever, with larger models bringing larger
improvements. At the time when this paper is
written, the Cohere model is still experimental
without much detail disclosed. We can only
tentatively hypothesize that training techniques
may have also played some role in the performance
difference.
5.2 HyDE with Fine-tuned Encoder
To begin with, HyDE with ﬁne-tuned encoder is
not the intended usage: HyDE is more powerful
and irreplaceable when few relevance labels are
present. Here we are interested to ﬁnd out if
and how HyDE embedding can affect ﬁne-tuned en-
coders. In Table 4, we see that less powerful instruc-
tion LMs can negatively impact the overall perfor-
mance of the ﬁne-tuned retriever. (To remind our
readers, ContrieverFT is in-domain supervisedly
ﬁne-tuned for TREC DL19/20). The performance
degradations remain small. On the other hand, we
also observe the InstructGPT model able to fur-
ther bring up the performance, especially on DL19.
This suggests that there may still exist certain fac-
tors not captured by the ﬁne-tuned encoder but only
by the generative model.
6 Conclusion
At the end of the paper, we encourage the readers
to take a moment and reﬂect on the HyDE model.
Compare it to some of the other recently seen re-
trievers or re-ranker. These other models probably
differ in their architecture, training method, and/or
task, but probably all of them involve modeling
relevance scores between a pair of query and docu-



Source: data\tc17_2312.15883v2\referenced_papers\[93]_2311.09210.pdf (Page 3):

the pertinence of each document to the query but
also pinpoints the most reliable information and
resolves conflicting information. This approach
effectively filters out irrelevant or less trustworthy
content, leading to responses that are both more
accurate and contextually relevant.
Given an input question x and k retrieved doc-
uments [d1, · ··, dk], the model aims to gener-
ate textual outputs comprising multiple segments
[yd1 , · ··, ydk , y]. Here, ydi signifies the tokens for
the i-th segment, representing the reading note for
the corresponding document di, as shown in Fig-
ure 2. After generating individual reading notes,
the model synthesizes the information to create a
consolidated final response y. The implementation
of the CHAIN -OF-NOTE (CON) involves three key
steps: (1) designing the notes ydi , (2) collecting the
data, and (3) training the model.
2.3.1 C HAIN -OF-NOTE Format Design
The framework primarily constructs three types of
reading notes, as shown in Figure 2 , based on the
relevance of the retrieved documents to the input
question: First, when a document directly answers
the query, the model formulates the final response
based on this relevant information, as shown in Fig-
ure 2(a). Second, if the retrieved document does
not directly answer the query but provides useful
context, the model leverages this information along
with its inherent knowledge to deduce an answer,
as shown in Figure 2(b). Third, in cases where the
retrieved documents are irrelevant, and the model
lacks sufficient knowledge to answer, it defaults
to responding with “unknown", as shown in Fig-
ure 2(c). This nuanced approach mirrors human
information processing, striking a balance between
direct retrieval, inferential reasoning, and the ac-
knowledgment of knowledge gaps.
2.3.2 Data Collection and Model Training
To equip the model with the ability to generate such
reading notes, it’s essential to gather appropriate
training data. Manual annotation for each reading
note is resource-intensive, so we employ a state-
of-the-art language model – GPT-4 – to generate
the notes data. This method is both cost-effective
and enhances reproducibility. We initiate this pro-
cess by randomly sampling 10k questions from
the NQ (Kwiatkowski et al., 2019) training dataset.
GPT-4 is then prompted with specific instructions
and in-context examples to the three distinct types
of note generation (detailed in Appendix A.5). The
quality of GPT-4’s predictions is subsequently as-
sessed through human evaluations on a small subset
of the data before proceeding to the entire set. The
NQ dataset is chosen as our primary dataset due to
its diverse range of real user queries from search
engines. However, to ensure the model’s adaptabil-
ity, we also test its performance on three additional
open-domain datasets, including TriviaQA, WebQ,
and RealTimeQA, showing its generalization capa-
bilities to out-of-domain (OOD) data.
After collecting 10K training data from GPT-4,
the next step involves using them to train a LLaMa-
2 7B model (Touvron et al., 2023), to validate the
feasibility of generating CHAIN -OF-NOTE (CON)
outputs. To do this, we concatenate the instruc-
tion, question and documents as a prompt and train
the model to generate notes and answer in a stan-
dard supervised way. Our in-house model learns to
sequentially generate reading notes for each doc-
ument to assess their relevance to the input query.
Responses are generated based on the document’s
relevance, enhancing accuracy and reducing mis-
information. If all documents are irrelevant, the
model either relies on inherent knowledge for an
answer or responds with “unknown” if the answer
cannot be determined accurately.
2.3.3 Hybrid Training for Better Efficiency
Generating CHAIN -OF-NOTE (CON) would in-
crease inference cost, potentially hindering real-
world usage. To address this, we experimented
with a simple yet effective strategy for internaliz-
ing CON reasoning, called Hybrid Training.
Specifically, we allocate 50% of the training time
to the standard RALM, which involves directly
generating answers without notes, and the other
50% to RALM with CoN. This strategy allows the
model to internalize intermediate reasoning steps
during training. Additionally, we add two different
prompt words before each category of data.
During the inference phase, we exclusively use
the standard RALM prompt to guide the model,
prompting it to output answers without relying on
explicit reading notes. This approach leverages the
hidden states developed during training for implicit
CON reasoning. The model trained with the hybrid
training strategy maintains the same inference time
while achieving only slightly lower performance
wit CoN. The results will be introduced in §3.5.



### Claim 6/36

#### Claim Text
Beyond optimizing submodels, HyDE [19] introduces an innovative method where instruction-following LLMs generate hypothesis documents based on user queries to enhance retriever performance, particularly in zero-shot scenarios.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[18]_2212.10496.pdf (Page 2):

instruction following generative LLM, as described
above.
Zero-Shot Dense Retrieval The tasks of zero-
shot (dense) retrieval are arguably empirically de-
ﬁned by Thakur et al. (2021) for the neural re-
trieval community. Their BEIR benchmark con-
sists of diverse retrieval tasks. The paper and
many follow-up research generally consider the
Transfer Learning setup where the dense re-
triever is ﬁrst learned using a diverse and richly
supervised corpus and query collection, namely
MS-MARCO (Thakur et al., 2021; Wang et al.,
2022; Yu et al., 2022).
However, as stated by Izacard et al. (2021), such
a large collection can rarely be assumed. In this
paper, therefore, we study the problem of building
effective dense retrieval systems without relevance
labels. Similar to Izacard et al. (2021), we also
do not assume access to the test time corpora for
training. This is a more realistic setup and prevents
over-engineering on the test corpora.
By the deﬁnition in Sachan et al. (2022), our
setup can be roughly considered as “unsuper-
vised”. Strictly, as with Sachan et al. (2022), the
only supervision resides in the LLM, in the pro-
cessing of learning to follow instructions.
Generative Retrieval Generative search is a new
class of retrieval methods that use neural generative
models as search indices (Metzler et al., 2021; Tay
et al., 2022; Bevilacqua et al., 2022; Lee et al.,
2022). These models use (constrained) decoding
to generate document identiﬁers, such as id and
sub-string, which map directly to real documents.
They have to go through special training procedures
over relevance data; effective search may also need
to use novel forms of search indices (Bevilacqua
et al., 2022; Lee et al., 2022). In comparison, our
method uses the standard MIPS index and requires
no training or training data. Our generative model
produces an intermediate hypothetical document
to be fed into a dense encoder, instead of a real
document.
3 Methodology
In this section, we ﬁrst formally deﬁne the prob-
lem of (zero-shot) dense retrieval. Then we will
introduce how HyDE is designed to solve it.
3.1 Preliminaries
Dense retrieval models similarity between query
and document with inner product similarity. Given
a query q and document d, it uses two encoder
function encq and encd to map them into d dimen-
sion vectors vq, vd, whose inner product is used
as similarity measurement.
sim(q, d) =⟨encq(q), encd(d)⟩ = ⟨vq, vd⟩ (1)
For zero-shot retrieval, we consider L query sets
Q1, Q2, ..., QL and their corresponding search cor-
pus, document sets D1, D2, ..., DL. Denote the
j-th query from i-th set query set Qi as qij. We
need to fully deﬁne mapping functions encq and
encd without access to any query set Qi, document
set Di, or any relevance judgment rij.
The difﬁculty of zero-shot dense retrieval lies
precisely in Equation 1: it requires learning of two
embedding functions (for query and document re-
spectively) into the same embedding space where
inner product captures relevance. Without rele-
vance judgments/scores to ﬁt, learning becomes
intractable.
3.2 HyDE
HyDE circumvents the aforementioned learning
problem by performing search in document-
only embedding space that captures document-
document similarity. This can be easily learned
using unsupervised contrastive learning (Izacard
et al., 2021; Gao et al., 2021; Gao and Callan,
2022). We set document encoder encd directly as a
contrastive encoder enccon.
f = encd = enccon (2)
This function is also denoted as f for simplic-
ity. This unsupervised contrastive encoder will
be shared by all incoming document corpus.
vd = f(d) ∀d ∈ D1 ∪ D2 ∪ ... ∪ DL (3)
To build the query vector, we consider in addition
an instruction following LM, InstructLM. It takes a
query q and a textual instruction INST and follows
them to perform the task speciﬁed by INST . For
simplicity, denote,
g(q, INST ) =InstructLM(q, INST ) (4)
Now we can use g to map queries to "hypotheti-
cal" documents by sampling from g, setting INST



Source: data\tc17_2312.15883v2\referenced_papers\[19]_2212.10496.pdf (Page 2):

instruction following generative LLM, as described
above.
Zero-Shot Dense Retrieval The tasks of zero-
shot (dense) retrieval are arguably empirically de-
ﬁned by Thakur et al. (2021) for the neural re-
trieval community. Their BEIR benchmark con-
sists of diverse retrieval tasks. The paper and
many follow-up research generally consider the
Transfer Learning setup where the dense re-
triever is ﬁrst learned using a diverse and richly
supervised corpus and query collection, namely
MS-MARCO (Thakur et al., 2021; Wang et al.,
2022; Yu et al., 2022).
However, as stated by Izacard et al. (2021), such
a large collection can rarely be assumed. In this
paper, therefore, we study the problem of building
effective dense retrieval systems without relevance
labels. Similar to Izacard et al. (2021), we also
do not assume access to the test time corpora for
training. This is a more realistic setup and prevents
over-engineering on the test corpora.
By the deﬁnition in Sachan et al. (2022), our
setup can be roughly considered as “unsuper-
vised”. Strictly, as with Sachan et al. (2022), the
only supervision resides in the LLM, in the pro-
cessing of learning to follow instructions.
Generative Retrieval Generative search is a new
class of retrieval methods that use neural generative
models as search indices (Metzler et al., 2021; Tay
et al., 2022; Bevilacqua et al., 2022; Lee et al.,
2022). These models use (constrained) decoding
to generate document identiﬁers, such as id and
sub-string, which map directly to real documents.
They have to go through special training procedures
over relevance data; effective search may also need
to use novel forms of search indices (Bevilacqua
et al., 2022; Lee et al., 2022). In comparison, our
method uses the standard MIPS index and requires
no training or training data. Our generative model
produces an intermediate hypothetical document
to be fed into a dense encoder, instead of a real
document.
3 Methodology
In this section, we ﬁrst formally deﬁne the prob-
lem of (zero-shot) dense retrieval. Then we will
introduce how HyDE is designed to solve it.
3.1 Preliminaries
Dense retrieval models similarity between query
and document with inner product similarity. Given
a query q and document d, it uses two encoder
function encq and encd to map them into d dimen-
sion vectors vq, vd, whose inner product is used
as similarity measurement.
sim(q, d) =⟨encq(q), encd(d)⟩ = ⟨vq, vd⟩ (1)
For zero-shot retrieval, we consider L query sets
Q1, Q2, ..., QL and their corresponding search cor-
pus, document sets D1, D2, ..., DL. Denote the
j-th query from i-th set query set Qi as qij. We
need to fully deﬁne mapping functions encq and
encd without access to any query set Qi, document
set Di, or any relevance judgment rij.
The difﬁculty of zero-shot dense retrieval lies
precisely in Equation 1: it requires learning of two
embedding functions (for query and document re-
spectively) into the same embedding space where
inner product captures relevance. Without rele-
vance judgments/scores to ﬁt, learning becomes
intractable.
3.2 HyDE
HyDE circumvents the aforementioned learning
problem by performing search in document-
only embedding space that captures document-
document similarity. This can be easily learned
using unsupervised contrastive learning (Izacard
et al., 2021; Gao et al., 2021; Gao and Callan,
2022). We set document encoder encd directly as a
contrastive encoder enccon.
f = encd = enccon (2)
This function is also denoted as f for simplic-
ity. This unsupervised contrastive encoder will
be shared by all incoming document corpus.
vd = f(d) ∀d ∈ D1 ∪ D2 ∪ ... ∪ DL (3)
To build the query vector, we consider in addition
an instruction following LM, InstructLM. It takes a
query q and a textual instruction INST and follows
them to perform the task speciﬁed by INST . For
simplicity, denote,
g(q, INST ) =InstructLM(q, INST ) (4)
Now we can use g to map queries to "hypotheti-
cal" documents by sampling from g, setting INST



Source: data\tc17_2312.15883v2\referenced_papers\[19]_2212.10496.pdf (Page 1):

HyDE 
GPT
Contriever
how long does it take to remove
wisdom tooth It usually takes between 30
minutes and two hours to
remove a wisdom tooth...
How wisdom teeth are removed... 
Some ... a few minutes, whereas
others can take 20 minutes or
longer....
How has the COVID-19 pandemic impacted
mental health?
...depression and anxiety had
increased by 20% since the
start of the pandemic...
... two studies investigating
COVID-19 patients ... significantly
higher level of depressive ...
write a passage to answer the question
write a scientific paper passage to answer
the question
인간은  언제  불을  사용했는가 ?
write a passage in Korean to answer the
question in detail
인간이  불을  사용한  기록은  약 
800 만년  전부터  나타난다 ... ... 불을  처음  사용한  시기는  호모 
에렉투스가  살았던  142 만  년  전으 
로  거슬러간다 ...
instruction query generated document real document
Figure 1: An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries
without changing the underlying GPT-3 and Contriever/mContriever models.
to human intent to follow instructions.
With these ingredients, we propose to
pivot through Hypothetical Document
Embeddings ( HyDE), and decompose dense
retrieval into two tasks, a generative task per-
formed by an instruction-following language
model and a document-document similarity task
performed by a contrastive encoder (Figure 1).
First, we feed the query to the generative model
and instruct it to "write a document that answers
the question", i.e. a hypothetical document.
We expect the generative process to capture
"relevance" by giving an example; the generated
document is not real, can contain factual errors but
is like a relevant document. In the second step,
we use an unsupervised contrastive encoder to
encode this document into an embedding vector.
Here, we expect the encoder’s dense bottleneck
to serve a lossy compressor, where the extra
(hallucinated) details are ﬁltered out from the
embedding. We use this vector to search against
the corpus embeddings. The most similar real
documents are retrieved and returned. The retrieval
leverages document-document similarity encoded
in the inner-product during contrastive training.
Note that, interestingly, with HyDE factorization,
the query-document similarity score is no longer
explicitly modeled nor computed. Instead, the
retrieval task is cast into two NLU and NLG tasks.
HyDEappears unsupervised. No model is trained
in HyDE: both the generative model and the con-
trastive encoder remain intact. Supervision signals
were only involved in instruction learning of our
backbone LLM.
In our experiments, we showHyDEusing Instruct-
GPT (Ouyang et al., 2022) and Contriever (Izacard
et al., 2021) as backbone models signiﬁcantly out-
performs the previous state-of-the-art Contriever-
only zero-shot no-relevance system on 11 queries
sets, covering tasks like Web Search, Question
Answering, Fact Veriﬁcation and languages like
Swahili, Korean, Japanese.
2 Related Works
Dense Retrieval (Lee et al., 2019; Karpukhin
et al., 2020) has been extensively studied after the
emergence of pre-trained Transformer language
models (Devlin et al., 2019). Researchers stud-
ied the metric learning problems, such as training
loss (Karpukhin et al., 2020) and negative sam-
pling (Xiong et al., 2021; Qu et al., 2021), and also
introduced distillation (Qu et al., 2021; Lin et al.,
2021b; Hofstätter et al., 2021). Later works studied
the second stage pre-training of language model
speciﬁcally for retrieval (Izacard et al., 2021; Gao
and Callan, 2021; Lu et al., 2021; Gao and Callan,
2022; Liu and Shao, 2022).
The popularity of dense retrieval can be partially
attributed to the rich and successful research in very
efﬁcient minimum inner product search (MIPS) at
very large (billion) scales (Johnson et al., 2017).
Instructions-Following Language Models
Soon after the emergence of LLMs, several groups
of researchers discover that LLMs trained on data
consisting of instructions and their execution can
zero-shot generalize to perform new tasks with new
instructions (Ouyang et al., 2022; Sanh et al., 2022;
Min et al., 2022; Wei et al., 2022). This can be
done by standard supervised sequence-to-sequence
learning or more effectively with reinforcement
learning (Ouyang et al., 2022).
Concurrent to us, Asai et al. (2022) studied
“Task-aware Retrieval with Instructions”. They
ﬁne-tuned dense encoders that can also encode
task-speciﬁc instruction prepended to query. In
comparison, we use an unsupervised encoder and
handle different tasks and their instruction with an



Source: data\tc17_2312.15883v2\referenced_papers\[18]_2212.10496.pdf (Page 1):

HyDE 
GPT
Contriever
how long does it take to remove
wisdom tooth It usually takes between 30
minutes and two hours to
remove a wisdom tooth...
How wisdom teeth are removed... 
Some ... a few minutes, whereas
others can take 20 minutes or
longer....
How has the COVID-19 pandemic impacted
mental health?
...depression and anxiety had
increased by 20% since the
start of the pandemic...
... two studies investigating
COVID-19 patients ... significantly
higher level of depressive ...
write a passage to answer the question
write a scientific paper passage to answer
the question
인간은  언제  불을  사용했는가 ?
write a passage in Korean to answer the
question in detail
인간이  불을  사용한  기록은  약 
800 만년  전부터  나타난다 ... ... 불을  처음  사용한  시기는  호모 
에렉투스가  살았던  142 만  년  전으 
로  거슬러간다 ...
instruction query generated document real document
Figure 1: An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries
without changing the underlying GPT-3 and Contriever/mContriever models.
to human intent to follow instructions.
With these ingredients, we propose to
pivot through Hypothetical Document
Embeddings ( HyDE), and decompose dense
retrieval into two tasks, a generative task per-
formed by an instruction-following language
model and a document-document similarity task
performed by a contrastive encoder (Figure 1).
First, we feed the query to the generative model
and instruct it to "write a document that answers
the question", i.e. a hypothetical document.
We expect the generative process to capture
"relevance" by giving an example; the generated
document is not real, can contain factual errors but
is like a relevant document. In the second step,
we use an unsupervised contrastive encoder to
encode this document into an embedding vector.
Here, we expect the encoder’s dense bottleneck
to serve a lossy compressor, where the extra
(hallucinated) details are ﬁltered out from the
embedding. We use this vector to search against
the corpus embeddings. The most similar real
documents are retrieved and returned. The retrieval
leverages document-document similarity encoded
in the inner-product during contrastive training.
Note that, interestingly, with HyDE factorization,
the query-document similarity score is no longer
explicitly modeled nor computed. Instead, the
retrieval task is cast into two NLU and NLG tasks.
HyDEappears unsupervised. No model is trained
in HyDE: both the generative model and the con-
trastive encoder remain intact. Supervision signals
were only involved in instruction learning of our
backbone LLM.
In our experiments, we showHyDEusing Instruct-
GPT (Ouyang et al., 2022) and Contriever (Izacard
et al., 2021) as backbone models signiﬁcantly out-
performs the previous state-of-the-art Contriever-
only zero-shot no-relevance system on 11 queries
sets, covering tasks like Web Search, Question
Answering, Fact Veriﬁcation and languages like
Swahili, Korean, Japanese.
2 Related Works
Dense Retrieval (Lee et al., 2019; Karpukhin
et al., 2020) has been extensively studied after the
emergence of pre-trained Transformer language
models (Devlin et al., 2019). Researchers stud-
ied the metric learning problems, such as training
loss (Karpukhin et al., 2020) and negative sam-
pling (Xiong et al., 2021; Qu et al., 2021), and also
introduced distillation (Qu et al., 2021; Lin et al.,
2021b; Hofstätter et al., 2021). Later works studied
the second stage pre-training of language model
speciﬁcally for retrieval (Izacard et al., 2021; Gao
and Callan, 2021; Lu et al., 2021; Gao and Callan,
2022; Liu and Shao, 2022).
The popularity of dense retrieval can be partially
attributed to the rich and successful research in very
efﬁcient minimum inner product search (MIPS) at
very large (billion) scales (Johnson et al., 2017).
Instructions-Following Language Models
Soon after the emergence of LLMs, several groups
of researchers discover that LLMs trained on data
consisting of instructions and their execution can
zero-shot generalize to perform new tasks with new
instructions (Ouyang et al., 2022; Sanh et al., 2022;
Min et al., 2022; Wei et al., 2022). This can be
done by standard supervised sequence-to-sequence
learning or more effectively with reinforcement
learning (Ouyang et al., 2022).
Concurrent to us, Asai et al. (2022) studied
“Task-aware Retrieval with Instructions”. They
ﬁne-tuned dense encoders that can also encode
task-speciﬁc instruction prepended to query. In
comparison, we use an unsupervised encoder and
handle different tasks and their instruction with an



Source: data\tc17_2312.15883v2\referenced_papers\[18]_2212.10496.pdf (Page 0):

Precise Zero-Shot Dense Retrieval without Relevance Labels
Luyu Gao∗† Xueguang Ma∗‡ Jimmy Lin‡ Jamie Callan†
†Language Technologies Institute, Carnegie Mellon University
‡David R. Cheriton School of Computer Science, University of Waterloo
{luyug, callan}@cs.cmu.edu, {x93ma, jimmylin}@uwaterloo.ca
Abstract
While dense retrieval has been shown effec-
tive and efﬁcient across tasks and languages,
it remains difﬁcult to create effective fully
zero-shot dense retrieval systems when no rel-
evance label is available. In this paper, we
recognize the difﬁculty of zero-shot learning
and encoding relevance. Instead, we pro-
pose to pivot through Hypothetical Document
Embeddings (HyDE). Given a query, HyDE ﬁrst
zero-shot instructs an instruction-following
language model (e.g. InstructGPT) to gen-
erate a hypothetical document. The docu-
ment captures relevance patterns but is unreal
and may contain false details. Then, an un-
supervised contrastively learned encoder (e.g.
Contriever) encodes the document into an
embedding vector. This vector identiﬁes a
neighborhood in the corpus embedding space,
where similar real documents are retrieved
based on vector similarity. This second step
ground the generated document to the actual
corpus, with the encoder’s dense bottleneck
ﬁltering out the incorrect details. Our exper-
iments show that HyDE signiﬁcantly outper-
forms the state-of-the-art unsupervised dense
retriever Contriever and shows strong per-
formance comparable to ﬁne-tuned retrievers,
across various tasks (e.g. web search, QA, fact
veriﬁcation) and languages (e.g. sw, ko, ja).1
1 Introduction
Dense retrieval (Lee et al., 2019; Karpukhin et al.,
2020), the method of retrieving documents using
semantic embedding similarities, has been shown
successful across tasks like web search, question
answering, and fact veriﬁcation. A variety of meth-
ods such as negative mining (Xiong et al., 2021; Qu
et al., 2021), distillation (Qu et al., 2021; Lin et al.,
2021b; Hofstätter et al., 2021) and task-speciﬁc
∗ Equal contribution.
1No models were trained or ﬁne-tuned in making this pre-
print. Our open source code is available at https://github.
com/texttron/hyde.
pre-training (Izacard et al., 2021; Gao and Callan,
2021; Lu et al., 2021; Gao and Callan, 2022; Liu
and Shao, 2022) have been proposed to improve the
effectiveness of supervised dense retrieval models.
On the other hand, zero-shot dense retrieval still
remains difﬁcult. Many recent works consider the
alternative transfer learning setup, where the dense
retrievers are trained on a high-resource dataset and
then evaluated on queries from new tasks. The MS-
MARCO collection (Bajaj et al., 2016), a massive
judged dataset with a large number of judged query-
document pairs, is arguably the most commonly
used. As argued by Izacard et al. (2021), in prac-
tice, however, the existence of such a large dataset
cannot always be assumed. Even MS-MARCO re-
stricts commercial use and cannot be adopted in a
variety of real-world search scenarios.
In this paper, we aim to build effective fully
zero-shot dense retrieval systems that require no
relevance supervision, work out-of-box and gener-
alize across tasks. As supervision is not available,
we start by examining self-supervised representa-
tion learning methods. Modern deep learning en-
ables two distinct learning algorithms. At the token
level, generative large language models (LLM) pre-
trained on large corpus have demonstrated strong
natural language understanding (NLU) and gen-
eration (NLG) capabilities (Brown et al., 2020;
Chen et al., 2021; Rae et al., 2021; Hoffmann
et al., 2022; Thoppilan et al., 2022; Chowdhery
et al., 2022). At the document level, text (chunk)
encoders pre-trained with contrastive objectives
learn to encode document-document similarity into
inner-product (Izacard et al., 2021; Gao and Callan,
2022). On top of these, one extra insight into LLM
is borrowed: the LLMs further trained to follow
instructions can zero-shot generalize to diverse un-
seen instructions (Ouyang et al., 2022; Sanh et al.,
2022; Min et al., 2022; Wei et al., 2022). Ouyang
et al. (2022) show that with a small amount of data,
GPT-3 (Brown et al., 2020) models can be aligned
arXiv:2212.10496v1  [cs.IR]  20 Dec 2022



### Claim 7/36

#### Claim Text
Other methods like CoN [93] and CoK [44] involve LLMs in note-making and step-wise reasoning verification through customized prompts, and greatly rely on frequent interactions with LLMs.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 8):

Published as a conference paper at ICLR 2024
be due to error propagation. On the other hand, the factual accuracy of CoK improves slightly for
the second rationale, which indicates that correcting previous rationales helps the LLM to generate
more factual rationales in future steps.
Human Evaluation To qualitatively examine whether CoK could output factually consistent rea-
soning chains, we also conducted a human study. Specifically, two volunteers are given 100 outputs
randomly selected from HotpotQA and FEVER datasets. The selected outputs are balanced, where
50 CoK outputs resulted in incorrect answers, and the other 50 resulted in correct answers. The vol-
unteers are asked to select which reasoning chain is factually correct, or if there is a tie. Then, they
are asked to answer whether the better CoT should lead to better results. Details on the instructions
and setup can be found in Appendix H.1. The results are given in Table 7. We could observe that
volunteers consistently confirm that CoK-generated reasoning chains are factually consistent while
the CoT-SC chains are not. For incorrect predictions, humans still believe that 44% of the time, the
CoK-generated CoT is improved on factual consistency, although it may not contain the necessary
information for a correct answer. Among these instances, humans believe 73% of the time that these
improved CoTs should have led to better answers. This implies that, even though the CoT quality
has been improved, many failure cases are caused by reasoning errors. Case studies can be found in
Appendix H.2. In general, the two volunteers show a Cohen Kappa’s agreement of 0.43, which is
considered moderate agreement (Landis & Koch, 1977).
6 R ELATED WORK
Knowledge-Intensive NLP While language models can generate highly coherent text and demon-
strate reasoning abilities, many real-world tasks require knowledge beyond the local context. For
example, fact-checking tasks may require models to locate suitable evidence on the internet or refer
to external knowledge (Thorne et al., 2018). In the realm of natural language processing (NLP), a
task is deemed to be knowledge-intensive when it exceeds the reasonable expectation of human capa-
bility to solve it without access to external knowledge. The resolution of such knowledge-intensive
NLP tasks typically involves the utilization of retriever-reader systems. Initially, a retriever extracts
a limited collection of pertinent documents from the knowledge source, after which a reader employs
the context extracted to generate an appropriate response (Chen et al., 2017; Lewis et al., 2020; Guu
et al., 2020). Hence, there is an urgent need to develop effective models for knowledge-intensive
tasks (Petroni et al., 2021).
Augmented Language Models The discipline of augmented language models (ALMs) addresses
hallucinations of traditional LLMs by equipping them with improved reasoning capabilities and
the capacity to utilize external resources (Chung et al., 2022). Furthermore, LLMs can learn to
leverage external tools or models to accomplish the relevant tasks (Schick et al., 2023; Shen et al.,
2023). ALMs can employ these enhancements independently or combine them in a specific order
to complete a given task, ultimately resulting in enhanced capabilities (Mialon et al., 2023; Zhao
et al., 2023a). However, previous works do not consider knowledge from multiple domains and lack
progressive editing throughout the generation process, which could lead to error propagation. In
this work, we propose an efficient framework to solve knowledge-intensive tasks by progressively
augmenting them with diverse sources of external knowledge.
7 C ONCLUSIONS
In this paper, we introduce chain-of-knowledge (CoK), a novel framework designed to enhance the
factual correctness of large language models (LLMs). CoK represents a promising and comprehen-
sive solution to progressive knowledge-grounded generation by incorporating heterogeneous sources
in multiple domains. We address the challenge of accurate query generation by proposing the adap-
tive query generator (AQG) which supports both unstructured and structured query languages. The
AQG can be easily transitioned between fine-tuned models and black-box LLMs. Our experimental
results on knowledge-intensive tasks demonstrate the substantial improvement achieved by CoK.
Furthermore, the modularity of CoK allows its application to various LLMs and different formats of
knowledge sources, which addresses important challenges, including privacy concerns, knowledge
source reliance, and rapid information updates.
9



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 4):

Published as a conference paper at ICLR 2024
fetch the results, which could be a singular value or a subset of the original table. The outcomes
from both SPARQL and SQL are then formatted into markdown text. For natural sentence queries,
knowledge is retrieved from domain-specific knowledge sources either through sentence similarity
matching or by utilizing a search engine. 2
Rationale Correction Existing methods such as ReAct (Yao et al., 2023) and Verify-and-Edit
(Zhao et al., 2023c) keep all retrieved information in the context throughout the process, no matter
if it contains reasoning mistakes. This often leads to error propagation and misguides further gener-
ations. To overcome this weakness, CoK involves a progressive rationale correction step. Given the
current rationale and the formatted knowledge from various knowledge sources, a corrected ratio-
nale is generated to replace the current one. This step helps in rectifying any factual incorrectness
and preventing error propagation.
Next Rationale Generation Using the question and preceding corrected rationales, the next ratio-
nale is generated, and the process is reiterated for the new rationale until a final answer is produced.
2.3 A NSWER CONSOLIDATION STAGE
Ultimately, the LLM is prompted with the question and corrected rationales to generate a consol-
idated answer, which is expected leading to a more accurate answer. This hypothesis is further
examined through a series of experiments, as detailed in Section 4.
3 T HE ADAPTIVE QUERY GENERATOR
CoK incorporates heterogeneous knowledge sources from four different domains, including factual,
medical, physics, and biology. Each of these knowledge sources necessitates the use of a unique
query language for retrieval, which could be either structured or unstructured. Therefore, we design
the adaptive query generator (AQG) to facilitate query generation for different knowledge sources.
Unstructured Query Languages Natural language sentences are the most natural way that hu-
man beings search for information. AQG utilizes two distinct approaches for generating unstruc-
tured queries based on the knowledge sources. A) For general factual knowledge sources, such as
Wikipedia, ChatGPT is utilized. B) For domain-specific knowledge sources ( e.g., Flashcard, Sci-
enceQA Physics, and ScienceQA Biology), using ChatGPT may lead to hallucination as it may not
have comprehensive knowledge of the specific domains. Therefore, we instruction-tune LLaMA-
2-7B using LoRA with pairs of input texts and output queries. Furthermore, the domain of the
training data is on par with the respective knowledge source. Consequently, the AQG is equipped
with the requisite knowledge for generating queries with greater precision.
Structured Query Languages Querying unstructured knowledge sources often leads to the re-
trieval of irrelevant and redundant information. On the other hand, structured knowledge sources
(e.g., Wikidata and tables) provide direct factual results. To generate structured queries, AQG uti-
lizes two approaches based on the query languages. A) When generating commonly used query
languages like SQL, we employ ChatGPT. It is empirically inferred that ChatGPT included SQL
during its pre-training, providing it with advantages in generating SQL queries (OpenAI, 2023).
All pertinent details are incorporated into the prompt to enhance the precision of query generation.
For instance, when generating SQL queries, we include both the table schema and data snippets.
B) For less common languages like SPARQL, we instruction-tune LLaMA-2-7B using LoRA with
sentence-SPARQL pairs. The training data is collected to match the logical granularity of the ratio-
nales, thereby facilitating more accurate query generation. For example in SPARQL, both training
data and rationales contain single entity and relation within each sentence. Inspired by chain-of-
hindsight (Liu et al., 2023), besides giving the correct queries, we also append negative examples
such as “incorrect queries:..” during instruction-tuning.
Detailed query language, model, and training datasets of each knowledge source are in Table 8 of
Appendix. The constructions of instruction-tuning datasets and training details are in Appendix D.
We also evaluate the performances of AQG in Appendix F.2.
2Details of the execution process for each knowledge source is in Appendix C.
5



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 2):

Published as a conference paper at ICLR 2024
Medical
Rationale 1
Knowledge 
Retrieval
Supporting 
Knowledge
Rationale 
Correction
Corrected Rationale 1
Next Rationale 
Generation
Corrected Rationale 2
…
…
Rationale 2
Knowledge 
Retrieval
Supporting 
Knowledge
Rationale 
Correction
Next Rationale 
Generation
Reasoning Generation & 
Knowledge Domain Selection Answer Consolidation
The answer is 1941What year was the Argentine actor who directed El 
Tio Disparate born? I III
II
Rationale
Query 
Generation
Query
Llama-2-LoRA
ChatGPT
Adaptive Query Generator
Wikidata (SPARQL)
 Table (SQL)Factual
 Wikipedia (n.s.)
ScienceQA Physics (n.s.)Physics
Biology
Supporting 
Knowledge
UpToDate (n.s.)
PysicsClassroom (n.s.)
Flashcard (n.s.)
ScienceQA Biology (n.s.)
 CK-12 (n.s.)
Figure 2: Our proposed chain-of-knowledge (CoK) framework, consisting of (I) Reasoning prepa-
ration, (II) Dynamic knowledge adapting, and (III) Answer consolidation. n.s.: natural sentence.
Given that different knowledge sources require distinct query languages, AQG holds a crucial role
in generating queries. AQG is versatile and can either be a fine-tuned model like Llama-2 (Touvron
et al., 2023) with LoRA (Hu et al., 2021) or an off-the-shelf LLM like ChatGPT. By leveraging both
unstructured and structured knowledge sources, CoK allows for better factual accuracy, improved
reliability, and easier information updates.
To summarize, our key contributions are the following: (1) We introduce chain-of-knowledge (CoK),
a novel framework to enhance the factual correctness of LLMs with heterogeneous knowledge
sources; (2) We propose an adaptive query generator (AQG), specially designed to generate queries
tailored to each knowledge source. AQG is versatile and can seamlessly transition between fine-
tuned models and black-box LLMs; (3) CoK corrects the rationales progressively, ensuring that
inaccuracies from preceding rationales do not propagate into the subsequent steps; (4) We perform
extensive experiments on knowledge-intensive tasks spanning a range of domains, including factual,
medical, physical, and biological. CoK outperforms the CoT baseline by 4.3% on average.
2 T HE CHAIN -OF-KNOWLEDGE FRAMEWORK
As shown in Figure 2, the CoK framework consists of three stages: (1) reasoning preparation, (2)
dynamic knowledge adapting, and (3) answer consolidation. In the first stage, given a knowledge-
intensive question, CoK generates preliminary rationales, i.e., reasoning units/sentences in the rea-
soning chain of CoT, and answers while identifying the relevant knowledge domains. Questions that
do not yield a majority consensus in their answers enter the dynamic knowledge adapting stage, in
which an adaptive query generator (AQG) is employed to generate queries to retrieve knowledge
from the knowledge sources of the identified domain. The rationales are progressively revised and
generated based on the retrieved knowledge. The final answer is then derived based on the corrected
rationales. Refer to Appendix A.1 for the prompts used for each step of our framework.
2.1 R EASONING PREPARATION STAGE
In real-world scenarios, when facing a complex knowledge-intensive question, it is necessary to
generate intermediate rationales before producing the final answer (Wei et al., 2022). Moreover,
before delving into external knowledge sources to address the question, it is crucial to identify the
relevant knowledge domains for effective retrieval. Thus, the reasoning preparation stage consists
of two essential components, namely, reasoning generation and knowledge domain selection.
3



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 7):

Published as a conference paper at ICLR 2024
Table 5: Parallel vs. dynamic
knowledge adapting.
Method HotpotQA (3-shot)
CoT 29.9%Verify-and-Edit 31.8%CoK (parallel) 31.2%CoK (dynamic) 34.1%
Table 6: Comparison of the
factual accuracy of rationales
on HotpotQA.
Method Rationale 1 Rationale 2
CoT-SC 54.3% 52.1%CoK 66.3% 69.5%
Table 7: Human study results
on the factuality of reasoning
chains.
Predictions CoK CoT-SC Tie
Correct predictions68% 4% 28%Incorrect predictions44% 24% 32%All predictions56% 14% 30%
5 A NALYSIS
5.1 S INGLE VS . M ULTIPLE KNOWLEDGE DOMAINS AND SOURCES
As outlined in Section 2.1, CoK integrates a step to select the appropriate knowledge domains for
each question. This step is crucial to ensure that CoK can retrieve the most pertinent knowledge to
correct the rationales and answer the questions accurately. It is possible that multiple knowledge
domains can be chosen for one question, and within each domain, there are several knowledge
sources. In this subsection, we investigate the necessity of utilizing multiple knowledge domains
and sources. We also include an evaluation of the domain selection performance in Appendix F.1.
Single vs. Multiple Knowledge Domains We show the domain distributions identified for each
dataset in Figure 3. Notably, we find that CoK predominantly selects one knowledge domain for
each dataset, while a small number of cases call for multiple domains. For instance, the primary
knowledge domain for MedMCQA is Medical, and 17.8% of the questions identify Biology as a rel-
evant domain as well. Furthermore, we conduct ablation experiments to demonstrate the necessity
of utilizing multiple domains. As shown in Table 4, compared to only using Medical domain knowl-
edge, CoK using additional knowledge from the Biology domain further improves the performance
by 1.3%. This indicates that knowledge spanning multiple domains is needed for answering some
questions, underscoring the necessity of incorporating various knowledge domains.
Single vs. Multiple Knowledge Sources Within one domain, numerous credible knowledge
sources exist, and it is unfeasible for a single source to encompass all knowledge from the domain.
Therefore, it is important to utilize multiple knowledge sources within one domain. For instance,
as shown in Table 4, the performance of CoK improves by 2.1% when utilizing both Flashcard and
UpToDate as medical knowledge sources, compared to using only Flashcard. 7
5.2 P ARALLEL VS . DYNAMIC KNOWLEDGE ADAPTING
As aforementioned, dynamic knowledge adapting helps CoK prevent error propagation, here we take
a closer look at how much improvement it brings in. As shown in Table 5, the performance of CoK
improves by 4.2% compared with CoT when dynamic knowledge adapting is applied. However,
parallel editing leads to poorer performance due to error propagation for rationales.
5.3 E VALUATING FACTUALITY IMPROVEMENT OF THE RATIONALES
While the main results have shown that CoK effectively improves the performance of LLMs in
knowledge-intensive tasks, we are also interested in reducing hallucination for the generated ratio-
nales. Hence, we conduct quantitative and qualitative evaluations to assess the factual accuracy.
Quantitative Evaluation To automatically evaluate how CoK can reduce hallucination in the
model outputs, we employ an existing fact-checking method to compare the original and corrected
rationales. Specifically, we use ProgramFC (Pan et al., 2023) which is a state-of-the-art method for
judging the factuality of claims with respect to Wikipedia. As shown in Table 6, we observe that
CoK has improved factual accuracy compared to the CoT-SC baseline on the HotpotQA dataset. No-
tably, the factual accuracy of CoT-SC decreases for rationale 2 compared to rationale 1, which could
7Note that using external sources may have limitations such as noise or conflicts between different sources.
We mainly address this by selecting authoritative knowledge sources, and discuss this further in Appendix G
8



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 12):

13  
Previous studies have utilized KG in conjunction with LLM for knowledge intensive tasks such as question-answering,(Yasunaga et al. 2021) multi-hop relational reasoning,(Feng et al. 2020) commonsense reasoning,(Lin et al. 2019; Lv et al. 2019) and model pre-training.(Moiseev et al. 2022; Yasunaga et al. 2022) Furthermore, enhancing prompts by incorporating structured knowledge has been described and studied.(Xiang Chen Zhejiang University, China et al.; Lewis et al. 2020; Pan et al. 2023) Naturally, these approaches have bolstered the positive reinforcement between KG and LLM. Nevertheless, it's worth noting that these approaches are only be task-specific and in some cases, the knowledge infusion could grow exponentially by the inclusion of higher order relations.(Bauer, Wang and Bansal 2018; Lin et al. 2019) Such approaches could compromise the limited token space of the LLM. Alternative methods employed knowledge infusion through the direct use of query languages like SPARQL.(Brate et al. 2022) However, this could render the system constraint to the schema of the underlying KG, potentially affecting the flexibility and adaptability of prompts. Moreover, as the KG expands and its schema grows, it could potentially occupy a significant portion of the LLM input token space. This explains why we noticed a greater token usage with the Cypher-RAG method (average usage of 8006 tokens), as it incorporates the entire graph schema into the input prompt for converting natural language into structured Cypher queries. In contrast, KG-RAG requires minimal graph schema, thus eliminating the need to include it in the prompt and resulting in substantial token savings, with a reduction of over 50% in token utilization compared to Cypher-RAG. This finding suggests that when dealing with a graph as large as SPOKE, which contains over 40 million nodes, Cypher-RAG requires LLMs that enable a larger context window. In contrast, KG-RAG is capable of managing this with LLMs that require a relatively smaller window size.



#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 8):

Published as a conference paper at ICLR 2024
be due to error propagation. On the other hand, the factual accuracy of CoK improves slightly for
the second rationale, which indicates that correcting previous rationales helps the LLM to generate
more factual rationales in future steps.
Human Evaluation To qualitatively examine whether CoK could output factually consistent rea-
soning chains, we also conducted a human study. Specifically, two volunteers are given 100 outputs
randomly selected from HotpotQA and FEVER datasets. The selected outputs are balanced, where
50 CoK outputs resulted in incorrect answers, and the other 50 resulted in correct answers. The vol-
unteers are asked to select which reasoning chain is factually correct, or if there is a tie. Then, they
are asked to answer whether the better CoT should lead to better results. Details on the instructions
and setup can be found in Appendix H.1. The results are given in Table 7. We could observe that
volunteers consistently confirm that CoK-generated reasoning chains are factually consistent while
the CoT-SC chains are not. For incorrect predictions, humans still believe that 44% of the time, the
CoK-generated CoT is improved on factual consistency, although it may not contain the necessary
information for a correct answer. Among these instances, humans believe 73% of the time that these
improved CoTs should have led to better answers. This implies that, even though the CoT quality
has been improved, many failure cases are caused by reasoning errors. Case studies can be found in
Appendix H.2. In general, the two volunteers show a Cohen Kappa’s agreement of 0.43, which is
considered moderate agreement (Landis & Koch, 1977).
6 R ELATED WORK
Knowledge-Intensive NLP While language models can generate highly coherent text and demon-
strate reasoning abilities, many real-world tasks require knowledge beyond the local context. For
example, fact-checking tasks may require models to locate suitable evidence on the internet or refer
to external knowledge (Thorne et al., 2018). In the realm of natural language processing (NLP), a
task is deemed to be knowledge-intensive when it exceeds the reasonable expectation of human capa-
bility to solve it without access to external knowledge. The resolution of such knowledge-intensive
NLP tasks typically involves the utilization of retriever-reader systems. Initially, a retriever extracts
a limited collection of pertinent documents from the knowledge source, after which a reader employs
the context extracted to generate an appropriate response (Chen et al., 2017; Lewis et al., 2020; Guu
et al., 2020). Hence, there is an urgent need to develop effective models for knowledge-intensive
tasks (Petroni et al., 2021).
Augmented Language Models The discipline of augmented language models (ALMs) addresses
hallucinations of traditional LLMs by equipping them with improved reasoning capabilities and
the capacity to utilize external resources (Chung et al., 2022). Furthermore, LLMs can learn to
leverage external tools or models to accomplish the relevant tasks (Schick et al., 2023; Shen et al.,
2023). ALMs can employ these enhancements independently or combine them in a specific order
to complete a given task, ultimately resulting in enhanced capabilities (Mialon et al., 2023; Zhao
et al., 2023a). However, previous works do not consider knowledge from multiple domains and lack
progressive editing throughout the generation process, which could lead to error propagation. In
this work, we propose an efficient framework to solve knowledge-intensive tasks by progressively
augmenting them with diverse sources of external knowledge.
7 C ONCLUSIONS
In this paper, we introduce chain-of-knowledge (CoK), a novel framework designed to enhance the
factual correctness of large language models (LLMs). CoK represents a promising and comprehen-
sive solution to progressive knowledge-grounded generation by incorporating heterogeneous sources
in multiple domains. We address the challenge of accurate query generation by proposing the adap-
tive query generator (AQG) which supports both unstructured and structured query languages. The
AQG can be easily transitioned between fine-tuned models and black-box LLMs. Our experimental
results on knowledge-intensive tasks demonstrate the substantial improvement achieved by CoK.
Furthermore, the modularity of CoK allows its application to various LLMs and different formats of
knowledge sources, which addresses important challenges, including privacy concerns, knowledge
source reliance, and rapid information updates.
9



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 4):

Published as a conference paper at ICLR 2024
fetch the results, which could be a singular value or a subset of the original table. The outcomes
from both SPARQL and SQL are then formatted into markdown text. For natural sentence queries,
knowledge is retrieved from domain-specific knowledge sources either through sentence similarity
matching or by utilizing a search engine. 2
Rationale Correction Existing methods such as ReAct (Yao et al., 2023) and Verify-and-Edit
(Zhao et al., 2023c) keep all retrieved information in the context throughout the process, no matter
if it contains reasoning mistakes. This often leads to error propagation and misguides further gener-
ations. To overcome this weakness, CoK involves a progressive rationale correction step. Given the
current rationale and the formatted knowledge from various knowledge sources, a corrected ratio-
nale is generated to replace the current one. This step helps in rectifying any factual incorrectness
and preventing error propagation.
Next Rationale Generation Using the question and preceding corrected rationales, the next ratio-
nale is generated, and the process is reiterated for the new rationale until a final answer is produced.
2.3 A NSWER CONSOLIDATION STAGE
Ultimately, the LLM is prompted with the question and corrected rationales to generate a consol-
idated answer, which is expected leading to a more accurate answer. This hypothesis is further
examined through a series of experiments, as detailed in Section 4.
3 T HE ADAPTIVE QUERY GENERATOR
CoK incorporates heterogeneous knowledge sources from four different domains, including factual,
medical, physics, and biology. Each of these knowledge sources necessitates the use of a unique
query language for retrieval, which could be either structured or unstructured. Therefore, we design
the adaptive query generator (AQG) to facilitate query generation for different knowledge sources.
Unstructured Query Languages Natural language sentences are the most natural way that hu-
man beings search for information. AQG utilizes two distinct approaches for generating unstruc-
tured queries based on the knowledge sources. A) For general factual knowledge sources, such as
Wikipedia, ChatGPT is utilized. B) For domain-specific knowledge sources ( e.g., Flashcard, Sci-
enceQA Physics, and ScienceQA Biology), using ChatGPT may lead to hallucination as it may not
have comprehensive knowledge of the specific domains. Therefore, we instruction-tune LLaMA-
2-7B using LoRA with pairs of input texts and output queries. Furthermore, the domain of the
training data is on par with the respective knowledge source. Consequently, the AQG is equipped
with the requisite knowledge for generating queries with greater precision.
Structured Query Languages Querying unstructured knowledge sources often leads to the re-
trieval of irrelevant and redundant information. On the other hand, structured knowledge sources
(e.g., Wikidata and tables) provide direct factual results. To generate structured queries, AQG uti-
lizes two approaches based on the query languages. A) When generating commonly used query
languages like SQL, we employ ChatGPT. It is empirically inferred that ChatGPT included SQL
during its pre-training, providing it with advantages in generating SQL queries (OpenAI, 2023).
All pertinent details are incorporated into the prompt to enhance the precision of query generation.
For instance, when generating SQL queries, we include both the table schema and data snippets.
B) For less common languages like SPARQL, we instruction-tune LLaMA-2-7B using LoRA with
sentence-SPARQL pairs. The training data is collected to match the logical granularity of the ratio-
nales, thereby facilitating more accurate query generation. For example in SPARQL, both training
data and rationales contain single entity and relation within each sentence. Inspired by chain-of-
hindsight (Liu et al., 2023), besides giving the correct queries, we also append negative examples
such as “incorrect queries:..” during instruction-tuning.
Detailed query language, model, and training datasets of each knowledge source are in Table 8 of
Appendix. The constructions of instruction-tuning datasets and training details are in Appendix D.
We also evaluate the performances of AQG in Appendix F.2.
2Details of the execution process for each knowledge source is in Appendix C.
5



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 2):

Published as a conference paper at ICLR 2024
Medical
Rationale 1
Knowledge 
Retrieval
Supporting 
Knowledge
Rationale 
Correction
Corrected Rationale 1
Next Rationale 
Generation
Corrected Rationale 2
…
…
Rationale 2
Knowledge 
Retrieval
Supporting 
Knowledge
Rationale 
Correction
Next Rationale 
Generation
Reasoning Generation & 
Knowledge Domain Selection Answer Consolidation
The answer is 1941What year was the Argentine actor who directed El 
Tio Disparate born? I III
II
Rationale
Query 
Generation
Query
Llama-2-LoRA
ChatGPT
Adaptive Query Generator
Wikidata (SPARQL)
 Table (SQL)Factual
 Wikipedia (n.s.)
ScienceQA Physics (n.s.)Physics
Biology
Supporting 
Knowledge
UpToDate (n.s.)
PysicsClassroom (n.s.)
Flashcard (n.s.)
ScienceQA Biology (n.s.)
 CK-12 (n.s.)
Figure 2: Our proposed chain-of-knowledge (CoK) framework, consisting of (I) Reasoning prepa-
ration, (II) Dynamic knowledge adapting, and (III) Answer consolidation. n.s.: natural sentence.
Given that different knowledge sources require distinct query languages, AQG holds a crucial role
in generating queries. AQG is versatile and can either be a fine-tuned model like Llama-2 (Touvron
et al., 2023) with LoRA (Hu et al., 2021) or an off-the-shelf LLM like ChatGPT. By leveraging both
unstructured and structured knowledge sources, CoK allows for better factual accuracy, improved
reliability, and easier information updates.
To summarize, our key contributions are the following: (1) We introduce chain-of-knowledge (CoK),
a novel framework to enhance the factual correctness of LLMs with heterogeneous knowledge
sources; (2) We propose an adaptive query generator (AQG), specially designed to generate queries
tailored to each knowledge source. AQG is versatile and can seamlessly transition between fine-
tuned models and black-box LLMs; (3) CoK corrects the rationales progressively, ensuring that
inaccuracies from preceding rationales do not propagate into the subsequent steps; (4) We perform
extensive experiments on knowledge-intensive tasks spanning a range of domains, including factual,
medical, physical, and biological. CoK outperforms the CoT baseline by 4.3% on average.
2 T HE CHAIN -OF-KNOWLEDGE FRAMEWORK
As shown in Figure 2, the CoK framework consists of three stages: (1) reasoning preparation, (2)
dynamic knowledge adapting, and (3) answer consolidation. In the first stage, given a knowledge-
intensive question, CoK generates preliminary rationales, i.e., reasoning units/sentences in the rea-
soning chain of CoT, and answers while identifying the relevant knowledge domains. Questions that
do not yield a majority consensus in their answers enter the dynamic knowledge adapting stage, in
which an adaptive query generator (AQG) is employed to generate queries to retrieve knowledge
from the knowledge sources of the identified domain. The rationales are progressively revised and
generated based on the retrieved knowledge. The final answer is then derived based on the corrected
rationales. Refer to Appendix A.1 for the prompts used for each step of our framework.
2.1 R EASONING PREPARATION STAGE
In real-world scenarios, when facing a complex knowledge-intensive question, it is necessary to
generate intermediate rationales before producing the final answer (Wei et al., 2022). Moreover,
before delving into external knowledge sources to address the question, it is crucial to identify the
relevant knowledge domains for effective retrieval. Thus, the reasoning preparation stage consists
of two essential components, namely, reasoning generation and knowledge domain selection.
3



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 7):

Published as a conference paper at ICLR 2024
Table 5: Parallel vs. dynamic
knowledge adapting.
Method HotpotQA (3-shot)
CoT 29.9%Verify-and-Edit 31.8%CoK (parallel) 31.2%CoK (dynamic) 34.1%
Table 6: Comparison of the
factual accuracy of rationales
on HotpotQA.
Method Rationale 1 Rationale 2
CoT-SC 54.3% 52.1%CoK 66.3% 69.5%
Table 7: Human study results
on the factuality of reasoning
chains.
Predictions CoK CoT-SC Tie
Correct predictions68% 4% 28%Incorrect predictions44% 24% 32%All predictions56% 14% 30%
5 A NALYSIS
5.1 S INGLE VS . M ULTIPLE KNOWLEDGE DOMAINS AND SOURCES
As outlined in Section 2.1, CoK integrates a step to select the appropriate knowledge domains for
each question. This step is crucial to ensure that CoK can retrieve the most pertinent knowledge to
correct the rationales and answer the questions accurately. It is possible that multiple knowledge
domains can be chosen for one question, and within each domain, there are several knowledge
sources. In this subsection, we investigate the necessity of utilizing multiple knowledge domains
and sources. We also include an evaluation of the domain selection performance in Appendix F.1.
Single vs. Multiple Knowledge Domains We show the domain distributions identified for each
dataset in Figure 3. Notably, we find that CoK predominantly selects one knowledge domain for
each dataset, while a small number of cases call for multiple domains. For instance, the primary
knowledge domain for MedMCQA is Medical, and 17.8% of the questions identify Biology as a rel-
evant domain as well. Furthermore, we conduct ablation experiments to demonstrate the necessity
of utilizing multiple domains. As shown in Table 4, compared to only using Medical domain knowl-
edge, CoK using additional knowledge from the Biology domain further improves the performance
by 1.3%. This indicates that knowledge spanning multiple domains is needed for answering some
questions, underscoring the necessity of incorporating various knowledge domains.
Single vs. Multiple Knowledge Sources Within one domain, numerous credible knowledge
sources exist, and it is unfeasible for a single source to encompass all knowledge from the domain.
Therefore, it is important to utilize multiple knowledge sources within one domain. For instance,
as shown in Table 4, the performance of CoK improves by 2.1% when utilizing both Flashcard and
UpToDate as medical knowledge sources, compared to using only Flashcard. 7
5.2 P ARALLEL VS . DYNAMIC KNOWLEDGE ADAPTING
As aforementioned, dynamic knowledge adapting helps CoK prevent error propagation, here we take
a closer look at how much improvement it brings in. As shown in Table 5, the performance of CoK
improves by 4.2% compared with CoT when dynamic knowledge adapting is applied. However,
parallel editing leads to poorer performance due to error propagation for rationales.
5.3 E VALUATING FACTUALITY IMPROVEMENT OF THE RATIONALES
While the main results have shown that CoK effectively improves the performance of LLMs in
knowledge-intensive tasks, we are also interested in reducing hallucination for the generated ratio-
nales. Hence, we conduct quantitative and qualitative evaluations to assess the factual accuracy.
Quantitative Evaluation To automatically evaluate how CoK can reduce hallucination in the
model outputs, we employ an existing fact-checking method to compare the original and corrected
rationales. Specifically, we use ProgramFC (Pan et al., 2023) which is a state-of-the-art method for
judging the factuality of claims with respect to Wikipedia. As shown in Table 6, we observe that
CoK has improved factual accuracy compared to the CoT-SC baseline on the HotpotQA dataset. No-
tably, the factual accuracy of CoT-SC decreases for rationale 2 compared to rationale 1, which could
7Note that using external sources may have limitations such as noise or conflicts between different sources.
We mainly address this by selecting authoritative knowledge sources, and discuss this further in Appendix G
8



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 12):

13  
Previous studies have utilized KG in conjunction with LLM for knowledge intensive tasks such as question-answering,(Yasunaga et al. 2021) multi-hop relational reasoning,(Feng et al. 2020) commonsense reasoning,(Lin et al. 2019; Lv et al. 2019) and model pre-training.(Moiseev et al. 2022; Yasunaga et al. 2022) Furthermore, enhancing prompts by incorporating structured knowledge has been described and studied.(Xiang Chen Zhejiang University, China et al.; Lewis et al. 2020; Pan et al. 2023) Naturally, these approaches have bolstered the positive reinforcement between KG and LLM. Nevertheless, it's worth noting that these approaches are only be task-specific and in some cases, the knowledge infusion could grow exponentially by the inclusion of higher order relations.(Bauer, Wang and Bansal 2018; Lin et al. 2019) Such approaches could compromise the limited token space of the LLM. Alternative methods employed knowledge infusion through the direct use of query languages like SPARQL.(Brate et al. 2022) However, this could render the system constraint to the schema of the underlying KG, potentially affecting the flexibility and adaptability of prompts. Moreover, as the KG expands and its schema grows, it could potentially occupy a significant portion of the LLM input token space. This explains why we noticed a greater token usage with the Cypher-RAG method (average usage of 8006 tokens), as it incorporates the entire graph schema into the input prompt for converting natural language into structured Cypher queries. In contrast, KG-RAG requires minimal graph schema, thus eliminating the need to include it in the prompt and resulting in substantial token savings, with a reduction of over 50% in token utilization compared to Cypher-RAG. This finding suggests that when dealing with a graph as large as SPOKE, which contains over 40 million nodes, Cypher-RAG requires LLMs that enable a larger context window. In contrast, KG-RAG is capable of managing this with LLMs that require a relatively smaller window size.



### Claim 8/36

#### Claim Text
KGRAG [65] uses the user query as a reference for retrieval in KGs, which suffers from misalignment between high-quality structured knowledge and varying-quality queries.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 15):

16  
parameterized implicit knowledge of an LLM. This knowledge integration results in the generation of domain-specific, reliable and up-to-date meaningful biomedical responses with rich annotations.  Materials and Methods Knowledge Graph based Retrieval Augmented Generation (KG-RAG) Framework The Schema of the proposed KG-RAG framework is shown in Fig 4. The following sections explain each component of this framework. 
 Fig 4. Schema for the Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) Framework. The direction of the arrows indicates the flow of the pipeline in this framework



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 12):

13  
Previous studies have utilized KG in conjunction with LLM for knowledge intensive tasks such as question-answering,(Yasunaga et al. 2021) multi-hop relational reasoning,(Feng et al. 2020) commonsense reasoning,(Lin et al. 2019; Lv et al. 2019) and model pre-training.(Moiseev et al. 2022; Yasunaga et al. 2022) Furthermore, enhancing prompts by incorporating structured knowledge has been described and studied.(Xiang Chen Zhejiang University, China et al.; Lewis et al. 2020; Pan et al. 2023) Naturally, these approaches have bolstered the positive reinforcement between KG and LLM. Nevertheless, it's worth noting that these approaches are only be task-specific and in some cases, the knowledge infusion could grow exponentially by the inclusion of higher order relations.(Bauer, Wang and Bansal 2018; Lin et al. 2019) Such approaches could compromise the limited token space of the LLM. Alternative methods employed knowledge infusion through the direct use of query languages like SPARQL.(Brate et al. 2022) However, this could render the system constraint to the schema of the underlying KG, potentially affecting the flexibility and adaptability of prompts. Moreover, as the KG expands and its schema grows, it could potentially occupy a significant portion of the LLM input token space. This explains why we noticed a greater token usage with the Cypher-RAG method (average usage of 8006 tokens), as it incorporates the entire graph schema into the input prompt for converting natural language into structured Cypher queries. In contrast, KG-RAG requires minimal graph schema, thus eliminating the need to include it in the prompt and resulting in substantial token savings, with a reduction of over 50% in token utilization compared to Cypher-RAG. This finding suggests that when dealing with a graph as large as SPOKE, which contains over 40 million nodes, Cypher-RAG requires LLMs that enable a larger context window. In contrast, KG-RAG is capable of managing this with LLMs that require a relatively smaller window size.



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 7):

8  
 Fig 2. Hyperparameter analysis and RAG comparison (A) Hyperparameter analysis performance curves using prompts with single (top) and two (below) disease entities mentioned in it. The x-axis denotes the ‘Context volume’ (number of associations from KG) and the y-axis denotes the mean performance (Jaccard similarity) across the prompts. The red curve denotes ‘PubMedBert’ and the blue curve denotes ‘MiniLM’ transformer models. (B) shows the comparative analysis between KG-RAG (green color) and Cypher-RAG (red color) in terms of retrieval accuracy (top) and token usage (bottom). Insight shows an example where Cypher-RAG fails to retrieve context from the KG when the input prompt is slightly perturbed, but KG-RAG remains robust in context retrieval. It is evident that KG-RAG has lesser token usage (average of 53.9% reduction in token usage) when compared to Cypher-RAG (bottom). Error bar in the token utilization bar plot (bottom) represents standard error of the mean (sem).     RAG comparative analysis Fig 2B shows the comparative analysis between the proposed KG-RAG and the existing Cypher-RAG approach for context retrieval from a KG (See Materials and Methods). We compared these two frameworks based on their retrieval accuracy, retrieval robustness and token usage. For a test dataset with 100 biomedical questions (S1 Text), Cypher-RAG and KG-RAG showed 75% and 97% retrieval accuracy respectively (Fig 2B top). To test the robustness in context retrieval,



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 11):

12  
Discussion In this work, we introduce a simple but highly effective framework that combines a biomedical knowledge graph with LLM chat models in a token optimized fashion. This integration resulted in a domain-specific generative system whose responses were firmly grounded in well-established biomedical knowledge. We compared the proposed framework with another RAG approach that utilizes Cypher query and showed that KG-RAG was more robust to prompt perturbation and more efficient in token utilization. In addition, KG-RAG consistently demonstrated superior performance compared to the prompt-based baseline LLM model on all human-curated benchmark datasets. We hypothesize that this performance improvement arises from the fusion of the explicit knowledge from the KG and the implicit knowledge from the LLM. This shows the value of providing domain-specific ground truth at a fine-grained resolution as context at the prompt level.   A heterogeneous knowledge graph with diverse concepts (the biomedical concepts in this case) interconnected at a massive scale has the potential to generate new knowledge as an “emergent property”.(Baranzini et al. 2022; Morris et al. 2023) In fact, as LLMs scale up in various dimensions like model parameters, training data, and training compute, they have been thought to exhibit reasoning or “emerging capabilities”(Wei et al. 2022a) although this observation could also be explained by “in-context learning” or other aspects of the examples.(Brown et al. 2020; Min et al. 2022; Lu et al. 2023) In any case, KG-RAG capitalized this capability and generated biomedical text with rich annotations such as provenance and statistical evidence (if available) thereby resulting in more reliable and knowledge-grounded responses. Additionally, the optimized and fine-grained context retrieval capability of KG-RAG ensured a budget friendly RAG system to apply on proprietary LLMs.



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 3):

4  
shot,(Kojima et al. 2022) few-shot,(Brown et al. 2020) and Chain-of-Thought(Wei et al. 2022b) prompting strategies.  Although prompt tuning methods have proven to be effective, their performance is restricted to knowledge-intensive tasks that require providing provenance and up-to-date knowledge about the world to address the user prompt. To address such knowledge-intensive tasks, an alternative approach which integrates knowledge graphs (KG) with language models was recently introduced.(Lin et al. 2019; Lv et al. 2019; Wang et al. 2019; Yang et al. 2019; Feng et al. 2020; Yasunaga et al. 2021, 2022) This approach was primarily implemented in question-answering tasks, where the structured information contained within the KG was used to provide context for predicting the answer to the question. While such multimodal integrative approach showed promise, its downstream supervised training was tailored to a specific task, limiting its versatility and broad applicability, thereby constraining its ability to fully harness the "emergent capabilities" of LLMs.(Wei et al. 2022a) Retrieval-augmented generation (RAG) involves enhancing a parametric pre-trained LLM with the ability to access a non-parametric memory containing updated knowledge about the world (for e.g., Wikipedia or SPOKE)(Lewis et al. 2020)  In this paper, we propose a robust and token-optimized framework called KG-RAG that integrates a KG with a pre-trained LLM within a RAG framework, thus capturing the best of both worlds. To achieve this, we make use of the biomedical KG called (SPOKE (Morris et al. 2023)) that  integrates more than 40 publicly available biomedical knowledge sources of separate domains where each source is centered around a biomedical concept, such as genes, proteins, drugs, compounds, diseases, and one or more of their known relationships. Because these concepts are recurrent entities forming defined sets (e.g., all named human genes, all FDA



### Claim 9/36

#### Claim Text
To be specific, we utilize the GTE embedding model [45] "gte_sentence-embedding"4, which is currently the top-performing model for text vector embedding in the retrieval field.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[45]_2308.03281.pdf (Page 0):

Towards General Text Embeddings with Multi-stage Contrastive Learning
Zehan Li1, Xin Zhang1, Yanzhao Zhang1, Dingkun Long1, Pengjun Xie1, Meishan Zhang
1Alibaba Group
{lizehan.lzh,linzhang.zx,zhangyanzhao.zyz,
dingkun.ldk,pengjun.xpj}@alibaba-inc.com
Abstract
We present GTE, a general-purpose text embed-
ding model trained with multi-stage contrastive
learning. In line with recent advancements in
unifying various NLP tasks into a single for-
mat, we train a unified text embedding model
by employing contrastive learning over a di-
verse mixture of datasets from multiple sources.
By significantly increasing the number of train-
ing data during both unsupervised pre-training
and supervised fine-tuning stages, we achieve
substantial performance gains over existing em-
bedding models. Notably, even with a relatively
modest parameter count of 110M, GTEbase out-
performs the black-box embedding API pro-
vided by OpenAI and even surpasses 10x larger
text embedding models on the massive text
embedding benchmark. Furthermore, without
additional fine-tuning on each programming
language individually, our model outperforms
previous best code retrievers of similar size by
treating code as text. In summary, our model
achieves impressive results by effectively har-
nessing multi-stage contrastive learning, offer-
ing a powerful and efficient text embedding
model with broad applicability across various
NLP and code-related tasks.1
1 Introduction
Text embeddings have became an indispensable
component in many natural language processing
tasks, such as text classification, text retrieval, ques-
tion answering and dialogue systems (Karpukhin
et al., 2020; Humeau et al., 2020; Choi et al., 2021;
Izacard et al., 2022a; Long et al., 2022a; Rajapakse,
2023). These embedding models represent texts us-
ing low-dimensional vectors and capture their sim-
ilarity through vector operations. The emergence
of recent large language models (LLMs) (Radford
et al., 2018; Touvron et al., 2023; OpenAI, 2023)
has generated considerable interest in retrieval-
1The GTE model is publicly available at https://
huggingface.co/thenlper/gte-large
UnsupervisedContrastive Pre-training on Massive Text Pairs mined from the WebSupervisedContrastive Fine-tuningon Annotated Text Triples from Multiple Tasks
… …
MSMARCO
NaturalQuestionsTriviaQAWebQuestionsHotpotQAMNLI
QuoraStackExchangeDupWebSearch
OpenQA NaturalLanguageInferenceSNLI
FactVerificationFEVER
Paraphrase
MEDIOthersBERRI
Figure 1: Illustration of the multi-stage contrastive learn-
ing pipeline used to train our text embedding model.
augmented systems based on text embedding mod-
els that integrate the reasoning and comprehension
capabilities of LLMs (Izacard et al., 2022b; Ram
et al., 2023; Shi et al., 2023). Consequently, there
has been a growing focus on general text represen-
tation in both industry and academia.
The pursuit of developing a unified model to ad-
dress a multitude of downstream tasks has been
long-standing due to the diverse formats, domains
and downstream applications of natural language.
The emergence of pre-trained language models has
further opened up possibilities for training such a
universal model. Nonetheless, within the realm
of text representation research, previous text em-
bedding models have primarily focused on specific
tasks, and their training strategies or models, tai-
lored to a single task, may not perform optimally
in other contexts. For example, the text represen-
tation model SimCSE (Gao et al., 2021), trained
on symmetric text pairs, demonstrates limitations
in text retrieval tasks. Similarly, certain text rep-
resentation models specifically designed for dense
retrieval tasks do not exhibit robust performance
in sentence textual similarity tasks. Recently, there
has been a shift in research focus towards develop-
ing more comprehensive models for text represen-
tation leveraging large quantities of unlabeled web
data through unsupervised contrastive pre-training,
coupled with task-specific data, prompts, or in-
structions to mitigate task conflicts during fine-
tuning (Ni et al., 2022a,b; Neelakantan et al., 2022;
arXiv:2308.03281v1  [cs.CL]  7 Aug 2023



Source: data\tc17_2312.15883v2\referenced_papers\[45]_2308.03281.pdf (Page 1):

Wang et al., 2022b; Su et al., 2023). Additionally,
the introduction of benchmarks, such as the Mas-
sive Text Embedding Benchmark (MTEB) (Muen-
nighoff et al., 2023), has established a robust basis
for assessing the universality of text representation
models. However, a significant limitation in ex-
isting research is the reliance on in-house data for
pre-training, creating a bottleneck in the utilization
of pre-trained model weights or APIs. Furthermore,
the formulation of prompts specifically tailored for
each task requires extra human effort during imple-
mentation (Su et al., 2023).
This work presents a straightforward approach
to construct a general text embedding (GTE) model
solely using contrastive learning on open-source
data, as illustrated in Figure 1. Specifically, we
first gather a large-scale dataset comprising un-
supervised text pairs extracted from various data
sources for contrastive pre-training. Surprisingly,
our model, pre-trained on this dataset, exhibits re-
markable performance, surpassing BM25 and E5
model (Wang et al., 2022b) in zero-shot text re-
trieval tasks and surpassing many supervised mod-
els in the MTEB benchmark. To further enhance
the quality of the learned text representations, we
obtain high-quality text pairs with human labels
from multiple sources for contrastive fine-tuning.
After supervised fine-tuning, our 110M BERT-
based (Devlin et al., 2019) model already outper-
forms the current commercial embedding API of
OpenAI and ranks highly in the MTEB benchmark.
Furthermore, since our model is trained using code
data as well, we evaluate its code search capabili-
ties on the CodeSearchNet benchmark, which en-
compasses six programming languages. Notably,
even without language-specific fine-tuning on each
subset, our model significantly outperforms state-
of-the-art code retrievers of similar size that have
been fine-tuned for each programming language.
In the rest of this paper, we provide a detailed
account of the data sources and training configu-
rations employed. Subsequently, we present the
evaluation results on widely recognized text em-
bedding benchmarks and compare them with the
performance of previous state-of-the-art baselines
that were specifically optimized for each individual
task. Our model consistently demonstrates supe-
rior performance or, at the very least, comparable
results to those achieved by larger models, owing
to its incorporation of a more diverse mixture of
training datasets. We aspire for our model to serve
as a robust baseline for the research community
investigating text and code embedding.
2 Related Work
Text embeddings serve as low-dimensional vector
representations for texts of varying lengths and are
essential in numerous natural language processing
(NLP) tasks. In contrast to high-dimensional and
sparse representations such as TF-IDF, dense text
embeddings possess the capacity to address the lex-
ical mismatch problem and enhance the efficiency
of text retrieval and matching.
Pre-trained language models, exemplified by
BERT (Devlin et al., 2019) and GPT (Radford
et al., 2018), have demonstrated remarkable suc-
cess across various NLP tasks. Nonetheless, ex-
tracting a high-quality sentence embedding from
pre-trained language models poses a significant
challenge due to the presence of anisotropic em-
bedding spaces resulting from the masked language
modeling objective. To address this issue, subse-
quent studies have proposed different approaches,
including supervised fine-tuning (Reimers and
Gurevych, 2019), normalizing flow (Li et al., 2020),
normalizing flow (Li et al., 2020), whitening (Su
et al., 2021), or unsupervised contrastive learn-
ing (Gao et al., 2021). These investigations pri-
marily concentrate on enhancing performance in
semantic textual similarity tasks, wherein two sen-
tences exhibit similar formats.
Another line of research focuses on the text re-
trieval problem, where the query and document
typically exhibit an asymmetric relationship. In
this context, the dual-encoder architecture necessi-
tates training with both positive and negative pairs.
Lee et al. (2019) propose the Inverse Close Task
(ICT) as a self-supervised pre-training approach for
generating a dense retriever. The ICT method in-
volves cropping a random sentence from a passage
to construct pseudo query-document pairs. Ad-
ditionally, Chang et al. (2020) leverage the link
structure within Wikipedia to introduce further su-
pervision signals in the pre-training data. In a sim-
ilar vein, REALM (Guu et al., 2020) proposes a
joint training approach, wherein a dense retriever
and a language model are trained concurrently. The
learning signal for the language model is derived
from masked language modeling, with backpropa-
gation incorporated through the retrieval step. Re-
cent advancements, such as Contriever (Izacard
et al., 2022a) and coCondenser (Gao and Callan,



Source: data\tc17_2312.15883v2\referenced_papers\[45]_2308.03281.pdf (Page 5):

Model Params LR GPUs BS Base LM
GTEsmall 30M 3 × 10−4 2 16384 microsoft/MiniLM-L12-H384-uncased
GTEbase 110M 2 × 10−4 4 16384 bert-base-uncased
GTElarge 330M 5 × 10−5 8 16384 bert-large-uncased
Table 3: Pre-training configurations of models of different sizes.
4 Experiments
In this section, we provide an extensive evaluation
of our embedding model, comparing to state-of-
the-art models for each task. Note that an apple-to-
apple comparison is hardly possible since different
models used different in-house data for pre-training
and the base language models vary a lot. We mainly
use the number of model parameters as a criterion
for performance comparison since it is closely re-
lated to the inference speed.
4.1 Zero-shot Text Classification
Model Params Prompting Accuracy
E5base 110M ✓ 81.3
E5large 330M ✓ 85.3
cpt-text 6B 88.1
cpt-text 6B ✓ 89.1
GTEbase 110M 85.1
GTEbase 110M ✓ 87.2
Table 4: Zero shot text classification performance on
SST-2. All compared models are the fine-tuned ones.
One method to assess the quality of learned
representation is through zero-shot classifica-
tion. (Radford et al., 2021; Neelakantan et al., 2022;
Wang et al., 2022b). We recast text classification
into an embedding-based similarity matching prob-
lem. In this setting, inputs texts are converted into
embeddings directly and labels are verbalized to
corresponding text to get label embeddings. Dis-
tances between input embeddings and label embed-
dings are measured by their inner product and label
with the most close embedding distance to the in-
put text is regarded as the classification result. An
example is SST-2 binary sentiment classification
task. We consider two types of label verbalizers
for evaluation. The vanilla version uses the sen-
timent word ‘positive’ or ‘negative’ to denote the
corresponding labels. Prompted version uses fuzzy
prompt template, such as ‘this is an example of
positive/negative movie review’.
Zero-shot text classification accuracy on SST-
2 is shown in Table 4. In the vanilla setting, our
110M model already matches the performance of
prompted E5large with 330M parameters. Using
prompting strategy further improves results signifi-
cantly and closes the gap with large models. Even
without explicit prompt or instruction during train-
ing, our model can somewhat understand the label
context better when formatted as a natural language
text.
4.2 Unsupervised Text Retrieval
Text retrieval requires retrieving most relevant doc-
uments from a large-scale candidate sets. We
use BEIR (Thakur et al., 2021) as our evalua-
tion benchmark for zero-shot unsupervised text
retrieval. BEIR is a heterogeneous information re-
trieval benchmark which contains retrieval tasks of
different formats and from different domains. We
use the open available 15 datasets for evaluation.
We compare our unsupervised pre-trained check-
point to recent unsupervised dense retrievers such
as Contriever (Izacard et al., 2022a) and E5 (Wang
et al., 2022b). According to Table 5, we find that
our base size model significantly outperforms the
models with comparable size, like SimCSE, Con-
triever and E5. Our base model is comparable to
E5large without using human supervision.
4.3 Massive Text Embedding Benchmark
Massive Text Embedding Benchmark (MTEB) is a
comprehensive semi-supervised benchmark that in-
corporates a limited amount of supervision data for
evaluation. In this paper, we evaluate the English
subsets which encompasses 56 English datasets
across seven distinct tasks, including text classi-
fication (Class.), text clustering (Clust.), pairwise
classification (Pair.), text reranking (Rerank.), text
retreival (Retr.), semantic textual similarity (STS)
and summarization (Summ.). The evaluation met-
rics employed in MTEB are accuracy, v-measure,
average precision, MAP, nDCG@10, and Spear-
man coefficients, respectively. For further details



Source: data\tc17_2312.15883v2\referenced_papers\[82]_2309.07597.pdf (Page 5):

SIGIR ’24, July 14–18, 2024, Washington, DC, USA Shitao Xiao et al.
Table 2: Performance of various models on C-MTEB.
model Dim Retrieval STS Pair CLF CLF Re-rank Cluster Average
Text2Vec (base) 768 38.79 43.41 67.41 62.19 49.45 37.66 48.59
Text2Vec (large) 1024 41.94 44.97 70.86 60.66 49.16 30.02 48.56
Luotuo (large) 1024 44.40 42.79 66.62 61.0 49.25 44.39 50.12
M3E (base) 768 56.91 50.47 63.99 67.52 59.34 47.68 57.79
M3E (large) 1024 54.75 50.42 64.30 68.20 59.66 48.88 57.66
Multi. E5 (base) 768 61.63 46.49 67.07 65.35 54.35 40.68 56.21
Multi. E5 (large) 1024 63.66 48.44 69.89 67.34 56.00 48.23 58.84
OpenAI-Ada-002 1536 52.00 43.35 69.56 64.31 54.28 45.68 53.02
BGE (small) 512 63.07 49.45 70.35 63.64 61.48 45.09 58.28
BGE (base) 768 69.53 54.12 77.50 67.07 64.91 47.63 62.80
BGE (large) 1024 71.53 54.98 78.94 68.32 65.11 48.39 63.96
(Enc, Dec indicate the encoding and decoding operations, 𝑋, ˜𝑋
indicate the clean and polluted text.)
•General purpose fine-tuning . The pre-trained model is fine-
tuned on C-MTP (unlabeled) via contrastive learning, where it is
learned to discriminate the paired texts from their negative samples:
min .
∑︁
(𝑝,𝑞)
−log 𝑒⟨e𝑝,e𝑞 ⟩/𝜏
𝑒⟨e𝑝,e𝑞 ⟩/𝜏 +Í
𝑄′𝑒⟨e𝑝,e𝑞′⟩/𝜏 .
(𝑝 and 𝑞are the paired texts, 𝑞′∈𝑄′is a negative sample, 𝜏 is the
temperature). One critical factor of contrastive learning is the neg-
ative samples. Instead of mining hard negative samples on purpose,
we purely rely on in-batch negative samples [25] and resort to a
big batch size (as large as 19,200) to improve the discriminativeness
of the embedding.
•Task-specific fine-tuning. The embedding model is further
fine-tuned with C-MTP (labeled). The labeled datasets are smaller
but of higher quality. However, the contained tasks are of different
types, whose impacts can be mutually contradicted. In this place,
we apply two strategies to mitigate this problem. On one hand, we
leverage instruction-based fine-tuning [7, 50], where the input is
differentiated to help the model accommodate different tasks. For
each text pair (𝑝, 𝑞), a task specific instruction 𝐼𝑡 is attached to the
query side: 𝑞′←𝑞+𝐼𝑡 . The instruction is a verbal prompt, which
specifies the nature of the task, e.g., “search relevant passages for
the query ”. On the other hand, the negative sampling is updated: in
addition to the in-batch negative samples, one hard negative sample
𝑞′is mined for each text pair (𝑝, 𝑞). The hard negative sample is
mined from the task’s original corpus, following the ANN-style
sampling strategy in [61].
4 EXPERIMENTS
In this section, we conduct experimental studies for the exploration
of following problems. P1. The extensive evaluation of different
Chinese text embeddings on C-MTEB. P2. The empirical verifica-
tion of the text embeddings by BGE. P3. The exploration of the
practical value brought by C-MTP. P4. The exploration of the im-
pacts introduced by the training recipe. We consider the following
popular Chinese text embedding models as the baselines for our
experiments: Text2Vec-Chinese10 base and large; Luotuo11; M3E12
base and large; multilingual E5 [53] and OpenAI text embedding
ada 00213. The main metric presented in Section 3.1 is reported for
each evaluation task in C-MTEB.
4.1 General Evaluation
We extensively evaluate BGE against popular Chinese text embed-
dings on C-MTEB as shown in Table 2.14, where we can make the
following observations.
First, our models outperform existing Chinese text embeddings
by large margins. There is not only an overwhelming advantage in
terms of the average performance, but also notable improvements
for the majority of tasks in C-MTEB. The biggest improvements
are on the retrieval task followed by STS, pair classification, and
re-ranking. Such aspects are the most common functionalities of
text embeddings, which are intensively utilized in applications
like search engines, open-domain question answering, and the
retrieval augmentation of large language models. Although the
advantages for classification and clustering tasks are not as obvious,
our performances are still on par or slightly better than the other
most competitive models. The above observations verify the strong
generality of BGE. Our models can be directly utilized to support
different types of application scenarios.
Second, we observe performance growth resulting from the scal-
ing up model size and embedding dimension. Particularly, the aver-
age performance improves from 58.28 to 63.96, when the embed-
ding model is expanded from small to large. Besides the growth in
average performance, there are also improvements across all the
evaluation tasks. Compared to the other two baselines (Text2Vec,
M3E), the impact of scaling up is more consistent and significant
for our models. It is worth noting that our small model is still em-
pirically competitive despite its highly reduced model size, where
the average performance is even higher than the large-scale op-
tion of many existing models. As a result, it provides people with
the flexibility to trade-off embedding quality and running efficiency :
people may resort to our large-scale embedding model to deal with
10https://huggingface.co/shibing624
11https://huggingface.co/silk-road/luotuo-bert-medium
12https://huggingface.co/moka-ai
13https://platform.openai.com/docs/guides/embeddings
14Our BGE models are named BGE in the tables.



Source: data\tc17_2312.15883v2\referenced_papers\[45]_2308.03281.pdf (Page 2):

2022), have demonstrated that constructing posi-
tive pairs through random passage cropping yields
superior results compared to the ICT task. Building
upon the ideas presented in (Chang et al., 2020),
some researchers have also put forth methods for
constructing higher-quality positive pairs using the
web link topology for retriever pre-training (Zhou
et al., 2022), a technique that proves effective in
zero-shot scenarios. Furthermore, in the field of
dense retrieval, significant research is dedicated to
enhancing the text representation capabilities of
pre-trained language models through the design of
auxiliary pre-training tasks (Gao and Callan, 2021;
Xiao et al., 2022; Gao and Callan, 2022; Wang
et al., 2022a; Long et al., 2022b; Li et al., 2023).
The previous two lines of research can be gen-
eralized as learning a vector representation for a
piece of text and distinguished by the type of down-
stream tasks. Recently, several studies have ex-
plored the construction of unified text representa-
tion models through large-scale contrastive learn-
ing and prompt-based learning (Neelakantan et al.,
2022; Wang et al., 2022b; Su et al., 2023). Ad-
ditionally, some research efforts have focused on
constructing evaluation datasets to better assess
the stability of text representation models across
different tasks and domains. BEIR (Benchmark-
ing IR) (Thakur et al., 2021) collects a substantial
number of retrieval tasks from various domains to
evaluate the robustness of dense retriever models in
zero-shot scenarios. Meanwhile, MTEB (Massive
Text Embedding Benchmark) (Muennighoff et al.,
2023) benchmarks over 56 datasets spanning seven
categories, providing a comprehensive evaluation
of text embedding models.
This study aims to develop a general text em-
bedding model through a multi-stage training ap-
proach. In the initial stage of unsupervised con-
trastive learning, we generate weak supervised cor-
relation text pairs using publicly available data
from various sources. Unlike previous study (Wang
et al., 2022b), we exclusively utilized open-source
data and did not employ any filtering or cleaning
methods. Pre-training on a large-scale text pairs
can effectively improve the domain generalization
of text representation models and bridge the gap
between the MLM training objective and the con-
trastive learning objective of representation models,
making the language model more suitable for text
representation tasks. In the supervised fine-tuning
stage, the mixture of training data in our approach
is more varied to further enhance the model’s ver-
satility. Moreover, our model does not incorpo-
rate task-specific prompts, which enhances repro-
ducibility and ease of use.
3 Approach
The training process of our model consists of two
stages: unsupervised pre-training and supervised
fine-tuning. Both stages employ the learning ob-
jective of contrastive learning. Firstly, we will in-
troduce the basic framework of the model. Subse-
quently, we will discuss the sources and construc-
tion methods of the training data in the two stages.
Finally, we will present some special optimization
strategies used to enhance the model’s performance
during the training process.
3.1 Model Architecture
The backbone of our embedding model is a deep
Transformer encoder (Vaswani et al., 2017) which
can be initialized with pre-trained language models
such as BERT (Devlin et al., 2019). Our model
follows the vanilla dual-encoder architecture with
mean pooling on top of the contextualized token
representations produced by the language model.
Formally, given a piece of textx = (x1, . . . , xn)
consisting of n tokens, an embedding modelE con-
vert the text into a low-dimensional dense vector
x = E(x) ∈ Rd. To implement E, we first employ
a language model to get the deep contextualized
token representations
h = LM(x) ∈ Rn×d. (1)
Then we apply a lightweight mean pooling
across the first dimension to get the text representa-
tion,
x = 1
n
nX
i=1
hi ∈ Rd (2)
The text representations are learned through the
contrastive objective, distinguishing semantic rele-
vant text pairs from irrelevant ones. Such training
procedure requires positive and negative pairs, tak-
ing the format of (q, d+, d−). For a query q, a rel-
evant document d+, a set of irrelevant documents
D− = {d−
1 , . . . , d−
n }, one popular contrastive ob-
jective is the InfoNCE loss (van den Oord et al.,
2018),
Lcl = −log es(q,d+)/τ
es(q,d+)/τ +
nP
i=1
es(q,d−
i )/τ
, (3)



### Claim 10/36

#### Claim Text
For the reranker base model, we use the "bge_reranker_large" 5 [82], trained through large-scale text pairs with asymmetric instruction tunning, to map text to a low-dimensional dense vector to rerank 𝑡𝑜𝑝𝐾 documents.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 9):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
Transformers-based models are commonly employed as rerankers, leveraging their capability to enhance the effective-
ness of information retrieval systems by capturing intricate relationships and contextual information within documents
and queries.
The initial utilization of transformers within a multi-stage ranking framework was presented in the study [28]. Their
proposed model, known as monoBERT, transforms the ranking process into a relevance classification problem. It
achieves this by sorting texts based on the conditional probability P(Relevant = 1|di; q), where q is the query and
di represents documents [21]. The model simultaneously processes both queries and documents. This simultaneous
processing leads to a more enriched interaction between them, often resulting in improved effectiveness [29], [30].
However, this neural models have a substantial number of parameters and the scoring of query-document pairs occurs
at inference time. This, in turn, increases computational costs and latency. This kind of model is also known by
cross-encoder. Refer to Figure 10 for an illustration of the query-document pair processing.
Figure 10: Cross-Encoder
In the other hand, monoT5 is a sequence-to-sequence reranker [31] that uses T5 models [32] to generate relevance
scores between a pair of query and document. T5 models treat all tasks as text-to-text, requiring some adaptations
for the query/document similarity task. During training, the format 'Query: {query} Document: {document}
Relevant:' is used, with labels yes if the document is relevant to the query, andno otherwise. At inference time, the
same format as the training data is used to format pairs of queries and documents to feed the model, and a single-token
greedy decode is performed. The score is then obtained by calculating the softmax value considering only the tokens
no and yes, and selecting the value corresponding to the yes token. Note that the tokens no and yes are the tokens
used in the version provided by [33], but the publication that introduced the monoT5 architecture, [31], uses the tokens
false and true. Figure 11 contains an illustration of the monoT5 architecture’s inference process.
Figure 11: monoT5’s inference process
In our experiments, Pyserini BM25 was employed as first-stage retrieval, retrieving 50 documents to be reranked in the
second stage. In the second stage, we utilized the model unicamp-dl/mt5-base-en-pt-msmarco-v2, a sequence-
to-sequence reranker trained on pairs of queries and documents in English and Portuguese from the dataset [33].



Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 8):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
set D and search results from different methods r in R, for each d in D, we can calculate the RRFscore(d ∈ D) as
follows [27]:
RRFscore(d ∈ D) =
X
r∈R
1
k + r(d), (2)
Considering that 1/r(d) is known as the reciprocal rank, where r(d) represents the position at which the document d
was retrieved by the search mechanism r. The term k is introduced to assist in controlling outlier systems [27].
Figure 8: Hybrid Search schema with k=1.
Figure 8 shows how to calculate RRFscore for k = 1. In the example, we have four chunks that were retrieved in
different orders by two search methods, BM25 (sparse search), and Custom ADA-002 (dense search). The reciprocal
rank score is calculated for each chunk. These values are then summed, creating a new score. The final hybrid list is
an ordering of chunks that uses this new score.
Table 5: Retriever comparison. Where MRR is the Mean Recriprocal Rank metric and R@k is the Recall.
Metric Hybrid-BM25-ADA-002 Hybrid-BM25-Custom ADA-002
MRR@10 0.758 0.850
R@3 0.829 0.921
R@5 0.879 0.943
R@7 0.921 0.964
R@9 0.957 0.979
In our experiments, only Pyserini’s BM25 was tested as the sparse retriever, while ADA-002 and Custom ADA-002
were tested as dense retrievers. The hybrid combination that yielded the best results was the one that used BM25 and
Custom ADA-002.
5.1.5 Reranker
The fundamental idea underlying multi-stage ranking is to divide document ranking into a sequence of stages. After
an initial retrieval, which usually involves a sparse retriever or dense retriever, each subsequent stage re-evaluates and
reranks the set of candidates forwarded from the preceding stage. Figure 9 represents a multi-stage pipeline where
Pyserini BM25 performs the first stage and the candidate chunks are then re-evaluated by the reranker. After that, the
final reranked list called retrieved chunks is presented as final result formed by k chunks.
Figure 9: Reranker Pipeline



Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 13):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
[14] O. Press, N. A. Smith, and M. Lewis, “Train short, test long: Attention with linear biases enables input length
extrapolation,” 2022.
[15] S. Chen, S. Wong, L. Chen, and Y . Tian, “Extending context window of large language models via positional
interpolation,” 2023.
[16] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian, E. Bakhturina, M. Shoeybi, and B. Catanzaro,
“Retrieval meets long context large language models,” 2023.
[17] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang, “Lost in the middle: How
language models use long contexts,” 2023.
[18] G. Kamradt, “Pressure testing claude-2.1 200k via needle-in-a-haystack,” 2023. [Online]. Available:
https://github.com/gkamradt/LLMTest_NeedleInAHaystack
[19] J. Liu, “Llamaindex,” 2022. [Online]. Available: https://github.com/jerryjliu/llama_index
[20] G. Mohandas and P. Moritz. (2023) Building rag-based llm applications for production. [Online]. Available:
https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1
[21] J. Lin, R. F. Nogueira, and A. Yates, “Pretrained transformers for text ranking: BERT and beyond,” CoRR, vol.
abs/2010.06467, 2020. [Online]. Available: https://arxiv.org/abs/2010.06467
[22] S. Humeau, K. Shuster, M. Lachaux, and J. Weston, “Real-time inference in multi-sentence tasks with deep
pretrained transformers,”CoRR, vol. abs/1905.01969, 2019. [Online]. Available: http://arxiv.org/abs/1905.01969
[23] G. M. Rosa, R. C. Rodrigues, R. Lotufo, and R. Nogueira, “Yes, bm25 is a strong baseline for legal case retrieval,”
2021.
[24] D. Brown, “Rank-BM25: A Collection of BM25 Algorithms in Python,” 2020. [Online]. Available:
https://doi.org/10.5281/zenodo.4520057
[25] J. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, and R. Nogueira, “Pyserini: An easy-to-use python toolkit to
support replicable ir research with sparse and dense representations,” 2021.
[26] M. Henderson, R. Al-Rfou, B. Strope, Y . hsuan Sung, L. Lukacs, R. Guo, S. Kumar, B. Miklos, and R. Kurzweil,
“Efficient natural language response suggestion for smart reply,” 2017.
[27] G. V . Cormack, C. L. A. Clarke, and S. Büttcher, “Reciprocal rank fusion outperforms condorcet and
individual rank learning methods,” Proceedings of the 32nd international ACM SIGIR conference on Research
and development in information retrieval, 2009. [Online]. Available: https://api.semanticscholar.org/CorpusID:
12408211
[28] R. F. Nogueira, W. Yang, K. Cho, and J. Lin, “Multi-stage document ranking with BERT,” CoRR, vol.
abs/1910.14424, 2019. [Online]. Available: http://arxiv.org/abs/1910.14424
[29] J. Guo, Y . Fan, L. Pang, L. Yang, Q. Ai, H. Zamani, C. Wu, W. B. Croft, and X. Cheng, “A deep look into neural
ranking models for information retrieval,” 2019.
[30] B. Mitra and N. Craswell, “An introduction to neural information retrieval,” Foundations and Trends®
in Information Retrieval , vol. 13, no. 1, pp. 1–126, 2018. [Online]. Available: http://dx.doi.org/10.1561/
1500000061
[31] R. F. Nogueira, Z. Jiang, and J. Lin, “Document ranking with a pretrained sequence-to-sequence model,” CoRR,
vol. abs/2003.06713, 2020. [Online]. Available: https://arxiv.org/abs/2003.06713
[32] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu, “Exploring the
limits of transfer learning with a unified text-to-text transformer,” 2023.
[33] L. H. Bonifacio, I. Campiotti, R. de Alencar Lotufo, and R. F. Nogueira, “mmarco: A multilingual
version of MS MARCO passage ranking dataset,” CoRR, vol. abs/2108.13897, 2021. [Online]. Available:
https://arxiv.org/abs/2108.13897
[34] W. X. Zhao, J. Liu, R. Ren, and J.-R. Wen, “Dense text retrieval based on pretrained language models: A survey,”
2022.



Source: data\tc17_2312.15883v2\referenced_papers\[45]_2308.03281.pdf (Page 5):

Model Params LR GPUs BS Base LM
GTEsmall 30M 3 × 10−4 2 16384 microsoft/MiniLM-L12-H384-uncased
GTEbase 110M 2 × 10−4 4 16384 bert-base-uncased
GTElarge 330M 5 × 10−5 8 16384 bert-large-uncased
Table 3: Pre-training configurations of models of different sizes.
4 Experiments
In this section, we provide an extensive evaluation
of our embedding model, comparing to state-of-
the-art models for each task. Note that an apple-to-
apple comparison is hardly possible since different
models used different in-house data for pre-training
and the base language models vary a lot. We mainly
use the number of model parameters as a criterion
for performance comparison since it is closely re-
lated to the inference speed.
4.1 Zero-shot Text Classification
Model Params Prompting Accuracy
E5base 110M ✓ 81.3
E5large 330M ✓ 85.3
cpt-text 6B 88.1
cpt-text 6B ✓ 89.1
GTEbase 110M 85.1
GTEbase 110M ✓ 87.2
Table 4: Zero shot text classification performance on
SST-2. All compared models are the fine-tuned ones.
One method to assess the quality of learned
representation is through zero-shot classifica-
tion. (Radford et al., 2021; Neelakantan et al., 2022;
Wang et al., 2022b). We recast text classification
into an embedding-based similarity matching prob-
lem. In this setting, inputs texts are converted into
embeddings directly and labels are verbalized to
corresponding text to get label embeddings. Dis-
tances between input embeddings and label embed-
dings are measured by their inner product and label
with the most close embedding distance to the in-
put text is regarded as the classification result. An
example is SST-2 binary sentiment classification
task. We consider two types of label verbalizers
for evaluation. The vanilla version uses the sen-
timent word ‘positive’ or ‘negative’ to denote the
corresponding labels. Prompted version uses fuzzy
prompt template, such as ‘this is an example of
positive/negative movie review’.
Zero-shot text classification accuracy on SST-
2 is shown in Table 4. In the vanilla setting, our
110M model already matches the performance of
prompted E5large with 330M parameters. Using
prompting strategy further improves results signifi-
cantly and closes the gap with large models. Even
without explicit prompt or instruction during train-
ing, our model can somewhat understand the label
context better when formatted as a natural language
text.
4.2 Unsupervised Text Retrieval
Text retrieval requires retrieving most relevant doc-
uments from a large-scale candidate sets. We
use BEIR (Thakur et al., 2021) as our evalua-
tion benchmark for zero-shot unsupervised text
retrieval. BEIR is a heterogeneous information re-
trieval benchmark which contains retrieval tasks of
different formats and from different domains. We
use the open available 15 datasets for evaluation.
We compare our unsupervised pre-trained check-
point to recent unsupervised dense retrievers such
as Contriever (Izacard et al., 2022a) and E5 (Wang
et al., 2022b). According to Table 5, we find that
our base size model significantly outperforms the
models with comparable size, like SimCSE, Con-
triever and E5. Our base model is comparable to
E5large without using human supervision.
4.3 Massive Text Embedding Benchmark
Massive Text Embedding Benchmark (MTEB) is a
comprehensive semi-supervised benchmark that in-
corporates a limited amount of supervision data for
evaluation. In this paper, we evaluate the English
subsets which encompasses 56 English datasets
across seven distinct tasks, including text classi-
fication (Class.), text clustering (Clust.), pairwise
classification (Pair.), text reranking (Rerank.), text
retreival (Retr.), semantic textual similarity (STS)
and summarization (Summ.). The evaluation met-
rics employed in MTEB are accuracy, v-measure,
average precision, MAP, nDCG@10, and Spear-
man coefficients, respectively. For further details



Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 7):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
the application of Multiple Negative Ranking Loss, as presented in the article "Efficient Natural Language Response
Suggestion for Smart Reply," Section 4.4 [26]. However, given our current focus on simplicity, we will reserve the
exploration of this technique for future work. At this moment, we choose to utilize the Mean Squared Error (MSE)
Loss.
For the fine-tuning stage, it is necessary to have two types of samples:
• positives: [question i, chunki, label = 1]
• negatives: [question i, chunkj, label = −1], for i ̸= j
Often, as is the case with our dataset, only positive examples are available. However, through a simple and random
shuffling, it is possible to generate negative examples. Demonstrating confidence in transfer learning, we found that
a few examples were sufficient. Our final dataset consisted of approximately 400 examples, maintaining a 1 : 3ratio
between positive and negative examples.
The hyperparameters that exert the most significant impact on performance include the learning rate, batch size, and
the number of dimensions in the projection matrix. The ADA-002 model has 1536 dimensions, and the projection
matrix is of size 1536 × N, where N ∈ 1024, 2048, 4096. In our experiments, we observed that 2048 dimensions
resulted in the best accuracy.
This type of fine-tuning requires low GPU resources, with a training time of approximately 5 minutes using the A100
GPU. The model itself is straightforward, consisting of a matrix with dropout (to mitigate overfitting), followed by the
hyperbolic tangent activation function, which provided additional accuracy gains in the training set.
Figure 6: Cosine similarity of positive and negative classes in the ADA-002 embedding; note the significant overlap
between the classes. Test accuracy (before training): 69.5%
Figure 7: Cosine similarity of positive and negative classes in the customized embedding; the intersection between the
classes is minimal. Test accuracy (after training): 84.3%
When analyzing the cosine similarity between positive and negative classes, we can observe the "shadow" shared by
the histograms. In an ideal scenario, we desire the classes to be disjoint to ensure a clear definition of space. Figure 6
illustrates a significant shadow in the embedding before training, while Figure 7 shows the result after training. Both
graphs are derived from the test set. Test accuracy also improved, leading to a better dense representation.
5.1.4 Hybrid Search
As stated before, hybrid search is applied when is necessary to combine results from two or more retrieval methods.
A widely used algorithm to address this type of problem is known as Reciprocal Rank Fusion (RRF). For a document



### Claim 11/36

#### Claim Text
Our experiments are conducted on two open-source query sets: MMCU-Medical [ 94] and CMB-Exam [ 77] datasets, which are designed for multi-task Q&A and encompass single and multiple-choice questions in the medical field, and one open-domain Q&A dataset CMB-Clin [77] which is the inaugural multi-round question-answering dataset based on real, complex medical diagnosis and treatment records.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[77]_2308.08833.pdf (Page 3):

DepartmentInternal
Medicine
Surgery Urology Neurology Hepatobiliary Endocrinology Pediatrics GynecologyOrthopedicsDermatology
Percentage (%)15.57 14.87 13.51 12.16 10.81 8.11 8.11 6.76 4.06 4.06
Table 3: Distribution of medical records across various departments.
Diagnosis processTreatment
principles
DiagnosisDifferential
diagnosis
Medical test
recommendation
Medical
history analysis
Operational
knowledge
Indications
for surgery
Percentage (%) 30.09 18.44 14.32 12.71 11.65 9.7 3.09
Table 4: Distribution of questions located within the consultation process.
ble grammatical or transcription errors, we use the
comment system of Medtiku to delete data items
with "problematic", "wrong question" and "incor-
rect" comments. Comparison of different directo-
ries before and after deletion is shown in Table 5.
Data Statistics Finally, we obtained a total of
280,839 multiple-choice questions. To assess the
model’s comprehension of medical knowledge, we
randomly selected 400 questions from each sub-
category as a test set. Additionally, to facilitate
experiments with few-shot learning strategies, we
randomly selected 10 questions from each subcat-
egory as a development set. We then enlisted the
help of three medical specialists to generate expla-
nations for each of these questions, specifically for
the purpose of conducting chain-of-thought experi-
ments (example shown in Figure 4). The remaining
269,359 questions were used as the train set.
3.3 Creation of CMB-Clin
CMB-Clin is designed to investigate models’ pro-
ficiency in knowledge application amidst real-life
diagnosis and treatment circumstances.
Data Preprocessing In order to obtain a high-
quality dataset, we initially collected 108 cases
with questions and answers from a variety of offi-
cial medical textbooks. These problems covered
a wide range of disease types. Subsequently, we
performed quality screening based on the follow-
ing criteria: Firstly, we eliminated problems that
required reliance on image information, such as
those that needed CT, MRI, and other imaging data
for resolution. Secondly, we selected problems that
contain sufficient diagnostic information to answer
the questions. Lastly, we removed cases with simi-
lar diseases to ensure the diversity of the problems.
After such screening, we finally obtained 74 high-
quality, complex, and real cases with 208 questions,
as exemplified in Figure 1, for the construction of
the CMB-Clin subset.
Task Formulation We transform the question
answering task into the multi-turn dialogue task.
Specifically, for each case with questions, we sim-
ulate dialogue interactions between an examiner
and a candidate, focusing on assessing the model’s
diagnostic and therapeutic capabilities.
Figure 1 illustrates the structure of each case,
which includes three main parts: I) Description
D: patient information, including medical history
summaries and chief complaints, physical exam-
inations (e.g., visual and tactile inspection), and
ancillary examinations (e.g., biopsy and CT scans);
II) Questions Q: questions related to diagnosis
and treatment based on descriptions. Some ques-
tions might be interrelated; and III) References R:
corresponding reference answers to questions.
Formally, to simulate dialogue interactions, we
concatenate the patient’s description with the previ-
ous question-answer pairs and the current question
in each conversation round (e.g., the k-th round).
This concatenated input, denoted as x, is repre-
sented as follows: x = Di + Qi + Ri + . . . Qi+k.
The reference answer for this input is Ri+k. For
the response ˆRi+k, we will evaluate it from four
dimensions, including: Fluency, Relevance, Com-
pleteness, and Proficiency. These dimensions are
adopted as per suggestions from experts.
Data Distribution We commissioned a medical
professional to examine the distribution of ques-
tions within the CMB-Clin from two perspectives:
the department associated with the medical record,
and the point in the consultation process where
the question arises. Table 3 displays the distri-
bution of medical records across various depart-
ments. It reveals a wide-ranging coverage, with in-
ternal medicine and surgery representing the most
substantial segments (15.57% and 14.87% respec-
tively). Table 4 illustrates where in the consultation
process the questions are situated. It indicates that
questions permeate the entire diagnostic procedure,
with a significant portion arising during the treat-
ment principles stage (30.09%).



Source: data\tc17_2312.15883v2\referenced_papers\[77]_2308.08833.pdf (Page 1):

with regional, climatic, dietary, and lifestyle charac-
teristics, over its long historical evolution. In TCM,
a disease has two aspects: "bìng" and "zhèng". The
former is often translated as "disease entity". The
latter, and more important one, is usually translated
as "pattern". For example, the disease entity of
a common cold might present with a pattern of
wind-cold in one person, and with the pattern of
wind-heat in another1.
Simultaneously, it poses significant challenges
when applying the Western medical framework to
a local environment, which needs cross-cultural
communication and understanding. In terms of dis-
ease diagnosis, examination of the tongue and the
pulse are among the principal diagnostic methods
in TCM. For example, redness on the tip of the
tongue might indicate heat in the Heart, while red-
ness on the sides of the tongue might indicate heat
in the Liver. For drugs, there are roughly 13,000
compounds used in China and over 100,000 TCM
recipes recorded in the ancient literature. There-
fore, we should adopt a native medical benchmark
instead of a translated medical benchmark for a
local environment. Note that the precise translation
of medical terminologies necessitates both medical
professions and the cultural context in the target
language.
CMB’s Philosophy. The CMB dataset com-
prises two parts: CMB-Exam, featuring multiple-
choice questions from qualification exams, and
CMB-Clin, including complex clinical diagnos-
tic questions derived from real case studies. The
dataset spans 6 major categories and 28 subcat-
egories, totaling 280,839 multiple-choice ques-
tions. For CMB-Exam, we selected 400 ques-
tions from each subcategory to create an evaluation
set. Additionally, CMB-Clin is formed from 74
expert-curated medical record consultations, drawn
from clinical diagnostic teaching materials. Each
multiple-choice question in the dataset offers four
to six options, with one or more correct answers.
The clinical diagnostic questions are based on real,
intricate cases, with correct answers determined by
a consensus of teaching experts.
Take-away messages from CMB. After bench-
marking various LLMs in CMB, we get the follow-
ing observations that might be insightful. I) GPT-4
and recent open-sourced LLMs such as Qwen-72B-
Chat and Yi-34B-Chat, have achieved an accuracy
1https://en.wikipedia.org/wiki/Traditional_
Chinese_medicine#Six_Excesses
rate exceeding 60%, surpassing the threshold re-
quired for obtaining license; II) Accuracy exhibits
significant disparities across professional levels and
knowledge areas, notably between traditional Chi-
nese medicine and Western medicine; III) The
effectiveness of the CoT and few-shot prompts
varies among models with different accuracy levels,
especially presenting potential risks in knowledge-
intensive tasks; and IV) Results of automatic eval-
uation using GPT-4 highly agree with expert eval-
uation results.
2 The Philosophy of CMB
2.1 The Overall Philosophy
We surveyed different medical professionals (physi-
cians, nurses, technicians, and pharmacists) about
the exams they encountered in their career develop-
ment. Our research focused on common assess-
ment types, leading us to select two key tasks
for further study: multiple-choice questions and
iterative questioning based on complex medical
records. The former evaluates the model’s knowl-
edge grasp, while the latter assesses its practical
problem-solving skills. Both tasks, having standard
answers, provide reliable and stable performance
indicators.
2.2 Philosophy of CMB-Exam
Existing medical benchmarks, sourced from the in-
ternet (Li et al., 2023b), hospitals, etc., face privacy
and accuracy challenges. We opted for qualifica-
tion exams as our data source, creating the CMB-
Exam subset. This choice is due to two key advan-
tages: (I) qualification exams offer objective and
typically accurate ground truths; (II) they provide
a clear benchmark, namely a 60% accuracy rate,
which corresponds to the expertise level in specific
domains. The multiple-choice questions in CMB-
Exam encompass four clinical medical professions:
physicians, nurses, medical technicians, and phar-
macists. These exams span the entire professional
journey, from undergraduate basics, graduate se-
lections, standardized tests, professional qualifica-
tions, to intermediate and advanced professional
title exams.
In the Chinese medical field, significant work has
been done on multiple-choice tasks. MLEC-QA (Li
et al., 2021) compiled 21,700 manually annotated
questions from the Chinese National Licensed Phar-
macist Examination. Similarly, CMExam (Liu
et al., 2023) gathered 68,119 tagged questions from



Source: data\tc17_2312.15883v2\referenced_papers\[77]_2308.08833.pdf (Page 2):

Category Subcategory # Subject # Questions
Physician (医师) Resident Physician (住院医师); Licensed Assistant Physician (执业助理医师); Licensed Physician (执业医师); Associate
Professional Physician (中级职称); Advanced Professional Physicians (高级职称)
81 124,926
Nurse (护理) Practicing Nurse ( 护士); Licensed Practical Nurse (护师); Charge Nurse (主管护师); Advanced Practice Nurse (高级护师) 8 16,919
Technicians (医技) Medical Technician ( 医技士); Medical Technologist (医技师); Supervising Technologist (主管技师) 21 27,004
Pharmacist (药师) Licensed Pharmacist (执业西药师); Licensed TCM Pharmacist (执业中药师); Junior Pharmacist (初级药师); Junior
Pharmacist Assistant (初级药士); Junior TCM Pharmacist (初级中药师); Junior TCM Pharmacist Assistant (初级中药士);
Chief Pharmacists (主管药师); Chief TCM Pharmacists (主管中药师)
8 33,354
Undergraduate Dis-
ciplines (学科考试)1
Fundamental Medicine (基础医学); Clinical Medicine (临床医学); Traditional Chinese (TCM) and Chinese Herbal Medicine
(中医学与中药学); Preventive Medicine and Public Health (预防医学与公共卫生学)
53 62,271
Graduate Entrance
Exam (考研)
Integrated Western Medicine (西医综合); Integrated TCM (中医综合); Political Science (政治); Nursing (护理学) 5 16,365
Total 28 176 280,839
1 We referenced the National Standard Subject Classification of the People’s Republic of China, seehttps://xkb.pku.edu.cn/docs/2018-10/20220328083301969071.
pdf.
Table 1: Statistics of the CMB-Exam Categories, Subcategories, Subjects, and Questions.
Split #subcategory #Q per subcategory #Q in total
Test 28 400 11,200
Dev 28 10 1 280
Train 28 - 2 269,359
1 It is with explanations in dev set.
2 Each subcategory has a different number of questions.
Table 2: Data split in CMB-Exam.
the same exam. However, it’s important to note
that the potential assistance provided by LLMs in
medical professions is not confined to just phar-
macy. It also encompasses a broad range of other
health-related occupations, such as nursing and
medical technology, among others. Given that the
Licensed Pharmacist Examination represents only
a fraction of the career growth spectrum, its limited
knowledge scope and occupational coverage do
not provide detailed feedback. To address this, we
compiled CMB-Exam, encompassing all medical-
related occupations and the full range of exams
encountered throughout their professional develop-
ment.
2.3 Philosophy of CMB-Clin
Besides the theoretical exam content in CMB-
Exam, the second subset, CMB-Clin, focuses on
practical skills. This subset comprises complex
clinical diagnostic problems to test the model’s
synthesis of knowledge and reasoning. It requires
the model to utilize its medical knowledge for an-
swering questions and to analyze case reports for
informed responses. CMB-Exam and CMB-Clin
together offer a comprehensive evaluation frame-
work, applicable to both the career development of
medical professionals and the learning trajectory
of medical LLMs. To our knowledge, CMB-Clin
is the inaugural multi-round question-answering
dataset based on real, complex medical records.
3 Dataset Creation
3.1 Taxonomy of CMB-Exam
To obtain a precise taxonomy of medical evaluation,
we aligned it with the disciplinary and examination
systems of the medical field. First, we chose four
main medical professions: physicians, pharmacists,
medical technicians, and nurses, covering various
occupational difficulty levels of examinations. Con-
sidering the learning trajectories and professional
growth paths, we additionally include discipline
examinations and graduate entrance examinations
for these four professions, ultimately resulting in
six categories: Physician, Nurse, Technician, Phar-
macist, Undergraduate Disciplines, and Graduate
Entrance Exam. One could refer to Table 1 for
the detailed taxonomy. Moreover, we carried out
a more detailed subject division within each sub-
category, resulting in a total of 174 categories, the
detailed directory list of which can be found in Ap-
pendix B. Through this structured arrangement, our
directory structure reflects characteristics closely
connected to the actual medical field, providing a
solid foundation for further analysis and research.
3.2 Creation of CMB-Exam
Data Sources The data is derived from publicly
available examination questions and coursework
exercises with clear solutions provided by experts.
A significant portion of these materials comes from
the Medtiku2, from which we obtain explicit per-
mission to share the data.
Data Preprocessing Questions undergo a stan-
dardized data preprocessing procedure, including
de-duplication and cleansing. For character recog-
nition errors caused by OCR, we conduct a large
amount of manual calibration to ensure the con-
sistency with the original document. For possi-
2https://www.medtiku.com/



Source: data\tc17_2312.15883v2\referenced_papers\[77]_2308.08833.pdf (Page 12):

A Related work
A.1 Medical Benchmark
Medical benchmarks have evolved to broadly en-
compass two types of tasks based on the capa-
bilities of the models they seek to probe: ob-
jective tasks and subjective tasks. The former
typically assumes the form of multiple-choice
questions (Welbl et al., 2018; Pal et al., 2022;
Hendrycks et al., 2021b; Singhal et al., 2022),
information retrieval (Abacha et al., 2017; Zhu
et al., 2019; Abacha et al., 2019), and cloze-style
reading comprehension (Suster and Daelemans,
2018; Pampari et al., 2018; Zhu et al., 2020),
which serve to evaluate a model’s medical knowl-
edge with unbiased accuracy. Sources for these
tasks range from medical textbooks and exams
to case reports (Suster and Daelemans, 2018),
Wikipedia (Welbl et al., 2018), and medical prac-
tices (Hendrycks et al., 2021b; Pal et al., 2022).
In contrast, subjective tasks involve open-ended
text generation constructed directly from con-
sumer queries and doctor responses, often sourced
from online medical forums. The task typically
demands models to generate consumer-oriented
replies (Singhal et al., 2022; Li et al., 2023b) or ex-
planations for multiple-choice questions (Liu et al.,
2023). As of now, there are relatively few open-
ended text generation question-answering tasks that
specifically center around providing consultation
based on diagnostic reports.
Few existing benchmark datasets encapsulate
both task types, with PromptCBLUE (Zhu et al.,
2023), MultiMedQA (Singhal et al., 2022) and
CMExam (Liu et al., 2023) sharing the closest re-
semblance to our work. Our dataset exceeds in size
and includes questions not only from the Chinese
National Medical Licensing Examination but also
from various authoritative medical textbooks.
Several other datasets have been developed that
contribute to the ongoing advancement of medical
NLP, albeit with a narrower career focus. For in-
stance, the ExplainCPE (Li et al., 2023a) is solely
career-specific to pharmacists, while GenMedical-
Eval (Yang et al., 2023), Medbench (Cai et al.,
2024) and Medbench_Opencompass6 are exclusive
to physician careers. Our dataset includes ques-
tions not only pertaining to physicians but also to
nurses, technicians, and pharmacists. These ques-
tions are derived from a mix of sources, including
6https://medbench.opencompass.org.cn/home
the Chinese National Medical Licensing Examina-
tion and various authoritative medical textbooks,
thereby offering a larger and more comprehensive
resource than previously available datasets.
A.2 Other Benchmarks of Large Language
Models
The explosive growth in the number and capability
of LLMs has led to a multitude of works aiming
to discern their true capacity, evaluating both their
general and specific abilities. General ability bench-
marks include comprehensive test suites, each tar-
geting different aspects of LLM’s proficiency, rang-
ing from handling multi-turn dialogues (Zheng
et al., 2023) to gauging language comprehension
and reasoning abilities (Srivastava et al., 2022;
Zhang et al., 2023a; Zhong et al., 2023).
In terms of specific abilities, several benchmarks,
apart from those related to medicine, aim to evalu-
ate different capabilities of models. ARB (Sawada
et al., 2023) was introduced to assess LLMs’ per-
formance in high-level reasoning tasks across mul-
tiple domains. C-Eval (Huang et al., 2023) serves
as the first comprehensive benchmark to evaluate
the advanced knowledge and reasoning abilities
of Chinese-based models. Gaokao (Zhang et al.,
2023b), MATH (Hendrycks et al., 2021c), and
APPS (Hendrycks et al., 2021a) focus on assess-
ing LLM proficiency in complex, context-specific
tasks, and code generation, respectively.
B Dataset
Table 15, 16, 17 present a detailed directory struc-
ture of CMB-Exam. Initially, the organization is
based on clinical professions and the exams com-
monly undertaken by these professionals, divided
into six primary sections. Upon this foundation,
each section is further categorized based on career
progression and examination subjects. Within each
sub-category, we have meticulously classified ac-
cording to specific departments or courses.
B.1 Keywords in the TCM subcatalog and
Medqa-USMLE
We used jieba7 and NLTK8 to perform 2-gram word
segmentation on the CMB TCM subcatalog and
Medqa-USMLE (Jin et al., 2020) respectively, and
compared the top 5 keywords (excluding words
with no medical meaning). The results are shown
7https://github.com/fxsjy/jieba
8https://www.nltk.org/



Source: data\tc17_2312.15883v2\referenced_papers\[77]_2308.08833.pdf (Page 4):

Category Physician Nurse Technicians Pharmacist Undergraduate Disciplines Graduate Entrance Exam
Before Processing125,102 16,923 27,008 33,362 62,435 16,367
After Processing 124,926 16,919 27,004 33,354 62,271 16,365
Table 5: Sample Numbers of different directories before and after Preprocessing
现病史
病史摘要：周XX，男，25岁，1年前因车祸致小肠损伤，行部分小肠切除吻合术。术中切除小肠约40cm。术后病人恢复良好，未诉特殊不适。1个月前病人无明
显诱因突然出现腹痛、腹胀、停止排气排便，至急诊就诊，诊断为肠梗阻，急诊行剖腹探查术。术中发现小肠与腹壁多处粘连带压迫小肠肠管，距屈氏韧带
60cm以下至距回盲部约25cm间小肠缺血坏死。遂切除坏死肠管，行空肠、回盲部端侧吻合 。 术后至今病人每日大便10余次，为水样便，量较多，无脓血，无
腹痛、腹胀。 查体：体温：36.9℃，血压：115/78mmHg，脉搏：96次/分。腹部平坦，全腹无压痛、反跳痛，无肌紧张。 辅助检查：大便常规提示：WBC 
0-1/HPF，潜血（-）。血常规提示：WBC 5.5×109/L，Hb 102g/L，血小板计数：192×109/L。血生化提示：Na+：134mmol/L，K+：3.4mmol/L， 
Ca2+：2.42mmol/L，Mg2+：0.75mmol/L。近1个月体重下降约4kg，睡眠质量差。 
问题1：该病人目前的诊断是什么？
参考答案：短肠综合征（急性期）。病人因车祸及术后粘连致肠坏死两次行小肠切除手术，小肠残留长度不足100cm，术后不足2个月，出现了大量腹泻、电解
质紊乱及体重明显下降，符合短肠综合征急性期的临床表现。
问题2：该病人的治疗方法是什么？
参考答案：①尽快给予全胃肠外营养支持，补充病人正常所需水份、钠钾镁等电解质、能量及营养物质；②给予抑制肠蠕动药物；③给予H2受体拮抗剂或质子
泵抑制剂；④腹泻症状初步被控制后，给予少量等渗肠内营养。
Present Medical History
Summary of Medical History: Mr. XX, male, 25 years old, had a small intestine injury due to a car accident one year ago, underwent partial small intestine 
resection and anastomosis surgery. Approximately 40cm of the small intestine was removed during the operation. The patient recovered well postoperatively 
and did not report any specific discomfort. One month ago, he suddenly experienced abdominal pain, abdominal distension, and cessation of gas and bowel 
movements without apparent cause. He sought emergency medical attention and was diagnosed with intestinal obstruction. Emergency exploratory laparotomy 
revealed adhesions between the small intestine and the abdominal wall, compressing the small intestine, with ischemic necrosis approximately 60cm below the 
ligament of Treitz to about 25cm proximal to the ileocecal junction. The necrotic segment was removed, and an end-to-end anastomosis between the jejunum 
and the ileocecal region was performed. Since the surgery, the patient has been having more than 10 watery bowel movements per day, with a significant 
amount, no pus or blood, and no abdominal pain or distension. Physical Examination: Temperature: 36.9℃, Blood Pressure: 115/78mmHg, Pulse: 96 
beats/min. The abdomen is flat, with no tenderness, rebound tenderness, or muscle tension throughout. Ancillary Examinations: Stool routine: WBC 
0-1/HPF, occult blood (-). Blood Routine: WBC 5.5×109/L, Hb 102g/L, Platelet count: 192×109/L. Blood Biochemistry: Na+: 134mmol/L, K+: 3.4mmol/L, 
Ca2+: 2.42mmol/L, Mg2+: 0.75mmol/L. Weight loss of approximately 4kg in the past month, poor sleep quality.
Question 1: What is the current diagnosis of the patient?
Reference: Short Bowel Syndrome (acute phase). The patient underwent two small intestine resection surgeries due to a car accident and postoperative 
adhesions leading to intestinal necrosis. The remaining length of the small intestine is less than 100cm. Less than two months after surgery, the patient has 
developed significant diarrhea, electrolyte disturbances, and substantial weight loss, which are consistent with the clinical manifestations of the acute phase of 
Short Bowel Syndrome.
Question 2: What is the treatment method for this patient?
Reference: ① Provide total parenteral nutrition support as soon as possible to supplement the patient's normal water, electrolytes such as sodium, potassium, 
and magnesium, energy, and nutrients; ② Administer drugs to inhibit intestinal motility; ③ Administer H2 receptor antagonists or proton pump inhibitors; ④ 
After the symptoms of diarrhea are initially controlled, provide a small amount of isotonic enteral nutrition.
Figure 1: Case of Short Bowel Syndrome from CMB-Clin. English translations are shown for better readability.
4 Experiments on CMB-Exam
4.1 Experimental Setup
Models We evaluate the following Chinese med-
ical LLMs to compare their performance on
CMB-Exam: HuatuoGPT-II (Chen et al., 2023a),
ChatMed-Consult (Zhu and Wang, 2023), Medi-
calGPT (Xu, 2023), ChatGLM-Med (Wang et al.,
2023b), DoctorGLM (Xiong et al., 2023), BianQue-
2 (Chen et al., 2023b), Bentsao (Wang et al.,
2023a), IvyGPT (Wang et al., 2023c), Sun-
simiao (Xin Yan, 2023) and DISC-MedLLM (Bao
et al., 2023). In addition to these special-
ized models, we also evaluate some publicly-
available general-domain instruction-following
model series, namely ChatGLM3-6B (Du et al.,
2022), Baichuan2 (Baichuan, 2023), Qwen (Bai
et al., 2023), Yi3, Deepseek-llm (AI, 2023), Mis-
tral (Jiang et al., 2023) and Internlm (Team, 2023).
For closed source commercial models, we evaluate
ChatGPT4, GPT-45, ShunkunGPT, AntGLM-Med
and JianpeiGPT. We also test the performance of
DISC-MedLLM trained on CMB-Exam-Train. All
experiments are conducted in August, 2023. Please
3https://github.com/01-ai/Yi
4We use the version gpt-3.5-turbo-16k-0613.
5We use the version gpt-4.
refer to Appendix C for more details about models
and training.
Decoding Hyperparameters For all the afore-
mentioned open source models, we adopt their de-
fault hyper-parameters. Besides, to reduce the vari-
ance in generation, we adopt greedy decoding for
all models on both CMB-Exam and CMB-Clin.
And the min_new_tokens and max_new_tokens
are set to 1 and 512, respectively, to avoid empty
or lengthy answers on CMB-Exam.
Evaluation Details We evaluate the models in
both answer-only and chain-of-thought (CoT) set-
tings. We extract answers from model outputs
using an empirically designed regular expression.
Each extracted answer is compared to the solution
and is deemed correct if and only if they are exactly
matched. We adopt accuracy as our metric. All
evaluation experiments and training experiments
take around 1000 GPU-hours on 8 NVIDIA A800
80GB GPUs.
4.2 Benchmarking Results
We report the results in Table 6 and Table 7. There
are several observations drawn from different as-
pects.



### Claim 12/36

#### Claim Text
To fairly verify whether HyKGE can effectively enhance LLMs, we selected the following two types of generaldomain large models as the base model and explored the gains brought by HyKGE: GPT 3.5 and Baichuan13B-chat [87]. 5.1.4 Compared Methods.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 8):

conversational dataset generated by ChatGPT.
• Chinese-Alpaca-Plus-13B (Cui et al., 2023): A
language model trained by fine-tuning LLaMA-
13B on the conversational dataset generated by
ChatGPT.
• XVERSE-13B: A 13B multilingual large
language model trained on more than 1.4 trillion
tokens.
5.1 Overall Performance
This section introduces the overall performance
of Baichuan 2 base models compared with other
similar-sized models. We choose 8 benchmarks
for comparison: MMLU (Hendrycks et al., 2021a)
The Massive Multitask Language Understanding
consists of a range of multiple-choice questions
on academic subjects. C-Eval (Huang et al.,
2023) is a comprehensive Chinese evaluation
benchmark consists of more than 10k multi-choice
questions. CMMLU (Li et al., 2023) is also a
general evaluation benchmark specifically designed
to evaluate the knowledge and reasoning abilities of
LLMs within the context of the Chinese language
and culture. AGIEval (Zhong et al., 2023) is a
human-centric benchmark specifically designed to
evaluate general abilities like human cognition and
problem-solving. Gaokao (Zhang et al., 2023)
is an evaluation framework that utilizes Chinese
high school entrance examination questions. BBH
(Suzgun et al., 2022) is a suite of challenging
BIG-Bench (Srivastava et al., 2022) tasks that the
language model evaluations did not outperform the
average human-rater. GSM8K (Cobbe et al., 2021)
is an evaluation benchmarks that focused on math.
HumanEval (Chen et al., 2021) is a docstring-to-
code dataset consisting of 164 coding problems
that test various aspects of programming logic.
For CMMLU and MMLU, we adopt the official
implementations and adopt 5-shot for evaluation.
For BBH we adopt 3-shot evaluations. For C-Eval,
Gaokao, and AGIEval we only select the multiple-
choice with four candidates for better evaluations.
For GSM8K, we adopt 4-shot testing derived from
OpenCompass (OpenCompass, 2023). We also
incorporate the result of GPT-4 6 and GPT-3.5-
Turbo7. Unless stated otherwise, the results in this
paper were obtained using our internal evaluation
tools.
The overall result is shown in Table 1. Compared
6gpt-4-0613
7gpt-3.5-turbo-0613
with other similar-sized open-sourced models,
our model has a clear performance advantage.
Especially in math and code problems, our model
achieves significant improvement over Baichuan 1.
5.2 Vertical Domain Evaluations
We also evaluate Baichuan 2 in vertical domains,
where we choose the law and medical field as they
has been widely studied in recent years.
In the law field, we report scores of JEC-QA
(Zhong et al., 2020), which is collected from the
National Judicial Examination of China. It contains
multiple-choice and multiple-answer questions.
For compatibility with our evaluation suite, we
only test the multiple-choice questions.
In the medical field, we report scores from two
medical benchmarks, MedQA (Jin et al., 2021)
and MedMCQA (Pal et al., 2022), as well as
average scores from medical-related disciplines in
C-Eval (val), MMLU, and CMMLU (abbreviated
as CMC). Specifically, MedMCQA is collected
from the professional medical board exams in
the USA and China, including three subsets, i.e.,
USMLE, MCMLE and TWMLE, and we report
the results of USMLE and MCMLE with five
candidates; MedMCQA is collected from from
Indian medical entrance exams, and we evaluate
multiple-choice questions and report the scores in
the dev set. The detail of MedMCQA includes
(1) clinical medicine, basic medicine of C-Eval
(val), (2) clinical knowledge, anatomy, college
medicine, college biology, nutrition, virology,
medical genetics, professional medicine of MMLU,
(3) anatomy, clinical knowledge, college medicine,
genetics, nutrition, traditional chinese medicine,
virology of CMMLU. Moreover, all these datasets
are evaluated in 5-shot.
As shown in Table 5 Baichuan 2-7B-Base
surpasses models such as GPT-3.5 Turbo,
ChatGLM 2-6B, and LLaMA 2-7B in the field of
Chinese law, second only to GPT-4. Compared
to Baichuan 1-7B, Baichuan 2-7B-Base shows an
improvement of nearly 10 points. In the medical
field, Baichuan 2-7B-Base outperforms models
like ChatGLM 2-6B and LLaMA 2-7B, showing
significant improvement over Baichuan 1-7B as
well.
Similarly, Baichuan 2-13B-Base surpasses
models other than GPT-4 in the field of Chinese
law. In the medical domain, Baichuan 2-13B-
Base outperforms models such as XVERSE-13B



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 10):

JEC-QA CMC USMLE MCMLE MedMCQA
GPT-4 59.32 77.16 80.28 74.58 72.51
GPT-3.5 Turbo 42.31 61.17 53.81 52.92 56.25
LLaMA-7B 27.45 33.34 24.12 21.72 27.45
LLaMA2-7B 29.20 36.75 27.49 24.78 37.93
MPT-7B 27.45 26.67 16.97 19.79 31.96
Falcon-7B 23.66 25.33 21.29 18.07 33.88
ChatGLM2-6B 40.76 44.54 26.24 45.53 30.22
Baichuan 1-7B 34.64 42.37 27.42 39.46 31.39
7B
Baichuan 2-7B-Base 44.46 56.39 32.68 54.93 41.73
LLaMA-13B 27.54 35.14 28.83 23.38 39.52
LLaMA 2-13B 34.08 47.42 35.04 29.74 42.12
Vicuna-13B 28.38 40.99 34.80 27.67 40.66
Chinese-Alpaca-Plus-13B 35.32 46.31 27.49 32.66 35.87
XVERSE-13B 46.42 58.08 32.99 58.76 41.34
Baichuan 1-13B-Base 41.34 51.77 29.07 43.67 39.60
13B
Baichuan 2-13B-Base 47.40 59.33 40.38 61.62 42.86
Table 5: The result of Baichuan 2 compared with other models on law and medical filed.
GSM8K MATH HumanEval MBPP
GPT-4 89.99 40.20 69.51 63.60
GPT-3.5 Turbo 57.77 13.96 52.44 61.40
LLaMA-7B 9.78 3.02 11.59 14.00
LLaMA 2-7B 16.22 3.24 12.80 14.80
MPT-7B 8.64 2.90 14.02 23.40
Falcon-7B 5.46 1.68 - 10.20
ChatGLM 2-6B 28.89 6.40 9.15 9.00
Baichuan 1-7B 9.17 2.54 9.20 6.60
7B
Baichuan 2-7B-Base 24.49 5.58 18.29 24.20
LLaMA-13B 20.55 3.68 15.24 21.40
LLaMA 2-13B 28.89 4.96 15.24 27.00
Vicuna-13B 28.13 4.36 16.46 15.00
Chinese-Alpaca-Plus-13B 11.98 2.50 16.46 20.00
XVERSE-13B 18.20 2.18 15.85 16.80
Baichuan 1-13B-Base 26.76 4.84 11.59 22.80
13B
Baichuan 2-13B-Base 52.77 10.08 17.07 30.20
Table 6: The result of Baichuan 2 compared with other models on mathematics and coding.



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 6):

GPUs, achieving a computational efficiency that
exceeds 180 TFLOPS.
3 Alignment
Baichuan 2 also introduces the alignment
procedure resulting in two chat models: Baichuan
2-7B-Chat and Baichuan 2-13B-Chat. The
alignment process of the Baichuan 2 encompasses
two main components: Supervised Fine-Tuning
(SFT) and Reinforcement Learning from Human
Feedback (RLHF).
3.1 Supervised Fine-Tuning
During the supervised fine-tuning phase, we use
human labelers to annotate prompts gathered from
various data sources. Each prompt is labeled as
being helpful or harmless based on key principles
similar to Claude (2023). To validate data quality,
we use cross-validation—an authoritative annotator
checks the quality of a sample batch annotated by a
specific crowd worker group, rejecting any batches
that do not meet our quality standards.
We collected over 100k supervised fine-tuning
samples and trained our base model on them. Next,
we delineated the reinforcement learning process
via the RLHF method to further improve results.
The whole process of RLHF, including RM and RL
training, is shown in Figure 5.
Figure 5: An illustration of Baichuan 2’s RLHF process.
3.2 Reward Model
We devised a three-tiered classification system for
all prompts, consisting of 6 primary categories,
30 secondary categories, and over 200 tertiary
categories. From the user’s perspective, we aim
for the classification system to comprehensively
cover all types of user needs. From the standpoint
of reward model training, prompts within each
Score Gap 1 2 3 4 5
Test Acc. 54.5% 61.1% 70.2% 77.8% 81.5%
Table 4: Reward Model test accuracy on different score
gaps of two responses. The larger the response gap,
the better RM accuracy. The gap 1,2,3,4,5 correspond
to unsure, negligibly better, slightly better, better, and
significantly better, respectively.
category should have sufficient diversity to ensure
the reward model can generalize well.
Given a prompt, responses are generated by
Baichuan 2 models of different sizes and stages
(SFT, PPO) to enhance response diversity. Only
responses generated by the Baichuan 2 model
family are used in the RM training. Responses from
other open-source datasets and proprietary models
do not improve the reward model’s accuracy. This
also underscores the intrinsic consistency of the
Baichuan 2 model series from another perspective.
The loss function used for training the reward
model is consistent with that in InstructGPT
(Ouyang et al., 2022). The reward model
derived from training exhibits a performance
consistent with that of LLaMA 2 (Touvron et al.,
2023b), indicating that the greater the score
difference between two responses, the higher the
discriminative accuracy of the reward model, as
shown in Table 4.
3.3 PPO
After obtaining the reward model, we employ the
PPO (Schulman et al., 2017) algorithm to train
our language model. We employ four models: the
actor model (responsible for generating responses),
the reference model (used to compute the KL
penalty with fixed parameters), the reward model
(providing an overarching reward for the entire
response with fixed parameters), and the critic
model (designed to learn per-token values).
3.4 Training Details
During the RLHF training process, the critic model
is warmed up with an initial 20 training steps ahead.
Subsequently, both the critic and actor models are
updated via the standard PPO algorithm. For all
models, we use gradient clipping of 0.5, a constant
learning rate of 5e-6, and a PPO clip threshold
ϵ = 0.1. We set the KL penalty coefficient β =
0.2, decaying to 0.005 over steps. We train for
350 iterations for all our chat models, resulting in
Baichuan 2-7B-Chat and Baichuan 2-13B-Chat.



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 11):

zh-en zh-fr zh-es zh-ar zh-ru zh-ja zh-de Average
GPT-4 29.94 29.56 20.01 10.76 18.62 13.26 20.83 20.43
GPT-3.5 Turbo 27.67 26.15 19.58 10.73 17.45 1.82 19.70 17.59
LLaMA-7B 17.27 12.02 9.54 0.00 4.47 1.41 8.73 7.63
LLaMA 2-7B 25.76 15.14 11.92 0.79 4.99 2.20 10.15 10.14
MPT-7B 20.77 9.53 8.96 0.10 3.54 2.91 6.54 7.48
Falcon-7B 22.13 15.67 9.28 0.11 1.35 0.41 6.41 7.91
ChatGLM 2-6B 22.28 9.42 7.77 0.64 1.78 0.26 4.61 6.68
Baichuan 1-7B 25.07 16.51 12.72 0.41 6.66 2.24 9.86 10.50
7B
Baichuan 2-7B-Base 27.27 20.87 16.17 1.39 11.21 3.11 12.76 13.25
LLaMA-13B 21.75 16.16 13.29 0.58 7.61 0.41 10.66 10.07
LLaMA 2-13B 25.44 19.25 17.49 1.38 10.34 0.13 11.13 12.17
Vicuna-13B 22.63 18.04 14.67 0.70 9.27 3.59 10.25 11.31
Chinese-Alpaca-Plus-13B 22.53 13.82 11.29 0.28 1.52 0.31 8.13 8.27
XVERSE-13B 29.26 24.03 16.67 2.78 11.61 3.08 14.26 14.53
Baichuan 1-13B-Base 30.24 20.90 15.92 0.98 9.65 2.64 12.00 13.19
13B
Baichuan 2-13B-Base 30.61 22.11 17.27 2.39 14.17 11.58 14.53 16.09
Table 7: The result of Baichuan 2 compared with other models on multilingual field.
version from the SafeNLP project8, distinguishing
neutral and hate types for the 13 minority groups,
forming a 6-shot dataset consistent with the
original Toxigen prompt format. Our decoding
parameters use temperature 0.1 and top-p 0.9
nucleus sampling.
We use the fine-tuned HateBert version
optimized in the Toxigen (Hartvigsen et al., 2022)
for model evaluation. Table 8 shows that compared
to LLaMA 2, the Baichuan 2-7B and Baichuan
2-13B model has some safety advantages.
Model Toxigen ↓
Baichuan 2-13B 11.48
Baichuan 2-7B 11.72
LLaMA 2-7B 12.28
LLaMA 2-13B 13.24
Table 8: Toxigen results of Baichuan 2 foundation
models compared with LLaMA 2.
Inspired by BeaverTails Ji et al. (2023) 9, we
constructed the Baichuan Harmless Evaluation
Dataset (BHED), covering 7 major safety
categories of bias/discrimination, insults/profanity,
illegal/unethical content, physical health, mental
health, financial privacy, and sensitive topics to
evaluate the safety of our chat models.
8https://github.com/microsoft/SafeNLP/
tree/main
9https://github.com/PKU-Alignment/
beavertails
To ensure comprehensive coverage within each
category, We ask human annotators to generate
1,400 data samples. This was further expanded
through self-instruction and cleaned by humans
for fluency, resulting in 70,000 total samples with
10,000 per category. Examples of those safety
prompts and principles are shown in the Appendix
D.
We use those samples to evaluate different
models and the result is shown in Table 9. We
can see that Baichuan 2 is on par or outperforms
other chat models in our safety evaluations.
5.6 Intermediate Checkpoints
We will also release the intermediate checkpoints
of 7B models, from 220 billion tokens checkpoint
to 2,640 billion tokens checkpoint, which is the
final output of Baichuan 2-7B-Base. We examine
their performance on several benchmarks and the
result is shown in Figure 7.
As shown in the figure, Baichuan 2 demonstrates
consistent improvement as training proceeds. Even
after 2.6 trillion tokens, there appears to be ample
room for further gains. This aligns with previous
work on scaling LLMs indicating that data size is
a critical factor (Hoffmann et al., 2022). In the
Appendix C, we provide more detailed training
dynamics for both the 7B and 13B models.
6 Related Work
The field of language models has undergone a
renaissance in recent years, sparked largely by
the development of deep neural networks and



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 2):

C-Eval MMLU CMMLU Gaokao AGIEval BBH GSM8K HumanEval
GPT-4 68.40 83.93 70.33 66.15 63.27 75.12 89.99 69.51
GPT-3.5 Turbo 51.10 68.54 54.06 47.07 46.13 61.59 57.77 52.44
LLaMA-7B 27.10 35.10 26.75 27.81 28.17 32.38 9.78 11.59
LLaMA 2-7B 28.90 45.73 31.38 25.97 26.53 39.16 16.22 12.80
MPT-7B 27.15 27.93 26.00 26.54 24.83 35.20 8.64 14.02
Falcon-7B 24.23 26.03 25.66 24.24 24.10 28.77 5.46 -
ChatGLM 2-6B (base)∗ 51.70 47.86 - - - 33.68 32.37 -
Baichuan 1-7B 42.80 42.30 44.02 36.34 34.44 32.48 9.17 9.20
7B
Baichuan 2-7B-Base 54.00 54.16 57.07 47.47 42.73 41.56 24.49 18.29
LLaMA-13B 28.50 46.30 31.15 28.23 28.22 37.89 20.55 15.24
LLaMA 2-13B 35.80 55.09 37.99 30.83 32.29 46.98 28.89 15.24
Vicuna-13B 32.80 52.00 36.28 30.11 31.55 43.04 28.13 16.46
Chinese-Alpaca-Plus-13B 38.80 43.90 33.43 34.78 35.46 28.94 11.98 16.46
XVERSE-13B 53.70 55.21 58.44 44.69 42.54 38.06 18.20 15.85
Baichuan 1-13B-Base 52.40 51.60 55.30 49.69 43.20 43.01 26.76 11.59
13B
Baichuan 2-13B-Base 58.10 59.17 61.97 54.33 48.17 48.78 52.77 17.07
Table 1: Overall results of Baichuan 2 compared with other similarly sized LLMs on general benchmarks. * denotes
results derived from official websites.
Figure 1: The distribution of different categories of
Baichuan 2 training data.
Data processing: For data processing, we focus
on data frequency and quality. Data frequency
relies on clustering and deduplication. We built
a large-scale deduplication and clustering system
supporting both LSH-like features and dense
embedding features. This system can cluster
and deduplicate trillion-scale data within hours.
Based on the clustering, individual documents,
paragraphs, and sentences are deduplicated and
scored. Those scores are then used for data
sampling in pre-training. The size of the training
data at different stages of data processing is shown
in Figure 2.
2.2 Architecture
The model architecture of Baichuan 2 is based on
the prevailing Transformer (Vaswani et al., 2017).
Nevertheless, we made several modifications which
we detailed below.
2.3 Tokenizer
A tokenizer needs to balance two critical factors:
a high compression rate for efficient inference,
and an appropriately sized vocabulary to ensure
adequate training of each word embedding. We
have taken both these aspects into account. We
have expanded the vocabulary size from 64,000
in Baichuan 1 to 125,696, aiming to strike a
balance between computational efficiency and
model performance.
Tokenizer V ocab Size Compression Rate ↓
LLaMA 2 32,000 1.037
Bloom 250,680 0.501
ChatGLM 2 64,794 0.527
Baichuan 1 64,000 0.570
Baichuan 2 125,696 0.498
Table 2: The vocab size and text compression rate of
Baichuan 2’s tokenizer compared with other models.
The lower the better.
We use byte-pair encoding (BPE) (Shibata et al.,
1999) from SentencePiece (Kudo and Richardson,
2018) to tokenize the data. Specifically, we do not
apply any normalization to the input text and we



### Claim 13/36

#### Claim Text
We use GPT 3.5 and Baichuan13B-chat as base models. (2) Knowledge Graph Retrieval-Augmented Generation (KGRAG) [ 63–65] uses user query as a reference to retrieve 7https://cpubmed.openi.org.cn/graph/wiki 8https://github.com/nuolade/disease-kb 9https://www.wikipedia.org/ 10https://baike.baidu.com/ 11https://www.yixue.com/ Conference’17, July 2017, Washington, DC, USA Jiang, Zhang and Xu et al. in the KGs, which is the base model of RAG on KG and has been widely applied in [63–65]. (3) Query Expansion (QE) [ 5] reformulate the user’s initial query by adding additional terms with a similar meaning with the help of LLMs. (4) CHAIN-OF-NOTE (CoN) [93] generates sequential reading notes for retrieved knowledge, enabling a thorough evaluation of their relevance to the given question and integrating these notes to formulate the final answer. (5) Chain-of-Knowledge (CoK) [ 44] utilize the power of LLMs and consists of reasoning preparation, dynamic knowledge adapting, and answer consolidation. (6) Knowledge-Augmented Language Model Verification (KALMV) [ 6] verifies the output and the knowledge of the knowledge-augmented LLMs with a separate verifier. (7) Knowledge Graph Generative Pre-Training (KG-GPT) [ 34] comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions. (8) Summarizing Retrievals (SuRe) [ 33] constructs summaries of the retrieved passages for each of the multiple answer candidates and confirms the most plausible answer from the candidate set by evaluating the validity and ranking of the generated summaries.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[93]_2311.09210.pdf (Page 3):

the pertinence of each document to the query but
also pinpoints the most reliable information and
resolves conflicting information. This approach
effectively filters out irrelevant or less trustworthy
content, leading to responses that are both more
accurate and contextually relevant.
Given an input question x and k retrieved doc-
uments [d1, · ··, dk], the model aims to gener-
ate textual outputs comprising multiple segments
[yd1 , · ··, ydk , y]. Here, ydi signifies the tokens for
the i-th segment, representing the reading note for
the corresponding document di, as shown in Fig-
ure 2. After generating individual reading notes,
the model synthesizes the information to create a
consolidated final response y. The implementation
of the CHAIN -OF-NOTE (CON) involves three key
steps: (1) designing the notes ydi , (2) collecting the
data, and (3) training the model.
2.3.1 C HAIN -OF-NOTE Format Design
The framework primarily constructs three types of
reading notes, as shown in Figure 2 , based on the
relevance of the retrieved documents to the input
question: First, when a document directly answers
the query, the model formulates the final response
based on this relevant information, as shown in Fig-
ure 2(a). Second, if the retrieved document does
not directly answer the query but provides useful
context, the model leverages this information along
with its inherent knowledge to deduce an answer,
as shown in Figure 2(b). Third, in cases where the
retrieved documents are irrelevant, and the model
lacks sufficient knowledge to answer, it defaults
to responding with “unknown", as shown in Fig-
ure 2(c). This nuanced approach mirrors human
information processing, striking a balance between
direct retrieval, inferential reasoning, and the ac-
knowledgment of knowledge gaps.
2.3.2 Data Collection and Model Training
To equip the model with the ability to generate such
reading notes, it’s essential to gather appropriate
training data. Manual annotation for each reading
note is resource-intensive, so we employ a state-
of-the-art language model – GPT-4 – to generate
the notes data. This method is both cost-effective
and enhances reproducibility. We initiate this pro-
cess by randomly sampling 10k questions from
the NQ (Kwiatkowski et al., 2019) training dataset.
GPT-4 is then prompted with specific instructions
and in-context examples to the three distinct types
of note generation (detailed in Appendix A.5). The
quality of GPT-4’s predictions is subsequently as-
sessed through human evaluations on a small subset
of the data before proceeding to the entire set. The
NQ dataset is chosen as our primary dataset due to
its diverse range of real user queries from search
engines. However, to ensure the model’s adaptabil-
ity, we also test its performance on three additional
open-domain datasets, including TriviaQA, WebQ,
and RealTimeQA, showing its generalization capa-
bilities to out-of-domain (OOD) data.
After collecting 10K training data from GPT-4,
the next step involves using them to train a LLaMa-
2 7B model (Touvron et al., 2023), to validate the
feasibility of generating CHAIN -OF-NOTE (CON)
outputs. To do this, we concatenate the instruc-
tion, question and documents as a prompt and train
the model to generate notes and answer in a stan-
dard supervised way. Our in-house model learns to
sequentially generate reading notes for each doc-
ument to assess their relevance to the input query.
Responses are generated based on the document’s
relevance, enhancing accuracy and reducing mis-
information. If all documents are irrelevant, the
model either relies on inherent knowledge for an
answer or responds with “unknown” if the answer
cannot be determined accurately.
2.3.3 Hybrid Training for Better Efficiency
Generating CHAIN -OF-NOTE (CON) would in-
crease inference cost, potentially hindering real-
world usage. To address this, we experimented
with a simple yet effective strategy for internaliz-
ing CON reasoning, called Hybrid Training.
Specifically, we allocate 50% of the training time
to the standard RALM, which involves directly
generating answers without notes, and the other
50% to RALM with CoN. This strategy allows the
model to internalize intermediate reasoning steps
during training. Additionally, we add two different
prompt words before each category of data.
During the inference phase, we exclusively use
the standard RALM prompt to guide the model,
prompting it to output answers without relying on
explicit reading notes. This approach leverages the
hidden states developed during training for implicit
CON reasoning. The model trained with the hybrid
training strategy maintains the same inference time
while achieving only slightly lower performance
wit CoN. The results will be introduced in §3.5.



Source: data\tc17_2312.15883v2\referenced_papers\[93]_2311.09210.pdf (Page 7):

34.3
54.361.464.667.269.2
41.8
56.663.465.970.073.3
36.3
55.863.665.270.173.1
304050607080
100%80%60%40%20%0%
EMScore
NQ
Standard RALMRALM with CoNRALM with CoN (hybrid)
Figure 3: Using a hybrid training strategy demonstrates
slightly lower robustness across various noise ratios but
consistently better performance than standard RALMs.
Models↓ Inference Time(s)
Retrieve-Read 0.6104
+ CHAIN-OF-NOTE 12.0192
+ CHAIN-OF-NOTE (hybrid) 0.6074
Table 5: The inference time comparison shows the aver-
age decoding time per example on 8×A100 GPUs.
questions / total questions. This highlights our
model’s enhanced capability to discern and disre-
gard information that is unfamiliar or not learned
during its initial training phase.
3.5 Evaluation on Hybrid Training Strategy
As illustrated in Figure 3 and Table 5, our proposed
RALM equipped with a hybrid strategy demon-
strates slightly lower robustness across various
noise ratios while but keeping similar effcient de-
coding time consumption to the standard RALM.
This indicates that our CHAIN -OF-NOTE frame-
work, when implemented with a hybrid training
strategy, is highly applicable to a wide range of
real-world business scenarios. This enhancement
in robustness without significant time overhead
highlights the practical value and efficiency of our
approach, making it a viable solution for environ-
ments where QA accuracy can vary but inference
time is crucial.
4 Related Work
Retrieval-Augmented Language Models (RALMs)
represent a significant advancement in natural lan-
guage processing, combining the power of large
language models with the specificity and detail pro-
vided by external knowledge sources (Guu et al.,
2020; Lewis et al., 2020; Izacard et al., 2022). Re-
cent studies highlight the impact of context rele-
vance on language model performance (Creswell
et al., 2022; Shi et al., 2023a; Yoran et al., 2023).
Notably, Creswell et al. (2022) demonstrated that
incorporating random or irrelevant contexts could
adversely affect QA performance. In contrast, Shi
et al. (2023a) discovered that adding irrelevant con-
text to exemplars or task-specific instructions can
sometimes enhance model performance, implying
that models might intrinsically possess capabilities,
developed during pre-training, to manage such sce-
narios. Most pertinent to our research is the study
by Yoran et al. (2023), which focused on training
RALMs to disregard irrelevant contexts. This ap-
proach, while distinct from our proposed solution,
underscores the importance of context relevance in
enhancing the effectiveness of RALMs.
Besides, we present more related Chain-of- Xs
approaches (e.g., Chain-of-Thought (CoT) (Wei
et al., 2022)) in the Appendix A.1.1 and A.2.
5 Conclusion
In this paper, we introduce the CHAIN -OF-NOTE
(CON) framework, a novel methodology designed
to enhance the robustness of RALMs. The central
concept of CON revolves around the generation of
sequential reading notes for each retrieved docu-
ment. This process allows for an in-depth assess-
ment of document relevance to the posed question
and aids in synthesizing this information to craft
the final answer. Our experiments show that GPT-4,
when equipped with CON, outperforms the CHAIN -
OF-THOUGHT approach. Besides, we utilized GPT-
4 to create 10K CON data, subsequently trained on
a LLaMa-2 7B model. Our experiments across four
open-domain QA benchmarks show that RALMs
equipped with CON significantly outperform stan-
dard fine-tuned RALMs.
6 Limitations
One major limitation of the CHAIN -OF-NOTE
(CON) approach is its increased inference cost
due to the sequential generation of notes. While
CON is beneficial for assessing the relevance and
integrating external knowledge, it results in longer
response times, which is problematic for time-
sensitive applications. Moreover, the system’s effi-
ciency depends on the conciseness and relevance
of the generated notes, which can fluctuate based
on the complexity of the retrieved documents.



Source: data\tc17_2312.15883v2\referenced_papers\[93]_2311.09210.pdf (Page 2):

Wikipedia 1: Deadpool 2 is a 2018 American super-hero film based on the Marvel Comics character Deadpooldistributed by 20th Cen-tury Fox. … This movie was followed by releases  in 49 markets over the next few days, including the United States on June1, 2018. 
When was the Deadpool2 being released?
Wikipedia 2: …Deadpool 2 premiered at Lei-cester Square in London on May 10, 2018 Itwas released in United States on May 18, 2018, having been previously scheduled for release on June 1 of that year. …
The first passage suggests that “Deadpool2"            released in the United States on June 1, 2018. However, the second passage clarifies that the movie was actually released earlier, on May 18, having been rescheduled from the initially planned date of June 1. Based on information provided in above passage, the answer is May 18, 2018.
Wikipedia 1: The nostalgic and melancholic song recounts the type of girls with whom the singer had relationships at various years: when he was 17, “small-town girls ... on the village green”; at 21, “city girls who lived up the stairs”
Who wrote the song “When I was 17”?
Wikipedia 2: …“It Was a Very Good Year” , commonly unknown as “When I was 17”, was made famous by Frank Sinatra‘s version in D minor, which won the Grammy Award for Best Male Vocal Performance in 1966.
Wikipedia 1: The Divergent Series is a feature film trilogy.… Distributed by Summit Enter-tainment and Lions-gate Films, the series hasthreescience fiction films set in a dystopian society:Divergent, Insurgent, and Allegiant.
When is the fourth movie of the Divergent series coming out?
Wikipedia 2: The film(Divergent) was a box office bomb, grossing just $179 million against its budget of $142 million, becoming the low-estgrossing “Divergent” film. This led initially to budget cuts on the fourth and final film.…
The first passage discusses the songremin--iscing about relationships at different agesbut does not specify the songwriter. The second clarifies the song often mistakenlycalled “When I Was 17” is actually “It Was a Very Good Year,” Both passages do not explicitly mention the songwriter, but imply the correct title. Based on my know-ledge, the answer is Ervin Drake.
The first passage suggests the "Divergent" series was initially planned as a trilogy, while the secondpassage discusses the poor box office leadsto budget cuts for a potential fourth movie. However, neither passage confirms the release date of the fourth filminDivergent Series. Given this lack of specific information and my current knowledge, the answer is unknown.
(a)CoN:Relevant->Findtheanswer(b)CoN:Irrelevant->Infertheanswer(c)CoN:Irrelevant->AnswerUnknown
Figure 2: Illustration of the CHAIN -OF-NOTE (CON) framework with three distinct types of reading notes. Type
(a) depicts the scenario where the language model identifies a document that directly answers the query, leading
to a final answer formulated from the retrieved information. Type (b) represents situations where the retrieved
document, while not directly answering the query, provides contextual insights, enabling the language model to
integrate this context with its inherent knowledge to deduce an answer. Type (c) illustrates instances where the
language model encounters irrelevant documents and lacks the necessary knowledge to respond, resulting in an
“unknown” answer. This figure exemplifies the CoN framework’s capability to adaptively process information,
balancing direct information retrieval, contextual inference, and the recognition of its knowledge boundaries.
2022; Shi et al., 2023c). The RALM models can
be represented as p(y|x) =P
i p(y|di, x)p(di|x).
Here, x represents the input query, and y signifies
the model’s generated response. In practice, it is
infeasible to compute the sum over all possible doc-
uments due to the vast number of potential sources.
Consequently, the most common approach involves
approximating the sum over d using the k highest
ranked documents, and providing all these docu-
ments as part of the input. We assume, w.l.o.g.,
that these documents are [d1, . . . , dk], yielding
p(y|x) =Pk
i=1 p(y|di, x)p(di|x).
However, the existing RALMs suffer from sev-
eral limitations:
• Risk of Surface-Level Processing: When
directly generating an answer, language models
might rely on surface-level information without
deep comprehension. Thus, they could easily over-
look the nuances of question or documents, partic-
ularly in complex or indirect questions.
• Difficulty in Handling Contradictory Informa-
tion: When faced with documents containing con-
tradictory information, directly generating an an-
swer becomes challenging. The model may strug-
gle of these contradictions or to determine which
piece of information is more credible or relevant.
• Reduced Transparency and Interpretability: Di-
rect answer generation offers limited insight into
how the model arrived at its conclusion. This lack
of transparency makes it challenging for users to
understand the basis of the model’s conclusions.
• Overdependence on Retrieved Documents: Di-
rect generation can lead to an overreliance on the
content of the retrieved documents (i.e. tendency to
extract information from retrieved documents (Shi
et al., 2023a)), ignoring the model’s inherent knowl-
edge base. This can be particularly limiting when
the retrieved documents are noisy or out-of-date.
2.3 The C HAIN -OF-NOTE Framework
The CHAIN -OF-NOTE (CON) framework presents
a solution to the challenges faced by retrieval-
augmented language models (RALMs). This
framework significantly enhances the ability of
RALMs to critically assess retrieved documents
through a structured note-taking process. Specifi-
cally, it involves generating concise and contextu-
ally relevant summaries or notes for each document.
This method allows the model to systematically
evaluate the relevance and accuracy of informa-
tion drawn from external documents. By creating
sequential reading notes, CON not only assesses



Source: data\tc17_2312.15883v2\referenced_papers\[93]_2311.09210.pdf (Page 1):

et al., 2023). Secondly, state-of-the-art LLMs of-
ten hallucinate when addressing fact-oriented ques-
tions, a deficiency that can be risky and may dis-
courage users (Ji et al., 2023; Zhang et al., 2023a).
Ideally, an intelligent system should be capable
of determining whether it has enough knowledge,
both intrinsic and retrieved, to provide an accurate
answer. In cases where knowledge is insufficient,
the system should respond with “unknown” when
the answer cannot be determined. Based on the
shortcomings of the standard RALM system, in
this paper, we aims to improve the robustness of
RALMs, mainly focusing on two pivotal aspects:
(1) Noise Robustness: The ability of a RALM to
discern and disregard noisy information present in
irrelevant retrieved documents, while appropriately
leveraging its intrinsic knowledge.
(2) Unknown Robustness: The capacity of a
RALM to acknowledge its limitations by respond-
ing with “unknown” when given a query it does
not have the corresponding knowledge to answer,
and the relevant information is not found within
the retrieved documents.
In this work, we introduce a novel framework
named CHAIN -OF-NOTE (CON), designed to en-
hance the robustness of RALMs. The cornerstone
of CON is to generate a series of reading notes for
retrieved documents, enabling a comprehensive as-
sessment of their relevance to the input query. This
approach not only evaluates each document’s perti-
nence but also pinpoints the most critical and reli-
able information therein. This process effectively
filters out irrelevant or less credible content, lead-
ing to responses that are more precise and contex-
tually relevant, as exemplified in Figure 1. Besides,
CON enhances the capability of RALM to handle
queries fall outside the scope of training data. In
cases where the retrieved documents do not pro-
vide any relevant information, CON can guide the
model to acknowledge its limitations and respond
with an “unknown” or provide possible explanation
based on available data, enhancing reliability.
To validate the effectiveness of the CON idea,
we first conducted a comparison with CHAIN -OF-
THOUGHT (COT) (Wei et al., 2022) using GPT-
4 as the reader, showing CON is more effective
than COT in retrieval-augmented scenarios. Next,
we prompted GPT-4 (OpenAI, 2023) to generate
a 10K training examples based on questions col-
lected from NQ (Kwiatkowski et al., 2019), and
subsequently trained on the LLaMa-2 7B, to valid
the note-taking ability for smaller-sized models.
Our evaluation of the RALM, integrated with CON
and compared to the standard RALM system, fo-
cused on three major aspects: (1) overall QA perfor-
mance using DPR-retrieved documents, (2) noise
robustness, assessed by introducing noisy infor-
mation to the system, and (3) unknown robust-
ness, evaluated through queries not covered in the
LLaMa-2 pre-training data, i.e., real-time ques-
tions. The evaluations were conducted on the NQ
and three additional out-of-domain open-domain
QA datasets, namely TriviaQA (Joshi et al., 2017),
WebQ (Berant et al., 2013), and RealTimeQA (Ka-
sai et al., 2023). Our experiments show that CON
not only improves overall QA performance when
employed with DPR-retrieved documents but also
significantly enhances robustness in both noise and
unknown aspects. This includes a +7.9 increase in
accuracy (measured by the exact match score) with
noisy retrieved documents, and a +10.5 increase in
the rejection rate for real-time questions1 that are
beyond the pre-training knowledge scope.
2 Proposed Method
2.1 Overview
In this section, we introduce CHAIN -OF-NOTE, an
innovative advancement for retrieval-augmented
language models (RALMs). Specifically, CON
framework generates sequential reading notes for
the retrieved documents, which enables a system-
atic evaluation of the relevance and accuracy of
information retrieved from external documents. By
creating sequential reading notes, the model not
only assesses the pertinence of each document to
the query but also identifies the most critical and
reliable pieces of information within these docu-
ments. This process helps in filtering out irrelevant
or less trustworthy content, leading to more accu-
rate and contextually relevant responses.
2.2 Background of Existing RALMs
RALMs signify a transformative development in
language models, enhancing their output by incor-
porating external knowledge. These models oper-
ate by introducing an auxiliary variable, denoted
as d, which represents retrieved documents. This
inclusion allows them to consider a range of pos-
sible documents, thereby producing responses that
are more informed and precise (Lazaridou et al.,
1We use real-time questions collected from RealTimeQA
after May 2023, which was not trained by LLaMa-2.



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 2):

Published as a conference paper at ICLR 2024
Medical
Rationale 1
Knowledge 
Retrieval
Supporting 
Knowledge
Rationale 
Correction
Corrected Rationale 1
Next Rationale 
Generation
Corrected Rationale 2
…
…
Rationale 2
Knowledge 
Retrieval
Supporting 
Knowledge
Rationale 
Correction
Next Rationale 
Generation
Reasoning Generation & 
Knowledge Domain Selection Answer Consolidation
The answer is 1941What year was the Argentine actor who directed El 
Tio Disparate born? I III
II
Rationale
Query 
Generation
Query
Llama-2-LoRA
ChatGPT
Adaptive Query Generator
Wikidata (SPARQL)
 Table (SQL)Factual
 Wikipedia (n.s.)
ScienceQA Physics (n.s.)Physics
Biology
Supporting 
Knowledge
UpToDate (n.s.)
PysicsClassroom (n.s.)
Flashcard (n.s.)
ScienceQA Biology (n.s.)
 CK-12 (n.s.)
Figure 2: Our proposed chain-of-knowledge (CoK) framework, consisting of (I) Reasoning prepa-
ration, (II) Dynamic knowledge adapting, and (III) Answer consolidation. n.s.: natural sentence.
Given that different knowledge sources require distinct query languages, AQG holds a crucial role
in generating queries. AQG is versatile and can either be a fine-tuned model like Llama-2 (Touvron
et al., 2023) with LoRA (Hu et al., 2021) or an off-the-shelf LLM like ChatGPT. By leveraging both
unstructured and structured knowledge sources, CoK allows for better factual accuracy, improved
reliability, and easier information updates.
To summarize, our key contributions are the following: (1) We introduce chain-of-knowledge (CoK),
a novel framework to enhance the factual correctness of LLMs with heterogeneous knowledge
sources; (2) We propose an adaptive query generator (AQG), specially designed to generate queries
tailored to each knowledge source. AQG is versatile and can seamlessly transition between fine-
tuned models and black-box LLMs; (3) CoK corrects the rationales progressively, ensuring that
inaccuracies from preceding rationales do not propagate into the subsequent steps; (4) We perform
extensive experiments on knowledge-intensive tasks spanning a range of domains, including factual,
medical, physical, and biological. CoK outperforms the CoT baseline by 4.3% on average.
2 T HE CHAIN -OF-KNOWLEDGE FRAMEWORK
As shown in Figure 2, the CoK framework consists of three stages: (1) reasoning preparation, (2)
dynamic knowledge adapting, and (3) answer consolidation. In the first stage, given a knowledge-
intensive question, CoK generates preliminary rationales, i.e., reasoning units/sentences in the rea-
soning chain of CoT, and answers while identifying the relevant knowledge domains. Questions that
do not yield a majority consensus in their answers enter the dynamic knowledge adapting stage, in
which an adaptive query generator (AQG) is employed to generate queries to retrieve knowledge
from the knowledge sources of the identified domain. The rationales are progressively revised and
generated based on the retrieved knowledge. The final answer is then derived based on the corrected
rationales. Refer to Appendix A.1 for the prompts used for each step of our framework.
2.1 R EASONING PREPARATION STAGE
In real-world scenarios, when facing a complex knowledge-intensive question, it is necessary to
generate intermediate rationales before producing the final answer (Wei et al., 2022). Moreover,
before delving into external knowledge sources to address the question, it is crucial to identify the
relevant knowledge domains for effective retrieval. Thus, the reasoning preparation stage consists
of two essential components, namely, reasoning generation and knowledge domain selection.
3



### Claim 14/36

#### Claim Text
Moreover, we also complement our analysis with ROUGE-Recall (ROUGE-R) [86].

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[93]_2311.09210.pdf (Page 4):

Datasets Full size IR Recall Subset size
NQ 3,610 73.82 2,086
TriviaQA 7,993 89.95 7,074
WebQ 2,032 64.22 1,231
Table 1: Dataset statistics. The recall evaluation is based
on DPR retrieval on the full test set.
3 Experiments
3.1 Experimental Settings and Evaluations
3.1.1 Datasets and Splits
We conducted comprehensive experiments using
three benchmark datasets in open-domain question
answering (QA): NQ (Kwiatkowski et al., 2019),
TriviaQA (Joshi et al., 2017), and WebQ (Berant
et al., 2013), with further details provided in Ap-
pendix A.3. Additionally, we employed Real-
TimeQA (Kasai et al., 2023) as a special case to
evaluate “unknown” robustness.
The evaluation was conducted based on two
evaluations sets: full set and subset evaluation.
Firstly, akin to traditional open-domain QA evalu-
ation, we assessed the models using all questions
from the test set to evaluate the overall QA per-
formance. The documents were retrieved using
DPR, and the top-k documents were fed into the
generator. We adhered to the same test splits for
the open-domain QA setting as used by Izacard and
Grave (2021); Karpukhin et al. (2020). For Trivi-
aQA, evaluations from LLaMa-2 (Touvron et al.,
2023) were conducted on the Wikipedia dev set
comprising 7,993 examples. Therefore, we also
follow the same evaluation on this dev set to facili-
tate comparisons with their performance. Secondly,
to assess the model’s noise robustness and un-
known robustness, we extracted subsets from the
above test sets that contained relevant documents
in the retrieved list. We then enumerated each re-
trieved document to determine if it was a golden
document for the given question. Based on the
noise ratio r, for instance, if the top-k documents
are needed for the generator, thenk·r would be the
number of noisy documents, and k · (1 − r) would
be the number of relevant documents. For example,
when noise ratio is 20% and top-5 documents are
needed, then 4 are relevant documents, and 1 is
irrelevant documents. During the enumeration of
the retrieved documents in data pre-processing, we
populated two lists; when one list reached its limit,
we stopped adding more documents to that list until
both lists were complete. In instances where no
relevant documents are retrieved by the DPR for
certain questions, we exclude these from robust-
ness evaluation. Therefore, the subset is smaller
than the original test set, as shown in Table 1.
3.1.2 Baseline Methods
CHAIN -OF-NOTE (CON) is built upon the tra-
ditional retrieve-then-read pipeline (Lewis et al.,
2020). Recent implementations such as Lazaridou
et al. (2022); Shi et al. (2023a); Luo et al. (2023)
integrate large language models to achieve better
performance. Therefore, we primarily compare our
approach against these retrieve-read methods. As
outlined in the §2.3, we denote an input question
as x and its corresponding answer as y. Besides, di
represents the i-th retrieved document, and ydi is
the associated reading note for that document. Here
we show the difference of methods to compare.
QA fine-tune w/o IR are trained to directly gen-
erate an answer from the input question, without
relying on any external retrieved information. Es-
sentially, it learns the function f : x → y, trans-
forming the question x directly to answer y.
Retrieve-Read (Shi et al., 2023c) are trained to
generate an answer not only from the question but
also by incorporating retrieved documents. It learns
the function f : {x, d1, · ··, dk} →y, meaning it
transforms the question x and a set of retrieved
documents {d1, · ··, dk} into an answer y.
Retrieve-Read with CHAIN -OF-NOTE are trained
to generate reading notes for each retrieved doc-
ument before formulating the final answer. It
learns the function f : {x, d1, · ··, dk} →
{yd1 , · ··, ydk , y}, thereby enabling the model
to process the question x and retrieved doc-
uments {d1, · ··, dk} to produce reading notes
{yd1 , · ··, ydk } and the final answer y.
For fair comparability, we trained all LLaMa-2
models on same training set, with the main differ-
ence being in the input and output formats.We also
note that the experiments conducted with GPT-4
were performed in a zero-shot setting. The prompts
used for various experimental conditions are de-
tailed in Appendix A.5.
3.1.3 Evaluation Metrics
For the evaluation of open-domain QA perfor-
mance, we have employed two widely recognized
metrics: Exact Match (EM) and F1 score, as sug-
gested by prior work in the Chen et al. (2017);



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 22):

Published as a conference paper at ICLR 2024
HotpotQA 6-shot, and 127 out of 1,000 instances for FEVER 3-shot. The plain ReAct method, on
the other hand, applies the ReAct prompt to every instance.
23



Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 5):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
5 Advanced Experiments
The studies and experiments outlined in section 4 have shown unsatisfactory performance, marked by a degradation
of at least 20% compared to the peak relative performance. Therefore, in this section, we explore various retrieval
approaches for the RAG, recognizing that the quality of the retriever is a crucial factor in enhancing performance for
this type of problem. We conducted an evaluation covering both sparse and dense search, a hybrid method, and even
a multi-stage architecture using a reranker.
In pursuit of code debugging flexibility and easier customization at each stage, we chose not to utilize an RAG frame-
work (like LangChain or Llama-Index). For a comprehensive guide on debugging RAG and more details about retrieval
systems, refer to [20] and [21].
5.1 Retrievers
When deploying retrieval systems, it is essential to achieve a balance between “effectiveness” ( How good are the
results returned?) and “efficiency” (How much time it takes to return the results? or How much resources are used in
terms of disk/RAM/GPU?). This balance ensures that latency, result quality, and computational budget remain within
our application’s required limits. This work will exclusively focus on effectiveness measures to quantify the retrievers
methods quality.
In our retriever experiments, the evaluation strategy centers around assessing how well the retriever performs in re-
trieving relevant information based on each given query qi. To achieve this, we employ the concept of recall. It is
defined as the fraction of the relevant documents for a given query qi that are successfully retrieved in a ranked list
R [21]. This metric is based on binary relevance judgments, assuming that documents are either relevant or not [21].
In this paper, each chunk is considered a document and only the respective chunkdi is considered relevant to the query
qi. While recall is easy to interpret, it does not consider the specific rank positions in which the relevant chunk appears
in R.
To overcome this limitation, we introduce Reciprocal Rank (RR) into our analysis. In this metric, the rank of the
first relevant document to the query in R is used to compute the RR score [21]. Therefore, Reciprocal Rank offers a
more nuanced evaluation by assigning a higher value when the relevant chunk is returned in the early positions of our
retrievers given the respective query.
Recall and Reciprocal Rank were evaluated at a specific cutoff so the measures are presented as R@k and MRR@k.
For each query, its results are evaluated and their mean serves as an aggregated measure of effectiveness of a given
retriever method. The retrievers are introduced below.
In the category of sparse retrievers, we emphasize the BM25, a technique grounded in statistical weighting to assess
relevance between search terms and documents. BM25 employs a scoring function that takes into account term
frequency and document length, offering an efficient approach for retrieving pertinent information and is typically
used as a strong baseline. However, it is exact-match based and can be powerless when query and document are
relevant to each other but has no common words.
On the other hand, when exploring dense retrievers, we often encounter approaches based on the called bi-encoder
design [22]. The bi-encoder independently encodes queries and documents, creating separate vector representations
before calculating similarity. An advantage of this approach is that it can be initialized ‘offline’: document embeddings
can be precomputed, leaving only the query embedding being calculated at search time, reducing latency.
The hybrid search technique aims to leverage the best of both sparse and dense search approaches. Given a question,
both searches are conducted in parallel, generating two lists of candidate documents to answer it. The challenge then
lies in combining the two results in the best possible way, ensuring that the final hybrid list surpasses the individual
searches. Essentially, we can conceptualize it as a voting system, where each searcher casts a vote on the relevance of
a document to a given query, and in the end, the opinions are combined to produce a better result.
The multi-stage search architecture is based on the retrieve-and-rerank pipeline. In the first stage, a retriever with good
recall is typically used to perform an initial filtering of the documents to be returned. From this narrowed-down list,
these candidate documents are then sent to a second stage, which involves higher computational complexity, to rerank
them and enhance the final effectiveness of the system.
Next, we provide more details about each retriever used.



Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 10):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
5.2 Retrievers Results
The results achieved with the various retrievers are presented in Table 6.
Table 6: Retriever comparison.
Metric ADA-002 Custom ADA-002 Hybrid-BM25-ADA-002 Hybrid-BM25-Custom ADA-002 BM25 BM25 + Reranker
MRR@10 0.565 0.665 0.758 0.850 0.879 0.919
R@3 0.628 0.735 0.829 0.921 0.914 0.971
R@5 0.692 0.835 0.879 0.943 0.971 0.985
R@7 0.750 0.871 0.921 0.964 0.985 0.992
R@9 0.814 0.921 0.957 0.979 0.985 1
The multi-stage pipeline was able to achieve the best results in MRR@10 and Recall@k.
6 Conclusions
The implementation of RAG systems faces challenges such as the effective integration of retrieval models, efficient
representation learning, the diversity of data, the optimization of computational efficiency, evaluation, and text genera-
tion quality. Faced with these constantly evolving obstacles, this article proposes best practices for the implementation,
optimization, and evaluation of RAG on a Brazilian Portuguese dataset, focusing on a simplified pipeline for inference
and experimentation.
So far, we have introduced the main components and methods along with their results and gaps. In this section, we will
discuss key points that contribute to the performance improvement in RAG applications. We will start by discussing
the relationship between the quality of the retriever and the achieved performance, in which our approach showed
a significant improvement of MRR@10 by 35.4% compared to the baseline. Next, we will address the impact of
input size on performance. In this domain, we observed that it is possible to enhance the best information retrieval
strategy by 2.4% through input size optimization. Finally, we will present the complete architecture of RAG with
our recommendations. When evaluating the final accuracy of our approach, we reached 98.61%, representing an
improvement of 40.73 in less degradation score compared to the baseline.
6.1 Retriever Score versus Performance
As mentioned in section 5.1, the performance of information/chunk retrieval, measured by the MRR@10 metric,
varies between (0.565, 0.919), as detailed in Table 6. This variation represents approximately 35.4%. It is important
to highlight that the RAG’s performance is directly influenced by the quality of the retriever. Figure 12 shows the
relationship between the retrieval metric MRR@10 metric and the degradation score for the studied retrieval methods.
Figure 12: Retriever effectiveness vs RAG Performance. x axis is the MRR@10 metric and y axis is the degradation
(where 0 is the perfect scenario) score.
6.2 Input Size versus Performance
We observed that the best performance was achieved with the retrieval of 3 chunks using the retrieve-and-rerank
strategy, as shown in Table 7. The use of a reranker (Figure 9) showed improved information retrieval in our tests.
With this same configuration, the Gemini Pro achieved performance similar to gpt-4, as indicated in Table 8.
Despite achieving perfect recall for 9 chunks, as evidenced in Table 6, using an input with 9000 tokens, 6000 more
than the best scenario ( 3 chunks), did not result in the best performance. As discussed in Section 4.2, the quality of



Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 1):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
presents a substantial challenge for AI practitioners. The task of methodically experimenting with, and critically
evaluating, each variant’s performance, scalability, and applicability becomes increasingly complex.
In this paper, we present a comprehensive series of experiments focused on the application of RAG specifically tailored
for Brazilian Portuguese. Our research delves into evaluating various retrieval techniques, including both sparse and
dense retrievers. Additionally, we explore two chunking strategies (naive and sentence window) to optimize the
integration of retrieved information into the generation process. We also investigate the impact of the positioning of
documents within the prompt, analyzing how this influences the overall quality and relevance of the generated content.
Finally, our experiments extend to comparing the performance of different LLMs, notably GPT-4 and Gemini, in their
ability to effectively incorporate the retrieved information and produce coherent, contextually accurate responses. This
paper aims to provide valuable insights and practical guidelines for implementing RAG in Brazilian Portuguese.
Our main contributions are summarized as follows: 1) we propose a methodology to prepare a dataset in a format that
allows quantifying the quality of the different steps in an RAG system. 2) We proposed a metric (maximum relative
score) that allow us to direct quantify the existent gap between each approach and a perfect RAG system. 3) We discuss
and compare different implementations, showing good practices and optimizations that can be used when developing
a RAG system.
2 Data Preparation
The chosen dataset was the first Harry Potter book in its Brazilian Portuguese version. This choice is motivated
since it is a well known book, and both the Gemini Pro and OpenAI models can answer general questions on the
subject. Additionally, over the application of the standard ChatGPT tokenizer cl100k_base, we observed that there
are approximately 140, 000 tokens in total, allowing the creation of prompts containing the entire book. Following, a
dataset consisting of questions and corresponding answers was developed, with both question and answer generated
by the gpt-4 model and based on a reference chunk.
Figure 1: From a large document (book), chunks were created, and for each chunk, a question and an answer were
generated using gpt-4, where the answer is contained within the chunk.
Figure 1 shows the data preparation process. Initially, the dataset was break into chunks with1000 tokens each, without
overlapping, resulting in 140 chunks. Then, using the prompt described in Appendix A, a {question, answer} pair
was created for each chunk in the style of the SQuAD dataset [6], meaning that the answer to the question is present
within the reference text (chunk).
3 How to Evaluate
The contextual comparison of two text samples is not a straightforward task. For instance, despite the sentence 1 =
"Brazil has won 5 FIFA World Cup titles." , and sentence 2 = "Brazil is the five-time champion of the FIFA World
Cup." (both sentences translated into English for convenience) convey the same meaning, traditional metrics such as
BLEU [7] and ROUGE [8] score may not be able to capture such similarity. Specifically, for the example cited:
• BLEU score [sentence1, sentence2] = 0.33
• ROUGE score [sentence1, sentence2] = 0.22
Therefore, an approach widely used in the literature is to employ gpt-4 to provide a score based on a given prompt, a
concept similar to what was done in the G-Eval work [9]. In this work, a scoring system divided into 5 categories to
compare two texts was devised, with scores defined as following (translated into English for convenience):



### Claim 15/36

#### Claim Text
For open-domain medical Q&A tasks, we utilize ROUGE-R and Bilingual Evaluation Understudy (BLEU1 for answer precision, BLEU-4 for answer fluency) [86] to gauge the similarity of LLMs responses to the ground-truth doctor analysis.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[77]_2308.08833.pdf (Page 15):

D Experiment Details of CMB-Clin
D.1 Screenshot of Human Evaluation UI
We show the screenshot of human evaluation UI in
Figure 6 and Figure 7. We split the screenshot into
two figures for better visual clarity.
D.2 Prompts for Automatic Evaluation
The prompt for automatic evaluation contains task
instructions, metrics, criteria, and placeholders for
information to be evaluated. It is designed based
on the suggestion of experts and used by both Chat-
GPT and GPT-4.
You are an AI evaluator specializing in
assessing the quality of answers
provided by other language models .
Your primary goal is to rate the
answers based on their fluency ,
relevance , completeness , proficiency
in medicine . Use the following
scales to evaluate each criterion :
Fluency :
1: Completely broken and unreadable
sentence pieces
2: Mostly broken with few readable
tokens
3: Moderately fluent but with limited
vocabulary
4: Mostly coherent in expressing complex
subjects
5: Human - level fluency
Relevance :
1: Completely unrelated to the question
2: Some relation to the question , but
mostly off - topic
3: Relevant , but lacking focus or key
details
4: Highly relevant , addressing the main
aspects of the question
5: Directly relevant and precisely
targeted to the question
Completeness :
1: Extremely incomplete
2: Almost incomplete with limited
information
3: Moderate completeness with some
information
4: Mostly complete with most of the
information displayed
5: Fully complete with all information
presented
Proficiency in medicine :
1: Using plain languages with no medical
terminology .
2: Equipped with some medical knowledge
but lacking in - depth details
3: Conveying moderately complex medical
information with clarity
4: Showing solid grasp of medical
terminology but having some minor
mistakes in detail
5: Fully correct in all presented
medical knowledge
You will be provided with the following
information :
- a description
- a conversation based on the
description ( optional )
- a question based on the description
and conversation
- the solution to the question
- a model 's answer to the question
[ description ]
{ description }
[ end of description ]
[ conversation ]
{ history }
[ end of conversation ]
[ question ]
{ question }
[ end of question ]
[ solution ]
{ solution }
[ end of solution ]
[ answer ]
{ answer }
[ end of answer ]
Make sure to provide your evaluation
results in JSON format and ONLY the
JSON , with separate ratings for each
of the mentioned criteria as in the
following example :
{`fluency ': 3, `relevance ': 3, `
completeness ': 3, `proficiency ': 3}
E Additional Results on CMB-Clin
E.1 Results of Expert evaluation
320 model responses are randomly sampled for this
experiment due to a large number of answers to be
evaluated and limited expert resources. We present
the detailed results of expert evaluation in Table 11.
E.2 Robustness Experiments on CMB-Clin
To show the robustness of GPT-4-as-a-judge, we
also present the results of 3 independent runs for
each general and medical LLMs in Table 13 and Ta-
ble 14, respectively. All experiments are conducted
between January 20th and January 26th, 2024.
For Medical LLMs, as shown in Table 14, al-
though the absolute scores deviate from the results
in Table 9, their rankings remain the same. For
General LLMs, as shown in Table 13, the rankings
remain the same except for Yi-34B. The standard
deviations shown in each cell indicate the robust-
ness of evaluating CMB-Clin with GPT-4.



Source: data\tc17_2312.15883v2\referenced_papers\[77]_2308.08833.pdf (Page 8):

GPT-4
ChatGPT
Baichuan-13B-chat
ChatGLM2-6B
HuatuoGPTBianQue-2
ChatMed-Consult
MedicalGPTDoctorGLM
Bentsao
ChatGLM-Med
11
10
9
8
7
6
5
4
3
2
1
Rankings by Perspective and Model
GPT-4
Fluency
Relevance
Completeness
Proficiency
Avg
Expert
Fluency
Relevance
Completeness
Proficiency
Avg
Expert
Fluency
Relevance
Completeness
Proficiency
Avg
Rankings
Figure 3: Rankings by perspective and model. Dashed
lines and solid lines are the resulted rankings from ex-
pert and ChatGPT evaluation, respectively. For visual
clarity, each line is shifted vertically for a small value.
A model is better if it has a smaller ranking (a higher
position) on the vertical axis.
construct multiple-choice questions in the form of
dialogues. To enhance the robustness of our find-
ings, we have included supplementary evaluation
results in Appendix E.2.
On Different Metrics Regarding the Fluency in-
dicator, there is not much difference between Gen-
eral LLMs with most LLM above 4.9, but there are
still many Medical LLMs models below 4.5, indi-
cating a lack of basic dialogue capabilities. Rele-
vance, Completeness and Proficiency are all highly
differentiated indicators, among which Complete-
ness has the lowest average value, indicating that
for medical record consultation scenarios, the com-
pleteness of the dialogue and obtaining complete
information are the most difficult task.
5.3 Agreements between Automatic and
Expert Evaluation
Figure 3 demonstrates a strong agreement of re-
sulted rankings between GPT-4 and expert evalu-
ation, with the spearman correlation of rankings
being 0.93. The rankings agree with each other
except for a flip for GPT-4 and ChatGPT (dashed
and solid brown lines are parallel, except for a flip
at GPT-4 and ChatGPT). We also provide a more
fine-grained alignment analysis in Appendix E.3.
The results indicate that the automatic evaluation
is highly aligned with expert evaluation.
6 Conclusion
Although LLMs have potential in the realm of
medicine, their accurate evaluation remains pivotal
for real-world applications. The introduction of the
CMB benchmark, tailored to the local cultural envi-
ronment in China, gives a more contextualized and
comprehensive evaluation benchmark. Although
not framed as a competitive leaderboard, it serves
as a crucial tool for tracking LLM progress in medi-
cal domains. This might pave the way for a broader
and more effective utilization of LLMs in China’s
medical landscape.
Ethical Statement
In terms of Data Anonymity, all data utilized in this
study primarily originate from publicly accessible
examination questions and coursework exercises
that have been processed by experts, and there is
no personal information.
In terms of Data Quality, we fully ensure data
quality from two aspects: data source and data
cleaning. Through strict screening of data sources,
the accuracy and authority of the exercises are en-
sured; through strict data processing, character er-
rors caused by OCR and low-level errors generated
during the transcription process are minimized.
In terms of Exaggerating the Abilities of the
LLM, we follow the reviewer’s suggestions and
conducted multiple experiments to ensure the sta-
bility of the experimental results. We provide an
explanation based on experimental results that pro-
vides a possible option for solving the evaluation
dilemma of open-ended questions.
Acknowledgement
This work was supported by the Shen-
zhen Science and Technology Program
(JCYJ20220818103001002), Shenzhen Doctoral
Startup Funding (RCBS20221008093330065),
Tianyuan Fund for Mathematics of National
Natural Science Foundation of China (NSFC)
(12326608).
Limitations
The limitations of our study are twofold. Firstly,
while our benchmark encompasses various subjects
in the Chinese medical domain, there remain nu-
merous subjects that necessitate multi-modal capa-
bilities for addressing real-world issues. Secondly,
within the CMB-Clin section, we standardized the
model evaluation method. However, in the real
world, diverse medical conditions may require dis-
tinct evaluation criteria. Therefore, we advocate the
adoption of disease-specific criteria for assessing
model performance.



Source: data\tc17_2312.15883v2\referenced_papers\[77]_2308.08833.pdf (Page 7):

Aspects GPT-4 Yi-34B Qwen-72B ChatGPT Baichuan2-13B ChatGLM3-6B Internlm-20B Deepseekllm-67B Mixtral-8x7B
Fluency 4.95 4.99 4.96 4.97 4.93 4.92 4.9 4.78 2.53
Relevance 4.71 4.69 4.58 4.49 4.41 4.11 3.91 4.04 2.28
Completeness 4.35 4.34 4.12 4.12 4.03 3.74 3.25 2.62 1.54
Proficiency 4.66 4.64 4.55 4.53 4.36 4.23 4.14 4.16 3.04
Average 4.67 4.67 4.55 4.53 4.43 4.25 4.05 3.90 2.35
Table 8: Results of CMB-Clin on Automatic Evaluation using GPT-4 for General Models.
Aspects HuatuoGPT-II-34B BianQue-2 DISC-MedLLM ChatMed-Consult MedicalGPT DISC-MedLLM-Train DoctorGLM Bentsao ChatGLM-Med
Fluency 4.96 4.86 4.82 4.88 4.48 4.57 4.74 3.88 3.55Relevance 4.61 3.52 3.24 3.08 2.64 2.52 2.00 2.05 1.97Completeness 4.31 3.02 2.75 2.67 2.19 1.89 1.65 1.71 1.61Proficiency 4.53 3.60 3.51 3.30 2.89 3.19 2.30 2.58 2.37
Average 4.60 3.75 3.58 3.48 3.05 3.04 2.67 2.55 2.38
Table 9: Results of CMB-Clin on Automatic Evaluation using GPT-4 for Medical Models.
traditional Chinese medicine always have relative
low accuracy, indicating that additional data on
traditional Chinese medicine still needs to be sup-
plemented.
5 Experiments on CMB-Clin
5.1 Experimental Setup
Prompt construction Every prompt comprises
two components: a description that may (or may
not) encompass conversation history Di, and the
question Qi. To integrate the conversation history
into the description, we prepend the appropriate
roles to each question and reference.
Expert and Automatic Evaluation To prove the
validity of our evaluation, we engage three annota-
tors with professional medical knowledge to evalu-
ate on a randomly selected subset of 320 responses
of 11 models from different tiers. Equipped with a
reference solution, they score each response across
four aspects — Fluency, Relevance, Completeness,
and Medical Proficiency — using a grading scale
from 1 to 5. The user interface is shown in Ap-
pendix D.1. To enhance efficiency and reduce ex-
pert evaluation costs, we leverage GPT-4 to assess
the responses of all models, adhering to the same
guidelines as those used in expert evaluations. The
prompt template for the automatic evaluation is
detailed in Appendix D.2.
5.2 Benchmarking Results
On General LLMs As shown in Table 8, except
for Deepseekllm-67B and Mixtral-8x7B, which
have insufficient support for Chinese models, the
other General LLMs have shown strong dialogue
capabilities based on complex medical records.
Taking GPT-4 and ChatGPT as the dividing line, Yi-
34B has achieved the same medical dialogue capa-
bility as GPT-4. Qwen-72B is weaker than GPT-4
but stronger than ChatGPT, and the remaining mod-
els are all weaker than ChatGPT. Compared with
their strong performance in CMB-Exam, domestic
General LLMs still lag behind OpenAI in CMB-
Clin, which is closer to real scenarios. Except for
the Yi LLMs, the ability of other domestic LLMs
to solve real problems does not match their ability
to answer multiple-choice questions, suggesting
that they may have been specially strengthened for
multiple-choice questions. Such model iteration di-
rection actually deviates from actual needs. During
the iteration process, we recommend using both
CMB-Exam and CMB-Clin for model capability
awareness.
On Medical LLMs As shown in Table 9, the
overall dialogue ability of Medical LLMs is lower
than that of General LLMs. Although the three
models of DoctorGLM, Bentsao, and ChatGLM-
Med all claim to be optimized for consultation,
the actual results show that their conversational
capabilities have not been enhanced. It is worth
noting that although BianQue-2 performed poorly
in CMB-Exam, it performed well in CMB-Clin, in-
dicating that it just lacks the ability to do multiple-
choice questions and follow instructions. Al-
though HuatuoGPT-II-34B surpasses GPT-4 in
CMB-Exam, it still lags behind GPT-4 and is even
lower than its base model Yi-34B in CMB-Clin,
indicating that multiple rounds of dialogue data
need to be added during its training process. It is
noted that performance of DISC-MedLLM trained
on CMB-Exam-Train drops significantly on CMB-
Clin, indicating the need to add other data or re-



Source: data\tc17_2312.15883v2\referenced_papers\[47]_2307.03172.pdf (Page 7):

1st 5th 10th
Position of Document with the Answer
50
55
60
65
70Accuracy
10 T otal Retrieved Documents (~2K tokens)
1st 5th 10th 15th 20th
Position of Document with the Answer
50
55
60
65
70Accuracy
20 T otal Retrieved Documents (~4K tokens)
1st 5th 10th 15th 20th 25th 30th
Position of Document with the Answer
50
55
60
65
70Accuracy
30 T otal Retrieved Documents (~6K tokens)
mpt-30b-instruct longchat-13b-16k flan-t5-xxl flan-ul2
Figure 8: When encoder-decoder models (Flan-UL2 and Flan-T5-XXL) evaluated on sequences that are shorter
than their encoder’s training-time maximum sequence length (2048 and 512 tokens, respectively), they are relatively
robust to changes in the position of relevant information within their input context (left subplot). In contrast, when
these models are evaluated on sequences longer than those seen during training (center and right subplots), we
observe a U-shaped performance curve—performance is higher when relevant information occurs at the beginning
or end of the input context, as opposed to the middle of the input context.
1st 5th 10th 15th 20th
Position of Document with the Answer
50
60
70
80Accuracy
20 T otal Retrieved Documents 
(~4K tokens, query-aware contextualization)
claude-1.3
claude-1.3-100k
gpt-3.5-turbo-0613
gpt-3.5-turbo-16k-0613
mpt-30b-instruct
longchat-13b-16k
Figure 9: Query-aware contextualization (placing the
query before and after the documents) does not sub-
stantially improve robustness of language models to
changing the position of relevant information in multi-
document QA; performance slightly increases when
relevant information occurs at the very beginning, but
otherwise slightly decreases.
of the prompt and decoder-only models can only
attend to prior tokens at each timestep. In contrast,
encoder-decoder models (which seem more robust
to changes in the position of relevant information;
§4.1) use a bidirectional encoder to contextualize
input contexts—can we use this observation to im-
prove decoder-only models by placing the query be-
fore and after the data, enabling query-aware con-
textualization of documents (or key-value pairs)?
We find that query-aware contextualization dra-
matically improves performance on the key-value
retrieval task—all models achieve near-perfect per-
formance on the 75, 140, and 300 key-value pair
settings. For example, GPT-3.5-Turbo (16K) with
query-aware contextualization achieves perfect per-
formance when evaluated with 300 key-value pairs.
In contrast, without query-aware contextualiza-
tion, the worst-case performance is 45.6% (Fig-
ure 7). Despite the significant impact on key-
value retrieval performance, query-aware contextu-
alization minimally affects performance trends in
the multi-document question answering task (Fig-
ure 9); it slightly improves performance when the
relevant information is located at the very begin-
ning of the input context, but slightly decreases
performance in other settings.
4.3 Effect of Instruction Fine-Tuning
The models we evaluated are all instruction fine-
tuned—after their initial pre-training, they undergo
supervised fine-tuning on a dataset of instructions
and responses. The task specification and/or in-
struction is commonly placed at the beginning of
the input context in supervised instruction fine-
tuning data, which might lead instruction fine-
tuned language models to place more weight on
the start of the input context. To better understand
the potential effects of instruction fine-tuning on
how language models use long input contexts, we
compare the multi-document question answering
performance of MPT-30B-Instruct against its base
model (i.e., before instruction fine-tuning) MPT-
30B. We use the same experimental setup as §2.
Figure 10 compares the multi-document QA
performance of MPT-30B and MPT-30B-Instruct
as a function of the position of the relevant in-



Source: data\tc17_2312.15883v2\referenced_papers\[47]_2307.03172.pdf (Page 14):

1st 5th 10th 15th 20th
Position of Document with the Answer
65
70
75
80Accuracy
20 T otal Retrieved Documents
(~4K tokens, random distractors)
claude-1.3
claude-1.3-100k
gpt-3.5-turbo-0613
gpt-3.5-turbo-16k-0613
mpt-30b-instruct
longchat-13b-16k
Figure 13: Language model performance on multi-
document QA when using random distractors, rather
than retrieved distractors.
Figure 14 presents the results of this experiment.
We continue to see a U-shaped performance curve,
with performance degrading when language mod-
els must use information in the middle of their
input contexts. Comparing the results in §2.3 with
those when randomizing the distractor order and
mentioning such in the prompt, we see that ran-
domization slightly decreases performance when
the relevant information is at the very beginning
of the context, and slightly increases performance
when using information in the middle and end of
the context.
D GPT-4 Performance
We evaluate GPT-4 (8K) on a subset of 500 ran-
dom multi-document QA examples with 20 total
documents in each input context (Figure 15). GPT-
4 achieves higher absolute performance than any
other language model, but still shows a U-shaped
performance curve—its performance is highest
when relevant information occurs at the very start
or end of the context, and performance degrades
when it must use information in the middle of its
input context.
E Llama-2 Performance
We evaluate Llama-2 (Touvron et al., 2023b) on
multi-document QA with 20 total documents in
each input context. The Llama tokenizer pro-
duces longer sequences than the tokenizers for our
previously-studied models, so we discard 20 exam-
1st 5th 10th 15th 20th
Position of Document with the Answer
55
60
65
70
75Accuracy
20 T otal Retrieved Documents
(~4K tokens, randomly ordered)
claude-1.3
claude-1.3-100k
gpt-3.5-turbo-0613
gpt-3.5-turbo-16k-0613
mpt-30b-instruct
longchat-13b-16k
Figure 14: Language model performance when random-
izing the order of the distractors (rather than presenting
them in order of decreasing relevance) and mentioning
as such in the prompt.
1st 5th 10th 15th 20th
Position of Document with the Answer
50
60
70
80
90Accuracy
20 T otal Retrieved Documents 
(~4K tokens, 500 question sample)
claude-1.3
claude-1.3-100k
gpt-3.5-turbo-0613
gpt-3.5-turbo-16k-0613
mpt-30b-instruct
longchat-13b-16k
gpt-4-0613
Figure 15: Although GPT-4 has higher absolute perfor-
mance than other models, its performance still degrades
when relevant information occurs in the middle of the
input context.



### Claim 16/36

#### Claim Text
Similar to previous work [65], because of the randomness of LLMs’ outputs, we repeat experiments with different random seeds five times and report the average and standard deviation results.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 9):

10  
using KG-RAG framework (T-test, p-value < 0.0001, t-statistic = -47.7, N = 1000) but not in the prompt-based approach.  
 Fig 3. LLM performance on True/False and MCQ datasets Performance (Accuracy) distributions of LLMs on (A) True/False and (B) MCQ datasets. Blue distribution denotes the performance using prompt-based approach and red distribution denotes the performance using KG-RAG based approach. Black vertical dashed line indicates the mean value of the distribution. The higher the value, the better the performance.



Source: data\tc17_2312.15883v2\referenced_papers\[47]_2307.03172.pdf (Page 15):

ples (out of 2655) that exceed Llama-2’s maximum
context length of 4096 tokens. We experiment with
models of varying sizes (7B, 13B, and 70B pa-
rameters), with and without additional supervised
fine-tuning and reinforcement learning from hu-
man feedback (“-chat-” models). The results are
presented in Figure 16.
Comparing Llama-2 models of varying sizes, we
find that only the larger models (13B and 70B)
exhibit the U-shaped performance curve (i.e., both
primacy and recency bias)—the smallest Llama-
2 models (7B) are solely recency-biased. Given
these results, we hypothesize that prior work (e.g.,
Khandelwal et al., 2018; Sun et al., 2021) did not
previously observe any primacy bias in language
models because the models they studied were too
small (less than 1B parameters).
Comparing between Llama-2 models with and
without additional supervised fine-tuning and re-
inforcement learning from human feedback, we
see that additional fine-tuning dramatically im-
proves performance on the multi-document QA
task. The 7B models with and without additional
fine-tuning show minimal primacy bias, and are
largely recency-biased. The 13B base model has
a dramatic primacy and recency bias—there is a
20-point accuracy disparity between the best- and
worst-case performance. Applying additional fine-
tuning to the 13B seems to slightly reduce this
bias (10-point worst-case degradation), but the bias
remains significant. However, the 70B models
with and without additional fine-tuning have largely
similar trends (showing both primacy and recency
bias), and additional fine-tuning minimally changes
the positional bias severity.
1st 5th 10th 15th 20th
Position of Document with the Answer
20
30
40
50
60
70Accuracy
20 T otal Retrieved Documents (~4K tokens)
Llama-2-7b-chat-hf
Llama-2-13b-chat-hf
Llama-2-70b-chat-hf
Llama-2-7b-hf
Llama-2-13b-hf
Llama-2-70b-hf
Figure 16: Multi-document QA performance (20 total
documents) of Llama-2 models of varying sizes (7B,
13B, 70B parameters), with and without additional su-
pervised fine-tuning and reinforcement learning from
human feedback (“-chat-” models).



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 10):

11  
Table 1. LLM performance (Accuracy: mean ± std) on True/False and MCQ dataset Model True/False dataset MCQ dataset Prompt-based KG-RAG Prompt-based KG-RAG 
Llama-2-13b 0.89±0.02 0.94±0.01 0.31±0.03 0.53±0.03 GPT-3.5-Turbo 0.87±0.02 0.95±0.01 0.63±0.03 0.79±0.02 
GPT-4 0.9±0.02 0.95±0.01 0.68±0.03 0.74±0.03



Source: data\tc17_2312.15883v2\referenced_papers\[93]_2311.09210.pdf (Page 6):

Models Noise NQ TriviaQA WebQ Average
Ratio EM F1 EM F1 EM F1 EM F1
Retrieve-Read 100% 34.28 41.74 55.30 61.67 29.58 46.34 39.72 49.92
+ CHAIN-OF-NOTE 41.83 49.58 64.30 70.00 36.85 53.07 47.66 57.55
(+7.55) (+7.84) (+9.00) (+8.33) (+7.27) (+6.73) (+7.94) (+7.63)
Retrieve-Read 80% 54.28 61.03 73.83 80.02 35.46 52.70 54.52 64.58
+ CHAIN-OF-NOTE 56.63 63.23 75.89 81.24 40.60 56.54 57.70 67.00
(+2.35) (+2.20) (+2.06) (+1.22) (+5.14) (+3.84) (+3.18) (+2.42)
Retrieve-Read 60% 61.44 67.94 78.44 83.65 37.01 54.16 58.96 68.58
+ CHAIN-OF-NOTE 63.43 69.33 78.79 84.07 41.26 56.91 61.16 70.10
(+1.99) (+1.39) (+0.35) (+0.42) (+4.25) (+2.75) (+2.20) (+1.52)
Retrieve-Read 40% 64.62 71.12 80.56 86.76 38.40 55.60 61.19 71.16
+ CHAIN-OF-NOTE 65.91 72.22 81.72 87.11 42.16 58.15 63.26 72.49
(+1.29) (+1.10) (+1.16) (+0.35) (+3.76) (+2.55) (+2.07) (+1.33)
Retrieve-Read 20% 67.21 73.69 81.73 87.89 39.95 56.66 62.96 72.75
+ CHAIN-OF-NOTE 70.00 76.08 82.86 88.24 44.36 60.13 65.74 74.82
(+2.79) (+2.39) (+1.13) (+0.35) (+4.41) (+3.47) (+2.78) (+2.07)
Retrieve-Read 0% 69.23 75.57 83.34 89.44 42.24 58.59 64.93 74.53
+ CHAIN-OF-NOTE 73.28 79.86 83.52 88.94 46.16 62.38 67.65 77.06
(+4.05) (+4.29) (+0.18) (-0.50) (+3.92) (+3.79) (+2.72) (+2.53)
Table 3: Evaluation on Noise Robustness. The backbone language model is LLaMa-2 7B. The CHAIN -OF-NOTE
framework shows superior performance compared to the standard RALM system, particularly notable at higher
noise ratios.We explain how we synthesize data with different noise ratios under real-world scenarios in§ 3.1.1.
Models↓ RealTimeQA
EM F1 RR
Retrieve-Read (Shi et al., 2023c)15.6 19.9 6.1
+ CHAIN-OF-NOTE (ours) 15.7 20.3 13.0
Table 4: Evaluation on Unknown Robustness. TheCON
shows better performance than standard RALM system.
3.3 Evaluation on Noise Robustness
As illustrated in Table 2, when faced with entirely
noisy documents, both the standard RALM and
our CHAIN -OF-NOTE enhanced RALM underper-
formed compared to the no-retrieval setting. This
suggests that RALMs can be misled by noisy infor-
mation, leading to more hallucinations.
Notably, equipping the model withCON enables
it to perform nearly as well as the baseline model
directly fine-tuned with QA pairs without retrieval,
showcasing its robustness to noise and its ability
to disregard irrelevant information. The CON ap-
proach is effective not only in fine-tuned, smaller-
sized models but also in large language models,
such as GPT-4, with adjustments made only to the
prompt. Besides, in comparison to the CHAIN -OF-
THOUGHT technique, commonly utilized in rea-
soning scenarios, CON presents a more efficient
strategy for retrieval-augmented settings, particu-
larly in addressing knowledge-intensive tasks.
Table 3 shows that RALM enhanced with CON
consistently outperforms the standard RALM, es-
pecially in scenarios with exclusively noisy doc-
uments. An average improvement of +7.9 in EM
score on fully noisy documents is observed on three
open-domain QA datasets, in average. Experiments
with lower noise ratios also consistently demon-
strate the improvements brought by CON, aligning
the overall QA performance.
3.4 Evaluation on Unknown Robustness
Table 4 illustrates that our RALM equipped with
CON exhibits superior robustness in handling un-
known scenario, particularly evident in the Real-
TimeQA benchmark. This benchmark falls com-
pletely outside the model’s domain and contains
real-time information that was not part of the
LLaMa-2 pre-training data. Despite this, models
are still capable of providing correct answers in
some cases, as the answers remain consistent over
time. In comparison to the standard RALM sys-
tem, our method shows a significant improvement,
exceeding +10.5 in its ability to reject to answer
questions in unknown scenario. The evaluation is
based on reject rate (RR), i.e., number of rejected



Source: data\tc17_2312.15883v2\referenced_papers\[93]_2311.09210.pdf (Page 12):

bfloat16 precision. The learning rates are set to
{1e-6, 2e-6, 5e-6, 1e-5, 2e-5}, and the empirical
results indicated that 5e-6 yielded the best model
performance, hence we standardized the learning
rate for all reported numbers. Greedy decoding
is applied during inference on all experiments to
ensure deterministic generations.
A.5 Instruction Prompts
(1) For standard RALM, the instruction is:
Task Description: The primary objective is to
briefly answer the question.
(2) For RALM with COT, the instruction is:
Task Description: Think step by step to answer the
question.
(3) For RALM with CON, the instruction is:
Task Description:
1. Read the given question and five Wikipedia
passages to gather relevant information.
2. Write reading notes summarizing the key points
from these passages.
3. Discuss the relevance of the given question and
Wikipedia passages.
4. If some passages are relevant to the given
question, provide a brief answer based on the
passages.
5. If no passage is relevant, direcly provide answer
without considering the passages.
A.6 Case Studies
In our case studies, as illustrated in Table 6, we
compare the responses generated by the standard
RALM and our enhanced RALM with COT. These
examples highlight the differences in how each
model processes and interprets information from
retrieved documents.
The first case shows a question pertains to the
most recent Summer Olympics held in the USA.
The standard RALM is misled by the mention of
"Chicago’s bid for the 2016 Summer Olympics."
Lacking a deep comprehension of the content, it
incorrectly focuses on the more recent year (2016),
resulting in an inaccurate answer. In contrast, the
RALM with CON carefully analyzes the informa-
tion. It notes that while Chicago bid for the 2016
Olympics, there’s no confirmation of it being a suc-
cessful bid. This leads to the correct conclusion
that the most recent Olympics in the USA were
held in 1996. The second case study involves iden-
tifying the language of the first Jnanpith Award
recipient. Here, the standard RALM fails to syn-
thesize information across documents. It identifies
G. Sankara Kurup as the award recipient but does
not connect this information to the language of his
work. Conversely, the RALM with CON effec-
tively combines details from both documents. It
recognizes that while the first document mentions
Kurup’s award, the second document provides the
missing language detail, leading to the correct an-
swer of Malayalam.
A.7 Licenses
The four open-domain QA benchmarks, LLaMa-2
models are all released under MIT License. They
are all for research purposes, and our experiments
are consistent with their intended usage.



### Claim 17/36

#### Claim Text
We argue that this was primarily due to the inundation of tokens, causing LLMs to lose in the middle [47], thereby impeding their ability to discern genuinely relevant knowledge from the retrieved chains and even ignore LLMs’ tasks.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 12):

13  
Previous studies have utilized KG in conjunction with LLM for knowledge intensive tasks such as question-answering,(Yasunaga et al. 2021) multi-hop relational reasoning,(Feng et al. 2020) commonsense reasoning,(Lin et al. 2019; Lv et al. 2019) and model pre-training.(Moiseev et al. 2022; Yasunaga et al. 2022) Furthermore, enhancing prompts by incorporating structured knowledge has been described and studied.(Xiang Chen Zhejiang University, China et al.; Lewis et al. 2020; Pan et al. 2023) Naturally, these approaches have bolstered the positive reinforcement between KG and LLM. Nevertheless, it's worth noting that these approaches are only be task-specific and in some cases, the knowledge infusion could grow exponentially by the inclusion of higher order relations.(Bauer, Wang and Bansal 2018; Lin et al. 2019) Such approaches could compromise the limited token space of the LLM. Alternative methods employed knowledge infusion through the direct use of query languages like SPARQL.(Brate et al. 2022) However, this could render the system constraint to the schema of the underlying KG, potentially affecting the flexibility and adaptability of prompts. Moreover, as the KG expands and its schema grows, it could potentially occupy a significant portion of the LLM input token space. This explains why we noticed a greater token usage with the Cypher-RAG method (average usage of 8006 tokens), as it incorporates the entire graph schema into the input prompt for converting natural language into structured Cypher queries. In contrast, KG-RAG requires minimal graph schema, thus eliminating the need to include it in the prompt and resulting in substantial token savings, with a reduction of over 50% in token utilization compared to Cypher-RAG. This finding suggests that when dealing with a graph as large as SPOKE, which contains over 40 million nodes, Cypher-RAG requires LLMs that enable a larger context window. In contrast, KG-RAG is capable of managing this with LLMs that require a relatively smaller window size.



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 22):

23  
performance of LLMs on True/False and MCQ datasets, 150 questions were randomly sampled with replacement 1000 times (using bootstrapping). The accuracy metric was then calculated for each sampling iteration, resulting in a performance distribution. Cypher-RAG  Cypher-RAG is a technique utilized for retrieving context associated with a node in a Neo4j graph database.(Bratanic 2023) This context can then be leveraged to generate information about the node in natural language using a LLM. The method involves explicitly embedding the schema of the graph into the input prompt, directing the LLM to generate a structured Cypher query based on this schema. The resulting Cypher query is used to make a call to the Neo4j database, and the returned information is utilized as context to respond to the user's prompt. This methodology is integrated into the LangChain python library as GraphCypherQAChain class.(Bratanic 2023) An advantage of this approach is that it allows for the creation of Cypher queries directly from natural language, eliminating the need for users to have the knowledge of Cypher query syntax. However, our analysis revealed certain limitations of this approach. We found that the explicit embedding of the graph schema restricts the input token space and increases token usage for this method. As the complexity of the graph schema increases, users may need to utilize LLMs with longer context window sizes for optimal performance. Additionally, we demonstrated that this method can be sensitive to how the prompt is formulated. Even slight perturbations to the prompt can lead to incorrect Cypher queries and subsequently impact downstream generative processes.



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 2):

3  
Introduction Large language models (LLM) have shown impressive performance in solving complex tasks across various domains that involve language modeling and processing.(Zhao et al. 2023) LLMs are pre-trained on a large corpora of text data in a self-supervised learning framework which can be either Masked Language Modeling (e.g. BERT like models)(Devlin et al. 2018; Liu et al. 2019) or Auto-Regressive framework (GPT like models)(Brown et al. 2020; Luo et al. 2022). This pre-training encodes knowledge about the language in the model parameters. Similar to the transfer learning approach commonly used in deep neural networks, this implicit knowledge can be refined through supervised training to excel in a range of domain-specific tasks.(Wei et al. 2021; Luo et al. 2022) Nevertheless, the “implicit representation” of knowledge in LLM has also been shown to generate non-factual information despite linguistically coherent answers (i.e. “hallucination”) as a response to the input prompt.(Maynez et al. 2020; Raunak, Menezes and Junczys-Dowmunt 2021; Ji et al. 2022) This issue poses a significant challenge for the adoption of LLM models in domains with stringent requirements for accuracy, such as biomedicine.  Various strategies have been introduced to address hallucinations in LLMs. One such solution involves the utilization of domain-specific data for pre-training the LLM, rather than relying on generic text corpora. This approach has led to the creation of models such as PubMedBERT(Yu Gu Microsoft Research, Redmond, WA et al. 2021), BioBERT(Lee et al. 2019), BlueBERT(Lee et al. 2019; Peng, Yan and Lu 2019), SciBERT(Beltagy, Lo and Cohan 2019), ClinicalBERT(Huang, Altosaar and Ranganath 2019), BioGPT(Luo et al. 2022), Med-PaLM(Singhal et al. 2023), and BioMedGPT.(Luo et al. 2023) However, pre-training an LLM from scratch imposes a significant computational cost and time overhead to attain the desired human-like performance. An alternative approach, known as prompt tuning, was recently proposed as a means to enhance LLM performance, for instance through the use of zero-



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 9):

10  
using KG-RAG framework (T-test, p-value < 0.0001, t-statistic = -47.7, N = 1000) but not in the prompt-based approach.  
 Fig 3. LLM performance on True/False and MCQ datasets Performance (Accuracy) distributions of LLMs on (A) True/False and (B) MCQ datasets. Blue distribution denotes the performance using prompt-based approach and red distribution denotes the performance using KG-RAG based approach. Black vertical dashed line indicates the mean value of the distribution. The higher the value, the better the performance.



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 0):

Published as a conference paper at ICLR 2024
CHAIN -OF-KNOWLEDGE : G ROUNDING LARGE LAN-
GUAGE MODELS VIA DYNAMIC KNOWLEDGE ADAPT-
ING OVER HETEROGENEOUS SOURCES
Xingxuan Li1,2∗† , Ruochen Zhao2∗‡ , Yew Ken Chia1,3∗† , Bosheng Ding1,2†, Shafiq Joty2,4
Soujanya Poria3, Lidong Bing1,5
1DAMO Academy, Alibaba Group, Singapore, 2Nanyang Technological University,
3Singapore University of Technology and Design, 4Salesforce Research,
5Hupan Lab, 310023, Hangzhou, China
{xingxuan.li, yewken.chia, bosheng.ding, l.bing}@alibaba-inc.com
{ruochen002, srjoty}@ntu.edu.sg sporia@sutd.edu.sg
ABSTRACT
We present chain-of-knowledge (CoK) , a novel framework that augments large
language models (LLMs) by dynamically incorporating grounding information
from heterogeneous sources. It results in more factual rationales and reduced
hallucination in generation. Specifically, CoK consists of three stages: reason-
ing preparation, dynamic knowledge adapting, and answer consolidation. Given
a knowledge-intensive question, CoK first prepares several preliminary rationales
and answers while identifying the relevant knowledge domains. If there is no ma-
jority consensus among the answers from samples, CoK corrects the rationales
step by step by adapting knowledge from the identified domains. These corrected
rationales can plausibly serve as a better foundation for the final answer consolida-
tion. Unlike prior studies that primarily use unstructured data, CoK also leverages
structured knowledge sources such as Wikidata and tables that provide more re-
liable factual information. To access both unstructured and structured knowledge
sources in the dynamic knowledge adapting stage, we propose an adaptive query
generator that allows the generation of queries for various types of query lan-
guages, including SPARQL, SQL, and natural sentences. Moreover, to minimize
error propagation between rationales, CoK corrects the rationales progressively
using preceding corrected rationales to generate and correct subsequent rationales.
Extensive experiments show that CoK consistently improves the performance of
LLMs on knowledge-intensive tasks across different domains. Our code is avail-
able at https://github.com/DAMO-NLP-SG/chain-of-knowledge.
1 I NTRODUCTION
In recent years, large language models (LLMs) such as ChatGPT (OpenAI, 2023) have demonstrated
impressive language generation capabilities (Cheng et al., 2023; Ding et al., 2023; Chen et al., 2024).
However, one major challenge of LLMs lies in hallucination, which is their tendency to confidently
generate plausible but factually incorrect texts (Ji et al., 2023; Zhao et al., 2023b). As shown in
Figure 1, given a question, “What year was the Argentine actor who directed El Tio Disparate born?”
which requires factual knowledge to answer, the most advanced LLMs often provide an incorrect
answer. While LLMs have the remarkable capability to recall information from their training data,
effectively updating or controlling the factual knowledge within these models remains challenging
(Luo et al., 2023).
∗ Equal contribution.
† Xingxuan Li, Yew Ken Chia, and Bosheng Ding are under the Joint Ph.D. Program between DAMO
Academy and their corresponding universities.
‡ Ruochen Zhao is under the AISG Ph.D. Fellowship Programme.
1
arXiv:2305.13269v4  [cs.CL]  21 Feb 2024



### Claim 18/36

#### Claim Text
In summary, because RAG involves a process of continuous trial and error [8], we experimented with many strategies and ultimately arrived at HyKGE. 6 CONCLUSION In this paper, we proposed HyKGE, a hypothesis knowledge graph enhanced framework for LLMs to improve accuracy and reliability.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[8]_2401.05856.pdf (Page 3):

CAIN 2024, April 2024, Lisbon, Portugal Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek
Case Study Domain Doc Types Dataset Size RAG Stages Sample Questions
Cognitive
Reviewer*
Research PDFs (Any size) Chunker, Rewriter, Re-
triever, Reader
What are the key points covered in
this paper?
AI Tutor* Education Videos, HTML,
PDF
38 Chunker, Rewriter,
Retriever, Reader
What were the topics covered in
week 6?
BioASQ Biomedical Scientific PDFs 4017 Chunker, Retriever,
Reader
Define pseudotumor cerebri. How
is it treated?
Table 1: A summary of the RAG case studies presented in this paper. Case studies marked with a * are running systems currently
in use.
OpenEvals technique implemented by OpenAI6. From the gener-
ated questions we manually inspected 40 issues and all issues that
the OpenEvals flagged as inaccurate. We found that the automated
evaluation was more pessimistic than a human rater for this domain.
However, one threat to validity with this finding is that BioASQ is
a domain specific dataset and the reviewers were not experts i.e.
the large language model may know more than a non-expert.
5 FAILURE POINTS OF RAG SYSTEMS
From the case studies we identified a set of failure points presented
below. The following section addresses the research question What
are the failure points that occur when engineering a RAG system?
FP1 Missing Content The first fail case is when asking a ques-
tion that cannot be answered from the available documents.
In the happy case the RAG system will respond with some-
thing like “Sorry, I don’t know". However, for questions that
are related to the content but don’t have answers the system
could be fooled into giving a response.
FP2 Missed the Top Ranked Documents The answer to the
question is in the document but did not rank highly enough
to be returned to the user. In theory, all documents are ranked
and used in the next steps. However, in practice the top K
documents are returned where K is a value selected based
on performance.
FP3 Not in Context - Consolidation strategy Limitations
Documents with the answer were retrieved from the data-
base but did not make it into the context for generating an
answer. This occurs when many documents are returned
from the database and a consolidation process takes place to
retrieve the answer.
FP4 Not Extracted Here the answer is present in the context,
but the large language model failed to extract out the correct
answer. Typically, this occurs when there is too much noise
or contradicting information in the context.
FP5 Wrong Format The question involved extracting informa-
tion in a certain format such as a table or list and the large
language model ignored the instruction.
FP6 Incorrect Specificity The answer is returned in the re-
sponse but is not specific enough or is too specific to address
the user’s need. This occurs when the RAG system designers
have a desired outcome for a given question such as teach-
ers for students. In this case, specific educational content
should be provided with answers not just the answer. Incor-
rect specificity also occurs when users are not sure how to
ask a question and are too general.
6https://github.com/openai/evals
FP7 Incomplete Incomplete answers are not incorrect but miss
some of the information even though that information was in
the context and available for extraction. An example question
such as “What are the key points covered in documents
A, B and C?” A better approach is to ask these questions
separately.
6 LESSONS AND FUTURE RESEARCH
DIRECTIONS
The lessons learned from the three case studies are shown in Table 2.
We present our findings for the research question: What are the
key considerations when engineering a RAG system? Based on our
takeaways we identified multiple potential research areas linked to
RAG as follows:
6.1 Chunking and Embeddings
Chunking documents sounds trivial. However, the quality of chunk-
ing affects the retrieval process in many ways and in particular
on the embeddings of the chunk then affects the similarity and
matching of chunks to user queries. There are two ways of chunk-
ing: heuristics based (using punctuation, end of paragraph, etc.),
and semantic chunking (using the semantics in the text to inform
start-end of a chunk). Further research should explore the tradeoffs
between these methods and their effects on critical downstream
processes like embedding and similarity matching. A systematic
evaluation framework comparing chunking techniques on metrics
like query relevance and retrieval accuracy would benefit the field.
Embeddings represent another active research area, including
generating embeddings for multimedia and multimodal chunks
such as tables, figures, formulas, etc. Chunk embeddings are typ-
ically created once during system development or when a new
document is indexed. Query preprocessing significantly impacts
a RAG system’s performance, particularly handling negative or
ambiguous queries. Further research is needed on architectural pat-
terns and approaches [5] to address the inherent limitations with
embeddings (quality of a match is domain specific).
6.2 RAG vs Finetuning
LLMs are great world models due to the amount of training data, and
finetuning tasks applied on the model before it’s released. However,
these models are general-purpose models (may not know the very
specifics of your domain) and also not up to date (there is a cutoff
date on their knowledge). Fine-tuning and RAG offer two potential
customisation pathways, each with distinct tradeoffs. Finetuning
requires curating internal datasets to adapt and train the LLM on.
However, all your data are baked into the model and you need to



Source: data\tc17_2312.15883v2\referenced_papers\[8]_2401.05856.pdf (Page 0):

Seven Failure Points When Engineering a Retrieval Augmented
Generation System
Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek
{scott.barnett,stefanus.kurniawan,srikanth.thudumu,zach.brannelly,mohamed.abdelrazek}@deakin.edu.au
Applied Artificial Intelligence Institute
Geelong, Australia
ABSTRACT
Software engineers are increasingly adding semantic search capabil-
ities to applications using a strategy known as Retrieval Augmented
Generation (RAG). A RAG system involves finding documents that
semantically match a query and then passing the documents to a
large language model (LLM) such as ChatGPT to extract the right
answer using an LLM. RAG systems aim to: a) reduce the problem
of hallucinated responses from LLMs, b) link sources/references
to generated responses, and c) remove the need for annotating
documents with meta-data. However, RAG systems suffer from lim-
itations inherent to information retrieval systems and from reliance
on LLMs. In this paper, we present an experience report on the
failure points of RAG systems from three case studies from separate
domains: research, education, and biomedical. We share the lessons
learned and present 7 failure points to consider when designing a
RAG system. The two key takeaways arising from our work are: 1)
validation of a RAG system is only feasible during operation, and
2) the robustness of a RAG system evolves rather than designed in
at the start. We conclude with a list of potential research directions
on RAG systems for the software engineering community.
CCS CONCEPTS
• Software and its engineering →Empirical software valida-
tion.
KEYWORDS
Retrieval Augmented Generation, RAG, SE4AI, Case Study
ACM Reference Format:
Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mo-
hamed Abdelrazek . 2024. Seven Failure Points When Engineering a Retrieval
Augmented Generation System. In Proceedings of 3rd International Confer-
ence on AI Engineering — Software Engineering for AI (CAIN 2024). ACM,
New York, NY, USA, 6 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
The new advancements of Large Language Models (LLMs), includ-
ing ChatGPT, have given software engineers new capabilities to
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CAIN 2024, April 2024, Lisbon, Portugal
© 2024 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn
build new HCI solutions, complete complex tasks, summarise docu-
ments, answer questions in a given artefact(s), and generate new
content. However, LLMs suffer from limitations when it comes
to up-to-date knowledge or domain-specific knowledge currently
captured in company’s repositories. Two options to address this
problem are: a) Finetuning LLMs (continue training an LLM using
domain specific artifacts) which requires managing or serving a
fine-tuned LLM; or b) use Retrieval-Augmented Generation (RAG)
Systems that rely on LLMs for generation of answers using existing
(extensible) knowledge artifacts. Both options have pros and cons
related to privacy/security of data, scalability, cost, skills required,
etc. In this paper, we focus on the RAG option.
Retrieval-Augmented Generation (RAG) systems offer a com-
pelling solution to this challenge. By integrating retrieval mecha-
nisms with the generative capabilities of LLMs, RAG systems can
synthesise contextually relevant, accurate, and up-to-date informa-
tion. A Retrieval-Augmented Generation (RAG) system combines
information retrieval capabilities, and generative prowess of LLMs.
The retrieval component focuses on retrieving relevant information
for a user query from a data store. The generation component fo-
cuses on using the retrieved information as a context to generate an
answer for the user query. RAG systems are an important use case
as all unstructured information can now be indexed and available
to query reducing development time no knowledge graph creation
and limited data curation and cleaning.
Software engineers building RAG systems are expected to pre-
process domain knowledge captured as artifacts in different formats,
store processed information in appropriate data store (vector data-
base), implement or integrate the right query-artifact matching
strategy, rank matched artifacts, and call the LLMs API passing in
user queries and context documents. New advances for building
RAG systems are constantly emerging [8, 12] but how they relate
and perform for a specific application context has to be discovered.
In this work we present the lessons learned and 7 failure points
arising from 3 case studies. The purpose of this paper is to provide
1) a reference to practitioners and 2) to present a research road
map for RAG systems. To the best of our knowledge, we present
the first empirical insight into the challenges with creating robust
RAG systems. As advances in LLMs continue to take place, the
software engineering community has a responsibility to provide
knowledge on how to realise robust systems with LLMs. This work
is an important step for robustness in building RAG systems.
Research questions for this work include:
•What are the failure points that occur when engineering a RAG
system? (section 5) We present an empirical experiment using
the BioASQ data set to report on potential failure points. The
experiment involved 15,000 documents and 1000 question
arXiv:2401.05856v1  [cs.SE]  11 Jan 2024



Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 11):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
Table 7: Performance of number of chunks retrieved with gpt-4.
# Retrieved Chunks ADA-002 Custom ADA-002 BM25 Hybrid BM25 + Reranker
3 6.19 6.41 7.10 7.31 7.44
5 6.29 6.61 7.32 7.37 7.43
7 6.42 6.82 7.17 7.20 7.32
9 6.57 6.88 7.22 7.34 7.37
Table 8: Performance of best retriever RAG.
Model Retriever Method # Retrieved Chunks Degradation
gpt-4 ADA-002 9 -13.0%
gpt-4 ADA-002 Custom 9 -8.8%
gpt-4 BM25 5 -3%
gpt-4 Hybrid 5 -2.3%
gpt-4 BM25 + Reranker 3 -1.4%
Gemini Pro BM25 + Reranker 3 -2.2%
RAG is directly related to the input size and the position where the answer is located. Therefore, the final results
confirm the observations made in our experiments.
Moreover, from a cost perspective, it is crucial to avoid overloading the LLM with a large number of input tokens, as
the cost is also based on the amount of input text.
It is important to note that the results obtained in this study cannot be considered as a generalization for other datasets.
Exploratory Data Analysis and the use of good retriever practices, as presented in [34], are always a solid path to
achieving good results.
6.3 Final Results
Despite this work being grounded in a single dataset, it is always crucial to emphasize the importance of data quality.
In a simplified manner, as illustrated in Figure 13, data quality in RAG can be divided into Input, Retriever, and
Evaluation.
Figure 13: Core Points in RAG.
• Input: How are the queries formulated? Are they synthetic or generic? What is the application’s purpose?
Building a RAG for a chatbot differs significantly from constructing a RAG for extracting information from
a long and complex document.
• Retriever: How do the data behave in information retrieval? Are the queries strongly linked by keywords in
the text? What is the cosine similarity of query-to-query, query-to-document, and document-to-document?
• Evaluation: How are the data measured? Defining metrics and success rates, as in the first experiment, is
always a safe path to avoid biases. Build the evaluation system before testing, avoiding bias.
In conclusion, the main contribution of this work was to identify and present the best possible configuration of tech-
niques and parameters for a RAG application. Figure 14 provides a summarized overview of the end-to-end experi-
ment results for the discussed approaches, and the best practices recommended by this study achieve a final accuracy
of 98.61%, representing an improvement of 40.73 percentage points compared to the baseline.



Source: data\tc17_2312.15883v2\referenced_papers\[8]_2401.05856.pdf (Page 1):

CAIN 2024, April 2024, Lisbon, Portugal Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek
and answer pairs. We indexed all documents then ran the
queries and stored the generated responses using GPT-4. All
question and answer pairs were then validated with OpenAI
evals 1. Manual inspection (all discrepancies, all flagged as
incorrect, and a sample of correct labels) was analysed to
identify the patterns.
•What are the key considerations when engineering a RAG
system? (section 6) We present the lessons learned from three
case studies involving the implementation of a RAG system.
This presents the challenges faced and insights gained.
Contributions arising from this work include:
•A catalogue of failure points (FP) that occur in RAG systems.
•An experience report from 3 case studies of implementing a
RAG system. Two currently running at Deakin University.
•A research direction for RAG systems based on the lessons
learned from the 3 case studies.
2 RELATED WORK
Retrieval augmented generation encompasses using documents
to augment large language models through pre-training and at
inference time [7, 9, 12]. Due to the compute cost, data preparation
time and required resources using RAG without training or fine-
tuning is an attractive proposition. However, challenges arise when
using large language models for information extraction such as
performance with long text [8].
A recent survey [ 19] showed that large language models are
used across the RAG pipeline including retriever, data generation,
rewriter, and reader. Our work complements this survey by taking
a software engineering perspective to shine a light on what issues
engineers will face and what software engineering research is nec-
essary to realise solutions with the current state-of-the-art RAG
systems.
Emerging work has looked at benchmarking RAG systems [3]
but not at the failures occurring during implementation. Software
engineering research has investigated the use of RAG systems for
code-related tasks [15]. However, the application of RAG systems
is broader than software engineering tasks. This paper comple-
ments existing work by presenting challenges faced during the
implementation of a RAG system with a focus on practitioners.
Errors and failures that arise from RAG systems overlap with
other information retrieval systems including 1) no metrics for
query rewriting, 2) document re-ranking, and 3) effective content
summarisation [19]. Our results confirm this The unique aspects
are related to the semantic and generative nature of the use of large
language models including evaluating factual accuracy [16].
3 RETRIEVAL AUGMENTED GENERATION
With the explosion in popularity of large language model services
such as ChatGPT2, Claude3, and Bard 4, people have explored their
use as a question and answering systems. While the performance
is impressive [16] there are two fundamental challenges: 1) hallu-
cinations - where the LLM produces a response that looks right
1https://github.com/openai/evals
2https://chat.openai.com/
3https://claude.ai/
4https://bard.google.com/
but is incorrect, and 2) unbounded - no way to direct or update
the content of the output (other than through prompt engineering).
A RAG system is an information retrieval approach designed to
overcome the limitations of using a LLM directly.
RAG works by taking a natural language query is converted into
an embedding which is used to semantically search a set of docu-
ments. Retrieved documents are then passed to a large language
model to generate an answer. An overview of a RAG system is
shown in Figure 1 as two separate processes, Index and Query. See
this survey for more details [19]
3.1 Index Process
In a RAG system, the retrieval system works using embeddings
that provide a compressed semantic representation of the docu-
ment. An embedding is expressed as a vector of numbers. During
the Index process each document is split into smaller chunks that
are converted into an embedding using an embedding model. The
original chunk and the embedding are then indexed in a database.
Software engineers face design decisions around how best to chunk
the document and how large a chunk should be. If chunks are too
small certain questions cannot be answered, if the chunks are too
long then the answers include generated noise.
Different types of documents require different chunking and pro-
cessing stages. For example, video content requires a transcription
pipeline to extract the audio and convert to text prior to encoding
(see subsection 4.2. The choice of which embedding to use also
matters as changing the embedding strategy requires re-indexing
all chunks. An embedding should be chosen based on the ability to
semantically retrieve correct responses. This process depends on
the size of the chunks, the types of questions expected, the structure
of the content and the application domain.
3.2 Query Process
The Query process takes place at run time. A question expressed
as natural language is first converted into a general query. To gen-
eralise the query a large language model is used which enables
additional context such as previous chat history to be included
in the new query. An embedding is then calculated from the new
query to use for locating relevant documents from the database.
Top-k similar documents are retrieved using a similarity method
such as cosine similarity (vector databases have techniques such as
inverted indexes to speed up retrieval time). The intuition is that
chunks that are semantically close to the query are likely to contain
the answer.
Retrieved documents are then re-ranked to maximise the likeli-
hood that the chunk with the answer is located near the top. The
next stage is the Consolidator which is responsible for processing
the chunks. This stage is needed to overcome the limitations of
large language models 1) token limit and 2) rate limit. Services such
as OpenAI have hard limits on the amount of text to include in a
prompt. This restricts the number of chunks to include in a prompt
to extract out an answer and a reduction strategy is needed to chain
prompts to obtain an answer. These online services also restrict the
number of tokens to use within a time frame restricting the latency
of a system. Software engineers need to consider these tradeoffs
when designing a RAG system.



Source: data\tc17_2312.15883v2\referenced_papers\[8]_2401.05856.pdf (Page 4):

Seven Failure Points When Engineering a Retrieval Augmented Generation System CAIN 2024, April 2024, Lisbon, Portugal
FP Lesson Description Case Studies
FP4 Larger context get better results (Context refers to a
particular setting or situation in which the content
occurs)
A larger context enabled more accurate responses
(8K vs 4K). Contrary to prior work with GPT-3.5 [13]
AI Tutor
FP1 Semantic caching drives cost and latency down RAG systems struggle with concurrent users due to
rate limits and the cost of LLMs. Prepopulate the
semantic cache with frequently asked questions [1].
AI Tutor
FP5-7 Jailbreaks bypass the RAG system and hit the safety
training.
Research suggests fine-tuning LLMs reverses safety
training [11], test all fine-tuned LLMs for RAG sys-
tem.
AI Tutor
FP2, FP4 Adding meta-data improves retrieval. Adding the file name and chunk number into the
retrieved context helped the reader extract the re-
quired information. Useful for chat dialogue.
AI Tutor
FP2, FP4-7 Open source embedding models perform better for
small text.
Opensource sentence embedding models performed
as well as closed source alternatives on small text.
BioASQ, AI Tutor
FP2-7 RAG systems require continuous calibration. RAG systems receive unknown input at runtime
requiring constant monitoring.
AI Tutor, BioASQ
FP1, FP2 Implement a RAG pipeline for configuration. A RAG system requires calibrating chunk size,
embedding strategy, chunking strategy, retrieval
strategy, consolidation strategy, context size, and
prompts.
Cognitive Reviewer,
AI Tutor, BioASQ
FP2, FP4 RAG pipelines created by assembling bespoke solu-
tions are suboptima.
End-to-end training enhances domain adaptation
in RAG systems [18].
BioASQ, AI Tutor
FP2-7 Testing performance characteristics are only possi-
ble at runtime.
Offline evaluation techniques such as G-Evals [14]
look promising but are premised on having access
to labelled question and answer pairs.
Cognitive Reviewer,
AI Tutor
Table 2: The lessons learned from the three case studies with key takeaways for future RAG implementations
sort out the security/privacy (who can access what). Furthermore,
as the foundation model itself evolves or you get new data to add to
the model, you will need to run finetuning again. On the other side,
RAG systems seem to offer a pragmatic solution allowing you to
chunk your data as needed and only use relevant chunks into the
context to ask the LLM to generate an answer from the included
context. This facilitates continuously updating the knowledge with
new documents and also gives the control over what chunks the user
is able to access. However, optimal strategies for chunk embedding,
retrieval, and contextual fusion remain active research. Further
work should systematically compare finetuning and RAG paradigms
across factors including accuracy, latency, operating costs, and
robustness.
6.3 Testing and Monitoring RAG systems
Software engineering best practices are still emerging for RAG sys-
tems. Software testing and test case generation are one of the areas
for refinement. RAG systems require questions and answers that are
application specific often unavailable when indexing unstructured
documents. Emerging work has considered using LLMs for gen-
erating questions from multiple documents [4]. How to generate
realistic domain relevant questions and answers remains an open
problem.
Once suitable test data is available quality metrics are also re-
quired to assist engineers in making quality tradeoffs. Using large
language models is expensive, introduces latency concerns, and has
performance characteristics that all change with each new release.
This characteristic has previously been studied for machine learn-
ing systems [5, 6] but the required adaptations (if any) have yet to
be applied to LLM based systems such as RAGs. Another idea is to
incorporate ideas from self-adaptive systems to support monitoring
and adapting RAG systems, preliminary work has started for other
machine learning applications [2].
7 CONCLUSION
RAG systems are a new information retrieval that leverages LLMs.
Software engineers increasingly interact with RAG systems a)
through implementing semantic search, or b) through new code-
dependent tasks. This paper presented the lessons learned from 3
case studies including an empirical investigation involving 15,000
documents and 1000 questions. Our findings provide a guide to
practitioners by presenting the challenges faced when implement-
ing RAG systems. We also included future research directions for
RAG systems related to 1) chunking and embeddings, 2) RAG vs
Finetuning, and 3) Testing and Monitoring. Large language models
are going to continue to obtain new capabilities of interest to engi-
neers and researchers. This paper presents the first investigation
into RAG systems from a software engineering perspective.
ACKNOWLEDGMENTS
To Amanda Edgar, Rajesh Vasa, Kon Mouzakis, Matteo Vergani,
Trish McCluskey, Kathryn Perus, Tara Draper, Joan Sutherland and
Ruary Ross for their support and involvement in making the AI
Tutor project possible.



### Claim 19/36

#### Claim Text
The method to estimate ⟨X⟩, ⟨Y ⟩, ⟨XY ⟩, ⟨XPy⟩ from an experimentally obtained image of the light beam is covered in the following .

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[18]_2212.10496.pdf (Page 2):

instruction following generative LLM, as described
above.
Zero-Shot Dense Retrieval The tasks of zero-
shot (dense) retrieval are arguably empirically de-
ﬁned by Thakur et al. (2021) for the neural re-
trieval community. Their BEIR benchmark con-
sists of diverse retrieval tasks. The paper and
many follow-up research generally consider the
Transfer Learning setup where the dense re-
triever is ﬁrst learned using a diverse and richly
supervised corpus and query collection, namely
MS-MARCO (Thakur et al., 2021; Wang et al.,
2022; Yu et al., 2022).
However, as stated by Izacard et al. (2021), such
a large collection can rarely be assumed. In this
paper, therefore, we study the problem of building
effective dense retrieval systems without relevance
labels. Similar to Izacard et al. (2021), we also
do not assume access to the test time corpora for
training. This is a more realistic setup and prevents
over-engineering on the test corpora.
By the deﬁnition in Sachan et al. (2022), our
setup can be roughly considered as “unsuper-
vised”. Strictly, as with Sachan et al. (2022), the
only supervision resides in the LLM, in the pro-
cessing of learning to follow instructions.
Generative Retrieval Generative search is a new
class of retrieval methods that use neural generative
models as search indices (Metzler et al., 2021; Tay
et al., 2022; Bevilacqua et al., 2022; Lee et al.,
2022). These models use (constrained) decoding
to generate document identiﬁers, such as id and
sub-string, which map directly to real documents.
They have to go through special training procedures
over relevance data; effective search may also need
to use novel forms of search indices (Bevilacqua
et al., 2022; Lee et al., 2022). In comparison, our
method uses the standard MIPS index and requires
no training or training data. Our generative model
produces an intermediate hypothetical document
to be fed into a dense encoder, instead of a real
document.
3 Methodology
In this section, we ﬁrst formally deﬁne the prob-
lem of (zero-shot) dense retrieval. Then we will
introduce how HyDE is designed to solve it.
3.1 Preliminaries
Dense retrieval models similarity between query
and document with inner product similarity. Given
a query q and document d, it uses two encoder
function encq and encd to map them into d dimen-
sion vectors vq, vd, whose inner product is used
as similarity measurement.
sim(q, d) =⟨encq(q), encd(d)⟩ = ⟨vq, vd⟩ (1)
For zero-shot retrieval, we consider L query sets
Q1, Q2, ..., QL and their corresponding search cor-
pus, document sets D1, D2, ..., DL. Denote the
j-th query from i-th set query set Qi as qij. We
need to fully deﬁne mapping functions encq and
encd without access to any query set Qi, document
set Di, or any relevance judgment rij.
The difﬁculty of zero-shot dense retrieval lies
precisely in Equation 1: it requires learning of two
embedding functions (for query and document re-
spectively) into the same embedding space where
inner product captures relevance. Without rele-
vance judgments/scores to ﬁt, learning becomes
intractable.
3.2 HyDE
HyDE circumvents the aforementioned learning
problem by performing search in document-
only embedding space that captures document-
document similarity. This can be easily learned
using unsupervised contrastive learning (Izacard
et al., 2021; Gao et al., 2021; Gao and Callan,
2022). We set document encoder encd directly as a
contrastive encoder enccon.
f = encd = enccon (2)
This function is also denoted as f for simplic-
ity. This unsupervised contrastive encoder will
be shared by all incoming document corpus.
vd = f(d) ∀d ∈ D1 ∪ D2 ∪ ... ∪ DL (3)
To build the query vector, we consider in addition
an instruction following LM, InstructLM. It takes a
query q and a textual instruction INST and follows
them to perform the task speciﬁed by INST . For
simplicity, denote,
g(q, INST ) =InstructLM(q, INST ) (4)
Now we can use g to map queries to "hypotheti-
cal" documents by sampling from g, setting INST



Source: data\tc17_2312.15883v2\referenced_papers\[19]_2212.10496.pdf (Page 2):

instruction following generative LLM, as described
above.
Zero-Shot Dense Retrieval The tasks of zero-
shot (dense) retrieval are arguably empirically de-
ﬁned by Thakur et al. (2021) for the neural re-
trieval community. Their BEIR benchmark con-
sists of diverse retrieval tasks. The paper and
many follow-up research generally consider the
Transfer Learning setup where the dense re-
triever is ﬁrst learned using a diverse and richly
supervised corpus and query collection, namely
MS-MARCO (Thakur et al., 2021; Wang et al.,
2022; Yu et al., 2022).
However, as stated by Izacard et al. (2021), such
a large collection can rarely be assumed. In this
paper, therefore, we study the problem of building
effective dense retrieval systems without relevance
labels. Similar to Izacard et al. (2021), we also
do not assume access to the test time corpora for
training. This is a more realistic setup and prevents
over-engineering on the test corpora.
By the deﬁnition in Sachan et al. (2022), our
setup can be roughly considered as “unsuper-
vised”. Strictly, as with Sachan et al. (2022), the
only supervision resides in the LLM, in the pro-
cessing of learning to follow instructions.
Generative Retrieval Generative search is a new
class of retrieval methods that use neural generative
models as search indices (Metzler et al., 2021; Tay
et al., 2022; Bevilacqua et al., 2022; Lee et al.,
2022). These models use (constrained) decoding
to generate document identiﬁers, such as id and
sub-string, which map directly to real documents.
They have to go through special training procedures
over relevance data; effective search may also need
to use novel forms of search indices (Bevilacqua
et al., 2022; Lee et al., 2022). In comparison, our
method uses the standard MIPS index and requires
no training or training data. Our generative model
produces an intermediate hypothetical document
to be fed into a dense encoder, instead of a real
document.
3 Methodology
In this section, we ﬁrst formally deﬁne the prob-
lem of (zero-shot) dense retrieval. Then we will
introduce how HyDE is designed to solve it.
3.1 Preliminaries
Dense retrieval models similarity between query
and document with inner product similarity. Given
a query q and document d, it uses two encoder
function encq and encd to map them into d dimen-
sion vectors vq, vd, whose inner product is used
as similarity measurement.
sim(q, d) =⟨encq(q), encd(d)⟩ = ⟨vq, vd⟩ (1)
For zero-shot retrieval, we consider L query sets
Q1, Q2, ..., QL and their corresponding search cor-
pus, document sets D1, D2, ..., DL. Denote the
j-th query from i-th set query set Qi as qij. We
need to fully deﬁne mapping functions encq and
encd without access to any query set Qi, document
set Di, or any relevance judgment rij.
The difﬁculty of zero-shot dense retrieval lies
precisely in Equation 1: it requires learning of two
embedding functions (for query and document re-
spectively) into the same embedding space where
inner product captures relevance. Without rele-
vance judgments/scores to ﬁt, learning becomes
intractable.
3.2 HyDE
HyDE circumvents the aforementioned learning
problem by performing search in document-
only embedding space that captures document-
document similarity. This can be easily learned
using unsupervised contrastive learning (Izacard
et al., 2021; Gao et al., 2021; Gao and Callan,
2022). We set document encoder encd directly as a
contrastive encoder enccon.
f = encd = enccon (2)
This function is also denoted as f for simplic-
ity. This unsupervised contrastive encoder will
be shared by all incoming document corpus.
vd = f(d) ∀d ∈ D1 ∪ D2 ∪ ... ∪ DL (3)
To build the query vector, we consider in addition
an instruction following LM, InstructLM. It takes a
query q and a textual instruction INST and follows
them to perform the task speciﬁed by INST . For
simplicity, denote,
g(q, INST ) =InstructLM(q, INST ) (4)
Now we can use g to map queries to "hypotheti-
cal" documents by sampling from g, setting INST



Source: data\tc17_2312.15883v2\referenced_papers\[19]_2212.10496.pdf (Page 3):

to be “write a paragraph that answers the
question”. The generated document is not real,
can and is likely to be ungrounded factually (Brown
et al., 2020; Thoppilan et al., 2022). We only re-
quire it to capture relevance pattern. This is done
by generating documents, i.e. providing exam-
ples. Critically, here we ofﬂoad relevance mod-
eling from representation learning model to an
NLG model that generalizes signiﬁcantly more eas-
ily, naturally, and effectively (Brown et al., 2020;
Ouyang et al., 2022). Generating examples also
replaces explicit modeling of relevance scores.
We can now encode the generated document using
the document encoder f. Write,
E[vqij ] =E[f(g(qij, INST i))] (5)
Formally, g deﬁnes a probability distribution based
on the chain rule. In this paper, we simply consider
the expectation value, assuming the distribution of
vqij is uni-modal, i.e. the query is not ambiguous.
The study of ambiguous queries and diversity is
left to future work. We estimate Equation 5 by
sampling N documents from g, [ ˆd1, ˆd2, ...,ˆdN ].
ˆvqij = 1
N
∑
ˆdk∼g(qij ,INST i)
f(dk) (6)
= 1
N
N∑
k=1
f( ˆdk) (7)
We also consider the query as a possible hypothesis,
ˆvqij = 1
N + 1[
N∑
k=1
f( ˆdk) +f(qij)] (8)
Inner product is computed between ˆvqij and the
set of all document vectors {f(d)|d ∈ Di}. The
most similar documents are retrieved. Here the
encoder function f serves as a lossy compressor
that outputs dense vectors, where the extra details
are ﬁltered and left out from the vector. It further
grounds the hypothetical vector to the actual corpus
and the real documents. The full HyDE system is
illustrated in Figure 1.
4 Experiments
4.1 Setup
Implementation We implement HyDE using
InstructGPT, a GPT-3 model from the instruct
series (text-davinci-003; Ouyang et al. (2022))
and Contrievermodels (Izacard et al., 2021). We
sample from InstructGPT using the OpenAI play-
ground default temperature of 0.7 for open-ended
generations. We use the English-only Contriever
model for English retrieval tasks and multilingual
mContrieverfor non-English tasks. We conducted
retrieval experiments with the Pyserini toolkit (Lin
et al., 2021a).
Datasets We consider web search query sets
TREC DL19 (Craswell et al., 2020a) and
DL20 (Craswell et al., 2020b); they are based on
the MS-MARCO dataset (Bajaj et al., 2016). We
also use a diverse collection of 6 low-resource
datasets from the BEIR dataset (Thakur et al.,
2021). For non-English retrieval, we consider
Swahili, Korean, Japanese, and Bengali from the
Mr.Tydi dataset (Zhang et al., 2021).
We use different instructions for each dataset.
They share a similar structure but have different
quantiﬁers to control the exact form of the gener-
ated hypothetical documents. These instructions
can be found in subsection A.1.
Compared Systems Contriever models,
Contrieverand mContriever, serve as our major
baseline. They are trained using unsupervised
contrastive learning. HyDE retrievers share the
exact same embedding spaces with them. The
only difference is how the query vector is built.
These comparisons allow us to easily examine
the effect of HyDE. The classical heuristic-based
lexical retriever BM25 is also included.
Several systems that involve ﬁne-tuning on mas-
sive relevance data are also included as refer-
ences. We consider models ﬁne-tuned on MS-
MARCO and transferred, DPR and ANCE, from
the BEIR paper. For multilingual, we include
the mDPR model from Mr.Tydi paper and MS-
MARCO ﬁne-tuned mBERT and XLM-R from
the Contriever paper. We also include the state-of-
the-art transfer learning models: Contrieverand
mContrieverﬁne-tuned on MS-MARCO, denoted
ContrieverFT and mContrieverFT. These mod-
els have run through the state-of-the-art retrieval
model training pipeline that involves second-stage
retrieval-speciﬁc pre-training (Lee et al., 2019) and
a few rounds of ﬁne-tuning (Qu et al., 2021); they
should be considered empirical upper bounds.
4.2 Web Search
In Table 1, we show retrieval results on TREC
DL19 and TREC DL20. We see HyDEbring sizable
improvements to Contriever across the board for



Source: data\tc17_2312.15883v2\referenced_papers\[18]_2212.10496.pdf (Page 3):

to be “write a paragraph that answers the
question”. The generated document is not real,
can and is likely to be ungrounded factually (Brown
et al., 2020; Thoppilan et al., 2022). We only re-
quire it to capture relevance pattern. This is done
by generating documents, i.e. providing exam-
ples. Critically, here we ofﬂoad relevance mod-
eling from representation learning model to an
NLG model that generalizes signiﬁcantly more eas-
ily, naturally, and effectively (Brown et al., 2020;
Ouyang et al., 2022). Generating examples also
replaces explicit modeling of relevance scores.
We can now encode the generated document using
the document encoder f. Write,
E[vqij ] =E[f(g(qij, INST i))] (5)
Formally, g deﬁnes a probability distribution based
on the chain rule. In this paper, we simply consider
the expectation value, assuming the distribution of
vqij is uni-modal, i.e. the query is not ambiguous.
The study of ambiguous queries and diversity is
left to future work. We estimate Equation 5 by
sampling N documents from g, [ ˆd1, ˆd2, ...,ˆdN ].
ˆvqij = 1
N
∑
ˆdk∼g(qij ,INST i)
f(dk) (6)
= 1
N
N∑
k=1
f( ˆdk) (7)
We also consider the query as a possible hypothesis,
ˆvqij = 1
N + 1[
N∑
k=1
f( ˆdk) +f(qij)] (8)
Inner product is computed between ˆvqij and the
set of all document vectors {f(d)|d ∈ Di}. The
most similar documents are retrieved. Here the
encoder function f serves as a lossy compressor
that outputs dense vectors, where the extra details
are ﬁltered and left out from the vector. It further
grounds the hypothetical vector to the actual corpus
and the real documents. The full HyDE system is
illustrated in Figure 1.
4 Experiments
4.1 Setup
Implementation We implement HyDE using
InstructGPT, a GPT-3 model from the instruct
series (text-davinci-003; Ouyang et al. (2022))
and Contrievermodels (Izacard et al., 2021). We
sample from InstructGPT using the OpenAI play-
ground default temperature of 0.7 for open-ended
generations. We use the English-only Contriever
model for English retrieval tasks and multilingual
mContrieverfor non-English tasks. We conducted
retrieval experiments with the Pyserini toolkit (Lin
et al., 2021a).
Datasets We consider web search query sets
TREC DL19 (Craswell et al., 2020a) and
DL20 (Craswell et al., 2020b); they are based on
the MS-MARCO dataset (Bajaj et al., 2016). We
also use a diverse collection of 6 low-resource
datasets from the BEIR dataset (Thakur et al.,
2021). For non-English retrieval, we consider
Swahili, Korean, Japanese, and Bengali from the
Mr.Tydi dataset (Zhang et al., 2021).
We use different instructions for each dataset.
They share a similar structure but have different
quantiﬁers to control the exact form of the gener-
ated hypothetical documents. These instructions
can be found in subsection A.1.
Compared Systems Contriever models,
Contrieverand mContriever, serve as our major
baseline. They are trained using unsupervised
contrastive learning. HyDE retrievers share the
exact same embedding spaces with them. The
only difference is how the query vector is built.
These comparisons allow us to easily examine
the effect of HyDE. The classical heuristic-based
lexical retriever BM25 is also included.
Several systems that involve ﬁne-tuning on mas-
sive relevance data are also included as refer-
ences. We consider models ﬁne-tuned on MS-
MARCO and transferred, DPR and ANCE, from
the BEIR paper. For multilingual, we include
the mDPR model from Mr.Tydi paper and MS-
MARCO ﬁne-tuned mBERT and XLM-R from
the Contriever paper. We also include the state-of-
the-art transfer learning models: Contrieverand
mContrieverﬁne-tuned on MS-MARCO, denoted
ContrieverFT and mContrieverFT. These mod-
els have run through the state-of-the-art retrieval
model training pipeline that involves second-stage
retrieval-speciﬁc pre-training (Lee et al., 2019) and
a few rounds of ﬁne-tuning (Qu et al., 2021); they
should be considered empirical upper bounds.
4.2 Web Search
In Table 1, we show retrieval results on TREC
DL19 and TREC DL20. We see HyDEbring sizable
improvements to Contriever across the board for



Source: data\tc17_2312.15883v2\referenced_papers\[18]_2212.10496.pdf (Page 5):

Swahili Korean Japanese Bengali
w/o relevance judgement
BM25 38.9 28.5 21.2 41.8
mContriever 38.3 22.3 19.5 35.3
HyDE 41.7 30.6 30.7 41.3
w/ relevance judgement
mDPR 7.3 21.9 18.1 25.8
mBERT 37.4 28.1 27.1 35.1
XLM-R 35.1 32.2 24.8 41.7
mContrieverFT 51.2 34.2 32.4 42.3
Table 3: MRR@100 on Mr.Tydi. Best performing w/o
relevance and overall system(s) are marked bold.
under-speciﬁcation of the instruction; more elabo-
rative instructions may help.
4.4 Multilingual Retrieval
Multilingual setup poses several additional chal-
lenges to HyDE. The small-sized contrastive en-
coder gets saturated as the number of languages
scales (Conneau et al., 2020; Izacard et al., 2021).
Meanwhile, our generative LLM faces an opposite
issue: with languages of not as high resource as
English or French, the high capacity LLM can get
under-trained (Hoffmann et al., 2022).
Nevertheless, in Table 3, we still ﬁnd HyDE
able to improve the mContriever model. It can
outperform non-Contriever models ﬁne-tuned on
and transferred from MS-MARCO. On the other
hand, we do observe some margins between HyDE
and ﬁne-tuned mContrieverFT. Since HyDE and
mContrieverFT use similar contrastive encoders,
we hypothesize this is because the non-English lan-
guages we considered are under-trained in both
pre-training and instruction learning stages.
5 Analysis
The generative LLM and contrastive encoder make
up the backbone of HyDE. In this section, we study
the effect of changing their realizations. In partic-
ular, we consider smaller language models (LM)
and ﬁne-tuned encoders. We conduct our studies
on TREC DL19/20.
5.1 Effect of Different Generative Models
In Table 4, we show HyDE using other
instruction-following language models. In
particular, we consider a 52-billion Cohere
model ( command-xlarge-20221108) and a
11-billion FLAN model ( FLAN-T5-xxl; Wei
et al. (2022)). 2 Generally, we observe that all
2Model sizes are from https://crfm.stanford.edu/
helm/v1.0/?models.
Model DL19 DL20
Contriever 44.5 42.1
ContrieverFT 62.1 63.2
HyDE
w/ Contriever
w/ Flan-T5 (11b) 48.9 52.9
w/ Cohere (52b) 53.8 53.8
w/ GPT (175b) 61.3 57.9
w/ ContrieverFT
w/ Flan-T5 (11b) 60.2 62.1
w/ Cohere (52b) 61.4 63.1
w/ GPT (175b) 67.4 63.5
Table 4: NDCG@10 on TREC DL19/20. Effect
of changing different instruction LMs and using ﬁne-
tuned encoder. Best w/o relevance and overall models
are marked bold.
models bring improvement to the unsupervised
Contriever, with larger models bringing larger
improvements. At the time when this paper is
written, the Cohere model is still experimental
without much detail disclosed. We can only
tentatively hypothesize that training techniques
may have also played some role in the performance
difference.
5.2 HyDE with Fine-tuned Encoder
To begin with, HyDE with ﬁne-tuned encoder is
not the intended usage: HyDE is more powerful
and irreplaceable when few relevance labels are
present. Here we are interested to ﬁnd out if
and how HyDE embedding can affect ﬁne-tuned en-
coders. In Table 4, we see that less powerful instruc-
tion LMs can negatively impact the overall perfor-
mance of the ﬁne-tuned retriever. (To remind our
readers, ContrieverFT is in-domain supervisedly
ﬁne-tuned for TREC DL19/20). The performance
degradations remain small. On the other hand, we
also observe the InstructGPT model able to fur-
ther bring up the performance, especially on DL19.
This suggests that there may still exist certain fac-
tors not captured by the ﬁne-tuned encoder but only
by the generative model.
6 Conclusion
At the end of the paper, we encourage the readers
to take a moment and reﬂect on the HyDE model.
Compare it to some of the other recently seen re-
trievers or re-ranker. These other models probably
differ in their architecture, training method, and/or
task, but probably all of them involve modeling
relevance scores between a pair of query and docu-



### Claim 20/36

#### Claim Text
In particular, while linear curvature feedback was initially considered ideal for fitting Chlamydomonas data , the non-linear sliding feedback model proposed by , which incorporates the attachment and detachment of antagonistic molecular motors, provides a better fit.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 7):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
the application of Multiple Negative Ranking Loss, as presented in the article "Efficient Natural Language Response
Suggestion for Smart Reply," Section 4.4 [26]. However, given our current focus on simplicity, we will reserve the
exploration of this technique for future work. At this moment, we choose to utilize the Mean Squared Error (MSE)
Loss.
For the fine-tuning stage, it is necessary to have two types of samples:
• positives: [question i, chunki, label = 1]
• negatives: [question i, chunkj, label = −1], for i ̸= j
Often, as is the case with our dataset, only positive examples are available. However, through a simple and random
shuffling, it is possible to generate negative examples. Demonstrating confidence in transfer learning, we found that
a few examples were sufficient. Our final dataset consisted of approximately 400 examples, maintaining a 1 : 3ratio
between positive and negative examples.
The hyperparameters that exert the most significant impact on performance include the learning rate, batch size, and
the number of dimensions in the projection matrix. The ADA-002 model has 1536 dimensions, and the projection
matrix is of size 1536 × N, where N ∈ 1024, 2048, 4096. In our experiments, we observed that 2048 dimensions
resulted in the best accuracy.
This type of fine-tuning requires low GPU resources, with a training time of approximately 5 minutes using the A100
GPU. The model itself is straightforward, consisting of a matrix with dropout (to mitigate overfitting), followed by the
hyperbolic tangent activation function, which provided additional accuracy gains in the training set.
Figure 6: Cosine similarity of positive and negative classes in the ADA-002 embedding; note the significant overlap
between the classes. Test accuracy (before training): 69.5%
Figure 7: Cosine similarity of positive and negative classes in the customized embedding; the intersection between the
classes is minimal. Test accuracy (after training): 84.3%
When analyzing the cosine similarity between positive and negative classes, we can observe the "shadow" shared by
the histograms. In an ideal scenario, we desire the classes to be disjoint to ensure a clear definition of space. Figure 6
illustrates a significant shadow in the embedding before training, while Figure 7 shows the result after training. Both
graphs are derived from the test set. Test accuracy also improved, leading to a better dense representation.
5.1.4 Hybrid Search
As stated before, hybrid search is applied when is necessary to combine results from two or more retrieval methods.
A widely used algorithm to address this type of problem is known as Reciprocal Rank Fusion (RRF). For a document



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 11):

12  
Discussion In this work, we introduce a simple but highly effective framework that combines a biomedical knowledge graph with LLM chat models in a token optimized fashion. This integration resulted in a domain-specific generative system whose responses were firmly grounded in well-established biomedical knowledge. We compared the proposed framework with another RAG approach that utilizes Cypher query and showed that KG-RAG was more robust to prompt perturbation and more efficient in token utilization. In addition, KG-RAG consistently demonstrated superior performance compared to the prompt-based baseline LLM model on all human-curated benchmark datasets. We hypothesize that this performance improvement arises from the fusion of the explicit knowledge from the KG and the implicit knowledge from the LLM. This shows the value of providing domain-specific ground truth at a fine-grained resolution as context at the prompt level.   A heterogeneous knowledge graph with diverse concepts (the biomedical concepts in this case) interconnected at a massive scale has the potential to generate new knowledge as an “emergent property”.(Baranzini et al. 2022; Morris et al. 2023) In fact, as LLMs scale up in various dimensions like model parameters, training data, and training compute, they have been thought to exhibit reasoning or “emerging capabilities”(Wei et al. 2022a) although this observation could also be explained by “in-context learning” or other aspects of the examples.(Brown et al. 2020; Min et al. 2022; Lu et al. 2023) In any case, KG-RAG capitalized this capability and generated biomedical text with rich annotations such as provenance and statistical evidence (if available) thereby resulting in more reliable and knowledge-grounded responses. Additionally, the optimized and fine-grained context retrieval capability of KG-RAG ensured a budget friendly RAG system to apply on proprietary LLMs.



Source: data\tc17_2312.15883v2\referenced_papers\[93]_2311.09210.pdf (Page 5):

Models NQ TriviaQA WebQ Average
EM F1 EM F1 EM F1 EM F1
Backbone language model: LLaMa-2 7B
QA fine-tune w/o IR 28.80 37.53 63.19 68.61 28.30 42.77 35.98 44.27
SAIL (Luo et al., 2023)* 36.20 44.23 73.20 80.92 27.92 40.65 45.77 55.27
Retrieve-Read (Shi et al., 2023c)47.39 55.81 74.92 81.53 29.58 43.51 48.49 56.97
+ CHAIN-OF-NOTE (ours) 48.92 57.53 76.27 82.25 32.33 46.68 50.46 58.78
(+1.53) (+1.72) (+1.35) (+0.72) (+2.75) (+3.17) (+1.97) (+1.81)
Backbone language model:GPT-4-1106†
QA prompt w/o IR 54.0 74.2 56.2 61.5
Retrieve-Read (Shi et al., 2023c) 61.8 70.6 56.8 63.1
+ CHAIN-OF-THOUGHT 63.6 71.2 58.4 64.4
+ CHAIN-OF-NOTE (OURS) 63.8 74.6 58.8 65.7
(+2.0) (+4.0) (+2.0) (+2.6)
Table 2: The RALM, when equipped with CHAIN -OF-NOTE (CON), demonstrates a marginal improvement over
the standard RALM in full test set evaluations. Significantly, it outperforms the standard RALM system in scenarios
with noisy documents, suggesting that CON can substantially enhance the model’s noise robustness.
* SAIL was designed for retrieval-augmented instruction tuning, and as such, may not be ideally factual QA.
† Evaluating GPT-4 outputs with EM score is challenging; we opt for Accuracy, with reasons outlined in§3.1.3.
Karpukhin et al. (2020); Zhu et al. (2021). For
EM score, an answer is deemed correct if its nor-
malized form – obtained through the normalization
procedure delineated by (Karpukhin et al., 2020)
– corresponds to any acceptable answer in the pro-
vided list. Similar to EM score, F1 score treats the
prediction and ground truth as bags of tokens, and
compute the average overlap between the predic-
tion and ground truth answer (Chen et al., 2017).
Besides, we use reject rate (RR) to evaluate the
unknown robustness when given questions beyond
a language model’s knowledge scope.
Finally, since GPT-4 is not directly trained on
open-domain QA benchmarks, employing EM /
F1 for evaluation is challenging. Therefore, we
adopt the approach outlined in Mallen et al. (2023);
Kandpal et al. (2023), utilizing accuracy as the
evaluation metric. Accuracy considers a prediction
correct if any substring of the prediction exactly
matches any of the provided correct answers.
3.2 Evaluation on Overall QA Performance
Table 2 demonstrates that the RALM consistently
outperforms the directly fine-tuned LLaMa-2 with
QA pairs, without retrieval. This improvement
is closely tied to the effectiveness of the retrieval
process. As indicated in Table 1, DPR demon-
strates markedly superior retrieval performance on
the NQ and TriviaQA datasets compared to WebQ.
Consequently, the benefits of retrieval are more
pronounced on NQ and TriviaQA. Furthermore,
when comparing our enhanced RALM, which inte-
grates CON, with the standard RALM, our method
persistently shows better performance. There is
an average improvement of +1.97 in EM scores
across all three datasets when using LLaMa-2 as
backbone language model. Delving deeper, we
find that this improvement varies depending on
whether DPR successfully retrieves relevant doc-
uments. Specifically, the average improvement is
+1.2 when DPR retrieves relevant documents and
+2.3 when it does not on the NQ dataset. This dis-
parity suggests that our CON improve RALM’s in
scenarios where more noisy documents are fetched
in the first retrieval stage. This observation aligns
with our findings on noise robustness, which are
elaborated in the subsequent sections detailing our
experimental results.
Furthermore, the dynamics observed with larger
language models differ from those noted in experi-
ments with smaller-sized models due to their supe-
rior factual knowledge. The impact of utilizing re-
trieval is observed to be less pronounced with larger
models and can even be detrimental in certain cases,
such as with TriviaQA, where questions are mostly
straightforward. Concerning the comparison be-
tween CON and the baseline, the performance trend
remains consistent with that observed in smaller-
sized models, suggesting that CON maintains its
significance across different model sizes.



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 19):

A Scaling laws
We use 7 models to fit the scaling laws of Baichuan
2. The parameter details are shown in Table 10.
Nhidden NFFN Nlayer Nhead Nparams (Millions)
384 1,152 6 6 11.51
704 2,112 8 8 51.56
832 2,496 12 8 108.01
1,216 3,648 16 8 307.60
1,792 5,376 20 14 835.00
2,240 6,720 24 14 1,565.60
2,880 8,640 28 20 3,019.33
Table 10: The model we choose for fitting scaling laws.
The losses of the 7 different models are shown
in Figure 8.
Figure 8: The various training loss of small models for
scaling law.
B NormHead
By conducting a word embedding KNN retrieval
task, where given a query word the nearest K
words are retrieved. We found that the semantic
information is mainly encoded by the cosine
similarity of embedding rather than L 2 distance.
i.e., The KNN results of cosine similarity are words
with semantic similarity while the KNN results of
L2 distance are meaningless in some way. Since
the current linear classifier computes logits by dot
product, which is a mixture of L 2 distance and
cosine similarity. To alleviate the distraction of L2
distance, We propose to compute the logits by the
angle only. We normalized the output Embedding
so that the dot product is not affected by the norm
of embedding.
To validate this operation, we conduct an
ablation experiment where we add or remove the
normalization before softmax and train a 7B model
for 12k steps. All the hyper-parameters and data are
the same with Baichuan 2-7B. The training loss is
shown in Figure 9. We can see that when removing
the NormHead the training became very unstable at
the beginning, on the contrary, after we normalized
the head the training became very stable, which
resulted in better performance.
Figure 9: The training loss with and without NormHead
operation. The experiments are conducted on 7 billion
parameters with the same hyper-parameters (torch
random seeds, data flow, batch size, learning rate, etc.)
C Training Dynamics
In this section, we analyze the training dynamics of
our model. We save the checkpoints of Baichuan 2-
7B and Baichuan 2-13B every 1000 steps. And
evaluate those intermediate results on C-Eval
development set (Huang et al., 2023), MMLU
(Hendrycks et al., 2021a) , CMMLU (Li et al.,
2023) , JEC-QA (Zhong et al., 2020), GSM8K (Shi
et al., 2022) and HumanEval (Chen et al., 2021).
The result is shown in Figure 10.
As shown, both the 7B and 13B models
demonstrate substantial gains as training
progresses. However, on general benchmarks
such as MMLU (Hendrycks et al., 2021a) and
C-Eval (Huang et al., 2023), improvements appear
to plateau after 2 trillion tokens. In contrast,
consistent gains are achieved on the GSM8K
math tasks even beyond 2 trillion tokens. This
suggests training FLOPs may strongly correlate
with improvements in math problem solving,
which may be further studied.
D Baichuan Harmless Evaluation Dataset
WARNING: this section contains unsafe, offensive,
or upsetting examples of text.
We proposed the Baichuan Harmless Evaluation
Dataset (BHED) to evaluate the chat models, as



Source: data\tc17_2312.15883v2\referenced_papers\[93]_2311.09210.pdf (Page 6):

Models Noise NQ TriviaQA WebQ Average
Ratio EM F1 EM F1 EM F1 EM F1
Retrieve-Read 100% 34.28 41.74 55.30 61.67 29.58 46.34 39.72 49.92
+ CHAIN-OF-NOTE 41.83 49.58 64.30 70.00 36.85 53.07 47.66 57.55
(+7.55) (+7.84) (+9.00) (+8.33) (+7.27) (+6.73) (+7.94) (+7.63)
Retrieve-Read 80% 54.28 61.03 73.83 80.02 35.46 52.70 54.52 64.58
+ CHAIN-OF-NOTE 56.63 63.23 75.89 81.24 40.60 56.54 57.70 67.00
(+2.35) (+2.20) (+2.06) (+1.22) (+5.14) (+3.84) (+3.18) (+2.42)
Retrieve-Read 60% 61.44 67.94 78.44 83.65 37.01 54.16 58.96 68.58
+ CHAIN-OF-NOTE 63.43 69.33 78.79 84.07 41.26 56.91 61.16 70.10
(+1.99) (+1.39) (+0.35) (+0.42) (+4.25) (+2.75) (+2.20) (+1.52)
Retrieve-Read 40% 64.62 71.12 80.56 86.76 38.40 55.60 61.19 71.16
+ CHAIN-OF-NOTE 65.91 72.22 81.72 87.11 42.16 58.15 63.26 72.49
(+1.29) (+1.10) (+1.16) (+0.35) (+3.76) (+2.55) (+2.07) (+1.33)
Retrieve-Read 20% 67.21 73.69 81.73 87.89 39.95 56.66 62.96 72.75
+ CHAIN-OF-NOTE 70.00 76.08 82.86 88.24 44.36 60.13 65.74 74.82
(+2.79) (+2.39) (+1.13) (+0.35) (+4.41) (+3.47) (+2.78) (+2.07)
Retrieve-Read 0% 69.23 75.57 83.34 89.44 42.24 58.59 64.93 74.53
+ CHAIN-OF-NOTE 73.28 79.86 83.52 88.94 46.16 62.38 67.65 77.06
(+4.05) (+4.29) (+0.18) (-0.50) (+3.92) (+3.79) (+2.72) (+2.53)
Table 3: Evaluation on Noise Robustness. The backbone language model is LLaMa-2 7B. The CHAIN -OF-NOTE
framework shows superior performance compared to the standard RALM system, particularly notable at higher
noise ratios.We explain how we synthesize data with different noise ratios under real-world scenarios in§ 3.1.1.
Models↓ RealTimeQA
EM F1 RR
Retrieve-Read (Shi et al., 2023c)15.6 19.9 6.1
+ CHAIN-OF-NOTE (ours) 15.7 20.3 13.0
Table 4: Evaluation on Unknown Robustness. TheCON
shows better performance than standard RALM system.
3.3 Evaluation on Noise Robustness
As illustrated in Table 2, when faced with entirely
noisy documents, both the standard RALM and
our CHAIN -OF-NOTE enhanced RALM underper-
formed compared to the no-retrieval setting. This
suggests that RALMs can be misled by noisy infor-
mation, leading to more hallucinations.
Notably, equipping the model withCON enables
it to perform nearly as well as the baseline model
directly fine-tuned with QA pairs without retrieval,
showcasing its robustness to noise and its ability
to disregard irrelevant information. The CON ap-
proach is effective not only in fine-tuned, smaller-
sized models but also in large language models,
such as GPT-4, with adjustments made only to the
prompt. Besides, in comparison to the CHAIN -OF-
THOUGHT technique, commonly utilized in rea-
soning scenarios, CON presents a more efficient
strategy for retrieval-augmented settings, particu-
larly in addressing knowledge-intensive tasks.
Table 3 shows that RALM enhanced with CON
consistently outperforms the standard RALM, es-
pecially in scenarios with exclusively noisy doc-
uments. An average improvement of +7.9 in EM
score on fully noisy documents is observed on three
open-domain QA datasets, in average. Experiments
with lower noise ratios also consistently demon-
strate the improvements brought by CON, aligning
the overall QA performance.
3.4 Evaluation on Unknown Robustness
Table 4 illustrates that our RALM equipped with
CON exhibits superior robustness in handling un-
known scenario, particularly evident in the Real-
TimeQA benchmark. This benchmark falls com-
pletely outside the model’s domain and contains
real-time information that was not part of the
LLaMa-2 pre-training data. Despite this, models
are still capable of providing correct answers in
some cases, as the answers remain consistent over
time. In comparison to the standard RALM sys-
tem, our method shows a significant improvement,
exceeding +10.5 in its ability to reject to answer
questions in unknown scenario. The evaluation is
based on reject rate (RR), i.e., number of rejected



### Claim 21/36

#### Claim Text
Since the method presented in serves as a foundation for many subsequent refinements, we will specify it here in more detail.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 5):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
5 Advanced Experiments
The studies and experiments outlined in section 4 have shown unsatisfactory performance, marked by a degradation
of at least 20% compared to the peak relative performance. Therefore, in this section, we explore various retrieval
approaches for the RAG, recognizing that the quality of the retriever is a crucial factor in enhancing performance for
this type of problem. We conducted an evaluation covering both sparse and dense search, a hybrid method, and even
a multi-stage architecture using a reranker.
In pursuit of code debugging flexibility and easier customization at each stage, we chose not to utilize an RAG frame-
work (like LangChain or Llama-Index). For a comprehensive guide on debugging RAG and more details about retrieval
systems, refer to [20] and [21].
5.1 Retrievers
When deploying retrieval systems, it is essential to achieve a balance between “effectiveness” ( How good are the
results returned?) and “efficiency” (How much time it takes to return the results? or How much resources are used in
terms of disk/RAM/GPU?). This balance ensures that latency, result quality, and computational budget remain within
our application’s required limits. This work will exclusively focus on effectiveness measures to quantify the retrievers
methods quality.
In our retriever experiments, the evaluation strategy centers around assessing how well the retriever performs in re-
trieving relevant information based on each given query qi. To achieve this, we employ the concept of recall. It is
defined as the fraction of the relevant documents for a given query qi that are successfully retrieved in a ranked list
R [21]. This metric is based on binary relevance judgments, assuming that documents are either relevant or not [21].
In this paper, each chunk is considered a document and only the respective chunkdi is considered relevant to the query
qi. While recall is easy to interpret, it does not consider the specific rank positions in which the relevant chunk appears
in R.
To overcome this limitation, we introduce Reciprocal Rank (RR) into our analysis. In this metric, the rank of the
first relevant document to the query in R is used to compute the RR score [21]. Therefore, Reciprocal Rank offers a
more nuanced evaluation by assigning a higher value when the relevant chunk is returned in the early positions of our
retrievers given the respective query.
Recall and Reciprocal Rank were evaluated at a specific cutoff so the measures are presented as R@k and MRR@k.
For each query, its results are evaluated and their mean serves as an aggregated measure of effectiveness of a given
retriever method. The retrievers are introduced below.
In the category of sparse retrievers, we emphasize the BM25, a technique grounded in statistical weighting to assess
relevance between search terms and documents. BM25 employs a scoring function that takes into account term
frequency and document length, offering an efficient approach for retrieving pertinent information and is typically
used as a strong baseline. However, it is exact-match based and can be powerless when query and document are
relevant to each other but has no common words.
On the other hand, when exploring dense retrievers, we often encounter approaches based on the called bi-encoder
design [22]. The bi-encoder independently encodes queries and documents, creating separate vector representations
before calculating similarity. An advantage of this approach is that it can be initialized ‘offline’: document embeddings
can be precomputed, leaving only the query embedding being calculated at search time, reducing latency.
The hybrid search technique aims to leverage the best of both sparse and dense search approaches. Given a question,
both searches are conducted in parallel, generating two lists of candidate documents to answer it. The challenge then
lies in combining the two results in the best possible way, ensuring that the final hybrid list surpasses the individual
searches. Essentially, we can conceptualize it as a voting system, where each searcher casts a vote on the relevance of
a document to a given query, and in the end, the opinions are combined to produce a better result.
The multi-stage search architecture is based on the retrieve-and-rerank pipeline. In the first stage, a retriever with good
recall is typically used to perform an initial filtering of the documents to be returned. From this narrowed-down list,
these candidate documents are then sent to a second stage, which involves higher computational complexity, to rerank
them and enhance the final effectiveness of the system.
Next, we provide more details about each retriever used.



Source: data\tc17_2312.15883v2\referenced_papers\[93]_2311.09210.pdf (Page 7):

34.3
54.361.464.667.269.2
41.8
56.663.465.970.073.3
36.3
55.863.665.270.173.1
304050607080
100%80%60%40%20%0%
EMScore
NQ
Standard RALMRALM with CoNRALM with CoN (hybrid)
Figure 3: Using a hybrid training strategy demonstrates
slightly lower robustness across various noise ratios but
consistently better performance than standard RALMs.
Models↓ Inference Time(s)
Retrieve-Read 0.6104
+ CHAIN-OF-NOTE 12.0192
+ CHAIN-OF-NOTE (hybrid) 0.6074
Table 5: The inference time comparison shows the aver-
age decoding time per example on 8×A100 GPUs.
questions / total questions. This highlights our
model’s enhanced capability to discern and disre-
gard information that is unfamiliar or not learned
during its initial training phase.
3.5 Evaluation on Hybrid Training Strategy
As illustrated in Figure 3 and Table 5, our proposed
RALM equipped with a hybrid strategy demon-
strates slightly lower robustness across various
noise ratios while but keeping similar effcient de-
coding time consumption to the standard RALM.
This indicates that our CHAIN -OF-NOTE frame-
work, when implemented with a hybrid training
strategy, is highly applicable to a wide range of
real-world business scenarios. This enhancement
in robustness without significant time overhead
highlights the practical value and efficiency of our
approach, making it a viable solution for environ-
ments where QA accuracy can vary but inference
time is crucial.
4 Related Work
Retrieval-Augmented Language Models (RALMs)
represent a significant advancement in natural lan-
guage processing, combining the power of large
language models with the specificity and detail pro-
vided by external knowledge sources (Guu et al.,
2020; Lewis et al., 2020; Izacard et al., 2022). Re-
cent studies highlight the impact of context rele-
vance on language model performance (Creswell
et al., 2022; Shi et al., 2023a; Yoran et al., 2023).
Notably, Creswell et al. (2022) demonstrated that
incorporating random or irrelevant contexts could
adversely affect QA performance. In contrast, Shi
et al. (2023a) discovered that adding irrelevant con-
text to exemplars or task-specific instructions can
sometimes enhance model performance, implying
that models might intrinsically possess capabilities,
developed during pre-training, to manage such sce-
narios. Most pertinent to our research is the study
by Yoran et al. (2023), which focused on training
RALMs to disregard irrelevant contexts. This ap-
proach, while distinct from our proposed solution,
underscores the importance of context relevance in
enhancing the effectiveness of RALMs.
Besides, we present more related Chain-of- Xs
approaches (e.g., Chain-of-Thought (CoT) (Wei
et al., 2022)) in the Appendix A.1.1 and A.2.
5 Conclusion
In this paper, we introduce the CHAIN -OF-NOTE
(CON) framework, a novel methodology designed
to enhance the robustness of RALMs. The central
concept of CON revolves around the generation of
sequential reading notes for each retrieved docu-
ment. This process allows for an in-depth assess-
ment of document relevance to the posed question
and aids in synthesizing this information to craft
the final answer. Our experiments show that GPT-4,
when equipped with CON, outperforms the CHAIN -
OF-THOUGHT approach. Besides, we utilized GPT-
4 to create 10K CON data, subsequently trained on
a LLaMa-2 7B model. Our experiments across four
open-domain QA benchmarks show that RALMs
equipped with CON significantly outperform stan-
dard fine-tuned RALMs.
6 Limitations
One major limitation of the CHAIN -OF-NOTE
(CON) approach is its increased inference cost
due to the sequential generation of notes. While
CON is beneficial for assessing the relevance and
integrating external knowledge, it results in longer
response times, which is problematic for time-
sensitive applications. Moreover, the system’s effi-
ciency depends on the conciseness and relevance
of the generated notes, which can fluctuate based
on the complexity of the retrieved documents.



Source: data\tc17_2312.15883v2\referenced_papers\[93]_2311.09210.pdf (Page 3):

the pertinence of each document to the query but
also pinpoints the most reliable information and
resolves conflicting information. This approach
effectively filters out irrelevant or less trustworthy
content, leading to responses that are both more
accurate and contextually relevant.
Given an input question x and k retrieved doc-
uments [d1, · ··, dk], the model aims to gener-
ate textual outputs comprising multiple segments
[yd1 , · ··, ydk , y]. Here, ydi signifies the tokens for
the i-th segment, representing the reading note for
the corresponding document di, as shown in Fig-
ure 2. After generating individual reading notes,
the model synthesizes the information to create a
consolidated final response y. The implementation
of the CHAIN -OF-NOTE (CON) involves three key
steps: (1) designing the notes ydi , (2) collecting the
data, and (3) training the model.
2.3.1 C HAIN -OF-NOTE Format Design
The framework primarily constructs three types of
reading notes, as shown in Figure 2 , based on the
relevance of the retrieved documents to the input
question: First, when a document directly answers
the query, the model formulates the final response
based on this relevant information, as shown in Fig-
ure 2(a). Second, if the retrieved document does
not directly answer the query but provides useful
context, the model leverages this information along
with its inherent knowledge to deduce an answer,
as shown in Figure 2(b). Third, in cases where the
retrieved documents are irrelevant, and the model
lacks sufficient knowledge to answer, it defaults
to responding with “unknown", as shown in Fig-
ure 2(c). This nuanced approach mirrors human
information processing, striking a balance between
direct retrieval, inferential reasoning, and the ac-
knowledgment of knowledge gaps.
2.3.2 Data Collection and Model Training
To equip the model with the ability to generate such
reading notes, it’s essential to gather appropriate
training data. Manual annotation for each reading
note is resource-intensive, so we employ a state-
of-the-art language model – GPT-4 – to generate
the notes data. This method is both cost-effective
and enhances reproducibility. We initiate this pro-
cess by randomly sampling 10k questions from
the NQ (Kwiatkowski et al., 2019) training dataset.
GPT-4 is then prompted with specific instructions
and in-context examples to the three distinct types
of note generation (detailed in Appendix A.5). The
quality of GPT-4’s predictions is subsequently as-
sessed through human evaluations on a small subset
of the data before proceeding to the entire set. The
NQ dataset is chosen as our primary dataset due to
its diverse range of real user queries from search
engines. However, to ensure the model’s adaptabil-
ity, we also test its performance on three additional
open-domain datasets, including TriviaQA, WebQ,
and RealTimeQA, showing its generalization capa-
bilities to out-of-domain (OOD) data.
After collecting 10K training data from GPT-4,
the next step involves using them to train a LLaMa-
2 7B model (Touvron et al., 2023), to validate the
feasibility of generating CHAIN -OF-NOTE (CON)
outputs. To do this, we concatenate the instruc-
tion, question and documents as a prompt and train
the model to generate notes and answer in a stan-
dard supervised way. Our in-house model learns to
sequentially generate reading notes for each doc-
ument to assess their relevance to the input query.
Responses are generated based on the document’s
relevance, enhancing accuracy and reducing mis-
information. If all documents are irrelevant, the
model either relies on inherent knowledge for an
answer or responds with “unknown” if the answer
cannot be determined accurately.
2.3.3 Hybrid Training for Better Efficiency
Generating CHAIN -OF-NOTE (CON) would in-
crease inference cost, potentially hindering real-
world usage. To address this, we experimented
with a simple yet effective strategy for internaliz-
ing CON reasoning, called Hybrid Training.
Specifically, we allocate 50% of the training time
to the standard RALM, which involves directly
generating answers without notes, and the other
50% to RALM with CoN. This strategy allows the
model to internalize intermediate reasoning steps
during training. Additionally, we add two different
prompt words before each category of data.
During the inference phase, we exclusively use
the standard RALM prompt to guide the model,
prompting it to output answers without relying on
explicit reading notes. This approach leverages the
hidden states developed during training for implicit
CON reasoning. The model trained with the hybrid
training strategy maintains the same inference time
while achieving only slightly lower performance
wit CoN. The results will be introduced in §3.5.



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 4):

Published as a conference paper at ICLR 2024
fetch the results, which could be a singular value or a subset of the original table. The outcomes
from both SPARQL and SQL are then formatted into markdown text. For natural sentence queries,
knowledge is retrieved from domain-specific knowledge sources either through sentence similarity
matching or by utilizing a search engine. 2
Rationale Correction Existing methods such as ReAct (Yao et al., 2023) and Verify-and-Edit
(Zhao et al., 2023c) keep all retrieved information in the context throughout the process, no matter
if it contains reasoning mistakes. This often leads to error propagation and misguides further gener-
ations. To overcome this weakness, CoK involves a progressive rationale correction step. Given the
current rationale and the formatted knowledge from various knowledge sources, a corrected ratio-
nale is generated to replace the current one. This step helps in rectifying any factual incorrectness
and preventing error propagation.
Next Rationale Generation Using the question and preceding corrected rationales, the next ratio-
nale is generated, and the process is reiterated for the new rationale until a final answer is produced.
2.3 A NSWER CONSOLIDATION STAGE
Ultimately, the LLM is prompted with the question and corrected rationales to generate a consol-
idated answer, which is expected leading to a more accurate answer. This hypothesis is further
examined through a series of experiments, as detailed in Section 4.
3 T HE ADAPTIVE QUERY GENERATOR
CoK incorporates heterogeneous knowledge sources from four different domains, including factual,
medical, physics, and biology. Each of these knowledge sources necessitates the use of a unique
query language for retrieval, which could be either structured or unstructured. Therefore, we design
the adaptive query generator (AQG) to facilitate query generation for different knowledge sources.
Unstructured Query Languages Natural language sentences are the most natural way that hu-
man beings search for information. AQG utilizes two distinct approaches for generating unstruc-
tured queries based on the knowledge sources. A) For general factual knowledge sources, such as
Wikipedia, ChatGPT is utilized. B) For domain-specific knowledge sources ( e.g., Flashcard, Sci-
enceQA Physics, and ScienceQA Biology), using ChatGPT may lead to hallucination as it may not
have comprehensive knowledge of the specific domains. Therefore, we instruction-tune LLaMA-
2-7B using LoRA with pairs of input texts and output queries. Furthermore, the domain of the
training data is on par with the respective knowledge source. Consequently, the AQG is equipped
with the requisite knowledge for generating queries with greater precision.
Structured Query Languages Querying unstructured knowledge sources often leads to the re-
trieval of irrelevant and redundant information. On the other hand, structured knowledge sources
(e.g., Wikidata and tables) provide direct factual results. To generate structured queries, AQG uti-
lizes two approaches based on the query languages. A) When generating commonly used query
languages like SQL, we employ ChatGPT. It is empirically inferred that ChatGPT included SQL
during its pre-training, providing it with advantages in generating SQL queries (OpenAI, 2023).
All pertinent details are incorporated into the prompt to enhance the precision of query generation.
For instance, when generating SQL queries, we include both the table schema and data snippets.
B) For less common languages like SPARQL, we instruction-tune LLaMA-2-7B using LoRA with
sentence-SPARQL pairs. The training data is collected to match the logical granularity of the ratio-
nales, thereby facilitating more accurate query generation. For example in SPARQL, both training
data and rationales contain single entity and relation within each sentence. Inspired by chain-of-
hindsight (Liu et al., 2023), besides giving the correct queries, we also append negative examples
such as “incorrect queries:..” during instruction-tuning.
Detailed query language, model, and training datasets of each knowledge source are in Table 8 of
Appendix. The constructions of instruction-tuning datasets and training details are in Appendix D.
We also evaluate the performances of AQG in Appendix F.2.
2Details of the execution process for each knowledge source is in Appendix C.
5



Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 8):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
set D and search results from different methods r in R, for each d in D, we can calculate the RRFscore(d ∈ D) as
follows [27]:
RRFscore(d ∈ D) =
X
r∈R
1
k + r(d), (2)
Considering that 1/r(d) is known as the reciprocal rank, where r(d) represents the position at which the document d
was retrieved by the search mechanism r. The term k is introduced to assist in controlling outlier systems [27].
Figure 8: Hybrid Search schema with k=1.
Figure 8 shows how to calculate RRFscore for k = 1. In the example, we have four chunks that were retrieved in
different orders by two search methods, BM25 (sparse search), and Custom ADA-002 (dense search). The reciprocal
rank score is calculated for each chunk. These values are then summed, creating a new score. The final hybrid list is
an ordering of chunks that uses this new score.
Table 5: Retriever comparison. Where MRR is the Mean Recriprocal Rank metric and R@k is the Recall.
Metric Hybrid-BM25-ADA-002 Hybrid-BM25-Custom ADA-002
MRR@10 0.758 0.850
R@3 0.829 0.921
R@5 0.879 0.943
R@7 0.921 0.964
R@9 0.957 0.979
In our experiments, only Pyserini’s BM25 was tested as the sparse retriever, while ADA-002 and Custom ADA-002
were tested as dense retrievers. The hybrid combination that yielded the best results was the one that used BM25 and
Custom ADA-002.
5.1.5 Reranker
The fundamental idea underlying multi-stage ranking is to divide document ranking into a sequence of stages. After
an initial retrieval, which usually involves a sparse retriever or dense retriever, each subsequent stage re-evaluates and
reranks the set of candidates forwarded from the preceding stage. Figure 9 represents a multi-stage pipeline where
Pyserini BM25 performs the first stage and the candidate chunks are then re-evaluated by the reranker. After that, the
final reranked list called retrieved chunks is presented as final result formed by k chunks.
Figure 9: Reranker Pipeline



### Claim 22/36

#### Claim Text
This is because the number of interactions that occur between any two individuals in a social scenario is often greater than the number of interactions they have in the household scenario .

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 12):

13  
Previous studies have utilized KG in conjunction with LLM for knowledge intensive tasks such as question-answering,(Yasunaga et al. 2021) multi-hop relational reasoning,(Feng et al. 2020) commonsense reasoning,(Lin et al. 2019; Lv et al. 2019) and model pre-training.(Moiseev et al. 2022; Yasunaga et al. 2022) Furthermore, enhancing prompts by incorporating structured knowledge has been described and studied.(Xiang Chen Zhejiang University, China et al.; Lewis et al. 2020; Pan et al. 2023) Naturally, these approaches have bolstered the positive reinforcement between KG and LLM. Nevertheless, it's worth noting that these approaches are only be task-specific and in some cases, the knowledge infusion could grow exponentially by the inclusion of higher order relations.(Bauer, Wang and Bansal 2018; Lin et al. 2019) Such approaches could compromise the limited token space of the LLM. Alternative methods employed knowledge infusion through the direct use of query languages like SPARQL.(Brate et al. 2022) However, this could render the system constraint to the schema of the underlying KG, potentially affecting the flexibility and adaptability of prompts. Moreover, as the KG expands and its schema grows, it could potentially occupy a significant portion of the LLM input token space. This explains why we noticed a greater token usage with the Cypher-RAG method (average usage of 8006 tokens), as it incorporates the entire graph schema into the input prompt for converting natural language into structured Cypher queries. In contrast, KG-RAG requires minimal graph schema, thus eliminating the need to include it in the prompt and resulting in substantial token savings, with a reduction of over 50% in token utilization compared to Cypher-RAG. This finding suggests that when dealing with a graph as large as SPOKE, which contains over 40 million nodes, Cypher-RAG requires LLMs that enable a larger context window. In contrast, KG-RAG is capable of managing this with LLMs that require a relatively smaller window size.



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 6):

Published as a conference paper at ICLR 2024
Table 3: Results of retrieval-based methods on
FEVER and HotpotQA. ReAct results are adapted
from Yao et al. (2023).
FEVER (3-shot) HotpotQA (6-shot)
Method Acc. ∆Acc. E.M. ∆E.M.
CoT-SC→ReAct 64.6% +4.2% 34.2% +0.8%
ReAct→CoT-SC 62.0% +1.6% 35.1% +1.7%
CoT-SC 59.9% - 33.4% -
Verify-and-Edit 60.6% +0.7% 34.4% +1.0%
CoK (ours) 63.4% +3.5% 35.4% +2.0%
Table 4: Results of using single or multiple
knowledge domains and sources on MedM-
CQA (3-shot).
Knowl.
Method Domains Knowl. Sources Acc.
CoT - - 59.6%
CoK Medical Flashcard 67.1%
CoK Medical Flashcard, UpToDate 69.2%
CoK Medical,
Biology
Flashcard, UpToDate,
ScienceQA, CK-1270.5%
6-shot is prominent on HotpotQA and FEVER, registering at 2.6% and 4.3% respectively. This
suggests that CoK is not only effective on multi-step reasoning datasets (HotpotQA), but benefits
less single-hop datasets (FEVER) as well with its accurate retrieval abilities. On domain-specific
datasets, such as MedMCQA, and MMLU Physics and Biology, CoK achieves an average accuracy
improvement of 4.9% over the CoT baseline on 3-shot and 6-shot settings. We notice that CoT
has worse performances than standard prompting on FetaQA, MedMCQA, and MMLU Physics.
This illustrates that, while CoT is effective for addressing complex reasoning tasks, it struggles
with hallucination in its rationales when handling knowledge-intensive tasks, leading to incorrect
answers. This outcome aligns with the findings of Yao et al. (2023) and Zhao et al. (2023c) as well.
With dynamic knowledge adapting, CoK can effectively reduce hallucination in the rationales and
we include analysis on the factual accuracy in Section 5.3.
CoK vs. Other Retrieval-based Methods As shown in Table 2, CoK consistently outperforms
state-of-the-art retrieval-based method Verify-and-Edit (VE) (Zhao et al., 2023c). For FEVER and
HotpotQA, we additionally compare with the results reported in ReAct (Yao et al., 2023) in Table 3.
Since the results in ReAct are reported on the PaLM model (Chowdhery et al., 2022), to add a more
justified perspective, we report the performance improvement gained on top of the CoT-SC baseline.
Compared with ReAct, CoK demonstrates a more substantial improvement over CoT-SC, especially
on HotpotQA. More specifically, for HotpotQA, CoK exhibits improvements of 2.0% compared to
0.8% by ReAct. On FEVER, CoK shows a 3.5% improvement, which is on par with the 4.2%
improvement gained by ReAct. This is attributed to the fact that FEVER is less multi-hop compared
to HotpotQA, thus benefitting less from an improved CoT. VE conducts knowledge retrieval and
editing for all rationales in parallel, and ReAct leaves past errors in the prompt, potentially leading
to error propagation. CoK alleviates this issue with progressive knowledge adapting. It is also worth
noting that CoK costs much less than ReAct, shown with a detailed cost analysis in Appendix I
Fever HotpotQA FeT aQAMedMCQA Physics Biology
FactualMedicalPhysicsBiology
0.0
0.2
0.4
0.6
0.8
1.0
Figure 3: A heatmap on distributions
of identified domains of each dataset.
Effect of Number of Demonstrations As shown in Table
2, CoK consistently exhibits enhanced performance across
multiple datasets under both 3-shot and 6-shot settings. Sev-
eral studies show that increasing the number of demonstra-
tions (shots) in the prompt can potentially lead to better per-
formances on reasoning tasks (Wei et al., 2022). However,
this is not universally true for knowledge-intensive tasks. For
example, as shown in Table 2, the performance of CoT on
MMLU Biology with six shots (81.7%) is nearly identical to
that with three shots (81.5%). This occurs because the bottle-
neck for LLMs in answering knowledge-intensive questions
accurately is their insufficient knowledge, not their reasoning
capability. The performance on FEVER for all reasoning-
based methods decrease with six shots. This is likely due
to the fact that FEVER questions are single-hop and require
less reasoning. Thus, increased guidance on reasoning could
lead to potential noise. This finding is consistent with ReAct (Yao et al., 2023), where the authors
state that increasing beyond 3-shot for FEVER does not lead to better performance.
7



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 7):

Published as a conference paper at ICLR 2024
Table 5: Parallel vs. dynamic
knowledge adapting.
Method HotpotQA (3-shot)
CoT 29.9%Verify-and-Edit 31.8%CoK (parallel) 31.2%CoK (dynamic) 34.1%
Table 6: Comparison of the
factual accuracy of rationales
on HotpotQA.
Method Rationale 1 Rationale 2
CoT-SC 54.3% 52.1%CoK 66.3% 69.5%
Table 7: Human study results
on the factuality of reasoning
chains.
Predictions CoK CoT-SC Tie
Correct predictions68% 4% 28%Incorrect predictions44% 24% 32%All predictions56% 14% 30%
5 A NALYSIS
5.1 S INGLE VS . M ULTIPLE KNOWLEDGE DOMAINS AND SOURCES
As outlined in Section 2.1, CoK integrates a step to select the appropriate knowledge domains for
each question. This step is crucial to ensure that CoK can retrieve the most pertinent knowledge to
correct the rationales and answer the questions accurately. It is possible that multiple knowledge
domains can be chosen for one question, and within each domain, there are several knowledge
sources. In this subsection, we investigate the necessity of utilizing multiple knowledge domains
and sources. We also include an evaluation of the domain selection performance in Appendix F.1.
Single vs. Multiple Knowledge Domains We show the domain distributions identified for each
dataset in Figure 3. Notably, we find that CoK predominantly selects one knowledge domain for
each dataset, while a small number of cases call for multiple domains. For instance, the primary
knowledge domain for MedMCQA is Medical, and 17.8% of the questions identify Biology as a rel-
evant domain as well. Furthermore, we conduct ablation experiments to demonstrate the necessity
of utilizing multiple domains. As shown in Table 4, compared to only using Medical domain knowl-
edge, CoK using additional knowledge from the Biology domain further improves the performance
by 1.3%. This indicates that knowledge spanning multiple domains is needed for answering some
questions, underscoring the necessity of incorporating various knowledge domains.
Single vs. Multiple Knowledge Sources Within one domain, numerous credible knowledge
sources exist, and it is unfeasible for a single source to encompass all knowledge from the domain.
Therefore, it is important to utilize multiple knowledge sources within one domain. For instance,
as shown in Table 4, the performance of CoK improves by 2.1% when utilizing both Flashcard and
UpToDate as medical knowledge sources, compared to using only Flashcard. 7
5.2 P ARALLEL VS . DYNAMIC KNOWLEDGE ADAPTING
As aforementioned, dynamic knowledge adapting helps CoK prevent error propagation, here we take
a closer look at how much improvement it brings in. As shown in Table 5, the performance of CoK
improves by 4.2% compared with CoT when dynamic knowledge adapting is applied. However,
parallel editing leads to poorer performance due to error propagation for rationales.
5.3 E VALUATING FACTUALITY IMPROVEMENT OF THE RATIONALES
While the main results have shown that CoK effectively improves the performance of LLMs in
knowledge-intensive tasks, we are also interested in reducing hallucination for the generated ratio-
nales. Hence, we conduct quantitative and qualitative evaluations to assess the factual accuracy.
Quantitative Evaluation To automatically evaluate how CoK can reduce hallucination in the
model outputs, we employ an existing fact-checking method to compare the original and corrected
rationales. Specifically, we use ProgramFC (Pan et al., 2023) which is a state-of-the-art method for
judging the factuality of claims with respect to Wikipedia. As shown in Table 6, we observe that
CoK has improved factual accuracy compared to the CoT-SC baseline on the HotpotQA dataset. No-
tably, the factual accuracy of CoT-SC decreases for rationale 2 compared to rationale 1, which could
7Note that using external sources may have limitations such as noise or conflicts between different sources.
We mainly address this by selecting authoritative knowledge sources, and discuss this further in Appendix G
8



Source: data\tc17_2312.15883v2\referenced_papers\[47]_2307.03172.pdf (Page 16):

F Token Counts
Table 2, Table 3, and Table 4 present the average and maximum number of tokens in each of the input
contexts for all experimental settings. Note that MPT-30B and MPT-30B-Instruct use the same tokenizer,
GPT-3.5-Turbo and GPT-3.5-Turbo (16K) use the same tokenizer, and Claude-1.3 and Claude-1.3 (100K)
use the same tokenizer. Furthermore, the Claude-1.3 tokenizer is the same as the GPT-3.5-Turbo tokenizer,
modulo some additional special tokens that do not appear in our data. As a result, the token counts for
these two model families is the same in our experimental settings.
Closed-Book Oracle
avg ± stdev max avg ± stdev max
LongChat-13B (16K) 55.6 ± 2.7 70 219.7 ± 48.5 588
MPT-30B 43.5 ± 2.2 58 187.9 ± 41.8 482
GPT-3.5-Turbo 15.3 ± 2.2 29 156.0 ± 41.8 449
Claude-1.3 15.3 ± 2.2 29 156.0 ± 41.8 449
Table 2: Token count statistics for each of the evaluated models on the closed-book and oracle multi-document
question answering settings.
10 docs 20 docs 30 docs
avg ± stdev max avg ± stdev max avg ± stdev max
LongChat-13B (16K) 1749.9 ± 112.4 2511 3464.6 ± 202.3 4955 5181.9 ± 294.7 7729
MPT-30B 1499.7 ± 88.5 1907 2962.4 ± 158.4 3730 4426.9 ± 230.5 5475
GPT-3.5-Turbo 1475.6 ± 86.5 1960 2946.2 ± 155.1 3920 4419.2 ± 226.5 6101
Claude-1.3 1475.6 ± 86.5 1960 2946.2 ± 155.1 3920 4419.2 ± 226.5 6101
Table 3: Token count statistics for each of the evaluated models on each of the document question answering
settings.
75 KV pairs 140 KV pairs 300 KV pairs
avg ± stdev max avg ± stdev max avg ± stdev max
LongChat-13B (16K) 5444.5 ± 19.1 5500 10072.4 ± 24.1 10139 21467.3 ± 35.9 21582
MPT-30B 4110.5 ± 23.8 4187 7600.9 ± 31.1 7687 16192.4 ± 46.6 16319
GPT-3.5-Turbo 3768.7 ± 25.6 3844 6992.8 ± 34.1 7088 14929.4 ± 50.7 15048
Claude-1.3 3768.7 ± 25.6 3844 6992.8 ± 34.1 7088 14929.4 ± 50.7 15048
Table 4: Token count statistics for each of the evaluated models on each of the key-value (KV) retrieval settings.



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 21):

Published as a conference paper at ICLR 2024
Table 13: Details on costs (by tokens).
Method Dataset tokens
ReAct HotpotQA 1638
Verify-and-Edit HotpotQA 630
CoK HotpotQA 787
ReAct FEVER 848
Verify-and-Edit FEVER 286
CoK FEVER 329
Q: Practical Magic is based on a French novel that was written by Alice Hoffman.
A: First, Practical Magic is a 1998 American fantasy romantic drama film based on the 1995 novel
of the same name by Alice Hoffman. Second, Alice Hoffman is an American author. The answer is
ChatGPT: SUPPORTS.
Example 3:
Prompt: [3-shot CoT prompt]
Q: Saturn Corporation has no other names.
A: First, The Saturn Corporation, also known as Saturn LLC, was a registered trademark established
on January 7, 1985, as a subsidiary of General Motors. Second, There is no information available
on any other names for Saturn Corporation, but it is also known as Saturn LLC. The answer is
ChatGPT: SUPPORTS.
In the first example, it is mentioned twice in the prompt that Anne Sullivan was born in April.
However, the LLM still supports the claim that she was born in June. In the second example, the
CoT specifies that the novel is American. However, ChatGPT overlooks the nationality and supports
the claim that it is based on a French novel. In the third example, the CoT mentions repetitively that
Saturn Corporation is also known as Saturn LLC. However, ChatGPT supports the claim that it has
no other names.
These examples show that, even though the CoT is successfully improved in terms of factual con-
sistency, the final answer may still be incorrect due to reasoning errors inherent to LLM itself. In
the human study for wrong predictions, 44% of the time humans claim that CoK still generates im-
proved CoTs. Among these 44% instances, 73% of the time humans think these CoTs should have
led to better answers.
I C OST ANALYSIS
As CoK always edits instances below a certain consistency threshold, there is a cost advantage
compared to other methods such as ReAct. The costs are on par with methods such as Verify-and-
Edit.
A table of the costs is shown in 13. The costs are calculated based on tokens used per instance.
Overall, the costs for CoK are on par with Verify-and-Edit. The extra costs are incurred by the
dynamic knowledge editing stage, which is shown to boost performance in the main results. CoK
also costs much less than ReAct, incurring only around 40% of ReAct’s costs. Specifically, it costs
787 compared to 1638 for HotpotQA, and 329 compared to 848 for FEVER.
The API cost for gpt-3.5-turbo is currently $0.0015 / 1K tokens for input, and $0.002 / 1K
tokens for output.
For details of the cost calculations, as the output length is the same for all methods, we only calculate
the input tokens. Following the original ReAct paper(Yao et al., 2023), we calculate based on 3-shot
prompts for FEVER and 6-shot prompts for HotpotQA. Verify-and-Edit and CoK tokens per instance
are calculated based on the CoT-SC threshold, which results in editing 86 out of 308 instances for
22



### Claim 23/36

#### Claim Text
Other authors have resorted to a description of the interface properties based on a (Cahn-Hilliard) phase-field model, coupled to immersed-boundary-type discretizations .

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 16):

Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson,
Itziar Gonzalez-Dios, Javier de la Rosa, Jenny
Chim, Jesse Dodge, Jian Zhu, Jonathan Chang,
Jorg Frohberg, Josephine L. Tobing, Joydeep
Bhattacharjee, Khalid Almubarak, Kimbo Chen,
Kyle Lo, Leandro von Werra, Leon Weber, Long
Phan, Loubna Ben Allal, Ludovic Tanguy, Manan
Dey, Manuel Romero Muñoz, Maraim Masoud,
Mar’ia Grandury, Mario vSavsko, Max Huang,
Maximin Coavoux, Mayank Singh, Mike Tian-
Jian Jiang, Minh Chien Vu, Mohammad Ali
Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora
Kassner, Nurulaqilla Khamis, Olivier Nguyen,
Omar Espejel, Ona de Gibert, Paulo Villegas,
Peter Henderson, Pierre Colombo, Priscilla A.
Amuok, Quentin Lhoest, Rheza Harliman, Rishi
Bommasani, Roberto L’opez, Rui Ribeiro, Salomey
Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose,
Shamsuddeen Hassan Muhammad, Shanya Sharma,
S. Longpre, Somaieh Nikpoor, Stanislav Silberberg,
Suhas Pai, Sydney Zink, Tiago Timponi Torrent,
Timo Schick, Tristan Thrush, Valentin Danchev,
Vassilina Nikoulina, Veronika Laippala, Violette
Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak
Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si,
Elizabeth Salesky, Sabrina J. Mielke, Wilson Y . Lee,
Abheesht Sharma, Andrea Santilli, Antoine Chaffin,
Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla,
Gunjan Chhablani, Han Wang, Harshit Pandey,
Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo
Gao, Lintang Sutawika, M Saiful Bari, Maged S.
Al-shaibani, Matteo Manica, Nihal V . Nayak, Ryan
Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-
David, Stephen H. Bach, Taewoon Kim, Tali Bers,
Thibault Févry, Trishala Neeraj, Urmish Thakker,
Vikas Raunak, Xiang Tang, Zheng Xin Yong, Zhiqing
Sun, Shaked Brody, Y Uri, Hadar Tojarieh, Adam
Roberts, Hyung Won Chung, Jaesung Tae, Jason
Phang, Ofir Press, Conglong Li, Deepak Narayanan,
Hatim Bourfoune, Jared Casper, Jeff Rasley,
Max Ryabinin, Mayank Mishra, Minjia Zhang,
Mohammad Shoeybi, Myriam Peyrounette, Nicolas
Patry, Nouamane Tazi, Omar Sanseviero, Patrick von
Platen, Pierre Cornette, Pierre Franccois Lavall’ee,
Rémi Lacroix, Samyam Rajbhandari, Sanchit
Gandhi, Shaden Smith, Stéphane Requena, Suraj
Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet
Singh, Anastasia Cheveleva, Anne-Laure Ligozat,
Arjun Subramonian, Aur’elie N’ev’eol, Charles
Lovering, Daniel H Garrette, Deepak R. Tunuguntla,
Ehud Reiter, Ekaterina Taktasheva, Ekaterina
V oloshina, Eli Bogdanov, Genta Indra Winata,
Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina
Novikova, Jessica Zosa Forde, Xiangru Tang, Jungo
Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat,
Miruna Clinciu, Najoung Kim, Newton Cheng,
Oleg Serikov, Omer Antverg, Oskar van der Wal,
Rui Zhang, Ruochen Zhang, Sebastian Gehrmann,
Shachar Mirkin, S. Osher Pais, Tatiana Shavrina,
Thomas Scialom, Tian Yun, Tomasz Limisiewicz,
Verena Rieser, Vitaly Protasov, Vladislav Mikhailov,
Yada Pruksachatkun, Yonatan Belinkov, Zachary
Bamberger, Zdenvek Kasner, Alice Rueda, Amanda
Pestana, Amir Feizpour, Ammar Khan, Amy
Faranak, Ananda Santa Rosa Santos, Anthony
Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo
Abdollahi, Aycha Tammour, Azadeh HajiHosseini,
Bahareh Behroozi, Benjamin Olusola Ajibade,
Bharat Kumar Saxena, Carlos Muñoz Ferrandis,
Danish Contractor, David M. Lansky, Davis David,
Douwe Kiela, Duong Anh Nguyen, Edward Tan,
Emily Baylor, Ezinwanne Ozoani, Fatim T Mirza,
Frankline Ononiwu, Habib Rezanejad, H.A. Jones,
Indrani Bhattacharya, Irene Solaiman, Irina Sedenko,
Isar Nejadgholi, Jan Passmore, Joshua Seltzer,
Julio Bonis Sanz, Karen Fort, Lívia Macedo
Dutra, Mairon Samagaio, Maraim Elbadri, Margot
Mieskes, Marissa Gerchick, Martha Akinlolu,
Michael McKenna, Mike Qiu, M. K. K. Ghauri,
Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour
Elkott, Nourhan Fahmy, Olanrewaju Samuel, Ran
An, R. P. Kromann, Ryan Hao, Samira Alizadeh,
Sarmad Shubber, Silas L. Wang, Sourav Roy, Sylvain
Viguier, Thanh-Cong Le, Tobi Oyebade, Trieu
Nguyen Hai Le, Yoyo Yang, Zachary Kyle Nguyen,
Abhinav Ramesh Kashyap, A. Palasciano, Alison
Callahan, Anima Shukla, Antonio Miranda-Escalada,
Ayush Kumar Singh, Benjamin Beilharz, Bo Wang,
Caio Matheus Fonseca de Brito, Chenxi Zhou, Chirag
Jain, Chuxin Xu, Clémentine Fourrier, Daniel Le’on
Perin’an, Daniel Molano, Dian Yu, Enrique
Manjavacas, Fabio Barth, Florian Fuhrimann,
Gabriel Altay, Giyaseddin Bayrak, Gully Burns,
Helena U. Vrabec, Iman I.B. Bello, Isha Dash, Ji Soo
Kang, John Giorgi, Jonas Golde, Jose David Posada,
Karthi Sivaraman, Lokesh Bulchandani, Lu Liu,
Luisa Shinzato, Madeleine Hahn de Bykhovetz,
Maiko Takeuchi, Marc Pàmies, María Andrea
Castillo, Marianna Nezhurina, Mario Sanger,
Matthias Samwald, Michael Cullan, Michael
Weinberg, M Wolf, Mina Mihaljcic, Minna
Liu, Moritz Freidank, Myungsun Kang, Natasha
Seelam, Nathan Dahlberg, Nicholas Michio Broad,
Nikolaus Muellner, Pascale Fung, Patricia Haller,
R. Chandrasekhar, R. Eisenberg, Robert Martin,
Rodrigo L. Canalli, Rosaline Su, Ruisi Su, Samuel
Cahyawijaya, Samuele Garda, Shlok S Deshmukh,
Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee
Sang-aroonsiri, Srishti Kumar, Stefan Schweter,
Sushil Pratap Bharati, T. A. Laud, Th’eo Gigant,
Tomoya Kainuma, Wojciech Kusa, Yanis Labrak,
Yashasvi Bajaj, Y . Venkatraman, Yifan Xu, Ying
Xu, Yun chao Xu, Zhee Xao Tan, Zhongli Xie,
Zifan Ye, Mathilde Bras, Younes Belkada, and
Thomas Wolf. 2022. Bloom: A 176b-parameter
open-access multilingual language model. ArXiv,
abs/2211.05100.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proximal
policy optimization algorithms. arXiv preprint
arXiv:1707.06347.
Noam Shazeer. 2020. Glu variants improve transformer.
arXiv preprint arXiv:2002.05202.
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,



Source: data\tc17_2312.15883v2\referenced_papers\[86]_2312.14335.pdf (Page 1):

tially produce more faithful summarization. A few works [Sh i+23; PCM22] make use of pointwise mutual
information to let the LM focus more on the input context to re duce hallucination.
In this work, we focus on Context-aware Decoding [Shi+23] (C AD). CAD leverages the idea of point-
wise mutual information and proposes a product-of-experts enhancement to the vanilla decoding method
to make the generation more conditioned on the input evidenc e. W e conduct a large-scale reproducibility
study with different choices of language models to study CAD ’s effectiveness in query-focused summariza-
tion, and include more rigorous analysis on computational c omplexity and hyperparameter sensitivity . Ex-
tensive experiments show that CAD improves QFS quality by (1 ) reducing factual mistakes/hallucinations
while (2) mostly retaining the match of lexical patterns, me asured by ROUGE scores. On the other hand,
CAD also introduces additional inference-time FLOPs and po tentially slows down decoding speed.
2 Background
Context-aware Decoding.Consider a language model parameterized by θ , a context c, an input query x,
and we want to generate a piece of text y. In QFS paradigm, c can be the source document, x and y will
be the query and summary , respectively . In the vanilla decod ing scenario, each token of generation y is
sampled autoregressively from the output probability dist ribution from language model θ conditioned on
input query x and context c:
yt ∼ pθ (yt |c, x, y<t ) ∝ Softmax
(
logitθ (yt |c, x, y<t )/τ
)
(1)
Here τ is a temperature hyperparameter to control the sharpness of probability distribution over the candi-
date vocabulary V. Examples of the sampling strategy include greedy and T op- p sampling [Hol+19].
As we are operating on a conditional generation problem, we h ope the generation is more conditioned
on input context c. [Shi+23] utilize the idea of pointwise mutual information (PMI). W e consider two events
c and yt :
PMI(pθ (yt ; c, x, y<t )) =log
( pθ (yt |c, x, y<t )
pθ (yt |x, y<t )
)
(2)
The interpretation of the above equation is we would like to m easure the association of event yt (predicting
a speciﬁc token) and event c (the presence of context c). pθ (yt |x, y<t ) is the prior probability , representing
the model’s prior belief from its parameters θ when no presence of context c, whereas pθ (yt |c, x, y<t ) is the
posterior probability with c. T o leverage PMI, Shi et al. [Shi+23] propose to multiply PMI with the original
probability distribution:
yt ∼ ˜pθ (yt |c, x, y<t ) ∝ pθ (yt |c, x, y<t )
( pθ (yt |c, x, y<t )
pθ (yt |x, y<t )
) α
(3)
Here α is a hyperparameter to control the PMI weighting. W e still ne ed to normalize the above equation to
acquire a valid probability distribution, rearranging the terms yields
yt ∼ Softmax
[(
(1 + α ) logitθ (yt |c, x, y<t ) − α logitθ (yt |x, y<t )
)
/τ
]
(4)
Here larger α yields more weight on the context c and α = 0 reduces to vanilla decoding. [Shi+23] recom-
mend setting α = 0.5 for summarization tasks and we also examine this choice i n Section 3.
Analysis of Computational Cost. W e follow the notations used in [Kap+20]. Let us consider the T rans-
former architecture with following hyperparameters: numb er of layers nlayer, dimension of the embedding
layer and intermediate representation dmodel, dimension of the intermediate feedforward layer dff, dimen-
sion of the attention output dattn and number of attention heads nheads ; and denote the number of tokens
in the input sequence as ninput . The model size N is the number of non-embedding parameters of the model,
N ≈ | θ |:
N ≈ 2 dmodel nlayer (2 dattn + dff ) =12 nlayer d2
model with dattn = dff/4 = dmodel (5)
The total FLOPs per token in one forward pass is approximatel y Cforward = 2N + 2nlayer ninput dattn (see
Section 2.1 in [Kap+20] for details). In LLMs, dmodel ≫ ninput /12, we can roughly ignore the second term
and use Cforward ≈ 2N.
W ith the above analysis, we know that performing one forward pass of CAD yields in total (2 |y<t | +
2 |x| + |c|) · Cforward FLOPs compared to (|y<t | + |x| + |c|) · Cforward from vanilla decoding.
2



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 0):

Published as a conference paper at ICLR 2024
CHAIN -OF-KNOWLEDGE : G ROUNDING LARGE LAN-
GUAGE MODELS VIA DYNAMIC KNOWLEDGE ADAPT-
ING OVER HETEROGENEOUS SOURCES
Xingxuan Li1,2∗† , Ruochen Zhao2∗‡ , Yew Ken Chia1,3∗† , Bosheng Ding1,2†, Shafiq Joty2,4
Soujanya Poria3, Lidong Bing1,5
1DAMO Academy, Alibaba Group, Singapore, 2Nanyang Technological University,
3Singapore University of Technology and Design, 4Salesforce Research,
5Hupan Lab, 310023, Hangzhou, China
{xingxuan.li, yewken.chia, bosheng.ding, l.bing}@alibaba-inc.com
{ruochen002, srjoty}@ntu.edu.sg sporia@sutd.edu.sg
ABSTRACT
We present chain-of-knowledge (CoK) , a novel framework that augments large
language models (LLMs) by dynamically incorporating grounding information
from heterogeneous sources. It results in more factual rationales and reduced
hallucination in generation. Specifically, CoK consists of three stages: reason-
ing preparation, dynamic knowledge adapting, and answer consolidation. Given
a knowledge-intensive question, CoK first prepares several preliminary rationales
and answers while identifying the relevant knowledge domains. If there is no ma-
jority consensus among the answers from samples, CoK corrects the rationales
step by step by adapting knowledge from the identified domains. These corrected
rationales can plausibly serve as a better foundation for the final answer consolida-
tion. Unlike prior studies that primarily use unstructured data, CoK also leverages
structured knowledge sources such as Wikidata and tables that provide more re-
liable factual information. To access both unstructured and structured knowledge
sources in the dynamic knowledge adapting stage, we propose an adaptive query
generator that allows the generation of queries for various types of query lan-
guages, including SPARQL, SQL, and natural sentences. Moreover, to minimize
error propagation between rationales, CoK corrects the rationales progressively
using preceding corrected rationales to generate and correct subsequent rationales.
Extensive experiments show that CoK consistently improves the performance of
LLMs on knowledge-intensive tasks across different domains. Our code is avail-
able at https://github.com/DAMO-NLP-SG/chain-of-knowledge.
1 I NTRODUCTION
In recent years, large language models (LLMs) such as ChatGPT (OpenAI, 2023) have demonstrated
impressive language generation capabilities (Cheng et al., 2023; Ding et al., 2023; Chen et al., 2024).
However, one major challenge of LLMs lies in hallucination, which is their tendency to confidently
generate plausible but factually incorrect texts (Ji et al., 2023; Zhao et al., 2023b). As shown in
Figure 1, given a question, “What year was the Argentine actor who directed El Tio Disparate born?”
which requires factual knowledge to answer, the most advanced LLMs often provide an incorrect
answer. While LLMs have the remarkable capability to recall information from their training data,
effectively updating or controlling the factual knowledge within these models remains challenging
(Luo et al., 2023).
∗ Equal contribution.
† Xingxuan Li, Yew Ken Chia, and Bosheng Ding are under the Joint Ph.D. Program between DAMO
Academy and their corresponding universities.
‡ Ruochen Zhao is under the AISG Ph.D. Fellowship Programme.
1
arXiv:2305.13269v4  [cs.CL]  21 Feb 2024



Source: data\tc17_2312.15883v2\referenced_papers\[47]_2307.03172.pdf (Page 12):

Ori Ram, Yoav Levine, Itay Dalmedigos, Dor
Muhlgay, Amnon Shashua, Kevin Leyton-
Brown, and Yoav Shoham. 2023. In-
context retrieval-augmented language models.
ArXiv:2302.00083.
Ohad Rubin and Jonathan Berant. 2023. Long-
range language modeling with self-retrieval.
ArXiv:2306.13421.
Chinnadhurai Sankar, Sandeep Subramanian, Chris
Pal, Sarath Chandar, and Yoshua Bengio. 2019.
Do neural dialog systems use the conversation
history effectively? an empirical study. In Proc.
of ACL.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì,
Roberta Raileanu, Maria Lomeli, Luke Zettle-
moyer, Nicola Cancedda, and Thomas Scialom.
2023. Toolformer: Language models can teach
themselves to use tools.
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Be-
rant, and Omer Levy. 2023. ZeroSCROLLS: A
zero-shot benchmark for long text understanding.
ArXiv:2305.14196.
Vatsal Sharan, Sham Kakade, Percy Liang, and
Gregory Valiant. 2018. Prediction with a short
memory. In Proc. of STOC.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Min-
joon Seo, Rich James, Mike Lewis, Luke Zettle-
moyer, and Wen tau Yih. 2023. REPLUG:
Retrieval-augmented black-box language mod-
els. ArXiv:2301.12652.
Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,
Eric Michael Smith, Stephen Roller, Megan
Ung, Moya Chen, Kushal Arora, Joshua Lane,
Morteza Behrooz, William Ngan, Spencer Poff,
Naman Goyal, Arthur Szlam, Y-Lan Boureau,
Melanie Kambadur, and Jason Weston. 2022.
BlenderBot 3: a deployed conversational agent
that continually learns to responsibly engage.
ArXiv:2208.03188.
Simeng Sun, Kalpesh Krishna, Andrew Mattarella-
Micke, and Mohit Iyyer. 2021. Do long-range
language models actually use long-range con-
text? In Proc. of EMNLP.
Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier
Garcia, Jason Wei, Xuezhi Wang, Hyung Won
Chung, Siamak Shakeri, Dara Bahri, Tal
Schuster, Huaixiu Steven Zheng, Denny Zhou,
Neil Houlsby, and Donald Metzler. 2023.
UL2: Unifying language learning paradigms.
ArXiv:2205.05131.
Romal Thoppilan, Daniel De Freitas, Jamie Hall,
Noam Shazeer, Apoorv Kulshreshtha, Heng-
Tze Cheng, Alicia Jin, Taylor Bos, Leslie
Baker, Yu Du, YaGuang Li, Hongrae Lee,
Huaixiu Steven Zheng, Amin Ghafouri, Marcelo
Menegali, Yanping Huang, Maxim Krikun,
Dmitry Lepikhin, James Qin, Dehao Chen,
Yuanzhong Xu, Zhifeng Chen, Adam Roberts,
Maarten Bosma, Vincent Zhao, Yanqi Zhou,
Chung-Ching Chang, Igor Krivokon, Will Rusch,
Marc Pickett, Pranesh Srinivasan, Laichee Man,
Kathleen Meier-Hellstern, Meredith Ringel Mor-
ris, Tulsee Doshi, Renelito Delos Santos, Toju
Duke, Johnny Soraker, Ben Zevenbergen, Vin-
odkumar Prabhakaran, Mark Diaz, Ben Hutchin-
son, Kristen Olson, Alejandra Molina, Erin
Hoffman-John, Josh Lee, Lora Aroyo, Ravi
Rajakumar, Alena Butryna, Matthew Lamm,
Viktoriya Kuzmina, Joe Fenton, Aaron Cohen,
Rachel Bernstein, Ray Kurzweil, Blaise Aguera-
Arcas, Claire Cui, Marian Croak, Ed Chi, and
Quoc Le. 2022. LaMDA: Language models for
dialog applications. ArXiv:2201.08239.
Hugo Touvron, Thibaut Lavril, Gautier Izac-
ard, Xavier Martinet, Marie-Anne Lachaux,
Timothée Lacroix, Baptiste Rozière, Naman
Goyal, Eric Hambro, Faisal Azhar, Aurelien
Rodriguez, Armand Joulin, Edouard Grave,
and Guillaume Lample. 2023a. LLaMA:
Open and efficient foundation language models.
ArXiv:2302.13971.
Hugo Touvron, Louis Martin, Kevin Stone, Pe-
ter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal
Bhargava, Shruti Bhosale, Dan Bikel, Lukas
Blecher, Cristian Canton Ferrer, Moya Chen,
Guillem Cucurull, David Esiobu, Jude Fernan-
des, Jeremy Fu, Wenyin Fu, Brian Fuller, Cyn-
thia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou,
Hakan Inan, Marcin Kardas, Viktor Kerkez,
Madian Khabsa, Isabel Kloumann, Artem Ko-
renev, Punit Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich,
Yinghai Lu, Yuning Mao, Xavier Martinet, Todor
Mihaylov, Pushkar Mishra, Igor Molybog, Yixin



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 19):

A Scaling laws
We use 7 models to fit the scaling laws of Baichuan
2. The parameter details are shown in Table 10.
Nhidden NFFN Nlayer Nhead Nparams (Millions)
384 1,152 6 6 11.51
704 2,112 8 8 51.56
832 2,496 12 8 108.01
1,216 3,648 16 8 307.60
1,792 5,376 20 14 835.00
2,240 6,720 24 14 1,565.60
2,880 8,640 28 20 3,019.33
Table 10: The model we choose for fitting scaling laws.
The losses of the 7 different models are shown
in Figure 8.
Figure 8: The various training loss of small models for
scaling law.
B NormHead
By conducting a word embedding KNN retrieval
task, where given a query word the nearest K
words are retrieved. We found that the semantic
information is mainly encoded by the cosine
similarity of embedding rather than L 2 distance.
i.e., The KNN results of cosine similarity are words
with semantic similarity while the KNN results of
L2 distance are meaningless in some way. Since
the current linear classifier computes logits by dot
product, which is a mixture of L 2 distance and
cosine similarity. To alleviate the distraction of L2
distance, We propose to compute the logits by the
angle only. We normalized the output Embedding
so that the dot product is not affected by the norm
of embedding.
To validate this operation, we conduct an
ablation experiment where we add or remove the
normalization before softmax and train a 7B model
for 12k steps. All the hyper-parameters and data are
the same with Baichuan 2-7B. The training loss is
shown in Figure 9. We can see that when removing
the NormHead the training became very unstable at
the beginning, on the contrary, after we normalized
the head the training became very stable, which
resulted in better performance.
Figure 9: The training loss with and without NormHead
operation. The experiments are conducted on 7 billion
parameters with the same hyper-parameters (torch
random seeds, data flow, batch size, learning rate, etc.)
C Training Dynamics
In this section, we analyze the training dynamics of
our model. We save the checkpoints of Baichuan 2-
7B and Baichuan 2-13B every 1000 steps. And
evaluate those intermediate results on C-Eval
development set (Huang et al., 2023), MMLU
(Hendrycks et al., 2021a) , CMMLU (Li et al.,
2023) , JEC-QA (Zhong et al., 2020), GSM8K (Shi
et al., 2022) and HumanEval (Chen et al., 2021).
The result is shown in Figure 10.
As shown, both the 7B and 13B models
demonstrate substantial gains as training
progresses. However, on general benchmarks
such as MMLU (Hendrycks et al., 2021a) and
C-Eval (Huang et al., 2023), improvements appear
to plateau after 2 trillion tokens. In contrast,
consistent gains are achieved on the GSM8K
math tasks even beyond 2 trillion tokens. This
suggests training FLOPs may strongly correlate
with improvements in math problem solving,
which may be further studied.
D Baichuan Harmless Evaluation Dataset
WARNING: this section contains unsafe, offensive,
or upsetting examples of text.
We proposed the Baichuan Harmless Evaluation
Dataset (BHED) to evaluate the chat models, as



### Claim 24/36

#### Claim Text
It is common practice to use a marker density comparable to the density of the Eulerian grid points .

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[45]_2308.03281.pdf (Page 4):

Task Type Text Pair Format Query Doc
Web Page (title, body) Providence Real Estate | Providence Homes for SaleFounded by Roger Williams in 1636, Providence isrecognized as one of the country’s oldest cities. . .
Academic Paper(title, abstract) Polymer Quantum Mechanics and its Continuum LimitA rather non-standard quantum representation of thecanonical commutation relations of quantum mechanics. . .
Hyperlink (citation, reference)After the championship in 1996, the PGA of Americaraised its stake to 50% and announced that . . .Pebble Beach Golf Links The largest margin of victoryever in a major championship, surpassing the 13-shot . . .
Social Media (post, comment)Pretty sure any team with Lebron James will be a playoffcontender. Considering UNC would be in the East. . .I was being sarcastic and making fun of the East, buthonestly I was really in deep thought about this . . .
Knowledge Base(entity, description) Animation Animation is the process of creating the illusion of motionand shape change by means of the rapid display of . . .
Community QA (question, answer)How the human species evolved? A tough question as it overlaps science and theology. Sinceyou asked “how the human species evolved?” I’ll assume . . .
News (summary, content) Nepalese Opposition Welcomes Return of ParliamentNepal’s opposition alliance formally calls off weeks ofpro-democracy protests after King Gyenandra reinstates . . .
Code (text, code) SetMaxRecords sets the MaxRecords field’s value.func (s *DescribeSnapshotCopyGrantsInput) SetMaxRecords(v int64) *DescribeSnapshotCopyGrantsInput { s.MaxRecords
Table 2: Examples of mined (query, document) pairs in the pre-training data.
Consider a batch of positive text pair samples
B = {(q1, d1), (q2, d2), ...,(qn, dn)},
we use an improved contrastive loss which takes
the form
Licl = −1
n
nX
i=1
log es(qi,di)/τ
Z (5)
with the partition function being
Z =
X
j
es(qi,dj )/τ +
X
j̸=i
es(qi,qj )/τ
+
X
j
es(qj ,di)/τ +
X
j̸=i
es(dj ,di)/τ (6)
in which the first two terms are used for query to
document contrast, where as the last two terms are
used for the inverse. In this work, we use the cosine
similarity as the distance metric
s(q, d) = q · d
||q||2 · ||d||2
. (7)
The temperature τ is fixed to 0.01 in this work.
Training and Evaluation The training of our
embedding model consists of two stages. In the
first stage of contrastive pre-training with only in-
batch negatives, using a large batch size is crucial
to better model performance by reducing the gap
between training and inference with more nega-
tives included and providing a better approximation
to the underlying learning objective. To facilitate
this, we limit the maximum sequence length to 128
during pre-training and distribute the use of nega-
tives across all GPUs. Popular techniques such as
automatic mixed precision training (Micikevicius
et al., 2018) with fp16, deepspeed ZeRO (Rajb-
handari et al., 2020) stage 1 and gradient check-
pointing (Chen et al., 2016) are also jointly used to
reduce memory cost and scale up batch size to over
ten thousands. We run the pre-training for 50, 000
steps, which roughly corresponds to one epoch on
the whole pre-training data. We only tuned the
learning rate to ensure the convergence of larger
models. we employ the AdamW optimizer with
linear learning rate decay and a warm-up period
during the initial 5% of training steps. We con-
ducted experiments on three distinct model scales:
small, base, and large. These models were initial-
ized using the small-sized MiniLM (Wang et al.,
2020) model and the base and large models of the
BERT (Devlin et al., 2019) model. Further details
can be found in Table 3.
In the second stage of contrastive fine-tuning
with supervised data and hard negatives, a large
batch size is unnecessary since hard negatives can
already provide a reliable gradient estimation of
the learning objective (Xiong et al., 2021; Li et al.,
2023). Therefore, a global batch size of 128 and a
train group size of 16 are utilized, with one positive
example and the remaining being either hard nega-
tives or random negatives. Instead we increase the
max sequence length to 512 to better handle texts
with longer lengths. The learning rate is decreased
by a factor of ten during fine-tuning. The model
is fine-tuned on the collected dataset for a single
epoch. In-batch texts are also incorporated as nega-
tive candidates using the enhanced contrastive loss
described in Equation 5.
After training, we directly take the last check-
point for evaluation. We run model training on up
to 8 NVIDIA A100 GPUs with 80GB memory and
model evaluation on up to 8 NVIDIA Tesla V100
GPUs with 32GB memory. Models are trained with
mixed precision using fp16 and evaluated with half
precision fp16 as well.



Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 11):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
Table 7: Performance of number of chunks retrieved with gpt-4.
# Retrieved Chunks ADA-002 Custom ADA-002 BM25 Hybrid BM25 + Reranker
3 6.19 6.41 7.10 7.31 7.44
5 6.29 6.61 7.32 7.37 7.43
7 6.42 6.82 7.17 7.20 7.32
9 6.57 6.88 7.22 7.34 7.37
Table 8: Performance of best retriever RAG.
Model Retriever Method # Retrieved Chunks Degradation
gpt-4 ADA-002 9 -13.0%
gpt-4 ADA-002 Custom 9 -8.8%
gpt-4 BM25 5 -3%
gpt-4 Hybrid 5 -2.3%
gpt-4 BM25 + Reranker 3 -1.4%
Gemini Pro BM25 + Reranker 3 -2.2%
RAG is directly related to the input size and the position where the answer is located. Therefore, the final results
confirm the observations made in our experiments.
Moreover, from a cost perspective, it is crucial to avoid overloading the LLM with a large number of input tokens, as
the cost is also based on the amount of input text.
It is important to note that the results obtained in this study cannot be considered as a generalization for other datasets.
Exploratory Data Analysis and the use of good retriever practices, as presented in [34], are always a solid path to
achieving good results.
6.3 Final Results
Despite this work being grounded in a single dataset, it is always crucial to emphasize the importance of data quality.
In a simplified manner, as illustrated in Figure 13, data quality in RAG can be divided into Input, Retriever, and
Evaluation.
Figure 13: Core Points in RAG.
• Input: How are the queries formulated? Are they synthetic or generic? What is the application’s purpose?
Building a RAG for a chatbot differs significantly from constructing a RAG for extracting information from
a long and complex document.
• Retriever: How do the data behave in information retrieval? Are the queries strongly linked by keywords in
the text? What is the cosine similarity of query-to-query, query-to-document, and document-to-document?
• Evaluation: How are the data measured? Defining metrics and success rates, as in the first experiment, is
always a safe path to avoid biases. Build the evaluation system before testing, avoiding bias.
In conclusion, the main contribution of this work was to identify and present the best possible configuration of tech-
niques and parameters for a RAG application. Figure 14 provides a summarized overview of the end-to-end experi-
ment results for the discussed approaches, and the best practices recommended by this study achieve a final accuracy
of 98.61%, representing an improvement of 40.73 percentage points compared to the baseline.



Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 8):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
set D and search results from different methods r in R, for each d in D, we can calculate the RRFscore(d ∈ D) as
follows [27]:
RRFscore(d ∈ D) =
X
r∈R
1
k + r(d), (2)
Considering that 1/r(d) is known as the reciprocal rank, where r(d) represents the position at which the document d
was retrieved by the search mechanism r. The term k is introduced to assist in controlling outlier systems [27].
Figure 8: Hybrid Search schema with k=1.
Figure 8 shows how to calculate RRFscore for k = 1. In the example, we have four chunks that were retrieved in
different orders by two search methods, BM25 (sparse search), and Custom ADA-002 (dense search). The reciprocal
rank score is calculated for each chunk. These values are then summed, creating a new score. The final hybrid list is
an ordering of chunks that uses this new score.
Table 5: Retriever comparison. Where MRR is the Mean Recriprocal Rank metric and R@k is the Recall.
Metric Hybrid-BM25-ADA-002 Hybrid-BM25-Custom ADA-002
MRR@10 0.758 0.850
R@3 0.829 0.921
R@5 0.879 0.943
R@7 0.921 0.964
R@9 0.957 0.979
In our experiments, only Pyserini’s BM25 was tested as the sparse retriever, while ADA-002 and Custom ADA-002
were tested as dense retrievers. The hybrid combination that yielded the best results was the one that used BM25 and
Custom ADA-002.
5.1.5 Reranker
The fundamental idea underlying multi-stage ranking is to divide document ranking into a sequence of stages. After
an initial retrieval, which usually involves a sparse retriever or dense retriever, each subsequent stage re-evaluates and
reranks the set of candidates forwarded from the preceding stage. Figure 9 represents a multi-stage pipeline where
Pyserini BM25 performs the first stage and the candidate chunks are then re-evaluated by the reranker. After that, the
final reranked list called retrieved chunks is presented as final result formed by k chunks.
Figure 9: Reranker Pipeline



Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 7):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
the application of Multiple Negative Ranking Loss, as presented in the article "Efficient Natural Language Response
Suggestion for Smart Reply," Section 4.4 [26]. However, given our current focus on simplicity, we will reserve the
exploration of this technique for future work. At this moment, we choose to utilize the Mean Squared Error (MSE)
Loss.
For the fine-tuning stage, it is necessary to have two types of samples:
• positives: [question i, chunki, label = 1]
• negatives: [question i, chunkj, label = −1], for i ̸= j
Often, as is the case with our dataset, only positive examples are available. However, through a simple and random
shuffling, it is possible to generate negative examples. Demonstrating confidence in transfer learning, we found that
a few examples were sufficient. Our final dataset consisted of approximately 400 examples, maintaining a 1 : 3ratio
between positive and negative examples.
The hyperparameters that exert the most significant impact on performance include the learning rate, batch size, and
the number of dimensions in the projection matrix. The ADA-002 model has 1536 dimensions, and the projection
matrix is of size 1536 × N, where N ∈ 1024, 2048, 4096. In our experiments, we observed that 2048 dimensions
resulted in the best accuracy.
This type of fine-tuning requires low GPU resources, with a training time of approximately 5 minutes using the A100
GPU. The model itself is straightforward, consisting of a matrix with dropout (to mitigate overfitting), followed by the
hyperbolic tangent activation function, which provided additional accuracy gains in the training set.
Figure 6: Cosine similarity of positive and negative classes in the ADA-002 embedding; note the significant overlap
between the classes. Test accuracy (before training): 69.5%
Figure 7: Cosine similarity of positive and negative classes in the customized embedding; the intersection between the
classes is minimal. Test accuracy (after training): 84.3%
When analyzing the cosine similarity between positive and negative classes, we can observe the "shadow" shared by
the histograms. In an ideal scenario, we desire the classes to be disjoint to ensure a clear definition of space. Figure 6
illustrates a significant shadow in the embedding before training, while Figure 7 shows the result after training. Both
graphs are derived from the test set. Test accuracy also improved, leading to a better dense representation.
5.1.4 Hybrid Search
As stated before, hybrid search is applied when is necessary to combine results from two or more retrieval methods.
A widely used algorithm to address this type of problem is known as Reciprocal Rank Fusion (RRF). For a document



Source: data\tc17_2312.15883v2\referenced_papers\[86]_2312.14335.pdf (Page 9):

T able 4: Prompting T emplates
Datasets Model Decoding T emplates
Dbpedia
Decoder-only V anilla Question: {query}. Document: {document}.
According to the Document, the one-sentence answer to the Question is:
Decoder-only CAD Question: {query}. Document: {None}.
According to the Document, the one-sentence answer to the Question is:
Encoder-Decoder V anilla Question: {query}. Document: {document}.
According to the Document, the one-sentence answer to the Question is:
Encoder-Decoder CAD Question: {query}. Document: {None}.
According to the Document, the one-sentence answer to the Question is:
PubMedQA
Decoder-only V anilla Question: {query}. Document: {document}.
According to the Document, the detailed answer to the Question is:
Decoder-only CAD Question: {query}. Document: {document}.
According to the Document, the detailed answer to the Question is:
Encoder-Decoder V anilla Question: {query}. Document: {document}.
According to the Document, the detailed answer to the Question is:
Encoder-Decoder CAD Question: {query}. Document: {None}.
According to the Document, the detailed answer to the Question is:
CNN Dailymail
Decoder-only V anilla News article: {document}.
Summary of the above news article:
Decoder-only CAD News article: {None}.
Summary of the above news article:
Encoder-Decoder V anilla Summarize the following article in one or two sentences.
{document}:
Encoder-Decoder CAD Summarize the following article in one or two sentences.
{None}:
XSUM
Decoder-only V anilla News article: {document}.
Summary of the above news article:
Decoder-only CAD News article: {None}.
Summary of the above news article:
Encoder-Decoder V anilla Summarize the following article in one or two sentences.
{document}:
Encoder-Decoder CAD Summarize the following article in one or two sentences.
{None}:
T able 5: Decoding Hyperparameters
Dataset T op-k T op-p num beams repetition penalty temperature min new tokens max new tokens
Dbpedia 50 0.9 1 1.0 1.0 5 30
PubMedQA 50 0.9 1 1.0 1.0 40 100
CNN Dailymail 50 0.9 1 1.0 1.0 30 70
XSUM 50 0.9 1 1.0 1.0 20 50
10



### Claim 25/36

#### Claim Text
Using the fitted center frequencies of 80 transitions from the P and R branches spanning the entire band, we created a spectral model for the ν 1+ν3 band of 15N2O in PGOPHER .

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 3):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
• Query: What was Harry’s strategy to stop Quirrell from focusing on the mirror and what did he try to do to
discover the location of the Philosopher’s Stone?
– Answer: What occurred to Harry was to keep Quirrell talking to stop him from focusing on the mirror.
• Query: What model of broom did Harry Potter receive and who mentioned the special circumstances to
Professor Flitwick?
– Answer: The model of the broom that Harry Potter received is a Nimbus 2000 and it was Professor.
Minerva who mentioned the special circumstances to Professor Flitwick.
The Table 2 shows the baseline results obtained using some known LLMs for the 140 questions built as described in
section 2 and evaluated as described in section 3. For this task, no retrieved context were used, only the question.
Table 2: Performance of the External Knowledge experiment.
Model Average Score Degradation
gpt-4 5.35 -29.1%
gpt-4-1106-preview 5.06 -30.9%
gpt-3.5-turbo-1106 4.91 -32.8%
Gemini Pro 3.81 -50.8%
4.2 Long Context
In comparison to the GPT 1 and 2 models [11, 12], which handle up to 1024 input tokens, the gpt-4-1106-preview
model stands out for its remarkable ability to process up to 128k input tokens. This represents an approximately 128
times increase in input capacity over just four years in model development.
The specific architecture of gpt-4 has not been disclosed, but it is believed that this model has not been pre-trained
with a 128k token input context [13]. Perhaps a post-pre-training technique could have been used, which would have
made it possible to expand the number of input tokens [14, 15]. However, it is essential to note that such a technique
may show degradation as the expansion limit is reached [16]. Similar to Recurrent Neural Networks (RNNs), which
theoretically have an infinite context, disregarding performance limitations and vanish-gradients, we are interested in
evaluating the performance of gpt-4-1106-preview over its 128k tokens.
To assess the impact of the gpt-4-1106-preview full context capacity on the model’s response, we proceed with
a similar analysis of "Lost in The Middle" [17] in our dataset. This analysis explores the model output for a
given question while changing the position of the answer throughout the prompt. To conduct this experiment, the
depth of the chunk containing the answer for the question was altered in increments of 10% of the total number
of tokens in the context’s prompt. Thus, on the y-axis, there are 11 variations of answer depth, represented by
0%, 10%, 20%, . . . ,90%, 100%, and the x-axis represents the quantity of tokens used as input in the context, as shown
in Figure 2. The colors represents the experiment score, where the greener the better.
Figure 2: Performance of gpt-4-1106-preview on the Harry Potter dataset, x-axis: spaced at every 1, 000 tokens of
input from the document, y-axis: represents the depth at which the answer is located in the document. The greener the
better. Image based on Gregory repository [18].



Source: data\tc17_2312.15883v2\referenced_papers\[45]_2308.03281.pdf (Page 4):

Task Type Text Pair Format Query Doc
Web Page (title, body) Providence Real Estate | Providence Homes for SaleFounded by Roger Williams in 1636, Providence isrecognized as one of the country’s oldest cities. . .
Academic Paper(title, abstract) Polymer Quantum Mechanics and its Continuum LimitA rather non-standard quantum representation of thecanonical commutation relations of quantum mechanics. . .
Hyperlink (citation, reference)After the championship in 1996, the PGA of Americaraised its stake to 50% and announced that . . .Pebble Beach Golf Links The largest margin of victoryever in a major championship, surpassing the 13-shot . . .
Social Media (post, comment)Pretty sure any team with Lebron James will be a playoffcontender. Considering UNC would be in the East. . .I was being sarcastic and making fun of the East, buthonestly I was really in deep thought about this . . .
Knowledge Base(entity, description) Animation Animation is the process of creating the illusion of motionand shape change by means of the rapid display of . . .
Community QA (question, answer)How the human species evolved? A tough question as it overlaps science and theology. Sinceyou asked “how the human species evolved?” I’ll assume . . .
News (summary, content) Nepalese Opposition Welcomes Return of ParliamentNepal’s opposition alliance formally calls off weeks ofpro-democracy protests after King Gyenandra reinstates . . .
Code (text, code) SetMaxRecords sets the MaxRecords field’s value.func (s *DescribeSnapshotCopyGrantsInput) SetMaxRecords(v int64) *DescribeSnapshotCopyGrantsInput { s.MaxRecords
Table 2: Examples of mined (query, document) pairs in the pre-training data.
Consider a batch of positive text pair samples
B = {(q1, d1), (q2, d2), ...,(qn, dn)},
we use an improved contrastive loss which takes
the form
Licl = −1
n
nX
i=1
log es(qi,di)/τ
Z (5)
with the partition function being
Z =
X
j
es(qi,dj )/τ +
X
j̸=i
es(qi,qj )/τ
+
X
j
es(qj ,di)/τ +
X
j̸=i
es(dj ,di)/τ (6)
in which the first two terms are used for query to
document contrast, where as the last two terms are
used for the inverse. In this work, we use the cosine
similarity as the distance metric
s(q, d) = q · d
||q||2 · ||d||2
. (7)
The temperature τ is fixed to 0.01 in this work.
Training and Evaluation The training of our
embedding model consists of two stages. In the
first stage of contrastive pre-training with only in-
batch negatives, using a large batch size is crucial
to better model performance by reducing the gap
between training and inference with more nega-
tives included and providing a better approximation
to the underlying learning objective. To facilitate
this, we limit the maximum sequence length to 128
during pre-training and distribute the use of nega-
tives across all GPUs. Popular techniques such as
automatic mixed precision training (Micikevicius
et al., 2018) with fp16, deepspeed ZeRO (Rajb-
handari et al., 2020) stage 1 and gradient check-
pointing (Chen et al., 2016) are also jointly used to
reduce memory cost and scale up batch size to over
ten thousands. We run the pre-training for 50, 000
steps, which roughly corresponds to one epoch on
the whole pre-training data. We only tuned the
learning rate to ensure the convergence of larger
models. we employ the AdamW optimizer with
linear learning rate decay and a warm-up period
during the initial 5% of training steps. We con-
ducted experiments on three distinct model scales:
small, base, and large. These models were initial-
ized using the small-sized MiniLM (Wang et al.,
2020) model and the base and large models of the
BERT (Devlin et al., 2019) model. Further details
can be found in Table 3.
In the second stage of contrastive fine-tuning
with supervised data and hard negatives, a large
batch size is unnecessary since hard negatives can
already provide a reliable gradient estimation of
the learning objective (Xiong et al., 2021; Li et al.,
2023). Therefore, a global batch size of 128 and a
train group size of 16 are utilized, with one positive
example and the remaining being either hard nega-
tives or random negatives. Instead we increase the
max sequence length to 512 to better handle texts
with longer lengths. The learning rate is decreased
by a factor of ten during fine-tuning. The model
is fine-tuned on the collected dataset for a single
epoch. In-batch texts are also incorporated as nega-
tive candidates using the enhanced contrastive loss
described in Equation 5.
After training, we directly take the last check-
point for evaluation. We run model training on up
to 8 NVIDIA A100 GPUs with 80GB memory and
model evaluation on up to 8 NVIDIA Tesla V100
GPUs with 32GB memory. Models are trained with
mixed precision using fp16 and evaluated with half
precision fp16 as well.



Source: data\tc17_2312.15883v2\referenced_papers\[17]_2401.07883.pdf (Page 4):

The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT
For instance, for (x = 100, 000, y = 40%), there are (39 chunks, followed by the chunk containing the answer, then
by the remaining 60 chunks, making up the 100, 000 tokens in the input context. Based on Figure 2, we can also
see when increasing the input length, we see a strong degradation in the score. Besides that, the Figure 3 shows that
answers located in the interval of (40%, 80%) exhibit the worst performance, as documented in the article "Lost In
The Middle" [17].
Figure 3: Average performance analysis of gpt-4-1006-preview using 128k tokens context per answer depth.
4.3 RAG Naive
Initially, a straightforward approach for RAG will be done using the llama-index [19], employing all default hyper-
parameters and using chunk retrieval by cosine similarity with the ADA-002 embedding. Figure 4 depicts the basic
diagram of how the problem is addressed.
Figure 4: 1. Pass the query to the embedding model to represent its semantics as an embedded query vector; 2.
Transfer the embedded query vector to vector database or sparse index (BM25); 3. Fetch the top-k relevant chunks,
determined by retriever algorithm; 4. Forward the query text and the chunks retrieved to Large Language Model
(LLM); 5. Use the LLM to produce a response based on the prompt filled by the retrieved content.
The Table 3 shows the average and degradation metrics for this approach using 2 retrieved chunks.
Table 3: Performance of the RAG naive.
Model Average Score Degradation
gpt-4 6.04 -20%
gpt-4-1106-preview 5.74 -21.6%
gpt-3.5-turbo-1106 5.80 -21.0%



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 7):

8  
 Fig 2. Hyperparameter analysis and RAG comparison (A) Hyperparameter analysis performance curves using prompts with single (top) and two (below) disease entities mentioned in it. The x-axis denotes the ‘Context volume’ (number of associations from KG) and the y-axis denotes the mean performance (Jaccard similarity) across the prompts. The red curve denotes ‘PubMedBert’ and the blue curve denotes ‘MiniLM’ transformer models. (B) shows the comparative analysis between KG-RAG (green color) and Cypher-RAG (red color) in terms of retrieval accuracy (top) and token usage (bottom). Insight shows an example where Cypher-RAG fails to retrieve context from the KG when the input prompt is slightly perturbed, but KG-RAG remains robust in context retrieval. It is evident that KG-RAG has lesser token usage (average of 53.9% reduction in token usage) when compared to Cypher-RAG (bottom). Error bar in the token utilization bar plot (bottom) represents standard error of the mean (sem).     RAG comparative analysis Fig 2B shows the comparative analysis between the proposed KG-RAG and the existing Cypher-RAG approach for context retrieval from a KG (See Materials and Methods). We compared these two frameworks based on their retrieval accuracy, retrieval robustness and token usage. For a test dataset with 100 biomedical questions (S1 Text), Cypher-RAG and KG-RAG showed 75% and 97% retrieval accuracy respectively (Fig 2B top). To test the robustness in context retrieval,



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 8):

9  
we introduced a slight perturbation to the test dataset by converting the entity names to lowercase (Fig 2B insight). We observed a significant decrease in the retrieval accuracy of Cypher-RAG to 0% (indicating failure to retrieve any context from the graph). This mainly occurs because Cypher-RAG utilizes precise matching of the entity keywords provided in the user prompt to formulate the Cypher query for extracting graph context. On the otherhand, KG-RAG maintained its retrieval accuracy at 97% (i.e. robust to the input perturbation, Fig 2B top). This is because KG-RAG employs a semantic embedding approach to extract graph context, which enhances its ability to effectively handle various representations of entities within user prompts. Next, we analyzed the total token usage of each framework for generating the response for the same test dataset (Fig 2B bottom). We found that Cypher-RAG had an average token usage of 8006 tokens whereas KG-RAG had an average token usage of 3693 tokens (Fig 2B bottom). This represents a 53.9% reduction in the token usage by KG-RAG compared to Cypher-RAG which points to a significant cost-effective retrieval ability of KG-RAG.           Performance on True/False and MCQ datasets Fig 3 shows bootstrap distributions of performance (accuracy) of the three LLMs using prompt-based and KG-RAG framework on True/False (Fig 3A) and MCQ (Fig 3B) datasets. Table 1 summarizes the performance of the three LLMs across these datasets. We observed a consistent performance enhancement for the LLM models under KG-RAG framework on both True/False and MCQ datasets (Table 1). KG-RAG significantly elevated the performance of Llama-2 by approximately 71% from its initial level (0.31±0.03 to 0.53±0.03) on the more challenging MCQ dataset (Table 1). Intriguingly, we also observed a small but significant drop in the performance of GPT-4 model (0.74±0.03) compared to GPT-3.5-Turbo model (0.79±0.02) on MCQ dataset



### Claim 26/36

#### Claim Text
Also, external radiation like cosmic rays or bremsstrahlung photons from very fast runaway electrons can act as an electron source .

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 26):

用户：鸡和兔在一个笼子里，共有26个头，68只脚，那么鸡有多少只，兔有多少只？
Baichuan 2-13B-Chat：我们可以用代数方法来解决这个问题。假设鸡的数量为x，兔子的
数量为y。根据题目给出的信息，我们可以得到以下两个方程：
1. x + y = 26 (头的总数) 2. 2x + 4y = 68 (脚的总数)
现在我们需要解这个二元一次方程组。首先从第一个方程中解出x：
x = 26 - y
然后将x的表达式代入第二个方程：
2(26 - y) + 4y = 68 52 - 2y + 4y = 68 2y = 16
接下来我们求解y的值：
y = 16 / 2 y = 8
现在我们得到了兔子的数量是8只。将y的值代入x的表达式求得鸡的数量：
x = 26 - 8 x = 18
所以鸡有18只，兔子有8只。
用户：以下天文学常识题目，哪一个是错误的? A.太阳系是指由太阳和围绕着它运行的
八大行星、矮行星、卫星、小行星带和彗星组成的一个行星系统。B.卫星是指绕行星
或其他天体运动的天体。C.彗星是指太阳系中一种较小的天体，其核心由冰和尘埃组
成。D.按一般的天体归类方法，月球属于行星。
Baichuan 2-13B-Chat：D.按一般的天体归类方法，月球属于行星是错误的。实际上，月球
是地球的卫星，不属于行星范畴。
用户：以下物理常识题目，哪一个是错误的?A.在自然环境下，声音在固体中传播速
度最快。B.牛顿第一定律:一个物体如果不受力作用，将保持静止或匀速直线运动的状
态。C.牛顿第三定律:对于每个作用力，都有一个相等而反向的反作用力。D.声音在空气
中的传播速度为1000m/s。
Baichuan 2-13B-Chat：本题考查物理基本概念和原理的理解和应用能力。A项：声音的传
播需要介质，在固体、液体和气体的三种介质中，声音在固体中的传播速度是最快的，
故A正确； B项：牛顿第一定律是惯性定律，其内容是：一个物体如果没有受到外力的
作用，要么保持静止状态，要么保持匀速直线运动状态，直到有外力迫使它改变运动状
态为止，故B正确； C项：牛顿第三定律的内容是：对于每一个作用力，都有一个大小
相等、方向相反的反作用力，这两个力分别作用在两个相互作用的物体上，故C错误；
D项：声音在空气中以波的形式传播，其传播速度与介质的温度、湿度等因素有关，一
般情况下，声音在空气中的传播速度约为340m/s左右，而不是1000m/s，故D错误。 因
此，本题的正确答案是D 。



Source: data\tc17_2312.15883v2\referenced_papers\[86]_2312.14335.pdf (Page 5):

T able 3: Performance comparison between difference choice s of hyperparameter α ,
recall that α = 0 reduces to vanilla decoding. The reported numbers are aver aged
over eight models.
Datasets α value ROUGE-1 ROUGE-2 ROUGE-L BER TScore-P FactKB
Dbpedia
α =0.0 15.5 4.1 13.8 71.2 32.7
α =0.15 14.9 3.8 13.3 71.1 35.1
α =0.3 14.7 3.6 13.1 70.1 38.1
α =0.5 14.5 3.6 13.0 70.9 38.7
PubMedQA
α =0.0 29.1 9.7 24.0 73.3 89.1
α =0.15 28.7 9.6 23.8 73.2 90.6
α =0.3 28.4 9.4 23.4 73.0 91.7
α =0.5 27.9 9.1 23.1 72.7 93.3
CNN Dailymail
α =0.0 29.1 9.6 25.8 73.9 85.2
α =0.15 30.4 10.9 27.1 74.3 87.2
α =0.3 30.7 11.2 27.4 74.4 88.4
α =0.5 30.7 11.2 27.3 74.4 87.7
XSUM
α =0.0 25.2 7.0 19.7 74.5 50.1
α =0.15 25.4 7.2 19.8 74.8 55.7
α =0.3 25.3 7.2 19.9 74.9 58.0
α =0.5 24.8 6.9 19.4 74.9 58.1
due to the design choices in building the datasets. On the new s summarization datasets, setting α = 0.3
yields an overall better performance.
Reporting Decoding Speed. Previously we have analyzed the additional complexity in te rms of FLOPs
of CAD. Now we take a look at the difference in decoding speed o n a real-world dataset. The experiment
is conducted on a server with a single Nvidia RTX 3090 GPU, CUD A version 12.1, and PyT orch version
2.1.0. W e implement two forward passes, ﬁrst with pθ (yt |c, x, y<t ), and second with pθ (yt |x, y<t ); another
option is packing two computation into one batch which will i ncrease CUDA memory usage.
On CNN Dailymail dataset, vanilla decoding with FP16 and bat ch size=1 leads to a decoding speed of
0.042 second/token, while the decoding speed of CAD with FP1 6 and batch size=1 is 0.077 second/token
with two forward passes. This suggests while CAD can improve generation quality , it also slows down
decoding speed or requires more CUDA memory .
5 Related W ork
Search as an Information-seeking Process.[Mar06] categorize search into three two different informa -
tion seeking processes— Information Lookup and Exploratory Search . Information lookup refers to the user
looking for simple facts or asking factoid questions, while exploratory search/search to learn often in-
volves multiple iterations. Different approaches have bee n explored to reduce users’ cognitive loads in the
information-seeking process. For example, Perplexity AI 3 transfers web search queries into QA-style ques-
tions, and cites sources in the answers to improve veriﬁabil ity . Snippets [BC14] provide short previews for
each webpage to allow users to locate relevant information w ithout examining the webpage. Counterfac-
tual explanations [Xu+23] suggest counterfactual queries as explanations and demonstrate search sessions’
effectiveness can be improved.
Hallucination in Natural Language Generation.Ji et al. [Ji+23] present a survey of hallucination in natu-
ral language generation. There are mainly two types of hallu cinations, namely intrinsic hallucination and
extrinsic hallucination. Intrinsic hallucination refers to the generated output that contradicts the source
content, whereas extrinsic hallucination refers to the gen erated output that cannot be veriﬁed from the
3 https://www .perplexity .ai/
6



Source: data\tc17_2312.15883v2\referenced_papers\[86]_2312.14335.pdf (Page 6):

source content. In retrieval augmented generation, we main ly focus on intrinsic hallucination. Pre-trained
language models (not instruction ﬁnetuned) are more prone t o hallucination, as the gigantic pre-training
corpus may contain untruthful texts and the teacher forcing training procedure lets the models pick up this
untruthful information. Shuster et al. [Shu+21] show that r etrieval augmentation can reduce hallucinations
in downstream tasks. Tian et al. [Tia+23] show that language models can be trained to reduce hallucination
via reinforcement learning techniques. V arshney et al. [V a r+23] propose to use lm logits to detect hallucina-
tion in the generation process and mitigate it by retrieving evidence and rewriting.
Decoding Methods to Improve Generation Quality . Li et al. [Li+23] propose Contrastive Decoding (CD)
to improve generation quality . The idea is to contrast the st ronger expert model’s prediction with a smaller
amateur model. A recent work [OL23] shows that CD can also imp rove the reasoning ability of the language
models on multiple reasoning benchmark datasets. Based on a similar idea, Liu et al. [Liu+21] show that
by contrasting to a toxic model (language model trained with toxicity contents), the toxicity of the expert
language model’s generations can be reduced. A different ﬂa vor of controlled generation aims to let the
model generate speciﬁc tokens/phrases. Lu et al. [Lu+21] sh ow that such a goal can be achieved by utilizing
speciﬁc data structure and interpolation of lm logits.
6 Conclusion and Limitations
In this work we carry out a reproducibility study of a recently proposed decoding method—Context-
aware Decoding (CAD). Experiments on query-focused summar ization datasets and news summariza-
tion datasets show that CAD can improve QFS and news summariz ation quality by reducing factual mis-
takes/hallucinations while mostly retaining the match of l exical patterns, but also with additional inference-
time FLOPs and reduced decoding speed. Due to the limited ban dwidth and resources, we only experiment
with language models no larger than 11B ( Flan-T5-xxl), larger language models may exhibit different per-
formance patterns.
References
[BC14] Hannah Bast and Marjan Celikik. “Efﬁcient index-based snippet generation”. In: ACM T ransac-
tions on Information Systems (TOIS) 32.2 (2014), pp. 1–24.
[Ben+21] Emily M Bender et al. “On the dangers of stochastic p arrots: Can language models be too
big?” In: Proceedings of the 2021 ACM conference on fairness, account ability, and transparency . 2021,
pp. 610–623.
[Che+20] W ei-Fan Chen et al. “Abstractive snippet generati on”. In: Proceedings of The Web Conference 2020 .
2020, pp. 1309–1319.
[Chu+22] Hyung W on Chung et al. “Scaling instruction-ﬁnetu ned language models”. In: arXiv preprint
arXiv:2210.11416 (2022).
[Fen+23] Shangbin Feng et al. “Factkb: Generalizable factu ality evaluation using language models en-
hanced with factual knowledge”. In: arXiv preprint arXiv:2305.08281 (2023).
[Hol+19] Ari Holtzman et al. “The curious case of neural text degeneration”. In: arXiv preprint arXiv:1904.09751
(2019).
[Ji+23] Ziwei Ji et al. “Survey of hallucination in natural l anguage generation”. In: ACM Computing
Surveys 55.12 (2023), pp. 1–38.
[Jia+23] Albert Q Jiang et al. “Mistral 7B”. In: arXiv preprint arXiv:2310.06825 (2023).
[Jin+19] Qiao Jin et al. “PubMedQA: A Dataset for Biomedical Research Question Answering”. In: Pro-
ceedings of the 2019 Conference on Empirical Methods in Natu ral Language Processing and the 9th
International Joint Conference on Natural Language Proces sing (EMNLP-IJCNLP) . Ed. by Kentaro
Inui et al. Hong Kong, China: Association for Computational Linguistics, Nov . 2019, pp. 2567–
2577. DOI : 10.18653/v1/D19-1259. U RL : https://aclanthology.org/D19-1259.
7



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 16):

Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson,
Itziar Gonzalez-Dios, Javier de la Rosa, Jenny
Chim, Jesse Dodge, Jian Zhu, Jonathan Chang,
Jorg Frohberg, Josephine L. Tobing, Joydeep
Bhattacharjee, Khalid Almubarak, Kimbo Chen,
Kyle Lo, Leandro von Werra, Leon Weber, Long
Phan, Loubna Ben Allal, Ludovic Tanguy, Manan
Dey, Manuel Romero Muñoz, Maraim Masoud,
Mar’ia Grandury, Mario vSavsko, Max Huang,
Maximin Coavoux, Mayank Singh, Mike Tian-
Jian Jiang, Minh Chien Vu, Mohammad Ali
Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora
Kassner, Nurulaqilla Khamis, Olivier Nguyen,
Omar Espejel, Ona de Gibert, Paulo Villegas,
Peter Henderson, Pierre Colombo, Priscilla A.
Amuok, Quentin Lhoest, Rheza Harliman, Rishi
Bommasani, Roberto L’opez, Rui Ribeiro, Salomey
Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose,
Shamsuddeen Hassan Muhammad, Shanya Sharma,
S. Longpre, Somaieh Nikpoor, Stanislav Silberberg,
Suhas Pai, Sydney Zink, Tiago Timponi Torrent,
Timo Schick, Tristan Thrush, Valentin Danchev,
Vassilina Nikoulina, Veronika Laippala, Violette
Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak
Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si,
Elizabeth Salesky, Sabrina J. Mielke, Wilson Y . Lee,
Abheesht Sharma, Andrea Santilli, Antoine Chaffin,
Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla,
Gunjan Chhablani, Han Wang, Harshit Pandey,
Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo
Gao, Lintang Sutawika, M Saiful Bari, Maged S.
Al-shaibani, Matteo Manica, Nihal V . Nayak, Ryan
Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-
David, Stephen H. Bach, Taewoon Kim, Tali Bers,
Thibault Févry, Trishala Neeraj, Urmish Thakker,
Vikas Raunak, Xiang Tang, Zheng Xin Yong, Zhiqing
Sun, Shaked Brody, Y Uri, Hadar Tojarieh, Adam
Roberts, Hyung Won Chung, Jaesung Tae, Jason
Phang, Ofir Press, Conglong Li, Deepak Narayanan,
Hatim Bourfoune, Jared Casper, Jeff Rasley,
Max Ryabinin, Mayank Mishra, Minjia Zhang,
Mohammad Shoeybi, Myriam Peyrounette, Nicolas
Patry, Nouamane Tazi, Omar Sanseviero, Patrick von
Platen, Pierre Cornette, Pierre Franccois Lavall’ee,
Rémi Lacroix, Samyam Rajbhandari, Sanchit
Gandhi, Shaden Smith, Stéphane Requena, Suraj
Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet
Singh, Anastasia Cheveleva, Anne-Laure Ligozat,
Arjun Subramonian, Aur’elie N’ev’eol, Charles
Lovering, Daniel H Garrette, Deepak R. Tunuguntla,
Ehud Reiter, Ekaterina Taktasheva, Ekaterina
V oloshina, Eli Bogdanov, Genta Indra Winata,
Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina
Novikova, Jessica Zosa Forde, Xiangru Tang, Jungo
Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat,
Miruna Clinciu, Najoung Kim, Newton Cheng,
Oleg Serikov, Omer Antverg, Oskar van der Wal,
Rui Zhang, Ruochen Zhang, Sebastian Gehrmann,
Shachar Mirkin, S. Osher Pais, Tatiana Shavrina,
Thomas Scialom, Tian Yun, Tomasz Limisiewicz,
Verena Rieser, Vitaly Protasov, Vladislav Mikhailov,
Yada Pruksachatkun, Yonatan Belinkov, Zachary
Bamberger, Zdenvek Kasner, Alice Rueda, Amanda
Pestana, Amir Feizpour, Ammar Khan, Amy
Faranak, Ananda Santa Rosa Santos, Anthony
Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo
Abdollahi, Aycha Tammour, Azadeh HajiHosseini,
Bahareh Behroozi, Benjamin Olusola Ajibade,
Bharat Kumar Saxena, Carlos Muñoz Ferrandis,
Danish Contractor, David M. Lansky, Davis David,
Douwe Kiela, Duong Anh Nguyen, Edward Tan,
Emily Baylor, Ezinwanne Ozoani, Fatim T Mirza,
Frankline Ononiwu, Habib Rezanejad, H.A. Jones,
Indrani Bhattacharya, Irene Solaiman, Irina Sedenko,
Isar Nejadgholi, Jan Passmore, Joshua Seltzer,
Julio Bonis Sanz, Karen Fort, Lívia Macedo
Dutra, Mairon Samagaio, Maraim Elbadri, Margot
Mieskes, Marissa Gerchick, Martha Akinlolu,
Michael McKenna, Mike Qiu, M. K. K. Ghauri,
Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour
Elkott, Nourhan Fahmy, Olanrewaju Samuel, Ran
An, R. P. Kromann, Ryan Hao, Samira Alizadeh,
Sarmad Shubber, Silas L. Wang, Sourav Roy, Sylvain
Viguier, Thanh-Cong Le, Tobi Oyebade, Trieu
Nguyen Hai Le, Yoyo Yang, Zachary Kyle Nguyen,
Abhinav Ramesh Kashyap, A. Palasciano, Alison
Callahan, Anima Shukla, Antonio Miranda-Escalada,
Ayush Kumar Singh, Benjamin Beilharz, Bo Wang,
Caio Matheus Fonseca de Brito, Chenxi Zhou, Chirag
Jain, Chuxin Xu, Clémentine Fourrier, Daniel Le’on
Perin’an, Daniel Molano, Dian Yu, Enrique
Manjavacas, Fabio Barth, Florian Fuhrimann,
Gabriel Altay, Giyaseddin Bayrak, Gully Burns,
Helena U. Vrabec, Iman I.B. Bello, Isha Dash, Ji Soo
Kang, John Giorgi, Jonas Golde, Jose David Posada,
Karthi Sivaraman, Lokesh Bulchandani, Lu Liu,
Luisa Shinzato, Madeleine Hahn de Bykhovetz,
Maiko Takeuchi, Marc Pàmies, María Andrea
Castillo, Marianna Nezhurina, Mario Sanger,
Matthias Samwald, Michael Cullan, Michael
Weinberg, M Wolf, Mina Mihaljcic, Minna
Liu, Moritz Freidank, Myungsun Kang, Natasha
Seelam, Nathan Dahlberg, Nicholas Michio Broad,
Nikolaus Muellner, Pascale Fung, Patricia Haller,
R. Chandrasekhar, R. Eisenberg, Robert Martin,
Rodrigo L. Canalli, Rosaline Su, Ruisi Su, Samuel
Cahyawijaya, Samuele Garda, Shlok S Deshmukh,
Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee
Sang-aroonsiri, Srishti Kumar, Stefan Schweter,
Sushil Pratap Bharati, T. A. Laud, Th’eo Gigant,
Tomoya Kainuma, Wojciech Kusa, Yanis Labrak,
Yashasvi Bajaj, Y . Venkatraman, Yifan Xu, Ying
Xu, Yun chao Xu, Zhee Xao Tan, Zhongli Xie,
Zifan Ye, Mathilde Bras, Younes Belkada, and
Thomas Wolf. 2022. Bloom: A 176b-parameter
open-access multilingual language model. ArXiv,
abs/2211.05100.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proximal
policy optimization algorithms. arXiv preprint
arXiv:1707.06347.
Noam Shazeer. 2020. Glu variants improve transformer.
arXiv preprint arXiv:2002.05202.
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 22):

Published as a conference paper at ICLR 2024
HotpotQA 6-shot, and 127 out of 1,000 instances for FEVER 3-shot. The plain ReAct method, on
the other hand, applies the ReAct prompt to every instance.
23



### Claim 27/36

#### Claim Text
Alternative force-based immersed boundary methods in an LB context have been proposed by e.g. .

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 0):

Baichuan 2: Open Large-scale Language Models
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan
Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai
Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji
Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma
Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun
Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng
Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang
Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, Zhiying Wu
Baichuan Inc.
Abstract
Large language models (LLMs) have
demonstrated remarkable performance on
a variety of natural language tasks based
on just a few examples of natural language
instructions, reducing the need for extensive
feature engineering. However, most powerful
LLMs are closed-source or limited in their
capability for languages other than English. In
this technical report, we present Baichuan 2,
a series of large-scale multilingual language
models containing 7 billion and 13 billion
parameters, trained from scratch, on 2.6 trillion
tokens. Baichuan 2 matches or outperforms
other open-source models of similar size on
public benchmarks like MMLU, CMMLU,
GSM8K, and HumanEval. Furthermore,
Baichuan 2 excels in vertical domains such
as medicine and law. We will release all
pre-training model checkpoints to benefit the
research community in better understanding
the training dynamics of Baichuan 2.
1 Introduction
The field of large language models has witnessed
promising and remarkable progress in recent years.
The size of language models has grown from
millions of parameters, such as ELMo (Peters
et al., 2018), GPT-1 (Radford et al., 2018), to
billions or even trillions of parameters such as GPT-
3 (Brown et al., 2020), PaLM (Chowdhery et al.,
2022; Anil et al., 2023) and Switch Transformers
(Fedus et al., 2022). This increase in scale has
led to significant improvements in the capabilities
of language models, enabling more human-like
fluency and the ability to perform a diverse range
of natural language tasks. With the introduction of
Authors are listed alphabetically, correspondent:
daniel@baichuan-inc.com.
ChatGPT (OpenAI, 2022) from OpenAI, the power
of these models to generate human-like text has
captured widespread public attention. ChatGPT
demonstrates strong language proficiency across
a variety of domains, from conversing casually to
explaining complex concepts. This breakthrough
highlights the potential for large language models
to automate tasks involving natural language
generation and comprehension.
While there have been exciting breakthroughs
and applications of LLMs, most leading LLMs like
GPT-4 (OpenAI, 2023), PaLM-2 (Anil et al., 2023),
and Claude (Claude, 2023) remain closed-sourced.
Developers and researchers have limited access to
the full model parameters, making it difficult for
the community to deeply study or fine-tune these
systems. More openness and transparency around
LLMs could accelerate research and responsible
development within this rapidly advancing field.
LLaMA (Touvron et al., 2023a), a series of large
language models developed by Meta containing up
to 65 billion parameters, has significantly benefited
the LLM research community by being fully open-
sourced. The open nature of LLaMA, along with
other open-source LLMs such as OPT (Zhang
et al., 2022), Bloom (Scao et al., 2022), MPT
(MosaicML, 2023) and Falcon (Penedo et al.,
2023), enables researchers to freely access the
models for examination, experimentation, and
further development. This transparency and access
distinguishes LLaMA from other proprietary
LLMs. By providing full access, the open-source
LLMs have accelerated research and advances in
the field, leading to new models like Alpaca (Taori
et al., 2023), Vicuna (Chiang et al., 2023), and
others (Wang et al., 2022; Zhu et al., 2023; Anand
et al., 2023).
arXiv:2309.10305v2  [cs.CL]  20 Sep 2023



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 2):

3  
Introduction Large language models (LLM) have shown impressive performance in solving complex tasks across various domains that involve language modeling and processing.(Zhao et al. 2023) LLMs are pre-trained on a large corpora of text data in a self-supervised learning framework which can be either Masked Language Modeling (e.g. BERT like models)(Devlin et al. 2018; Liu et al. 2019) or Auto-Regressive framework (GPT like models)(Brown et al. 2020; Luo et al. 2022). This pre-training encodes knowledge about the language in the model parameters. Similar to the transfer learning approach commonly used in deep neural networks, this implicit knowledge can be refined through supervised training to excel in a range of domain-specific tasks.(Wei et al. 2021; Luo et al. 2022) Nevertheless, the “implicit representation” of knowledge in LLM has also been shown to generate non-factual information despite linguistically coherent answers (i.e. “hallucination”) as a response to the input prompt.(Maynez et al. 2020; Raunak, Menezes and Junczys-Dowmunt 2021; Ji et al. 2022) This issue poses a significant challenge for the adoption of LLM models in domains with stringent requirements for accuracy, such as biomedicine.  Various strategies have been introduced to address hallucinations in LLMs. One such solution involves the utilization of domain-specific data for pre-training the LLM, rather than relying on generic text corpora. This approach has led to the creation of models such as PubMedBERT(Yu Gu Microsoft Research, Redmond, WA et al. 2021), BioBERT(Lee et al. 2019), BlueBERT(Lee et al. 2019; Peng, Yan and Lu 2019), SciBERT(Beltagy, Lo and Cohan 2019), ClinicalBERT(Huang, Altosaar and Ranganath 2019), BioGPT(Luo et al. 2022), Med-PaLM(Singhal et al. 2023), and BioMedGPT.(Luo et al. 2023) However, pre-training an LLM from scratch imposes a significant computational cost and time overhead to attain the desired human-like performance. An alternative approach, known as prompt tuning, was recently proposed as a means to enhance LLM performance, for instance through the use of zero-



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 16):

Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson,
Itziar Gonzalez-Dios, Javier de la Rosa, Jenny
Chim, Jesse Dodge, Jian Zhu, Jonathan Chang,
Jorg Frohberg, Josephine L. Tobing, Joydeep
Bhattacharjee, Khalid Almubarak, Kimbo Chen,
Kyle Lo, Leandro von Werra, Leon Weber, Long
Phan, Loubna Ben Allal, Ludovic Tanguy, Manan
Dey, Manuel Romero Muñoz, Maraim Masoud,
Mar’ia Grandury, Mario vSavsko, Max Huang,
Maximin Coavoux, Mayank Singh, Mike Tian-
Jian Jiang, Minh Chien Vu, Mohammad Ali
Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora
Kassner, Nurulaqilla Khamis, Olivier Nguyen,
Omar Espejel, Ona de Gibert, Paulo Villegas,
Peter Henderson, Pierre Colombo, Priscilla A.
Amuok, Quentin Lhoest, Rheza Harliman, Rishi
Bommasani, Roberto L’opez, Rui Ribeiro, Salomey
Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose,
Shamsuddeen Hassan Muhammad, Shanya Sharma,
S. Longpre, Somaieh Nikpoor, Stanislav Silberberg,
Suhas Pai, Sydney Zink, Tiago Timponi Torrent,
Timo Schick, Tristan Thrush, Valentin Danchev,
Vassilina Nikoulina, Veronika Laippala, Violette
Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak
Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si,
Elizabeth Salesky, Sabrina J. Mielke, Wilson Y . Lee,
Abheesht Sharma, Andrea Santilli, Antoine Chaffin,
Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla,
Gunjan Chhablani, Han Wang, Harshit Pandey,
Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo
Gao, Lintang Sutawika, M Saiful Bari, Maged S.
Al-shaibani, Matteo Manica, Nihal V . Nayak, Ryan
Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-
David, Stephen H. Bach, Taewoon Kim, Tali Bers,
Thibault Févry, Trishala Neeraj, Urmish Thakker,
Vikas Raunak, Xiang Tang, Zheng Xin Yong, Zhiqing
Sun, Shaked Brody, Y Uri, Hadar Tojarieh, Adam
Roberts, Hyung Won Chung, Jaesung Tae, Jason
Phang, Ofir Press, Conglong Li, Deepak Narayanan,
Hatim Bourfoune, Jared Casper, Jeff Rasley,
Max Ryabinin, Mayank Mishra, Minjia Zhang,
Mohammad Shoeybi, Myriam Peyrounette, Nicolas
Patry, Nouamane Tazi, Omar Sanseviero, Patrick von
Platen, Pierre Cornette, Pierre Franccois Lavall’ee,
Rémi Lacroix, Samyam Rajbhandari, Sanchit
Gandhi, Shaden Smith, Stéphane Requena, Suraj
Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet
Singh, Anastasia Cheveleva, Anne-Laure Ligozat,
Arjun Subramonian, Aur’elie N’ev’eol, Charles
Lovering, Daniel H Garrette, Deepak R. Tunuguntla,
Ehud Reiter, Ekaterina Taktasheva, Ekaterina
V oloshina, Eli Bogdanov, Genta Indra Winata,
Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina
Novikova, Jessica Zosa Forde, Xiangru Tang, Jungo
Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat,
Miruna Clinciu, Najoung Kim, Newton Cheng,
Oleg Serikov, Omer Antverg, Oskar van der Wal,
Rui Zhang, Ruochen Zhang, Sebastian Gehrmann,
Shachar Mirkin, S. Osher Pais, Tatiana Shavrina,
Thomas Scialom, Tian Yun, Tomasz Limisiewicz,
Verena Rieser, Vitaly Protasov, Vladislav Mikhailov,
Yada Pruksachatkun, Yonatan Belinkov, Zachary
Bamberger, Zdenvek Kasner, Alice Rueda, Amanda
Pestana, Amir Feizpour, Ammar Khan, Amy
Faranak, Ananda Santa Rosa Santos, Anthony
Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo
Abdollahi, Aycha Tammour, Azadeh HajiHosseini,
Bahareh Behroozi, Benjamin Olusola Ajibade,
Bharat Kumar Saxena, Carlos Muñoz Ferrandis,
Danish Contractor, David M. Lansky, Davis David,
Douwe Kiela, Duong Anh Nguyen, Edward Tan,
Emily Baylor, Ezinwanne Ozoani, Fatim T Mirza,
Frankline Ononiwu, Habib Rezanejad, H.A. Jones,
Indrani Bhattacharya, Irene Solaiman, Irina Sedenko,
Isar Nejadgholi, Jan Passmore, Joshua Seltzer,
Julio Bonis Sanz, Karen Fort, Lívia Macedo
Dutra, Mairon Samagaio, Maraim Elbadri, Margot
Mieskes, Marissa Gerchick, Martha Akinlolu,
Michael McKenna, Mike Qiu, M. K. K. Ghauri,
Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour
Elkott, Nourhan Fahmy, Olanrewaju Samuel, Ran
An, R. P. Kromann, Ryan Hao, Samira Alizadeh,
Sarmad Shubber, Silas L. Wang, Sourav Roy, Sylvain
Viguier, Thanh-Cong Le, Tobi Oyebade, Trieu
Nguyen Hai Le, Yoyo Yang, Zachary Kyle Nguyen,
Abhinav Ramesh Kashyap, A. Palasciano, Alison
Callahan, Anima Shukla, Antonio Miranda-Escalada,
Ayush Kumar Singh, Benjamin Beilharz, Bo Wang,
Caio Matheus Fonseca de Brito, Chenxi Zhou, Chirag
Jain, Chuxin Xu, Clémentine Fourrier, Daniel Le’on
Perin’an, Daniel Molano, Dian Yu, Enrique
Manjavacas, Fabio Barth, Florian Fuhrimann,
Gabriel Altay, Giyaseddin Bayrak, Gully Burns,
Helena U. Vrabec, Iman I.B. Bello, Isha Dash, Ji Soo
Kang, John Giorgi, Jonas Golde, Jose David Posada,
Karthi Sivaraman, Lokesh Bulchandani, Lu Liu,
Luisa Shinzato, Madeleine Hahn de Bykhovetz,
Maiko Takeuchi, Marc Pàmies, María Andrea
Castillo, Marianna Nezhurina, Mario Sanger,
Matthias Samwald, Michael Cullan, Michael
Weinberg, M Wolf, Mina Mihaljcic, Minna
Liu, Moritz Freidank, Myungsun Kang, Natasha
Seelam, Nathan Dahlberg, Nicholas Michio Broad,
Nikolaus Muellner, Pascale Fung, Patricia Haller,
R. Chandrasekhar, R. Eisenberg, Robert Martin,
Rodrigo L. Canalli, Rosaline Su, Ruisi Su, Samuel
Cahyawijaya, Samuele Garda, Shlok S Deshmukh,
Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee
Sang-aroonsiri, Srishti Kumar, Stefan Schweter,
Sushil Pratap Bharati, T. A. Laud, Th’eo Gigant,
Tomoya Kainuma, Wojciech Kusa, Yanis Labrak,
Yashasvi Bajaj, Y . Venkatraman, Yifan Xu, Ying
Xu, Yun chao Xu, Zhee Xao Tan, Zhongli Xie,
Zifan Ye, Mathilde Bras, Younes Belkada, and
Thomas Wolf. 2022. Bloom: A 176b-parameter
open-access multilingual language model. ArXiv,
abs/2211.05100.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proximal
policy optimization algorithms. arXiv preprint
arXiv:1707.06347.
Noam Shazeer. 2020. Glu variants improve transformer.
arXiv preprint arXiv:2002.05202.
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,



Source: data\tc17_2312.15883v2\referenced_papers\[77]_2308.08833.pdf (Page 14):

Dataset Top 5 Medicine-Related Keywords
CMB-TCM 气血(Qi and blood),舌苔(tongue coating),虚证(deficiency syndrome),病程(course of disease),舌淡(pale tongue)
Medqa-USMLEblood pressure, respiratory rate，physical examination, heart rate, abdominal pain
Table 10: Keywords in the TCM subcatalog and Medqa-USMLE
慢性呼吸衰竭患者机械通气的指征，错误的是 ____
A. PaCO2进行性升高
B. PaCO2升高达到80mmHg以上
C. 严重的低氧血症，积极氧疗后，PaO2仍小于40mmHg
D. 呼吸频率超过35次/分
E. 并发肺性脑病
答案：B
解释：慢性呼吸衰竭是在原有肺部疾病基础上发生的，最常见病因为COPD，早期可表现为Ⅰ型呼吸衰竭，随着病情逐渐加重，肺功能愈来愈差，可表现为Ⅱ型呼
吸衰竭。慢性呼吸衰竭稳定期，虽PaCO2降低和PaCO2升高，但患者通过代偿和治疗，可稳定在一定范围内，患者仍能从事一般的工作或日常生活活动。一旦由于
呼吸道感染加重或其他诱因，可表现为PaCO2明显下降，PaCO2显著升高，此时可称为慢性呼吸衰竭的急性发作，这是我国临床上最常见的慢性呼吸衰竭类型。长
期家庭氧疗应在极重度慢阻肺患者中应用，具体指征：①PaO2小于等于55mmHg或SaO2小于等于88%，有或无高碳酸血症；②PaO2为55～60mmHg或SaO2为0.55。长期
家庭氧疗一般是经鼻导管吸入氧气，流量1.0～2.0L/分，每日吸氧持续时间大于15小时。故选B。
Indications for mechanical ventilation in patients with chronic respiratory failure, the incorrect one is ____ 
Progressive increase in PaCO2 
PaCO2 increases to above 80mmHg 
Severe hypoxemia, after aggressive oxygen therapy, PaO2 is still less than 40mmHg
Respiratory rate exceeds 35 times/minute 
Complicated with pulmonary encephalopathy
Answer: B
Explanation: Chronic respiratory failure occurs on the basis of existing lung diseases, with the most common cause being COPD. Early stages may manifest as 
Type I respiratory failure. As the condition progressively worsens and lung function deteriorates, it can manifest as Type II respiratory failure. During the stable 
period of chronic respiratory failure, although PaCO2 decreases and PaCO2 increases, patients can stabilize within a certain range through compensation and 
treatment, and they can still engage in general work or daily life activities. Once exacerbated due to respiratory infections or other triggers, it can manifest as a 
significant drop in PaCO2 and a significant increase in PaCO2, which can be referred to as an acute exacerbation of chronic respiratory failure, the most 
common type of chronic respiratory failure in our country's clinical practice. Long-term home oxygen therapy should be applied in patients with severe COPD, 
with specific indications: ①PaO2 less than or equal to 55mmHg or SaO2 less than or equal to 88%, with or without hypercapnia; ②PaO2 is 55～60mmHg or 
SaO2 is 0.55. Long-term home oxygen therapy is generally inhaled through a nasal cannula, with a flow rate of 1.0～2.0L/min, and the duration of oxygen 
inhalation is more than 15 hours per day. Therefore, the answer is B.
Figure 4: An development example with explanations in CMB-Exam. English translations are shown for better
readability.
Publicly-available general-domain LLMs:
• ChatGLM-3: The third version of ChatGLM,
which is an open source, bilingual dialogue
language model.
• Baichuan2-chat: An advanced variant of
Baichuan-13B model, focuses on dialogue
tasks, boasting 13 billion parameters for ef-
ficient and effective conversation generation.
The number of parameters of the model is 7B
and 13B.
• Qwen-Chat: Tongyi Qianwen large model se-
ries developed by Alibaba Cloud based on
Transformer, which is trained on extremely
large-scale pre-training data. The number of
parameters of the model is 1.8B, 7B and 72B.
• Yi: Large language models trained from
scratch by developers at 01.AI 13. The num-
ber of parameters of the model is 6B and 34B.
• Deepseek-llm-67B-Chat: An advanced lan-
guage model comprising 67 billion parame-
13https://www.lingyiwanwu.com/
ters. It has been trained from scratch on a vast
dataset of 2 trillion tokens in both English and
Chinese.
• Internlm-Chat: It’s pre-trained on over 2.3T
Tokens containing high-quality English, Chi-
nese, and code data. Chat version has under-
gone SFT and RLHF training, enabling it to
better and more securely meet users’ needs.
The number of parameters of the model is 7B
and 20B.
• Mistral14: A 7B dense Transformer, fast-
deployed and easily customisable. Small, yet
powerful for a variety of use cases. Supports
English and code, and a 8k context window.
• Mixtral-8x7B-32kseqlen: A 7B sparse
Mixture-of-Experts model with stronger ca-
pabilities than Mistral 7B. Uses 12B active
parameters out of 45B total. Supports multi-
ple languages, code and 32k context window.
14https://mistral.ai/



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 12):

sensitive topicsdiscriminationprofanity unethical contentphysical healthmental healthfinancial privacyAverage
ChatGLM 2-6B 61.80% 96.40% 99.10% 97.31% 100.00% 98.23% 97.34% 93.01%
Vicuna 13B 61.00% 98.03% 99.10% 98.32% 99.80% 99.40% 98.50% 93.58%
LLaMA 2 7B-chat 51.90% 95.23% 98.23% 97.25% 99.60% 98.23% 95.34% 90.83%
LLaMA 2 13B-chat 53.40% 98.27% 99.04% 97.25% 100.00% 99.80% 97.79% 92.25%
Chinese Alpaca 2-13B 53.20% 96.34% 93.17% 85.12% 99.60% 99.31% 96.53% 89.04%
Baichuan 2-7B-chat 78.20% 96.00% 99.10% 97.12% 100.00% 99.80% 96.84% 95.45%
Baichuan 2-13B-chat 87.10% 98.97% 99.10% 98.36% 100.00% 99.80% 98.12% 97.50%
Table 9: The result of different chat models on our safety evaluation benchmarks.
表格 1
Tokens C-Eval val 5-
shot MMLU 5-shot CMMLU 5-shot
220 23.61 24.04 25.69
440 31.25 33.04 37.29
660 44.17 46.45 47.04
880 48.33 46.51 48.24
1100 48.43 48.32 49.75
1320 51.25 50.04 51.51
1540 51.43 50.98 52.30
1760 53.79 52.15 55.16
1980 54.77 52.98 56.18
2200 56.75 53.32 56.61
2420 55.08 53.35 56.81
2640 54.00 54.16 57.07
20
30
40
50
60
Baichuan 2-7B Checkpoints (in billions of tokens)
220 440 660 880 1100 1320 1540 1760 1980 2200 2420 2640
C-Eval 5-shot
 MMLU 5-shot
 CMMLU 5-shot
1
Figure 7: The results of intermediary checkpoints of
Baichuan 2-7B which will be released to the public.
Transformers (Vaswani et al., 2017). Kaplan et al.
(2020) proposed the scaling laws for large model
pre-training. By systematically analyzing model
performance as parameters and data size increased,
they provided a blueprint for the current era of
massive models with hundreds of or even billions
of parameters.
Seizing upon these scaling laws, organizations
like OpenAI, Google, Meta, and Anthropic have
engaged in a computing arms race to create ever-
larger LLMs. Spurred by the OpenAI’s 175
billion parameters proprietary language model
GPT-3 (Brown et al., 2020). The few-shot or
even zero-shot ability of LLMs has revolved most
natural language understanding tasks. From code
generation to math-solving problems or even open-
world scenarios. Specialized scientific LLMs
like Galactica (Taylor et al., 2022) have also
emerged to showcase the potential for large models
to assimilate technical knowledge. However,
raw parameter count alone does not determine
model capability - Chinchilla (Hoffmann et al.,
2022) demonstrated that scaling model capacity
according to the number of tokens, rather than just
parameters, can yield better sample efficiency.
Concurrent with the development of private
LLMs, academic and non-profit efforts have
worked to develop open-source alternatives like
Bloom (Scao et al., 2022), OPT (Zhang et al., 2022)
and Pythia (Biderman et al., 2023b). Although
some open-source large language models contain
up to 175 billion parameters, most are trained on
only 500 billion tokens or less. This is relatively
small considering that 7 billion parameter models
can still significantly improve after being trained
on trillions of tokens. Among those open-sourced
models, LLaMA (Touvron et al., 2023b) and its
successor LLaMA 2 (Touvron et al., 2023c) stands
out for its performance and transparency. Which
was quickly optimized by the community for better
inference speed and various applications.
In addition to those foundation models, a lot
of chat models have also been proposed to follow
human instructions. Most of them fine-tune the
foundation models to align with human (OpenAI,
2022; Wang et al., 2023). Those chat models
have demonstrated a marked improvement in
understanding human instructions and solving
complex tasks (Chiang et al., 2023; Xu et al.,
2023; Sun et al., 2023). To further improve
alignment, (Ouyang et al., 2022) incorporates the
Reinforcement Learning from Human Feedback
(RLHF) approach. This involves learning from
human preferences by training a reward model
on human-rated outputs. Other methods such as
direct preference optimization (DPO) (Rafailov
et al., 2023) and reinforcement learning from AI
feedback (RLAIF) (Bai et al., 2022b) have also
been proposed to improve the RLHF both in terms
of efficiency and effectiveness.



### Claim 28/36

#### Claim Text
To reproduce a beating frequency close to 20 · 2π rad/s – which is the one shown in – we adjust the internal friction λ to 7 pN · s/µm2 (which is comparable to the one from ).

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[45]_2308.03281.pdf (Page 4):

Task Type Text Pair Format Query Doc
Web Page (title, body) Providence Real Estate | Providence Homes for SaleFounded by Roger Williams in 1636, Providence isrecognized as one of the country’s oldest cities. . .
Academic Paper(title, abstract) Polymer Quantum Mechanics and its Continuum LimitA rather non-standard quantum representation of thecanonical commutation relations of quantum mechanics. . .
Hyperlink (citation, reference)After the championship in 1996, the PGA of Americaraised its stake to 50% and announced that . . .Pebble Beach Golf Links The largest margin of victoryever in a major championship, surpassing the 13-shot . . .
Social Media (post, comment)Pretty sure any team with Lebron James will be a playoffcontender. Considering UNC would be in the East. . .I was being sarcastic and making fun of the East, buthonestly I was really in deep thought about this . . .
Knowledge Base(entity, description) Animation Animation is the process of creating the illusion of motionand shape change by means of the rapid display of . . .
Community QA (question, answer)How the human species evolved? A tough question as it overlaps science and theology. Sinceyou asked “how the human species evolved?” I’ll assume . . .
News (summary, content) Nepalese Opposition Welcomes Return of ParliamentNepal’s opposition alliance formally calls off weeks ofpro-democracy protests after King Gyenandra reinstates . . .
Code (text, code) SetMaxRecords sets the MaxRecords field’s value.func (s *DescribeSnapshotCopyGrantsInput) SetMaxRecords(v int64) *DescribeSnapshotCopyGrantsInput { s.MaxRecords
Table 2: Examples of mined (query, document) pairs in the pre-training data.
Consider a batch of positive text pair samples
B = {(q1, d1), (q2, d2), ...,(qn, dn)},
we use an improved contrastive loss which takes
the form
Licl = −1
n
nX
i=1
log es(qi,di)/τ
Z (5)
with the partition function being
Z =
X
j
es(qi,dj )/τ +
X
j̸=i
es(qi,qj )/τ
+
X
j
es(qj ,di)/τ +
X
j̸=i
es(dj ,di)/τ (6)
in which the first two terms are used for query to
document contrast, where as the last two terms are
used for the inverse. In this work, we use the cosine
similarity as the distance metric
s(q, d) = q · d
||q||2 · ||d||2
. (7)
The temperature τ is fixed to 0.01 in this work.
Training and Evaluation The training of our
embedding model consists of two stages. In the
first stage of contrastive pre-training with only in-
batch negatives, using a large batch size is crucial
to better model performance by reducing the gap
between training and inference with more nega-
tives included and providing a better approximation
to the underlying learning objective. To facilitate
this, we limit the maximum sequence length to 128
during pre-training and distribute the use of nega-
tives across all GPUs. Popular techniques such as
automatic mixed precision training (Micikevicius
et al., 2018) with fp16, deepspeed ZeRO (Rajb-
handari et al., 2020) stage 1 and gradient check-
pointing (Chen et al., 2016) are also jointly used to
reduce memory cost and scale up batch size to over
ten thousands. We run the pre-training for 50, 000
steps, which roughly corresponds to one epoch on
the whole pre-training data. We only tuned the
learning rate to ensure the convergence of larger
models. we employ the AdamW optimizer with
linear learning rate decay and a warm-up period
during the initial 5% of training steps. We con-
ducted experiments on three distinct model scales:
small, base, and large. These models were initial-
ized using the small-sized MiniLM (Wang et al.,
2020) model and the base and large models of the
BERT (Devlin et al., 2019) model. Further details
can be found in Table 3.
In the second stage of contrastive fine-tuning
with supervised data and hard negatives, a large
batch size is unnecessary since hard negatives can
already provide a reliable gradient estimation of
the learning objective (Xiong et al., 2021; Li et al.,
2023). Therefore, a global batch size of 128 and a
train group size of 16 are utilized, with one positive
example and the remaining being either hard nega-
tives or random negatives. Instead we increase the
max sequence length to 512 to better handle texts
with longer lengths. The learning rate is decreased
by a factor of ten during fine-tuning. The model
is fine-tuned on the collected dataset for a single
epoch. In-batch texts are also incorporated as nega-
tive candidates using the enhanced contrastive loss
described in Equation 5.
After training, we directly take the last check-
point for evaluation. We run model training on up
to 8 NVIDIA A100 GPUs with 80GB memory and
model evaluation on up to 8 NVIDIA Tesla V100
GPUs with 32GB memory. Models are trained with
mixed precision using fp16 and evaluated with half
precision fp16 as well.



Source: data\tc17_2312.15883v2\referenced_papers\[77]_2308.08833.pdf (Page 19):

Use via API · Built with Gradio
上一题 下一题 
当前进度： 334/340
病例资料 男性病人， 30 岁，体重 65kg ，身高 170cm ，术 前心脏超声提示：二尖瓣中重度关闭不全； 拟在体外循环下行二尖瓣瓣置换术，常规开 胸，建立体外循环，降温，灌注冷血 4 ∶ 1 心脏 停搏液进行心肌保护，体外循环时间 97 分 钟，升主动脉阻断时间 64 分钟，顺利停机。 
体外循环实施 1. 胸骨正中开胸，全身肝素化，肝素使 用量？ 如何监测抗凝？ 2. 需要哪些体外循环耗材？ 3. 体外循环维持温度是多少？ 4. 撤离体外循环的指标有哪些？ 
体外循环实施答案 1. 胸骨正中开胸，全身肝素化，肝素使 用量？ 如何监测抗凝？ 素化 全血活化凝血时 间 
该病人心肌保护是使用细胞外液型 
还是细胞内液型心脏停搏液？ 
细胞外液型心脏停搏液。 
心脏停搏液的灌注方法选择哪种？ 
经主动脉顺行灌注。 
请说明心肌保护的作用机制。 
### 回答 :心肌保护的作用机制主要包括以下几个方 面： 
1. 降低心肌代谢率：心肌保护液中的成分可 以降低心肌的代谢率，减少对氧和能量的需 求，从而减轻心肌的负担。 
2. 抗氧化作用：心肌保护液中的抗氧化剂可 以清除自由基，减少氧化应激对心肌细胞的 损伤，保护心肌细胞的结构和功能。 
3. 抗炎作用：心肌保护液中的抗炎成分可以 抑制炎症反应，减少炎症介质的释放，降低 心肌组织的炎症损伤。 
4. 细胞膜稳定作用：心肌保护液中的成分可 以增强细胞膜的稳定性，减少细胞膜的通透 性，防止细胞内外物质的交换和损伤。 
①使用高钾化学诱导方法，使心脏迅速停 搏，避免电机械活动，减少能量消耗。 ②降低心脏温度，减缓心肌代谢率，保存能 量储备。常用 4 ℃心脏停搏液灌注，成人予冰 屑、儿童予冰水心包腔局部降温。 ③提供氧和能量底物，维持心脏缺血期间和 恢复灌注后所需的能量物质 
1 2
3 4
5
1 2
3 4
5
1 2
3 4
5
1 2
3 4
5
对话 
进度条 
病史描述 题目 
模型回答 
参考答案 
流畅性 相关性 完整性 医学知识专业性 
Figure 7: The user interface for scoring an answer (in Chinese). Note that Figure 7 follows Figure 6 in the same
webpage.



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 6):

GPUs, achieving a computational efficiency that
exceeds 180 TFLOPS.
3 Alignment
Baichuan 2 also introduces the alignment
procedure resulting in two chat models: Baichuan
2-7B-Chat and Baichuan 2-13B-Chat. The
alignment process of the Baichuan 2 encompasses
two main components: Supervised Fine-Tuning
(SFT) and Reinforcement Learning from Human
Feedback (RLHF).
3.1 Supervised Fine-Tuning
During the supervised fine-tuning phase, we use
human labelers to annotate prompts gathered from
various data sources. Each prompt is labeled as
being helpful or harmless based on key principles
similar to Claude (2023). To validate data quality,
we use cross-validation—an authoritative annotator
checks the quality of a sample batch annotated by a
specific crowd worker group, rejecting any batches
that do not meet our quality standards.
We collected over 100k supervised fine-tuning
samples and trained our base model on them. Next,
we delineated the reinforcement learning process
via the RLHF method to further improve results.
The whole process of RLHF, including RM and RL
training, is shown in Figure 5.
Figure 5: An illustration of Baichuan 2’s RLHF process.
3.2 Reward Model
We devised a three-tiered classification system for
all prompts, consisting of 6 primary categories,
30 secondary categories, and over 200 tertiary
categories. From the user’s perspective, we aim
for the classification system to comprehensively
cover all types of user needs. From the standpoint
of reward model training, prompts within each
Score Gap 1 2 3 4 5
Test Acc. 54.5% 61.1% 70.2% 77.8% 81.5%
Table 4: Reward Model test accuracy on different score
gaps of two responses. The larger the response gap,
the better RM accuracy. The gap 1,2,3,4,5 correspond
to unsure, negligibly better, slightly better, better, and
significantly better, respectively.
category should have sufficient diversity to ensure
the reward model can generalize well.
Given a prompt, responses are generated by
Baichuan 2 models of different sizes and stages
(SFT, PPO) to enhance response diversity. Only
responses generated by the Baichuan 2 model
family are used in the RM training. Responses from
other open-source datasets and proprietary models
do not improve the reward model’s accuracy. This
also underscores the intrinsic consistency of the
Baichuan 2 model series from another perspective.
The loss function used for training the reward
model is consistent with that in InstructGPT
(Ouyang et al., 2022). The reward model
derived from training exhibits a performance
consistent with that of LLaMA 2 (Touvron et al.,
2023b), indicating that the greater the score
difference between two responses, the higher the
discriminative accuracy of the reward model, as
shown in Table 4.
3.3 PPO
After obtaining the reward model, we employ the
PPO (Schulman et al., 2017) algorithm to train
our language model. We employ four models: the
actor model (responsible for generating responses),
the reference model (used to compute the KL
penalty with fixed parameters), the reward model
(providing an overarching reward for the entire
response with fixed parameters), and the critic
model (designed to learn per-token values).
3.4 Training Details
During the RLHF training process, the critic model
is warmed up with an initial 20 training steps ahead.
Subsequently, both the critic and actor models are
updated via the standard PPO algorithm. For all
models, we use gradient clipping of 0.5, a constant
learning rate of 5e-6, and a PPO clip threshold
ϵ = 0.1. We set the KL penalty coefficient β =
0.2, decaying to 0.005 over steps. We train for
350 iterations for all our chat models, resulting in
Baichuan 2-7B-Chat and Baichuan 2-13B-Chat.



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 3):

Figure 2: The data processing procedure of Baichuan 2’s pre-training data.
Models positional embedding hidden size FFN size num heads num layers seq. length max LR
Baichuan 2-7B RoPE 4,096 11,008 32 32 4,096 2e-4
Baichuan 2-13B ALiBi 5,120 13,696 40 40 4,096 1.5e-4
Table 3: Model details of Baichuan 2.
do not add a dummy prefix as in Baichuan 1. We
split numbers into individual digits to better encode
numeric data. To handle code data containing extra
whitespaces, we add whitespace-only tokens to the
tokenizer. The character coverage is set to 0.9999,
with rare characters falling back to UTF-8 bytes.
We set the maximum token length to 32 to account
for long Chinese phrases. The training data for the
Baichuan 2 tokenizer comes from the Baichuan
2 pre-training corpus, with more sampled code
examples and academic papers to improve coverage
(Taylor et al., 2022). Table 2 shows a detailed
comparison of Baichuan 2’s tokenizer with others.
2.3.1 Positional Embeddings
Building on Baichuan 1, we adopt Rotary
Positional Embedding (RoPE) (Su et al., 2021)
for Baichuan 2-7B and ALiBi (Press et al.,
2021) for Baichuan 2-13B. ALiBi is a more
recent positional encoding technique that has
shown improved extrapolation performance.
However, most open-sourced models use RoPE for
positional embeddings, and optimized attention
implementations like Flash Attention (Dao et al.,
2022; Dao, 2023) are currently better suited to
RoPE since it is multiplication-based, bypassing
the need for passing attention_mask to the
attention operation. Nevertheless, in preliminary
experiments, the choice of positional embedding
did not significantly impact model performance.
To enable further research on bias-based and
multiplication-based attention, we apply RoPE on
Baichuan 2-7B and ALiBi on Baichuan 2-13B,
consistent with Baichuan 1.
2.4 Activations and Normalizations
We use SwiGLU (Shazeer, 2020) activation
function, a switch-activated variant of GLU
(Dauphin et al., 2017) which shows improved
results. However, SwiGLU has a “bilinear” layer
and contains three parameter matrices, differing
from the vanilla Transformer’s feed-forward layer
that has two matrices, so we reduce the hidden size
from 4 times the hidden size to 8
3 hidden size and
rounded to the multiply of 128.
For the attention layer of Baichuan 2, we
adopt the memory efficient attention (Rabe and
Staats, 2021) implemented by xFormers 2. By
leveraging xFormers’ optimized attention with
biasing capabilities, we can efficiently incorporate
ALiBi’s bias-based positional encoding while
reducing memory overhead. This provides
performance and efficiency benefits for Baichuan
2’s large-scale training.
We apply Layer Normalization (Ba et al., 2016)
to the input of the Transformer block which is more
robust to the warm-up schedule (Xiong et al., 2020).
In addition, we use the RMSNorm implementation
2https://github.com/facebookresearch/
xformers



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 26):

用户：鸡和兔在一个笼子里，共有26个头，68只脚，那么鸡有多少只，兔有多少只？
Baichuan 2-13B-Chat：我们可以用代数方法来解决这个问题。假设鸡的数量为x，兔子的
数量为y。根据题目给出的信息，我们可以得到以下两个方程：
1. x + y = 26 (头的总数) 2. 2x + 4y = 68 (脚的总数)
现在我们需要解这个二元一次方程组。首先从第一个方程中解出x：
x = 26 - y
然后将x的表达式代入第二个方程：
2(26 - y) + 4y = 68 52 - 2y + 4y = 68 2y = 16
接下来我们求解y的值：
y = 16 / 2 y = 8
现在我们得到了兔子的数量是8只。将y的值代入x的表达式求得鸡的数量：
x = 26 - 8 x = 18
所以鸡有18只，兔子有8只。
用户：以下天文学常识题目，哪一个是错误的? A.太阳系是指由太阳和围绕着它运行的
八大行星、矮行星、卫星、小行星带和彗星组成的一个行星系统。B.卫星是指绕行星
或其他天体运动的天体。C.彗星是指太阳系中一种较小的天体，其核心由冰和尘埃组
成。D.按一般的天体归类方法，月球属于行星。
Baichuan 2-13B-Chat：D.按一般的天体归类方法，月球属于行星是错误的。实际上，月球
是地球的卫星，不属于行星范畴。
用户：以下物理常识题目，哪一个是错误的?A.在自然环境下，声音在固体中传播速
度最快。B.牛顿第一定律:一个物体如果不受力作用，将保持静止或匀速直线运动的状
态。C.牛顿第三定律:对于每个作用力，都有一个相等而反向的反作用力。D.声音在空气
中的传播速度为1000m/s。
Baichuan 2-13B-Chat：本题考查物理基本概念和原理的理解和应用能力。A项：声音的传
播需要介质，在固体、液体和气体的三种介质中，声音在固体中的传播速度是最快的，
故A正确； B项：牛顿第一定律是惯性定律，其内容是：一个物体如果没有受到外力的
作用，要么保持静止状态，要么保持匀速直线运动状态，直到有外力迫使它改变运动状
态为止，故B正确； C项：牛顿第三定律的内容是：对于每一个作用力，都有一个大小
相等、方向相反的反作用力，这两个力分别作用在两个相互作用的物体上，故C错误；
D项：声音在空气中以波的形式传播，其传播速度与介质的温度、湿度等因素有关，一
般情况下，声音在空气中的传播速度约为340m/s左右，而不是1000m/s，故D错误。 因
此，本题的正确答案是D 。



### Claim 29/36

#### Claim Text
Under the wellmixed assumption, we introduce a mean-field (MF) method , which assumes that individuals of the same age are equivalent.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[93]_2311.09210.pdf (Page 6):

Models Noise NQ TriviaQA WebQ Average
Ratio EM F1 EM F1 EM F1 EM F1
Retrieve-Read 100% 34.28 41.74 55.30 61.67 29.58 46.34 39.72 49.92
+ CHAIN-OF-NOTE 41.83 49.58 64.30 70.00 36.85 53.07 47.66 57.55
(+7.55) (+7.84) (+9.00) (+8.33) (+7.27) (+6.73) (+7.94) (+7.63)
Retrieve-Read 80% 54.28 61.03 73.83 80.02 35.46 52.70 54.52 64.58
+ CHAIN-OF-NOTE 56.63 63.23 75.89 81.24 40.60 56.54 57.70 67.00
(+2.35) (+2.20) (+2.06) (+1.22) (+5.14) (+3.84) (+3.18) (+2.42)
Retrieve-Read 60% 61.44 67.94 78.44 83.65 37.01 54.16 58.96 68.58
+ CHAIN-OF-NOTE 63.43 69.33 78.79 84.07 41.26 56.91 61.16 70.10
(+1.99) (+1.39) (+0.35) (+0.42) (+4.25) (+2.75) (+2.20) (+1.52)
Retrieve-Read 40% 64.62 71.12 80.56 86.76 38.40 55.60 61.19 71.16
+ CHAIN-OF-NOTE 65.91 72.22 81.72 87.11 42.16 58.15 63.26 72.49
(+1.29) (+1.10) (+1.16) (+0.35) (+3.76) (+2.55) (+2.07) (+1.33)
Retrieve-Read 20% 67.21 73.69 81.73 87.89 39.95 56.66 62.96 72.75
+ CHAIN-OF-NOTE 70.00 76.08 82.86 88.24 44.36 60.13 65.74 74.82
(+2.79) (+2.39) (+1.13) (+0.35) (+4.41) (+3.47) (+2.78) (+2.07)
Retrieve-Read 0% 69.23 75.57 83.34 89.44 42.24 58.59 64.93 74.53
+ CHAIN-OF-NOTE 73.28 79.86 83.52 88.94 46.16 62.38 67.65 77.06
(+4.05) (+4.29) (+0.18) (-0.50) (+3.92) (+3.79) (+2.72) (+2.53)
Table 3: Evaluation on Noise Robustness. The backbone language model is LLaMa-2 7B. The CHAIN -OF-NOTE
framework shows superior performance compared to the standard RALM system, particularly notable at higher
noise ratios.We explain how we synthesize data with different noise ratios under real-world scenarios in§ 3.1.1.
Models↓ RealTimeQA
EM F1 RR
Retrieve-Read (Shi et al., 2023c)15.6 19.9 6.1
+ CHAIN-OF-NOTE (ours) 15.7 20.3 13.0
Table 4: Evaluation on Unknown Robustness. TheCON
shows better performance than standard RALM system.
3.3 Evaluation on Noise Robustness
As illustrated in Table 2, when faced with entirely
noisy documents, both the standard RALM and
our CHAIN -OF-NOTE enhanced RALM underper-
formed compared to the no-retrieval setting. This
suggests that RALMs can be misled by noisy infor-
mation, leading to more hallucinations.
Notably, equipping the model withCON enables
it to perform nearly as well as the baseline model
directly fine-tuned with QA pairs without retrieval,
showcasing its robustness to noise and its ability
to disregard irrelevant information. The CON ap-
proach is effective not only in fine-tuned, smaller-
sized models but also in large language models,
such as GPT-4, with adjustments made only to the
prompt. Besides, in comparison to the CHAIN -OF-
THOUGHT technique, commonly utilized in rea-
soning scenarios, CON presents a more efficient
strategy for retrieval-augmented settings, particu-
larly in addressing knowledge-intensive tasks.
Table 3 shows that RALM enhanced with CON
consistently outperforms the standard RALM, es-
pecially in scenarios with exclusively noisy doc-
uments. An average improvement of +7.9 in EM
score on fully noisy documents is observed on three
open-domain QA datasets, in average. Experiments
with lower noise ratios also consistently demon-
strate the improvements brought by CON, aligning
the overall QA performance.
3.4 Evaluation on Unknown Robustness
Table 4 illustrates that our RALM equipped with
CON exhibits superior robustness in handling un-
known scenario, particularly evident in the Real-
TimeQA benchmark. This benchmark falls com-
pletely outside the model’s domain and contains
real-time information that was not part of the
LLaMa-2 pre-training data. Despite this, models
are still capable of providing correct answers in
some cases, as the answers remain consistent over
time. In comparison to the standard RALM sys-
tem, our method shows a significant improvement,
exceeding +10.5 in its ability to reject to answer
questions in unknown scenario. The evaluation is
based on reject rate (RR), i.e., number of rejected



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 19):

A Scaling laws
We use 7 models to fit the scaling laws of Baichuan
2. The parameter details are shown in Table 10.
Nhidden NFFN Nlayer Nhead Nparams (Millions)
384 1,152 6 6 11.51
704 2,112 8 8 51.56
832 2,496 12 8 108.01
1,216 3,648 16 8 307.60
1,792 5,376 20 14 835.00
2,240 6,720 24 14 1,565.60
2,880 8,640 28 20 3,019.33
Table 10: The model we choose for fitting scaling laws.
The losses of the 7 different models are shown
in Figure 8.
Figure 8: The various training loss of small models for
scaling law.
B NormHead
By conducting a word embedding KNN retrieval
task, where given a query word the nearest K
words are retrieved. We found that the semantic
information is mainly encoded by the cosine
similarity of embedding rather than L 2 distance.
i.e., The KNN results of cosine similarity are words
with semantic similarity while the KNN results of
L2 distance are meaningless in some way. Since
the current linear classifier computes logits by dot
product, which is a mixture of L 2 distance and
cosine similarity. To alleviate the distraction of L2
distance, We propose to compute the logits by the
angle only. We normalized the output Embedding
so that the dot product is not affected by the norm
of embedding.
To validate this operation, we conduct an
ablation experiment where we add or remove the
normalization before softmax and train a 7B model
for 12k steps. All the hyper-parameters and data are
the same with Baichuan 2-7B. The training loss is
shown in Figure 9. We can see that when removing
the NormHead the training became very unstable at
the beginning, on the contrary, after we normalized
the head the training became very stable, which
resulted in better performance.
Figure 9: The training loss with and without NormHead
operation. The experiments are conducted on 7 billion
parameters with the same hyper-parameters (torch
random seeds, data flow, batch size, learning rate, etc.)
C Training Dynamics
In this section, we analyze the training dynamics of
our model. We save the checkpoints of Baichuan 2-
7B and Baichuan 2-13B every 1000 steps. And
evaluate those intermediate results on C-Eval
development set (Huang et al., 2023), MMLU
(Hendrycks et al., 2021a) , CMMLU (Li et al.,
2023) , JEC-QA (Zhong et al., 2020), GSM8K (Shi
et al., 2022) and HumanEval (Chen et al., 2021).
The result is shown in Figure 10.
As shown, both the 7B and 13B models
demonstrate substantial gains as training
progresses. However, on general benchmarks
such as MMLU (Hendrycks et al., 2021a) and
C-Eval (Huang et al., 2023), improvements appear
to plateau after 2 trillion tokens. In contrast,
consistent gains are achieved on the GSM8K
math tasks even beyond 2 trillion tokens. This
suggests training FLOPs may strongly correlate
with improvements in math problem solving,
which may be further studied.
D Baichuan Harmless Evaluation Dataset
WARNING: this section contains unsafe, offensive,
or upsetting examples of text.
We proposed the Baichuan Harmless Evaluation
Dataset (BHED) to evaluate the chat models, as



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 9):

10  
using KG-RAG framework (T-test, p-value < 0.0001, t-statistic = -47.7, N = 1000) but not in the prompt-based approach.  
 Fig 3. LLM performance on True/False and MCQ datasets Performance (Accuracy) distributions of LLMs on (A) True/False and (B) MCQ datasets. Blue distribution denotes the performance using prompt-based approach and red distribution denotes the performance using KG-RAG based approach. Black vertical dashed line indicates the mean value of the distribution. The higher the value, the better the performance.



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 16):

Published as a conference paper at ICLR 2024
Incorrect query 4: select distinct ?obj where { ?obj wdt:/instance of/ wd:/magazine/ . wd:/Delta Air
Lines/ wdt:/house publication/ ?obj . }
D.2 F LASHCARDS (NATURAL SENTENCE )
D.2.1 I NSTRUCTION -TUNING DATASET
We employ a natural sentence format for querying Medical knowledge. To instruction-tune our
AQG specifically for this purpose, we utilize the Medical Flashcards dataset (Han et al., 2023). This
dataset consists of question-answering pairs covering various subjects in the medical source, such
as anatomy, physiology, pathology, and pharmacology. It contains summaries and mnemonics of
crucial medical concepts, making it an ideal choice for instruction-tuning the AQG to effectively
handle medical knowledge queries. The size of the dataset is provided in Table 8.
D.2.2 D ATA EXAMPLE
Answer the question truthfully.
### Instruction:
Answer this question truthfully.
### Input:
What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ levels?
### Output:
Very low Mg2+ levels correspond to low PTH levels which in turn results in low Ca2+ levels.
D.3 S CIENCE QA P HYSICS (NATURAL SENTENCE )
D.3.1 I NSTRUCTION -TUNING DATASET
To instruction-tune our AQG for physics knowledge, we utilize the physics segment of the Sci-
enceQA dataset (Lu et al., 2022). Each entry in this dataset consists of a question, options, context,
answer, lecture, and explanation. The lecture contains necessary knowledge to answer the question.
We use the question and the options as input and the lecture as the output for instruction-tuning the
model.
D.3.2 D ATA EXAMPLE
Answer the question truthfully.
### Instruction:
Answer this question truthfully.
### Input:
The objects are identical except for their temperatures. Which object has less thermal energy?
Choose from: a 300-gram glass of water at a temperature of 75°F, a 300-gram glass of water at a
temperature of 80°F.
### Output:
The two glasses of water have the same mass but different temperatures. Since the 75°F glass of
water is colder than the 80°F glass of water, it has less thermal energy.
D.4 S CIENCE QA B IOLOGY (NATURAL SENTENCE )
D.4.1 I NSTRUCTION -TUNING DATASET
To instruction-tune our AQG for biology knowledge, we utilize the biology segment of the Sci-
enceQA dataset (Lu et al., 2022).
D.4.2 D ATA EXAMPLE
Answer the question truthfully.
### Instruction:
Answer this question truthfully.
17



Source: data\tc17_2312.15883v2\referenced_papers\[45]_2308.03281.pdf (Page 9):

+ ++ +++
56
58
60
62
64
Level
PT
FT
(a) Number of training datasets.
211 212 213 214
56
58
60
62
64
B
PT
(b) Batch size.
30M 110M 330M
56
58
60
62
64
N
PT
FT
(c) Number of model parameters.
Figure 3: Scaling analysis of different factors during contrastive pre-training and fine-tuning. Model performance is
mesured by average performance on MTEB.
Figure 4: Loss during contrastive pre-training for mod-
els of different size.
5.2 Training Behavior
We plot the training loss of different sized models
during contrastive pre-training in Figure 4. Larger
models have better ablity at learning to distinguish
positive pairs from negative ones. The training loss
experiences minor fluctuations consistently across
all model scales, which suggests variations in the
quality and difficulty of data per batch.3
We also evaluate model performance at different
training steps. It’s shown that model performance
saturates at 20k steps roughly corresponds to train-
ing convergence.
Steps 10k 20k 30k 40k 50k
MTEB 56.4 59.0 57.8 57.7 59.0
Table 8: Model performance at different training steps
during unsupervised contrastive pre-training.
3We use a fixed random seed for data sampling during
model training, ensuring that each model encounters the data
batches in the same order.
5.3 Influence of Different Training Stages
To examine the efficacy of multi-stage contrastive
learning, we conducted an analysis on the training
strategies. We compared three settings: a) solely
pre-training on unsupervised text pairs extracted
from diverse sources; b) solely fine-tuning on super-
vised datasets; c) contrastive pre-training followed
by fine-tuning. All models were initialized from
the original BERT base model.
Setting PT FT Full
MTEB 59.0 57.8 62.4
Table 9: Model performance at different training stages.
PT denotes run only unsupervised pre-training. FT only
use supervised data for model trainining. Full apply two
stages in a sequential manner.
It can be observed from Table 9 that relying
solely on supervised data for fine-tuning is insuf-
ficient to achieve a high-quality text embedding
model, likely due to its limited size. Conversely, un-
supervised pre-training using web-scale text pairs
yields superior text embeddings compared to solely
relying on labeled data for fine-tuning. Neverthe-
less, the incorporation of supervised data in a multi-
stage fashion with unsupervised pre-training can
still contribute to the refinement of the acquired
text embeddings.
5.4 Training Data Mixture
We study the influence of mixing ratio used in sam-
pling distribution on pre-training data to model
performance.
The performance on two task categories, re-
trieval and STS, as well as the average performance
on MTEB is reported in Table 10. We observe
that neither uniformly sampling from each pre-



### Claim 30/36

#### Claim Text
These hypotheses are not compatible with the hypothesis of uniform sum of transition rates formulated in , which holds when the two rates are almost uniform.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 21):

Published as a conference paper at ICLR 2024
Table 13: Details on costs (by tokens).
Method Dataset tokens
ReAct HotpotQA 1638
Verify-and-Edit HotpotQA 630
CoK HotpotQA 787
ReAct FEVER 848
Verify-and-Edit FEVER 286
CoK FEVER 329
Q: Practical Magic is based on a French novel that was written by Alice Hoffman.
A: First, Practical Magic is a 1998 American fantasy romantic drama film based on the 1995 novel
of the same name by Alice Hoffman. Second, Alice Hoffman is an American author. The answer is
ChatGPT: SUPPORTS.
Example 3:
Prompt: [3-shot CoT prompt]
Q: Saturn Corporation has no other names.
A: First, The Saturn Corporation, also known as Saturn LLC, was a registered trademark established
on January 7, 1985, as a subsidiary of General Motors. Second, There is no information available
on any other names for Saturn Corporation, but it is also known as Saturn LLC. The answer is
ChatGPT: SUPPORTS.
In the first example, it is mentioned twice in the prompt that Anne Sullivan was born in April.
However, the LLM still supports the claim that she was born in June. In the second example, the
CoT specifies that the novel is American. However, ChatGPT overlooks the nationality and supports
the claim that it is based on a French novel. In the third example, the CoT mentions repetitively that
Saturn Corporation is also known as Saturn LLC. However, ChatGPT supports the claim that it has
no other names.
These examples show that, even though the CoT is successfully improved in terms of factual con-
sistency, the final answer may still be incorrect due to reasoning errors inherent to LLM itself. In
the human study for wrong predictions, 44% of the time humans claim that CoK still generates im-
proved CoTs. Among these 44% instances, 73% of the time humans think these CoTs should have
led to better answers.
I C OST ANALYSIS
As CoK always edits instances below a certain consistency threshold, there is a cost advantage
compared to other methods such as ReAct. The costs are on par with methods such as Verify-and-
Edit.
A table of the costs is shown in 13. The costs are calculated based on tokens used per instance.
Overall, the costs for CoK are on par with Verify-and-Edit. The extra costs are incurred by the
dynamic knowledge editing stage, which is shown to boost performance in the main results. CoK
also costs much less than ReAct, incurring only around 40% of ReAct’s costs. Specifically, it costs
787 compared to 1638 for HotpotQA, and 329 compared to 848 for FEVER.
The API cost for gpt-3.5-turbo is currently $0.0015 / 1K tokens for input, and $0.002 / 1K
tokens for output.
For details of the cost calculations, as the output length is the same for all methods, we only calculate
the input tokens. Following the original ReAct paper(Yao et al., 2023), we calculate based on 3-shot
prompts for FEVER and 6-shot prompts for HotpotQA. Verify-and-Edit and CoK tokens per instance
are calculated based on the CoT-SC threshold, which results in editing 86 out of 308 instances for
22



Source: data\tc17_2312.15883v2\referenced_papers\[47]_2307.03172.pdf (Page 17):

G Full Multi-Document Question Answering Results
This section tabulates model performance when evaluated on the multi-document QA task with varying
numbers of documents (Figure 5). “Index n” indicates performance when the document with the answer
occurs at position n + 1, where lower indices are closer to the start of the input context. For example,
index 0 refers to performance when the document with the answer is placed at the very start of the context
(i.e., first amongst all documents).
G.1 10 Total Retrieved Documents
Model Index 0 Index 4 Index 9
Claude-1.3 62.9% 58.3% 59.7%
Claude-1.3 (100K) 63.1% 58.3% 59.7%
GPT-3.5-Turbo 76.8% 61.2% 62.4%
GPT-3.5-Turbo (16K) 76.9% 61.0% 62.5%
MPT-30B-Instruct 60.2% 56.2% 59.7%
LongChat-13B (16K) 72.1% 58.9% 58.5%
Table 5: Model performance when evaluated on the multi-document QA task with 10 total retrieved documents.
G.2 20 Total Retrieved Documents
Model Index 0 Index 4 Index 9 Index 14 Index 19
Claude-1.3 59.9% 55.9% 56.8% 57.2% 60.1%
Claude-1.3 (100K) 59.8% 55.9% 57.0% 57.4% 60.0%
GPT-3.5-Turbo 75.8% 57.2% 53.8% 55.4% 63.2%
GPT-3.5-Turbo (16K) 75.7% 57.3% 54.1% 55.4% 63.1%
MPT-30B-Instruct 53.7% 51.8% 52.2% 52.7% 56.3%
LongChat-13B (16K) 68.6% 57.4% 55.3% 52.5% 55.0%
Table 6: Model performance when evaluated on the multi-document QA task with 20 total retrieved documents.
G.3 30 Total Retrieved Documents
Model Index 0 Index 4 Index 9 Index 14 Index 19 Index 24 Index 29
Claude-1.3 59.1% 55.1% 54.8% 55.7% 56.4% 56.2% 59.9%
Claude-1.3 (100K) 59.1% 55.1% 54.9% 55.7% 56.6% 56.1% 60.0%
GPT-3.5-Turbo (16K) 73.4% 55.1% 50.5% 50.9% 51.8% 54.9% 63.7%
MPT-30B-Instruct 51.6% 51.3% 51.2% 49.0% 49.6% 51.3% 54.1%
LongChat-13B (16K) 66.9% 54.8% 52.5% 52.9% 52.2% 51.3% 55.1%
Table 7: Model performance when evaluated on the multi-document QA task with 30 total retrieved documents.



Source: data\tc17_2312.15883v2\referenced_papers\[93]_2311.09210.pdf (Page 6):

Models Noise NQ TriviaQA WebQ Average
Ratio EM F1 EM F1 EM F1 EM F1
Retrieve-Read 100% 34.28 41.74 55.30 61.67 29.58 46.34 39.72 49.92
+ CHAIN-OF-NOTE 41.83 49.58 64.30 70.00 36.85 53.07 47.66 57.55
(+7.55) (+7.84) (+9.00) (+8.33) (+7.27) (+6.73) (+7.94) (+7.63)
Retrieve-Read 80% 54.28 61.03 73.83 80.02 35.46 52.70 54.52 64.58
+ CHAIN-OF-NOTE 56.63 63.23 75.89 81.24 40.60 56.54 57.70 67.00
(+2.35) (+2.20) (+2.06) (+1.22) (+5.14) (+3.84) (+3.18) (+2.42)
Retrieve-Read 60% 61.44 67.94 78.44 83.65 37.01 54.16 58.96 68.58
+ CHAIN-OF-NOTE 63.43 69.33 78.79 84.07 41.26 56.91 61.16 70.10
(+1.99) (+1.39) (+0.35) (+0.42) (+4.25) (+2.75) (+2.20) (+1.52)
Retrieve-Read 40% 64.62 71.12 80.56 86.76 38.40 55.60 61.19 71.16
+ CHAIN-OF-NOTE 65.91 72.22 81.72 87.11 42.16 58.15 63.26 72.49
(+1.29) (+1.10) (+1.16) (+0.35) (+3.76) (+2.55) (+2.07) (+1.33)
Retrieve-Read 20% 67.21 73.69 81.73 87.89 39.95 56.66 62.96 72.75
+ CHAIN-OF-NOTE 70.00 76.08 82.86 88.24 44.36 60.13 65.74 74.82
(+2.79) (+2.39) (+1.13) (+0.35) (+4.41) (+3.47) (+2.78) (+2.07)
Retrieve-Read 0% 69.23 75.57 83.34 89.44 42.24 58.59 64.93 74.53
+ CHAIN-OF-NOTE 73.28 79.86 83.52 88.94 46.16 62.38 67.65 77.06
(+4.05) (+4.29) (+0.18) (-0.50) (+3.92) (+3.79) (+2.72) (+2.53)
Table 3: Evaluation on Noise Robustness. The backbone language model is LLaMa-2 7B. The CHAIN -OF-NOTE
framework shows superior performance compared to the standard RALM system, particularly notable at higher
noise ratios.We explain how we synthesize data with different noise ratios under real-world scenarios in§ 3.1.1.
Models↓ RealTimeQA
EM F1 RR
Retrieve-Read (Shi et al., 2023c)15.6 19.9 6.1
+ CHAIN-OF-NOTE (ours) 15.7 20.3 13.0
Table 4: Evaluation on Unknown Robustness. TheCON
shows better performance than standard RALM system.
3.3 Evaluation on Noise Robustness
As illustrated in Table 2, when faced with entirely
noisy documents, both the standard RALM and
our CHAIN -OF-NOTE enhanced RALM underper-
formed compared to the no-retrieval setting. This
suggests that RALMs can be misled by noisy infor-
mation, leading to more hallucinations.
Notably, equipping the model withCON enables
it to perform nearly as well as the baseline model
directly fine-tuned with QA pairs without retrieval,
showcasing its robustness to noise and its ability
to disregard irrelevant information. The CON ap-
proach is effective not only in fine-tuned, smaller-
sized models but also in large language models,
such as GPT-4, with adjustments made only to the
prompt. Besides, in comparison to the CHAIN -OF-
THOUGHT technique, commonly utilized in rea-
soning scenarios, CON presents a more efficient
strategy for retrieval-augmented settings, particu-
larly in addressing knowledge-intensive tasks.
Table 3 shows that RALM enhanced with CON
consistently outperforms the standard RALM, es-
pecially in scenarios with exclusively noisy doc-
uments. An average improvement of +7.9 in EM
score on fully noisy documents is observed on three
open-domain QA datasets, in average. Experiments
with lower noise ratios also consistently demon-
strate the improvements brought by CON, aligning
the overall QA performance.
3.4 Evaluation on Unknown Robustness
Table 4 illustrates that our RALM equipped with
CON exhibits superior robustness in handling un-
known scenario, particularly evident in the Real-
TimeQA benchmark. This benchmark falls com-
pletely outside the model’s domain and contains
real-time information that was not part of the
LLaMa-2 pre-training data. Despite this, models
are still capable of providing correct answers in
some cases, as the answers remain consistent over
time. In comparison to the standard RALM sys-
tem, our method shows a significant improvement,
exceeding +10.5 in its ability to reject to answer
questions in unknown scenario. The evaluation is
based on reject rate (RR), i.e., number of rejected



Source: data\tc17_2312.15883v2\referenced_papers\[77]_2308.08833.pdf (Page 16):

Models Fluency Relevance Completeness Proficiency Avg.
ChatGPT 4.93 4.65 4.22 4.34 4.53
GPT-4 4.88 4.61 4.20 4.39 4.52
Baichuan-13B-chat 4.79 4.29 4.22 4.30 4.40
ChatGLM2-6B 4.77 4.06 3.96 3.99 4.20
HuatuoGPT 4.70 3.89 3.69 3.81 4.02
BianQue-2 4.44 3.50 3.30 3.43 3.67
ChatMed-Consult 4.26 3.39 3.16 3.27 3.52
MedicalGPT 4.21 3.40 3.09 3.10 3.45
DoctorGLM 3.74 2.46 2.35 2.30 2.71
Bentsao 3.52 2.62 2.36 2.30 2.70
ChatGLM-Med 2.92 2.23 1.98 1.92 2.26
Table 11: Results ofexpert evaluation on CMB-Clin. Avg. are the averaged scores of each model over all perspectives.
Models are arranged in descending order of Avg.
1 2 3 4 5
1
2
3
4
5
Fluency
pearson=0.71
1 2 3 4 5
Relevance
pearson=0.81
1 2 3 4 5
Completeness
pearson=0.78
1 2 3 4 5
Proficiency
pearson=0.75
Expert
GPT-4
Figure 5: Correlation of expert and automatic evaluation on CMB-Clin of each perspective with pearson correlation.
The four plots show correlations in fluency, relevance, completeness and proficiency in medicine, respectively. Each
plot consists of 320 data points with many overlapped. The darker a point is, the more overlapped data there are at
that position. Each expert score is averaged over the three expert annotators.
Settings Original T-0.2 T-0.6 T-1.0 T-1.5
Original 1.00 0.95 0.90 0.87 0.87
T-0.2 0.95 1.00 0.98 0.88 0.88
T-0.6 0.90 0.98 1.00 0.90 0.90
T-1.0 0.87 0.88 0.90 1.00 1.00
T-1.5 0.87 0.88 0.90 1.00 1.00
Table 12: Pairwise Spearman correlations between re-
sults under different decoding temperatures. Original:
results of greedy decoding (temperature 0). T-x: results
of using nucleus sampling under temperature x.
E.3 Agreement of Expert and GPT-4
Evaluation
Figure 5 shows the agreement between expert and
GPT-4 evaluation on each perspective. The pearson
correlations are all above 0.71, indicating a strong
linear correlation between the two evaluation ap-
proaches.
E.4 Pairwise Correlation of Rankings under
Different Temperatures
We evaluate the results generated under each setting
(i.e., under different temperatures) using ChatGPT.
Then for each setting, we obtain a ranking for all
models. We then calculate the pairwise spearman
correlation between all sets of rankings. The results
are summarized in Table 12.



Source: data\tc17_2312.15883v2\referenced_papers\[47]_2307.03172.pdf (Page 16):

F Token Counts
Table 2, Table 3, and Table 4 present the average and maximum number of tokens in each of the input
contexts for all experimental settings. Note that MPT-30B and MPT-30B-Instruct use the same tokenizer,
GPT-3.5-Turbo and GPT-3.5-Turbo (16K) use the same tokenizer, and Claude-1.3 and Claude-1.3 (100K)
use the same tokenizer. Furthermore, the Claude-1.3 tokenizer is the same as the GPT-3.5-Turbo tokenizer,
modulo some additional special tokens that do not appear in our data. As a result, the token counts for
these two model families is the same in our experimental settings.
Closed-Book Oracle
avg ± stdev max avg ± stdev max
LongChat-13B (16K) 55.6 ± 2.7 70 219.7 ± 48.5 588
MPT-30B 43.5 ± 2.2 58 187.9 ± 41.8 482
GPT-3.5-Turbo 15.3 ± 2.2 29 156.0 ± 41.8 449
Claude-1.3 15.3 ± 2.2 29 156.0 ± 41.8 449
Table 2: Token count statistics for each of the evaluated models on the closed-book and oracle multi-document
question answering settings.
10 docs 20 docs 30 docs
avg ± stdev max avg ± stdev max avg ± stdev max
LongChat-13B (16K) 1749.9 ± 112.4 2511 3464.6 ± 202.3 4955 5181.9 ± 294.7 7729
MPT-30B 1499.7 ± 88.5 1907 2962.4 ± 158.4 3730 4426.9 ± 230.5 5475
GPT-3.5-Turbo 1475.6 ± 86.5 1960 2946.2 ± 155.1 3920 4419.2 ± 226.5 6101
Claude-1.3 1475.6 ± 86.5 1960 2946.2 ± 155.1 3920 4419.2 ± 226.5 6101
Table 3: Token count statistics for each of the evaluated models on each of the document question answering
settings.
75 KV pairs 140 KV pairs 300 KV pairs
avg ± stdev max avg ± stdev max avg ± stdev max
LongChat-13B (16K) 5444.5 ± 19.1 5500 10072.4 ± 24.1 10139 21467.3 ± 35.9 21582
MPT-30B 4110.5 ± 23.8 4187 7600.9 ± 31.1 7687 16192.4 ± 46.6 16319
GPT-3.5-Turbo 3768.7 ± 25.6 3844 6992.8 ± 34.1 7088 14929.4 ± 50.7 15048
Claude-1.3 3768.7 ± 25.6 3844 6992.8 ± 34.1 7088 14929.4 ± 50.7 15048
Table 4: Token count statistics for each of the evaluated models on each of the key-value (KV) retrieval settings.



### Claim 31/36

#### Claim Text
The time-dependent S chrödinger equation (here and below, atomic units are used) ( ) Ψ+Ψ      +=∂ Ψ ∂ rVc t Apti /arrowrightnosp /arrowrightnosp /arrowrightnosp 2 ) ( 2 1 (1) was solved by direct numerical integration using th e split-operator method with the fast Fourier transform .

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 26):

用户：鸡和兔在一个笼子里，共有26个头，68只脚，那么鸡有多少只，兔有多少只？
Baichuan 2-13B-Chat：我们可以用代数方法来解决这个问题。假设鸡的数量为x，兔子的
数量为y。根据题目给出的信息，我们可以得到以下两个方程：
1. x + y = 26 (头的总数) 2. 2x + 4y = 68 (脚的总数)
现在我们需要解这个二元一次方程组。首先从第一个方程中解出x：
x = 26 - y
然后将x的表达式代入第二个方程：
2(26 - y) + 4y = 68 52 - 2y + 4y = 68 2y = 16
接下来我们求解y的值：
y = 16 / 2 y = 8
现在我们得到了兔子的数量是8只。将y的值代入x的表达式求得鸡的数量：
x = 26 - 8 x = 18
所以鸡有18只，兔子有8只。
用户：以下天文学常识题目，哪一个是错误的? A.太阳系是指由太阳和围绕着它运行的
八大行星、矮行星、卫星、小行星带和彗星组成的一个行星系统。B.卫星是指绕行星
或其他天体运动的天体。C.彗星是指太阳系中一种较小的天体，其核心由冰和尘埃组
成。D.按一般的天体归类方法，月球属于行星。
Baichuan 2-13B-Chat：D.按一般的天体归类方法，月球属于行星是错误的。实际上，月球
是地球的卫星，不属于行星范畴。
用户：以下物理常识题目，哪一个是错误的?A.在自然环境下，声音在固体中传播速
度最快。B.牛顿第一定律:一个物体如果不受力作用，将保持静止或匀速直线运动的状
态。C.牛顿第三定律:对于每个作用力，都有一个相等而反向的反作用力。D.声音在空气
中的传播速度为1000m/s。
Baichuan 2-13B-Chat：本题考查物理基本概念和原理的理解和应用能力。A项：声音的传
播需要介质，在固体、液体和气体的三种介质中，声音在固体中的传播速度是最快的，
故A正确； B项：牛顿第一定律是惯性定律，其内容是：一个物体如果没有受到外力的
作用，要么保持静止状态，要么保持匀速直线运动状态，直到有外力迫使它改变运动状
态为止，故B正确； C项：牛顿第三定律的内容是：对于每一个作用力，都有一个大小
相等、方向相反的反作用力，这两个力分别作用在两个相互作用的物体上，故C错误；
D项：声音在空气中以波的形式传播，其传播速度与介质的温度、湿度等因素有关，一
般情况下，声音在空气中的传播速度约为340m/s左右，而不是1000m/s，故D错误。 因
此，本题的正确答案是D 。



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 16):

Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson,
Itziar Gonzalez-Dios, Javier de la Rosa, Jenny
Chim, Jesse Dodge, Jian Zhu, Jonathan Chang,
Jorg Frohberg, Josephine L. Tobing, Joydeep
Bhattacharjee, Khalid Almubarak, Kimbo Chen,
Kyle Lo, Leandro von Werra, Leon Weber, Long
Phan, Loubna Ben Allal, Ludovic Tanguy, Manan
Dey, Manuel Romero Muñoz, Maraim Masoud,
Mar’ia Grandury, Mario vSavsko, Max Huang,
Maximin Coavoux, Mayank Singh, Mike Tian-
Jian Jiang, Minh Chien Vu, Mohammad Ali
Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora
Kassner, Nurulaqilla Khamis, Olivier Nguyen,
Omar Espejel, Ona de Gibert, Paulo Villegas,
Peter Henderson, Pierre Colombo, Priscilla A.
Amuok, Quentin Lhoest, Rheza Harliman, Rishi
Bommasani, Roberto L’opez, Rui Ribeiro, Salomey
Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose,
Shamsuddeen Hassan Muhammad, Shanya Sharma,
S. Longpre, Somaieh Nikpoor, Stanislav Silberberg,
Suhas Pai, Sydney Zink, Tiago Timponi Torrent,
Timo Schick, Tristan Thrush, Valentin Danchev,
Vassilina Nikoulina, Veronika Laippala, Violette
Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak
Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si,
Elizabeth Salesky, Sabrina J. Mielke, Wilson Y . Lee,
Abheesht Sharma, Andrea Santilli, Antoine Chaffin,
Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla,
Gunjan Chhablani, Han Wang, Harshit Pandey,
Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo
Gao, Lintang Sutawika, M Saiful Bari, Maged S.
Al-shaibani, Matteo Manica, Nihal V . Nayak, Ryan
Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-
David, Stephen H. Bach, Taewoon Kim, Tali Bers,
Thibault Févry, Trishala Neeraj, Urmish Thakker,
Vikas Raunak, Xiang Tang, Zheng Xin Yong, Zhiqing
Sun, Shaked Brody, Y Uri, Hadar Tojarieh, Adam
Roberts, Hyung Won Chung, Jaesung Tae, Jason
Phang, Ofir Press, Conglong Li, Deepak Narayanan,
Hatim Bourfoune, Jared Casper, Jeff Rasley,
Max Ryabinin, Mayank Mishra, Minjia Zhang,
Mohammad Shoeybi, Myriam Peyrounette, Nicolas
Patry, Nouamane Tazi, Omar Sanseviero, Patrick von
Platen, Pierre Cornette, Pierre Franccois Lavall’ee,
Rémi Lacroix, Samyam Rajbhandari, Sanchit
Gandhi, Shaden Smith, Stéphane Requena, Suraj
Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet
Singh, Anastasia Cheveleva, Anne-Laure Ligozat,
Arjun Subramonian, Aur’elie N’ev’eol, Charles
Lovering, Daniel H Garrette, Deepak R. Tunuguntla,
Ehud Reiter, Ekaterina Taktasheva, Ekaterina
V oloshina, Eli Bogdanov, Genta Indra Winata,
Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina
Novikova, Jessica Zosa Forde, Xiangru Tang, Jungo
Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat,
Miruna Clinciu, Najoung Kim, Newton Cheng,
Oleg Serikov, Omer Antverg, Oskar van der Wal,
Rui Zhang, Ruochen Zhang, Sebastian Gehrmann,
Shachar Mirkin, S. Osher Pais, Tatiana Shavrina,
Thomas Scialom, Tian Yun, Tomasz Limisiewicz,
Verena Rieser, Vitaly Protasov, Vladislav Mikhailov,
Yada Pruksachatkun, Yonatan Belinkov, Zachary
Bamberger, Zdenvek Kasner, Alice Rueda, Amanda
Pestana, Amir Feizpour, Ammar Khan, Amy
Faranak, Ananda Santa Rosa Santos, Anthony
Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo
Abdollahi, Aycha Tammour, Azadeh HajiHosseini,
Bahareh Behroozi, Benjamin Olusola Ajibade,
Bharat Kumar Saxena, Carlos Muñoz Ferrandis,
Danish Contractor, David M. Lansky, Davis David,
Douwe Kiela, Duong Anh Nguyen, Edward Tan,
Emily Baylor, Ezinwanne Ozoani, Fatim T Mirza,
Frankline Ononiwu, Habib Rezanejad, H.A. Jones,
Indrani Bhattacharya, Irene Solaiman, Irina Sedenko,
Isar Nejadgholi, Jan Passmore, Joshua Seltzer,
Julio Bonis Sanz, Karen Fort, Lívia Macedo
Dutra, Mairon Samagaio, Maraim Elbadri, Margot
Mieskes, Marissa Gerchick, Martha Akinlolu,
Michael McKenna, Mike Qiu, M. K. K. Ghauri,
Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour
Elkott, Nourhan Fahmy, Olanrewaju Samuel, Ran
An, R. P. Kromann, Ryan Hao, Samira Alizadeh,
Sarmad Shubber, Silas L. Wang, Sourav Roy, Sylvain
Viguier, Thanh-Cong Le, Tobi Oyebade, Trieu
Nguyen Hai Le, Yoyo Yang, Zachary Kyle Nguyen,
Abhinav Ramesh Kashyap, A. Palasciano, Alison
Callahan, Anima Shukla, Antonio Miranda-Escalada,
Ayush Kumar Singh, Benjamin Beilharz, Bo Wang,
Caio Matheus Fonseca de Brito, Chenxi Zhou, Chirag
Jain, Chuxin Xu, Clémentine Fourrier, Daniel Le’on
Perin’an, Daniel Molano, Dian Yu, Enrique
Manjavacas, Fabio Barth, Florian Fuhrimann,
Gabriel Altay, Giyaseddin Bayrak, Gully Burns,
Helena U. Vrabec, Iman I.B. Bello, Isha Dash, Ji Soo
Kang, John Giorgi, Jonas Golde, Jose David Posada,
Karthi Sivaraman, Lokesh Bulchandani, Lu Liu,
Luisa Shinzato, Madeleine Hahn de Bykhovetz,
Maiko Takeuchi, Marc Pàmies, María Andrea
Castillo, Marianna Nezhurina, Mario Sanger,
Matthias Samwald, Michael Cullan, Michael
Weinberg, M Wolf, Mina Mihaljcic, Minna
Liu, Moritz Freidank, Myungsun Kang, Natasha
Seelam, Nathan Dahlberg, Nicholas Michio Broad,
Nikolaus Muellner, Pascale Fung, Patricia Haller,
R. Chandrasekhar, R. Eisenberg, Robert Martin,
Rodrigo L. Canalli, Rosaline Su, Ruisi Su, Samuel
Cahyawijaya, Samuele Garda, Shlok S Deshmukh,
Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee
Sang-aroonsiri, Srishti Kumar, Stefan Schweter,
Sushil Pratap Bharati, T. A. Laud, Th’eo Gigant,
Tomoya Kainuma, Wojciech Kusa, Yanis Labrak,
Yashasvi Bajaj, Y . Venkatraman, Yifan Xu, Ying
Xu, Yun chao Xu, Zhee Xao Tan, Zhongli Xie,
Zifan Ye, Mathilde Bras, Younes Belkada, and
Thomas Wolf. 2022. Bloom: A 176b-parameter
open-access multilingual language model. ArXiv,
abs/2211.05100.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proximal
policy optimization algorithms. arXiv preprint
arXiv:1707.06347.
Noam Shazeer. 2020. Glu variants improve transformer.
arXiv preprint arXiv:2002.05202.
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,



Source: data\tc17_2312.15883v2\referenced_papers\[73]_2401.14268.pdf (Page 0):

GPTVoiceTasker: Advancing Multi-step Mobile Task Efficiency
Through Dynamic Interface Exploration and Learning
Minh Duc Vu
dustin.vu@csiro.au
CSIRO’s Data61
Melbourne, Australia
Han Wang∗
han.wang@monash.edu
Monash University
Melbourne, Australia
Zhuang Li
zhuang.li@monash.edu
Monash University
Melbourne, Australia
Jieshan Chen
jieshan.chen@data61.csiro.au
CSIRO’s Data61
Sydney, Australia
Shengdong Zhao
City University of Hong Kong
Hong Kong, China
shezhao@cityu.edu.hk
Zhenchang Xing
CSIRO’s Data61 & Australian
National University
Canberra, Australia
zhenchang.xing@data61.csiro.au
Chunyang Chen†
Technical University of Munich &
Monash University
Heilbronn, Germany
chun-yang.chen@tum.de
Figure 1: GptVoiceTasker provides an intuitive method for automating complex commands on smartphones during physically
demanding activities, such as cooking. It automatically explores step-by-step interactions to complete unprecedented tasks and
uses the saved information to accelerate the automation process for tasks that have been previously encountered.
∗Minh Duc Vu and Han Wang contributed equally.
†Chunyang Chen is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
UIST’24, October 13–16, 2024, Pittsburgh, PA, USA
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/3654777.3676356
arXiv:2401.14268v3  [cs.HC]  14 Aug 2024



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 23):

24  
Acknowledgements SEB holds the Heidrich Family and Friends Endowed Chair of Neurology at UCSF. SEB holds the Distinguished Professorship in Neurology I at UCSF. We also acknowledge the Versa team at UCSF for providing the infrastructure to access OpenAI API. The development of SPOKE and its applications are being funded by grants from the National Science Foundation [NSF_2033569], NIH/NCATS [NIH_NOA_1OT2TR003450]. 
Data availability SPOKE KG can be accessed at https://spoke.rbvi.ucsf.edu/neighborhood.html. It can also be accessed using REST-API (https://spoke.rbvi.ucsf.edu/swagger/). KG-RAG code is made available at https://github.com/BaranziniLab/KG_RAG. Biomedical benchmark datasets used in this study are made available to the research community in the same GitHub repository.



Source: data\tc17_2312.15883v2\referenced_papers\[93]_2311.09210.pdf (Page 2):

Wikipedia 1: Deadpool 2 is a 2018 American super-hero film based on the Marvel Comics character Deadpooldistributed by 20th Cen-tury Fox. … This movie was followed by releases  in 49 markets over the next few days, including the United States on June1, 2018. 
When was the Deadpool2 being released?
Wikipedia 2: …Deadpool 2 premiered at Lei-cester Square in London on May 10, 2018 Itwas released in United States on May 18, 2018, having been previously scheduled for release on June 1 of that year. …
The first passage suggests that “Deadpool2"            released in the United States on June 1, 2018. However, the second passage clarifies that the movie was actually released earlier, on May 18, having been rescheduled from the initially planned date of June 1. Based on information provided in above passage, the answer is May 18, 2018.
Wikipedia 1: The nostalgic and melancholic song recounts the type of girls with whom the singer had relationships at various years: when he was 17, “small-town girls ... on the village green”; at 21, “city girls who lived up the stairs”
Who wrote the song “When I was 17”?
Wikipedia 2: …“It Was a Very Good Year” , commonly unknown as “When I was 17”, was made famous by Frank Sinatra‘s version in D minor, which won the Grammy Award for Best Male Vocal Performance in 1966.
Wikipedia 1: The Divergent Series is a feature film trilogy.… Distributed by Summit Enter-tainment and Lions-gate Films, the series hasthreescience fiction films set in a dystopian society:Divergent, Insurgent, and Allegiant.
When is the fourth movie of the Divergent series coming out?
Wikipedia 2: The film(Divergent) was a box office bomb, grossing just $179 million against its budget of $142 million, becoming the low-estgrossing “Divergent” film. This led initially to budget cuts on the fourth and final film.…
The first passage discusses the songremin--iscing about relationships at different agesbut does not specify the songwriter. The second clarifies the song often mistakenlycalled “When I Was 17” is actually “It Was a Very Good Year,” Both passages do not explicitly mention the songwriter, but imply the correct title. Based on my know-ledge, the answer is Ervin Drake.
The first passage suggests the "Divergent" series was initially planned as a trilogy, while the secondpassage discusses the poor box office leadsto budget cuts for a potential fourth movie. However, neither passage confirms the release date of the fourth filminDivergent Series. Given this lack of specific information and my current knowledge, the answer is unknown.
(a)CoN:Relevant->Findtheanswer(b)CoN:Irrelevant->Infertheanswer(c)CoN:Irrelevant->AnswerUnknown
Figure 2: Illustration of the CHAIN -OF-NOTE (CON) framework with three distinct types of reading notes. Type
(a) depicts the scenario where the language model identifies a document that directly answers the query, leading
to a final answer formulated from the retrieved information. Type (b) represents situations where the retrieved
document, while not directly answering the query, provides contextual insights, enabling the language model to
integrate this context with its inherent knowledge to deduce an answer. Type (c) illustrates instances where the
language model encounters irrelevant documents and lacks the necessary knowledge to respond, resulting in an
“unknown” answer. This figure exemplifies the CoN framework’s capability to adaptively process information,
balancing direct information retrieval, contextual inference, and the recognition of its knowledge boundaries.
2022; Shi et al., 2023c). The RALM models can
be represented as p(y|x) =P
i p(y|di, x)p(di|x).
Here, x represents the input query, and y signifies
the model’s generated response. In practice, it is
infeasible to compute the sum over all possible doc-
uments due to the vast number of potential sources.
Consequently, the most common approach involves
approximating the sum over d using the k highest
ranked documents, and providing all these docu-
ments as part of the input. We assume, w.l.o.g.,
that these documents are [d1, . . . , dk], yielding
p(y|x) =Pk
i=1 p(y|di, x)p(di|x).
However, the existing RALMs suffer from sev-
eral limitations:
• Risk of Surface-Level Processing: When
directly generating an answer, language models
might rely on surface-level information without
deep comprehension. Thus, they could easily over-
look the nuances of question or documents, partic-
ularly in complex or indirect questions.
• Difficulty in Handling Contradictory Informa-
tion: When faced with documents containing con-
tradictory information, directly generating an an-
swer becomes challenging. The model may strug-
gle of these contradictions or to determine which
piece of information is more credible or relevant.
• Reduced Transparency and Interpretability: Di-
rect answer generation offers limited insight into
how the model arrived at its conclusion. This lack
of transparency makes it challenging for users to
understand the basis of the model’s conclusions.
• Overdependence on Retrieved Documents: Di-
rect generation can lead to an overreliance on the
content of the retrieved documents (i.e. tendency to
extract information from retrieved documents (Shi
et al., 2023a)), ignoring the model’s inherent knowl-
edge base. This can be particularly limiting when
the retrieved documents are noisy or out-of-date.
2.3 The C HAIN -OF-NOTE Framework
The CHAIN -OF-NOTE (CON) framework presents
a solution to the challenges faced by retrieval-
augmented language models (RALMs). This
framework significantly enhances the ability of
RALMs to critically assess retrieved documents
through a structured note-taking process. Specifi-
cally, it involves generating concise and contextu-
ally relevant summaries or notes for each document.
This method allows the model to systematically
evaluate the relevance and accuracy of informa-
tion drawn from external documents. By creating
sequential reading notes, CON not only assesses



### Claim 32/36

#### Claim Text
The total shift can be expressed as a combination of imaginary (momentum-domain/angular) shift and real (position-domain/spatial) shift both of which show weak value amplification beavior (∼ cot ϵ) .

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[86]_2312.14335.pdf (Page 1):

tially produce more faithful summarization. A few works [Sh i+23; PCM22] make use of pointwise mutual
information to let the LM focus more on the input context to re duce hallucination.
In this work, we focus on Context-aware Decoding [Shi+23] (C AD). CAD leverages the idea of point-
wise mutual information and proposes a product-of-experts enhancement to the vanilla decoding method
to make the generation more conditioned on the input evidenc e. W e conduct a large-scale reproducibility
study with different choices of language models to study CAD ’s effectiveness in query-focused summariza-
tion, and include more rigorous analysis on computational c omplexity and hyperparameter sensitivity . Ex-
tensive experiments show that CAD improves QFS quality by (1 ) reducing factual mistakes/hallucinations
while (2) mostly retaining the match of lexical patterns, me asured by ROUGE scores. On the other hand,
CAD also introduces additional inference-time FLOPs and po tentially slows down decoding speed.
2 Background
Context-aware Decoding.Consider a language model parameterized by θ , a context c, an input query x,
and we want to generate a piece of text y. In QFS paradigm, c can be the source document, x and y will
be the query and summary , respectively . In the vanilla decod ing scenario, each token of generation y is
sampled autoregressively from the output probability dist ribution from language model θ conditioned on
input query x and context c:
yt ∼ pθ (yt |c, x, y<t ) ∝ Softmax
(
logitθ (yt |c, x, y<t )/τ
)
(1)
Here τ is a temperature hyperparameter to control the sharpness of probability distribution over the candi-
date vocabulary V. Examples of the sampling strategy include greedy and T op- p sampling [Hol+19].
As we are operating on a conditional generation problem, we h ope the generation is more conditioned
on input context c. [Shi+23] utilize the idea of pointwise mutual information (PMI). W e consider two events
c and yt :
PMI(pθ (yt ; c, x, y<t )) =log
( pθ (yt |c, x, y<t )
pθ (yt |x, y<t )
)
(2)
The interpretation of the above equation is we would like to m easure the association of event yt (predicting
a speciﬁc token) and event c (the presence of context c). pθ (yt |x, y<t ) is the prior probability , representing
the model’s prior belief from its parameters θ when no presence of context c, whereas pθ (yt |c, x, y<t ) is the
posterior probability with c. T o leverage PMI, Shi et al. [Shi+23] propose to multiply PMI with the original
probability distribution:
yt ∼ ˜pθ (yt |c, x, y<t ) ∝ pθ (yt |c, x, y<t )
( pθ (yt |c, x, y<t )
pθ (yt |x, y<t )
) α
(3)
Here α is a hyperparameter to control the PMI weighting. W e still ne ed to normalize the above equation to
acquire a valid probability distribution, rearranging the terms yields
yt ∼ Softmax
[(
(1 + α ) logitθ (yt |c, x, y<t ) − α logitθ (yt |x, y<t )
)
/τ
]
(4)
Here larger α yields more weight on the context c and α = 0 reduces to vanilla decoding. [Shi+23] recom-
mend setting α = 0.5 for summarization tasks and we also examine this choice i n Section 3.
Analysis of Computational Cost. W e follow the notations used in [Kap+20]. Let us consider the T rans-
former architecture with following hyperparameters: numb er of layers nlayer, dimension of the embedding
layer and intermediate representation dmodel, dimension of the intermediate feedforward layer dff, dimen-
sion of the attention output dattn and number of attention heads nheads ; and denote the number of tokens
in the input sequence as ninput . The model size N is the number of non-embedding parameters of the model,
N ≈ | θ |:
N ≈ 2 dmodel nlayer (2 dattn + dff ) =12 nlayer d2
model with dattn = dff/4 = dmodel (5)
The total FLOPs per token in one forward pass is approximatel y Cforward = 2N + 2nlayer ninput dattn (see
Section 2.1 in [Kap+20] for details). In LLMs, dmodel ≫ ninput /12, we can roughly ignore the second term
and use Cforward ≈ 2N.
W ith the above analysis, we know that performing one forward pass of CAD yields in total (2 |y<t | +
2 |x| + |c|) · Cforward FLOPs compared to (|y<t | + |x| + |c|) · Cforward from vanilla decoding.
2



Source: data\tc17_2312.15883v2\referenced_papers\[47]_2307.03172.pdf (Page 6):

1st 25th 50th 75th
Position of Key to Retrieve
40
50
60
70
80
90
100Accuracy
75 Key-Value Pairs (~4K tokens)
1st 35th 70th 105th 140th
Position of Key to Retrieve
40
50
60
70
80
90
100Accuracy
140 Key-Value Pairs (~8K tokens)
1st 50th 100th 150th 200th 250th 300th
Position of Key to Retrieve
40
50
60
70
80
90
100Accuracy
300 Key-Value Pairs (~16K tokens)
claude-1.3 claude-1.3-100k gpt-3.5-turbo-0613 gpt-3.5-turbo-16k-0613 mpt-30b-instruct longchat-13b-16k
Figure 7: The effect of changing the input context length and the position of relevant information on key-value
retrieval performance. Lower positions are closer to the start of the input context. Although some models show
perfect accuracy on this synthetic task (e.g., Claude-1.3 and Claude-1.3 (100K)), we see again that performance is
often highest when relevant information is occurs at the very start or end of the context, and rapidly degrades when
models must retrieve from the middle of the input context.
placed at the start of the input context, LongChat-
13B (16K) tends to generate code to retrieve the
key, rather than outputting the value directly.
4 Why Are Language Models Not Robust
to Changes in the Position of Relevant
Information?
Our multi-document question answering and key-
value retrieval results show that language models
struggle to robustly access and use information in
long input contexts, since performance degrades
significantly when changing the position of rele-
vant information. To better understand why, we per-
form some preliminary investigations into the role
of model architecture (decoder-only vs. encoder-
decoder), query-aware contextualization, and in-
struction fine-tuning.
4.1 Effect of Model Architecture
The open models we evaluated are all decoder-only
models—at each timestep, they may only attend
to prior tokens. To better understand the poten-
tial effects of model architecture on how language
model use context, we compare decoder-only and
encoder-decoder language models.
We experiment with Flan-T5-XXL (Raffel et al.,
2020; Chung et al., 2022) and Flan-UL2 (Tay et al.,
2023). Flan-T5-XXL is trained with a sequences
of 512 tokens (encoder and decoder). Flan-UL2 is
initially trained with sequences of 512 tokens (en-
coder and decoder), but is then pre-trained for an
extra 100K steps with 1024 tokens (encoder and de-
coder) before instruction fine-tuning on sequences
with 2048 tokens in the encoder and 512 tokens
in the decoder. However, since these models use
relative positional embeddings, they can (in prin-
ciple) extrapolate beyond these maximum context
lengths; Shaham et al. (2023) find that both mod-
els can perform well with sequences of up to 8K
tokens.
Figure 8 compares the performance of decoder-
only and encoder-decoder models. When Flan-UL2
is evaluated on sequences within its 2048-token
training-time context window (Figure 8; left sub-
plot), its performance is relatively robust to changes
in the position of relevant information within the
input context (1.9% absolute difference between
best- and worst-case performance). When evalu-
ated on settings with sequences longer than 2048
tokens (Figure 8; center and right), Flan-UL2 per-
formance begins to degrade when relevant informa-
tion is placed in the middle. Flan-T5-XXL shows
a similar trend, where longer input contexts result
in a greater performance degradation when placing
relevant information in the middle of the input con-
text. We hypothesize that encoder-decoder models
may make better use of their context windows be-
cause their bidirectional encoder allows processing
each document in the context of future documents,
potentially improving relative importance estima-
tion between documents.
4.2 Effect of Query-Aware Contextualization
Our multi-document QA and key-value retrieval
experiments place the query (i.e., question to an-
swer or key to retrieve) after the data to process
(i.e., the documents or the key-value pairs). As a
result, decoder-only models cannot attend to query
tokens when contextualizing documents or key-
value pairs, since the query only appears at the end



Source: data\tc17_2312.15883v2\referenced_papers\[47]_2307.03172.pdf (Page 9):

5 10 20 30 40 50
Number of Retrieved Docs
50
60
70
80
90Metric
claude-1.3
claude-1.3-100k
gpt-3.5-turbo-0613
gpt-3.5-turbo-16k-0613
mpt-30b-instruct
longchat-13b-16k
contriever recall
Figure 11: Retriever recall and model performance as a
function of the number of retrieved documents. Model
performance saturates long before retriever recall, indi-
cating that the models have difficulty making use of the
extra retrieved documents.
domain QA results. We see that reader model
performance saturates long before retriever per-
formance saturates, indicating that readers are not
effectively using the extra context. Using more
than 20 retrieved documents only marginally im-
proves reader performance ( ∼1.5% for GPT-3.5-
Turbo and∼1% for Claude-1.3), while significantly
increasing the input context length (and thus la-
tency and cost). These results, coupled with the
observation that models are often better at retriev-
ing and using information at the start or end of
the input contexts, suggest that effective rerank-
ing of retrieved documents (pushing relevant infor-
mation closer to the start of the input context) or
ranked list truncation (retrieving fewer documents
when appropriate; Arampatzis et al., 2009) may be
promising directions for improving how language-
model-based readers use retrieved context.
6 Related Work
6.1 Long-Context Language Models
There is much prior work in designing performant
language models with cheaper scaling than Trans-
formers in the context length. Many lines of work
pursue Transformer variants with attention modi-
fications like recurrence (Dai et al., 2019), factor-
izing attention into computationally less intensive
approximations (Beltagy et al., 2020; Zaheer et al.,
2020), or low-rank approximations (Wang et al.,
2020; Peng et al., 2021). Dao et al. (2022) in-
stead provide a faster exact attention by a carefully-
crafted IO-aware CUDA kernel. Separately, there
are attempts to do away with attention entirely to
remove quadratic sequence length complexity, of-
ten through convolution and/or linear RNNs, e.g.,
in RWKV (Peng, 2023), S4 (Gu et al., 2022), or
Hyena (Poli et al., 2023). Many prior efforts evalu-
ate perplexity on a diverse web corpus as a proxy
for the ability to process long contexts; this work
shows that precise knowledge access on long con-
texts may be an added challenge.
6.2 How Do Language Models Use Context?
The pioneering work of Khandelwal et al. (2018)
showed that small LSTM language models make
increasingly coarse use of longer-term context;
Sankar et al. (2019) found similar results in di-
alogue models. In a similar vein, Daniluk et al.
(2017) find that attentive LSTM language mod-
els tend to mainly use recent history. Petroni
et al. (2020) were among the first to demonstrate
the potential of combining context from an in-
formation retrieval system with a pretrained lan-
guage models for unsupervised question answering.
O’Connor and Andreas (2021) found that many
information-destroying operations had marginal ef-
fects on Transformer LMs’ predictions. Krishna
et al. (2022) found that long-context neural gen-
eration in modestly-sized Transformer language
models degenerates because models fail to prop-
erly condition on long context. Finally, studying
long-context models, Sun et al. (2021) found that
longer contexts improves prediction of only a few
tokens, an empirical finding consistent with the
theory of Sharan et al. (2018), who showed that
sequence distributions with bounded mutual infor-
mation necessarily lead to marginalaverage predic-
tion benefits from increasingly long context. Qin
et al. (2023) analyze how efficient Transformers
perform on a variety of long-context downstream
NLP tasks, finding that long-context transformers
are recency-biased and do not effectively use long-
range context.
6.3 The Serial-Position Effect
The U-shaped curve we observe in this work has
a connection in psychology known as the serial-
position effect (Ebbinghaus, 1913; Murdock Jr,
1962), that states that in free-association recall
of elements from a list, humans tend to best re-
member the first and last elements of the list. The
serial-position effect plays a role in understanding
how humans develop short- and long-term mem-



Source: data\tc17_2312.15883v2\referenced_papers\[47]_2307.03172.pdf (Page 4):

1st 5th 10th
Position of Document with the Answer
50
55
60
65
70
75Accuracy
10 T otal Retrieved Documents (~2K tokens)
1st 5th 10th 15th 20th
Position of Document with the Answer
50
55
60
65
70
75Accuracy
20 T otal Retrieved Documents (~4K tokens)
1st 5th 10th 15th 20th 25th 30th
Position of Document with the Answer
50
55
60
65
70
75Accuracy
30 T otal Retrieved Documents (~6K tokens)
claude-1.3 claude-1.3-100k gpt-3.5-turbo-0613 gpt-3.5-turbo-16k-0613 mpt-30b-instruct longchat-13b-16k
Figure 5: The effect of changing the position of relevant information (document containing the answer) on multi-
document question answering performance. Lower positions are closer to the start of the input context. Performance
is highest when relevant information occurs at the very start or end of the context, and rapidly degrades when models
must reason over information in the middle of their input context.
(16K).5 GPT-3.5-Turbo has a maximum context
length of 4K tokens, and GPT-3.5-Turbo (16K) is a
version with an extended maximum context length
of 16K tokens. We evaluate Claude-1.3 and Claude-
1.3 (100K) with the Anthropic API; Claude-1.3
has a maximum context length of 8K tokens, and
Claude-1.3 (100K) has an extended context length
of 100K tokens. 6
2.3 Results and Discussion
We experiment with input contexts containing 10,
20, and 30 total documents. Figure 5 presents multi-
document question answering performance when
varying the position of relevant information within
the input context. To contextualize model perfor-
mance, we also evaluate on the closed-book and
oracle settings (Table 1). In the closed-book setting,
models are not given any documents in their input
context, and must rely on their parametric memory
to generate the correct answer. On the other hand,
in the oracle setting, language models are given the
single document that contains the answer and must
use it to answer the question.
Model performance is highest when relevant in-
formation occurs at the beginning or end of its
input context. As illustrated in Figure 5, chang-
ing the position of relevant information in the in-
put context leads to substantial decreases in model
performance. In particular, we see a distinctive U-
5We use the 0613 OpenAI model versions.
6We also evaluate GPT-4 (8K) on a subset of multi-
document QA experiments, finding similar results and trends
as other models (though GPT-4 has higher absolute perfor-
mance). Evaluating GPT-4 on the full multi-document QA
and key-value retrieval experiments would cost upwards of
$6000. See Appendix D for GPT-4 results and discussion.
Model Closed-Book Oracle
LongChat-13B (16K) 35.0% 83.4%
MPT-30B-Instruct 31.5% 81.9%
GPT-3.5-Turbo 56.1% 88.3%
GPT-3.5-Turbo (16K) 56.0% 88.6%
Claude-1.3 48.3% 76.1%
Claude-1.3 (100K) 48.2% 76.4%
Table 1: Closed-book and oracle accuracy of language
models on the multi-document question answering task.
shaped performance curve—models are often much
better at using relevant information that occurs at
the very beginning (primacy bias) and very end of
contexts (recency bias), and suffer degraded perfor-
mance when forced to use information within the
middle of its input context. For example, GPT-3.5-
Turbo’s multi-document QA performance can drop
by more than 20%—in the worst case, performance
in 20- and 30-document settings is lower than per-
formance without any input documents (i.e., closed-
book performance; 56.1%). These results indicate
that current models cannot effectively reason over
their entire context window when prompted for
downstream tasks.
Extended-context models are not necessarily bet-
ter at using input context.When the input con-
text fits in the context window of both a model
and its extended-context counterpart, we see that
performance between them is nearly identical. For
example, the 10- and 20-document settings both
fit in the context window of GPT-3.5-Turbo and
GPT-3.5-Turbo (16K), and we observe that their
performance as a function of position of relative
information is nearly superimposed (solid purple
and dashed brown series in Figure 5). These results



Source: data\tc17_2312.15883v2\referenced_papers\[45]_2308.03281.pdf (Page 2):

2022), have demonstrated that constructing posi-
tive pairs through random passage cropping yields
superior results compared to the ICT task. Building
upon the ideas presented in (Chang et al., 2020),
some researchers have also put forth methods for
constructing higher-quality positive pairs using the
web link topology for retriever pre-training (Zhou
et al., 2022), a technique that proves effective in
zero-shot scenarios. Furthermore, in the field of
dense retrieval, significant research is dedicated to
enhancing the text representation capabilities of
pre-trained language models through the design of
auxiliary pre-training tasks (Gao and Callan, 2021;
Xiao et al., 2022; Gao and Callan, 2022; Wang
et al., 2022a; Long et al., 2022b; Li et al., 2023).
The previous two lines of research can be gen-
eralized as learning a vector representation for a
piece of text and distinguished by the type of down-
stream tasks. Recently, several studies have ex-
plored the construction of unified text representa-
tion models through large-scale contrastive learn-
ing and prompt-based learning (Neelakantan et al.,
2022; Wang et al., 2022b; Su et al., 2023). Ad-
ditionally, some research efforts have focused on
constructing evaluation datasets to better assess
the stability of text representation models across
different tasks and domains. BEIR (Benchmark-
ing IR) (Thakur et al., 2021) collects a substantial
number of retrieval tasks from various domains to
evaluate the robustness of dense retriever models in
zero-shot scenarios. Meanwhile, MTEB (Massive
Text Embedding Benchmark) (Muennighoff et al.,
2023) benchmarks over 56 datasets spanning seven
categories, providing a comprehensive evaluation
of text embedding models.
This study aims to develop a general text em-
bedding model through a multi-stage training ap-
proach. In the initial stage of unsupervised con-
trastive learning, we generate weak supervised cor-
relation text pairs using publicly available data
from various sources. Unlike previous study (Wang
et al., 2022b), we exclusively utilized open-source
data and did not employ any filtering or cleaning
methods. Pre-training on a large-scale text pairs
can effectively improve the domain generalization
of text representation models and bridge the gap
between the MLM training objective and the con-
trastive learning objective of representation models,
making the language model more suitable for text
representation tasks. In the supervised fine-tuning
stage, the mixture of training data in our approach
is more varied to further enhance the model’s ver-
satility. Moreover, our model does not incorpo-
rate task-specific prompts, which enhances repro-
ducibility and ease of use.
3 Approach
The training process of our model consists of two
stages: unsupervised pre-training and supervised
fine-tuning. Both stages employ the learning ob-
jective of contrastive learning. Firstly, we will in-
troduce the basic framework of the model. Subse-
quently, we will discuss the sources and construc-
tion methods of the training data in the two stages.
Finally, we will present some special optimization
strategies used to enhance the model’s performance
during the training process.
3.1 Model Architecture
The backbone of our embedding model is a deep
Transformer encoder (Vaswani et al., 2017) which
can be initialized with pre-trained language models
such as BERT (Devlin et al., 2019). Our model
follows the vanilla dual-encoder architecture with
mean pooling on top of the contextualized token
representations produced by the language model.
Formally, given a piece of textx = (x1, . . . , xn)
consisting of n tokens, an embedding modelE con-
vert the text into a low-dimensional dense vector
x = E(x) ∈ Rd. To implement E, we first employ
a language model to get the deep contextualized
token representations
h = LM(x) ∈ Rn×d. (1)
Then we apply a lightweight mean pooling
across the first dimension to get the text representa-
tion,
x = 1
n
nX
i=1
hi ∈ Rd (2)
The text representations are learned through the
contrastive objective, distinguishing semantic rele-
vant text pairs from irrelevant ones. Such training
procedure requires positive and negative pairs, tak-
ing the format of (q, d+, d−). For a query q, a rel-
evant document d+, a set of irrelevant documents
D− = {d−
1 , . . . , d−
n }, one popular contrastive ob-
jective is the InfoNCE loss (van den Oord et al.,
2018),
Lcl = −log es(q,d+)/τ
es(q,d+)/τ +
nP
i=1
es(q,d−
i )/τ
, (3)



### Claim 33/36

#### Claim Text
The authors are grateful to the Joint Supercomputer Cen ter of RAS for the provided supercomputer sources. T.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 16):

Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson,
Itziar Gonzalez-Dios, Javier de la Rosa, Jenny
Chim, Jesse Dodge, Jian Zhu, Jonathan Chang,
Jorg Frohberg, Josephine L. Tobing, Joydeep
Bhattacharjee, Khalid Almubarak, Kimbo Chen,
Kyle Lo, Leandro von Werra, Leon Weber, Long
Phan, Loubna Ben Allal, Ludovic Tanguy, Manan
Dey, Manuel Romero Muñoz, Maraim Masoud,
Mar’ia Grandury, Mario vSavsko, Max Huang,
Maximin Coavoux, Mayank Singh, Mike Tian-
Jian Jiang, Minh Chien Vu, Mohammad Ali
Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora
Kassner, Nurulaqilla Khamis, Olivier Nguyen,
Omar Espejel, Ona de Gibert, Paulo Villegas,
Peter Henderson, Pierre Colombo, Priscilla A.
Amuok, Quentin Lhoest, Rheza Harliman, Rishi
Bommasani, Roberto L’opez, Rui Ribeiro, Salomey
Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose,
Shamsuddeen Hassan Muhammad, Shanya Sharma,
S. Longpre, Somaieh Nikpoor, Stanislav Silberberg,
Suhas Pai, Sydney Zink, Tiago Timponi Torrent,
Timo Schick, Tristan Thrush, Valentin Danchev,
Vassilina Nikoulina, Veronika Laippala, Violette
Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak
Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si,
Elizabeth Salesky, Sabrina J. Mielke, Wilson Y . Lee,
Abheesht Sharma, Andrea Santilli, Antoine Chaffin,
Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla,
Gunjan Chhablani, Han Wang, Harshit Pandey,
Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo
Gao, Lintang Sutawika, M Saiful Bari, Maged S.
Al-shaibani, Matteo Manica, Nihal V . Nayak, Ryan
Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-
David, Stephen H. Bach, Taewoon Kim, Tali Bers,
Thibault Févry, Trishala Neeraj, Urmish Thakker,
Vikas Raunak, Xiang Tang, Zheng Xin Yong, Zhiqing
Sun, Shaked Brody, Y Uri, Hadar Tojarieh, Adam
Roberts, Hyung Won Chung, Jaesung Tae, Jason
Phang, Ofir Press, Conglong Li, Deepak Narayanan,
Hatim Bourfoune, Jared Casper, Jeff Rasley,
Max Ryabinin, Mayank Mishra, Minjia Zhang,
Mohammad Shoeybi, Myriam Peyrounette, Nicolas
Patry, Nouamane Tazi, Omar Sanseviero, Patrick von
Platen, Pierre Cornette, Pierre Franccois Lavall’ee,
Rémi Lacroix, Samyam Rajbhandari, Sanchit
Gandhi, Shaden Smith, Stéphane Requena, Suraj
Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet
Singh, Anastasia Cheveleva, Anne-Laure Ligozat,
Arjun Subramonian, Aur’elie N’ev’eol, Charles
Lovering, Daniel H Garrette, Deepak R. Tunuguntla,
Ehud Reiter, Ekaterina Taktasheva, Ekaterina
V oloshina, Eli Bogdanov, Genta Indra Winata,
Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina
Novikova, Jessica Zosa Forde, Xiangru Tang, Jungo
Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat,
Miruna Clinciu, Najoung Kim, Newton Cheng,
Oleg Serikov, Omer Antverg, Oskar van der Wal,
Rui Zhang, Ruochen Zhang, Sebastian Gehrmann,
Shachar Mirkin, S. Osher Pais, Tatiana Shavrina,
Thomas Scialom, Tian Yun, Tomasz Limisiewicz,
Verena Rieser, Vitaly Protasov, Vladislav Mikhailov,
Yada Pruksachatkun, Yonatan Belinkov, Zachary
Bamberger, Zdenvek Kasner, Alice Rueda, Amanda
Pestana, Amir Feizpour, Ammar Khan, Amy
Faranak, Ananda Santa Rosa Santos, Anthony
Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo
Abdollahi, Aycha Tammour, Azadeh HajiHosseini,
Bahareh Behroozi, Benjamin Olusola Ajibade,
Bharat Kumar Saxena, Carlos Muñoz Ferrandis,
Danish Contractor, David M. Lansky, Davis David,
Douwe Kiela, Duong Anh Nguyen, Edward Tan,
Emily Baylor, Ezinwanne Ozoani, Fatim T Mirza,
Frankline Ononiwu, Habib Rezanejad, H.A. Jones,
Indrani Bhattacharya, Irene Solaiman, Irina Sedenko,
Isar Nejadgholi, Jan Passmore, Joshua Seltzer,
Julio Bonis Sanz, Karen Fort, Lívia Macedo
Dutra, Mairon Samagaio, Maraim Elbadri, Margot
Mieskes, Marissa Gerchick, Martha Akinlolu,
Michael McKenna, Mike Qiu, M. K. K. Ghauri,
Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour
Elkott, Nourhan Fahmy, Olanrewaju Samuel, Ran
An, R. P. Kromann, Ryan Hao, Samira Alizadeh,
Sarmad Shubber, Silas L. Wang, Sourav Roy, Sylvain
Viguier, Thanh-Cong Le, Tobi Oyebade, Trieu
Nguyen Hai Le, Yoyo Yang, Zachary Kyle Nguyen,
Abhinav Ramesh Kashyap, A. Palasciano, Alison
Callahan, Anima Shukla, Antonio Miranda-Escalada,
Ayush Kumar Singh, Benjamin Beilharz, Bo Wang,
Caio Matheus Fonseca de Brito, Chenxi Zhou, Chirag
Jain, Chuxin Xu, Clémentine Fourrier, Daniel Le’on
Perin’an, Daniel Molano, Dian Yu, Enrique
Manjavacas, Fabio Barth, Florian Fuhrimann,
Gabriel Altay, Giyaseddin Bayrak, Gully Burns,
Helena U. Vrabec, Iman I.B. Bello, Isha Dash, Ji Soo
Kang, John Giorgi, Jonas Golde, Jose David Posada,
Karthi Sivaraman, Lokesh Bulchandani, Lu Liu,
Luisa Shinzato, Madeleine Hahn de Bykhovetz,
Maiko Takeuchi, Marc Pàmies, María Andrea
Castillo, Marianna Nezhurina, Mario Sanger,
Matthias Samwald, Michael Cullan, Michael
Weinberg, M Wolf, Mina Mihaljcic, Minna
Liu, Moritz Freidank, Myungsun Kang, Natasha
Seelam, Nathan Dahlberg, Nicholas Michio Broad,
Nikolaus Muellner, Pascale Fung, Patricia Haller,
R. Chandrasekhar, R. Eisenberg, Robert Martin,
Rodrigo L. Canalli, Rosaline Su, Ruisi Su, Samuel
Cahyawijaya, Samuele Garda, Shlok S Deshmukh,
Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee
Sang-aroonsiri, Srishti Kumar, Stefan Schweter,
Sushil Pratap Bharati, T. A. Laud, Th’eo Gigant,
Tomoya Kainuma, Wojciech Kusa, Yanis Labrak,
Yashasvi Bajaj, Y . Venkatraman, Yifan Xu, Ying
Xu, Yun chao Xu, Zhee Xao Tan, Zhongli Xie,
Zifan Ye, Mathilde Bras, Younes Belkada, and
Thomas Wolf. 2022. Bloom: A 176b-parameter
open-access multilingual language model. ArXiv,
abs/2211.05100.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proximal
policy optimization algorithms. arXiv preprint
arXiv:1707.06347.
Noam Shazeer. 2020. Glu variants improve transformer.
arXiv preprint arXiv:2002.05202.
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 5):

models, we can obtain a mapping from the training
flops to the target loss.
Figure 4: The scaling law of Baichuan 2. We trained
various models ranging from 10 million to 3 billion
parameters with 1 trillion tokens. By fitting a power
law term to the losses given training flops, we predicted
losses for training Baichuan 2-7B and Baichuan 2-13B
on 2.6 trillion tokens. This fitting process precisely
predicted the final models’ losses (marked with two
stars).
To fit the scaling law of the model, we employed
the formula given by Henighan et al. (2020):
LC = a × Cb + L∞ (2)
where L∞ is the irreducible loss and the first
term is the reducible loss which is formulated as a
power-law scaling term. C are training flops and
the LC are final loss of the model in that flops. We
used the curve_fit function from the SciPy 4
library to fit the parameters. The final fitted scaling
curve and the predicted 7 billion and 13 billion
parameters model’s final loss are shown in Figure
4. We can see that the fitted scaling law predicted
Baichuan 2’s final loss with high accuracy.
2.7 Infrastructure
Efficiently leveraging existing GPU resources
plays a critically important role in training and
developing large language models today. To
accomplish this, we develop a co-design approach
for an elastic training framework and a smart
cluster scheduling policy.
Since our GPUs are shared among multiple
users and tasks, the specific behavior of each
task is unpredictable, often leading to idle GPU
nodes within the cluster. Considering that a single
machine equipped with eight A800 GPUs could
adequately meet the memory requirements for our
Baichuan 2-7B and Baichuan 2-13B models, the
4https://scipy.org/
primary design criterion for our training framework
is the machine-level elasticity, which supports that
resources for tasks can be dynamically modified
according to the cluster status and thereby serves as
the foundation for our smart scheduling algorithm.
To meet the requirement of the machine-level
elasticity, our training framework integrates tensor
parallelism (Narayanan et al., 2021) and ZeRO-
powered data parallelism (Rajbhandari et al.,
2020), where we set tensor parallelism inside each
machine and employ ZeRO shared data parallelism
for elastic scaling across machines.
In addition, we employ a tensor-splitting
technique (Nie et al., 2022) where we split certain
calculations to reduce peak memory consumption,
such as the cross-entropy calculations with large
vocabularies. This approach enables us to meet
memory needs without extra computing and
communication, making the system more efficient.
To further accelerate training without
compromising model accuracy, we implement
mixed-precision training, where we perform
forward and backward computations in BFloat16,
while performing optimizer updating in Float32.
Furthermore, in order to efficiently scale our
training cluster to thousands of GPUs, we integrate
the following techniques to avoid the degradation
of communication efficiency:
• Topology-aware distributed training. In large-
scale clusters, network connections frequently
span multiple layers of switches. We strategically
arrange the ranks for distributed training
to minimize frequent access across different
switches, which reduces latency and thereby
enhances overall training efficiency.
• Hybrid and hierarchical partition for ZeRO .
By partitioning parameters across GPUs,
ZeRO3 reduces memory consumption at the
expense of additional all-gather communications.
This approach would lead to a significant
communication bottleneck when scaling to
thousands of GPUs (Jiang et al., 2023a). To
address this issue, we propose a hybrid and
hierarchical partitioning scheme. Specifically,
our framework first partitions the optimizer states
across all GPUs, and then adaptively decides
which layers need to activate ZeRO3, and
whether partitioning parameters hierarchically.
By integrating these strategies, our system is
capable of training Baichuan 2-7B and Baichuan
2-13B models efficiently on 1,024 NVIDIA A800



Source: data\tc17_2312.15883v2\referenced_papers\[93]_2311.09210.pdf (Page 12):

bfloat16 precision. The learning rates are set to
{1e-6, 2e-6, 5e-6, 1e-5, 2e-5}, and the empirical
results indicated that 5e-6 yielded the best model
performance, hence we standardized the learning
rate for all reported numbers. Greedy decoding
is applied during inference on all experiments to
ensure deterministic generations.
A.5 Instruction Prompts
(1) For standard RALM, the instruction is:
Task Description: The primary objective is to
briefly answer the question.
(2) For RALM with COT, the instruction is:
Task Description: Think step by step to answer the
question.
(3) For RALM with CON, the instruction is:
Task Description:
1. Read the given question and five Wikipedia
passages to gather relevant information.
2. Write reading notes summarizing the key points
from these passages.
3. Discuss the relevance of the given question and
Wikipedia passages.
4. If some passages are relevant to the given
question, provide a brief answer based on the
passages.
5. If no passage is relevant, direcly provide answer
without considering the passages.
A.6 Case Studies
In our case studies, as illustrated in Table 6, we
compare the responses generated by the standard
RALM and our enhanced RALM with COT. These
examples highlight the differences in how each
model processes and interprets information from
retrieved documents.
The first case shows a question pertains to the
most recent Summer Olympics held in the USA.
The standard RALM is misled by the mention of
"Chicago’s bid for the 2016 Summer Olympics."
Lacking a deep comprehension of the content, it
incorrectly focuses on the more recent year (2016),
resulting in an inaccurate answer. In contrast, the
RALM with CON carefully analyzes the informa-
tion. It notes that while Chicago bid for the 2016
Olympics, there’s no confirmation of it being a suc-
cessful bid. This leads to the correct conclusion
that the most recent Olympics in the USA were
held in 1996. The second case study involves iden-
tifying the language of the first Jnanpith Award
recipient. Here, the standard RALM fails to syn-
thesize information across documents. It identifies
G. Sankara Kurup as the award recipient but does
not connect this information to the language of his
work. Conversely, the RALM with CON effec-
tively combines details from both documents. It
recognizes that while the first document mentions
Kurup’s award, the second document provides the
missing language detail, leading to the correct an-
swer of Malayalam.
A.7 Licenses
The four open-domain QA benchmarks, LLaMa-2
models are all released under MIT License. They
are all for research purposes, and our experiments
are consistent with their intended usage.



Source: data\tc17_2312.15883v2\referenced_papers\[86]_2312.14335.pdf (Page 8):

[SLM17] Abigail See, Peter J. Liu, and Christopher D. Mannin g. “Get T o The Point: Summarization with
Pointer-Generator Networks”. In: Proceedings of the 55th Annual Meeting of the Association fo r
Computational Linguistics (V olume 1: Long Papers) . Ed. by Regina Barzilay and Min-Y en Kan.
V ancouver , Canada: Association for Computational Linguis tics, July 2017, pp. 1073–1083. DOI :
10.18653/v1/P17-1099. U RL : https://aclanthology.org/P17-1099.
[T ea23] MosaicML NLP T eam. Introducing MPT-7B: A New Standard for Open-Source, Commer cially Usable
LLMs. Accessed: 2023-05-05. 2023. U RL : www.mosaicml.com/blog/mpt-7b (visited on 05/05/2023).
[Tia+23] Katherine Tian et al. “Fine-tuning Language Model s for Factuality”. In: arXiv preprint arXiv:2311.08401
(2023).
[T ou+23] Hugo T ouvron et al. “Llama: Open and efﬁcient found ation language models”. In: arXiv preprint
arXiv:2302.13971 (2023).
[V ar+23] Neeraj V arshney et al. A Stitch in Time Saves Nine: Detecting and Mitigating Halluc inations of LLMs
by V alidating Low-Conﬁdence Generation . 2023. arXiv: 2307.03987 [cs.CL].
[XC23] Zhichao Xu and Daniel Cohen. “A Lightweight Constrai ned Generation Alternative for Query-
focused Summarization”. In: arXiv preprint arXiv:2304.11721 (2023).
[Xu+23] Zhichao Xu et al. “Counterfactual Editing for Searc h Result Explanation”. In: arXiv preprint
arXiv:2301.10389 (2023).
[Zha+19] Tianyi Zhang et al. “Bertscore: Evaluating text ge neration with bert”. In: arXiv preprint arXiv:1904.09675
(2019).
7 Appendix
7.1 Prompting T emplates
W e report the prompting templates in T able 4.
7.2 Hyperparameters
W e report the hyperparameters in T able 5.
9



Source: data\tc17_2312.15883v2\referenced_papers\[8]_2401.05856.pdf (Page 4):

Seven Failure Points When Engineering a Retrieval Augmented Generation System CAIN 2024, April 2024, Lisbon, Portugal
FP Lesson Description Case Studies
FP4 Larger context get better results (Context refers to a
particular setting or situation in which the content
occurs)
A larger context enabled more accurate responses
(8K vs 4K). Contrary to prior work with GPT-3.5 [13]
AI Tutor
FP1 Semantic caching drives cost and latency down RAG systems struggle with concurrent users due to
rate limits and the cost of LLMs. Prepopulate the
semantic cache with frequently asked questions [1].
AI Tutor
FP5-7 Jailbreaks bypass the RAG system and hit the safety
training.
Research suggests fine-tuning LLMs reverses safety
training [11], test all fine-tuned LLMs for RAG sys-
tem.
AI Tutor
FP2, FP4 Adding meta-data improves retrieval. Adding the file name and chunk number into the
retrieved context helped the reader extract the re-
quired information. Useful for chat dialogue.
AI Tutor
FP2, FP4-7 Open source embedding models perform better for
small text.
Opensource sentence embedding models performed
as well as closed source alternatives on small text.
BioASQ, AI Tutor
FP2-7 RAG systems require continuous calibration. RAG systems receive unknown input at runtime
requiring constant monitoring.
AI Tutor, BioASQ
FP1, FP2 Implement a RAG pipeline for configuration. A RAG system requires calibrating chunk size,
embedding strategy, chunking strategy, retrieval
strategy, consolidation strategy, context size, and
prompts.
Cognitive Reviewer,
AI Tutor, BioASQ
FP2, FP4 RAG pipelines created by assembling bespoke solu-
tions are suboptima.
End-to-end training enhances domain adaptation
in RAG systems [18].
BioASQ, AI Tutor
FP2-7 Testing performance characteristics are only possi-
ble at runtime.
Offline evaluation techniques such as G-Evals [14]
look promising but are premised on having access
to labelled question and answer pairs.
Cognitive Reviewer,
AI Tutor
Table 2: The lessons learned from the three case studies with key takeaways for future RAG implementations
sort out the security/privacy (who can access what). Furthermore,
as the foundation model itself evolves or you get new data to add to
the model, you will need to run finetuning again. On the other side,
RAG systems seem to offer a pragmatic solution allowing you to
chunk your data as needed and only use relevant chunks into the
context to ask the LLM to generate an answer from the included
context. This facilitates continuously updating the knowledge with
new documents and also gives the control over what chunks the user
is able to access. However, optimal strategies for chunk embedding,
retrieval, and contextual fusion remain active research. Further
work should systematically compare finetuning and RAG paradigms
across factors including accuracy, latency, operating costs, and
robustness.
6.3 Testing and Monitoring RAG systems
Software engineering best practices are still emerging for RAG sys-
tems. Software testing and test case generation are one of the areas
for refinement. RAG systems require questions and answers that are
application specific often unavailable when indexing unstructured
documents. Emerging work has considered using LLMs for gen-
erating questions from multiple documents [4]. How to generate
realistic domain relevant questions and answers remains an open
problem.
Once suitable test data is available quality metrics are also re-
quired to assist engineers in making quality tradeoffs. Using large
language models is expensive, introduces latency concerns, and has
performance characteristics that all change with each new release.
This characteristic has previously been studied for machine learn-
ing systems [5, 6] but the required adaptations (if any) have yet to
be applied to LLM based systems such as RAGs. Another idea is to
incorporate ideas from self-adaptive systems to support monitoring
and adapting RAG systems, preliminary work has started for other
machine learning applications [2].
7 CONCLUSION
RAG systems are a new information retrieval that leverages LLMs.
Software engineers increasingly interact with RAG systems a)
through implementing semantic search, or b) through new code-
dependent tasks. This paper presented the lessons learned from 3
case studies including an empirical investigation involving 15,000
documents and 1000 questions. Our findings provide a guide to
practitioners by presenting the challenges faced when implement-
ing RAG systems. We also included future research directions for
RAG systems related to 1) chunking and embeddings, 2) RAG vs
Finetuning, and 3) Testing and Monitoring. Large language models
are going to continue to obtain new capabilities of interest to engi-
neers and researchers. This paper presents the first investigation
into RAG systems from a software engineering perspective.
ACKNOWLEDGMENTS
To Amanda Edgar, Rajesh Vasa, Kon Mouzakis, Matteo Vergani,
Trish McCluskey, Kathryn Perus, Tara Draper, Joan Sutherland and
Ruary Ross for their support and involvement in making the AI
Tutor project possible.



### Claim 34/36

#### Claim Text
The second AnDi challenge took place in 2024 and aimed at evaluating the performances of various methods for detecting and quantifying changes in single-particle motion .

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 22):

Published as a conference paper at ICLR 2024
HotpotQA 6-shot, and 127 out of 1,000 instances for FEVER 3-shot. The plain ReAct method, on
the other hand, applies the ReAct prompt to every instance.
23



Source: data\tc17_2312.15883v2\referenced_papers\[45]_2308.03281.pdf (Page 4):

Task Type Text Pair Format Query Doc
Web Page (title, body) Providence Real Estate | Providence Homes for SaleFounded by Roger Williams in 1636, Providence isrecognized as one of the country’s oldest cities. . .
Academic Paper(title, abstract) Polymer Quantum Mechanics and its Continuum LimitA rather non-standard quantum representation of thecanonical commutation relations of quantum mechanics. . .
Hyperlink (citation, reference)After the championship in 1996, the PGA of Americaraised its stake to 50% and announced that . . .Pebble Beach Golf Links The largest margin of victoryever in a major championship, surpassing the 13-shot . . .
Social Media (post, comment)Pretty sure any team with Lebron James will be a playoffcontender. Considering UNC would be in the East. . .I was being sarcastic and making fun of the East, buthonestly I was really in deep thought about this . . .
Knowledge Base(entity, description) Animation Animation is the process of creating the illusion of motionand shape change by means of the rapid display of . . .
Community QA (question, answer)How the human species evolved? A tough question as it overlaps science and theology. Sinceyou asked “how the human species evolved?” I’ll assume . . .
News (summary, content) Nepalese Opposition Welcomes Return of ParliamentNepal’s opposition alliance formally calls off weeks ofpro-democracy protests after King Gyenandra reinstates . . .
Code (text, code) SetMaxRecords sets the MaxRecords field’s value.func (s *DescribeSnapshotCopyGrantsInput) SetMaxRecords(v int64) *DescribeSnapshotCopyGrantsInput { s.MaxRecords
Table 2: Examples of mined (query, document) pairs in the pre-training data.
Consider a batch of positive text pair samples
B = {(q1, d1), (q2, d2), ...,(qn, dn)},
we use an improved contrastive loss which takes
the form
Licl = −1
n
nX
i=1
log es(qi,di)/τ
Z (5)
with the partition function being
Z =
X
j
es(qi,dj )/τ +
X
j̸=i
es(qi,qj )/τ
+
X
j
es(qj ,di)/τ +
X
j̸=i
es(dj ,di)/τ (6)
in which the first two terms are used for query to
document contrast, where as the last two terms are
used for the inverse. In this work, we use the cosine
similarity as the distance metric
s(q, d) = q · d
||q||2 · ||d||2
. (7)
The temperature τ is fixed to 0.01 in this work.
Training and Evaluation The training of our
embedding model consists of two stages. In the
first stage of contrastive pre-training with only in-
batch negatives, using a large batch size is crucial
to better model performance by reducing the gap
between training and inference with more nega-
tives included and providing a better approximation
to the underlying learning objective. To facilitate
this, we limit the maximum sequence length to 128
during pre-training and distribute the use of nega-
tives across all GPUs. Popular techniques such as
automatic mixed precision training (Micikevicius
et al., 2018) with fp16, deepspeed ZeRO (Rajb-
handari et al., 2020) stage 1 and gradient check-
pointing (Chen et al., 2016) are also jointly used to
reduce memory cost and scale up batch size to over
ten thousands. We run the pre-training for 50, 000
steps, which roughly corresponds to one epoch on
the whole pre-training data. We only tuned the
learning rate to ensure the convergence of larger
models. we employ the AdamW optimizer with
linear learning rate decay and a warm-up period
during the initial 5% of training steps. We con-
ducted experiments on three distinct model scales:
small, base, and large. These models were initial-
ized using the small-sized MiniLM (Wang et al.,
2020) model and the base and large models of the
BERT (Devlin et al., 2019) model. Further details
can be found in Table 3.
In the second stage of contrastive fine-tuning
with supervised data and hard negatives, a large
batch size is unnecessary since hard negatives can
already provide a reliable gradient estimation of
the learning objective (Xiong et al., 2021; Li et al.,
2023). Therefore, a global batch size of 128 and a
train group size of 16 are utilized, with one positive
example and the remaining being either hard nega-
tives or random negatives. Instead we increase the
max sequence length to 512 to better handle texts
with longer lengths. The learning rate is decreased
by a factor of ten during fine-tuning. The model
is fine-tuned on the collected dataset for a single
epoch. In-batch texts are also incorporated as nega-
tive candidates using the enhanced contrastive loss
described in Equation 5.
After training, we directly take the last check-
point for evaluation. We run model training on up
to 8 NVIDIA A100 GPUs with 80GB memory and
model evaluation on up to 8 NVIDIA Tesla V100
GPUs with 32GB memory. Models are trained with
mixed precision using fp16 and evaluated with half
precision fp16 as well.



Source: data\tc17_2312.15883v2\referenced_papers\[77]_2308.08833.pdf (Page 9):

References
Asma Ben Abacha, Eugene Agichtein, Yuval Pinter, and
Dina Demner-Fushman. 2017. Overview of the med-
ical question answering task at TREC 2017 liveqa. In
Proceedings of The Twenty-Sixth Text REtrieval Con-
ference, TREC 2017, Gaithersburg, Maryland, USA,
November 15-17, 2017, volume 500-324 ofNIST Spe-
cial Publication. National Institute of Standards and
Technology (NIST).
Asma Ben Abacha, Yassine Mrabet, Mark Sharp,
Travis R. Goodwin, Sonya E. Shooshan, and Dina
Demner-Fushman. 2019. Bridging the gap between
consumers’ medication questions and trusted an-
swers. In MEDINFO 2019: Health and Wellbeing
e-Networks for All - Proceedings of the 17th World
Congress on Medical and Health Informatics, Lyon,
France, 25-30 August 2019, volume 264 of Studies
in Health Technology and Informatics, pages 25–29.
IOS Press.
DeepSeek AI. 2023. Deepseek llm: Let there
be answers. https://github.com/deepseek-ai/
DeepSeek-LLM.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,
Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,
Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong
Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-
guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,
Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,
Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-
uan Zhang, Yichang Zhang, Zhenru Zhang, Chang
Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang
Zhu. 2023. Qwen technical report. arXiv preprint
arXiv:2309.16609.
Baichuan. 2023. Baichuan 2: Open large-scale lan-
guage models. arXiv preprint arXiv:2309.10305.
Zhijie Bao, Wei Chen, Shengze Xiao, Kuang Ren, Jiaao
Wu, Cheng Zhong, Jiajie Peng, Xuanjing Huang, and
Zhongyu Wei. 2023. Disc-medllm: Bridging gen-
eral large language models and real-world medical
consultation.
Yan Cai, Linlin Wang, Ye Wang, Gerard de Melo,
Ya Zhang, Yanfeng Wang, and Liang He. 2024. Med-
bench: A large-scale chinese benchmark for evaluat-
ing medical large language models. In Thirty-Eighth
AAAI Conference on Artificial Intelligence, AAAI
2024, Thirty-Sixth Conference on Innovative Applica-
tions of Artificial Intelligence, IAAI 2024, Fourteenth
Symposium on Educational Advances in Artificial
Intelligence, EAAI 2014, February 20-27, 2024, Van-
couver, Canada, pages 17709–17717. AAAI Press.
Junying Chen, Xidong Wang, Anningzhe Gao, Feng
Jiang, Shunian Chen, Hongbo Zhang, Dingjie Song,
Wenya Xie, Chuyi Kong, Jianquan Li, Xiang Wan,
Haizhou Li, and Benyou Wang. 2023a. Huatuogpt-ii,
one-stage training for medical adaption of llms.
Yirong Chen, Zhenyu Wang, Xiaofen Xing, Zhipei Xu,
Kai Fang, Sihang Li, Junhong Wang, and Xiangmin
Xu. 2023b. Bianque-1.0: Improving the "question"
ability of medical chat model through finetuning with
hybrid instructions and multi-turn doctor qa datasets.
github.
Yiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient
and effective text encoding for chinese llama and
alpaca. arXiv preprint arXiv:2304.08177.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:
General language model pretraining with autoregres-
sive blank infilling. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 320–335.
Dan Hendrycks, Steven Basart, Saurav Kadavath, Man-
tas Mazeika, Akul Arora, Ethan Guo, Collin Burns,
Samir Puranik, Horace He, Dawn Song, and Jacob
Steinhardt. 2021a. Measuring coding challenge com-
petence with APPS. In Proceedings of the Neural
Information Processing Systems Track on Datasets
and Benchmarks 1, NeurIPS Datasets and Bench-
marks 2021, December 2021, virtual.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou
andchen2023bianque1 Mantas Mazeika, Dawn Song,
and Jacob Steinhardt. 2021b. Measuring massive
multitask language understanding. In 9th Inter-
national Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021 .
OpenReview.net.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. 2021c. Measuring mathematical
problem solving with the MATH dataset. In Pro-
ceedings of the Neural Information Processing Sys-
tems Track on Datasets and Benchmarks 1, NeurIPS
Datasets and Benchmarks 2021, December 2021, vir-
tual.
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei
Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,
Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. 2023.
C-eval: A multi-level multi-discipline chinese eval-
uation suite for foundation models. arXiv preprint
arXiv:2305.08322.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023. Mistral
7b. arXiv preprint arXiv:2310.06825.
Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,
Hanyi Fang, and Peter Szolovits. 2020. What dis-
ease does this patient have? A large-scale open do-
main question answering dataset from medical exams.
CoRR, abs/2009.13081.
Dongfang Li, Jindi Yu, Baotian Hu, Zhenran Xu, and
Min Zhang. 2023a. Explaincpe: A free-text explana-
tion benchmark of chinese pharmacist examination.



Source: data\tc17_2312.15883v2\referenced_papers\[45]_2308.03281.pdf (Page 16):

Task Type Text Triple Format query doc hard neg
Web Search (query, passage, negative) finger cellulitis symptomsThe following are the most commonsymptoms of cellulitis. However. . .Cellulitis usually begins asa small area of pain and . . .
Open QA (question, passage, negative) big little lies season 2how many episodesBig Little Lies (TV series).series garnered several accolades. . .Little People, Big World.final minutes of the season two. . .
Natural Language Inference (sentence, entailment, contradiction)(Read for Slate ’s takeon Jackson’s findings.)Slate had an opinionon Jackson’s findings. Slate did not hold any opinionon Jackson’s findings.
Fact Verification (argument, evidence, others)Roman Atwood is acontent creator.
Roman Bernard Atwood (bornMay 28, 1983) is an AmericanYouTube personality. . .
6th Streamy Awards Casey Neistatand Jesse Wellens, PrankvsPrank . . .
Paraphrase (sentence, paraphrase, others)Lexapro taken withcrestor any reaction?Can dayquil be taken with Lexapro?Can stopping lexapro causea longer period?
Table 12: Examples of (query, positive, negative) text triples in fine-tuning data.
smaller and harder to distinguish. In line with pre-
vious work, the main evaluation metric is MAP
(mean average precision).
Retrieval We omit the text retrieval evaluation
since it’s similar to that introduced in previous sec-
tion.
Pair Classification This task needs to assign a
label for a pair of texts. Popular tasks include
duplicate or paraphrase identification, where the
label is binary. The similarity score is the cosine
similarity between the embeddings of two texts.
The average precision score is reported as the main
evaluation metric using the best binary threshold.
Semantic Textual Similarity To determine the
similarity between a given pair of sentences, con-
tinuous scores are assigned, with higher values in-
dicating greater similarity. The embedding model
is employed to embed the sentences, and their sim-
ilarity is computed using cosine similarity. The
estimated similarity scores is compared against hu-
man labeled scores ranging from 1 to 5. We report
Spearman’s correlation, which measures the rank-
ings instead of the actual scores and better suits the
need of evaluating sentence embeddings.
Summarization This is a text generation evalua-
tion task which aims to automatically evaluate the
quality of generated text. For the summarization
task, the quality of each generated summary is com-
puted by measuring the cosine similarity between
its embedding and the embedding of the ground
truth references. In the case of multiple gold refer-
ences, the closest one with highest similarity score
is used for quality estimation. Similar to STS task,
we use the Spearman correlation between the rank-
ing produced by the text embedding model and the
human assessments for evaluation.
C Original CodeSearchNet Results
We list the results of the original setting on Code-
SearchNet in Table 13, where the retrieval cor-
pus contains 1k randomly sampled code snip-
pets. Compared to previous open-source code lan-
guage models with similar architecture and size
(CodeBERT (Feng et al., 2020) and GraphCode-
BERT (Guo et al., 2021)), our model is superior
in most programming languages. There is still a
performance gap to the code embedding model
trained by Neelakantan et al. (2022), which used
Codex (Chen et al., 2021) as backbone and trained
on a large-scale (code, text) pairs extracted from
open-source code. It is worthwhile to explore how
to further close this gap.



Source: data\tc17_2312.15883v2\referenced_papers\[82]_2309.07597.pdf (Page 7):

SIGIR ’24, July 14–18, 2024, Washington, DC, USA Shitao Xiao et al.
Table 5: Performance of English Models on MTEB.
Model Name Dim. Average Retrieval Cluster Pair CLF Re-rank STS Summarize CLF
GTE (large) 1024 63.13 52.22 46.84 85.00 59.13 83.35 31.66 73.33
GTE (base) 768 62.39 51.14 46.2 84.57 58.61 82.3 31.17 73.01
E5 (large) 1024 62.25 50.56 44.49 86.03 56.61 82.05 30.19 75.24
Instructor-XL 768 61.79 49.26 44.74 86.62 57.29 83.06 32.32 61.79
E5 (base) 768 61.5 50.29 43.80 85.73 55.91 81.05 30.28 73.84
GTE (small) 384 61.36 49.46 44.89 83.54 57.7 82.07 30.42 72.31
OpenAI Ada 002 1536 60.99 49.25 45.9 84.89 56.32 80.97 30.8 70.93
E5 (small) 384 59.93 49.04 39.92 84.67 54.32 80.39 31.16 72.94
ST5 (XXL) 768 59.51 42.24 43.72 85.06 56.42 82.63 30.08 73.42
MPNet (base) 768 57.78 43.81 43.69 83.04 59.36 80.28 27.49 65.07
SGPT Bloom (7.1B) 4096 57.59 48.22 38.93 81.9 55.65 77.74 33.60 66.19
BGE (small) 384 62.17 51.68 43.82 84.92 58.36 81.59 30.12 74.14
BGE (base) 768 63.55 53.25 45.77 86.55 58.86 82.4 31.07 75.53
BGE (large) 1024 64.23 54.29 46.08 87.12 60.03 83.11 31.61 75.97
BGE-f gives rise to better average performance. Besides, there are
more significant empirical advantages on retrieval, STS, pair classi-
fication, and re-rank. All these perspectives are closely related to
the training data at the final stage, i.e. C-MTP (labeled) , where
the model is fine-tuned on a small group of tasks. This indicates
that using instructions may substantially contribute to the quality of
task-specific fine-tuning.
One more characteristic is that we use a specifically pre-trained
text encoder to train BGE, rather than using common choices, like
BERT [15] and RoBERTa [29]. To explore its impact, we replace the
pre-trained text encoder with the widely used Chinese-RoBERTa15,
noted as “BGE w.o. pre-train”. According to the comparison between
BGE-pretrain and BGE w.o. pre-train, the using of pre-trained text
encoder notably improves the retrieval capability, while preserving
similar performances on other perspectives.
REFERENCES
[1] Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor
Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada
Mihalcea, et al . 2015. Semeval-2015 task 2: Semantic textual similarity, eng-
lish, spanish and pilot on interpretability. In Proceedings of the 9th international
workshop on semantic evaluation (SemEval 2015) . 252–263.
[2] Eneko Agirre, Carmen Banea, Claire Cardie, Daniel M Cer, Mona T Diab, Aitor
Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe.
2014. SemEval-2014 Task 10: Multilingual Semantic Textual Similarity.. In Se-
mEval@ COLING . 81–91.
[3] Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez Agirre,
Rada Mihalcea, German Rigau Claramunt, and Janyce Wiebe. 2016. Semeval-2016
task 1: Semantic textual similarity, monolingual and cross-lingual evaluation.
In SemEval-2016. 10th International Workshop on Semantic Evaluation; 2016 Jun
16-17; San Diego, CA. Stroudsburg (PA): ACL; 2016. p. 497-511. ACL (Association
for Computational Linguistics).
[4] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-
2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First Joint
Conference on Lexical and Computational Semantics–Volume 1: Proceedings of
the main conference and the shared task, and Volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation (SemEval 2012) . 385–393.
[5] Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013. * SEM 2013 shared task: Semantic textual similarity. In Second joint confer-
ence on lexical and computational semantics (* SEM), volume 1: proceedings of the
Main conference and the shared task: semantic textual similarity . 32–43.
[6] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher
Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu,
15huggingface.co/hfl/chinese-roberta-wwm-ext-large
Manan Dey, et al. 2023. SantaCoder: don’t reach for the stars! arXiv preprint
arXiv:2301.03988 (2023).
[7] Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian
Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. 2022. Task-aware retrieval with
instructions. arXiv preprint arXiv:2211.09260 (2022).
[8] Luiz Bonifacio, Vitor Jeronymo, Hugo Queiroz Abonizio, Israel Campiotti,
Marzieh Fadaee, Roberto Lotufo, and Rodrigo Nogueira. 2021. mmarco: A
multilingual version of the ms marco passage ranking dataset. arXiv preprint
arXiv:2108.13897 (2021).
[9] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Ruther-
ford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bog-
dan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving
from trillions of tokens. In International conference on machine learning . PMLR,
2206–2240.
[10] Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning.
2015. A large annotated corpus for learning natural language inference. arXiv
preprint arXiv:1508.05326 (2015).
[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877–1901.
[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-
bastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
arXiv preprint arXiv:2204.02311 (2022).
[13] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling
instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).
[14] Alexis Conneau and Douwe Kiela. 2018. SentEval: An Evaluation Toolkit for
Universal Sentence Representations. arXiv preprint arXiv:1803.05449 (2018).
[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding.arXiv
preprint arXiv:1810.04805 (2018).
[16] Luyu Gao and Jamie Callan. 2021. Condenser: a pre-training architecture for
dense retrieval. arXiv preprint arXiv:2104.08253 (2021).
[17] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason
Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and
Andy Zou. 2021. A framework for few-shot language model evaluation . https:
//doi.org/10.5281/zenodo.5371628
[18] Luyu Gao, Yunyi Zhang, Jiawei Han, and Jamie Callan. 2021. Scaling deep
contrastive learning batch size under memory limited setup. arXiv preprint
arXiv:2101.06983 (2021).
[19] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020.
Retrieval augmented language model pre-training. In International conference on
machine learning . PMLR, 3929–3938.
[20] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu,
Yizhong Wang, Hua Wu, Qiaoqiao She, et al. 2017. Dureader: a chinese machine
reading comprehension dataset from real-world applications. arXiv preprint
arXiv:1711.05073 (2017).
[21] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes



### Claim 35/36

#### Claim Text
This assumption is based on the fact that, under the effect of homophily , individuals tend to imitate the behavior of their peers.

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[47]_2307.03172.pdf (Page 9):

5 10 20 30 40 50
Number of Retrieved Docs
50
60
70
80
90Metric
claude-1.3
claude-1.3-100k
gpt-3.5-turbo-0613
gpt-3.5-turbo-16k-0613
mpt-30b-instruct
longchat-13b-16k
contriever recall
Figure 11: Retriever recall and model performance as a
function of the number of retrieved documents. Model
performance saturates long before retriever recall, indi-
cating that the models have difficulty making use of the
extra retrieved documents.
domain QA results. We see that reader model
performance saturates long before retriever per-
formance saturates, indicating that readers are not
effectively using the extra context. Using more
than 20 retrieved documents only marginally im-
proves reader performance ( ∼1.5% for GPT-3.5-
Turbo and∼1% for Claude-1.3), while significantly
increasing the input context length (and thus la-
tency and cost). These results, coupled with the
observation that models are often better at retriev-
ing and using information at the start or end of
the input contexts, suggest that effective rerank-
ing of retrieved documents (pushing relevant infor-
mation closer to the start of the input context) or
ranked list truncation (retrieving fewer documents
when appropriate; Arampatzis et al., 2009) may be
promising directions for improving how language-
model-based readers use retrieved context.
6 Related Work
6.1 Long-Context Language Models
There is much prior work in designing performant
language models with cheaper scaling than Trans-
formers in the context length. Many lines of work
pursue Transformer variants with attention modi-
fications like recurrence (Dai et al., 2019), factor-
izing attention into computationally less intensive
approximations (Beltagy et al., 2020; Zaheer et al.,
2020), or low-rank approximations (Wang et al.,
2020; Peng et al., 2021). Dao et al. (2022) in-
stead provide a faster exact attention by a carefully-
crafted IO-aware CUDA kernel. Separately, there
are attempts to do away with attention entirely to
remove quadratic sequence length complexity, of-
ten through convolution and/or linear RNNs, e.g.,
in RWKV (Peng, 2023), S4 (Gu et al., 2022), or
Hyena (Poli et al., 2023). Many prior efforts evalu-
ate perplexity on a diverse web corpus as a proxy
for the ability to process long contexts; this work
shows that precise knowledge access on long con-
texts may be an added challenge.
6.2 How Do Language Models Use Context?
The pioneering work of Khandelwal et al. (2018)
showed that small LSTM language models make
increasingly coarse use of longer-term context;
Sankar et al. (2019) found similar results in di-
alogue models. In a similar vein, Daniluk et al.
(2017) find that attentive LSTM language mod-
els tend to mainly use recent history. Petroni
et al. (2020) were among the first to demonstrate
the potential of combining context from an in-
formation retrieval system with a pretrained lan-
guage models for unsupervised question answering.
O’Connor and Andreas (2021) found that many
information-destroying operations had marginal ef-
fects on Transformer LMs’ predictions. Krishna
et al. (2022) found that long-context neural gen-
eration in modestly-sized Transformer language
models degenerates because models fail to prop-
erly condition on long context. Finally, studying
long-context models, Sun et al. (2021) found that
longer contexts improves prediction of only a few
tokens, an empirical finding consistent with the
theory of Sharan et al. (2018), who showed that
sequence distributions with bounded mutual infor-
mation necessarily lead to marginalaverage predic-
tion benefits from increasingly long context. Qin
et al. (2023) analyze how efficient Transformers
perform on a variety of long-context downstream
NLP tasks, finding that long-context transformers
are recency-biased and do not effectively use long-
range context.
6.3 The Serial-Position Effect
The U-shaped curve we observe in this work has
a connection in psychology known as the serial-
position effect (Ebbinghaus, 1913; Murdock Jr,
1962), that states that in free-association recall
of elements from a list, humans tend to best re-
member the first and last elements of the list. The
serial-position effect plays a role in understanding
how humans develop short- and long-term mem-



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 11):

12  
Discussion In this work, we introduce a simple but highly effective framework that combines a biomedical knowledge graph with LLM chat models in a token optimized fashion. This integration resulted in a domain-specific generative system whose responses were firmly grounded in well-established biomedical knowledge. We compared the proposed framework with another RAG approach that utilizes Cypher query and showed that KG-RAG was more robust to prompt perturbation and more efficient in token utilization. In addition, KG-RAG consistently demonstrated superior performance compared to the prompt-based baseline LLM model on all human-curated benchmark datasets. We hypothesize that this performance improvement arises from the fusion of the explicit knowledge from the KG and the implicit knowledge from the LLM. This shows the value of providing domain-specific ground truth at a fine-grained resolution as context at the prompt level.   A heterogeneous knowledge graph with diverse concepts (the biomedical concepts in this case) interconnected at a massive scale has the potential to generate new knowledge as an “emergent property”.(Baranzini et al. 2022; Morris et al. 2023) In fact, as LLMs scale up in various dimensions like model parameters, training data, and training compute, they have been thought to exhibit reasoning or “emerging capabilities”(Wei et al. 2022a) although this observation could also be explained by “in-context learning” or other aspects of the examples.(Brown et al. 2020; Min et al. 2022; Lu et al. 2023) In any case, KG-RAG capitalized this capability and generated biomedical text with rich annotations such as provenance and statistical evidence (if available) thereby resulting in more reliable and knowledge-grounded responses. Additionally, the optimized and fine-grained context retrieval capability of KG-RAG ensured a budget friendly RAG system to apply on proprietary LLMs.



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 7):

Published as a conference paper at ICLR 2024
Table 5: Parallel vs. dynamic
knowledge adapting.
Method HotpotQA (3-shot)
CoT 29.9%Verify-and-Edit 31.8%CoK (parallel) 31.2%CoK (dynamic) 34.1%
Table 6: Comparison of the
factual accuracy of rationales
on HotpotQA.
Method Rationale 1 Rationale 2
CoT-SC 54.3% 52.1%CoK 66.3% 69.5%
Table 7: Human study results
on the factuality of reasoning
chains.
Predictions CoK CoT-SC Tie
Correct predictions68% 4% 28%Incorrect predictions44% 24% 32%All predictions56% 14% 30%
5 A NALYSIS
5.1 S INGLE VS . M ULTIPLE KNOWLEDGE DOMAINS AND SOURCES
As outlined in Section 2.1, CoK integrates a step to select the appropriate knowledge domains for
each question. This step is crucial to ensure that CoK can retrieve the most pertinent knowledge to
correct the rationales and answer the questions accurately. It is possible that multiple knowledge
domains can be chosen for one question, and within each domain, there are several knowledge
sources. In this subsection, we investigate the necessity of utilizing multiple knowledge domains
and sources. We also include an evaluation of the domain selection performance in Appendix F.1.
Single vs. Multiple Knowledge Domains We show the domain distributions identified for each
dataset in Figure 3. Notably, we find that CoK predominantly selects one knowledge domain for
each dataset, while a small number of cases call for multiple domains. For instance, the primary
knowledge domain for MedMCQA is Medical, and 17.8% of the questions identify Biology as a rel-
evant domain as well. Furthermore, we conduct ablation experiments to demonstrate the necessity
of utilizing multiple domains. As shown in Table 4, compared to only using Medical domain knowl-
edge, CoK using additional knowledge from the Biology domain further improves the performance
by 1.3%. This indicates that knowledge spanning multiple domains is needed for answering some
questions, underscoring the necessity of incorporating various knowledge domains.
Single vs. Multiple Knowledge Sources Within one domain, numerous credible knowledge
sources exist, and it is unfeasible for a single source to encompass all knowledge from the domain.
Therefore, it is important to utilize multiple knowledge sources within one domain. For instance,
as shown in Table 4, the performance of CoK improves by 2.1% when utilizing both Flashcard and
UpToDate as medical knowledge sources, compared to using only Flashcard. 7
5.2 P ARALLEL VS . DYNAMIC KNOWLEDGE ADAPTING
As aforementioned, dynamic knowledge adapting helps CoK prevent error propagation, here we take
a closer look at how much improvement it brings in. As shown in Table 5, the performance of CoK
improves by 4.2% compared with CoT when dynamic knowledge adapting is applied. However,
parallel editing leads to poorer performance due to error propagation for rationales.
5.3 E VALUATING FACTUALITY IMPROVEMENT OF THE RATIONALES
While the main results have shown that CoK effectively improves the performance of LLMs in
knowledge-intensive tasks, we are also interested in reducing hallucination for the generated ratio-
nales. Hence, we conduct quantitative and qualitative evaluations to assess the factual accuracy.
Quantitative Evaluation To automatically evaluate how CoK can reduce hallucination in the
model outputs, we employ an existing fact-checking method to compare the original and corrected
rationales. Specifically, we use ProgramFC (Pan et al., 2023) which is a state-of-the-art method for
judging the factuality of claims with respect to Wikipedia. As shown in Table 6, we observe that
CoK has improved factual accuracy compared to the CoT-SC baseline on the HotpotQA dataset. No-
tably, the factual accuracy of CoT-SC decreases for rationale 2 compared to rationale 1, which could
7Note that using external sources may have limitations such as noise or conflicts between different sources.
We mainly address this by selecting authoritative knowledge sources, and discuss this further in Appendix G
8



Source: data\tc17_2312.15883v2\referenced_papers\[65]_2311.17330.pdf (Page 24):

25  
References Baranzini SE, Börner K, Morris J et al. A biomedical open knowledge network harnesses the power of AI to understand deep human biology. AI Magazine 2022;43:46–58. Bauer L, Wang Y, Bansal M. Commonsense for generative multi-hop question answering tasks. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Stroudsburg, PA, USA: Association for Computational Linguistics, 2018. Beltagy I, Lo K, Cohan A. SciBERT: A Pretrained Language Model for Scientific Text. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019, 3615–20. Bizon C, Cox S, Balhoff J et al. ROBOKOP KG and KGB: Integrated Knowledge Graphs from Federated Sources. J Chem Inf Model 2019;59:4968–73. Bratanic T. Using a Knowledge Graph to Implement a DevOps RAG Application. Neo4j Developer Blog 2023. Brate R, Dang M-H, Hoppe F et al. Improving language model predictions via prompts enriched with knowledge graphs. 2022, DOI: 10.5445/IR/1000151291. Brown TB, Mann B, Ryder N et al. Language Models are Few-Shot Learners. 2020. Chen L, Zaharia M, Zou J. How is ChatGPT’s behavior changing over time? 2023. Deka P, Jurek-Loughrey A, Deepak. Improved methods to aid unsupervised evidence-based fact checking for online heath news. Journal of Data Intelligence 2022;3:474–504. Devlin J, Chang M-W, Lee K et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 2018. Dhungana K. An Overview of ChromaDB: The Vector Database. Medium 2023. Feng Y, Chen X, Lin BY et al. Scalable multi-hop relational reasoning for knowledge-aware question answering. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Stroudsburg, PA, USA: Association for Computational Linguistics, 2020. Himmelstein DS, Baranzini SE. Heterogeneous Network Edge Prediction: A Data Integration Approach to Prioritize Disease-Associated Genes. PLoS Comput Biol 2015;11:e1004259. Himmelstein DS, Lizee A, Hessler C et al. Systematic integration of biomedical knowledge prioritizes drugs for repurposing. 2017, DOI: 10.7554/eLife.26726.



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 0):

Published as a conference paper at ICLR 2024
CHAIN -OF-KNOWLEDGE : G ROUNDING LARGE LAN-
GUAGE MODELS VIA DYNAMIC KNOWLEDGE ADAPT-
ING OVER HETEROGENEOUS SOURCES
Xingxuan Li1,2∗† , Ruochen Zhao2∗‡ , Yew Ken Chia1,3∗† , Bosheng Ding1,2†, Shafiq Joty2,4
Soujanya Poria3, Lidong Bing1,5
1DAMO Academy, Alibaba Group, Singapore, 2Nanyang Technological University,
3Singapore University of Technology and Design, 4Salesforce Research,
5Hupan Lab, 310023, Hangzhou, China
{xingxuan.li, yewken.chia, bosheng.ding, l.bing}@alibaba-inc.com
{ruochen002, srjoty}@ntu.edu.sg sporia@sutd.edu.sg
ABSTRACT
We present chain-of-knowledge (CoK) , a novel framework that augments large
language models (LLMs) by dynamically incorporating grounding information
from heterogeneous sources. It results in more factual rationales and reduced
hallucination in generation. Specifically, CoK consists of three stages: reason-
ing preparation, dynamic knowledge adapting, and answer consolidation. Given
a knowledge-intensive question, CoK first prepares several preliminary rationales
and answers while identifying the relevant knowledge domains. If there is no ma-
jority consensus among the answers from samples, CoK corrects the rationales
step by step by adapting knowledge from the identified domains. These corrected
rationales can plausibly serve as a better foundation for the final answer consolida-
tion. Unlike prior studies that primarily use unstructured data, CoK also leverages
structured knowledge sources such as Wikidata and tables that provide more re-
liable factual information. To access both unstructured and structured knowledge
sources in the dynamic knowledge adapting stage, we propose an adaptive query
generator that allows the generation of queries for various types of query lan-
guages, including SPARQL, SQL, and natural sentences. Moreover, to minimize
error propagation between rationales, CoK corrects the rationales progressively
using preceding corrected rationales to generate and correct subsequent rationales.
Extensive experiments show that CoK consistently improves the performance of
LLMs on knowledge-intensive tasks across different domains. Our code is avail-
able at https://github.com/DAMO-NLP-SG/chain-of-knowledge.
1 I NTRODUCTION
In recent years, large language models (LLMs) such as ChatGPT (OpenAI, 2023) have demonstrated
impressive language generation capabilities (Cheng et al., 2023; Ding et al., 2023; Chen et al., 2024).
However, one major challenge of LLMs lies in hallucination, which is their tendency to confidently
generate plausible but factually incorrect texts (Ji et al., 2023; Zhao et al., 2023b). As shown in
Figure 1, given a question, “What year was the Argentine actor who directed El Tio Disparate born?”
which requires factual knowledge to answer, the most advanced LLMs often provide an incorrect
answer. While LLMs have the remarkable capability to recall information from their training data,
effectively updating or controlling the factual knowledge within these models remains challenging
(Luo et al., 2023).
∗ Equal contribution.
† Xingxuan Li, Yew Ken Chia, and Bosheng Ding are under the Joint Ph.D. Program between DAMO
Academy and their corresponding universities.
‡ Ruochen Zhao is under the AISG Ph.D. Fellowship Programme.
1
arXiv:2305.13269v4  [cs.CL]  21 Feb 2024



### Claim 36/36

#### Claim Text
The terms of O(u3) is included here to ensure that the Navier-Stokes equation is exactly recovered under the Chapman-Enskog approximation .

#### Retrieved Documents
Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 16):

Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson,
Itziar Gonzalez-Dios, Javier de la Rosa, Jenny
Chim, Jesse Dodge, Jian Zhu, Jonathan Chang,
Jorg Frohberg, Josephine L. Tobing, Joydeep
Bhattacharjee, Khalid Almubarak, Kimbo Chen,
Kyle Lo, Leandro von Werra, Leon Weber, Long
Phan, Loubna Ben Allal, Ludovic Tanguy, Manan
Dey, Manuel Romero Muñoz, Maraim Masoud,
Mar’ia Grandury, Mario vSavsko, Max Huang,
Maximin Coavoux, Mayank Singh, Mike Tian-
Jian Jiang, Minh Chien Vu, Mohammad Ali
Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora
Kassner, Nurulaqilla Khamis, Olivier Nguyen,
Omar Espejel, Ona de Gibert, Paulo Villegas,
Peter Henderson, Pierre Colombo, Priscilla A.
Amuok, Quentin Lhoest, Rheza Harliman, Rishi
Bommasani, Roberto L’opez, Rui Ribeiro, Salomey
Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose,
Shamsuddeen Hassan Muhammad, Shanya Sharma,
S. Longpre, Somaieh Nikpoor, Stanislav Silberberg,
Suhas Pai, Sydney Zink, Tiago Timponi Torrent,
Timo Schick, Tristan Thrush, Valentin Danchev,
Vassilina Nikoulina, Veronika Laippala, Violette
Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak
Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si,
Elizabeth Salesky, Sabrina J. Mielke, Wilson Y . Lee,
Abheesht Sharma, Andrea Santilli, Antoine Chaffin,
Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla,
Gunjan Chhablani, Han Wang, Harshit Pandey,
Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo
Gao, Lintang Sutawika, M Saiful Bari, Maged S.
Al-shaibani, Matteo Manica, Nihal V . Nayak, Ryan
Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-
David, Stephen H. Bach, Taewoon Kim, Tali Bers,
Thibault Févry, Trishala Neeraj, Urmish Thakker,
Vikas Raunak, Xiang Tang, Zheng Xin Yong, Zhiqing
Sun, Shaked Brody, Y Uri, Hadar Tojarieh, Adam
Roberts, Hyung Won Chung, Jaesung Tae, Jason
Phang, Ofir Press, Conglong Li, Deepak Narayanan,
Hatim Bourfoune, Jared Casper, Jeff Rasley,
Max Ryabinin, Mayank Mishra, Minjia Zhang,
Mohammad Shoeybi, Myriam Peyrounette, Nicolas
Patry, Nouamane Tazi, Omar Sanseviero, Patrick von
Platen, Pierre Cornette, Pierre Franccois Lavall’ee,
Rémi Lacroix, Samyam Rajbhandari, Sanchit
Gandhi, Shaden Smith, Stéphane Requena, Suraj
Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet
Singh, Anastasia Cheveleva, Anne-Laure Ligozat,
Arjun Subramonian, Aur’elie N’ev’eol, Charles
Lovering, Daniel H Garrette, Deepak R. Tunuguntla,
Ehud Reiter, Ekaterina Taktasheva, Ekaterina
V oloshina, Eli Bogdanov, Genta Indra Winata,
Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina
Novikova, Jessica Zosa Forde, Xiangru Tang, Jungo
Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat,
Miruna Clinciu, Najoung Kim, Newton Cheng,
Oleg Serikov, Omer Antverg, Oskar van der Wal,
Rui Zhang, Ruochen Zhang, Sebastian Gehrmann,
Shachar Mirkin, S. Osher Pais, Tatiana Shavrina,
Thomas Scialom, Tian Yun, Tomasz Limisiewicz,
Verena Rieser, Vitaly Protasov, Vladislav Mikhailov,
Yada Pruksachatkun, Yonatan Belinkov, Zachary
Bamberger, Zdenvek Kasner, Alice Rueda, Amanda
Pestana, Amir Feizpour, Ammar Khan, Amy
Faranak, Ananda Santa Rosa Santos, Anthony
Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo
Abdollahi, Aycha Tammour, Azadeh HajiHosseini,
Bahareh Behroozi, Benjamin Olusola Ajibade,
Bharat Kumar Saxena, Carlos Muñoz Ferrandis,
Danish Contractor, David M. Lansky, Davis David,
Douwe Kiela, Duong Anh Nguyen, Edward Tan,
Emily Baylor, Ezinwanne Ozoani, Fatim T Mirza,
Frankline Ononiwu, Habib Rezanejad, H.A. Jones,
Indrani Bhattacharya, Irene Solaiman, Irina Sedenko,
Isar Nejadgholi, Jan Passmore, Joshua Seltzer,
Julio Bonis Sanz, Karen Fort, Lívia Macedo
Dutra, Mairon Samagaio, Maraim Elbadri, Margot
Mieskes, Marissa Gerchick, Martha Akinlolu,
Michael McKenna, Mike Qiu, M. K. K. Ghauri,
Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour
Elkott, Nourhan Fahmy, Olanrewaju Samuel, Ran
An, R. P. Kromann, Ryan Hao, Samira Alizadeh,
Sarmad Shubber, Silas L. Wang, Sourav Roy, Sylvain
Viguier, Thanh-Cong Le, Tobi Oyebade, Trieu
Nguyen Hai Le, Yoyo Yang, Zachary Kyle Nguyen,
Abhinav Ramesh Kashyap, A. Palasciano, Alison
Callahan, Anima Shukla, Antonio Miranda-Escalada,
Ayush Kumar Singh, Benjamin Beilharz, Bo Wang,
Caio Matheus Fonseca de Brito, Chenxi Zhou, Chirag
Jain, Chuxin Xu, Clémentine Fourrier, Daniel Le’on
Perin’an, Daniel Molano, Dian Yu, Enrique
Manjavacas, Fabio Barth, Florian Fuhrimann,
Gabriel Altay, Giyaseddin Bayrak, Gully Burns,
Helena U. Vrabec, Iman I.B. Bello, Isha Dash, Ji Soo
Kang, John Giorgi, Jonas Golde, Jose David Posada,
Karthi Sivaraman, Lokesh Bulchandani, Lu Liu,
Luisa Shinzato, Madeleine Hahn de Bykhovetz,
Maiko Takeuchi, Marc Pàmies, María Andrea
Castillo, Marianna Nezhurina, Mario Sanger,
Matthias Samwald, Michael Cullan, Michael
Weinberg, M Wolf, Mina Mihaljcic, Minna
Liu, Moritz Freidank, Myungsun Kang, Natasha
Seelam, Nathan Dahlberg, Nicholas Michio Broad,
Nikolaus Muellner, Pascale Fung, Patricia Haller,
R. Chandrasekhar, R. Eisenberg, Robert Martin,
Rodrigo L. Canalli, Rosaline Su, Ruisi Su, Samuel
Cahyawijaya, Samuele Garda, Shlok S Deshmukh,
Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee
Sang-aroonsiri, Srishti Kumar, Stefan Schweter,
Sushil Pratap Bharati, T. A. Laud, Th’eo Gigant,
Tomoya Kainuma, Wojciech Kusa, Yanis Labrak,
Yashasvi Bajaj, Y . Venkatraman, Yifan Xu, Ying
Xu, Yun chao Xu, Zhee Xao Tan, Zhongli Xie,
Zifan Ye, Mathilde Bras, Younes Belkada, and
Thomas Wolf. 2022. Bloom: A 176b-parameter
open-access multilingual language model. ArXiv,
abs/2211.05100.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proximal
policy optimization algorithms. arXiv preprint
arXiv:1707.06347.
Noam Shazeer. 2020. Glu variants improve transformer.
arXiv preprint arXiv:2002.05202.
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 26):

用户：鸡和兔在一个笼子里，共有26个头，68只脚，那么鸡有多少只，兔有多少只？
Baichuan 2-13B-Chat：我们可以用代数方法来解决这个问题。假设鸡的数量为x，兔子的
数量为y。根据题目给出的信息，我们可以得到以下两个方程：
1. x + y = 26 (头的总数) 2. 2x + 4y = 68 (脚的总数)
现在我们需要解这个二元一次方程组。首先从第一个方程中解出x：
x = 26 - y
然后将x的表达式代入第二个方程：
2(26 - y) + 4y = 68 52 - 2y + 4y = 68 2y = 16
接下来我们求解y的值：
y = 16 / 2 y = 8
现在我们得到了兔子的数量是8只。将y的值代入x的表达式求得鸡的数量：
x = 26 - 8 x = 18
所以鸡有18只，兔子有8只。
用户：以下天文学常识题目，哪一个是错误的? A.太阳系是指由太阳和围绕着它运行的
八大行星、矮行星、卫星、小行星带和彗星组成的一个行星系统。B.卫星是指绕行星
或其他天体运动的天体。C.彗星是指太阳系中一种较小的天体，其核心由冰和尘埃组
成。D.按一般的天体归类方法，月球属于行星。
Baichuan 2-13B-Chat：D.按一般的天体归类方法，月球属于行星是错误的。实际上，月球
是地球的卫星，不属于行星范畴。
用户：以下物理常识题目，哪一个是错误的?A.在自然环境下，声音在固体中传播速
度最快。B.牛顿第一定律:一个物体如果不受力作用，将保持静止或匀速直线运动的状
态。C.牛顿第三定律:对于每个作用力，都有一个相等而反向的反作用力。D.声音在空气
中的传播速度为1000m/s。
Baichuan 2-13B-Chat：本题考查物理基本概念和原理的理解和应用能力。A项：声音的传
播需要介质，在固体、液体和气体的三种介质中，声音在固体中的传播速度是最快的，
故A正确； B项：牛顿第一定律是惯性定律，其内容是：一个物体如果没有受到外力的
作用，要么保持静止状态，要么保持匀速直线运动状态，直到有外力迫使它改变运动状
态为止，故B正确； C项：牛顿第三定律的内容是：对于每一个作用力，都有一个大小
相等、方向相反的反作用力，这两个力分别作用在两个相互作用的物体上，故C错误；
D项：声音在空气中以波的形式传播，其传播速度与介质的温度、湿度等因素有关，一
般情况下，声音在空气中的传播速度约为340m/s左右，而不是1000m/s，故D错误。 因
此，本题的正确答案是D 。



Source: data\tc17_2312.15883v2\referenced_papers\[86]_2312.14335.pdf (Page 1):

tially produce more faithful summarization. A few works [Sh i+23; PCM22] make use of pointwise mutual
information to let the LM focus more on the input context to re duce hallucination.
In this work, we focus on Context-aware Decoding [Shi+23] (C AD). CAD leverages the idea of point-
wise mutual information and proposes a product-of-experts enhancement to the vanilla decoding method
to make the generation more conditioned on the input evidenc e. W e conduct a large-scale reproducibility
study with different choices of language models to study CAD ’s effectiveness in query-focused summariza-
tion, and include more rigorous analysis on computational c omplexity and hyperparameter sensitivity . Ex-
tensive experiments show that CAD improves QFS quality by (1 ) reducing factual mistakes/hallucinations
while (2) mostly retaining the match of lexical patterns, me asured by ROUGE scores. On the other hand,
CAD also introduces additional inference-time FLOPs and po tentially slows down decoding speed.
2 Background
Context-aware Decoding.Consider a language model parameterized by θ , a context c, an input query x,
and we want to generate a piece of text y. In QFS paradigm, c can be the source document, x and y will
be the query and summary , respectively . In the vanilla decod ing scenario, each token of generation y is
sampled autoregressively from the output probability dist ribution from language model θ conditioned on
input query x and context c:
yt ∼ pθ (yt |c, x, y<t ) ∝ Softmax
(
logitθ (yt |c, x, y<t )/τ
)
(1)
Here τ is a temperature hyperparameter to control the sharpness of probability distribution over the candi-
date vocabulary V. Examples of the sampling strategy include greedy and T op- p sampling [Hol+19].
As we are operating on a conditional generation problem, we h ope the generation is more conditioned
on input context c. [Shi+23] utilize the idea of pointwise mutual information (PMI). W e consider two events
c and yt :
PMI(pθ (yt ; c, x, y<t )) =log
( pθ (yt |c, x, y<t )
pθ (yt |x, y<t )
)
(2)
The interpretation of the above equation is we would like to m easure the association of event yt (predicting
a speciﬁc token) and event c (the presence of context c). pθ (yt |x, y<t ) is the prior probability , representing
the model’s prior belief from its parameters θ when no presence of context c, whereas pθ (yt |c, x, y<t ) is the
posterior probability with c. T o leverage PMI, Shi et al. [Shi+23] propose to multiply PMI with the original
probability distribution:
yt ∼ ˜pθ (yt |c, x, y<t ) ∝ pθ (yt |c, x, y<t )
( pθ (yt |c, x, y<t )
pθ (yt |x, y<t )
) α
(3)
Here α is a hyperparameter to control the PMI weighting. W e still ne ed to normalize the above equation to
acquire a valid probability distribution, rearranging the terms yields
yt ∼ Softmax
[(
(1 + α ) logitθ (yt |c, x, y<t ) − α logitθ (yt |x, y<t )
)
/τ
]
(4)
Here larger α yields more weight on the context c and α = 0 reduces to vanilla decoding. [Shi+23] recom-
mend setting α = 0.5 for summarization tasks and we also examine this choice i n Section 3.
Analysis of Computational Cost. W e follow the notations used in [Kap+20]. Let us consider the T rans-
former architecture with following hyperparameters: numb er of layers nlayer, dimension of the embedding
layer and intermediate representation dmodel, dimension of the intermediate feedforward layer dff, dimen-
sion of the attention output dattn and number of attention heads nheads ; and denote the number of tokens
in the input sequence as ninput . The model size N is the number of non-embedding parameters of the model,
N ≈ | θ |:
N ≈ 2 dmodel nlayer (2 dattn + dff ) =12 nlayer d2
model with dattn = dff/4 = dmodel (5)
The total FLOPs per token in one forward pass is approximatel y Cforward = 2N + 2nlayer ninput dattn (see
Section 2.1 in [Kap+20] for details). In LLMs, dmodel ≫ ninput /12, we can roughly ignore the second term
and use Cforward ≈ 2N.
W ith the above analysis, we know that performing one forward pass of CAD yields in total (2 |y<t | +
2 |x| + |c|) · Cforward FLOPs compared to (|y<t | + |x| + |c|) · Cforward from vanilla decoding.
2



Source: data\tc17_2312.15883v2\referenced_papers\[44]_2305.13269.pdf (Page 16):

Published as a conference paper at ICLR 2024
Incorrect query 4: select distinct ?obj where { ?obj wdt:/instance of/ wd:/magazine/ . wd:/Delta Air
Lines/ wdt:/house publication/ ?obj . }
D.2 F LASHCARDS (NATURAL SENTENCE )
D.2.1 I NSTRUCTION -TUNING DATASET
We employ a natural sentence format for querying Medical knowledge. To instruction-tune our
AQG specifically for this purpose, we utilize the Medical Flashcards dataset (Han et al., 2023). This
dataset consists of question-answering pairs covering various subjects in the medical source, such
as anatomy, physiology, pathology, and pharmacology. It contains summaries and mnemonics of
crucial medical concepts, making it an ideal choice for instruction-tuning the AQG to effectively
handle medical knowledge queries. The size of the dataset is provided in Table 8.
D.2.2 D ATA EXAMPLE
Answer the question truthfully.
### Instruction:
Answer this question truthfully.
### Input:
What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ levels?
### Output:
Very low Mg2+ levels correspond to low PTH levels which in turn results in low Ca2+ levels.
D.3 S CIENCE QA P HYSICS (NATURAL SENTENCE )
D.3.1 I NSTRUCTION -TUNING DATASET
To instruction-tune our AQG for physics knowledge, we utilize the physics segment of the Sci-
enceQA dataset (Lu et al., 2022). Each entry in this dataset consists of a question, options, context,
answer, lecture, and explanation. The lecture contains necessary knowledge to answer the question.
We use the question and the options as input and the lecture as the output for instruction-tuning the
model.
D.3.2 D ATA EXAMPLE
Answer the question truthfully.
### Instruction:
Answer this question truthfully.
### Input:
The objects are identical except for their temperatures. Which object has less thermal energy?
Choose from: a 300-gram glass of water at a temperature of 75°F, a 300-gram glass of water at a
temperature of 80°F.
### Output:
The two glasses of water have the same mass but different temperatures. Since the 75°F glass of
water is colder than the 80°F glass of water, it has less thermal energy.
D.4 S CIENCE QA B IOLOGY (NATURAL SENTENCE )
D.4.1 I NSTRUCTION -TUNING DATASET
To instruction-tune our AQG for biology knowledge, we utilize the biology segment of the Sci-
enceQA dataset (Lu et al., 2022).
D.4.2 D ATA EXAMPLE
Answer the question truthfully.
### Instruction:
Answer this question truthfully.
17



Source: data\tc17_2312.15883v2\referenced_papers\[87]_2309.10305.pdf (Page 5):

models, we can obtain a mapping from the training
flops to the target loss.
Figure 4: The scaling law of Baichuan 2. We trained
various models ranging from 10 million to 3 billion
parameters with 1 trillion tokens. By fitting a power
law term to the losses given training flops, we predicted
losses for training Baichuan 2-7B and Baichuan 2-13B
on 2.6 trillion tokens. This fitting process precisely
predicted the final models’ losses (marked with two
stars).
To fit the scaling law of the model, we employed
the formula given by Henighan et al. (2020):
LC = a × Cb + L∞ (2)
where L∞ is the irreducible loss and the first
term is the reducible loss which is formulated as a
power-law scaling term. C are training flops and
the LC are final loss of the model in that flops. We
used the curve_fit function from the SciPy 4
library to fit the parameters. The final fitted scaling
curve and the predicted 7 billion and 13 billion
parameters model’s final loss are shown in Figure
4. We can see that the fitted scaling law predicted
Baichuan 2’s final loss with high accuracy.
2.7 Infrastructure
Efficiently leveraging existing GPU resources
plays a critically important role in training and
developing large language models today. To
accomplish this, we develop a co-design approach
for an elastic training framework and a smart
cluster scheduling policy.
Since our GPUs are shared among multiple
users and tasks, the specific behavior of each
task is unpredictable, often leading to idle GPU
nodes within the cluster. Considering that a single
machine equipped with eight A800 GPUs could
adequately meet the memory requirements for our
Baichuan 2-7B and Baichuan 2-13B models, the
4https://scipy.org/
primary design criterion for our training framework
is the machine-level elasticity, which supports that
resources for tasks can be dynamically modified
according to the cluster status and thereby serves as
the foundation for our smart scheduling algorithm.
To meet the requirement of the machine-level
elasticity, our training framework integrates tensor
parallelism (Narayanan et al., 2021) and ZeRO-
powered data parallelism (Rajbhandari et al.,
2020), where we set tensor parallelism inside each
machine and employ ZeRO shared data parallelism
for elastic scaling across machines.
In addition, we employ a tensor-splitting
technique (Nie et al., 2022) where we split certain
calculations to reduce peak memory consumption,
such as the cross-entropy calculations with large
vocabularies. This approach enables us to meet
memory needs without extra computing and
communication, making the system more efficient.
To further accelerate training without
compromising model accuracy, we implement
mixed-precision training, where we perform
forward and backward computations in BFloat16,
while performing optimizer updating in Float32.
Furthermore, in order to efficiently scale our
training cluster to thousands of GPUs, we integrate
the following techniques to avoid the degradation
of communication efficiency:
• Topology-aware distributed training. In large-
scale clusters, network connections frequently
span multiple layers of switches. We strategically
arrange the ranks for distributed training
to minimize frequent access across different
switches, which reduces latency and thereby
enhances overall training efficiency.
• Hybrid and hierarchical partition for ZeRO .
By partitioning parameters across GPUs,
ZeRO3 reduces memory consumption at the
expense of additional all-gather communications.
This approach would lead to a significant
communication bottleneck when scaling to
thousands of GPUs (Jiang et al., 2023a). To
address this issue, we propose a hybrid and
hierarchical partitioning scheme. Specifically,
our framework first partitions the optimizer states
across all GPUs, and then adaptively decides
which layers need to activate ZeRO3, and
whether partitioning parameters hierarchically.
By integrating these strategies, our system is
capable of training Baichuan 2-7B and Baichuan
2-13B models efficiently on 1,024 NVIDIA A800



## Processing Completed
Finished at: 2025-01-24 17:49:36
